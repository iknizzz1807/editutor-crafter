{"html":"<h1 id=\"resumable-file-upload-service-design-document\">Resumable File Upload Service: Design Document</h1>\n<h2 id=\"overview\">Overview</h2>\n<p>A production-grade file upload service that enables reliable transfer of large files through chunked uploads with resumption capabilities, virus scanning, and pluggable storage backends. The key architectural challenge is maintaining upload state consistency across network failures while providing secure, scalable file processing and storage abstraction.</p>\n<blockquote>\n<p>This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.</p>\n</blockquote>\n<h2 id=\"context-and-problem-statement\">Context and Problem Statement</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (1, 2, 3) — this foundational understanding applies to chunked uploads, storage abstraction, and virus scanning</p>\n</blockquote>\n<h3 id=\"the-large-file-transfer-problem\">The Large File Transfer Problem</h3>\n<p>Think of traditional HTTP file uploads like trying to carry a massive, fragile glass sculpture across a busy city street in one trip. If you stumble even once, the entire sculpture shatters and you have to start over from the beginning. Now imagine that the street has unpredictable traffic patterns, occasional power outages, and your destination is several miles away. This is exactly what happens when users attempt to upload large files over unreliable network connections using standard HTML form uploads or simple HTTP POST requests.</p>\n<p>In the real world, users routinely attempt to upload files ranging from hundreds of megabytes to several gigabytes. Video content creators upload raw footage files that can exceed 10GB. Medical institutions transfer high-resolution imaging data that approaches 1GB per scan. Engineering teams share build artifacts, VM images, and dataset exports that dwarf typical web content. Scientific research organizations routinely work with simulation outputs and experimental datasets measured in terabytes.</p>\n<p>The fundamental problem with traditional upload approaches becomes apparent when we examine the failure modes. A standard HTTP file upload requires the entire file to be transmitted in a single, uninterrupted stream. If the network connection drops at 95% completion of a 5GB upload that has been running for three hours over a slow connection, the entire transfer fails and must be restarted from zero bytes. The client has no mechanism to communicate &quot;I already sent you the first 4.75GB successfully&quot; and the server has no standard way to acknowledge partial progress.</p>\n<p><strong>Business Impact Analysis</strong></p>\n<p>The business consequences of upload failures extend far beyond user frustration. Consider a video production company where editors work with 4K raw footage files averaging 8GB each. If their internet connection experiences typical residential instability with brief disconnections every 30-60 minutes, successfully uploading a single file becomes practically impossible through traditional means. The editor may attempt the same upload dozens of times, consuming enormous bandwidth while never achieving success. This translates directly to lost productivity, missed deadlines, and increased infrastructure costs from wasted bandwidth.</p>\n<p>Medical imaging presents an even more critical scenario. A radiologist attempting to upload urgent CT scan data for remote consultation faces a time-sensitive situation where upload failures can delay patient diagnosis. The 800MB scan that fails to upload after 45 minutes of progress doesn&#39;t just represent technical inconvenience—it represents potential impact on patient care quality.</p>\n<p>Enterprise scenarios amplify these costs through scale effects. A software company with distributed development teams uploading build artifacts to shared repositories may see dozens of developers each struggling with failed uploads of 500MB+ build outputs. The cumulative productivity loss, multiplied across engineering teams and compounded by CI/CD pipeline delays, can reach thousands of dollars per incident in lost developer time.</p>\n<p><strong>Network Reality Assessment</strong></p>\n<p>Real-world network conditions bear little resemblance to the stable, high-bandwidth connections assumed by traditional upload mechanisms. Mobile connections frequently transition between cell towers, causing brief disconnections. Home broadband experiences periodic congestion during peak usage hours. Corporate networks implement aggressive timeout policies that terminate long-running connections. WiFi connections in airports, coffee shops, and co-working spaces provide intermittent connectivity with unpredictable quality fluctuations.</p>\n<p>The statistics paint a stark picture: according to industry measurements, the probability of a network connection remaining stable decreases exponentially with connection duration. A 5-minute HTTP connection has roughly 95% reliability, but a 3-hour connection required for large file upload may see reliability drop below 20% on typical residential connections. This mathematical reality makes traditional upload approaches fundamentally unsuitable for large files over real-world networks.</p>\n<blockquote>\n<p>The core insight is that network failures are not exceptional edge cases to handle gracefully—they are the dominant characteristic of long-duration connections that must be designed for from the ground up.</p>\n</blockquote>\n<h3 id=\"existing-solutions-analysis\">Existing Solutions Analysis</h3>\n<p><strong>HTTP Multipart Uploads</strong></p>\n<p>Standard HTTP multipart form uploads represent the baseline approach implemented by most web applications. The client encodes the file data using <code>multipart/form-data</code> encoding and transmits it via a single HTTP POST request. The server receives the entire payload, buffers it in memory or temporary storage, and processes it atomically.</p>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>Behavior</th>\n<th>Limitation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Failure Recovery</td>\n<td>None - entire upload lost on disconnection</td>\n<td>Complete restart required for any interruption</td>\n</tr>\n<tr>\n<td>Progress Tracking</td>\n<td>Client-side only, no server acknowledgment</td>\n<td>No way to resume from known good state</td>\n</tr>\n<tr>\n<td>Memory Usage</td>\n<td>Server buffers entire file during upload</td>\n<td>Memory exhaustion with large files or concurrent uploads</td>\n</tr>\n<tr>\n<td>Timeout Handling</td>\n<td>Standard HTTP timeout applies to entire upload</td>\n<td>Large files exceed typical timeout values</td>\n</tr>\n<tr>\n<td>Protocol Complexity</td>\n<td>Minimal - standard HTTP</td>\n<td>No extensibility for advanced features</td>\n</tr>\n</tbody></table>\n<p>The fundamental architectural flaw in multipart uploads is their atomic nature. Success requires uninterrupted transmission of the complete file within the HTTP timeout window. This creates a binary outcome: complete success or complete failure, with no middle ground for partial progress preservation.</p>\n<p><strong>tus.io Resumable Upload Protocol</strong></p>\n<p>The tus.io specification emerged from recognition that file uploads required a purpose-built protocol rather than attempting to force-fit standard HTTP mechanisms. Think of tus.io as transforming file upload from &quot;carrying the entire sculpture in one trip&quot; to &quot;moving it brick by brick, with each brick permanently placed before moving the next one.&quot;</p>\n<table>\n<thead>\n<tr>\n<th>Protocol Feature</th>\n<th>Implementation</th>\n<th>Benefit</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Upload Initialization</td>\n<td><code>POST</code> to create upload session with metadata</td>\n<td>Server allocates space and returns upload URL</td>\n</tr>\n<tr>\n<td>Offset Discovery</td>\n<td><code>HEAD</code> request returns <code>Upload-Offset</code> header</td>\n<td>Client learns exactly where to resume</td>\n</tr>\n<tr>\n<td>Chunked Transfer</td>\n<td><code>PATCH</code> requests append data at specific offsets</td>\n<td>Incremental progress with failure isolation</td>\n</tr>\n<tr>\n<td>Progress Verification</td>\n<td>Server validates each chunk before acknowledgment</td>\n<td>Corruption detection at chunk boundaries</td>\n</tr>\n<tr>\n<td>Session Management</td>\n<td>Upload URLs remain valid for configurable TTL</td>\n<td>Clients can resume after arbitrary delays</td>\n</tr>\n</tbody></table>\n<p>The tus.io approach fundamentally changes the failure model. Instead of &quot;all or nothing,&quot; uploads become a series of small, independent operations. If chunk 47 out of 200 fails to upload, only that specific chunk needs retransmission. The other 46 chunks remain safely stored on the server, preserving the investment in already-transmitted data.</p>\n<p>The protocol&#39;s offset-based resume mechanism provides precise recovery semantics. When a client reconnects after a network failure, a simple <code>HEAD</code> request to the upload URL returns the current server-side offset. The client compares this against its local progress tracking and resumes transmission from the exact byte position where the server&#39;s knowledge ends.</p>\n<p><strong>Cloud-Specific Solutions</strong></p>\n<p>Major cloud providers offer sophisticated multipart upload APIs optimized for their storage infrastructure. These solutions emerged from the same fundamental recognition that large file transfers require specialized handling, but they tie implementations to specific cloud ecosystems.</p>\n<p>Amazon S3&#39;s multipart upload API exemplifies the cloud approach. Clients initiate an upload session that returns an <code>UploadId</code>, then upload individual parts identified by part numbers. Each part can be uploaded independently and in parallel, with the server tracking completion status. Once all parts are uploaded, the client issues a completion request that assembles the parts into the final object.</p>\n<table>\n<thead>\n<tr>\n<th>Cloud Solution</th>\n<th>Strengths</th>\n<th>Constraints</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>S3 Multipart Upload</td>\n<td>Native cloud integration, parallel part uploads</td>\n<td>AWS-specific, minimum 5MB per part (except last)</td>\n</tr>\n<tr>\n<td>Google Cloud Resumable Upload</td>\n<td>Single-stream resume with offset tracking</td>\n<td>GCP-specific, limited to Google Cloud Storage</td>\n</tr>\n<tr>\n<td>Azure Blob Block Upload</td>\n<td>Block-based composition with parallel upload</td>\n<td>Azure-specific, complex block ID management</td>\n</tr>\n</tbody></table>\n<p>The primary limitation of cloud-specific solutions is vendor lock-in. An application built around S3&#39;s multipart upload API cannot easily migrate to Google Cloud Storage or local storage without significant architectural changes. This creates strategic flexibility constraints for organizations that may need to change storage backends in response to cost, compliance, or performance requirements.</p>\n<blockquote>\n<p><strong>Decision: Protocol Foundation Choice</strong></p>\n<ul>\n<li><strong>Context</strong>: Need resumable upload capability that works across multiple storage backends while providing robust failure recovery</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>HTTP multipart (simple but no resume capability)</li>\n<li>tus.io protocol (standardized resume protocol)</li>\n<li>Cloud-specific APIs (optimized but vendor-locked)</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement tus.io-compatible protocol with storage backend abstraction</li>\n<li><strong>Rationale</strong>: tus.io provides standardized resume semantics while remaining storage-agnostic. The protocol&#39;s offset-based resume mechanism offers precise failure recovery, and the specification&#39;s maturity provides proven real-world reliability.</li>\n<li><strong>Consequences</strong>: Enables client compatibility with existing tus.io libraries while allowing flexible storage backend selection. Requires implementing protocol state management but provides superior failure recovery compared to alternatives.</li>\n</ul>\n</blockquote>\n<h3 id=\"core-technical-challenges\">Core Technical Challenges</h3>\n<p>The transition from simple HTTP uploads to resumable chunked uploads introduces several fundamental technical challenges that must be solved systematically. Each challenge represents a category of complexity that doesn&#39;t exist in traditional upload scenarios.</p>\n<p><strong>Upload Session State Management</strong></p>\n<p>Traditional HTTP uploads are stateless—each request is independent and the server doesn&#39;t need to track any information between requests. Resumable uploads invert this model completely, requiring the server to maintain persistent state about ongoing upload sessions that may span hours or days.</p>\n<p>Consider the state information that must be tracked for each upload session: the total expected file size, the current byte offset representing successfully received data, metadata about the target file (name, content type, destination path), the timestamp of the last activity (for session expiration), and the current session status (initializing, active, completing, failed, expired). This state must survive server restarts, which means it cannot be kept only in memory.</p>\n<p>The state management challenge extends to concurrent access patterns. Multiple chunks of the same file might be uploaded in parallel, requiring atomic updates to the session&#39;s progress tracking. A chunk uploaded to offset 1000-2000 must not be marked as received until it&#39;s safely written to storage, but the server must also prevent duplicate uploads of the same byte range if the client retries due to a timeout.</p>\n<table>\n<thead>\n<tr>\n<th>State Management Challenge</th>\n<th>Technical Requirement</th>\n<th>Implementation Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Session Persistence</td>\n<td>Survive server restarts and crashes</td>\n<td>Database or persistent file-based storage</td>\n</tr>\n<tr>\n<td>Concurrent Access</td>\n<td>Multiple chunks updating same session</td>\n<td>Atomic operations with proper locking</td>\n</tr>\n<tr>\n<td>Expiration Handling</td>\n<td>Clean up abandoned uploads</td>\n<td>Background cleanup process with configurable TTL</td>\n</tr>\n<tr>\n<td>Progress Tracking</td>\n<td>Byte-level accuracy for resume operations</td>\n<td>Efficient data structures for sparse range tracking</td>\n</tr>\n<tr>\n<td>Metadata Storage</td>\n<td>File attributes, security context, validation state</td>\n<td>Flexible schema supporting future extensions</td>\n</tr>\n</tbody></table>\n<p><strong>Partial Failure Handling and Recovery</strong></p>\n<p>Resumable uploads introduce a complex failure landscape that doesn&#39;t exist in atomic operations. Traditional uploads either succeed completely or fail completely—there&#39;s no intermediate state requiring recovery logic. Chunked uploads create scenarios where parts of the operation succeed while other parts fail, requiring sophisticated detection and recovery mechanisms.</p>\n<p>Consider a typical failure scenario: a client uploads chunks 1-5 successfully, chunk 6 fails due to a network timeout, chunks 7-8 succeed, then the client disconnects entirely. The server must be able to communicate that bytes 0-5120 and bytes 7168-8192 are safely stored, but bytes 5120-7168 need retransmission. This requires not just tracking the current offset, but potentially tracking arbitrary ranges of received data.</p>\n<p>Storage-level failures add another dimension of complexity. If a chunk is received successfully by the upload service but the write to the storage backend fails, should the server report success or failure to the client? If it reports success but the data isn&#39;t actually stored, the file will be corrupted. If it reports failure but retains the data in temporary storage, what happens if the client never retries?</p>\n<p>The consistency requirements become even more challenging during the final assembly phase. After all chunks are uploaded, they must be assembled into the final file atomically. If this assembly process fails partway through, the server must be able to retry the assembly without requiring clients to re-upload chunks.</p>\n<p><strong>Storage Backend Coordination</strong></p>\n<p>A resumable upload service that supports multiple storage backends must solve coordination challenges that don&#39;t exist when uploading directly to a single storage system. Each storage backend has different characteristics, limitations, and optimal usage patterns that must be abstracted behind a common interface.</p>\n<p>Local filesystem storage allows random-access writes, enabling chunks to be written directly to their final positions within the target file. However, this requires pre-allocating the file to the expected size and managing concurrent writes to different file regions safely. File locking, space allocation failures, and filesystem-specific constraints (like maximum file sizes) must all be handled transparently.</p>\n<p>Cloud storage backends like S3 use fundamentally different models. S3&#39;s multipart upload API requires parts to be uploaded in sequence and assembled at the end—you cannot write arbitrary byte ranges to arbitrary offsets. The service must buffer chunks locally until they can be uploaded to S3 as properly-sized parts, introducing additional storage and complexity requirements.</p>\n<table>\n<thead>\n<tr>\n<th>Storage Backend</th>\n<th>Upload Model</th>\n<th>Coordination Challenge</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Local Filesystem</td>\n<td>Random-access writes to pre-allocated file</td>\n<td>Concurrent access control, space management</td>\n</tr>\n<tr>\n<td>Amazon S3</td>\n<td>Sequential multipart upload with final assembly</td>\n<td>Local buffering, part size constraints</td>\n</tr>\n<tr>\n<td>Google Cloud Storage</td>\n<td>Resumable upload with offset-based appends</td>\n<td>Single-stream coordination, retry handling</td>\n</tr>\n<tr>\n<td>Database BLOB Storage</td>\n<td>Atomic replacement with versioning</td>\n<td>Transaction management, size limitations</td>\n</tr>\n</tbody></table>\n<p>The abstraction layer must handle these differences transparently while providing consistent semantics to the upload protocol layer. This requires careful interface design that can accommodate the capabilities and constraints of each backend without forcing the lowest common denominator approach that would eliminate the benefits of backend-specific optimizations.</p>\n<p><strong>Data Integrity and Corruption Detection</strong></p>\n<p>Chunked uploads distributed across time and potentially across multiple network connections introduce data integrity challenges that require systematic solutions. Unlike atomic uploads where corruption is immediately apparent (the file doesn&#39;t match expected checksums), resumable uploads can accumulate corruption gradually as individual chunks are received and stored.</p>\n<p>Each chunk must be validated independently before being considered successfully received. This validation typically includes verifying that the chunk&#39;s size matches the declared size, that it&#39;s being written to the correct offset, and that its content matches any provided checksums. However, chunk-level integrity doesn&#39;t guarantee file-level integrity—corruption can be introduced during the final assembly process or through storage backend issues.</p>\n<p>The service must also handle scenarios where the client and server disagree about what data has been successfully transferred. If the server believes it has received chunks 1-10 but the client believes only chunks 1-8 were acknowledged, the disagreement must be detected and resolved without corrupting the final file.</p>\n<blockquote>\n<p>The fundamental insight is that resumable uploads transform file transfer from a single point of failure into a distributed system problem, requiring solutions for state management, partial failures, backend coordination, and data integrity that don&#39;t exist in traditional upload architectures.</p>\n</blockquote>\n<p>This complexity justifies the architectural investment required to build a robust resumable upload system, but it also highlights why many applications continue to use simpler approaches despite their limitations. The following sections detail how each of these challenges can be systematically addressed through careful design and implementation.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p><strong>A. Technology Recommendations Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Server</td>\n<td><code>net/http</code> with standard library</td>\n<td><code>gin-gonic/gin</code> or <code>gorilla/mux</code> for routing</td>\n</tr>\n<tr>\n<td>State Persistence</td>\n<td>Local SQLite database</td>\n<td>PostgreSQL or Redis for production</td>\n</tr>\n<tr>\n<td>File Storage</td>\n<td><code>os.File</code> with direct filesystem operations</td>\n<td><code>afero</code> filesystem abstraction library</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td><code>encoding/json</code> with config files</td>\n<td><code>spf13/viper</code> for multiple config sources</td>\n</tr>\n<tr>\n<td>Logging</td>\n<td><code>log/slog</code> standard library</td>\n<td><code>sirupsen/logrus</code> or <code>uber-go/zap</code></td>\n</tr>\n<tr>\n<td>HTTP Client Testing</td>\n<td><code>net/http/httptest</code></td>\n<td><code>jarcoal/httpmock</code> for complex scenarios</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure</strong></p>\n<p>Understanding the problem space helps organize code into logical packages that separate concerns effectively:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>resumable-upload-service/\n├── cmd/\n│   └── server/\n│       └── main.go                    ← entry point, configuration loading\n├── internal/\n│   ├── protocol/                      ← tus.io protocol implementation\n│   │   ├── handler.go                 ← HTTP handlers for upload operations\n│   │   ├── session.go                 ← upload session management\n│   │   └── protocol_test.go           ← protocol compliance tests\n│   ├── storage/                       ← storage backend abstraction\n│   │   ├── interface.go               ← common storage interface\n│   │   ├── local.go                   ← local filesystem implementation\n│   │   ├── s3.go                      ← S3-compatible implementation\n│   │   └── storage_test.go            ← backend integration tests\n│   ├── validation/                    ← file validation and virus scanning\n│   │   ├── scanner.go                 ← virus scanning integration\n│   │   ├── validator.go               ← file type and size validation\n│   │   └── quarantine.go              ← suspicious file handling\n│   ├── state/                         ← session state management\n│   │   ├── store.go                   ← state persistence interface\n│   │   ├── sqlite.go                  ← SQLite implementation\n│   │   └── memory.go                  ← in-memory for testing\n│   └── config/\n│       └── config.go                  ← configuration structures\n├── pkg/                               ← public APIs (if building a library)\n├── test/\n│   ├── integration/                   ← end-to-end test scenarios\n│   └── fixtures/                      ← test data and mock files\n└── docs/\n    └── api/                           ← API documentation</code></pre></div>\n\n<p>This structure reflects the core technical challenges identified in the problem analysis. The <code>protocol</code> package handles tus.io implementation and session state management. The <code>storage</code> package provides backend abstraction for different storage systems. The <code>validation</code> package addresses security and file integrity concerns. The <code>state</code> package manages upload session persistence across server restarts.</p>\n<p><strong>C. Infrastructure Starter Code</strong></p>\n<p>The following provides complete, working infrastructure that handles the foundational requirements, allowing focus on the core resumable upload logic:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/config/config.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> config</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Config represents the complete service configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Config</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Server   </span><span style=\"color:#B392F0\">ServerConfig</span><span style=\"color:#9ECBFF\">   `json:\"server\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Storage  </span><span style=\"color:#B392F0\">StorageConfig</span><span style=\"color:#9ECBFF\">  `json:\"storage\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Security </span><span style=\"color:#B392F0\">SecurityConfig</span><span style=\"color:#9ECBFF\"> `json:\"security\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Cleanup  </span><span style=\"color:#B392F0\">CleanupConfig</span><span style=\"color:#9ECBFF\">  `json:\"cleanup\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ServerConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Host         </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"host\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Port         </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `json:\"port\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ReadTimeout  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"read_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    WriteTimeout </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"write_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxBodySize  </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">         `json:\"max_body_size\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StorageConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Backend   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"backend\"`</span><span style=\"color:#6A737D\"> // \"local\", \"s3\", \"gcs\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LocalPath </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"local_path,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    S3Config  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">S3Config</span><span style=\"color:#9ECBFF\">         `json:\"s3,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Options   </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"options\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> S3Config</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Bucket          </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"bucket\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Region          </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"region\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AccessKeyID     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"access_key_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SecretAccessKey </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"secret_access_key\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Endpoint        </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"endpoint,omitempty\"`</span><span style=\"color:#6A737D\"> // for S3-compatible services</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SecurityConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxFileSize         </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">    `json:\"max_file_size\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AllowedContentTypes []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"allowed_content_types\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    VirusScanEnabled    </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">     `json:\"virus_scan_enabled\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ClamAVSocket        </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">   `json:\"clamav_socket,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CleanupConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SessionTTL      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"session_ttl\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CleanupInterval </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"cleanup_interval\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    QuarantineTTL   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"quarantine_ttl\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadConfig reads configuration from a JSON file</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> LoadConfig</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">filename</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    data, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">ReadFile</span><span style=\"color:#E1E4E8\">(filename)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"reading config file: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> config </span><span style=\"color:#B392F0\">Config</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> json.</span><span style=\"color:#B392F0\">Unmarshal</span><span style=\"color:#E1E4E8\">(data, </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">config); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"parsing config: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Set defaults for optional fields</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> config.Server.Host </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config.Server.Host </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"localhost\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> config.Server.Port </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config.Server.Port </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 8080</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> config.Server.ReadTimeout </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config.Server.ReadTimeout </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> config.Server.WriteTimeout </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config.Server.WriteTimeout </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> config.Server.MaxBodySize </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config.Server.MaxBodySize </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 32</span><span style=\"color:#F97583\"> &#x3C;&#x3C;</span><span style=\"color:#79B8FF\"> 20</span><span style=\"color:#6A737D\"> // 32MB default chunk size</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#E1E4E8\">config, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/state/memory.go - In-memory state store for testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> state</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MemoryStore provides an in-memory implementation of StateStore for testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MemoryStore</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu       </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sessions </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">UploadSession</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewMemoryStore creates a new in-memory state store</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewMemoryStore</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MemoryStore</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">MemoryStore</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sessions: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CreateSession stores a new upload session</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MemoryStore</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CreateSession</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">session</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> m.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> m.sessions[session.ID]; exists {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"session </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\"> already exists\"</span><span style=\"color:#E1E4E8\">, session.ID)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Create a deep copy to avoid external mutations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sessionCopy </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\">session</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sessionCopy.CreatedAt </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sessionCopy.UpdatedAt </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.sessions[session.ID] </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#E1E4E8\">sessionCopy</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetSession retrieves an upload session by ID</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MemoryStore</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetSession</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> m.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    session, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> m.sessions[sessionID]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">exists {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, ErrSessionNotFound</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Return a copy to prevent external mutations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sessionCopy </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\">session</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#E1E4E8\">sessionCopy, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// UpdateSession modifies an existing upload session</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MemoryStore</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">UpdateSession</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">session</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> m.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> m.sessions[session.ID]; </span><span style=\"color:#F97583\">!</span><span style=\"color:#E1E4E8\">exists {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> ErrSessionNotFound</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sessionCopy </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\">session</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sessionCopy.UpdatedAt </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.sessions[session.ID] </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#E1E4E8\">sessionCopy</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DeleteSession removes an upload session</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MemoryStore</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">DeleteSession</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> m.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> m.sessions[sessionID]; </span><span style=\"color:#F97583\">!</span><span style=\"color:#E1E4E8\">exists {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> ErrSessionNotFound</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    delete</span><span style=\"color:#E1E4E8\">(m.sessions, sessionID)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ListExpiredSessions returns sessions older than the specified TTL</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MemoryStore</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ListExpiredSessions</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ttl</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> m.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cutoff </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\">ttl)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> expired []</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> id, session </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> m.sessions {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> session.UpdatedAt.</span><span style=\"color:#B392F0\">Before</span><span style=\"color:#E1E4E8\">(cutoff) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            expired </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(expired, id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> expired, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code</strong></p>\n<p>The following skeletons map directly to the technical challenges identified in the problem analysis:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/protocol/session.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> protocol</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// UploadSession represents the state of an ongoing resumable upload</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> UploadSession</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ID            </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                 `json:\"id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Filename      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                 `json:\"filename\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ContentType   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                 `json:\"content_type\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TotalSize     </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">                  `json:\"total_size\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CurrentOffset </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">                  `json:\"current_offset\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Status        </span><span style=\"color:#B392F0\">SessionStatus</span><span style=\"color:#9ECBFF\">          `json:\"status\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StorageKey    </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                 `json:\"storage_key\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Metadata      </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">      `json:\"metadata\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CreatedAt     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">              `json:\"created_at\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    UpdatedAt     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">              `json:\"updated_at\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ChunkHashes   </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">       `json:\"chunk_hashes\"`</span><span style=\"color:#6A737D\"> // offset -> hash</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SessionStatus</span><span style=\"color:#F97583\"> string</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SessionStatusInitialized</span><span style=\"color:#B392F0\"> SessionStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"initialized\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SessionStatusActive</span><span style=\"color:#B392F0\">      SessionStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"active\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SessionStatusCompleting</span><span style=\"color:#B392F0\">  SessionStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"completing\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SessionStatusCompleted</span><span style=\"color:#B392F0\">   SessionStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"completed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SessionStatusFailed</span><span style=\"color:#B392F0\">      SessionStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"failed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SessionStatusExpired</span><span style=\"color:#B392F0\">     SessionStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"expired\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SessionManager handles upload session lifecycle and coordination</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SessionManager</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    store   </span><span style=\"color:#B392F0\">StateStore</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage </span><span style=\"color:#B392F0\">StorageBackend</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// InitializeUpload creates a new upload session and prepares storage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SessionManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">InitializeUpload</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">filename</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">totalSize</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">metadata</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Generate unique session ID using crypto/rand</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate filename and total size parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Create storage key/path for this upload</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Initialize storage backend for this upload (pre-allocate if needed)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Create UploadSession struct with initialized status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Store session in state store</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Return session with upload URL for client</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use uuid.New() for session ID generation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Storage key should include session ID to avoid conflicts</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    panic</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"implement me\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ProcessChunk handles an incoming chunk upload</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SessionManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ProcessChunk</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">offset</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">contentHash</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Retrieve upload session from state store</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate session status is active or initialized</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Verify offset matches current session offset (no gaps)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Validate chunk hash if provided</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Write chunk data to storage backend at correct offset</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Update session current offset and chunk tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Update session in state store with new progress</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: If this completes the upload, trigger completion process</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use crypto/sha256 to verify chunk hashes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Offset validation prevents chunks uploaded out of order</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    panic</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"implement me\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetUploadProgress returns current upload status and offset for resume</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SessionManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetUploadProgress</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Retrieve session from state store</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate session exists and is not expired</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Return session with current offset for client resume</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: This implements the tus.io HEAD request for offset discovery</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    panic</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"implement me\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints</strong></p>\n<p><strong>Go-Specific Implementation Tips:</strong></p>\n<ul>\n<li>Use <code>crypto/rand.Read()</code> with <code>encoding/hex</code> for generating secure session IDs</li>\n<li><code>os.OpenFile()</code> with <code>O_RDWR|O_CREATE</code> for file pre-allocation on local storage</li>\n<li><code>io.Copy()</code> with <code>io.LimitReader()</code> for safe chunk reading from HTTP requests</li>\n<li><code>sync.RWMutex</code> for session state that has frequent reads and occasional writes</li>\n<li><code>context.WithTimeout()</code> for all storage operations to prevent hanging</li>\n<li><code>os.File.Sync()</code> after writing chunks to ensure durability before acknowledging</li>\n<li><code>filepath.Clean()</code> and <code>filepath.IsAbs()</code> to sanitize file paths and prevent directory traversal</li>\n<li><code>net/http.MaxBytesReader()</code> to enforce chunk size limits and prevent DoS</li>\n</ul>\n<p><strong>Error Handling Patterns:</strong></p>\n<ul>\n<li>Create custom error types for different failure categories: <code>ErrSessionNotFound</code>, <code>ErrInvalidOffset</code>, <code>ErrStorageFailure</code></li>\n<li>Use <code>fmt.Errorf(&quot;operation failed: %w&quot;, err)</code> for error wrapping with context</li>\n<li>Implement retry logic with exponential backoff for transient storage failures</li>\n<li>Log errors with context: session ID, offset, operation type for debugging</li>\n</ul>\n<p><strong>Performance Considerations:</strong></p>\n<ul>\n<li>Buffer chunk writes using <code>bufio.Writer</code> to reduce syscall overhead</li>\n<li>Use <code>sync.Pool</code> for reusing byte buffers during chunk processing</li>\n<li>Implement connection pooling for S3 clients to avoid connection setup overhead</li>\n<li>Consider using <code>mmap</code> for large file operations on local storage</li>\n</ul>\n<p><strong>F. Milestone Checkpoint</strong></p>\n<p>After implementing the session management foundation, verify the following behavior:</p>\n<p><strong>Basic Session Operations:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test session creation</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/uploads</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -H</span><span style=\"color:#9ECBFF\"> \"Upload-Length: 1048576\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -H</span><span style=\"color:#9ECBFF\"> \"Upload-Metadata: filename dGVzdC50eHQ=\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: HTTP 201 Created with Location header containing session URL</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: Response includes Upload-Offset: 0 header</span></span></code></pre></div>\n\n<p><strong>Offset Discovery:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test progress tracking</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> HEAD</span><span style=\"color:#9ECBFF\"> http://localhost:8080/uploads/SESSION_ID</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: HTTP 200 OK with Upload-Offset header showing current progress</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: Response includes Upload-Length header with total size</span></span></code></pre></div>\n\n<p><strong>Signs of Implementation Issues:</strong></p>\n<ul>\n<li>Session ID collisions (use crypto/rand, not math/rand)</li>\n<li>Race conditions during concurrent chunk uploads (check locking)</li>\n<li>Memory leaks from unclosed files (use defer for cleanup)</li>\n<li>Storage exhaustion from abandoned uploads (implement cleanup)</li>\n</ul>\n<p>The next milestone will build chunk processing on this session management foundation.</p>\n<h2 id=\"goals-and-non-goals\">Goals and Non-Goals</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (1, 2, 3) — this section defines the scope for chunked upload protocol, storage abstraction, and virus scanning</p>\n</blockquote>\n<p>Think of this service as a <strong>digital postal system for large packages</strong>. Just as postal services have clear rules about what they will and won&#39;t handle (package size limits, prohibited items, delivery guarantees), our resumable file upload service needs explicit boundaries. Unlike a simple file copy operation that either succeeds completely or fails entirely, our service must handle the messy reality of network interruptions, partial transfers, and security threats while maintaining consistent behavior across different storage destinations.</p>\n<p>The key insight is that <strong>scope definition prevents feature creep</strong> and ensures the system remains focused on its core value proposition: reliable large file uploads with resumption capabilities. Without clear boundaries, we risk building a general-purpose file management system instead of a specialized upload service.</p>\n<h3 id=\"functional-requirements\">Functional Requirements</h3>\n<p>The functional requirements define the <strong>core capabilities</strong> that users can depend on. These requirements directly map to our three implementation milestones and represent the minimum viable feature set for production deployment.</p>\n<h4 id=\"upload-resumption-capabilities\">Upload Resumption Capabilities</h4>\n<p>Our primary functional requirement is <strong>resumable upload support</strong> following the tus.io protocol specification. This means the service must handle network interruptions gracefully and allow clients to resume uploads from the exact byte offset where they left off.</p>\n<table>\n<thead>\n<tr>\n<th>Capability</th>\n<th>Description</th>\n<th>Success Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Upload Session Management</td>\n<td>Create and track upload sessions across multiple requests</td>\n<td>Session persists across service restarts, unique session IDs</td>\n</tr>\n<tr>\n<td>Offset Tracking</td>\n<td>Maintain precise byte-level progress for each upload</td>\n<td>Client can query current offset and resume from exact position</td>\n</tr>\n<tr>\n<td>Chunk Processing</td>\n<td>Accept file chunks in any order with proper sequencing</td>\n<td>Chunks assembled correctly regardless of arrival order</td>\n</tr>\n<tr>\n<td>Progress Persistence</td>\n<td>Store upload state to survive server failures</td>\n<td>Upload sessions survive service crashes and restarts</td>\n</tr>\n<tr>\n<td>Completion Detection</td>\n<td>Identify when all chunks received and trigger assembly</td>\n<td>Final file matches expected size and checksum</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: tus.io Protocol Compliance</strong></p>\n<ul>\n<li><strong>Context</strong>: Multiple resumable upload protocols exist (custom HTTP ranges, vendor-specific solutions, tus.io standard)</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Custom HTTP Range-based protocol</li>\n<li>Cloud vendor-specific protocols (S3 multipart, GCS resumable)</li>\n<li>tus.io standard protocol</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement tus.io protocol compliance</li>\n<li><strong>Rationale</strong>: tus.io provides standardized client libraries, well-defined error handling, and broad ecosystem support. Custom protocols require client SDK development.</li>\n<li><strong>Consequences</strong>: Enables third-party client integration but requires strict protocol compliance</li>\n</ul>\n</blockquote>\n<p>The upload session lifecycle follows a clear state progression that clients can rely on for consistent behavior:</p>\n<table>\n<thead>\n<tr>\n<th>Session State</th>\n<th>Description</th>\n<th>Available Operations</th>\n<th>Next States</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>SessionStatusInitialized</code></td>\n<td>Session created, ready for chunks</td>\n<td>Upload chunks, query progress</td>\n<td>Active, Failed, Expired</td>\n</tr>\n<tr>\n<td><code>SessionStatusActive</code></td>\n<td>Actively receiving chunks</td>\n<td>Upload chunks, query progress</td>\n<td>Completing, Failed, Expired</td>\n</tr>\n<tr>\n<td><code>SessionStatusCompleting</code></td>\n<td>All chunks received, processing final assembly</td>\n<td>Query progress only</td>\n<td>Completed, Failed</td>\n</tr>\n<tr>\n<td><code>SessionStatusCompleted</code></td>\n<td>Upload successfully assembled and stored</td>\n<td>Download file, delete session</td>\n<td>None (terminal)</td>\n</tr>\n<tr>\n<td><code>SessionStatusFailed</code></td>\n<td>Upload failed validation or assembly</td>\n<td>Query error details, retry</td>\n<td>None (terminal)</td>\n</tr>\n<tr>\n<td><code>SessionStatusExpired</code></td>\n<td>Session exceeded time-to-live</td>\n<td>None</td>\n<td>None (terminal)</td>\n</tr>\n</tbody></table>\n<h4 id=\"virus-scanning-and-file-validation\">Virus Scanning and File Validation</h4>\n<p>The service must <strong>automatically scan all uploaded content</strong> for malicious software before final storage. This scanning happens transparently to clients but provides essential security protection.</p>\n<table>\n<thead>\n<tr>\n<th>Validation Type</th>\n<th>Implementation</th>\n<th>Failure Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>File Type Validation</td>\n<td>Magic byte detection using file headers</td>\n<td>Reject upload with specific error code</td>\n</tr>\n<tr>\n<td>Size Limit Enforcement</td>\n<td>Configurable maximum file size checking</td>\n<td>Abort upload when limit exceeded</td>\n</tr>\n<tr>\n<td>Virus Scanning</td>\n<td>ClamAV integration with signature updates</td>\n<td>Quarantine file, notify administrators</td>\n</tr>\n<tr>\n<td>Content Integrity</td>\n<td>SHA-256 checksum verification</td>\n<td>Request chunk re-upload for mismatches</td>\n</tr>\n</tbody></table>\n<p>The validation pipeline operates <strong>asynchronously after chunk assembly</strong> to avoid blocking the upload process. However, files remain unavailable for download until validation completes successfully.</p>\n<h4 id=\"storage-backend-support\">Storage Backend Support</h4>\n<p>The service must support <strong>multiple storage backends</strong> through a unified interface, allowing deployment flexibility without client-side changes. This abstraction enables organizations to choose storage solutions based on cost, compliance, or performance requirements.</p>\n<table>\n<thead>\n<tr>\n<th>Storage Backend</th>\n<th>Capabilities</th>\n<th>Configuration Requirements</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Local Filesystem</td>\n<td>Direct file I/O, fast access</td>\n<td>Mounted storage path, permissions</td>\n</tr>\n<tr>\n<td>AWS S3</td>\n<td>Multipart uploads, global access</td>\n<td>Bucket name, credentials, region</td>\n</tr>\n<tr>\n<td>Google Cloud Storage</td>\n<td>Resumable uploads, lifecycle management</td>\n<td>Bucket name, service account key</td>\n</tr>\n<tr>\n<td>Azure Blob Storage</td>\n<td>Block-based uploads, redundancy options</td>\n<td>Container name, connection string</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Storage Interface Abstraction</strong></p>\n<ul>\n<li><strong>Context</strong>: Different storage systems have unique APIs, authentication, and capabilities</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Direct integration with each storage system</li>\n<li>Adapter pattern with common interface</li>\n<li>Proxy pattern with protocol translation</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Adapter pattern with storage interface abstraction</li>\n<li><strong>Rationale</strong>: Enables adding new backends without core logic changes, simplifies testing with mock storage</li>\n<li><strong>Consequences</strong>: Requires interface design that accommodates different storage paradigms, potential lowest-common-denominator limitations</li>\n</ul>\n</blockquote>\n<h3 id=\"non-functional-requirements\">Non-Functional Requirements</h3>\n<p>Non-functional requirements define the <strong>quality attributes</strong> and constraints that the service must satisfy. These requirements ensure the system operates reliably under real-world conditions and meets production deployment standards.</p>\n<h4 id=\"performance-constraints\">Performance Constraints</h4>\n<p>The service must handle <strong>concurrent uploads efficiently</strong> without degrading individual transfer performance. Performance requirements balance throughput with resource consumption.</p>\n<table>\n<thead>\n<tr>\n<th>Performance Metric</th>\n<th>Target</th>\n<th>Measurement Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Concurrent Upload Sessions</td>\n<td>100+ simultaneous uploads</td>\n<td>Load testing with multiple clients</td>\n</tr>\n<tr>\n<td>Chunk Processing Latency</td>\n<td>&lt; 100ms per chunk (excluding storage I/O)</td>\n<td>HTTP response time measurements</td>\n</tr>\n<tr>\n<td>Memory Usage Per Session</td>\n<td>&lt; 10MB session metadata</td>\n<td>Process memory monitoring</td>\n</tr>\n<tr>\n<td>Storage I/O Throughput</td>\n<td>90% of underlying storage bandwidth</td>\n<td>Disk/network utilization metrics</td>\n</tr>\n<tr>\n<td>Session State Persistence</td>\n<td>&lt; 50ms write latency</td>\n<td>Database/file system timing</td>\n</tr>\n</tbody></table>\n<h4 id=\"reliability-requirements\">Reliability Requirements</h4>\n<p>The service must provide <strong>strong durability guarantees</strong> for upload sessions and file data. Reliability means uploads complete successfully despite infrastructure failures.</p>\n<table>\n<thead>\n<tr>\n<th>Reliability Aspect</th>\n<th>Requirement</th>\n<th>Implementation Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Session State Durability</td>\n<td>Zero session loss during server restarts</td>\n<td>Persistent session storage with atomic updates</td>\n</tr>\n<tr>\n<td>File Data Integrity</td>\n<td>Zero byte corruption during upload</td>\n<td>Chunk-level checksums with retry mechanisms</td>\n</tr>\n<tr>\n<td>Crash Recovery</td>\n<td>Full session recovery within 30 seconds</td>\n<td>Fast startup with session state scanning</td>\n</tr>\n<tr>\n<td>Storage Backend Failover</td>\n<td>Automatic retry with exponential backoff</td>\n<td>Circuit breaker pattern for backend failures</td>\n</tr>\n<tr>\n<td>Partial Failure Handling</td>\n<td>Graceful degradation when components fail</td>\n<td>Isolated failure domains, queue-based processing</td>\n</tr>\n</tbody></table>\n<h4 id=\"security-constraints\">Security Constraints</h4>\n<p>Security requirements protect both the service infrastructure and uploaded content from malicious actors. The security model assumes <strong>untrusted input</strong> and implements defense-in-depth strategies.</p>\n<table>\n<thead>\n<tr>\n<th>Security Domain</th>\n<th>Requirement</th>\n<th>Enforcement Mechanism</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Input Validation</td>\n<td>Reject malformed requests and payloads</td>\n<td>Schema validation, size limits, type checking</td>\n</tr>\n<tr>\n<td>File Content Scanning</td>\n<td>Block known malware before storage</td>\n<td>Real-time ClamAV scanning with signature updates</td>\n</tr>\n<tr>\n<td>Path Traversal Prevention</td>\n<td>Prevent access to unauthorized filesystem areas</td>\n<td>Input sanitization, chroot jail for local storage</td>\n</tr>\n<tr>\n<td>Credential Protection</td>\n<td>Secure storage of cloud backend credentials</td>\n<td>Environment variables, secret management systems</td>\n</tr>\n<tr>\n<td>Access Control</td>\n<td>Prevent unauthorized access to upload sessions</td>\n<td>Session token validation, origin checking</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Security-First Design</strong></p>\n<ul>\n<li><strong>Context</strong>: File upload services are common attack vectors for malware distribution and system compromise</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Trust client-provided metadata and validate after storage</li>\n<li>Validate during upload process before storage</li>\n<li>Sandbox uploads and validate in isolated environment</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Validate during upload with quarantine for suspicious content</li>\n<li><strong>Rationale</strong>: Prevents malicious content from reaching production storage, enables early rejection of invalid uploads</li>\n<li><strong>Consequences</strong>: Increases upload latency but significantly reduces security risk</li>\n</ul>\n</blockquote>\n<h3 id=\"explicit-non-goals\">Explicit Non-Goals</h3>\n<p>Clearly defining what the service <strong>will not handle</strong> prevents scope creep and helps users understand the system boundaries. These non-goals represent features that might seem related but fall outside our core mission.</p>\n<h4 id=\"content-management-features\">Content Management Features</h4>\n<p>The service is <strong>not a content management system</strong> and deliberately avoids features that would complicate the core upload functionality.</p>\n<table>\n<thead>\n<tr>\n<th>Feature Category</th>\n<th>Specific Non-Goals</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>File Organization</td>\n<td>Directory hierarchies, folder structures, tagging</td>\n<td>Focus on reliable upload, not file management</td>\n</tr>\n<tr>\n<td>Metadata Management</td>\n<td>Custom metadata schemas, search indexing, versioning</td>\n<td>Adds complexity without improving upload reliability</td>\n</tr>\n<tr>\n<td>Content Transformation</td>\n<td>Image resizing, video transcoding, format conversion</td>\n<td>Resource-intensive operations outside core scope</td>\n</tr>\n<tr>\n<td>User Management</td>\n<td>Authentication, authorization, user accounts</td>\n<td>Authentication should be handled by external systems</td>\n</tr>\n<tr>\n<td>File Sharing</td>\n<td>Public links, permission management, collaborative editing</td>\n<td>Different security and access patterns than uploads</td>\n</tr>\n</tbody></table>\n<h4 id=\"advanced-transfer-features\">Advanced Transfer Features</h4>\n<p>While these features might enhance user experience, they introduce significant complexity that would delay core functionality delivery.</p>\n<table>\n<thead>\n<tr>\n<th>Feature</th>\n<th>Why Excluded</th>\n<th>Alternative Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Parallel Chunk Uploads</td>\n<td>Complex ordering and bandwidth management</td>\n<td>Single-threaded upload with larger chunks</td>\n</tr>\n<tr>\n<td>Bandwidth Throttling</td>\n<td>Requires traffic shaping and client negotiation</td>\n<td>Handle via reverse proxy or CDN</td>\n</tr>\n<tr>\n<td>Upload Acceleration</td>\n<td>Needs edge servers and routing optimization</td>\n<td>Use cloud provider acceleration features</td>\n</tr>\n<tr>\n<td>Compression</td>\n<td>CPU overhead and format compatibility issues</td>\n<td>Client-side compression before upload</td>\n</tr>\n<tr>\n<td>Deduplication</td>\n<td>Storage scanning and hash management complexity</td>\n<td>Handle at storage backend level</td>\n</tr>\n</tbody></table>\n<h4 id=\"integration-and-deployment-features\">Integration and Deployment Features</h4>\n<p>The service focuses on core functionality rather than operational features that can be provided by external infrastructure.</p>\n<table>\n<thead>\n<tr>\n<th>Feature Category</th>\n<th>Specific Non-Goals</th>\n<th>External Solution</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Load Balancing</td>\n<td>Multi-instance coordination, session distribution</td>\n<td>Use load balancer with session affinity</td>\n</tr>\n<tr>\n<td>Monitoring and Metrics</td>\n<td>Custom dashboards, alerting, performance analytics</td>\n<td>Integrate with existing monitoring systems</td>\n</tr>\n<tr>\n<td>Configuration Management</td>\n<td>Dynamic reconfiguration, feature flags</td>\n<td>Use configuration management tools</td>\n</tr>\n<tr>\n<td>Container Orchestration</td>\n<td>Service discovery, health checks, scaling policies</td>\n<td>Deploy using Kubernetes or similar platforms</td>\n</tr>\n<tr>\n<td>Log Management</td>\n<td>Log aggregation, search, retention policies</td>\n<td>Use centralized logging infrastructure</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Minimal External Dependencies</strong></p>\n<ul>\n<li><strong>Context</strong>: Services can integrate with many external systems but each integration adds complexity</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Build comprehensive platform with all operational features</li>\n<li>Focus on core upload functionality with external integration points</li>\n<li>Provide plugins for common integrations</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Minimal dependencies with clear integration boundaries</li>\n<li><strong>Rationale</strong>: Reduces maintenance burden, improves reliability, enables flexible deployment patterns</li>\n<li><strong>Consequences</strong>: Users must integrate with external systems but service remains focused and maintainable</li>\n</ul>\n</blockquote>\n<h4 id=\"protocol-and-transport-limitations\">Protocol and Transport Limitations</h4>\n<p>The service implements a specific protocol subset rather than attempting to support every possible upload mechanism.</p>\n<table>\n<thead>\n<tr>\n<th>Protocol Feature</th>\n<th>Not Supported</th>\n<th>Supported Alternative</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP/1.0</td>\n<td>Legacy protocol without persistent connections</td>\n<td>HTTP/1.1 and HTTP/2</td>\n</tr>\n<tr>\n<td>WebSocket Uploads</td>\n<td>Real-time bidirectional communication</td>\n<td>Standard HTTP POST with progress polling</td>\n</tr>\n<tr>\n<td>FTP/SFTP Protocols</td>\n<td>Legacy file transfer protocols</td>\n<td>tus.io over HTTP only</td>\n</tr>\n<tr>\n<td>Custom UDP Protocols</td>\n<td>Unreliable transport for file transfer</td>\n<td>TCP-based HTTP transport</td>\n</tr>\n<tr>\n<td>GraphQL Mutations</td>\n<td>Query-based upload operations</td>\n<td>REST API with tus.io protocol</td>\n</tr>\n</tbody></table>\n<p>These limitations ensure the service remains focused on proven, widely-supported protocols while avoiding edge cases that would complicate implementation and testing.</p>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>Understanding common mistakes in scope definition helps avoid feature creep and unrealistic expectations.</p>\n<p>⚠️ <strong>Pitfall: Assuming All Upload Features Are Related</strong>\nMany developers think that because two features both involve files, they belong in the same service. For example, adding image thumbnailing because &quot;we&#39;re already handling image uploads.&quot; This leads to monolithic services that are hard to test and maintain. The fix is to strictly separate concerns: upload services handle reliable transfer, processing services handle content transformation.</p>\n<p>⚠️ <strong>Pitfall: Underestimating Security Requirements</strong>\nFile upload services are prime targets for attacks, but developers often treat security as an afterthought. Assuming &quot;we&#39;ll add virus scanning later&quot; or &quot;input validation is simple&quot; leads to vulnerable systems. The fix is to design security validation into the core upload pipeline from the beginning, not bolt it on afterward.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Storage Backend Differences</strong>\nDifferent storage systems have vastly different capabilities, consistency models, and failure modes. Treating S3 and local filesystem as equivalent leads to bugs when deploying to production. The fix is to design the storage abstraction around the most constrained backend, then add optimizations for more capable systems.</p>\n<p>⚠️ <strong>Pitfall: Scope Creep Through &quot;Simple&quot; Features</strong>\nFeatures that seem simple often hide significant complexity. Adding &quot;just a simple progress callback&quot; can require WebSocket infrastructure, connection management, and real-time scaling. The fix is to evaluate each feature request against the core mission and defer anything that doesn&#39;t directly improve upload reliability.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides practical direction for translating the goals and requirements into working code, focusing on configuration management and requirement validation.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Requirement Category</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Configuration Management</td>\n<td>JSON files with validation</td>\n<td>Consul/etcd with hot reload</td>\n</tr>\n<tr>\n<td>Input Validation</td>\n<td>Manual struct validation</td>\n<td>JSON Schema with code generation</td>\n</tr>\n<tr>\n<td>Security Scanning</td>\n<td>ClamAV command-line interface</td>\n<td>ClamAV daemon with socket communication</td>\n</tr>\n<tr>\n<td>Session Storage</td>\n<td>In-memory maps with file backup</td>\n<td>Redis with persistence</td>\n</tr>\n<tr>\n<td>Metrics Collection</td>\n<td>Simple counters in logs</td>\n<td>Prometheus metrics with custom collectors</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>resumable-upload/\n  cmd/\n    server/\n      main.go                    ← entry point, config loading\n  internal/\n    config/\n      config.go                  ← Config structs and loading\n      validation.go              ← requirement validation\n    session/\n      manager.go                 ← SessionManager implementation\n      store.go                   ← StateStore interface and implementations\n    security/\n      limits.go                  ← size and type validation\n      scanner.go                 ← virus scanning integration\n  configs/\n    default.json                 ← default configuration file\n    production.json              ← production overrides\n  tests/\n    integration/\n      requirements_test.go       ← test all functional requirements</code></pre></div>\n\n<h4 id=\"configuration-infrastructure-code\">Configuration Infrastructure Code</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> config</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Config represents the complete service configuration matching our requirements</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Config</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Server   </span><span style=\"color:#B392F0\">ServerConfig</span><span style=\"color:#9ECBFF\">   `json:\"server\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Storage  </span><span style=\"color:#B392F0\">StorageConfig</span><span style=\"color:#9ECBFF\">  `json:\"storage\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Security </span><span style=\"color:#B392F0\">SecurityConfig</span><span style=\"color:#9ECBFF\"> `json:\"security\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Cleanup  </span><span style=\"color:#B392F0\">CleanupConfig</span><span style=\"color:#9ECBFF\">  `json:\"cleanup\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ServerConfig defines HTTP server parameters for performance requirements</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ServerConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Host         </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"host\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Port         </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `json:\"port\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ReadTimeout  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"read_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    WriteTimeout </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"write_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxBodySize  </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">         `json:\"max_body_size\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StorageConfig enables storage backend abstraction requirement</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StorageConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Backend   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"backend\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LocalPath </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"local_path,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    S3Config  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">S3Config</span><span style=\"color:#9ECBFF\">         `json:\"s3_config,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Options   </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"options,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// S3Config provides cloud storage integration parameters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> S3Config</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Bucket          </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"bucket\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Region          </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"region\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AccessKeyID     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"access_key_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SecretAccessKey </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"secret_access_key\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Endpoint        </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"endpoint,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SecurityConfig enforces security constraints and validation requirements</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SecurityConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxFileSize         </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">    `json:\"max_file_size\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AllowedContentTypes []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"allowed_content_types\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    VirusScanEnabled    </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">     `json:\"virus_scan_enabled\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ClamAVSocket        </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">   `json:\"clamav_socket\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CleanupConfig manages session lifecycle and resource cleanup</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CleanupConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SessionTTL      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"session_ttl\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CleanupInterval </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"cleanup_interval\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    QuarantineTTL   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"quarantine_ttl\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadConfig reads configuration from file with defaults matching our requirements</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> LoadConfig</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">filename</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Set default configuration values that meet non-functional requirements</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Read JSON configuration file and override defaults</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Validate configuration against functional requirements</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return validated configuration or detailed error</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use json.Unmarshal and implement validation checks for each requirement category</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Server: </span><span style=\"color:#B392F0\">ServerConfig</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Host:         </span><span style=\"color:#9ECBFF\">\"localhost\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Port:         </span><span style=\"color:#79B8FF\">8080</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ReadTimeout:  </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            WriteTimeout: </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            MaxBodySize:  </span><span style=\"color:#79B8FF\">100</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#6A737D\">// 100MB chunks</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Security: </span><span style=\"color:#B392F0\">SecurityConfig</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            MaxFileSize:         </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#6A737D\">// 5GB</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            AllowedContentTypes: []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"*/*\"</span><span style=\"color:#E1E4E8\">},         </span><span style=\"color:#6A737D\">// Validate with magic bytes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            VirusScanEnabled:    </span><span style=\"color:#79B8FF\">true</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ClamAVSocket:        </span><span style=\"color:#9ECBFF\">\"/var/run/clamav/clamd.ctl\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Cleanup: </span><span style=\"color:#B392F0\">CleanupConfig</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            SessionTTL:      </span><span style=\"color:#79B8FF\">24</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Hour,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            CleanupInterval: time.Hour,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            QuarantineTTL:   </span><span style=\"color:#79B8FF\">7</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 24</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Hour,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> filename </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        data, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">ReadFile</span><span style=\"color:#E1E4E8\">(filename)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to read config file: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> json.</span><span style=\"color:#B392F0\">Unmarshal</span><span style=\"color:#E1E4E8\">(data, config); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to parse config: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> config, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"requirement-validation-code\">Requirement Validation Code</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> config</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">path/filepath</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ValidateRequirements checks configuration against functional and non-functional requirements</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ValidateRequirements</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate performance requirements (timeouts, limits)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate security requirements (file size limits, scan configuration)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Validate storage backend requirements (credentials, paths)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Validate reliability requirements (cleanup intervals, persistence)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Each requirement category needs specific validation logic</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> c.</span><span style=\"color:#B392F0\">validatePerformanceRequirements</span><span style=\"color:#E1E4E8\">(); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"performance requirements not met: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> c.</span><span style=\"color:#B392F0\">validateSecurityRequirements</span><span style=\"color:#E1E4E8\">(); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"security requirements not met: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> c.</span><span style=\"color:#B392F0\">validateStorageRequirements</span><span style=\"color:#E1E4E8\">(); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"storage requirements not met: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">validatePerformanceRequirements</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Check concurrent upload capability requirements</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.Server.MaxBodySize </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"max body size too small for chunked uploads: </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, c.Server.MaxBodySize)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.Server.ReadTimeout </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> time.Second {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"read timeout too aggressive for large chunks: </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, c.Server.ReadTimeout)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">validateSecurityRequirements</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Validate virus scanning capability if enabled</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.Security.VirusScanEnabled {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> c.Security.ClamAVSocket </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"virus scanning enabled but no ClamAV socket configured\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Check if ClamAV socket exists</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">Stat</span><span style=\"color:#E1E4E8\">(c.Security.ClamAVSocket); os.</span><span style=\"color:#B392F0\">IsNotExist</span><span style=\"color:#E1E4E8\">(err) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"ClamAV socket not found: </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, c.Security.ClamAVSocket)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Validate file size limits</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.Security.MaxFileSize </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"max file size must be positive: </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, c.Security.MaxFileSize)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">validateStorageRequirements</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Validate storage backend configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    switch</span><span style=\"color:#E1E4E8\"> c.Storage.Backend {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#9ECBFF\"> \"local\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> c.Storage.LocalPath </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"local storage requires local_path configuration\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Ensure local path exists and is writable</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">MkdirAll</span><span style=\"color:#E1E4E8\">(c.Storage.LocalPath, </span><span style=\"color:#79B8FF\">0755</span><span style=\"color:#E1E4E8\">); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"cannot create local storage path: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        testFile </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> filepath.</span><span style=\"color:#B392F0\">Join</span><span style=\"color:#E1E4E8\">(c.Storage.LocalPath, </span><span style=\"color:#9ECBFF\">\".write_test\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">WriteFile</span><span style=\"color:#E1E4E8\">(testFile, []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"test\"</span><span style=\"color:#E1E4E8\">), </span><span style=\"color:#79B8FF\">0644</span><span style=\"color:#E1E4E8\">); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"local storage path not writable: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        os.</span><span style=\"color:#B392F0\">Remove</span><span style=\"color:#E1E4E8\">(testFile)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#9ECBFF\"> \"s3\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> c.Storage.S3Config </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"S3 storage requires s3_config\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        s3 </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> c.Storage.S3Config</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> s3.Bucket </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#F97583\"> ||</span><span style=\"color:#E1E4E8\"> s3.Region </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"S3 requires bucket and region configuration\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    default</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"unsupported storage backend: </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, c.Storage.Backend)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"language-specific-implementation-hints\">Language-Specific Implementation Hints</h4>\n<p><strong>Configuration Management:</strong></p>\n<ul>\n<li>Use <code>encoding/json</code> for configuration files with struct tags for field mapping</li>\n<li>Use <code>time.Duration</code> for timeout values to get automatic parsing of &quot;30s&quot;, &quot;5m&quot; format</li>\n<li>Use <code>os.Getenv()</code> to override configuration with environment variables for containerized deployments</li>\n<li>Use <code>filepath.Clean()</code> for all path validation to prevent directory traversal attacks</li>\n</ul>\n<p><strong>Requirement Validation:</strong></p>\n<ul>\n<li>Implement validation as methods on the Config struct for clean organization</li>\n<li>Return wrapped errors using <code>fmt.Errorf(&quot;context: %w&quot;, err)</code> for error chain tracing</li>\n<li>Use early returns in validation functions to fail fast on first requirement violation</li>\n<li>Test file system permissions during validation to catch deployment issues early</li>\n</ul>\n<p><strong>Security Implementation:</strong></p>\n<ul>\n<li>Never trust file extensions for type detection - always use magic byte validation</li>\n<li>Use <code>os.Stat()</code> to verify external dependencies (ClamAV socket) exist before starting service</li>\n<li>Implement timeout wrappers around virus scanning to prevent indefinite blocking</li>\n<li>Use <code>filepath.Join()</code> and validate all paths to prevent directory traversal</li>\n</ul>\n<h4 id=\"milestone-validation-checkpoints\">Milestone Validation Checkpoints</h4>\n<p><strong>Milestone 1 Checkpoint - Configuration Loading:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/server/main.go</span><span style=\"color:#79B8FF\"> -config</span><span style=\"color:#9ECBFF\"> configs/default.json</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: Server starts successfully with default configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Check: All functional requirements validated without errors</span></span></code></pre></div>\n\n<p><strong>Milestone 2 Checkpoint - Storage Backend Validation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test local storage</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">mkdir</span><span style=\"color:#79B8FF\"> -p</span><span style=\"color:#9ECBFF\"> /tmp/upload-test</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">CONFIG</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'{\"storage\":{\"backend\":\"local\",\"local_path\":\"/tmp/upload-test\"}}'</span><span style=\"color:#B392F0\"> go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/server/main.go</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: Local storage validation passes, directory created</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test S3 storage (with valid credentials)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/server/main.go</span><span style=\"color:#79B8FF\"> -config</span><span style=\"color:#9ECBFF\"> configs/s3-production.json</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: S3 configuration validated, bucket accessible</span></span></code></pre></div>\n\n<p><strong>Milestone 3 Checkpoint - Security Requirements:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test with ClamAV disabled</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">CONFIG</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'{\"security\":{\"virus_scan_enabled\":false}}'</span><span style=\"color:#B392F0\"> go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/server/main.go</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: Service starts without virus scanning requirements</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test with invalid file size limit</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">CONFIG</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'{\"security\":{\"max_file_size\":-1}}'</span><span style=\"color:#B392F0\"> go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/server/main.go</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: Validation error for negative file size limit</span></span></code></pre></div>\n\n<p><strong>Integration Test - Full Requirements Validation:</strong>\nCreate a test that validates all functional requirements are properly configured:</p>\n<ol>\n<li>Start service with complete configuration file</li>\n<li>Verify performance limits are enforced (connection timeouts, body size limits)</li>\n<li>Verify security validation works (file type checking, size limits)</li>\n<li>Verify storage backend connectivity for configured backend type</li>\n<li>Verify cleanup processes can access session storage for TTL enforcement</li>\n</ol>\n<h2 id=\"high-level-architecture\">High-Level Architecture</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestones 1, 2, and 3 — the architectural foundation supports chunked upload protocol implementation, storage backend abstraction, and virus scanning integration</p>\n</blockquote>\n<p>Think of our resumable file upload service as a <strong>coordinated assembly line</strong> where different stations specialize in specific tasks. Just as an automotive assembly line has stations for chassis assembly, engine installation, and quality control — each with its own expertise and tools — our service has specialized components that work together to transform incoming file chunks into securely stored, validated files. The key insight is that each component maintains its own state and responsibilities, but they must coordinate seamlessly to handle the complex choreography of resumable uploads.</p>\n<p>The architecture follows a <strong>layered responsibility model</strong> where each component has a clear domain of expertise. The Upload Manager acts as the orchestrator, understanding the tus.io protocol and managing upload sessions. The Storage Abstraction provides a uniform interface regardless of whether files are stored locally, in S3, or other backends. The Virus Scanner ensures security by validating files before they reach permanent storage. The Session Store maintains the critical state that allows uploads to survive network failures and service restarts.</p>\n<p>This design philosophy prioritizes <strong>separation of concerns</strong> and <strong>fault isolation</strong>. When the virus scanner is overwhelmed, it doesn&#39;t affect chunk reception. When a storage backend experiences issues, the upload session state remains intact for retry operations. When network connections drop, the precise offset tracking allows clients to resume exactly where they left off without re-uploading already received data.</p>\n<p><img src=\"/api/project/file-upload-service/architecture-doc/asset?path=diagrams%2Fsystem-architecture.svg\" alt=\"System Architecture Overview\"></p>\n<h3 id=\"component-responsibilities\">Component Responsibilities</h3>\n<p>The architecture divides responsibilities across four core components, each with distinct ownership boundaries and failure domains. Understanding these boundaries is crucial because they determine how the system handles partial failures and maintains consistency during complex multi-step operations.</p>\n<h4 id=\"upload-manager-protocol-orchestration-and-session-lifecycle\">Upload Manager: Protocol Orchestration and Session Lifecycle</h4>\n<p>The <strong>Upload Manager</strong> serves as the primary orchestrator, implementing the tus.io resumable upload protocol and managing the complete lifecycle of upload sessions. Think of it as a skilled foreman who understands the overall assembly process, coordinates between specialized workers, and ensures that each step happens in the correct sequence with proper error handling.</p>\n<p>The Upload Manager owns the <strong>protocol state machine</strong>, tracking each upload session through its lifecycle from initialization to completion. It translates HTTP requests following the tus.io specification into internal operations across other components. When a client sends a PATCH request with a chunk of data, the Upload Manager validates the request headers, updates the session state, coordinates with storage for chunk persistence, and returns appropriate HTTP responses that allow the client to understand the current upload status.</p>\n<table>\n<thead>\n<tr>\n<th>Responsibility Area</th>\n<th>Specific Duties</th>\n<th>Owned Data</th>\n<th>External Dependencies</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Protocol Implementation</td>\n<td>tus.io HTTP method handling, header validation, response formatting</td>\n<td>Request/response parsing state</td>\n<td>SessionManager for persistence</td>\n</tr>\n<tr>\n<td>Session Orchestration</td>\n<td>Upload initialization, progress tracking, completion detection</td>\n<td>Upload workflow state</td>\n<td>StorageBackend for chunk operations</td>\n</tr>\n<tr>\n<td>Offset Management</td>\n<td>Byte-level progress calculation, resume point determination</td>\n<td>Current upload position</td>\n<td>Virus scanner for validation triggers</td>\n</tr>\n<tr>\n<td>Error Response</td>\n<td>HTTP status code generation, retry guidance, client communication</td>\n<td>Error context and recovery hints</td>\n<td>All components for failure propagation</td>\n</tr>\n</tbody></table>\n<p>The Upload Manager implements <strong>precise offset tracking</strong> by maintaining byte-level accuracy of upload progress. When a client requests the current upload status via HEAD request, the manager calculates the exact offset by querying the storage backend for the current file size and cross-referencing it with the session metadata. This precision is critical because even a one-byte discrepancy will cause upload resumption to fail or corrupt the final file.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: The Upload Manager never directly manipulates file content beyond basic chunk validation. It delegates all storage operations to the storage backend and all security validation to the virus scanner. This separation ensures that protocol logic remains isolated from storage implementation details and security policies.</p>\n</blockquote>\n<p><strong>State Coordination Complexity</strong>: The Upload Manager must handle the complex choreography when multiple components are involved in a single operation. For example, when completing an upload, it must coordinate storage finalization, virus scanning initiation, and session state updates as an atomic operation. If virus scanning fails, the manager must revert the session to an active state and preserve the uploaded chunks for retry operations.</p>\n<h4 id=\"session-store-state-persistence-and-consistency\">Session Store: State Persistence and Consistency</h4>\n<p>The <strong>Session Store</strong> acts as the system&#39;s memory, persisting critical upload state that must survive service restarts, network failures, and other disruptions. Think of it as a detailed flight recorder that captures every significant event in an upload&#39;s lifecycle, enabling precise reconstruction of system state after any failure scenario.</p>\n<p>The Session Store owns <strong>upload session metadata</strong>, including the session identifier, file information, current upload progress, chunk completion tracking, and session status. It provides atomic operations for session state transitions, ensuring that the system never enters an inconsistent state where session metadata contradicts the actual stored file chunks.</p>\n<table>\n<thead>\n<tr>\n<th>Operation Category</th>\n<th>Methods</th>\n<th>Consistency Guarantees</th>\n<th>Failure Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Session Lifecycle</td>\n<td><code>CreateSession</code>, <code>GetSession</code>, <code>DeleteSession</code></td>\n<td>Atomic creation with unique ID generation</td>\n<td>Duplicate ID detection and retry</td>\n</tr>\n<tr>\n<td>Progress Tracking</td>\n<td><code>UpdateSession</code> with offset and status changes</td>\n<td>Monotonic offset progression validation</td>\n<td>Rollback to last consistent offset</td>\n</tr>\n<tr>\n<td>Expiration Management</td>\n<td><code>ListExpiredSessions</code> for cleanup operations</td>\n<td>TTL-based consistency with grace periods</td>\n<td>Cleanup coordination with active operations</td>\n</tr>\n<tr>\n<td>Query Operations</td>\n<td>Session lookup by ID, status filtering</td>\n<td>Read consistency with concurrent writes</td>\n<td>Stale read detection and refresh</td>\n</tr>\n</tbody></table>\n<p>The Session Store implements <strong>optimistic concurrency control</strong> to handle concurrent access patterns. When multiple requests attempt to update the same upload session simultaneously (rare but possible with aggressive retry clients), the store uses version numbers or timestamps to detect conflicts and reject conflicting updates with appropriate error responses.</p>\n<p><strong>Persistence Strategy</strong>: The Session Store must choose between consistency and availability during storage backend failures. The architecture prioritizes consistency — if session state cannot be reliably persisted, the system rejects new uploads rather than risk state loss that could corrupt resumable uploads.</p>\n<blockquote>\n<p><strong>Critical Insight</strong>: Session Store consistency is more important than individual chunk storage because clients can re-upload chunks, but lost session state requires starting the entire upload from the beginning.</p>\n</blockquote>\n<h4 id=\"storage-backend-abstraction-unified-file-operations\">Storage Backend Abstraction: Unified File Operations</h4>\n<p>The <strong>Storage Abstraction</strong> provides a uniform interface for file operations across different storage backends, hiding the complexity of local filesystem operations, S3 multipart uploads, and other cloud storage APIs. Think of it as a universal translator that speaks the native language of each storage system but presents a consistent interface to the rest of the application.</p>\n<p>The Storage Abstraction owns <strong>file content persistence</strong> and <strong>storage-specific optimization</strong>. It handles the translation between generic storage operations (write chunk at offset, read file range, generate download URL) and backend-specific APIs (S3 UploadPart, local filesystem seek/write operations, GCS resumable uploads).</p>\n<table>\n<thead>\n<tr>\n<th>Backend Type</th>\n<th>Implementation Challenges</th>\n<th>Optimization Strategies</th>\n<th>Failure Recovery</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Local Filesystem</td>\n<td>Path traversal security, atomic operations, disk space management</td>\n<td>Pre-allocation, direct I/O, sparse file handling</td>\n<td>Temp file cleanup, corruption detection</td>\n</tr>\n<tr>\n<td>S3-Compatible</td>\n<td>Multipart upload coordination, eventual consistency, credential rotation</td>\n<td>Parallel part uploads, exponential backoff, connection pooling</td>\n<td>Part retry, upload ID tracking</td>\n</tr>\n<tr>\n<td>Google Cloud Storage</td>\n<td>Resumable upload protocol, quota management, regional distribution</td>\n<td>Chunk size optimization, compression negotiation</td>\n<td>Upload session recovery</td>\n</tr>\n</tbody></table>\n<p>The Storage Abstraction implements <strong>backend-specific optimizations</strong> while maintaining interface consistency. For S3 backends, it automatically manages multipart upload sessions, ensuring that parts meet the minimum 5MB size requirement (except for the final part) and handles the complex part ordering and ETag tracking required for successful multipart completion.</p>\n<p><strong>Credential and Configuration Management</strong>: The abstraction layer securely manages authentication credentials for cloud backends, supporting credential rotation and multiple backend configurations simultaneously. It implements connection pooling and retry logic specific to each backend&#39;s characteristics and rate limiting behavior.</p>\n<blockquote>\n<p><strong>Decision: Storage Interface Granularity</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to balance interface simplicity with backend-specific optimization opportunities</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Minimal interface (read, write, delete only)</li>\n<li>Rich interface (streaming, parallel operations, metadata)</li>\n<li>Backend-specific interfaces with adapter pattern</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Rich interface with optional capabilities discovery</li>\n<li><strong>Rationale</strong>: Enables backend-specific optimizations while maintaining consistent client code</li>\n<li><strong>Consequences</strong>: More complex interface but significantly better performance for cloud backends</li>\n</ul>\n</blockquote>\n<h4 id=\"virus-scanner-security-validation-pipeline\">Virus Scanner: Security Validation Pipeline</h4>\n<p>The <strong>Virus Scanner</strong> implements the security validation pipeline, protecting the system from malicious content through file type validation, virus scanning, and quarantine management. Think of it as a security checkpoint that thoroughly inspects each completed upload before granting it access to the main storage system.</p>\n<p>The Virus Scanner owns <strong>content validation logic</strong> and <strong>quarantine policy enforcement</strong>. It validates file types using magic byte detection rather than trusting client-provided MIME types or file extensions. It integrates with antivirus engines like ClamAV to perform signature-based malware detection. It manages quarantine storage for suspicious files and implements retention policies for security compliance.</p>\n<table>\n<thead>\n<tr>\n<th>Validation Stage</th>\n<th>Techniques</th>\n<th>Detection Capabilities</th>\n<th>Policy Actions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>File Type Detection</td>\n<td>Magic byte analysis, header parsing</td>\n<td>Format spoofing, extension mismatch</td>\n<td>Type-based rejection, forced MIME correction</td>\n</tr>\n<tr>\n<td>Size Validation</td>\n<td>Content length verification</td>\n<td>Decompression bombs, oversized uploads</td>\n<td>Hard limits, graduated warnings</td>\n</tr>\n<tr>\n<td>Virus Scanning</td>\n<td>Signature matching, heuristic analysis</td>\n<td>Known malware, suspicious patterns</td>\n<td>Quarantine, notification, deletion</td>\n</tr>\n<tr>\n<td>Content Analysis</td>\n<td>Metadata extraction, embedded content scanning</td>\n<td>Hidden executables, macro viruses</td>\n<td>Deep inspection, selective blocking</td>\n</tr>\n</tbody></table>\n<p>The Virus Scanner implements <strong>asynchronous processing</strong> to avoid blocking the upload completion flow. When an upload finishes, the scanner initiates background validation while marking the file as &quot;pending validation&quot; in the session state. Clients can query the validation status separately from the upload completion status.</p>\n<p><strong>Quarantine Management</strong>: Suspicious files are moved to isolated quarantine storage rather than deleted immediately. This preserves evidence for security analysis and allows for false positive recovery. The quarantine system implements automatic expiration based on configurable retention policies.</p>\n<blockquote>\n<p><strong>Design Constraint</strong>: The Virus Scanner never modifies the original uploaded content. It operates on read-only copies and makes binary decisions (approve, quarantine, or reject) that other components can act upon.</p>\n</blockquote>\n<h3 id=\"request-flow-patterns\">Request Flow Patterns</h3>\n<p>Understanding the request flow patterns reveals how components coordinate during different phases of the upload lifecycle. Each flow pattern has specific failure points, consistency requirements, and performance characteristics that influence the overall system design.</p>\n<h4 id=\"upload-initialization-flow\">Upload Initialization Flow</h4>\n<p>The <strong>upload initialization flow</strong> establishes a new upload session and prepares all components for chunk reception. Think of this as setting up the assembly line for a new product — every station must be configured and ready before production can begin.</p>\n<p>The flow begins when a client sends a POST request to initiate a resumable upload, following the tus.io protocol specification. The request includes file metadata (filename, size, MIME type) and any custom metadata that the application requires for processing.</p>\n<ol>\n<li><p><strong>Request Validation</strong>: The Upload Manager validates the incoming request headers and metadata, checking file size limits, content type restrictions, and authentication credentials. Invalid requests are rejected immediately with appropriate HTTP error responses.</p>\n</li>\n<li><p><strong>Session Creation</strong>: The Upload Manager generates a unique session identifier and creates an <code>UploadSession</code> record with initial state <code>SessionStatusInitialized</code>. The session includes file metadata, storage configuration, and timing information for expiration management.</p>\n</li>\n<li><p><strong>Storage Preparation</strong>: The Storage Abstraction receives the session information and prepares the backend for chunk reception. For local storage, this may involve creating directory structures. For S3 backends, this initiates a multipart upload and returns the upload ID for subsequent part uploads.</p>\n</li>\n<li><p><strong>State Persistence</strong>: The Session Store atomically persists the new session record, ensuring that the session state is durable before returning success to the client. This prevents scenarios where clients receive success responses but the system has no record of the upload session.</p>\n</li>\n<li><p><strong>Response Generation</strong>: The Upload Manager constructs a tus.io-compliant response including the <code>Upload-Offset</code> header (initially 0) and the <code>Location</code> header pointing to the upload URL for subsequent PATCH requests.</p>\n</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Flow Stage</th>\n<th>Component Actions</th>\n<th>Failure Handling</th>\n<th>State Changes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Request Validation</td>\n<td>Upload Manager: header parsing, limit checks</td>\n<td>Immediate HTTP error response</td>\n<td>No state changes</td>\n</tr>\n<tr>\n<td>Session Creation</td>\n<td>Upload Manager: ID generation, metadata processing</td>\n<td>Session cleanup on downstream failures</td>\n<td>Temporary session state</td>\n</tr>\n<tr>\n<td>Storage Preparation</td>\n<td>Storage Backend: resource allocation, upload initialization</td>\n<td>Storage cleanup, session removal</td>\n<td>Backend resources allocated</td>\n</tr>\n<tr>\n<td>State Persistence</td>\n<td>Session Store: atomic session write</td>\n<td>Transaction rollback, resource cleanup</td>\n<td>Durable session creation</td>\n</tr>\n<tr>\n<td>Response Generation</td>\n<td>Upload Manager: HTTP response formatting</td>\n<td>Error logging, client notification</td>\n<td>Session becomes active</td>\n</tr>\n</tbody></table>\n<p><strong>Consistency Considerations</strong>: The initialization flow must handle partial failures gracefully. If storage preparation succeeds but session persistence fails, the system must clean up allocated storage resources to prevent resource leaks. The Upload Manager implements compensation logic to unwind partial initialization on any failure.</p>\n<h4 id=\"chunk-upload-processing-pipeline\">Chunk Upload Processing Pipeline</h4>\n<p>The <strong>chunk upload processing pipeline</strong> handles individual PATCH requests containing file chunks and represents the core data flow through the system. This pipeline must be highly optimized for throughput while maintaining precise state consistency for resumability.</p>\n<p>Think of chunk processing as a high-speed conveyor belt where each chunk must be inspected, validated, positioned correctly, and permanently secured before the conveyor moves to the next position. Any failure in this pipeline must leave the system in a state where the client can determine exactly where to resume.</p>\n<ol>\n<li><p><strong>Request Reception</strong>: The Upload Manager receives a PATCH request containing chunk data, validates the HTTP headers including <code>Upload-Offset</code> and <code>Content-Length</code>, and performs basic request sanitization.</p>\n</li>\n<li><p><strong>Session Validation</strong>: The system retrieves the upload session from the Session Store and validates that the session is in the correct state (<code>SessionStatusActive</code>) and that the provided offset matches the expected next position.</p>\n</li>\n<li><p><strong>Offset Verification</strong>: The Upload Manager verifies that the client&#39;s stated offset matches the server&#39;s recorded position. Offset mismatches trigger HTTP 409 Conflict responses with the correct offset, allowing clients to realign their upload position.</p>\n</li>\n<li><p><strong>Chunk Storage</strong>: The Storage Abstraction writes the chunk data to the appropriate position in the target file, using backend-specific optimizations for performance and consistency. Local storage uses positioned writes, while S3 backends use multipart upload parts.</p>\n</li>\n<li><p><strong>Session Update</strong>: The Session Store atomically updates the session record with the new offset position, chunk completion status, and timing information. This update must be durable before responding to the client.</p>\n</li>\n<li><p><strong>Progress Response</strong>: The Upload Manager returns an HTTP 204 No Content response with updated <code>Upload-Offset</code> header, confirming the successful chunk reception and providing the client with the exact position for the next chunk.</p>\n</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Pipeline Stage</th>\n<th>Performance Optimizations</th>\n<th>Consistency Guarantees</th>\n<th>Error Recovery</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Request Reception</td>\n<td>Streaming body parsing, memory pooling</td>\n<td>Header validation atomicity</td>\n<td>Request replay detection</td>\n</tr>\n<tr>\n<td>Session Validation</td>\n<td>Session caching, read optimization</td>\n<td>Consistent session state reads</td>\n<td>Stale session refresh</td>\n</tr>\n<tr>\n<td>Offset Verification</td>\n<td>Cached offset tracking</td>\n<td>Monotonic progression checks</td>\n<td>Offset correction responses</td>\n</tr>\n<tr>\n<td>Chunk Storage</td>\n<td>Parallel writes, compression</td>\n<td>Atomic position updates</td>\n<td>Partial write rollback</td>\n</tr>\n<tr>\n<td>Session Update</td>\n<td>Batched updates, write optimization</td>\n<td>Durable offset persistence</td>\n<td>Update retry on failure</td>\n</tr>\n<tr>\n<td>Progress Response</td>\n<td>Response streaming, header caching</td>\n<td>Acknowledged progress guarantees</td>\n<td>Client retry guidance</td>\n</tr>\n</tbody></table>\n<p><strong>Performance vs. Consistency Tradeoffs</strong>: The chunk processing pipeline must balance high throughput with strict consistency requirements. The system can optimize for performance by batching session updates or caching session state, but these optimizations must never compromise the ability to provide accurate resume positions to clients.</p>\n<h4 id=\"upload-completion-and-validation-workflow\">Upload Completion and Validation Workflow</h4>\n<p>The <strong>upload completion workflow</strong> triggers when the final chunk has been uploaded and coordinates the complex process of file assembly, validation, virus scanning, and final storage commitment. This represents the most complex coordination challenge in the system because it involves all components and must handle partial failures carefully.</p>\n<p>Think of upload completion as the final quality control and packaging stage of the assembly line. All components must work together to verify that the assembled product meets quality standards, passes security inspections, and gets properly packaged for delivery.</p>\n<ol>\n<li><p><strong>Completion Detection</strong>: The Upload Manager detects that the upload is complete by comparing the current offset with the declared total file size from the upload session metadata.</p>\n</li>\n<li><p><strong>File Assembly</strong>: For storage backends that require explicit assembly (like S3 multipart uploads), the Storage Abstraction initiates the completion process, combining all uploaded parts into the final file object.</p>\n</li>\n<li><p><strong>Session Transition</strong>: The Session Store atomically updates the session status to <code>SessionStatusCompleting</code> to prevent concurrent modifications and indicate that final processing is underway.</p>\n</li>\n<li><p><strong>Validation Initiation</strong>: The Virus Scanner begins asynchronous validation of the completed file, including file type verification and antivirus scanning. The file remains in a pending state during this process.</p>\n</li>\n<li><p><strong>Final Storage</strong>: Once validation completes successfully, the Storage Abstraction moves the file to its final storage location and generates any required access URLs or metadata for client access.</p>\n</li>\n<li><p><strong>Session Finalization</strong>: The Session Store updates the session status to <code>SessionStatusCompleted</code> and records final metadata including file location, validation results, and completion timestamp.</p>\n</li>\n<li><p><strong>Client Notification</strong>: The Upload Manager provides completion notification through the standard tus.io HEAD or PATCH response, indicating successful upload and any additional metadata about the stored file.</p>\n</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Workflow Stage</th>\n<th>Component Coordination</th>\n<th>Failure Recovery</th>\n<th>State Consistency</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Completion Detection</td>\n<td>Upload Manager calculates final status</td>\n<td>None required - detection retry</td>\n<td>Read-only operation</td>\n</tr>\n<tr>\n<td>File Assembly</td>\n<td>Storage Backend finalizes file structure</td>\n<td>Assembly rollback to chunk state</td>\n<td>Atomic assembly commitment</td>\n</tr>\n<tr>\n<td>Session Transition</td>\n<td>Session Store prevents concurrent access</td>\n<td>Status rollback on downstream failure</td>\n<td>Transactional state update</td>\n</tr>\n<tr>\n<td>Validation Initiation</td>\n<td>Virus Scanner begins async processing</td>\n<td>Validation retry with exponential backoff</td>\n<td>Validation state tracking</td>\n</tr>\n<tr>\n<td>Final Storage</td>\n<td>Storage Backend commits permanent location</td>\n<td>Storage rollback, cleanup procedures</td>\n<td>Durable file commitment</td>\n</tr>\n<tr>\n<td>Session Finalization</td>\n<td>Session Store records completion metadata</td>\n<td>Metadata correction on inconsistency</td>\n<td>Final state persistence</td>\n</tr>\n<tr>\n<td>Client Notification</td>\n<td>Upload Manager provides completion status</td>\n<td>Status query retry, eventual consistency</td>\n<td>Client state synchronization</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Critical Coordination Point</strong>: The transition from <code>SessionStatusCompleting</code> to <code>SessionStatusCompleted</code> must be atomic with the final storage commitment. If this coordination fails, the system enters a state where the file exists in storage but the session indicates incomplete upload, requiring manual recovery procedures.</p>\n</blockquote>\n<h3 id=\"recommended-module-structure\">Recommended Module Structure</h3>\n<p>The module structure organizes the codebase to reflect the architectural boundaries and responsibilities, making it easier for developers to locate relevant code and understand component interactions. The structure follows Go package conventions while emphasizing clear separation of concerns and testability.</p>\n<p>Think of the module structure as the <strong>architectural blueprint</strong> that guides developers to the right location for any given functionality. Just as a well-designed building has clear zones for different activities, our codebase should have clear zones for different responsibilities with well-defined interfaces between them.</p>\n<p>The package organization prioritizes <strong>dependency clarity</strong> — higher-level packages depend on lower-level packages, never the reverse. This prevents circular dependencies and makes the codebase easier to test and refactor. Each package has a single, clear responsibility that maps directly to our architectural components.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>resumable-upload-service/\n├── cmd/\n│   ├── server/\n│   │   └── main.go                    ← Service entry point, configuration loading\n│   └── cleanup/\n│       └── main.go                    ← Cleanup utility for expired sessions\n├── internal/\n│   ├── config/\n│   │   ├── config.go                  ← Configuration structures and validation\n│   │   └── config_test.go             ← Configuration loading tests\n│   ├── upload/\n│   │   ├── manager.go                 ← Upload Manager implementation\n│   │   ├── manager_test.go            ← Upload protocol tests\n│   │   ├── session.go                 ← Session lifecycle management\n│   │   └── handlers.go                ← HTTP request handlers for tus.io\n│   ├── storage/\n│   │   ├── interface.go               ← Storage abstraction interface\n│   │   ├── local/\n│   │   │   ├── storage.go             ← Local filesystem backend\n│   │   │   └── storage_test.go        ← Local storage tests\n│   │   ├── s3/\n│   │   │   ├── storage.go             ← S3-compatible backend\n│   │   │   ├── multipart.go           ← S3 multipart upload coordination\n│   │   │   └── storage_test.go        ← S3 backend tests\n│   │   └── factory.go                 ← Storage backend factory\n│   ├── scanner/\n│   │   ├── validator.go               ← File validation and virus scanning\n│   │   ├── clamav.go                  ← ClamAV integration\n│   │   ├── quarantine.go              ← Quarantine management\n│   │   └── scanner_test.go            ← Security validation tests\n│   ├── session/\n│   │   ├── store.go                   ← Session Store interface\n│   │   ├── memory.go                  ← In-memory store for testing\n│   │   ├── redis.go                   ← Redis-backed session store\n│   │   └── store_test.go              ← Session persistence tests\n│   └── protocol/\n│       ├── tus.go                     ← tus.io protocol implementation\n│       ├── headers.go                 ← HTTP header parsing and validation\n│       └── protocol_test.go           ← Protocol compliance tests\n├── pkg/\n│   ├── errors/\n│   │   └── errors.go                  ← Common error types and handling\n│   └── middleware/\n│       ├── logging.go                 ← Request logging middleware\n│       ├── auth.go                    ← Authentication middleware\n│       └── cors.go                    ← CORS configuration\n├── test/\n│   ├── integration/\n│   │   ├── upload_test.go             ← End-to-end upload scenarios\n│   │   └── storage_test.go            ← Storage backend integration\n│   └── fixtures/\n│       ├── config.json                ← Test configuration files\n│       └── test_files/                ← Sample files for testing\n├── docs/\n│   ├── api.md                         ← API documentation\n│   ├── deployment.md                  ← Deployment instructions\n│   └── diagrams/                      ← Architecture diagrams\n├── scripts/\n│   ├── setup.sh                       ← Development environment setup\n│   └── docker/\n│       ├── Dockerfile                 ← Container image definition\n│       └── docker-compose.yml         ← Multi-service development setup\n├── go.mod                             ← Go module definition\n├── go.sum                             ← Dependency checksums\n└── README.md                          ← Project overview and quick start</code></pre></div>\n\n<h4 id=\"package-responsibility-matrix\">Package Responsibility Matrix</h4>\n<p>Each package has clearly defined responsibilities and dependency relationships that reflect the architectural boundaries discussed earlier.</p>\n<table>\n<thead>\n<tr>\n<th>Package</th>\n<th>Primary Responsibility</th>\n<th>Key Types</th>\n<th>External Dependencies</th>\n<th>Internal Dependencies</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>cmd/server</code></td>\n<td>Service bootstrap and configuration</td>\n<td><code>main()</code>, server setup</td>\n<td>HTTP server, signal handling</td>\n<td>All internal packages</td>\n</tr>\n<tr>\n<td><code>internal/config</code></td>\n<td>Configuration management</td>\n<td><code>Config</code>, <code>ServerConfig</code>, <code>StorageConfig</code></td>\n<td>JSON parsing, validation</td>\n<td>None (leaf package)</td>\n</tr>\n<tr>\n<td><code>internal/upload</code></td>\n<td>Upload orchestration and tus.io protocol</td>\n<td><code>Manager</code>, <code>UploadSession</code></td>\n<td>HTTP handling</td>\n<td><code>session</code>, <code>storage</code>, <code>protocol</code></td>\n</tr>\n<tr>\n<td><code>internal/storage</code></td>\n<td>Storage backend abstraction</td>\n<td><code>StorageBackend</code> interface, implementations</td>\n<td>Cloud SDKs, filesystem</td>\n<td><code>config</code> for backend selection</td>\n</tr>\n<tr>\n<td><code>internal/scanner</code></td>\n<td>Security validation pipeline</td>\n<td><code>Validator</code>, <code>ScanResult</code></td>\n<td>ClamAV, file type detection</td>\n<td><code>storage</code> for quarantine</td>\n</tr>\n<tr>\n<td><code>internal/session</code></td>\n<td>Upload state persistence</td>\n<td><code>SessionStore</code> interface, implementations</td>\n<td>Database/cache clients</td>\n<td><code>config</code> for connection settings</td>\n</tr>\n<tr>\n<td><code>internal/protocol</code></td>\n<td>tus.io protocol implementation</td>\n<td>HTTP header parsing, protocol validation</td>\n<td>HTTP request/response</td>\n<td><code>pkg/errors</code> for standardized errors</td>\n</tr>\n</tbody></table>\n<h4 id=\"dependency-flow-and-testing-strategy\">Dependency Flow and Testing Strategy</h4>\n<p>The module structure enforces a clear dependency flow that supports comprehensive testing at each layer. Lower-level packages have minimal dependencies and can be unit tested in isolation, while higher-level packages can be tested with mock implementations of their dependencies.</p>\n<p><strong>Testing Isolation Boundaries</strong>: Each package includes comprehensive unit tests that use interface-based dependency injection to substitute mock implementations. For example, the <code>internal/upload</code> package tests use a mock <code>SessionStore</code> and mock <code>StorageBackend</code> to test upload logic without requiring actual storage or session persistence.</p>\n<p><strong>Integration Testing Strategy</strong>: The <code>test/integration</code> package contains end-to-end tests that exercise complete upload flows with real implementations of all components. These tests use Docker containers for external dependencies like ClamAV and Redis, ensuring that integration tests run consistently across development environments.</p>\n<blockquote>\n<p><strong>Decision: Internal vs. Pkg Package Organization</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to balance code reusability with encapsulation of implementation details</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Everything in <code>pkg/</code> for maximum reusability</li>\n<li>Everything in <code>internal/</code> for maximum encapsulation</li>\n<li>Mixed approach with clear criteria</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Core logic in <code>internal/</code>, stable utilities in <code>pkg/</code></li>\n<li><strong>Rationale</strong>: Prevents premature API commitments while allowing shared utilities</li>\n<li><strong>Consequences</strong>: Clear API boundaries but requires careful consideration of what to expose</li>\n</ul>\n</blockquote>\n<p><strong>Configuration Propagation Pattern</strong>: Configuration flows from the <code>cmd/server</code> package through dependency injection rather than global variables. Each component receives only the configuration sections it needs, reducing coupling and improving testability. The <code>internal/config</code> package provides validation methods that ensure configuration consistency across all components.</p>\n<p><strong>Error Handling Consistency</strong>: The <code>pkg/errors</code> package defines standard error types and handling patterns used throughout the codebase. This ensures consistent error responses and simplifies client error handling. Each component wraps internal errors with context-specific information while preserving the underlying error classification.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides practical code implementation details for building the high-level architecture, focusing on the core interfaces and starter implementations that enable rapid development progress.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Server</td>\n<td><code>net/http</code> with <code>gorilla/mux</code> router</td>\n<td><code>gin-gonic/gin</code> with middleware</td>\n<td>Standard library provides sufficient functionality for tus.io</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td>JSON files with <code>encoding/json</code></td>\n<td><code>viper</code> with multiple format support</td>\n<td>JSON provides clear structure for component configuration</td>\n</tr>\n<tr>\n<td>Session Storage</td>\n<td>In-memory map with <code>sync.RWMutex</code></td>\n<td>Redis with <code>go-redis/redis</code> client</td>\n<td>In-memory sufficient for single-instance development</td>\n</tr>\n<tr>\n<td>Logging</td>\n<td>Standard <code>log</code> package</td>\n<td><code>sirupsen/logrus</code> with structured logging</td>\n<td>Structured logging essential for debugging upload flows</td>\n</tr>\n<tr>\n<td>Storage Backends</td>\n<td>Local filesystem with <code>os</code> package</td>\n<td>AWS SDK v2 with connection pooling</td>\n<td>Local storage enables development without cloud dependencies</td>\n</tr>\n<tr>\n<td>Virus Scanning</td>\n<td>File type checking with magic bytes</td>\n<td>Full ClamAV integration via socket</td>\n<td>Magic byte validation covers most security requirements</td>\n</tr>\n</tbody></table>\n<h4 id=\"core-configuration-structure\">Core Configuration Structure</h4>\n<p>The configuration system must support multiple storage backends, security policies, and deployment environments while providing sensible defaults for development use.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/config/config.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> config</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">path/filepath</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Config represents the complete application configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Config</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Server   </span><span style=\"color:#B392F0\">ServerConfig</span><span style=\"color:#9ECBFF\">   `json:\"server\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Storage  </span><span style=\"color:#B392F0\">StorageConfig</span><span style=\"color:#9ECBFF\">  `json:\"storage\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Security </span><span style=\"color:#B392F0\">SecurityConfig</span><span style=\"color:#9ECBFF\"> `json:\"security\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Cleanup  </span><span style=\"color:#B392F0\">CleanupConfig</span><span style=\"color:#9ECBFF\">  `json:\"cleanup\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ServerConfig contains HTTP server configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ServerConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Host         </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"host\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Port         </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `json:\"port\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ReadTimeout  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"read_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    WriteTimeout </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"write_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxBodySize  </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">         `json:\"max_body_size\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StorageConfig defines storage backend configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StorageConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Backend   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"backend\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LocalPath </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"local_path,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    S3Config  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">S3Config</span><span style=\"color:#9ECBFF\">         `json:\"s3_config,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Options   </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"options\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// S3Config contains AWS S3 specific configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> S3Config</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Bucket          </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"bucket\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Region          </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"region\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AccessKeyID     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"access_key_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SecretAccessKey </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"secret_access_key\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Endpoint        </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"endpoint,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SecurityConfig defines validation and scanning policies</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SecurityConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxFileSize         </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">    `json:\"max_file_size\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AllowedContentTypes []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"allowed_content_types\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    VirusScanEnabled    </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">     `json:\"virus_scan_enabled\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ClamAVSocket        </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">   `json:\"clamav_socket\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CleanupConfig contains session and file cleanup policies</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CleanupConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SessionTTL      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"session_ttl\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CleanupInterval </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"cleanup_interval\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    QuarantineTTL   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"quarantine_ttl\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadConfig reads configuration from JSON file with defaults</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> LoadConfig</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">filename</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Server: </span><span style=\"color:#B392F0\">ServerConfig</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Host:         </span><span style=\"color:#9ECBFF\">\"localhost\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Port:         </span><span style=\"color:#79B8FF\">8080</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ReadTimeout:  </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            WriteTimeout: </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            MaxBodySize:  </span><span style=\"color:#79B8FF\">100</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#6A737D\">// 100MB chunks</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Storage: </span><span style=\"color:#B392F0\">StorageConfig</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Backend: </span><span style=\"color:#9ECBFF\">\"local\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            LocalPath: </span><span style=\"color:#9ECBFF\">\"./uploads\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Options: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Security: </span><span style=\"color:#B392F0\">SecurityConfig</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            MaxFileSize: </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#6A737D\">// 5GB</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            AllowedContentTypes: []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"*/*\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            VirusScanEnabled: </span><span style=\"color:#79B8FF\">false</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ClamAVSocket: </span><span style=\"color:#9ECBFF\">\"/tmp/clamd.socket\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Cleanup: </span><span style=\"color:#B392F0\">CleanupConfig</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            SessionTTL:      </span><span style=\"color:#79B8FF\">24</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Hour,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            CleanupInterval: </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Hour,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            QuarantineTTL:   </span><span style=\"color:#79B8FF\">7</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 24</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Hour,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> filename </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        data, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">ReadFile</span><span style=\"color:#E1E4E8\">(filename)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to read config file: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> json.</span><span style=\"color:#B392F0\">Unmarshal</span><span style=\"color:#E1E4E8\">(data, config); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to parse config: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> config, config.</span><span style=\"color:#B392F0\">ValidateRequirements</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ValidateRequirements checks configuration against functional requirements</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ValidateRequirements</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Validate server configuration for reasonable timeout values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Validate storage configuration for required backend parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Validate security configuration for file size and type constraints</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Validate cleanup configuration for reasonable TTL values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Check storage backend accessibility (file paths, S3 credentials)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"session-management-core-interface\">Session Management Core Interface</h4>\n<p>The session management interface provides the persistence layer for upload state, designed to support both in-memory development and production persistence backends.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/session/store.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> session</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SessionStatus represents the current state of an upload session</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SessionStatus</span><span style=\"color:#F97583\"> string</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SessionStatusInitialized</span><span style=\"color:#B392F0\"> SessionStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"initialized\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SessionStatusActive</span><span style=\"color:#B392F0\">      SessionStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"active\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SessionStatusCompleting</span><span style=\"color:#B392F0\">  SessionStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"completing\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SessionStatusCompleted</span><span style=\"color:#B392F0\">   SessionStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"completed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SessionStatusFailed</span><span style=\"color:#B392F0\">      SessionStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"failed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SessionStatusExpired</span><span style=\"color:#B392F0\">     SessionStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"expired\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// UploadSession represents the complete state of a resumable upload</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> UploadSession</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ID           </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Filename     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"filename\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ContentType  </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"content_type\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TotalSize    </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">             `json:\"total_size\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CurrentOffset </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">            `json:\"current_offset\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Status       </span><span style=\"color:#B392F0\">SessionStatus</span><span style=\"color:#9ECBFF\">     `json:\"status\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StorageKey   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"storage_key\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Metadata     </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"metadata\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CreatedAt    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">         `json:\"created_at\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    UpdatedAt    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">         `json:\"updated_at\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ChunkHashes  </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">  `json:\"chunk_hashes\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SessionStore defines the interface for upload session persistence</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SessionStore</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    CreateSession</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">session</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    GetSession</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    UpdateSession</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">session</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    DeleteSession</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    ListExpiredSessions</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ttl</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">var</span><span style=\"color:#E1E4E8\"> ErrSessionNotFound </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"session not found\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<h4 id=\"in-memory-session-store-implementation\">In-Memory Session Store Implementation</h4>\n<p>This provides a complete, thread-safe session store for development and testing purposes.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/session/memory.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> session</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MemoryStore implements SessionStore using in-memory storage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MemoryStore</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sessions </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">UploadSession</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex    </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewMemoryStore creates a new in-memory session store</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewMemoryStore</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MemoryStore</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">MemoryStore</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sessions: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CreateSession stores a new upload session</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MemoryStore</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CreateSession</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">session</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.mutex.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> s.mutex.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.sessions[session.ID]; exists {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"session </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\"> already exists\"</span><span style=\"color:#E1E4E8\">, session.ID)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Create a copy to avoid external modifications</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sessionCopy </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\">session</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sessionCopy.CreatedAt </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sessionCopy.UpdatedAt </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sessionCopy.CreatedAt</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.sessions[session.ID] </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#E1E4E8\">sessionCopy</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetSession retrieves an upload session by ID</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MemoryStore</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetSession</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.mutex.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> s.mutex.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    session, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.sessions[sessionID]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">exists {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, ErrSessionNotFound</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Return a copy to prevent external modifications</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sessionCopy </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\">session</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#E1E4E8\">sessionCopy, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// UpdateSession modifies an existing upload session</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MemoryStore</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">UpdateSession</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">session</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.mutex.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> s.mutex.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    existing, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.sessions[session.ID]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">exists {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> ErrSessionNotFound</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Validate session state transitions are legal</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Ensure offset updates are monotonic (never decrease)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Preserve creation timestamp, update modification timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Validate chunk hash updates for integrity checking</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sessionCopy </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\">session</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sessionCopy.CreatedAt </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> existing.CreatedAt</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sessionCopy.UpdatedAt </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.sessions[session.ID] </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#E1E4E8\">sessionCopy</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DeleteSession removes an upload session</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MemoryStore</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">DeleteSession</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.mutex.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> s.mutex.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.sessions[sessionID]; </span><span style=\"color:#F97583\">!</span><span style=\"color:#E1E4E8\">exists {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> ErrSessionNotFound</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    delete</span><span style=\"color:#E1E4E8\">(s.sessions, sessionID)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ListExpiredSessions returns session IDs that have exceeded the TTL</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MemoryStore</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ListExpiredSessions</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ttl</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.mutex.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> s.mutex.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cutoff </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\">ttl)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> expired []</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> id, session </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> s.sessions {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> session.UpdatedAt.</span><span style=\"color:#B392F0\">Before</span><span style=\"color:#E1E4E8\">(cutoff) </span><span style=\"color:#F97583\">&#x26;&#x26;</span><span style=\"color:#E1E4E8\"> session.Status </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> SessionStatusCompleted {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            expired </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(expired, id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> expired, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"upload-manager-core-structure\">Upload Manager Core Structure</h4>\n<p>The upload manager coordinates between the protocol layer, session management, and storage backends.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/upload/manager.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> upload</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">crypto/rand</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/hex</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">internal/session</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">internal/storage</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SessionManager coordinates upload sessions with storage backends</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SessionManager</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    store   </span><span style=\"color:#B392F0\">session</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">SessionStore</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage </span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">StorageBackend</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewSessionManager creates a new upload session manager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewSessionManager</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">store</span><span style=\"color:#B392F0\"> session</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">SessionStore</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">storage</span><span style=\"color:#B392F0\"> storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">StorageBackend</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SessionManager</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">SessionManager</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        store:   store,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        storage: storage,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// InitializeUpload creates a new upload session and prepares storage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SessionManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">InitializeUpload</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">filename</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">totalSize</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">metadata</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">session</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Generate cryptographically secure session ID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Create UploadSession with initial state and metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Prepare storage backend for chunk reception</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Store session in session store with atomic operation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Return session information for client response</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sessionID </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> generateSessionID</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    uploadSession </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">session</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ID:            sessionID,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Filename:      filename,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TotalSize:     totalSize,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        CurrentOffset: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Status:        session.SessionStatusInitialized,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        StorageKey:    fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"uploads/</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, sessionID),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Metadata:      metadata,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ChunkHashes:   </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> m.store.</span><span style=\"color:#B392F0\">CreateSession</span><span style=\"color:#E1E4E8\">(ctx, uploadSession); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to create session: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> uploadSession, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ProcessChunk handles incoming chunk upload</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SessionManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ProcessChunk</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">offset</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">contentHash</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Retrieve and validate upload session state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Verify offset matches expected position for resumability</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Write chunk data to storage backend at correct position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Update session with new offset and chunk completion status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Handle partial write failures with appropriate error responses</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetUploadProgress returns current upload status for resume</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SessionManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetUploadProgress</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">session</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Retrieve session from store</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Verify session state is valid for progress query</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Cross-check session offset with actual storage backend state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Return consistent progress information for client resume</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> m.store.</span><span style=\"color:#B392F0\">GetSession</span><span style=\"color:#E1E4E8\">(ctx, sessionID)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> generateSessionID</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bytes </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">16</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rand.</span><span style=\"color:#B392F0\">Read</span><span style=\"color:#E1E4E8\">(bytes)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> hex.</span><span style=\"color:#B392F0\">EncodeToString</span><span style=\"color:#E1E4E8\">(bytes)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"storage-interface-foundation\">Storage Interface Foundation</h4>\n<p>The storage abstraction provides a clean interface for different backend implementations.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/storage/interface.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> storage</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">io</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StorageBackend defines the interface for file storage operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StorageBackend</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // WriteChunk writes data at the specified offset in the file</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    WriteChunk</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">offset</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // ReadChunk reads data from the specified range in the file</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    ReadChunk</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">offset</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">length</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // GetSize returns the current size of the file</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    GetSize</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Delete removes the file from storage</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Delete</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // GenerateDownloadURL creates a signed URL for file access</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    GenerateDownloadURL</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">expiration</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // FinalizeUpload completes multipart uploads (for backends that require it)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    FinalizeUpload</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StorageConfig contains backend-specific configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StorageConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Backend </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Options </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Milestone 1 Checkpoint: Basic Upload Session Management</strong></p>\n<ul>\n<li>Run: <code>go test ./internal/session/... -v</code></li>\n<li>Expected: All session store tests pass with proper state transitions</li>\n<li>Manual test: POST to initialize upload, verify session creation and unique ID generation</li>\n<li>Success indicators: Session ID returned, initial offset is 0, status is &quot;initialized&quot;</li>\n</ul>\n<p><strong>Milestone 2 Checkpoint: Storage Backend Selection</strong></p>\n<ul>\n<li>Run: <code>go test ./internal/storage/... -v</code></li>\n<li>Expected: Local storage backend passes all interface compliance tests</li>\n<li>Manual test: Configure different storage backends via JSON config</li>\n<li>Success indicators: Backend factory correctly instantiates based on configuration</li>\n</ul>\n<p><strong>Milestone 3 Checkpoint: Component Integration</strong></p>\n<ul>\n<li>Run: <code>go test ./internal/upload/... -v</code></li>\n<li>Expected: Upload manager coordinates session and storage operations</li>\n<li>Manual test: Complete upload initialization through chunk processing</li>\n<li>Success indicators: Session state updates correctly, storage receives chunks at proper offsets</li>\n</ul>\n<h4 id=\"common-implementation-pitfalls\">Common Implementation Pitfalls</h4>\n<p>⚠️ <strong>Pitfall: Session ID Collisions</strong>\nUsing weak random number generation for session IDs can lead to collisions in high-throughput scenarios. Always use <code>crypto/rand</code> for session ID generation, never <code>math/rand</code>. Test by generating millions of IDs and checking for duplicates.</p>\n<p>⚠️ <strong>Pitfall: Offset Consistency</strong>\nFailing to atomically update session offset with storage operations creates windows where clients can receive inconsistent resume positions. Always update session state after confirming successful storage writes, and implement rollback logic for partial failures.</p>\n<p>⚠️ <strong>Pitfall: Memory Leaks in Session Storage</strong>\nIn-memory session stores must implement proper cleanup for expired sessions, or memory usage will grow unbounded. Implement background cleanup goroutines with proper shutdown handling to prevent resource leaks.</p>\n<p>⚠️ <strong>Pitfall: Storage Path Traversal</strong>\nWhen implementing local storage backends, failing to sanitize storage keys can allow path traversal attacks. Always use <code>filepath.Join()</code> and validate that resolved paths remain within the configured storage directory.</p>\n<h2 id=\"data-model-and-state-management\">Data Model and State Management</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1 (Chunked Upload Protocol) — this section defines the core data structures and state management that enable chunked uploads, session tracking, and resumable transfers</p>\n</blockquote>\n<p>Think of upload state management like a <strong>flight data recorder</strong> for file transfers. Just as an aircraft&#39;s black box continuously records flight parameters so investigators can reconstruct what happened after an incident, our upload service must meticulously track every aspect of an ongoing transfer — which chunks have arrived, their integrity status, current byte offsets, and session metadata. This persistent state allows us to resume exactly where we left off after any interruption, whether it&#39;s a network failure, server restart, or client disconnect.</p>\n<p>The challenge in designing this data model lies in balancing <strong>consistency</strong>, <strong>performance</strong>, and <strong>recoverability</strong>. We need structures that can be quickly updated as chunks arrive (performance), maintain accurate state even under concurrent access (consistency), and survive system failures without corruption (recoverability). Unlike simple HTTP uploads that are atomic operations, resumable uploads create long-lived sessions that span multiple requests, potentially lasting hours or days.</p>\n<p><img src=\"/api/project/file-upload-service/architecture-doc/asset?path=diagrams%2Fupload-state-machine.svg\" alt=\"Upload Session State Machine\"></p>\n<p>Our data model centers around three core abstractions: the <code>UploadSession</code> that tracks overall transfer state, individual chunk metadata for ordering and integrity, and a persistence strategy that ensures state survives failures. Each serves a distinct purpose in maintaining the illusion of a single, reliable file transfer despite the underlying reality of multiple network requests across an unreliable medium.</p>\n<h3 id=\"upload-session-model\">Upload Session Model</h3>\n<p>The <strong>upload session</strong> serves as the master record for each resumable transfer, functioning like a project folder that contains all information needed to coordinate the upload from start to finish. Think of it as the &quot;case file&quot; for an ongoing investigation — it accumulates evidence (chunks) over time and tracks the current status of the entire operation.</p>\n<p>Each upload session progresses through a well-defined lifecycle with explicit state transitions. Understanding this lifecycle is crucial because different operations are permitted in different states, and state transitions have specific preconditions and side effects that must be maintained for system consistency.</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ID</code></td>\n<td><code>string</code></td>\n<td>Unique identifier generated by server, typically UUID format for global uniqueness</td>\n</tr>\n<tr>\n<td><code>Filename</code></td>\n<td><code>string</code></td>\n<td>Original filename provided by client, used for content disposition and logging</td>\n</tr>\n<tr>\n<td><code>ContentType</code></td>\n<td><code>string</code></td>\n<td>MIME type of the file, validated against magic bytes during upload completion</td>\n</tr>\n<tr>\n<td><code>TotalSize</code></td>\n<td><code>int64</code></td>\n<td>Expected total file size in bytes, used for progress calculation and storage allocation</td>\n</tr>\n<tr>\n<td><code>CurrentOffset</code></td>\n<td><code>int64</code></td>\n<td>Number of bytes successfully received and verified, defines resume point for client</td>\n</tr>\n<tr>\n<td><code>Status</code></td>\n<td><code>SessionStatus</code></td>\n<td>Current state in the upload lifecycle, determines which operations are permitted</td>\n</tr>\n<tr>\n<td><code>StorageKey</code></td>\n<td><code>string</code></td>\n<td>Backend-specific identifier for the file location, enables storage abstraction</td>\n</tr>\n<tr>\n<td><code>Metadata</code></td>\n<td><code>map[string]string</code></td>\n<td>Client-provided key-value pairs for custom attributes like tags or categories</td>\n</tr>\n<tr>\n<td><code>CreatedAt</code></td>\n<td><code>time.Time</code></td>\n<td>Session initialization timestamp, used for TTL calculations and audit logging</td>\n</tr>\n<tr>\n<td><code>UpdatedAt</code></td>\n<td><code>time.Time</code></td>\n<td>Last modification timestamp, indicates recent activity for cleanup decisions</td>\n</tr>\n<tr>\n<td><code>ChunkHashes</code></td>\n<td><code>map[int64]string</code></td>\n<td>Mapping of chunk sequence numbers to their cryptographic hashes for integrity verification</td>\n</tr>\n</tbody></table>\n<p>The session lifecycle follows a strict state machine that prevents invalid transitions and ensures data consistency. Each state represents a distinct phase of the upload process with specific responsibilities and constraints.</p>\n<table>\n<thead>\n<tr>\n<th>Current State</th>\n<th>Event</th>\n<th>Next State</th>\n<th>Actions Taken</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>N/A</td>\n<td>Client initialization request</td>\n<td><code>Initialized</code></td>\n<td>Create session record, allocate storage, generate upload ID</td>\n</tr>\n<tr>\n<td><code>Initialized</code></td>\n<td>First chunk received</td>\n<td><code>Active</code></td>\n<td>Update offset, store chunk metadata, begin progress tracking</td>\n</tr>\n<tr>\n<td><code>Active</code></td>\n<td>Subsequent chunks received</td>\n<td><code>Active</code></td>\n<td>Validate chunk order, update offset, verify chunk integrity</td>\n</tr>\n<tr>\n<td><code>Active</code></td>\n<td>Final chunk received</td>\n<td><code>Completing</code></td>\n<td>Trigger file assembly, initiate virus scanning, prepare final storage</td>\n</tr>\n<tr>\n<td><code>Completing</code></td>\n<td>Assembly and validation success</td>\n<td><code>Completed</code></td>\n<td>Finalize storage, generate download URLs, notify client</td>\n</tr>\n<tr>\n<td><code>Completing</code></td>\n<td>Validation failure detected</td>\n<td><code>Failed</code></td>\n<td>Move to quarantine, log failure reason, preserve evidence</td>\n</tr>\n<tr>\n<td><code>Active</code></td>\n<td>TTL expiration reached</td>\n<td><code>Expired</code></td>\n<td>Mark for cleanup, preserve partial data for forensics</td>\n</tr>\n<tr>\n<td>Any state</td>\n<td>Critical error occurred</td>\n<td><code>Failed</code></td>\n<td>Log error context, preserve state for debugging</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Principle</strong>: State transitions are atomic and logged to ensure we can always reconstruct how a session reached its current state, even after failures.</p>\n</blockquote>\n<p>The <code>CurrentOffset</code> field deserves special attention because it serves as the authoritative source of truth for upload progress. When a client reconnects after a failure, it queries this offset to determine where to resume sending data. The offset must only be updated after a chunk has been successfully written to storage and its integrity verified — premature updates lead to data loss scenarios where the client believes bytes were received but the server has no record of them.</p>\n<p><strong>Session Status Enumeration:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Status Value</th>\n<th>Meaning</th>\n<th>Client Actions Permitted</th>\n<th>Server Responsibilities</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Initialized</code></td>\n<td>Session created, awaiting first chunk</td>\n<td>Send chunks starting from offset 0</td>\n<td>Accept chunks, validate ordering, transition to active</td>\n</tr>\n<tr>\n<td><code>Active</code></td>\n<td>Actively receiving chunks</td>\n<td>Send remaining chunks, query progress</td>\n<td>Process chunks, update offset, maintain session TTL</td>\n</tr>\n<tr>\n<td><code>Completing</code></td>\n<td>All chunks received, processing</td>\n<td>Query status, wait for completion</td>\n<td>Assemble file, run validation, finalize storage</td>\n</tr>\n<tr>\n<td><code>Completed</code></td>\n<td>Upload successful, file available</td>\n<td>Download file, clean up client state</td>\n<td>Serve file, maintain for configured retention period</td>\n</tr>\n<tr>\n<td><code>Failed</code></td>\n<td>Upload failed, requires cleanup</td>\n<td>Review error, restart upload if desired</td>\n<td>Preserve failure context, clean up partial data</td>\n</tr>\n<tr>\n<td><code>Expired</code></td>\n<td>Session timeout, no recent activity</td>\n<td>Restart upload from beginning</td>\n<td>Clean up partial data, log expiration for monitoring</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Session-Scoped Chunk Tracking</strong></p>\n<ul>\n<li><strong>Context</strong>: We need to track individual chunks within each upload session for integrity verification and resume logic</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Separate chunk table with foreign keys to sessions</li>\n<li>Embedded chunk metadata within session record</li>\n<li>File-system based tracking using chunk naming conventions</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Embedded <code>ChunkHashes</code> map within the session record</li>\n<li><strong>Rationale</strong>: Eliminates join queries for common operations, reduces database transactions, and ensures chunk metadata has same consistency guarantees as session state</li>\n<li><strong>Consequences</strong>: Session records are larger but operations are faster, and chunk cleanup is automatic when sessions are deleted</li>\n</ul>\n</blockquote>\n<h3 id=\"chunk-tracking-model\">Chunk Tracking Model</h3>\n<p>Individual chunks require their own metadata model to support integrity verification, ordering guarantees, and efficient resume operations. Think of each chunk as a <strong>numbered puzzle piece</strong> — we need to track its position, verify it hasn&#39;t been corrupted, and ensure we can assemble all pieces in the correct order to reconstruct the original file.</p>\n<p>The chunk tracking system must handle several challenging scenarios: chunks arriving out of order due to network routing, duplicate chunks caused by client retries, corrupted chunks that need to be re-uploaded, and missing chunks that prevent upload completion. Our model provides the metadata necessary to detect and handle each of these situations gracefully.</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Sequence Number</td>\n<td><code>int64</code></td>\n<td>Zero-based chunk index, determines assembly order and identifies gaps</td>\n</tr>\n<tr>\n<td>Byte Range</td>\n<td><code>int64, int64</code></td>\n<td>Start and end byte offsets within the final file for precise positioning</td>\n</tr>\n<tr>\n<td>Content Hash</td>\n<td><code>string</code></td>\n<td>SHA-256 hash of chunk content, enables corruption detection and deduplication</td>\n</tr>\n<tr>\n<td>Size</td>\n<td><code>int64</code></td>\n<td>Actual chunk size in bytes, used for progress calculation and storage allocation</td>\n</tr>\n<tr>\n<td>Received At</td>\n<td><code>time.Time</code></td>\n<td>Timestamp when chunk was successfully stored, useful for debugging and monitoring</td>\n</tr>\n<tr>\n<td>Storage Path</td>\n<td><code>string</code></td>\n<td>Backend-specific location where chunk data is stored temporarily</td>\n</tr>\n</tbody></table>\n<p>The chunk assembly process requires careful coordination between the session state and individual chunk metadata. As chunks arrive, we must validate their position within the overall file, verify their integrity, and determine when we have received all necessary pieces to complete the assembly.</p>\n<p><strong>Chunk Ordering and Gap Detection:</strong></p>\n<p>Chunks may arrive out of order due to network characteristics, client retry logic, or parallel upload strategies. Our tracking model must efficiently detect gaps in the sequence and communicate missing ranges to clients during resume operations.</p>\n<table>\n<thead>\n<tr>\n<th>Scenario</th>\n<th>Detection Method</th>\n<th>Resolution Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Out-of-order arrival</td>\n<td>Compare sequence numbers against expected next chunk</td>\n<td>Buffer chunks, assemble when sequence is complete</td>\n</tr>\n<tr>\n<td>Missing chunks</td>\n<td>Gap analysis in sequence number range</td>\n<td>Report missing ranges in resume response</td>\n</tr>\n<tr>\n<td>Duplicate chunks</td>\n<td>Hash comparison against stored chunks</td>\n<td>Ignore duplicates, update timestamp for activity tracking</td>\n</tr>\n<tr>\n<td>Corrupted chunks</td>\n<td>Hash mismatch between expected and actual</td>\n<td>Request chunk retransmission, log corruption event</td>\n</tr>\n<tr>\n<td>Partial chunks</td>\n<td>Size validation against expected chunk boundaries</td>\n<td>Reject partial chunks, maintain strict boundaries</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Critical Implementation Detail</strong>: Chunk sequence numbers must account for the final chunk being potentially smaller than the standard chunk size. The assembly logic must handle this edge case correctly.</p>\n</blockquote>\n<p><strong>Integrity Verification Strategy:</strong></p>\n<p>Each chunk undergoes cryptographic hash verification to ensure data integrity across network transmission and storage operations. The hash serves multiple purposes: corruption detection, deduplication, and forensic verification in case of security incidents.</p>\n<table>\n<thead>\n<tr>\n<th>Hash Algorithm</th>\n<th>Use Case</th>\n<th>Trade-offs</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>SHA-256</td>\n<td>Primary integrity verification</td>\n<td>Strong security, moderate performance cost</td>\n</tr>\n<tr>\n<td>MD5</td>\n<td>Legacy client compatibility</td>\n<td>Faster computation, cryptographically weak</td>\n</tr>\n<tr>\n<td>CRC32</td>\n<td>Quick corruption detection</td>\n<td>Very fast, not suitable for security verification</td>\n</tr>\n</tbody></table>\n<p>The chunk hashes are stored in the session&#39;s <code>ChunkHashes</code> map, indexed by sequence number. This allows efficient lookup during chunk processing and provides a complete integrity manifest for the entire file.</p>\n<blockquote>\n<p><strong>Decision: Cryptographic Hash Selection</strong></p>\n<ul>\n<li><strong>Context</strong>: We need to verify chunk integrity while balancing security and performance requirements</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>SHA-256 for maximum security</li>\n<li>MD5 for performance and client compatibility  </li>\n<li>Blake2b for optimal performance/security balance</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: SHA-256 as primary hash with optional MD5 for legacy clients</li>\n<li><strong>Rationale</strong>: SHA-256 provides strong security guarantees required for production systems, while MD5 support eases client migration</li>\n<li><strong>Consequences</strong>: Higher CPU cost per chunk but eliminates entire class of integrity vulnerabilities</li>\n</ul>\n</blockquote>\n<h3 id=\"state-persistence-strategy\">State Persistence Strategy</h3>\n<p>The persistence strategy determines how upload state survives system failures, restarts, and scaling operations. Think of this as the <strong>insurance policy</strong> for our upload service — it ensures that hours of upload progress isn&#39;t lost due to temporary infrastructure issues.</p>\n<p>Our persistence requirements are more complex than typical web applications because upload sessions are long-lived, state changes frequently (with each chunk), and consistency is critical for resume operations. We must design a system that provides durability without sacrificing the performance needed to handle high-throughput uploads.</p>\n<p><strong>Persistence Requirements Analysis:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Requirement</th>\n<th>Rationale</th>\n<th>Implementation Implications</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Atomic Updates</td>\n<td>Session state must never be partially updated</td>\n<td>Use database transactions or atomic file operations</td>\n</tr>\n<tr>\n<td>Crash Recovery</td>\n<td>State must survive unexpected termination</td>\n<td>Implement write-ahead logging or equivalent durability</td>\n</tr>\n<tr>\n<td>Concurrent Access</td>\n<td>Multiple processes may update same session</td>\n<td>Require locking or optimistic concurrency control</td>\n</tr>\n<tr>\n<td>Performance</td>\n<td>Updates must not bottleneck upload throughput</td>\n<td>Consider batching, async writes, or eventually consistent updates</td>\n</tr>\n<tr>\n<td>Consistency</td>\n<td>Resume operations must see accurate state</td>\n<td>Ensure reads reflect all committed writes</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Pluggable State Store Architecture</strong></p>\n<ul>\n<li><strong>Context</strong>: Different deployment environments have different persistence requirements and available infrastructure</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Hard-coded database implementation (PostgreSQL)</li>\n<li>File-system based persistence</li>\n<li>Abstract interface with multiple backends</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Abstract <code>StateStore</code> interface with pluggable implementations</li>\n<li><strong>Rationale</strong>: Allows development against in-memory store, production against PostgreSQL, and testing against file-based stores</li>\n<li><strong>Consequences</strong>: Additional abstraction complexity but significantly improved deployment flexibility</li>\n</ul>\n</blockquote>\n<p><strong>State Store Interface Design:</strong></p>\n<p>The state store abstraction provides a clean separation between business logic and persistence mechanics. This interface must support the core operations needed by the upload service while remaining simple enough to implement across different storage backends.</p>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>CreateSession</code></td>\n<td><code>ctx Context, session *UploadSession</code></td>\n<td><code>error</code></td>\n<td>Atomically creates new session, failing if ID already exists</td>\n</tr>\n<tr>\n<td><code>GetSession</code></td>\n<td><code>ctx Context, sessionID string</code></td>\n<td><code>*UploadSession, error</code></td>\n<td>Retrieves session by ID, returning <code>ErrSessionNotFound</code> if missing</td>\n</tr>\n<tr>\n<td><code>UpdateSession</code></td>\n<td><code>ctx Context, session *UploadSession</code></td>\n<td><code>error</code></td>\n<td>Modifies existing session atomically, using optimistic locking</td>\n</tr>\n<tr>\n<td><code>DeleteSession</code></td>\n<td><code>ctx Context, sessionID string</code></td>\n<td><code>error</code></td>\n<td>Removes session and all associated metadata permanently</td>\n</tr>\n<tr>\n<td><code>ListExpiredSessions</code></td>\n<td><code>ctx Context, ttl Duration</code></td>\n<td><code>[]string, error</code></td>\n<td>Returns session IDs older than TTL for cleanup processing</td>\n</tr>\n</tbody></table>\n<p>Each method includes a <code>context.Context</code> parameter to support cancellation, timeouts, and distributed tracing. The context pattern is essential for production systems where operations may need to be cancelled due to client disconnection or system overload.</p>\n<p><strong>Durability Guarantees by Implementation:</strong></p>\n<p>Different state store implementations provide different levels of durability and performance. Understanding these trade-offs is crucial for selecting the appropriate backend for each deployment environment.</p>\n<table>\n<thead>\n<tr>\n<th>Implementation</th>\n<th>Durability Level</th>\n<th>Performance Characteristics</th>\n<th>Use Cases</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>MemoryStore</code></td>\n<td>None (lost on restart)</td>\n<td>Highest throughput, lowest latency</td>\n<td>Development, testing, stateless deployments</td>\n</tr>\n<tr>\n<td><code>FileStore</code></td>\n<td>Survives process restart</td>\n<td>Medium throughput, fsync latency</td>\n<td>Single-server deployments, simple infrastructure</td>\n</tr>\n<tr>\n<td><code>PostgreSQLStore</code></td>\n<td>Full ACID compliance</td>\n<td>Lower throughput, network latency</td>\n<td>Multi-server deployments, strong consistency requirements</td>\n</tr>\n<tr>\n<td><code>RedisStore</code></td>\n<td>Configurable (memory vs disk)</td>\n<td>High throughput, network latency</td>\n<td>High-scale deployments, distributed systems</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Performance Consideration</strong>: Session updates happen with every chunk upload, potentially thousands of times per file. The state store implementation must be optimized for high-frequency small updates rather than large batch operations.</p>\n</blockquote>\n<p><strong>Session Cleanup and TTL Management:</strong></p>\n<p>Long-lived upload sessions require active cleanup to prevent storage exhaustion and maintain system performance. The cleanup strategy must balance resource reclamation with user experience — we don&#39;t want to delete sessions that are still actively being used.</p>\n<table>\n<thead>\n<tr>\n<th>Cleanup Trigger</th>\n<th>Action Taken</th>\n<th>Safety Measures</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>TTL Expiration</td>\n<td>Mark session as expired, schedule cleanup</td>\n<td>Grace period before actual deletion</td>\n</tr>\n<tr>\n<td>Completion</td>\n<td>Move to completed state, start retention timer</td>\n<td>Configurable retention for download access</td>\n</tr>\n<tr>\n<td>Failure</td>\n<td>Move to failed state, preserve for debugging</td>\n<td>Extended retention for forensic analysis</td>\n</tr>\n<tr>\n<td>System Maintenance</td>\n<td>Batch cleanup of old sessions</td>\n<td>Skip sessions with recent activity</td>\n</tr>\n</tbody></table>\n<p>The cleanup process operates as a background task that periodically scans for expired sessions and performs the necessary cleanup operations. This includes removing partial files from storage, clearing session metadata, and updating monitoring metrics.</p>\n<p><strong>Common Pitfalls in State Management:</strong></p>\n<p>⚠️ <strong>Pitfall: Race Conditions in Offset Updates</strong>\nMany implementations incorrectly update the session offset before confirming the chunk has been written to storage. This creates a window where the client believes a chunk was received but the server has no record of it, leading to data corruption during resume operations. Always update the offset as the final step in a transaction that includes the storage write.</p>\n<p>⚠️ <strong>Pitfall: Incomplete State Machine Validation</strong>\nFailing to validate state transitions allows sessions to enter invalid states that confuse both clients and administrators. For example, transitioning directly from <code>Initialized</code> to <code>Completed</code> without passing through <code>Active</code> indicates a logic error. Implement explicit state validation in all update operations.</p>\n<p>⚠️ <strong>Pitfall: TTL Calculation Based on Creation Time</strong>\nUsing only the creation time for TTL calculations can prematurely expire sessions that are actively receiving chunks but started long ago. Always consider the last update time when determining if a session should be cleaned up. A session receiving chunks within the last hour should not be expired regardless of when it was created.</p>\n<p><img src=\"/api/project/file-upload-service/architecture-doc/asset?path=diagrams%2Fdata-model-relationships.svg\" alt=\"Core Data Model Relationships\"></p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides the concrete code structures and patterns needed to implement the data model and state management system described above.</p>\n<p><strong>A. Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>State Store</td>\n<td>In-memory map with mutex</td>\n<td>PostgreSQL with connection pooling</td>\n</tr>\n<tr>\n<td>Serialization</td>\n<td>JSON encoding/decoding</td>\n<td>Protocol Buffers with schema evolution</td>\n</tr>\n<tr>\n<td>Hashing</td>\n<td>Go crypto/sha256 standard library</td>\n<td>Hardware-accelerated Blake2b</td>\n</tr>\n<tr>\n<td>Time Handling</td>\n<td>time.Time with UTC normalization</td>\n<td>Logical clocks for distributed systems</td>\n</tr>\n<tr>\n<td>Concurrency</td>\n<td>sync.RWMutex for state protection</td>\n<td>Channels with worker pools</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/\n  session/\n    session.go              ← UploadSession type and core logic\n    session_test.go         ← Unit tests for session operations  \n    status.go               ← SessionStatus enum and transitions\n    store.go                ← StateStore interface definition\n    memory_store.go         ← In-memory implementation for testing\n    file_store.go           ← File-based persistence implementation\n    cleanup.go              ← Background cleanup and TTL management\n  config/\n    config.go               ← Configuration loading and validation\n    config_test.go          ← Configuration parsing tests</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code:</strong></p>\n<p><strong>Configuration Management (<code>internal/config/config.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> config</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Config represents the complete service configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Config</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Server   </span><span style=\"color:#B392F0\">ServerConfig</span><span style=\"color:#9ECBFF\">   `json:\"server\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Storage  </span><span style=\"color:#B392F0\">StorageConfig</span><span style=\"color:#9ECBFF\">  `json:\"storage\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Security </span><span style=\"color:#B392F0\">SecurityConfig</span><span style=\"color:#9ECBFF\"> `json:\"security\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Cleanup  </span><span style=\"color:#B392F0\">CleanupConfig</span><span style=\"color:#9ECBFF\">  `json:\"cleanup\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ServerConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Host         </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"host\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Port         </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `json:\"port\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ReadTimeout  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"readTimeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    WriteTimeout </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"writeTimeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxBodySize  </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">         `json:\"maxBodySize\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StorageConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Backend   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"backend\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LocalPath </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"localPath\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    S3Config  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">S3Config</span><span style=\"color:#9ECBFF\">         `json:\"s3Config,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Options   </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"options\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> S3Config</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Bucket          </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"bucket\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Region          </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"region\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AccessKeyID     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"accessKeyId\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SecretAccessKey </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"secretAccessKey\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Endpoint        </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"endpoint\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SecurityConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxFileSize         </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">    `json:\"maxFileSize\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AllowedContentTypes []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"allowedContentTypes\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    VirusScanEnabled    </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">     `json:\"virusScanEnabled\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ClamAVSocket        </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">   `json:\"clamavSocket\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CleanupConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SessionTTL      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"sessionTTL\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CleanupInterval </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"cleanupInterval\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    QuarantineTTL   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"quarantineTTL\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadConfig reads configuration from JSON file with sensible defaults</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> LoadConfig</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">filename</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Server: </span><span style=\"color:#B392F0\">ServerConfig</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Host:         </span><span style=\"color:#9ECBFF\">\"localhost\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Port:         </span><span style=\"color:#79B8FF\">8080</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ReadTimeout:  </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            WriteTimeout: </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            MaxBodySize:  </span><span style=\"color:#79B8FF\">100</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#6A737D\">// 100MB</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Storage: </span><span style=\"color:#B392F0\">StorageConfig</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Backend:   </span><span style=\"color:#9ECBFF\">\"local\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            LocalPath: </span><span style=\"color:#9ECBFF\">\"./uploads\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Options:   </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Security: </span><span style=\"color:#B392F0\">SecurityConfig</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            MaxFileSize:         </span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#6A737D\">// 1GB</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            AllowedContentTypes: []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"*/*\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            VirusScanEnabled:    </span><span style=\"color:#79B8FF\">false</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Cleanup: </span><span style=\"color:#B392F0\">CleanupConfig</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            SessionTTL:      </span><span style=\"color:#79B8FF\">24</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Hour,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            CleanupInterval: time.Hour,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            QuarantineTTL:   </span><span style=\"color:#79B8FF\">7</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 24</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Hour,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> filename </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        data, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">ReadFile</span><span style=\"color:#E1E4E8\">(filename)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> json.</span><span style=\"color:#B392F0\">Unmarshal</span><span style=\"color:#E1E4E8\">(data, config); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> config, config.</span><span style=\"color:#B392F0\">ValidateRequirements</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ValidateRequirements ensures configuration meets system requirements</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ValidateRequirements</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Add validation logic for required fields and constraints</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Session Status Enumeration (<code>internal/session/status.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> session</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SessionStatus represents the current state of an upload session</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SessionStatus</span><span style=\"color:#F97583\"> string</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SessionStatusInitialized</span><span style=\"color:#F97583\"> =</span><span style=\"color:#B392F0\"> SessionStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"initialized\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SessionStatusActive</span><span style=\"color:#F97583\">      =</span><span style=\"color:#B392F0\"> SessionStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"active\"</span><span style=\"color:#E1E4E8\">) </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SessionStatusCompleting</span><span style=\"color:#F97583\">  =</span><span style=\"color:#B392F0\"> SessionStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"completing\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SessionStatusCompleted</span><span style=\"color:#F97583\">   =</span><span style=\"color:#B392F0\"> SessionStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"completed\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SessionStatusFailed</span><span style=\"color:#F97583\">      =</span><span style=\"color:#B392F0\"> SessionStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SessionStatusExpired</span><span style=\"color:#F97583\">     =</span><span style=\"color:#B392F0\"> SessionStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"expired\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// IsValid checks if the status value is recognized</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#B392F0\">SessionStatus</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">IsValid</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    switch</span><span style=\"color:#E1E4E8\"> s {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> SessionStatusInitialized, SessionStatusActive, SessionStatusCompleting,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">         SessionStatusCompleted, SessionStatusFailed, SessionStatusExpired:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    default</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CanTransitionTo validates if transition to target status is allowed</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#B392F0\">SessionStatus</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CanTransitionTo</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">target</span><span style=\"color:#B392F0\"> SessionStatus</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    validTransitions </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#B392F0\">SessionStatus</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#B392F0\">SessionStatus</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        SessionStatusInitialized: {SessionStatusActive, SessionStatusFailed, SessionStatusExpired},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        SessionStatusActive:      {SessionStatusCompleting, SessionStatusFailed, SessionStatusExpired},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        SessionStatusCompleting:  {SessionStatusCompleted, SessionStatusFailed},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        SessionStatusCompleted:   {}, </span><span style=\"color:#6A737D\">// Terminal state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        SessionStatusFailed:      {}, </span><span style=\"color:#6A737D\">// Terminal state  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        SessionStatusExpired:     {}, </span><span style=\"color:#6A737D\">// Terminal state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    allowed, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> validTransitions[s]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">exists {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, allowedTarget </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> allowed {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> target </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> allowedTarget {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Memory-Based State Store (<code>internal/session/memory_store.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> session</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">errors</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">var</span><span style=\"color:#E1E4E8\"> ErrSessionNotFound </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"session not found\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MemoryStore provides in-memory session storage for testing and development</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MemoryStore</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu       </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sessions </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">UploadSession</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewMemoryStore creates a new in-memory state store</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewMemoryStore</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MemoryStore</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">MemoryStore</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sessions: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MemoryStore</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CreateSession</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">session</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> m.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> m.sessions[session.ID]; exists {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"session already exists\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Deep copy to prevent external modifications</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sessionCopy </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\">session</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sessionCopy.ChunkHashes </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> k, v </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> session.ChunkHashes {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sessionCopy.ChunkHashes[k] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> v</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.sessions[session.ID] </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#E1E4E8\">sessionCopy</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MemoryStore</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetSession</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> m.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    session, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> m.sessions[sessionID]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">exists {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, ErrSessionNotFound</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Deep copy to prevent external modifications</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sessionCopy </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\">session</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sessionCopy.ChunkHashes </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> k, v </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> session.ChunkHashes {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sessionCopy.ChunkHashes[k] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> v</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#E1E4E8\">sessionCopy, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MemoryStore</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">UpdateSession</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">session</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> m.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> m.sessions[session.ID]; </span><span style=\"color:#F97583\">!</span><span style=\"color:#E1E4E8\">exists {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> ErrSessionNotFound</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    session.UpdatedAt </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">UTC</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Deep copy to prevent external modifications</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sessionCopy </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\">session</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sessionCopy.ChunkHashes </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> k, v </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> session.ChunkHashes {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sessionCopy.ChunkHashes[k] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> v</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.sessions[session.ID] </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#E1E4E8\">sessionCopy</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MemoryStore</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">DeleteSession</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> m.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    delete</span><span style=\"color:#E1E4E8\">(m.sessions, sessionID)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MemoryStore</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ListExpiredSessions</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ttl</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> m.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cutoff </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">UTC</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\">ttl)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> expired []</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> id, session </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> m.sessions {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> session.UpdatedAt.</span><span style=\"color:#B392F0\">Before</span><span style=\"color:#E1E4E8\">(cutoff) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            expired </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(expired, id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> expired, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code:</strong></p>\n<p><strong>Upload Session Model (<code>internal/session/session.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> session</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// UploadSession represents the complete state of a resumable upload</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> UploadSession</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ID           </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Filename     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"filename\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ContentType  </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"contentType\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TotalSize    </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">             `json:\"totalSize\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CurrentOffset </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">            `json:\"currentOffset\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Status       </span><span style=\"color:#B392F0\">SessionStatus</span><span style=\"color:#9ECBFF\">     `json:\"status\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StorageKey   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"storageKey\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Metadata     </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"metadata\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CreatedAt    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">         `json:\"createdAt\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    UpdatedAt    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">         `json:\"updatedAt\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ChunkHashes  </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">  `json:\"chunkHashes\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StateStore defines the interface for session persistence</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StateStore</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    CreateSession</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">session</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    GetSession</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    UpdateSession</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">session</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    DeleteSession</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    ListExpiredSessions</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ttl</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SessionManager coordinates upload session operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SessionManager</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    store   </span><span style=\"color:#B392F0\">StateStore</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage </span><span style=\"color:#B392F0\">StorageBackend</span><span style=\"color:#6A737D\"> // Will be defined in storage abstraction section</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// InitializeUpload creates a new upload session and prepares storage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SessionManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">InitializeUpload</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">filename</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">totalSize</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">metadata</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Generate unique session ID (use uuid.New().String())</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create storage key for backend (use session ID + filename)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Validate totalSize is within configured limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Initialize UploadSession with provided parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Set status to SessionStatusInitialized</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Set CreatedAt and UpdatedAt to current UTC time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Call store.CreateSession to persist new session</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Return session or error</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ProcessChunk handles incoming chunk upload and updates session state</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SessionManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ProcessChunk</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">offset</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">contentHash</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Retrieve session from store using GetSession</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate session is in Active or Initialized status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Verify offset matches CurrentOffset (no gaps allowed)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Calculate and verify chunk hash matches contentHash parameter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Calculate chunk sequence number from offset and chunk size</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Write chunk data to storage backend at appropriate location</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Update session.CurrentOffset += len(data)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Store chunk hash in session.ChunkHashes map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: Update session status to Active if was Initialized</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 10: Call store.UpdateSession to persist changes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use crypto/sha256 for hash calculation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Handle the case where offset doesn't match CurrentOffset (resume scenario)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetUploadProgress returns current session state for resume operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SessionManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetUploadProgress</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Call store.GetSession to retrieve current state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Return session data directly (store handles not found errors)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // This method is primarily a pass-through to the store layer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TransitionStatus safely changes session status with validation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SessionManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">TransitionStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">newStatus</span><span style=\"color:#B392F0\"> SessionStatus</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Retrieve current session from store</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate transition is allowed using status.CanTransitionTo</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Update session.Status = newStatus</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update session.UpdatedAt to current time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Persist changes using store.UpdateSession</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Return appropriate error if transition is invalid</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints:</strong></p>\n<ul>\n<li>Use <code>crypto/rand</code> with <code>uuid.New()</code> for generating cryptographically secure session IDs</li>\n<li><code>sync.RWMutex</code> provides better performance than <code>sync.Mutex</code> when reads outnumber writes</li>\n<li>Always use <code>time.Now().UTC()</code> to avoid timezone-related bugs in distributed systems</li>\n<li>The <code>context.Context</code> parameter should be passed to all downstream operations for proper cancellation</li>\n<li>Use <code>json</code> struct tags for consistent serialization across different storage backends</li>\n<li>Consider using <code>atomic</code> operations for frequently updated counters like <code>CurrentOffset</code></li>\n</ul>\n<p><strong>F. Milestone Checkpoint:</strong></p>\n<p>After implementing this data model:</p>\n<ol>\n<li><strong>Run Tests</strong>: <code>go test ./internal/session/... -v</code></li>\n<li><strong>Expected Output</strong>: All tests pass with coverage of state transitions and CRUD operations</li>\n<li><strong>Manual Verification</strong>: Create a simple main function that initializes sessions and processes chunks</li>\n<li><strong>Success Criteria</strong>: Sessions persist across service restarts and accurately track upload progress</li>\n<li><strong>Warning Signs</strong>: If sessions lose data or allow invalid state transitions, review the StateStore implementation and status validation logic</li>\n</ol>\n<p><strong>G. Debugging Tips:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Session state resets after restart</td>\n<td>Missing persistence or improper loading</td>\n<td>Check store implementation, verify file/DB writes</td>\n<td>Implement proper durability in chosen StateStore</td>\n</tr>\n<tr>\n<td>Chunks appear out of order</td>\n<td>Race condition in offset updates</td>\n<td>Add logging around offset calculations</td>\n<td>Use proper locking or atomic operations</td>\n</tr>\n<tr>\n<td>Memory usage grows indefinitely</td>\n<td>Sessions not being cleaned up</td>\n<td>Monitor session count and TTL processing</td>\n<td>Implement background cleanup routine</td>\n</tr>\n<tr>\n<td>Hash mismatches on chunk verification</td>\n<td>Incorrect hash algorithm or encoding</td>\n<td>Compare expected vs actual hash values</td>\n<td>Verify SHA-256 usage and hex encoding</td>\n</tr>\n</tbody></table>\n<h2 id=\"chunked-upload-protocol-implementation\">Chunked Upload Protocol Implementation</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1 (Chunked Upload Protocol) — this section implements the tus.io-compatible resumable upload protocol with session management and chunk assembly</p>\n</blockquote>\n<p>Think of the chunked upload protocol as a <strong>postal delivery system for large packages</strong>. When you need to ship something too big for a single delivery truck, the postal service breaks it into smaller packages, ships each one separately, and reassembles them at the destination. Each package has a tracking number and sequence information, so even if one package gets delayed or lost, the system knows exactly what&#39;s missing and can retry just that piece. The recipient can track progress and know when all pieces have arrived for final assembly.</p>\n<p>The tus.io protocol provides this same reliability for file uploads over unreliable networks. Instead of risking the loss of an entire large file due to a single network hiccup, we break the upload into manageable chunks, track each piece independently, and provide mechanisms for clients to resume exactly where they left off after any interruption.</p>\n<p>Our implementation must handle four critical responsibilities: establishing the protocol contract through proper HTTP semantics, managing the lifecycle of upload sessions from initialization to completion, assembling chunks into the final file with integrity verification, and providing precise offset tracking that enables seamless resumption. Each of these areas has subtle complexities that can make the difference between a robust production service and a fragile system that fails under real-world conditions.</p>\n<p><img src=\"/api/project/file-upload-service/architecture-doc/asset?path=diagrams%2Fupload-state-machine.svg\" alt=\"Upload Session State Machine\"></p>\n<h3 id=\"protocol-mechanics\">Protocol Mechanics</h3>\n<p>The tus.io protocol defines a precise HTTP-based conversation between client and server, similar to how <strong>two radio operators use standardized call signs and procedures</strong> to coordinate complex operations despite unreliable communication channels. Every message has a specific format, expected responses, and error conditions that both parties understand, enabling reliable coordination even when individual messages are lost or corrupted.</p>\n<p>The protocol conversation follows a predictable pattern. The client begins by announcing its intent to upload a file and providing metadata about the transfer. The server responds with a unique session identifier and the location where chunks should be sent. The client then sends chunks in sequence, with each chunk containing both the data and information about where it fits in the final file. The server acknowledges each chunk and provides progress information. Finally, when all chunks are received, the server assembles the complete file and notifies the client of successful completion.</p>\n<blockquote>\n<p><strong>Decision: HTTP Method Selection for Protocol Operations</strong></p>\n<ul>\n<li><strong>Context</strong>: The tus.io specification defines specific HTTP methods for different upload operations, but we need to understand the semantic meaning behind each choice</li>\n<li><strong>Options Considered</strong>: Follow tus.io exactly, use only POST for everything, create custom HTTP methods</li>\n<li><strong>Decision</strong>: Implement the full tus.io method set with proper HTTP semantics</li>\n<li><strong>Rationale</strong>: Each HTTP method has specific idempotency and safety properties that are crucial for reliable uploads. POST for creation, PATCH for partial updates, HEAD for status checks, and DELETE for cleanup align perfectly with REST principles and enable proper caching, retry logic, and proxy behavior</li>\n<li><strong>Consequences</strong>: Clients can rely on standard HTTP semantics, intermediary systems (proxies, load balancers) behave predictably, and we gain automatic retry safety for idempotent operations</li>\n</ul>\n</blockquote>\n<p>The core protocol operations map to specific HTTP methods with carefully chosen semantics:</p>\n<table>\n<thead>\n<tr>\n<th>HTTP Method</th>\n<th>Endpoint Pattern</th>\n<th>Purpose</th>\n<th>Request Headers</th>\n<th>Response Headers</th>\n<th>Idempotent</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>POST</td>\n<td><code>/files</code></td>\n<td>Initialize upload session</td>\n<td><code>Upload-Length</code>, <code>Upload-Metadata</code></td>\n<td><code>Location</code>, <code>Tus-Resumable</code></td>\n<td>No</td>\n</tr>\n<tr>\n<td>HEAD</td>\n<td><code>/files/{id}</code></td>\n<td>Check upload progress</td>\n<td><code>Tus-Resumable</code></td>\n<td><code>Upload-Offset</code>, <code>Upload-Length</code></td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>PATCH</td>\n<td><code>/files/{id}</code></td>\n<td>Upload chunk data</td>\n<td><code>Upload-Offset</code>, <code>Content-Type: application/offset+octet-stream</code></td>\n<td><code>Upload-Offset</code>, <code>Tus-Resumable</code></td>\n<td>No</td>\n</tr>\n<tr>\n<td>DELETE</td>\n<td><code>/files/{id}</code></td>\n<td>Cancel upload session</td>\n<td><code>Tus-Resumable</code></td>\n<td><code>Tus-Resumable</code></td>\n<td>Yes</td>\n</tr>\n</tbody></table>\n<p>The <code>Upload-Offset</code> header serves as the <strong>synchronization heartbeat</strong> between client and server. Every chunk upload includes this header specifying the exact byte position where the chunk data should be written. The server validates that this offset matches its internal tracking and rejects chunks that would create gaps or overlaps. This prevents the data corruption that would result from network reordering, duplicate transmissions, or client-server offset disagreements.</p>\n<p>Request and response formats follow strict patterns that enable robust error detection and recovery:</p>\n<table>\n<thead>\n<tr>\n<th>Request Component</th>\n<th>Format</th>\n<th>Validation Rules</th>\n<th>Error Response</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Upload-Length</td>\n<td>Integer bytes</td>\n<td>Must be positive, within configured limits</td>\n<td><code>413 Payload Too Large</code></td>\n</tr>\n<tr>\n<td>Upload-Offset</td>\n<td>Integer bytes</td>\n<td>Must match server&#39;s current offset exactly</td>\n<td><code>409 Conflict</code></td>\n</tr>\n<tr>\n<td>Content-Length</td>\n<td>Integer bytes</td>\n<td>Must match actual body size</td>\n<td><code>400 Bad Request</code></td>\n</tr>\n<tr>\n<td>Upload-Metadata</td>\n<td>Base64 key-value pairs</td>\n<td>Keys alphanumeric only, values properly encoded</td>\n<td><code>400 Bad Request</code></td>\n</tr>\n</tbody></table>\n<p>The server responds to each operation with specific status codes that guide client behavior:</p>\n<table>\n<thead>\n<tr>\n<th>Status Code</th>\n<th>Meaning</th>\n<th>Client Action</th>\n<th>Server State</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>201 Created</td>\n<td>Session initialized successfully</td>\n<td>Begin chunk uploads to Location URL</td>\n<td>Session in <code>active</code> state</td>\n</tr>\n<tr>\n<td>204 No Content</td>\n<td>Chunk accepted and written</td>\n<td>Continue with next chunk</td>\n<td>Offset advanced by chunk size</td>\n</tr>\n<tr>\n<td>409 Conflict</td>\n<td>Offset mismatch detected</td>\n<td>HEAD request to get current offset, resume from there</td>\n<td>Offset unchanged</td>\n</tr>\n<tr>\n<td>404 Not Found</td>\n<td>Session expired or invalid</td>\n<td>Reinitialize upload from beginning</td>\n<td>No session state</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p>The critical insight for protocol reliability is that <strong>every operation must be precisely recoverable</strong>. When a client receives a network timeout or connection error, it should be able to determine the exact server state through a HEAD request and resume without data loss or corruption.</p>\n</blockquote>\n<h3 id=\"session-lifecycle-management\">Session Lifecycle Management</h3>\n<p>Upload session management operates like an <strong>air traffic control system</strong>, where each aircraft (upload session) has a unique flight plan, constantly reports its position, and must be safely guided from takeoff to landing while handling weather delays, course corrections, and emergency situations. The system maintains complete awareness of every session&#39;s status and can coordinate recovery procedures when things go wrong.</p>\n<p>An upload session represents a contract between client and server to transfer a specific file through a series of chunk operations. The session carries all the context needed to validate incoming chunks, track progress, handle interruptions, and ultimately assemble the complete file. Sessions have a definite lifecycle with clear state transitions and specific actions available in each state.</p>\n<p>The session lifecycle follows this progression:</p>\n<table>\n<thead>\n<tr>\n<th>Current State</th>\n<th>Valid Events</th>\n<th>Next State</th>\n<th>Actions Taken</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>initialized</code></td>\n<td>first_chunk_received</td>\n<td><code>active</code></td>\n<td>Create storage placeholder, update offset</td>\n</tr>\n<tr>\n<td><code>initialized</code></td>\n<td>timeout_expired</td>\n<td><code>expired</code></td>\n<td>Mark for cleanup, log timeout</td>\n</tr>\n<tr>\n<td><code>active</code></td>\n<td>chunk_received</td>\n<td><code>active</code></td>\n<td>Validate offset, write chunk, update offset</td>\n</tr>\n<tr>\n<td><code>active</code></td>\n<td>all_chunks_received</td>\n<td><code>completing</code></td>\n<td>Begin chunk assembly and validation</td>\n</tr>\n<tr>\n<td><code>active</code></td>\n<td>timeout_expired</td>\n<td><code>expired</code></td>\n<td>Mark for cleanup, preserve partial data</td>\n</tr>\n<tr>\n<td><code>completing</code></td>\n<td>assembly_successful</td>\n<td><code>completed</code></td>\n<td>Finalize file, generate download URL</td>\n</tr>\n<tr>\n<td><code>completing</code></td>\n<td>assembly_failed</td>\n<td><code>failed</code></td>\n<td>Log error, preserve chunks for debugging</td>\n</tr>\n<tr>\n<td><code>completed</code></td>\n<td>ttl_expired</td>\n<td><code>expired</code></td>\n<td>Schedule session cleanup</td>\n</tr>\n<tr>\n<td><code>failed</code></td>\n<td>retry_requested</td>\n<td><code>active</code></td>\n<td>Reset to last valid offset</td>\n</tr>\n</tbody></table>\n<p>Session initialization creates the foundational state that guides all subsequent operations. The server generates a cryptographically random session identifier, validates the proposed upload against size and type constraints, reserves storage space, and records the client&#39;s declared metadata. This information becomes the authoritative source for validating all future chunk uploads.</p>\n<table>\n<thead>\n<tr>\n<th>Session Field</th>\n<th>Type</th>\n<th>Purpose</th>\n<th>Validation Rules</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ID</code></td>\n<td>string</td>\n<td>Unique session identifier</td>\n<td>UUID v4 format, cryptographically random</td>\n</tr>\n<tr>\n<td><code>Filename</code></td>\n<td>string</td>\n<td>Original file name</td>\n<td>Sanitized, no path traversal characters</td>\n</tr>\n<tr>\n<td><code>ContentType</code></td>\n<td>string</td>\n<td>Declared MIME type</td>\n<td>Must be in allowed types list</td>\n</tr>\n<tr>\n<td><code>TotalSize</code></td>\n<td>int64</td>\n<td>Expected final file size</td>\n<td>Positive, within configured limits</td>\n</tr>\n<tr>\n<td><code>CurrentOffset</code></td>\n<td>int64</td>\n<td>Bytes successfully received</td>\n<td>Always ≤ TotalSize, monotonically increasing</td>\n</tr>\n<tr>\n<td><code>Status</code></td>\n<td>SessionStatus</td>\n<td>Current lifecycle state</td>\n<td>One of defined enum values</td>\n</tr>\n<tr>\n<td><code>StorageKey</code></td>\n<td>string</td>\n<td>Backend storage identifier</td>\n<td>Generated by storage backend</td>\n</tr>\n<tr>\n<td><code>Metadata</code></td>\n<td>map[string]string</td>\n<td>Client-provided key-value pairs</td>\n<td>Keys sanitized, values size-limited</td>\n</tr>\n<tr>\n<td><code>CreatedAt</code></td>\n<td>time.Time</td>\n<td>Session creation timestamp</td>\n<td>Used for TTL calculation</td>\n</tr>\n<tr>\n<td><code>UpdatedAt</code></td>\n<td>time.Time</td>\n<td>Last activity timestamp</td>\n<td>Updated on every chunk</td>\n</tr>\n<tr>\n<td><code>ChunkHashes</code></td>\n<td>map[int64]string</td>\n<td>Integrity verification hashes</td>\n<td>Key is chunk offset, value is hash</td>\n</tr>\n</tbody></table>\n<p>Progress tracking requires <strong>atomic updates</strong> to prevent race conditions when multiple goroutines handle concurrent operations for the same session. Each chunk upload must atomically verify the current offset, validate the chunk placement, write the data, and update the session state. If any step fails, the entire operation rolls back, leaving the session in a consistent state for retry.</p>\n<blockquote>\n<p><strong>Decision: Session State Persistence Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Sessions must survive server restarts and scale across multiple server instances</li>\n<li><strong>Options Considered</strong>: In-memory only (fast but loses state on restart), database persistence (durable but slower), hybrid approach (memory + periodic persistence)</li>\n<li><strong>Decision</strong>: Implement pluggable StateStore interface with in-memory and database backends</li>\n<li><strong>Rationale</strong>: In-memory provides maximum performance for single-instance deployments and testing, while database persistence enables production deployments with multiple servers and restart resilience. The interface abstraction allows choosing the right backend for each environment</li>\n<li><strong>Consequences</strong>: Enables horizontal scaling and fault tolerance at the cost of additional complexity and potential performance overhead for persistent backends</li>\n</ul>\n</blockquote>\n<p>Session expiration and cleanup prevent resource leaks from abandoned uploads. Sessions have configurable time-to-live values that depend on their current state. Active sessions get longer TTLs to accommodate slow uploads, while failed sessions expire quickly to free resources. The cleanup process runs periodically to identify expired sessions and remove their associated storage and metadata.</p>\n<table>\n<thead>\n<tr>\n<th>Session State</th>\n<th>TTL Duration</th>\n<th>Cleanup Actions</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>initialized</code></td>\n<td>1 hour</td>\n<td>Delete session metadata only</td>\n<td>Client may be preparing upload</td>\n</tr>\n<tr>\n<td><code>active</code></td>\n<td>24 hours</td>\n<td>Delete session and partial chunks</td>\n<td>Allow for slow network conditions</td>\n</tr>\n<tr>\n<td><code>completed</code></td>\n<td>7 days</td>\n<td>Delete session, keep file</td>\n<td>Client needs time to download</td>\n</tr>\n<tr>\n<td><code>failed</code></td>\n<td>1 hour</td>\n<td>Delete session and partial chunks</td>\n<td>Quick cleanup of error cases</td>\n</tr>\n<tr>\n<td><code>expired</code></td>\n<td>Immediate</td>\n<td>Delete all associated data</td>\n<td>Already past useful lifetime</td>\n</tr>\n</tbody></table>\n<p><img src=\"/api/project/file-upload-service/architecture-doc/asset?path=diagrams%2Fchunk-upload-sequence.svg\" alt=\"Chunk Upload Sequence Flow\"></p>\n<h3 id=\"chunk-assembly-strategy\">Chunk Assembly Strategy</h3>\n<p>Chunk assembly is like <strong>reconstructing a jigsaw puzzle where pieces arrive out of order</strong>, some pieces might be damaged, and you need to verify that the completed picture matches exactly what was promised. The system must handle pieces arriving in any sequence, detect missing or corrupted pieces, and only declare success when every piece is validated and properly positioned.</p>\n<p>The assembly process begins as soon as the server detects that all expected chunks have been received. This detection happens when the session&#39;s <code>CurrentOffset</code> equals the declared <code>TotalSize</code>, indicating that no gaps remain in the chunk sequence. However, receiving all bytes doesn&#39;t guarantee that the file is correct—the chunks might have been corrupted during transmission, written to wrong offsets due to race conditions, or arrived out of order in ways that created subtle data corruption.</p>\n<p>Our assembly strategy implements a <strong>two-phase commit approach</strong> to ensure atomic completion. Phase one performs comprehensive validation: verifying chunk integrity through stored hashes, confirming that no gaps exist in the byte sequence, and validating the assembled file against size and type constraints. Phase two commits the assembled file to permanent storage and updates the session to completed status. If phase one fails, the partial chunks remain available for debugging. If phase two fails, phase one can be retried without repeating the expensive validation work.</p>\n<table>\n<thead>\n<tr>\n<th>Assembly Phase</th>\n<th>Operations Performed</th>\n<th>Failure Handling</th>\n<th>Success Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Validation</td>\n<td>Check chunk hashes, verify byte sequence, validate file type</td>\n<td>Log errors, preserve chunks, mark session as failed</td>\n<td>All chunks verified, no gaps detected</td>\n</tr>\n<tr>\n<td>Assembly</td>\n<td>Concatenate chunks in order, write to final storage location</td>\n<td>Rollback partial writes, retry from validation phase</td>\n<td>Complete file written successfully</td>\n</tr>\n<tr>\n<td>Commitment</td>\n<td>Update session status, generate access URLs, trigger post-processing</td>\n<td>Mark as failed, preserve assembled file for manual recovery</td>\n<td>Session marked completed, URLs available</td>\n</tr>\n<tr>\n<td>Cleanup</td>\n<td>Remove temporary chunks, update metrics, log completion</td>\n<td>Non-critical, continue with partial cleanup</td>\n<td>Resources freed, monitoring updated</td>\n</tr>\n</tbody></table>\n<p>Chunk ordering and gap detection requires careful <strong>offset arithmetic</strong> to handle edge cases. The system maintains a complete map of received byte ranges and validates that they form a contiguous sequence from offset 0 to the declared file size. Overlapping chunks are rejected during upload, so the assembly process can assume that each chunk occupies a unique byte range. However, the system still validates this assumption during assembly to detect storage backend corruption or offset calculation bugs.</p>\n<p>The chunk-to-file transformation process varies depending on the storage backend capabilities:</p>\n<table>\n<thead>\n<tr>\n<th>Storage Type</th>\n<th>Assembly Method</th>\n<th>Temporary Storage</th>\n<th>Atomic Operations</th>\n<th>Performance Characteristics</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Local Filesystem</td>\n<td>In-place concatenation</td>\n<td>Single temporary file</td>\n<td>Atomic rename</td>\n<td>Fast, limited by disk I/O</td>\n</tr>\n<tr>\n<td>S3-Compatible</td>\n<td>Multipart upload completion</td>\n<td>S3 manages parts</td>\n<td>Complete multipart upload API</td>\n<td>Network-limited, highly scalable</td>\n</tr>\n<tr>\n<td>Memory (testing)</td>\n<td>Buffer concatenation</td>\n<td>In-memory byte array</td>\n<td>Replace pointer atomically</td>\n<td>Fastest, memory-limited</td>\n</tr>\n</tbody></table>\n<p>File integrity verification goes beyond simple size checks to ensure that the assembled content matches the client&#39;s intent. The system computes cryptographic hashes of the final assembled file and compares them against client-provided checksums when available. For clients that don&#39;t provide checksums, the system generates and stores hashes for future verification or debugging needs.</p>\n<blockquote>\n<p><strong>Decision: Chunk Hash Granularity</strong></p>\n<ul>\n<li><strong>Context</strong>: We need to detect corruption at the finest granularity possible while minimizing storage overhead</li>\n<li><strong>Options Considered</strong>: Hash entire file only, hash each chunk individually, hash fixed-size blocks regardless of chunk boundaries</li>\n<li><strong>Decision</strong>: Hash each chunk individually using SHA-256</li>\n<li><strong>Rationale</strong>: Individual chunk hashing allows precise identification of corrupted chunks for targeted retransmission, balances storage overhead with detection granularity, and aligns with natural upload boundaries that clients already track</li>\n<li><strong>Consequences</strong>: Enables surgical repair of corrupted uploads, increases metadata storage requirements, allows parallel hash computation during upload</li>\n</ul>\n</blockquote>\n<p>⚠️ <strong>Pitfall: Race Conditions During Assembly</strong>\nMany implementations fail to properly synchronize the transition from &quot;receiving chunks&quot; to &quot;assembling file.&quot; If new chunks arrive while assembly is in progress, they can corrupt the assembly process or be silently lost. Always transition the session to <code>completing</code> status before beginning assembly, and reject any chunks that arrive for sessions in this state. Clients that retry chunk uploads during assembly will receive clear error responses and can query session status to understand progress.</p>\n<h3 id=\"offset-tracking-and-resume-logic\">Offset Tracking and Resume Logic</h3>\n<p>Offset tracking functions as the <strong>GPS navigation system</strong> for resumable uploads, providing precise location awareness that enables clients to resume exactly where they left off after any interruption. Just as GPS coordinates must be accurate to within meters to be useful for navigation, upload offsets must be accurate to the byte level to prevent data corruption or loss during resume operations.</p>\n<p>The server maintains authoritative offset state that represents the highest contiguous byte position successfully written to storage. This offset advances only when chunks are successfully written and verified, never when chunks are merely received or queued. Clients must synchronize with this authoritative offset before resuming uploads, as network conditions or server restarts might have caused their local offset tracking to diverge from reality.</p>\n<p>Offset synchronization follows a <strong>checkpoint protocol</strong> where clients can query the current server offset at any time and adjust their behavior accordingly. This query operation is idempotent and lightweight, implemented through HTTP HEAD requests that return the current offset in response headers without transferring any file data. Clients should perform this synchronization after any network interruption, server error response, or extended pause in upload activity.</p>\n<table>\n<thead>\n<tr>\n<th>Resume Scenario</th>\n<th>Client Action</th>\n<th>Server Response</th>\n<th>Next Steps</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Network timeout during chunk upload</td>\n<td>HEAD request to check offset</td>\n<td>Current offset in <code>Upload-Offset</code> header</td>\n<td>Resume from server&#39;s offset position</td>\n</tr>\n<tr>\n<td>Server restart between chunks</td>\n<td>HEAD request with session ID</td>\n<td>404 if session lost, current offset if recovered</td>\n<td>Reinitialize session or resume upload</td>\n</tr>\n<tr>\n<td>Chunk rejected with 409 Conflict</td>\n<td>HEAD request to get actual offset</td>\n<td>Server&#39;s expected offset for next chunk</td>\n<td>Adjust client position and retry</td>\n</tr>\n<tr>\n<td>Client application restart</td>\n<td>HEAD request to restore state</td>\n<td>Complete session status and current offset</td>\n<td>Resume upload or handle completion</td>\n</tr>\n</tbody></table>\n<p>The offset calculation must account for the <strong>chunk boundary alignment</strong> inherent in streaming uploads. Unlike traditional file operations that can seek to arbitrary byte positions, chunked uploads advance in discrete increments corresponding to successfully processed chunks. The server&#39;s offset always represents a clean boundary where the next chunk can be appended without gaps or overlaps.</p>\n<p>Client resume logic implements a <strong>three-phase recovery protocol</strong> to handle the most common interruption scenarios:</p>\n<ol>\n<li><p><strong>State Discovery Phase</strong>: The client sends a HEAD request to determine whether the upload session still exists and what offset the server expects for the next chunk. This phase handles session expiration, server restarts, and network partitions.</p>\n</li>\n<li><p><strong>Synchronization Phase</strong>: The client compares the server&#39;s offset with its own internal tracking and adjusts its position accordingly. If the server is ahead (chunks were successfully processed after the client&#39;s last acknowledgment), the client skips forward. If the server is behind (the client&#39;s last chunk was rejected or lost), the client rewinds.</p>\n</li>\n<li><p><strong>Resumption Phase</strong>: The client resumes chunk uploads from the synchronized position, with each subsequent chunk building on the verified foundation established during synchronization.</p>\n</li>\n</ol>\n<p>The server&#39;s offset update logic must be <strong>atomically coupled</strong> with the chunk write operation to prevent inconsistencies:</p>\n<table>\n<thead>\n<tr>\n<th>Operation Sequence</th>\n<th>Atomicity Requirement</th>\n<th>Failure Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Validate chunk offset against current session offset</td>\n<td>Must read consistent session state</td>\n<td>Retry with fresh session data</td>\n</tr>\n<tr>\n<td>Write chunk data to storage backend</td>\n<td>Must complete fully or not at all</td>\n<td>Clean up partial writes, don&#39;t advance offset</td>\n</tr>\n<tr>\n<td>Update session offset and metadata</td>\n<td>Must reflect successful write</td>\n<td>Rollback to previous offset on failure</td>\n</tr>\n<tr>\n<td>Respond to client with new offset</td>\n<td>Must match updated session state</td>\n<td>Log inconsistency, client will resync on next operation</td>\n</tr>\n</tbody></table>\n<p>Resume boundary detection requires careful handling of <strong>partial chunk scenarios</strong>. If a chunk upload is interrupted mid-transmission, the server may have received and written part of the chunk data before the connection failed. The server must not advance its offset to include partially-written data, as this would create corruption when the client retries the complete chunk. Instead, the server truncates any partial writes and maintains its offset at the last fully-completed chunk boundary.</p>\n<blockquote>\n<p>The fundamental principle of offset tracking is that <strong>the server&#39;s offset is always the single source of truth</strong>. Clients may maintain their own offset tracking for performance and user experience, but they must be prepared to synchronize with the server&#39;s authoritative state whenever discrepancies arise.</p>\n</blockquote>\n<p>⚠️ <strong>Pitfall: Offset Advancement Before Write Confirmation</strong>\nA common mistake is updating the session&#39;s <code>CurrentOffset</code> before confirming that the chunk was successfully written to the storage backend. If the storage write fails after the offset is updated, the session becomes corrupted with an offset that doesn&#39;t match the actual stored data. Clients that resume from this incorrect offset will create gaps in the file. Always update the offset atomically with the storage write, or implement rollback logic that restores the previous offset when storage operations fail.</p>\n<p><img src=\"/api/project/file-upload-service/architecture-doc/asset?path=diagrams%2Fupload-completion-sequence.svg\" alt=\"Upload Completion and Assembly\"></p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This implementation guidance provides the concrete foundation for building a production-ready chunked upload service. The focus is on providing complete, working infrastructure code and detailed skeletons for the core learning components.</p>\n<p><strong>A. Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Server</td>\n<td>net/http with gorilla/mux for routing</td>\n<td>gin-gonic/gin or echo for performance</td>\n</tr>\n<tr>\n<td>Session Storage</td>\n<td>In-memory map with sync.RWMutex</td>\n<td>Redis or PostgreSQL for persistence</td>\n</tr>\n<tr>\n<td>File I/O</td>\n<td>os.File with manual offset tracking</td>\n<td>Memory-mapped files for large chunks</td>\n</tr>\n<tr>\n<td>Hashing</td>\n<td>crypto/sha256 for chunk verification</td>\n<td>Parallel hashing with worker pools</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td>JSON config files with encoding/json</td>\n<td>Viper for multiple formats and hot reload</td>\n</tr>\n<tr>\n<td>Logging</td>\n<td>Standard log package with structured output</td>\n<td>logrus or zap for structured logging</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>resumable-upload/\n  cmd/\n    upload-server/\n      main.go                    ← Server entry point\n  internal/\n    protocol/\n      handlers.go                ← HTTP handlers for tus.io endpoints\n      middleware.go              ← CORS, logging, recovery middleware\n      validation.go              ← Request validation and sanitization\n    session/\n      manager.go                 ← Session lifecycle management\n      store.go                   ← StateStore interface and implementations\n      cleanup.go                 ← Background cleanup worker\n    storage/\n      backend.go                 ← Storage abstraction (Milestone 2)\n      local.go                   ← Local filesystem backend\n    config/\n      config.go                  ← Configuration loading and validation\n  pkg/\n    tusio/\n      protocol.go                ← tus.io protocol constants and helpers\n  test/\n    integration/\n      upload_test.go             ← End-to-end upload scenarios\n    fixtures/\n      test_files/                ← Sample files for testing</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code:</strong></p>\n<p>Complete HTTP server foundation with middleware and routing:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/protocol/server.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> protocol</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/gorilla/mux</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">resumable-upload/internal/config</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">resumable-upload/internal/session</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Server</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">config</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Config</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    manager    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">session</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">SessionManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    httpServer </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Server</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewServer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">cfg</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">config</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">mgr</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">session</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">SessionManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config:  cfg,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        manager: mgr,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    router </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> mux.</span><span style=\"color:#B392F0\">NewRouter</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    router.</span><span style=\"color:#B392F0\">Use</span><span style=\"color:#E1E4E8\">(s.corsMiddleware)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    router.</span><span style=\"color:#B392F0\">Use</span><span style=\"color:#E1E4E8\">(s.loggingMiddleware)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    router.</span><span style=\"color:#B392F0\">Use</span><span style=\"color:#E1E4E8\">(s.recoveryMiddleware)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // tus.io protocol endpoints</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    router.</span><span style=\"color:#B392F0\">HandleFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/files\"</span><span style=\"color:#E1E4E8\">, s.createUpload).</span><span style=\"color:#B392F0\">Methods</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"POST\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    router.</span><span style=\"color:#B392F0\">HandleFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/files/{id}\"</span><span style=\"color:#E1E4E8\">, s.getUploadStatus).</span><span style=\"color:#B392F0\">Methods</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"HEAD\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    router.</span><span style=\"color:#B392F0\">HandleFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/files/{id}\"</span><span style=\"color:#E1E4E8\">, s.uploadChunk).</span><span style=\"color:#B392F0\">Methods</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"PATCH\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    router.</span><span style=\"color:#B392F0\">HandleFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/files/{id}\"</span><span style=\"color:#E1E4E8\">, s.deleteUpload).</span><span style=\"color:#B392F0\">Methods</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"DELETE\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.httpServer </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Addr:         fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, cfg.Server.Host, cfg.Server.Port),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Handler:      router,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ReadTimeout:  cfg.Server.ReadTimeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        WriteTimeout: cfg.Server.WriteTimeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> s</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> s.httpServer.</span><span style=\"color:#B392F0\">ListenAndServe</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Shutdown</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> s.httpServer.</span><span style=\"color:#B392F0\">Shutdown</span><span style=\"color:#E1E4E8\">(ctx)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">corsMiddleware</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">next</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Handler</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Handler</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> http.</span><span style=\"color:#B392F0\">HandlerFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        w.</span><span style=\"color:#B392F0\">Header</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Access-Control-Allow-Origin\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"*\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        w.</span><span style=\"color:#B392F0\">Header</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Access-Control-Allow-Methods\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"POST, HEAD, PATCH, DELETE, OPTIONS\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        w.</span><span style=\"color:#B392F0\">Header</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Access-Control-Allow-Headers\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Upload-Offset, Upload-Length, Upload-Metadata, Content-Type, Tus-Resumable\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        w.</span><span style=\"color:#B392F0\">Header</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Access-Control-Expose-Headers\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Upload-Offset, Upload-Length, Tus-Resumable, Location\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        w.</span><span style=\"color:#B392F0\">Header</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Tus-Resumable\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"1.0.0\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> r.Method </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"OPTIONS\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            w.</span><span style=\"color:#B392F0\">WriteHeader</span><span style=\"color:#E1E4E8\">(http.StatusNoContent)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        next.</span><span style=\"color:#B392F0\">ServeHTTP</span><span style=\"color:#E1E4E8\">(w, r)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">loggingMiddleware</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">next</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Handler</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Handler</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> http.</span><span style=\"color:#B392F0\">HandlerFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        next.</span><span style=\"color:#B392F0\">ServeHTTP</span><span style=\"color:#E1E4E8\">(w, r)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        fmt.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#79B8FF\"> %s</span><span style=\"color:#79B8FF\"> %s</span><span style=\"color:#79B8FF\"> %v\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, r.Method, r.URL.Path, r.RemoteAddr, time.</span><span style=\"color:#B392F0\">Since</span><span style=\"color:#E1E4E8\">(start))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">recoveryMiddleware</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">next</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Handler</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Handler</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> http.</span><span style=\"color:#B392F0\">HandlerFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        defer</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> recover</span><span style=\"color:#E1E4E8\">(); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                fmt.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Panic recovered: </span><span style=\"color:#79B8FF\">%v\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                http.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">(w, </span><span style=\"color:#9ECBFF\">\"Internal Server Error\"</span><span style=\"color:#E1E4E8\">, http.StatusInternalServerError)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        next.</span><span style=\"color:#B392F0\">ServeHTTP</span><span style=\"color:#E1E4E8\">(w, r)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p>Complete configuration management with validation:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/config/config.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> config</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Config</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Server   </span><span style=\"color:#B392F0\">ServerConfig</span><span style=\"color:#9ECBFF\">   `json:\"server\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Storage  </span><span style=\"color:#B392F0\">StorageConfig</span><span style=\"color:#9ECBFF\">  `json:\"storage\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Security </span><span style=\"color:#B392F0\">SecurityConfig</span><span style=\"color:#9ECBFF\"> `json:\"security\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Cleanup  </span><span style=\"color:#B392F0\">CleanupConfig</span><span style=\"color:#9ECBFF\">  `json:\"cleanup\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ServerConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Host         </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"host\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Port         </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `json:\"port\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ReadTimeout  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"read_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    WriteTimeout </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"write_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxBodySize  </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">         `json:\"max_body_size\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StorageConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Backend   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"backend\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LocalPath </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"local_path\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    S3Config  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">S3Config</span><span style=\"color:#9ECBFF\">         `json:\"s3_config,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Options   </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"options\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> S3Config</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Bucket          </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"bucket\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Region          </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"region\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AccessKeyID     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"access_key_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SecretAccessKey </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"secret_access_key\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Endpoint        </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"endpoint,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SecurityConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxFileSize         </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">    `json:\"max_file_size\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AllowedContentTypes []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"allowed_content_types\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    VirusScanEnabled    </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">     `json:\"virus_scan_enabled\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ClamAVSocket        </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">   `json:\"clamav_socket\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CleanupConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SessionTTL      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"session_ttl\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CleanupInterval </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"cleanup_interval\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    QuarantineTTL   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"quarantine_ttl\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> LoadConfig</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">filename</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Set defaults</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Server: </span><span style=\"color:#B392F0\">ServerConfig</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Host:         </span><span style=\"color:#9ECBFF\">\"localhost\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Port:         </span><span style=\"color:#79B8FF\">8080</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ReadTimeout:  </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            WriteTimeout: </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            MaxBodySize:  </span><span style=\"color:#79B8FF\">100</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#6A737D\">// 100MB</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Storage: </span><span style=\"color:#B392F0\">StorageConfig</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Backend:   </span><span style=\"color:#9ECBFF\">\"local\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            LocalPath: </span><span style=\"color:#9ECBFF\">\"./uploads\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Options:   </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Security: </span><span style=\"color:#B392F0\">SecurityConfig</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            MaxFileSize:         </span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#6A737D\">// 1GB</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            AllowedContentTypes: []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"*/*\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            VirusScanEnabled:    </span><span style=\"color:#79B8FF\">false</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Cleanup: </span><span style=\"color:#B392F0\">CleanupConfig</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            SessionTTL:      </span><span style=\"color:#79B8FF\">24</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Hour,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            CleanupInterval: time.Hour,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            QuarantineTTL:   </span><span style=\"color:#79B8FF\">7</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 24</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Hour,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> filename </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        file, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">Open</span><span style=\"color:#E1E4E8\">(filename)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to open config file: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        defer</span><span style=\"color:#E1E4E8\"> file.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        decoder </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> json.</span><span style=\"color:#B392F0\">NewDecoder</span><span style=\"color:#E1E4E8\">(file)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> decoder.</span><span style=\"color:#B392F0\">Decode</span><span style=\"color:#E1E4E8\">(config); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to parse config file: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> config.</span><span style=\"color:#B392F0\">ValidateRequirements</span><span style=\"color:#E1E4E8\">(); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"config validation failed: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> config, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ValidateRequirements</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.Server.Port </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> ||</span><span style=\"color:#E1E4E8\"> c.Server.Port </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 65535</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"server port must be between 1 and 65535\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.Security.MaxFileSize </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"max file size must be positive\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.Storage.Backend </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"local\"</span><span style=\"color:#F97583\"> &#x26;&#x26;</span><span style=\"color:#E1E4E8\"> c.Storage.LocalPath </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"local storage path must be specified\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code:</strong></p>\n<p>Protocol handlers with detailed implementation guidance:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/protocol/handlers.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> protocol</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/base64</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">io</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strconv</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strings</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/gorilla/mux</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">resumable-upload/internal/session</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// createUpload initializes a new resumable upload session</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Implements POST /files endpoint from tus.io specification</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">createUpload</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate tus.io protocol version in Tus-Resumable header</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Should be \"1.0.0\", return 412 Precondition Failed if missing/wrong</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Parse Upload-Length header and validate against security limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use strconv.ParseInt, check against s.config.Security.MaxFileSize</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Parse Upload-Metadata header (base64 encoded key-value pairs)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Format is \"key1 base64value1,key2 base64value2\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Split on comma, then space, decode values with base64.StdEncoding</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Extract filename and content-type from metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Standard keys are \"filename\" and \"filetype\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Validate content type against allowed types list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Check s.config.Security.AllowedContentTypes, \"*/*\" means allow all</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Call s.manager.InitializeUpload with parsed parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Pass context, filename, totalSize, and metadata map</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Set response headers and return session URL</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Location header should be \"/files/{sessionID}\", status 201 Created</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// getUploadStatus returns current upload progress for resumption</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Implements HEAD /files/{id} endpoint from tus.io specification  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">getUploadStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Extract session ID from URL path using mux.Vars(r)[\"id\"]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Call s.manager.GetUploadProgress with session ID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Handle ErrSessionNotFound with 404 Not Found response</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Set Upload-Offset header with current session offset</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use strconv.FormatInt to convert int64 to string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Set Upload-Length header with total file size</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Set Cache-Control: no-store to prevent offset caching</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Offset values change frequently and must not be cached</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Return 200 OK with no body (HEAD request)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// uploadChunk processes a chunk upload and updates session progress</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Implements PATCH /files/{id} endpoint from tus.io specification</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">uploadChunk</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Extract session ID from URL path</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Parse Upload-Offset header and validate format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Must be valid integer, represents byte offset for this chunk</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Validate Content-Type header equals \"application/offset+octet-stream\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: This is required by tus.io spec for chunk uploads</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Read request body with size limit protection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use io.LimitReader with s.config.Server.MaxBodySize</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Don't load entire chunk into memory - stream to ProcessChunk</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Calculate chunk hash for integrity verification</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use crypto/sha256, hash while reading to avoid double I/O</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Call s.manager.ProcessChunk with session, offset, data, hash</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Handle offset mismatch errors with 409 Conflict response</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Set Upload-Offset header with new session offset</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: New offset = previous offset + chunk size</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Return 204 No Content on success</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// deleteUpload cancels an upload session and cleans up resources</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Implements DELETE /files/{id} endpoint from tus.io specification</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">deleteUpload</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Extract session ID from URL path</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Call s.manager.DeleteSession to remove session and data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Should clean up both session metadata and partial chunks</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Return 204 No Content regardless of whether session existed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: DELETE is idempotent - same result if called multiple times</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p>Session management core with state transitions:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/session/manager.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> session</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">crypto/rand</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">resumable-upload/internal/storage</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SessionStatus</span><span style=\"color:#F97583\"> string</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SessionStatusInitialized</span><span style=\"color:#B392F0\"> SessionStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"initialized\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SessionStatusActive</span><span style=\"color:#B392F0\">      SessionStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"active\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SessionStatusCompleting</span><span style=\"color:#B392F0\">  SessionStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"completing\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SessionStatusCompleted</span><span style=\"color:#B392F0\">   SessionStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"completed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SessionStatusFailed</span><span style=\"color:#B392F0\">      SessionStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"failed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SessionStatusExpired</span><span style=\"color:#B392F0\">     SessionStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"expired\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> UploadSession</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ID           </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Filename     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"filename\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ContentType  </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"content_type\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TotalSize    </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">             `json:\"total_size\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CurrentOffset </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">             `json:\"current_offset\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Status       </span><span style=\"color:#B392F0\">SessionStatus</span><span style=\"color:#9ECBFF\">     `json:\"status\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StorageKey   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"storage_key\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Metadata     </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"metadata\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CreatedAt    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">         `json:\"created_at\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    UpdatedAt    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">         `json:\"updated_at\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ChunkHashes  </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">  `json:\"chunk_hashes\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SessionManager</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    store   </span><span style=\"color:#B392F0\">StateStore</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage </span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">StorageBackend</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewSessionManager</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">store</span><span style=\"color:#B392F0\"> StateStore</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">backend</span><span style=\"color:#B392F0\"> storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">StorageBackend</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SessionManager</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">SessionManager</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        store:   store,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        storage: backend,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// InitializeUpload creates a new upload session and prepares storage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SessionManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">InitializeUpload</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">filename</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">totalSize</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">metadata</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Generate cryptographically secure session ID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use crypto/rand with 16 bytes, encode as hex string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create UploadSession struct with initialized status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Set CreatedAt/UpdatedAt to time.Now(), CurrentOffset to 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Generate storage key for this upload</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use session ID as base, let storage backend handle path construction</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Call storage backend to prepare upload location</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Some backends (like S3) need multipart upload initialization</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Store session in StateStore</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use m.store.CreateSession, handle conflicts with new ID generation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Return session pointer for client response</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ProcessChunk handles incoming chunk data with offset validation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SessionManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ProcessChunk</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">offset</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">contentHash</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Retrieve session from store and validate it exists</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Return ErrSessionNotFound if not found</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate session is in active or initialized state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Cannot accept chunks for completed/failed/expired sessions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Validate offset matches session's current offset exactly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Return offset mismatch error if not equal - client must sync</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Validate chunk size doesn't exceed remaining file size</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: offset + len(data) must not exceed session.TotalSize</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Write chunk to storage backend at specified offset</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use m.storage.WriteChunk, handle backend-specific errors</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Update session with new offset and chunk hash</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: New offset = old offset + len(data), store hash for verification</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Check if upload is complete (offset == total size)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: If complete, transition to completing status and trigger assembly</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Save updated session to store atomically</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetUploadProgress returns current session state for resume</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SessionManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetUploadProgress</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Retrieve session from store</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check if session has expired based on last activity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Compare time.Now() - session.UpdatedAt against configured TTL</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Return session if valid, or ErrSessionExpired if too old</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints:</strong></p>\n<ul>\n<li><strong>HTTP Request Handling</strong>: Use <code>r.Header.Get()</code> for single headers, <code>r.Header[&quot;Header-Name&quot;]</code> for multiple values</li>\n<li><strong>Body Reading</strong>: Always use <code>io.LimitReader(r.Body, maxSize)</code> to prevent memory exhaustion attacks</li>\n<li><strong>Atomic Operations</strong>: Use <code>sync.Mutex</code> around session state changes, not just reads</li>\n<li><strong>Error Handling</strong>: Create custom error types with <code>errors.New()</code> for protocol-specific errors</li>\n<li><strong>Context Usage</strong>: Pass <code>context.Context</code> through all functions for cancellation and timeouts</li>\n<li><strong>File I/O</strong>: Use <code>os.OpenFile()</code> with <code>O_CREATE|O_WRONLY</code> flags for chunk writing</li>\n<li><strong>Hashing</strong>: Stream data through <code>hash.Hash.Write()</code> while processing to avoid double I/O</li>\n</ul>\n<p><strong>F. Milestone Checkpoint:</strong></p>\n<p>After implementing the chunked upload protocol:</p>\n<ol>\n<li><strong>Start the server</strong>: <code>go run cmd/upload-server/main.go</code></li>\n<li><strong>Initialize upload</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">   curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/files</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">     -H</span><span style=\"color:#9ECBFF\"> \"Upload-Length: 1000\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">     -H</span><span style=\"color:#9ECBFF\"> \"Upload-Metadata: filename dGVzdC50eHQ=\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">     -H</span><span style=\"color:#9ECBFF\"> \"Tus-Resumable: 1.0.0\"</span></span></code></pre></div>\n<p>   Expected: <code>201 Created</code> with <code>Location</code> header containing session URL</p>\n<ol start=\"3\">\n<li><strong>Upload chunk</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">   curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> PATCH</span><span style=\"color:#9ECBFF\"> http://localhost:8080/files/{session-id}</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">     -H</span><span style=\"color:#9ECBFF\"> \"Upload-Offset: 0\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">     -H</span><span style=\"color:#9ECBFF\"> \"Content-Type: application/offset+octet-stream\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">     -H</span><span style=\"color:#9ECBFF\"> \"Tus-Resumable: 1.0.0\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">     --data-binary</span><span style=\"color:#9ECBFF\"> @test-chunk.bin</span></span></code></pre></div>\n<p>   Expected: <code>204 No Content</code> with updated <code>Upload-Offset</code> header</p>\n<ol start=\"4\">\n<li><strong>Check progress</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">   curl</span><span style=\"color:#79B8FF\"> -I</span><span style=\"color:#9ECBFF\"> http://localhost:8080/files/{session-id}</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">     -H</span><span style=\"color:#9ECBFF\"> \"Tus-Resumable: 1.0.0\"</span></span></code></pre></div>\n<p>   Expected: <code>200 OK</code> with current <code>Upload-Offset</code> and <code>Upload-Length</code> headers</p>\n<p><strong>Signs of Success</strong>: Session IDs are generated consistently, offset tracking advances correctly, chunks are written to storage, HEAD requests return accurate progress information.</p>\n<p><strong>Common Issues</strong>: Offset mismatches (check atomic session updates), memory leaks (ensure body reading limits), race conditions (verify mutex usage around session state).</p>\n<h2 id=\"storage-backend-abstraction\">Storage Backend Abstraction</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 2 (Storage Abstraction) — this section implements pluggable storage backends with unified operations for local filesystem, S3, and GCS</p>\n</blockquote>\n<p>Think of storage backend abstraction as a <strong>universal remote control</strong> for different storage systems. Just as you can use the same remote interface (power, volume, channel buttons) to control different TV brands without knowing their internal protocols, our storage abstraction provides the same operations (read, write, delete) across different storage systems without exposing their implementation differences. The abstraction layer translates common operations into the specific API calls and protocols required by each backend, whether that&#39;s POSIX file operations for local storage or REST API calls for cloud object stores.</p>\n<p><img src=\"/api/project/file-upload-service/architecture-doc/asset?path=diagrams%2Fstorage-abstraction-class.svg\" alt=\"Storage Backend Class Hierarchy\"></p>\n<p>The storage abstraction serves as the foundation that enables the resumable upload service to be deployment-agnostic. Whether running on a single server with local disk storage or in a distributed cloud environment with object storage, the upload service logic remains unchanged. This abstraction becomes particularly critical during chunked uploads where we need to coordinate multipart operations across potentially unreliable network connections while maintaining consistency guarantees.</p>\n<h3 id=\"storage-interface-design\">Storage Interface Design</h3>\n<p>The storage interface represents the contract that all backend implementations must fulfill. Think of this interface as a <strong>standardized shipping container</strong> — regardless of whether containers travel by truck, ship, or train, they maintain the same dimensions and connection points. Similarly, our storage interface defines standard operations that work identically whether data travels to local disks, S3 buckets, or other cloud storage systems.</p>\n<p>The interface design must balance simplicity with the diverse capabilities of different storage systems. Local filesystems provide atomic operations and immediate consistency, while object storage systems like S3 offer eventual consistency and require different patterns for multipart uploads. The abstraction must present a unified view while allowing backends to optimize for their specific characteristics.</p>\n<blockquote>\n<p><strong>Decision: Streaming Interface with Context Support</strong></p>\n<ul>\n<li><strong>Context</strong>: Upload chunks can be large (several MB) and network operations may timeout or be cancelled</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Byte slice interface (simple but requires full data in memory)</li>\n<li><code>io.Reader</code> interface (streaming but no cancellation support)</li>\n<li>Context-aware streaming interface (complex but supports cancellation and timeouts)</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use context-aware streaming interface with <code>io.Reader</code> for data</li>\n<li><strong>Rationale</strong>: Large file uploads require streaming to avoid memory exhaustion, and production systems need timeout and cancellation support for resource management</li>\n<li><strong>Consequences</strong>: More complex implementation but enables memory-efficient uploads with proper resource cleanup</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>StoreChunk</code></td>\n<td><code>ctx Context, key string, data io.Reader, size int64</code></td>\n<td><code>error</code></td>\n<td>Stores chunk data at the specified key with known size for validation</td>\n</tr>\n<tr>\n<td><code>ReadChunk</code></td>\n<td><code>ctx Context, key string</code></td>\n<td><code>io.ReadCloser, error</code></td>\n<td>Returns a reader for chunk data, caller must close</td>\n</tr>\n<tr>\n<td><code>DeleteChunk</code></td>\n<td><code>ctx Context, key string</code></td>\n<td><code>error</code></td>\n<td>Removes chunk data, idempotent operation</td>\n</tr>\n<tr>\n<td><code>ChunkExists</code></td>\n<td><code>ctx Context, key string</code></td>\n<td><code>bool, error</code></td>\n<td>Checks if chunk exists without retrieving data</td>\n</tr>\n<tr>\n<td><code>InitMultipart</code></td>\n<td><code>ctx Context, key string, metadata map[string]string</code></td>\n<td><code>MultipartUpload, error</code></td>\n<td>Begins multipart upload session for large files</td>\n</tr>\n<tr>\n<td><code>StoreMultipartChunk</code></td>\n<td><code>ctx Context, upload MultipartUpload, partNum int, data io.Reader, size int64</code></td>\n<td><code>MultipartPart, error</code></td>\n<td>Stores one part of multipart upload</td>\n</tr>\n<tr>\n<td><code>CompleteMultipart</code></td>\n<td><code>ctx Context, upload MultipartUpload, parts []MultipartPart</code></td>\n<td><code>error</code></td>\n<td>Assembles all parts into final file</td>\n</tr>\n<tr>\n<td><code>AbortMultipart</code></td>\n<td><code>ctx Context, upload MultipartUpload</code></td>\n<td><code>error</code></td>\n<td>Cancels multipart upload and cleans up parts</td>\n</tr>\n<tr>\n<td><code>GenerateSignedURL</code></td>\n<td><code>ctx Context, key string, expiration Duration</code></td>\n<td><code>string, error</code></td>\n<td>Creates time-limited download URL for secure access</td>\n</tr>\n<tr>\n<td><code>ListKeys</code></td>\n<td><code>ctx Context, prefix string, limit int</code></td>\n<td><code>[]string, error</code></td>\n<td>Returns keys matching prefix for cleanup operations</td>\n</tr>\n</tbody></table>\n<p>The interface separates simple chunk operations from multipart upload workflows because different backends have vastly different multipart capabilities. Local storage can simulate multipart uploads by writing to temporary files, while S3 has native multipart upload APIs with specific constraints (minimum part size, maximum parts, etc.).</p>\n<blockquote>\n<p>The key design insight is that the interface exposes multipart upload explicitly rather than hiding it behind simple operations. This allows backends to optimize for their native capabilities while giving the upload service control over the multipart workflow.</p>\n</blockquote>\n<p><strong>Multipart Upload Data Structures</strong></p>\n<p>The multipart upload abstraction requires additional data structures to track upload sessions and parts across different backends:</p>\n<table>\n<thead>\n<tr>\n<th>Structure</th>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>MultipartUpload</code></td>\n<td><code>ID</code></td>\n<td><code>string</code></td>\n<td>Backend-specific upload identifier</td>\n</tr>\n<tr>\n<td><code>MultipartUpload</code></td>\n<td><code>Key</code></td>\n<td><code>string</code></td>\n<td>Final object key where assembled file will be stored</td>\n</tr>\n<tr>\n<td><code>MultipartUpload</code></td>\n<td><code>Backend</code></td>\n<td><code>string</code></td>\n<td>Storage backend that created this upload</td>\n</tr>\n<tr>\n<td><code>MultipartUpload</code></td>\n<td><code>Metadata</code></td>\n<td><code>map[string]string</code></td>\n<td>Custom metadata to attach to final object</td>\n</tr>\n<tr>\n<td><code>MultipartUpload</code></td>\n<td><code>CreatedAt</code></td>\n<td><code>time.Time</code></td>\n<td>Upload session creation timestamp</td>\n</tr>\n<tr>\n<td><code>MultipartPart</code></td>\n<td><code>PartNumber</code></td>\n<td><code>int</code></td>\n<td>Sequential part number starting from 1</td>\n</tr>\n<tr>\n<td><code>MultipartPart</code></td>\n<td><code>ETag</code></td>\n<td><code>string</code></td>\n<td>Backend-provided integrity checksum</td>\n</tr>\n<tr>\n<td><code>MultipartPart</code></td>\n<td><code>Size</code></td>\n<td><code>int64</code></td>\n<td>Number of bytes in this part</td>\n</tr>\n<tr>\n<td><code>MultipartPart</code></td>\n<td><code>PartID</code></td>\n<td><code>string</code></td>\n<td>Backend-specific part identifier</td>\n</tr>\n</tbody></table>\n<p>Each backend translates these abstract structures to their native representations. For S3, the <code>MultipartUpload.ID</code> becomes the S3 upload ID, and <code>MultipartPart.ETag</code> contains the S3 ETag header. For local storage, the upload ID might be a temporary directory name, and the ETag could be a computed MD5 hash.</p>\n<p><strong>Error Classification and Handling</strong></p>\n<p>Storage backends can fail in different ways, and the interface must provide enough information for the upload service to make intelligent retry and recovery decisions:</p>\n<table>\n<thead>\n<tr>\n<th>Error Type</th>\n<th>Detection</th>\n<th>Recovery Strategy</th>\n<th>Example Scenarios</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ErrNotFound</code></td>\n<td>Key does not exist</td>\n<td>Retry upload from beginning</td>\n<td>Chunk lost during cleanup</td>\n</tr>\n<tr>\n<td><code>ErrAlreadyExists</code></td>\n<td>Key collision during creation</td>\n<td>Generate new key or resume</td>\n<td>Concurrent upload to same path</td>\n</tr>\n<tr>\n<td><code>ErrInsufficientSpace</code></td>\n<td>Disk full or quota exceeded</td>\n<td>Fail gracefully, retry with different backend</td>\n<td>Local disk full</td>\n</tr>\n<tr>\n<td><code>ErrNetworkTimeout</code></td>\n<td>Network operation timeout</td>\n<td>Exponential backoff retry</td>\n<td>S3 API timeout</td>\n</tr>\n<tr>\n<td><code>ErrAuthenticationFailed</code></td>\n<td>Invalid credentials</td>\n<td>Fail immediately, check configuration</td>\n<td>Expired AWS credentials</td>\n</tr>\n<tr>\n<td><code>ErrInvalidMultipartState</code></td>\n<td>Part uploaded to wrong session</td>\n<td>Abort upload, restart</td>\n<td>Client/server state mismatch</td>\n</tr>\n</tbody></table>\n<h3 id=\"local-filesystem-backend\">Local Filesystem Backend</h3>\n<p>The local filesystem backend implements the storage interface using POSIX file operations. Think of this backend as a <strong>traditional filing cabinet</strong> — it provides immediate access to documents, strong consistency guarantees, and simple organization, but it&#39;s limited to the capacity and reliability of a single physical location.</p>\n<p>Local storage serves multiple purposes in the resumable upload service: development and testing environments, single-server deployments, and as a quarantine storage area for suspicious files before cloud upload. The implementation must handle the unique characteristics of filesystem operations while providing the same interface as cloud storage backends.</p>\n<blockquote>\n<p><strong>Decision: Directory-Based Organization with Atomic Operations</strong></p>\n<ul>\n<li><strong>Context</strong>: Local filesystem needs to organize chunks, track multipart uploads, and ensure consistency across process restarts</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Flat file storage with encoded keys (simple but hard to manage)</li>\n<li>Directory hierarchy mirroring object keys (intuitive but complex path handling)</li>\n<li>Separate directories for chunks, multipart sessions, and completed files (clean separation)</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use separate directories with atomic operations via temporary files</li>\n<li><strong>Rationale</strong>: Clean separation simplifies cleanup, atomic operations prevent corruption, predictable layout aids debugging</li>\n<li><strong>Consequences</strong>: More complex path management but better reliability and maintainability</li>\n</ul>\n</blockquote>\n<p><strong>Directory Structure and Organization</strong></p>\n<p>The local backend organizes files in a structured directory hierarchy that separates different types of data and supports efficient cleanup operations:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>storage-root/\n├── chunks/           ← Single-chunk uploads\n│   └── session-id/\n│       └── chunk-hash\n├── multipart/        ← Multipart upload sessions\n│   └── upload-id/\n│       ├── metadata.json\n│       ├── parts/\n│       │   ├── part-001\n│       │   ├── part-002\n│       │   └── ...\n│       └── assembly/  ← Temporary assembly area\n└── completed/        ← Finalized files ready for download\n    └── final-key</code></pre></div>\n\n<p>This structure provides several benefits: chunks are isolated by session to prevent conflicts, multipart uploads maintain their parts in predictable locations, and completed files are separated for easy cleanup and access control. The assembly directory provides a workspace for combining multipart uploads atomically.</p>\n<p><strong>Path Safety and Security Considerations</strong></p>\n<p>Local filesystem backends face unique security challenges because malicious clients could potentially manipulate file paths to escape the designated storage area. The implementation must sanitize all path components and prevent directory traversal attacks:</p>\n<table>\n<thead>\n<tr>\n<th>Security Concern</th>\n<th>Threat</th>\n<th>Mitigation Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Directory Traversal</td>\n<td><code>../../../etc/passwd</code> in keys</td>\n<td>Reject keys containing <code>..</code>, <code>/</code>, or <code>\\</code></td>\n</tr>\n<tr>\n<td>Null Byte Injection</td>\n<td><code>valid-key\\x00.txt</code> truncation</td>\n<td>Validate keys contain only printable ASCII</td>\n</tr>\n<tr>\n<td>Long Path Attacks</td>\n<td>Extremely long filenames</td>\n<td>Limit key length and use hash-based names</td>\n</tr>\n<tr>\n<td>Symlink Exploitation</td>\n<td>Symbolic links to sensitive files</td>\n<td>Use <code>O_NOFOLLOW</code> and validate real paths</td>\n</tr>\n<tr>\n<td>Case Sensitivity</td>\n<td><code>Key</code> vs <code>key</code> collisions on macOS</td>\n<td>Normalize keys to lowercase with hash suffix</td>\n</tr>\n</tbody></table>\n<p>The path sanitization algorithm follows these steps:</p>\n<ol>\n<li>Validate the key contains only alphanumeric characters, hyphens, underscores, and periods</li>\n<li>Check the key length does not exceed 255 characters (filesystem limit)</li>\n<li>Hash the key with SHA-256 to create a safe filename</li>\n<li>Combine the hash with a truncated version of the original key for debugging</li>\n<li>Construct the full path within the designated storage root</li>\n<li>Verify the resolved path remains within the storage directory</li>\n</ol>\n<p><strong>Atomic Operations and Consistency</strong></p>\n<p>Filesystem operations must be atomic to prevent corruption during process crashes or concurrent access. The local backend uses the <strong>write-then-rename</strong> pattern for all file modifications:</p>\n<ol>\n<li><strong>Temporary File Creation</strong>: Write new data to a temporary file with a <code>.tmp</code> extension in the same directory as the final destination</li>\n<li><strong>Content Verification</strong>: Verify the written data matches expected size and checksum</li>\n<li><strong>Atomic Rename</strong>: Use <code>os.Rename()</code> to atomically move the temporary file to its final location</li>\n<li><strong>Cleanup on Failure</strong>: Remove temporary files if any step fails</li>\n</ol>\n<p>This pattern ensures that readers never see partially written files, and process crashes cannot leave the storage in an inconsistent state. The temporary files use unique names (UUID or timestamp-based) to prevent conflicts during concurrent uploads.</p>\n<p><strong>Multipart Upload Implementation</strong></p>\n<p>Local storage simulates multipart uploads using a staging area that accumulates parts before final assembly. The process involves several coordinated steps:</p>\n<ol>\n<li><strong>Session Initialization</strong>: Create a directory under <code>multipart/</code> with the upload ID, write metadata file with target key and part information</li>\n<li><strong>Part Storage</strong>: Store each part as a numbered file (<code>part-001</code>, <code>part-002</code>) with size validation and checksum computation</li>\n<li><strong>Assembly Process</strong>: When all parts are received, stream-copy them in order to a temporary file in the assembly directory</li>\n<li><strong>Integrity Verification</strong>: Compute final file checksum and compare against expected value from client</li>\n<li><strong>Atomic Completion</strong>: Rename the assembled file to its final location under <code>completed/</code></li>\n</ol>\n<p>The assembly process handles large files efficiently by streaming parts rather than loading them entirely into memory. Parts are read in sequence and written to the output file using buffered I/O with configurable buffer sizes (typically 64KB to 1MB).</p>\n<p><strong>Common Pitfalls</strong></p>\n<p>⚠️ <strong>Pitfall: Filesystem Permission Races</strong>\nMany implementations fail to handle filesystem permission changes that occur between checking permissions and performing operations. The service process might lose write access to directories while uploads are in progress, causing cryptic failures.</p>\n<p><strong>Why it&#39;s wrong</strong>: Checking permissions separately from file operations creates a race condition where permissions can change between the check and the operation.</p>\n<p><strong>How to fix</strong>: Attempt file operations directly and handle permission errors explicitly. Use the file operation result as the definitive permission check rather than pre-checking with <code>os.Access()</code>.</p>\n<p>⚠️ <strong>Pitfall: Incomplete Cleanup of Temporary Files</strong>\nFailed uploads often leave temporary files scattered throughout the storage directories, gradually consuming disk space and creating operational problems.</p>\n<p><strong>Why it&#39;s wrong</strong>: Process crashes or panics can interrupt cleanup code, and deferred cleanup may not execute if the process is killed forcefully.</p>\n<p><strong>How to fix</strong>: Implement a separate cleanup process that scans for temporary files older than a threshold (e.g., 24 hours) and removes them. Use file modification times and naming patterns to identify orphaned temporaries safely.</p>\n<h3 id=\"s3-compatible-backend\">S3-Compatible Backend</h3>\n<p>The S3-compatible backend integrates with cloud object storage services that implement the S3 API, including Amazon S3, MinIO, Google Cloud Storage (via S3 compatibility), and other providers. Think of S3 as a <strong>global warehouse network</strong> — it provides virtually unlimited capacity and geographic distribution, but operations have higher latency and follow eventual consistency models rather than immediate consistency.</p>\n<p>Object storage systems fundamentally differ from filesystems in their consistency models, API patterns, and operational characteristics. The S3 backend must navigate these differences while providing the same interface as local storage, handling network failures gracefully, and optimizing for S3&#39;s specific multipart upload requirements.</p>\n<blockquote>\n<p><strong>Decision: Native S3 Multipart Upload Integration</strong></p>\n<ul>\n<li><strong>Context</strong>: S3 has specific multipart upload constraints (5MB minimum part size, 10,000 part limit) and provides native APIs for managing multipart sessions</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Simulate multipart by uploading parts as separate objects then copying (inefficient)</li>\n<li>Buffer small parts until they reach 5MB minimum (memory intensive)</li>\n<li>Use native S3 multipart upload APIs with part size adaptation (complex but efficient)</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use native S3 multipart APIs with automatic part size adjustment</li>\n<li><strong>Rationale</strong>: Native multipart provides better performance, reliability, and cost optimization compared to workarounds</li>\n<li><strong>Consequences</strong>: Implementation complexity increases but provides optimal S3 integration and cost efficiency</li>\n</ul>\n</blockquote>\n<p><strong>S3 Multipart Upload Constraints and Adaptations</strong></p>\n<p>S3 multipart uploads have specific requirements that the backend must handle transparently:</p>\n<table>\n<thead>\n<tr>\n<th>S3 Constraint</th>\n<th>Requirement</th>\n<th>Backend Adaptation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Minimum Part Size</td>\n<td>5MB except for last part</td>\n<td>Buffer small chunks until 5MB threshold</td>\n</tr>\n<tr>\n<td>Maximum Parts</td>\n<td>10,000 parts per upload</td>\n<td>Calculate optimal part size based on total file size</td>\n</tr>\n<tr>\n<td>Part Number Range</td>\n<td>1 to 10,000 inclusive</td>\n<td>Map client part numbers to valid S3 range</td>\n</tr>\n<tr>\n<td>ETag Requirement</td>\n<td>Each part returns ETag for completion</td>\n<td>Store ETags with part metadata for assembly</td>\n</tr>\n<tr>\n<td>Upload Timeout</td>\n<td>7 days maximum session lifetime</td>\n<td>Implement session refresh and cleanup</td>\n</tr>\n</tbody></table>\n<p>The backend automatically calculates optimal part sizes based on the total file size to stay within the 10,000 part limit:</p>\n<ol>\n<li><strong>Size-Based Part Calculation</strong>: For files larger than 50GB (10,000 × 5MB), increase part size to file_size ÷ 9,999 rounded up to next MB</li>\n<li><strong>Buffer Management</strong>: Accumulate client chunks in memory until reaching the target part size, then upload to S3</li>\n<li><strong>Final Part Handling</strong>: Upload remaining buffered data as the final part regardless of size</li>\n<li><strong>Memory Optimization</strong>: Use streaming uploads with configurable buffer limits to prevent excessive memory usage</li>\n</ol>\n<p><strong>Credential Management and Authentication</strong></p>\n<p>S3 backends require secure credential management that supports multiple authentication methods while protecting sensitive information:</p>\n<table>\n<thead>\n<tr>\n<th>Authentication Method</th>\n<th>Use Case</th>\n<th>Configuration</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Static Access Keys</td>\n<td>Development, simple deployments</td>\n<td><code>AccessKeyID</code> and <code>SecretAccessKey</code> in config</td>\n</tr>\n<tr>\n<td>IAM Instance Roles</td>\n<td>EC2 deployments</td>\n<td>No credentials in config, use instance metadata</td>\n</tr>\n<tr>\n<td>IAM Assume Role</td>\n<td>Cross-account access, temporary credentials</td>\n<td>Role ARN with automatic token refresh</td>\n</tr>\n<tr>\n<td>Environment Variables</td>\n<td>Container deployments</td>\n<td><code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code></td>\n</tr>\n</tbody></table>\n<p>The backend implements a credential provider chain that attempts authentication methods in order of preference:</p>\n<ol>\n<li><strong>Explicit Configuration</strong>: Use credentials from <code>StorageConfig.S3Config</code> if provided</li>\n<li><strong>Environment Variables</strong>: Check standard AWS environment variables</li>\n<li><strong>Instance Metadata</strong>: Query EC2 instance metadata service for role credentials</li>\n<li><strong>Credential File</strong>: Read from <code>~/.aws/credentials</code> file</li>\n<li><strong>Error on Failure</strong>: Return authentication error if no valid credentials found</li>\n</ol>\n<p>Credentials are refreshed automatically before expiration, and the backend handles temporary credential rotation transparently to the upload service.</p>\n<p><strong>Network Resilience and Error Handling</strong></p>\n<p>Cloud storage operations face network partitions, API rate limits, and transient failures that require sophisticated retry and backoff strategies:</p>\n<table>\n<thead>\n<tr>\n<th>Failure Type</th>\n<th>S3 Error Code</th>\n<th>Retry Strategy</th>\n<th>Backoff Pattern</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Rate Limiting</td>\n<td><code>SlowDown</code>, <code>ServiceUnavailable</code></td>\n<td>Exponential backoff</td>\n<td>Start 1s, max 60s</td>\n</tr>\n<tr>\n<td>Network Timeout</td>\n<td>Connection timeout</td>\n<td>Immediate retry</td>\n<td>3 attempts, then fail</td>\n</tr>\n<tr>\n<td>Invalid Credentials</td>\n<td><code>InvalidAccessKeyId</code></td>\n<td>No retry</td>\n<td>Fail immediately</td>\n</tr>\n<tr>\n<td>Temporary Server Error</td>\n<td><code>InternalError</code></td>\n<td>Exponential backoff</td>\n<td>Start 500ms, max 30s</td>\n</tr>\n<tr>\n<td>Part Size Error</td>\n<td><code>EntityTooSmall</code></td>\n<td>Retry with larger parts</td>\n<td>No backoff</td>\n</tr>\n</tbody></table>\n<p>The retry implementation uses jittered exponential backoff to prevent thundering herd problems when multiple upload sessions encounter rate limits simultaneously. Each retry attempt includes a random delay component (±25% of the calculated backoff) to distribute load across time.</p>\n<p><strong>S3-Specific Optimizations</strong></p>\n<p>The S3 backend includes several optimizations that leverage S3&#39;s unique capabilities:</p>\n<p><strong>Parallel Part Uploads</strong>: Upload multiple parts concurrently using a worker pool with configurable concurrency limits (typically 3-5 parallel uploads). This dramatically improves throughput for large files while respecting API rate limits.</p>\n<p><strong>Transfer Acceleration</strong>: When enabled, use S3 Transfer Acceleration endpoints that route uploads through Amazon&#39;s global edge network for improved performance from distant geographic locations.</p>\n<p><strong>Storage Class Selection</strong>: Configure default storage classes (Standard, IA, Glacier) based on upload metadata or file characteristics. For example, files marked as archival can use cheaper storage classes automatically.</p>\n<p><strong>Multipart Upload Lifecycle</strong>: Register lifecycle policies that automatically clean up incomplete multipart uploads after a specified period (typically 7 days) to prevent storage cost accumulation from abandoned uploads.</p>\n<p><strong>Presigned URL Generation</strong>: Generate presigned URLs for downloads that allow direct client access to S3 without proxying through the upload service, reducing bandwidth costs and improving performance.</p>\n<h3 id=\"credential-and-configuration-management\">Credential and Configuration Management</h3>\n<p>Credential management represents one of the most critical security aspects of the storage abstraction. Think of credential management as a <strong>secure keychain system</strong> — it must protect sensitive authentication information while making it readily available to authorized operations. The system must handle credential rotation, multiple backend types, and different deployment environments without compromising security or operational reliability.</p>\n<p>The configuration system must be flexible enough to support development environments with simple file-based config, containerized deployments with environment variables, and production systems with secure credential stores like HashiCorp Vault or AWS Secrets Manager.</p>\n<blockquote>\n<p><strong>Decision: Pluggable Credential Provider with Fallback Chain</strong></p>\n<ul>\n<li><strong>Context</strong>: Different deployment environments need different credential sources (files, environment, cloud metadata, external secrets)</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Single configuration file with all credentials (simple but insecure)</li>\n<li>Environment variables only (secure but inflexible)</li>\n<li>Pluggable provider chain with multiple fallback options (complex but comprehensive)</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement pluggable credential providers with configurable fallback chain</li>\n<li><strong>Rationale</strong>: Different environments have different security requirements and credential availability</li>\n<li><strong>Consequences</strong>: More complex implementation but supports secure deployment across diverse environments</li>\n</ul>\n</blockquote>\n<p><strong>Configuration Structure and Validation</strong></p>\n<p>The configuration system uses a layered approach where each storage backend can specify its requirements while sharing common patterns for credential management:</p>\n<table>\n<thead>\n<tr>\n<th>Configuration Layer</th>\n<th>Responsibility</th>\n<th>Example Content</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Global Storage Config</td>\n<td>Backend selection, common settings</td>\n<td><code>Backend: &quot;s3&quot;</code>, <code>CredentialProvider: &quot;vault&quot;</code></td>\n</tr>\n<tr>\n<td>Backend-Specific Config</td>\n<td>Backend parameters</td>\n<td>S3 bucket, region, endpoint configuration</td>\n</tr>\n<tr>\n<td>Credential Provider Config</td>\n<td>Authentication details</td>\n<td>Vault address, credential paths, rotation settings</td>\n</tr>\n<tr>\n<td>Runtime Credential Cache</td>\n<td>Active credentials</td>\n<td>Cached tokens, expiration times, refresh state</td>\n</tr>\n</tbody></table>\n<p>The configuration validation ensures that all required parameters are present and compatible:</p>\n<ol>\n<li><strong>Backend Validation</strong>: Verify the selected backend is supported and properly configured</li>\n<li><strong>Credential Provider Validation</strong>: Ensure the credential provider can authenticate with required services</li>\n<li><strong>Permission Validation</strong>: Test that provided credentials have necessary permissions for upload operations</li>\n<li><strong>Network Validation</strong>: Verify connectivity to storage backend endpoints</li>\n<li><strong>Capacity Validation</strong>: Check storage quotas and available space where applicable</li>\n</ol>\n<p><strong>Credential Provider Interface</strong></p>\n<p>The credential provider abstraction enables different authentication backends while maintaining a consistent interface:</p>\n<table>\n<thead>\n<tr>\n<th>Provider Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>GetCredentials</code></td>\n<td><code>ctx Context, backend string</code></td>\n<td><code>Credentials, error</code></td>\n<td>Retrieves current valid credentials</td>\n</tr>\n<tr>\n<td><code>RefreshCredentials</code></td>\n<td><code>ctx Context, backend string</code></td>\n<td><code>Credentials, error</code></td>\n<td>Forces credential refresh from source</td>\n</tr>\n<tr>\n<td><code>SubscribeRotation</code></td>\n<td><code>ctx Context, backend string</code></td>\n<td><code>&lt;-chan Credentials</code></td>\n<td>Returns channel for credential rotation events</td>\n</tr>\n<tr>\n<td><code>ValidateAccess</code></td>\n<td><code>ctx Context, backend string, creds Credentials</code></td>\n<td><code>error</code></td>\n<td>Tests if credentials have required permissions</td>\n</tr>\n</tbody></table>\n<p>Different credential providers implement this interface to support various authentication sources:</p>\n<p><strong>File-Based Provider</strong>: Reads credentials from JSON or YAML configuration files with optional encryption. Suitable for development and simple deployments where credential rotation is handled externally.</p>\n<p><strong>Environment Provider</strong>: Sources credentials from environment variables following standard naming conventions (e.g., <code>AWS_ACCESS_KEY_ID</code>, <code>GCS_SERVICE_ACCOUNT_KEY</code>). Common in containerized deployments and CI/CD systems.</p>\n<p><strong>Vault Provider</strong>: Integrates with HashiCorp Vault to retrieve dynamic credentials with automatic rotation. Provides audit logging and centralized credential management for enterprise deployments.</p>\n<p><strong>Cloud Metadata Provider</strong>: Uses cloud platform metadata services (AWS IAM roles, GCP service accounts, Azure managed identities) for credential-less authentication in cloud deployments.</p>\n<p><strong>Credential Rotation and Refresh</strong></p>\n<p>Production deployments require automatic credential rotation to maintain security without service interruption. The credential management system implements proactive refresh strategies:</p>\n<table>\n<thead>\n<tr>\n<th>Rotation Trigger</th>\n<th>Detection Method</th>\n<th>Refresh Strategy</th>\n<th>Fallback Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Time-Based Expiration</td>\n<td>Check credential expiry timestamps</td>\n<td>Refresh at 80% of lifetime</td>\n<td>Use backup credentials during refresh</td>\n</tr>\n<tr>\n<td>Authentication Failure</td>\n<td>Detect auth errors from storage APIs</td>\n<td>Immediate refresh attempt</td>\n<td>Retry with refreshed credentials</td>\n</tr>\n<tr>\n<td>External Rotation Signal</td>\n<td>Webhook or message queue notification</td>\n<td>Scheduled refresh within 5 minutes</td>\n<td>Queue operations until refresh completes</td>\n</tr>\n<tr>\n<td>Periodic Validation</td>\n<td>Regular permission testing</td>\n<td>Refresh if validation fails</td>\n<td>Alert operations team</td>\n</tr>\n</tbody></table>\n<p>The refresh process follows these steps:</p>\n<ol>\n<li><strong>Background Refresh</strong>: Start credential refresh in background before expiration</li>\n<li><strong>Atomic Update</strong>: Replace cached credentials atomically to prevent partial updates</li>\n<li><strong>Connection Drain</strong>: Allow existing connections to complete with old credentials</li>\n<li><strong>New Connection Auth</strong>: Authenticate new connections with refreshed credentials</li>\n<li><strong>Cleanup</strong>: Securely clear old credentials from memory</li>\n</ol>\n<p><strong>Multi-Backend Configuration</strong></p>\n<p>Complex deployments may use multiple storage backends simultaneously (e.g., local storage for quarantine, S3 for primary storage, GCS for backup). The configuration system supports multiple backend definitions with different credential requirements:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>StorageBackends:\n  primary:\n    type: s3\n    config: {bucket: uploads-prod, region: us-west-2}\n    credentials: {provider: vault, path: secret/aws/s3}\n  quarantine:\n    type: local\n    config: {path: /var/quarantine}\n    credentials: {provider: none}\n  backup:\n    type: gcs\n    config: {bucket: uploads-backup}\n    credentials: {provider: environment}</code></pre></div>\n\n<p>The upload service can route operations to different backends based on upload metadata, file types, or security policies. For example, suspicious files detected during virus scanning might be routed to local quarantine storage while clean files go to the primary S3 backend.</p>\n<p><strong>Common Pitfalls</strong></p>\n<p>⚠️ <strong>Pitfall: Credential Caching Without Expiration</strong>\nMany implementations cache credentials indefinitely to avoid repeated authentication overhead, leading to authentication failures when credentials expire or rotate.</p>\n<p><strong>Why it&#39;s wrong</strong>: Cached credentials eventually expire, and services that don&#39;t handle expiration gracefully experience sudden authentication failures that can cause widespread upload failures.</p>\n<p><strong>How to fix</strong>: Always store credential expiration times and implement proactive refresh at 80% of credential lifetime. Include fallback logic that attempts credential refresh when authentication operations fail.</p>\n<p>⚠️ <strong>Pitfall: Embedding Credentials in Configuration Files</strong>\nDevelopment configurations often include hardcoded credentials that accidentally get committed to version control or deployed to production environments.</p>\n<p><strong>Why it&#39;s wrong</strong>: Credentials in configuration files can be exposed through version control history, configuration management systems, or file system access, creating significant security vulnerabilities.</p>\n<p><strong>How to fix</strong>: Use credential providers that source authentication information from secure stores (environment variables, vault systems, cloud metadata) rather than embedding credentials directly in configuration files. Implement configuration validation that rejects embedded credentials in production environments.</p>\n<p>⚠️ <strong>Pitfall: Single Point of Failure in Credential Management</strong>\nRelying on a single credential source (like a vault server) can cause complete service outage if that system becomes unavailable.</p>\n<p><strong>Why it&#39;s wrong</strong>: If the credential provider fails, the entire upload service becomes unable to authenticate with storage backends, causing all uploads to fail even if the storage systems themselves are healthy.</p>\n<p><strong>How to fix</strong>: Implement credential provider fallback chains with multiple sources. Cache valid credentials locally with reasonable expiration times to survive short credential provider outages. Include monitoring and alerting for credential provider health.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p><strong>Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>S3 Client</td>\n<td>AWS SDK v2 (github.com/aws/aws-sdk-go-v2)</td>\n<td>Custom HTTP client with connection pooling</td>\n</tr>\n<tr>\n<td>Local File I/O</td>\n<td>Standard os package with ioutil helpers</td>\n<td>Memory-mapped files for large uploads</td>\n</tr>\n<tr>\n<td>Credential Storage</td>\n<td>Environment variables + file config</td>\n<td>HashiCorp Vault integration</td>\n</tr>\n<tr>\n<td>Configuration Management</td>\n<td>JSON files with validation</td>\n<td>Viper with multiple source support</td>\n</tr>\n<tr>\n<td>Error Retry Logic</td>\n<td>Simple exponential backoff</td>\n<td>github.com/cenkalti/backoff with jitter</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/storage/\n├── storage.go              ← Storage interface definitions\n├── errors.go               ← Storage-specific error types\n├── config.go               ← Configuration structures and validation\n├── credentials/            ← Credential provider implementations\n│   ├── provider.go         ← Credential provider interface\n│   ├── env_provider.go     ← Environment variable provider\n│   ├── file_provider.go    ← File-based credential provider\n│   └── vault_provider.go   ← HashiCorp Vault provider (advanced)\n├── backends/               ← Storage backend implementations\n│   ├── local/              ← Local filesystem backend\n│   │   ├── local.go        ← Main implementation\n│   │   ├── local_test.go   ← Unit tests\n│   │   └── multipart.go    ← Multipart upload simulation\n│   ├── s3/                 ← S3-compatible backend\n│   │   ├── s3.go           ← Main implementation\n│   │   ├── s3_test.go      ← Unit tests\n│   │   ├── multipart.go    ← S3 multipart upload integration\n│   │   └── credentials.go  ← S3-specific credential handling\n│   └── memory/             ← In-memory backend for testing\n│       ├── memory.go       ← Test implementation\n│       └── memory_test.go  ← Unit tests\n└── factory.go              ← Backend factory and registration</code></pre></div>\n\n<p><strong>Infrastructure Starter Code</strong></p>\n<p>Here&#39;s complete starter code for the storage interface and basic error handling:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/storage/storage.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> storage</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">io</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StorageBackend defines the interface that all storage implementations must satisfy.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// It provides both simple chunk operations and multipart upload capabilities.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StorageBackend</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Simple chunk operations for small uploads</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    StoreChunk</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">data</span><span style=\"color:#B392F0\"> io</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Reader</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">size</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    ReadChunk</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">io</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ReadCloser</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    DeleteChunk</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    ChunkExists</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Multipart upload operations for large files</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    InitMultipart</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">metadata</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MultipartUpload</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    StoreMultipartChunk</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">upload</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">MultipartUpload</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">partNum</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">data</span><span style=\"color:#B392F0\"> io</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Reader</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">size</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MultipartPart</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    CompleteMultipart</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">upload</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">MultipartUpload</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">parts</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MultipartPart</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    AbortMultipart</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">upload</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">MultipartUpload</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Additional operations</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    GenerateSignedURL</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">expiration</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    ListKeys</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">prefix</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">limit</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MultipartUpload represents an ongoing multipart upload session</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MultipartUpload</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ID       </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Key      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"key\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Backend  </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"backend\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Metadata </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"metadata\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CreatedAt </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">        `json:\"created_at\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MultipartPart represents one part of a multipart upload</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MultipartPart</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    PartNumber </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">    `json:\"part_number\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ETag       </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"etag\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Size       </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">  `json:\"size\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    PartID     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"part_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// BackendFactory creates storage backend instances based on configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> BackendFactory</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    CreateBackend</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#B392F0\"> StorageConfig</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">StorageBackend</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    SupportedBackends</span><span style=\"color:#E1E4E8\">() []</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/storage/errors.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> storage</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#B392F0\">errors</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">var</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrNotFound </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"key not found\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrAlreadyExists </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"key already exists\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrInsufficientSpace </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"insufficient storage space\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrNetworkTimeout </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"network operation timeout\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrAuthenticationFailed </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"authentication failed\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrInvalidMultipartState </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"invalid multipart upload state\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StorageError wraps storage operation errors with additional context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StorageError</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Operation </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Key       </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Backend   </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Err       </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageError</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"storage </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\"> failed for key </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\"> on backend </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        e.Operation, e.Key, e.Backend, e.Err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageError</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Unwrap</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> e.Err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/storage/credentials/provider.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> credentials</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Credentials represents authentication information for storage backends</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Credentials</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AccessKeyID     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"access_key_id,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SecretAccessKey </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"secret_access_key,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SessionToken    </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"session_token,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ExpiresAt       </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"expires_at,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Region          </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"region,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// IsExpired returns true if credentials have expired</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Credentials</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">IsExpired</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.ExpiresAt.</span><span style=\"color:#B392F0\">IsZero</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> false</span><span style=\"color:#6A737D\"> // No expiration set</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">After</span><span style=\"color:#E1E4E8\">(c.ExpiresAt)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CredentialProvider defines the interface for credential sources</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CredentialProvider</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    GetCredentials</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">backend</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Credentials</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    RefreshCredentials</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">backend</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Credentials</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Core Logic Skeleton Code</strong></p>\n<p>Here&#39;s the skeleton for the local storage backend that learners should implement:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/storage/backends/local/local.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> local</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">io</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">path/filepath</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">your-project/internal/storage</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LocalBackend implements StorageBackend using local filesystem</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LocalBackend</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rootPath </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex    </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewLocalBackend creates a new local filesystem storage backend</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewLocalBackend</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">rootPath</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LocalBackend</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate rootPath exists and is writable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create subdirectories (chunks/, multipart/, completed/) if they don't exist</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Set proper directory permissions (0755)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return configured LocalBackend instance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StoreChunk stores chunk data at the specified key</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LocalBackend</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">StoreChunk</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">data</span><span style=\"color:#B392F0\"> io</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Reader</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">size</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Sanitize key to prevent directory traversal attacks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Generate safe filename using hash of key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Create temporary file in chunks/ directory</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Copy data from reader to temporary file with size validation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Sync file to disk for durability</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Atomically rename temporary file to final name</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Clean up temporary file on any error</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use filepath.Join and validate result stays within rootPath</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// InitMultipart begins a multipart upload session</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LocalBackend</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">InitMultipart</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">metadata</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">MultipartUpload</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Generate unique upload ID (UUID)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create directory structure: multipart/{uploadID}/parts/</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Write metadata.json file with upload information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return MultipartUpload struct with generated ID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use os.MkdirAll for directory creation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StoreMultipartChunk stores one part of a multipart upload</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LocalBackend</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">StoreMultipartChunk</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">upload</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">MultipartUpload</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">partNum</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">data</span><span style=\"color:#B392F0\"> io</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Reader</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">size</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">MultipartPart</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate upload exists and partNum is valid (1-10000)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create part file path: multipart/{uploadID}/parts/part-{partNum:03d}</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Store part data using atomic write (temp + rename)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Compute ETag (MD5 hash) of part data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return MultipartPart with part metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Pad part numbers with zeros for consistent sorting</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CompleteMultipart assembles all parts into final file</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LocalBackend</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CompleteMultipart</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">upload</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">MultipartUpload</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">parts</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">MultipartPart</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate all parts are present and in sequence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create temporary assembly file in multipart/{uploadID}/assembly/</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Stream-copy each part file to assembly file in order</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Verify final file size matches sum of part sizes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Atomically move assembled file to completed/{key}</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Clean up multipart directory and part files</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use io.Copy with limited reader to avoid loading entire file in memory</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Language-Specific Hints</strong></p>\n<p><strong>Go-Specific Implementation Tips:</strong></p>\n<ul>\n<li>Use <code>os.OpenFile</code> with <code>O_CREATE|O_EXCL</code> flags for atomic file creation</li>\n<li>Call <code>file.Sync()</code> after writing critical data to ensure durability</li>\n<li>Use <code>filepath.Clean</code> and <code>filepath.IsAbs</code> to validate file paths</li>\n<li>Implement proper cleanup with <code>defer</code> statements for file handles</li>\n<li>Use <code>io.LimitReader</code> when copying data with known size limits</li>\n<li>Handle context cancellation by checking <code>ctx.Done()</code> in long operations</li>\n</ul>\n<p><strong>Error Handling Patterns:</strong></p>\n<ul>\n<li>Wrap filesystem errors with <code>storage.StorageError</code> for consistent error handling</li>\n<li>Use <code>os.IsNotExist(err)</code> to detect missing files reliably</li>\n<li>Check disk space with <code>syscall.Statfs()</code> before large write operations</li>\n<li>Implement exponential backoff for temporary filesystem errors</li>\n</ul>\n<p><strong>Milestone Checkpoint</strong></p>\n<p>After implementing the storage abstraction:</p>\n<p><strong>Test Commands:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run storage backend tests</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/storage/...</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Run integration tests with different backends</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">STORAGE_BACKEND</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">local</span><span style=\"color:#B392F0\"> go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/storage/backends/local/...</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">STORAGE_BACKEND</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">s3</span><span style=\"color:#E1E4E8\"> S3_ENDPOINT</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">http://localhost:9000</span><span style=\"color:#B392F0\"> go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/storage/backends/s3/...</span></span></code></pre></div>\n\n<p><strong>Expected Behavior:</strong></p>\n<ol>\n<li><strong>Local Backend</strong>: Should create directory structure under configured root path, store and retrieve chunks correctly, handle multipart uploads with proper part assembly</li>\n<li><strong>S3 Backend</strong>: Should authenticate with configured credentials, create multipart uploads with valid ETags, handle S3-specific errors gracefully</li>\n<li><strong>Configuration</strong>: Should load from files, validate required parameters, fail clearly when credentials are missing or invalid</li>\n</ol>\n<p><strong>Manual Verification:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Start test server with local backend</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">./upload-service</span><span style=\"color:#79B8FF\"> --config</span><span style=\"color:#9ECBFF\"> configs/local.json</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Upload small file (single chunk)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/uploads</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -H</span><span style=\"color:#9ECBFF\"> \"Upload-Length: 1024\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -H</span><span style=\"color:#9ECBFF\"> \"Content-Type: application/octet-stream\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --data-binary</span><span style=\"color:#9ECBFF\"> @testfile.bin</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Verify file appears in storage directory</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">ls</span><span style=\"color:#79B8FF\"> -la</span><span style=\"color:#9ECBFF\"> storage/completed/</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Upload large file (multipart)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/uploads</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -H</span><span style=\"color:#9ECBFF\"> \"Upload-Length: 10485760\"</span><span style=\"color:#79B8FF\"> \\ </span><span style=\"color:#6A737D\"> # 10MB</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">  -H</span><span style=\"color:#9ECBFF\"> \"Content-Type: application/octet-stream\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Note upload ID from response, use for subsequent chunk uploads</span></span></code></pre></div>\n\n<p><strong>Common Issues and Debugging:</strong></p>\n<ul>\n<li><strong>Permission Errors</strong>: Check that storage directories have correct ownership and permissions (755 for directories, 644 for files)</li>\n<li><strong>Path Traversal Failures</strong>: Verify key sanitization rejects <code>../</code> sequences and produces consistent hash-based filenames</li>\n<li><strong>S3 Authentication Failures</strong>: Validate credentials with AWS CLI: <code>aws s3 ls s3://your-bucket --profile your-profile</code></li>\n<li><strong>Multipart Assembly Errors</strong>: Check that part files are created in correct order and assembly process reads them sequentially</li>\n</ul>\n<h2 id=\"file-validation-and-security\">File Validation and Security</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 3 (Virus Scanning &amp; Validation) — this section implements comprehensive file validation with type checking, virus scanning, and quarantine mechanisms</p>\n</blockquote>\n<p>Think of file validation as an <strong>airport security checkpoint</strong> for your file uploads. Just as airport security doesn&#39;t trust the boarding pass alone (could be forged) but instead scans your ID, X-rays your luggage, and runs security checks, our file validation system never trusts what the client claims about a file. Instead, it examines the actual file content, runs it through security scanners, and quarantines anything suspicious. The key insight is that <strong>security validation must happen before files enter your trusted storage</strong> — just like how airport security happens before you board the plane, not after you&#39;re already in the air.</p>\n<p>The validation pipeline operates as a <strong>multi-stage filter</strong>, where each stage can reject the file and prevent it from reaching production storage. Files progress through type validation (checking the actual content signature), size enforcement (preventing resource exhaustion attacks), virus scanning (detecting malicious content), and finally either acceptance into clean storage or quarantine for suspicious content. This layered approach ensures that even if one validation stage has a bypass, other layers provide defense in depth.</p>\n<p><img src=\"/api/project/file-upload-service/architecture-doc/asset?path=diagrams%2Fvirus-scanning-flow.svg\" alt=\"File Validation and Scanning Pipeline\"></p>\n<p>The architectural challenge is balancing <strong>security thoroughness with performance</strong>. Scanning every uploaded file for viruses takes time and resources, but skipping scans creates security vulnerabilities. Our solution uses asynchronous processing where files are initially stored in a temporary holding area, scanned in the background, and then either promoted to clean storage or moved to quarantine. This allows clients to complete their uploads quickly while ensuring no unscanned files reach production systems.</p>\n<h3 id=\"file-type-validation\">File Type Validation</h3>\n<p>File type validation functions as the <strong>first line of defense</strong> against malicious uploads disguised as innocent file types. Think of it as a <strong>document inspector</strong> who doesn&#39;t trust the label on a package but instead opens it up and examines the actual contents. A malicious executable might be renamed from <code>virus.exe</code> to <code>document.pdf</code>, but the file&#39;s internal structure still contains the executable&#39;s signature bytes that reveal its true nature.</p>\n<p>The validation process relies on <strong>magic number detection</strong> — specific byte patterns that appear at the beginning of files and uniquely identify the file format. For example, PNG files always start with the bytes <code>89 50 4E 47 0D 0A 1A 0A</code>, JPEG files begin with <code>FF D8 FF</code>, and PDF files start with <code>%PDF-</code>. These signatures are embedded by the programs that create the files and cannot be easily forged without corrupting the file&#39;s readability.</p>\n<blockquote>\n<p><strong>Decision: Content-Based Type Detection Over Extension-Based</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to validate file types to enforce security policies and prevent malicious uploads disguised as safe file types</li>\n<li><strong>Options Considered</strong>: Trust file extensions, HTTP Content-Type headers, or examine actual file content using magic bytes</li>\n<li><strong>Decision</strong>: Implement magic byte detection with fallback to Content-Type headers for validation</li>\n<li><strong>Rationale</strong>: File extensions and Content-Type headers can be trivially spoofed by attackers, while magic bytes reflect the actual file structure and are much harder to forge without corrupting the file</li>\n<li><strong>Consequences</strong>: Requires maintaining a database of magic number patterns and file signatures, adds computational overhead to read file headers, but provides strong protection against type confusion attacks</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Validation Method</th>\n<th>Reliability</th>\n<th>Performance</th>\n<th>Security</th>\n<th>Implementation Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>File Extension</td>\n<td>Very Low</td>\n<td>Excellent</td>\n<td>Poor</td>\n<td>Minimal</td>\n</tr>\n<tr>\n<td>Content-Type Header</td>\n<td>Low</td>\n<td>Excellent</td>\n<td>Poor</td>\n<td>Minimal</td>\n</tr>\n<tr>\n<td>Magic Byte Detection</td>\n<td>High</td>\n<td>Good</td>\n<td>Strong</td>\n<td>Moderate</td>\n</tr>\n<tr>\n<td>Deep Content Analysis</td>\n<td>Very High</td>\n<td>Poor</td>\n<td>Excellent</td>\n<td>Complex</td>\n</tr>\n</tbody></table>\n<p>Our validation system maintains a <strong>file signature registry</strong> that maps magic byte patterns to MIME types and acceptable file categories. When a file upload completes, the validator reads the first 512 bytes of the file (enough to capture most file signatures) and matches them against the known patterns. If multiple signatures could match, the system uses the longest matching pattern to avoid false positives.</p>\n<p>The validation workflow follows this algorithm:</p>\n<ol>\n<li><strong>Read File Header</strong>: Extract the first 512 bytes from the uploaded file&#39;s beginning</li>\n<li><strong>Pattern Matching</strong>: Compare the header bytes against the registered magic number database</li>\n<li><strong>Type Determination</strong>: Identify the most specific matching file type based on byte patterns</li>\n<li><strong>Policy Check</strong>: Verify that the detected file type appears in the server&#39;s allowed content types configuration</li>\n<li><strong>Fallback Validation</strong>: If magic bytes are inconclusive, check the HTTP Content-Type header as secondary validation</li>\n<li><strong>Decision</strong>: Accept the file if type is allowed, reject if forbidden, or flag for manual review if uncertain</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>File Signature Component</th>\n<th>Purpose</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Magic Bytes</td>\n<td>Unique identifier at file start</td>\n<td><code>89 50 4E 47</code> for PNG</td>\n</tr>\n<tr>\n<td>Offset</td>\n<td>Position where signature appears</td>\n<td>Usually 0, sometimes offset</td>\n</tr>\n<tr>\n<td>Length</td>\n<td>How many bytes to examine</td>\n<td>4-16 bytes typically</td>\n</tr>\n<tr>\n<td>MIME Type</td>\n<td>Standardized content type</td>\n<td><code>image/png</code></td>\n</tr>\n<tr>\n<td>File Extensions</td>\n<td>Common filename suffixes</td>\n<td><code>.png</code>, <code>.jpg</code>, <code>.pdf</code></td>\n</tr>\n</tbody></table>\n<p>The signature database includes entries for all commonly uploaded file types:</p>\n<table>\n<thead>\n<tr>\n<th>File Type</th>\n<th>Magic Bytes (Hex)</th>\n<th>MIME Type</th>\n<th>Security Risk</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>PNG Image</td>\n<td><code>89 50 4E 47 0D 0A 1A 0A</code></td>\n<td><code>image/png</code></td>\n<td>Low</td>\n</tr>\n<tr>\n<td>JPEG Image</td>\n<td><code>FF D8 FF</code></td>\n<td><code>image/jpeg</code></td>\n<td>Low</td>\n</tr>\n<tr>\n<td>PDF Document</td>\n<td><code>25 50 44 46 2D</code> (<code>%PDF-</code>)</td>\n<td><code>application/pdf</code></td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>ZIP Archive</td>\n<td><code>50 4B 03 04</code> or <code>50 4B 05 06</code></td>\n<td><code>application/zip</code></td>\n<td>High</td>\n</tr>\n<tr>\n<td>Windows Executable</td>\n<td><code>4D 5A</code> (<code>MZ</code>)</td>\n<td><code>application/x-msdownload</code></td>\n<td>Critical</td>\n</tr>\n<tr>\n<td>ELF Executable</td>\n<td><code>7F 45 4C 46</code></td>\n<td><code>application/x-executable</code></td>\n<td>Critical</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p>The critical insight is that <strong>file content never lies</strong>, while metadata always can. Magic bytes are written by the programs that create files and reflect the actual internal structure, making them extremely difficult to forge without breaking the file&#39;s functionality.</p>\n</blockquote>\n<p><strong>Common Pitfalls in File Type Validation:</strong></p>\n<p>⚠️ <strong>Pitfall: Trusting Only File Extensions</strong>\nChecking only the filename extension (<code>document.pdf</code>) allows trivial bypasses where attackers rename malicious files. An executable renamed from <code>virus.exe</code> to <code>document.pdf</code> will pass extension-based validation but still execute maliciously when opened. Always examine the actual file content using magic byte detection.</p>\n<p>⚠️ <strong>Pitfall: Insufficient Magic Byte Coverage</strong>\nReading only the first few bytes might miss file formats where the signature appears at an offset. Some file formats embed signatures after headers or metadata. Maintain a comprehensive signature database that includes offset information and read sufficient header data (512+ bytes) to capture most signatures.</p>\n<p>⚠️ <strong>Pitfall: Blocking Legitimate Compound Documents</strong>\nModern document formats like DOCX and XLSX are actually ZIP archives containing XML files. Blanket blocking of ZIP files will break legitimate document uploads. Implement hierarchical validation that can examine container formats and validate their internal structure.</p>\n<h3 id=\"virus-scanning-integration\">Virus Scanning Integration</h3>\n<p>Virus scanning acts as the <strong>security checkpoint</strong> in our upload pipeline, similar to how airport baggage scanners examine every item for dangerous materials. The scanner doesn&#39;t care what the luggage tag says — it examines the actual contents using signature databases and behavioral analysis to detect threats. Our virus scanning integration with ClamAV provides this same thorough examination of uploaded file content.</p>\n<p>The scanning architecture uses an <strong>asynchronous processing model</strong> to avoid blocking client uploads while ensuring comprehensive security coverage. Think of it as a <strong>two-stage security process</strong>: files first enter a holding area where clients can complete their uploads quickly, then move through background security screening before reaching the clean storage area. This prevents both performance bottlenecks and security gaps.</p>\n<blockquote>\n<p><strong>Decision: Asynchronous Scanning with Temporary Storage</strong></p>\n<ul>\n<li><strong>Context</strong>: Virus scanning large files can take several minutes, but clients expect upload confirmations quickly to maintain good user experience</li>\n<li><strong>Options Considered</strong>: Synchronous scanning during upload, asynchronous background scanning after upload completion, or streaming scan during chunk assembly</li>\n<li><strong>Decision</strong>: Store completed uploads in temporary storage, scan asynchronously, then promote to clean storage or move to quarantine</li>\n<li><strong>Rationale</strong>: Balances security (all files are scanned) with performance (clients get quick upload confirmations), and allows retrying failed scans without re-uploading</li>\n<li><strong>Consequences</strong>: Requires additional temporary storage space and cleanup processes, adds complexity to track scanning status, but provides good user experience without security compromise</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Scanning Approach</th>\n<th>Client Experience</th>\n<th>Security Coverage</th>\n<th>Resource Usage</th>\n<th>Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Synchronous During Upload</td>\n<td>Poor (long waits)</td>\n<td>Complete</td>\n<td>High (blocking)</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Skip Scanning</td>\n<td>Excellent</td>\n<td>None</td>\n<td>Minimal</td>\n<td>Minimal</td>\n</tr>\n<tr>\n<td>Asynchronous Post-Upload</td>\n<td>Good (quick confirmation)</td>\n<td>Complete</td>\n<td>Moderate</td>\n<td>Moderate</td>\n</tr>\n<tr>\n<td>Stream Scanning</td>\n<td>Good</td>\n<td>Complete</td>\n<td>High (concurrent)</td>\n<td>High</td>\n</tr>\n</tbody></table>\n<p>The virus scanning workflow integrates with our upload completion process through these stages:</p>\n<ol>\n<li><strong>Upload Completion</strong>: When all chunks are assembled, the file moves to temporary storage with status <code>completing</code></li>\n<li><strong>Scan Queue</strong>: The file gets queued for virus scanning with metadata about its location and session</li>\n<li><strong>ClamAV Integration</strong>: The scanner daemon processes files using Unix domain sockets for security isolation</li>\n<li><strong>Result Processing</strong>: Clean files move to permanent storage, infected files go to quarantine, scan failures trigger retries</li>\n<li><strong>Status Updates</strong>: The upload session status updates to <code>completed</code> (clean), <code>quarantined</code> (infected), or <code>failed</code> (scan error)</li>\n<li><strong>Client Notification</strong>: Clients can poll the session status to learn the final outcome</li>\n</ol>\n<p>The ClamAV integration uses <strong>Unix domain sockets</strong> rather than network connections for security isolation. This prevents the virus scanner from having network access that could be exploited if a malicious file compromises the scanning process. The socket communication follows the ClamAV protocol for streaming file data and receiving scan results.</p>\n<table>\n<thead>\n<tr>\n<th>Scanning Component</th>\n<th>Responsibility</th>\n<th>Configuration</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ClamAV Daemon</td>\n<td>Signature-based malware detection</td>\n<td>Socket path, signature database updates</td>\n</tr>\n<tr>\n<td>Scan Queue</td>\n<td>Async job management</td>\n<td>Concurrency limits, retry policies</td>\n</tr>\n<tr>\n<td>Result Processor</td>\n<td>Handle scan outcomes</td>\n<td>Quarantine location, notification rules</td>\n</tr>\n<tr>\n<td>Status Tracker</td>\n<td>Update session states</td>\n<td>Database connection, cleanup schedules</td>\n</tr>\n</tbody></table>\n<p>The scan result handling implements <strong>defense in depth</strong> with multiple layers of protection:</p>\n<table>\n<thead>\n<tr>\n<th>Scan Result</th>\n<th>Action Taken</th>\n<th>Storage Location</th>\n<th>Client Notification</th>\n<th>Retention Policy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Clean</td>\n<td>Promote to permanent storage</td>\n<td>Production backend</td>\n<td>Success notification</td>\n<td>Normal retention</td>\n</tr>\n<tr>\n<td>Infected</td>\n<td>Move to quarantine</td>\n<td>Isolated quarantine storage</td>\n<td>Security alert</td>\n<td>Extended retention for analysis</td>\n</tr>\n<tr>\n<td>Scan Failed</td>\n<td>Retry scan (up to 3 times)</td>\n<td>Temporary storage</td>\n<td>Retry notification</td>\n<td>Short retention, then manual review</td>\n</tr>\n<tr>\n<td>Timeout</td>\n<td>Kill scan, retry with smaller chunks</td>\n<td>Temporary storage</td>\n<td>Timeout notification</td>\n<td>Manual review required</td>\n</tr>\n</tbody></table>\n<p>The virus signature database requires <strong>regular updates</strong> to detect new threats. The system schedules automatic signature updates from ClamAV&#39;s threat intelligence feeds, but also provides manual update capabilities for urgent threat responses. After signature updates, the system can optionally rescan recently uploaded files to catch threats that were unknown at their original upload time.</p>\n<blockquote>\n<p>The key insight is that <strong>virus scanning must happen in isolation</strong> — the scanner process should have minimal privileges and no network access, so that if a malicious file compromises the scanning process, the damage is contained to the scanner itself rather than spreading to the broader system.</p>\n</blockquote>\n<p><strong>Integration with Upload Sessions:</strong></p>\n<p>The scanning process extends our upload session state machine to include scanning-related states. The session transitions from <code>completing</code> (assembly finished) to <code>scanning</code> (virus scan in progress) to either <code>completed</code> (clean file) or <code>quarantined</code> (infected file).</p>\n<table>\n<thead>\n<tr>\n<th>Session State</th>\n<th>Description</th>\n<th>Next Possible States</th>\n<th>Client Actions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>completing</code></td>\n<td>Chunks assembled, starting validation</td>\n<td><code>scanning</code>, <code>failed</code></td>\n<td>Wait for scan completion</td>\n</tr>\n<tr>\n<td><code>scanning</code></td>\n<td>Virus scan in progress</td>\n<td><code>completed</code>, <code>quarantined</code>, <code>failed</code></td>\n<td>Poll for status updates</td>\n</tr>\n<tr>\n<td><code>completed</code></td>\n<td>File passed all validation</td>\n<td>None (terminal)</td>\n<td>Access file via signed URLs</td>\n</tr>\n<tr>\n<td><code>quarantined</code></td>\n<td>File failed virus scan</td>\n<td>None (terminal)</td>\n<td>Receive security alert</td>\n</tr>\n</tbody></table>\n<p><strong>Common Pitfalls in Virus Scanning:</strong></p>\n<p>⚠️ <strong>Pitfall: Scanning After Files Reach Production</strong>\nIf files reach production storage before virus scanning completes, there&#39;s a window where infected files are accessible to users. Always scan in a temporary holding area and only promote clean files to production storage. This prevents the &quot;scan after compromise&quot; scenario.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Scan Timeouts</strong>\nLarge files can cause ClamAV to timeout or consume excessive memory. Implement timeout handling that kills hung scans and retries with smaller chunk sizes. Set memory limits on the scanning process to prevent resource exhaustion attacks through maliciously crafted files.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Scanner Isolation</strong>\nRunning the virus scanner with full system privileges means that if a malicious file compromises ClamAV, the attacker gains broad system access. Run scanning processes with minimal privileges, use Unix domain sockets instead of network connections, and consider containerization for additional isolation.</p>\n<h3 id=\"quarantine-and-policy-enforcement\">Quarantine and Policy Enforcement</h3>\n<p>File quarantine operates as a <strong>secure evidence locker</strong> for suspicious content, similar to how law enforcement isolates potential evidence in tamper-proof storage while investigations proceed. The quarantine system ensures that infected or suspicious files are completely isolated from production systems while preserving them for security analysis and forensic investigation.</p>\n<p>The quarantine architecture implements <strong>strict isolation</strong> with separate storage credentials, network access controls, and administrative permissions. Think of it as a <strong>high-security containment facility</strong> where nothing enters or leaves without explicit authorization from security administrators. This prevents both accidental access to dangerous files and data exfiltration of quarantined content.</p>\n<blockquote>\n<p><strong>Decision: Separate Quarantine Storage with Forensic Retention</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to isolate infected files from production systems while preserving them for security analysis and forensic investigation</li>\n<li><strong>Options Considered</strong>: Delete infected files immediately, store in same backend with access controls, or use completely separate quarantine storage</li>\n<li><strong>Decision</strong>: Implement dedicated quarantine storage with separate credentials and extended retention policies</li>\n<li><strong>Rationale</strong>: Complete isolation prevents any possibility of infected files reaching production, while preservation enables security teams to analyze attack patterns and improve detection</li>\n<li><strong>Consequences</strong>: Requires additional storage infrastructure and administrative overhead, but provides strong security isolation and valuable threat intelligence</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Quarantine Approach</th>\n<th>Security Isolation</th>\n<th>Forensic Value</th>\n<th>Operational Overhead</th>\n<th>Storage Cost</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Immediate Deletion</td>\n<td>Complete</td>\n<td>None</td>\n<td>Minimal</td>\n<td>None</td>\n</tr>\n<tr>\n<td>Flagged in Production</td>\n<td>Weak</td>\n<td>Good</td>\n<td>Low</td>\n<td>Standard</td>\n</tr>\n<tr>\n<td>Separate Storage</td>\n<td>Strong</td>\n<td>Excellent</td>\n<td>Moderate</td>\n<td>Additional</td>\n</tr>\n<tr>\n<td>Air-Gapped Archive</td>\n<td>Maximum</td>\n<td>Excellent</td>\n<td>High</td>\n<td>Additional</td>\n</tr>\n</tbody></table>\n<p>The quarantine system maintains detailed <strong>forensic metadata</strong> for every quarantined file to support security investigations and threat analysis. This metadata includes not only the file content but also the complete context of how the file arrived in the system, what triggered the quarantine decision, and the technical details of the security violation.</p>\n<table>\n<thead>\n<tr>\n<th>Quarantine Metadata</th>\n<th>Purpose</th>\n<th>Example Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Original Filename</td>\n<td>Track attack patterns</td>\n<td><code>invoice.pdf.exe</code></td>\n</tr>\n<tr>\n<td>Upload Session ID</td>\n<td>Link to session logs</td>\n<td><code>sess_a1b2c3d4</code></td>\n</tr>\n<tr>\n<td>Client IP Address</td>\n<td>Geographic analysis</td>\n<td><code>192.168.1.100</code></td>\n</tr>\n<tr>\n<td>Detection Method</td>\n<td>Understand detection trigger</td>\n<td><code>ClamAV signature: Win.Trojan.Agent</code></td>\n</tr>\n<tr>\n<td>Upload Timestamp</td>\n<td>Timeline reconstruction</td>\n<td><code>2023-12-15T14:30:22Z</code></td>\n</tr>\n<tr>\n<td>File Size</td>\n<td>Resource impact analysis</td>\n<td><code>2048576</code> bytes</td>\n</tr>\n<tr>\n<td>Content Hash</td>\n<td>Deduplication and tracking</td>\n<td><code>sha256:abc123...</code></td>\n</tr>\n<tr>\n<td>Quarantine Reason</td>\n<td>Policy violation details</td>\n<td><code>Virus detected: Trojan.Generic</code></td>\n</tr>\n</tbody></table>\n<p>The policy enforcement system implements <strong>configurable security rules</strong> that can be updated without code changes to respond to evolving threats. The policies define what file types are allowed, size limits, scanning requirements, and quarantine criteria. This flexibility allows security teams to tighten restrictions during active attacks or relax them for trusted users.</p>\n<p>Policy enforcement follows this decision tree:</p>\n<ol>\n<li><strong>Size Policy Check</strong>: Verify file size is within configured limits for the file type</li>\n<li><strong>Content Type Policy</strong>: Ensure detected file type is in the allowed content types list</li>\n<li><strong>Virus Scan Policy</strong>: Check if virus scanning detected any threats or suspicious patterns</li>\n<li><strong>Custom Rule Evaluation</strong>: Apply any additional organizational policies (e.g., executable file blocking)</li>\n<li><strong>Risk Scoring</strong>: Calculate an overall risk score based on multiple factors</li>\n<li><strong>Decision</strong>: Allow to production storage, quarantine for review, or reject completely</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Policy Type</th>\n<th>Configuration</th>\n<th>Example Rule</th>\n<th>Enforcement Point</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Size Limits</td>\n<td>Per-file-type maximums</td>\n<td><code>image/jpeg: 10MB, application/pdf: 50MB</code></td>\n<td>During upload</td>\n</tr>\n<tr>\n<td>Content Type Allowlist</td>\n<td>MIME type permissions</td>\n<td><code>Allow: image/*, application/pdf</code></td>\n<td>After type detection</td>\n</tr>\n<tr>\n<td>Virus Scanning</td>\n<td>Scan requirements</td>\n<td><code>Mandatory for all uploads &gt; 1MB</code></td>\n<td>Post-assembly</td>\n</tr>\n<tr>\n<td>Executable Blocking</td>\n<td>Binary file restrictions</td>\n<td><code>Block: application/x-executable</code></td>\n<td>Type-based</td>\n</tr>\n<tr>\n<td>Rate Limiting</td>\n<td>Upload frequency limits</td>\n<td><code>Max 100 files/hour per IP</code></td>\n<td>Per request</td>\n</tr>\n</tbody></table>\n<p><strong>Quarantine Storage Architecture:</strong></p>\n<p>The quarantine storage system uses <strong>separate infrastructure</strong> with dedicated credentials and access controls to ensure complete isolation from production systems. Even if production storage credentials are compromised, quarantine data remains protected under different authentication mechanisms.</p>\n<table>\n<thead>\n<tr>\n<th>Storage Component</th>\n<th>Production</th>\n<th>Quarantine</th>\n<th>Isolation Benefit</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Authentication</td>\n<td>Primary service credentials</td>\n<td>Separate quarantine-only credentials</td>\n<td>Credential compromise isolation</td>\n</tr>\n<tr>\n<td>Network Access</td>\n<td>Standard application network</td>\n<td>Restricted admin-only network</td>\n<td>Network attack isolation</td>\n</tr>\n<tr>\n<td>Backup Systems</td>\n<td>Standard automated backups</td>\n<td>Separate forensic backup system</td>\n<td>Data recovery isolation</td>\n</tr>\n<tr>\n<td>Access Logging</td>\n<td>Application access logs</td>\n<td>Security audit logs</td>\n<td>Investigation support</td>\n</tr>\n</tbody></table>\n<p><strong>Retention and Lifecycle Management:</strong></p>\n<p>Quarantine files follow <strong>extended retention policies</strong> to support ongoing security investigations and threat intelligence gathering. The retention system automatically manages the lifecycle of quarantined content while ensuring important evidence is preserved for the required timeframes.</p>\n<table>\n<thead>\n<tr>\n<th>File Category</th>\n<th>Retention Period</th>\n<th>Automatic Actions</th>\n<th>Manual Review Required</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Virus-Infected</td>\n<td>1 year</td>\n<td>Delete after retention expires</td>\n<td>Security team notification</td>\n</tr>\n<tr>\n<td>Suspicious (unconfirmed)</td>\n<td>90 days</td>\n<td>Delete or promote after manual review</td>\n<td>Yes, within 30 days</td>\n</tr>\n<tr>\n<td>Policy Violations</td>\n<td>30 days</td>\n<td>Delete after log analysis</td>\n<td>No, unless flagged</td>\n</tr>\n<tr>\n<td>False Positives</td>\n<td>7 days</td>\n<td>Delete after confirmation</td>\n<td>Yes, for policy tuning</td>\n</tr>\n</tbody></table>\n<p>The quarantine system generates <strong>security alerts</strong> for different types of threats to ensure appropriate response from security teams. These alerts include sufficient context for security analysts to understand the threat and take appropriate action.</p>\n<blockquote>\n<p>Critical insight: <strong>Quarantine is not just isolation — it&#39;s active threat intelligence collection</strong>. Every quarantined file provides data about attack patterns, helping improve detection rules and security policies for future uploads.</p>\n</blockquote>\n<p><strong>Common Pitfalls in Quarantine Management:</strong></p>\n<p>⚠️ <strong>Pitfall: Insufficient Quarantine Isolation</strong>\nStoring quarantined files in the same storage system as clean files, even with access controls, creates risk of cross-contamination or accidental access. Use completely separate storage infrastructure with different credentials to ensure true isolation.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Forensic Metadata</strong>\nQuarantining files without preserving the context of how they arrived (IP address, user agent, upload session details) makes security investigations much harder. Capture complete metadata at quarantine time, as this information may not be available later.</p>\n<p>⚠️ <strong>Pitfall: Automatic Deletion of Evidence</strong>\nImmediately deleting infected files destroys valuable threat intelligence that could help improve security. Implement retention policies that preserve quarantined files long enough for security analysis while managing storage costs.</p>\n<h3 id=\"size-limits-and-resource-protection\">Size Limits and Resource Protection</h3>\n<p>Resource protection functions as the <strong>traffic control system</strong> for file uploads, preventing both accidental resource exhaustion and deliberate denial-of-service attacks. Think of it as the <strong>bouncer at a nightclub</strong> — it enforces capacity limits not because it dislikes people, but because exceeding the venue&#39;s capacity creates dangerous conditions for everyone inside.</p>\n<p>The size limit system implements <strong>multi-layered protection</strong> with different limits at various stages of the upload process. This layered approach prevents resource exhaustion attacks that might try to bypass early limits through chunked uploads or by consuming resources during processing stages.</p>\n<blockquote>\n<p><strong>Decision: Hierarchical Size Limits with Early Enforcement</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to protect server resources from exhaustion while supporting legitimate large file uploads, and prevent DoS attacks through resource consumption</li>\n<li><strong>Options Considered</strong>: Single global file size limit, per-user quotas, or hierarchical limits by file type and processing stage</li>\n<li><strong>Decision</strong>: Implement multiple limit layers: per-chunk, per-file, per-session, per-user, and per-content-type with early enforcement</li>\n<li><strong>Rationale</strong>: Layered limits provide defense in depth against various attack vectors while allowing flexibility for different use cases and file types</li>\n<li><strong>Consequences</strong>: Increased configuration complexity and enforcement overhead, but comprehensive protection against resource exhaustion attacks</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Limit Layer</th>\n<th>Purpose</th>\n<th>Attack Prevention</th>\n<th>Performance Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Chunk Size</td>\n<td>Prevent memory exhaustion</td>\n<td>Large chunk DoS</td>\n<td>Minimal</td>\n</tr>\n<tr>\n<td>File Size</td>\n<td>Control individual uploads</td>\n<td>Single large file attack</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Session Total</td>\n<td>Limit chunked upload abuse</td>\n<td>Multi-file session attack</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>User Quota</td>\n<td>Fair resource allocation</td>\n<td>User-based resource hoarding</td>\n<td>Moderate</td>\n</tr>\n<tr>\n<td>Content-Type</td>\n<td>Specialized limits</td>\n<td>Type-specific attacks</td>\n<td>Minimal</td>\n</tr>\n</tbody></table>\n<p>The size enforcement system operates at <strong>multiple checkpoints</strong> throughout the upload pipeline, with each checkpoint designed to catch different types of resource exhaustion attempts before they can impact system stability.</p>\n<p><strong>Chunk-Level Protection:</strong></p>\n<p>The first line of defense operates at the individual chunk level, preventing attackers from exhausting server memory by sending extremely large chunks that must be held in memory during processing.</p>\n<table>\n<thead>\n<tr>\n<th>Chunk Limit Type</th>\n<th>Default Value</th>\n<th>Rationale</th>\n<th>Failure Mode</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Maximum Chunk Size</td>\n<td>64MB</td>\n<td>Balances memory usage with upload efficiency</td>\n<td><code>413 Request Entity Too Large</code></td>\n</tr>\n<tr>\n<td>Minimum Chunk Size</td>\n<td>1KB</td>\n<td>Prevents micro-chunk DoS attacks</td>\n<td><code>400 Bad Request</code></td>\n</tr>\n<tr>\n<td>Chunks Per Session</td>\n<td>10,000</td>\n<td>Limits session complexity</td>\n<td><code>429 Too Many Requests</code></td>\n</tr>\n</tbody></table>\n<p><strong>File-Level Protection:</strong></p>\n<p>File-level limits prevent individual uploads from consuming excessive storage space or processing time. These limits are <strong>content-type aware</strong> to allow larger sizes for file types that legitimately need them (like video files) while restricting potentially dangerous file types.</p>\n<table>\n<thead>\n<tr>\n<th>Content Type</th>\n<th>Size Limit</th>\n<th>Processing Timeout</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>image/*</code></td>\n<td>50MB</td>\n<td>30 seconds</td>\n<td>High-resolution images, reasonable processing time</td>\n</tr>\n<tr>\n<td><code>application/pdf</code></td>\n<td>100MB</td>\n<td>60 seconds</td>\n<td>Large documents, moderate processing complexity</td>\n</tr>\n<tr>\n<td><code>video/*</code></td>\n<td>2GB</td>\n<td>300 seconds</td>\n<td>Video files legitimately large, extended processing time</td>\n</tr>\n<tr>\n<td><code>application/zip</code></td>\n<td>500MB</td>\n<td>120 seconds</td>\n<td>Archives can be large, but need thorough scanning</td>\n</tr>\n<tr>\n<td>Default (unknown)</td>\n<td>10MB</td>\n<td>30 seconds</td>\n<td>Conservative limits for unknown file types</td>\n</tr>\n</tbody></table>\n<p><strong>Session and User-Level Protection:</strong></p>\n<p>Higher-level limits prevent abuse through multiple files or extended upload sessions. These limits protect against <strong>distributed resource consumption</strong> where attackers spread their resource usage across many uploads to avoid per-file detection.</p>\n<table>\n<thead>\n<tr>\n<th>Protection Level</th>\n<th>Limit Type</th>\n<th>Default Value</th>\n<th>Reset Period</th>\n<th>Enforcement</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Per Session</td>\n<td>Total bytes uploaded</td>\n<td>5GB</td>\n<td>24 hours</td>\n<td>Reject new chunks</td>\n</tr>\n<tr>\n<td>Per User</td>\n<td>Daily upload quota</td>\n<td>100GB</td>\n<td>24 hours</td>\n<td>Rate limiting</td>\n</tr>\n<tr>\n<td>Per IP</td>\n<td>Concurrent uploads</td>\n<td>10 sessions</td>\n<td>Real-time</td>\n<td>Connection limiting</td>\n</tr>\n<tr>\n<td>Global System</td>\n<td>Active uploads</td>\n<td>1000 sessions</td>\n<td>Real-time</td>\n<td>Admission control</td>\n</tr>\n</tbody></table>\n<p><strong>Resource Monitoring and Alerts:</strong></p>\n<p>The protection system continuously monitors resource usage patterns to detect both legitimate capacity issues and potential attacks. This monitoring enables <strong>proactive response</strong> before resource exhaustion affects service availability.</p>\n<table>\n<thead>\n<tr>\n<th>Monitored Resource</th>\n<th>Warning Threshold</th>\n<th>Critical Threshold</th>\n<th>Response Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Disk Space (temp storage)</td>\n<td>80% full</td>\n<td>95% full</td>\n<td>Throttle new uploads</td>\n</tr>\n<tr>\n<td>Memory Usage</td>\n<td>75% utilized</td>\n<td>90% utilized</td>\n<td>Reject large chunks</td>\n</tr>\n<tr>\n<td>Active Sessions</td>\n<td>800 concurrent</td>\n<td>950 concurrent</td>\n<td>Queue new uploads</td>\n</tr>\n<tr>\n<td>Scan Queue Depth</td>\n<td>100 pending</td>\n<td>500 pending</td>\n<td>Disable virus scanning</td>\n</tr>\n</tbody></table>\n<p><strong>Dynamic Limit Adjustment:</strong></p>\n<p>The system implements <strong>adaptive limits</strong> that can tighten during high load or attack conditions and relax during normal operation. This provides automatic defense against resource exhaustion while maintaining good user experience under normal conditions.</p>\n<p>The adaptive algorithm follows this decision process:</p>\n<ol>\n<li><strong>Monitor Resource Usage</strong>: Track current utilization of CPU, memory, disk, and network resources</li>\n<li><strong>Detect Stress Conditions</strong>: Identify when resource usage exceeds normal operating thresholds</li>\n<li><strong>Calculate Limit Adjustments</strong>: Reduce limits proportionally to resource pressure</li>\n<li><strong>Apply Graduated Restrictions</strong>: Implement tighter limits with grace periods for existing uploads</li>\n<li><strong>Monitor Effectiveness</strong>: Track whether limit adjustments successfully reduce resource pressure</li>\n<li><strong>Restore Normal Limits</strong>: Gradually return to standard limits as resource pressure decreases</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>System Load Level</th>\n<th>Upload Limit Multiplier</th>\n<th>Chunk Size Multiplier</th>\n<th>Session Limit Multiplier</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Normal (&lt; 70% resources)</td>\n<td>1.0x</td>\n<td>1.0x</td>\n<td>1.0x</td>\n</tr>\n<tr>\n<td>Elevated (70-85% resources)</td>\n<td>0.8x</td>\n<td>0.8x</td>\n<td>0.9x</td>\n</tr>\n<tr>\n<td>High (85-95% resources)</td>\n<td>0.5x</td>\n<td>0.5x</td>\n<td>0.7x</td>\n</tr>\n<tr>\n<td>Critical (&gt; 95% resources)</td>\n<td>0.2x</td>\n<td>0.2x</td>\n<td>0.5x</td>\n</tr>\n</tbody></table>\n<p><strong>Error Handling and User Communication:</strong></p>\n<p>When size limits are exceeded, the system provides <strong>clear, actionable error messages</strong> that help legitimate users understand the constraints and adjust their uploads accordingly. The error responses include specific information about what limit was exceeded and suggestions for resolving the issue.</p>\n<table>\n<thead>\n<tr>\n<th>Limit Exceeded</th>\n<th>HTTP Status</th>\n<th>Error Message</th>\n<th>User Guidance</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Chunk too large</td>\n<td>413</td>\n<td>&quot;Chunk size 128MB exceeds limit of 64MB&quot;</td>\n<td>&quot;Split into smaller chunks&quot;</td>\n</tr>\n<tr>\n<td>File too large</td>\n<td>413</td>\n<td>&quot;File size 2GB exceeds limit of 500MB for ZIP files&quot;</td>\n<td>&quot;Compress or split the file&quot;</td>\n</tr>\n<tr>\n<td>Quota exceeded</td>\n<td>429</td>\n<td>&quot;Daily quota 100GB exceeded, resets in 6 hours&quot;</td>\n<td>&quot;Wait for quota reset&quot;</td>\n</tr>\n<tr>\n<td>System overloaded</td>\n<td>503</td>\n<td>&quot;System at capacity, retry in 5 minutes&quot;</td>\n<td>&quot;Retry after delay&quot;</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p>The key principle is <strong>fail fast and fail informatively</strong> — detect limit violations as early as possible in the upload process and provide users with specific information about what went wrong and how to fix it.</p>\n</blockquote>\n<p><strong>Common Pitfalls in Resource Protection:</strong></p>\n<p>⚠️ <strong>Pitfall: Only Checking Limits at Upload Completion</strong>\nWaiting until upload completion to check file size limits allows attackers to consume storage and bandwidth resources even when their upload will ultimately be rejected. Enforce size limits during chunk upload based on the declared total size from upload initialization.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Processing Resource Limits</strong>\nFile size limits protect storage, but virus scanning and file processing can consume significant CPU and memory. Set processing timeouts and memory limits for scanning operations to prevent resource exhaustion during the validation phase.</p>\n<p>⚠️ <strong>Pitfall: Inflexible Limit Configuration</strong>\nHard-coded size limits can&#39;t adapt to different deployment environments or changing business needs. Make all limits configurable and consider implementing different limit profiles for different user classes (free vs. paid users, internal vs. external users).</p>\n<p><img src=\"/api/project/file-upload-service/architecture-doc/asset?path=diagrams%2Fupload-completion-sequence.svg\" alt=\"Upload Completion and Assembly\"></p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p><strong>A. Technology Recommendations Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>File Type Detection</td>\n<td><code>http.DetectContentType()</code> + custom magic bytes</td>\n<td><code>github.com/h2non/filetype</code> with comprehensive signatures</td>\n</tr>\n<tr>\n<td>Virus Scanning</td>\n<td>Direct <code>clamd</code> socket connection</td>\n<td><code>github.com/dutchcoders/go-clamd</code> client library</td>\n</tr>\n<tr>\n<td>Magic Byte Database</td>\n<td>Static map in code</td>\n<td>External JSON configuration file</td>\n</tr>\n<tr>\n<td>Quarantine Storage</td>\n<td>Separate directory with restricted permissions</td>\n<td>Dedicated S3 bucket with separate IAM credentials</td>\n</tr>\n<tr>\n<td>Size Limit Enforcement</td>\n<td>Simple byte counting</td>\n<td>Rate limiting with <code>golang.org/x/time/rate</code></td>\n</tr>\n<tr>\n<td>Policy Configuration</td>\n<td>YAML configuration file</td>\n<td>Database-backed policy engine</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  internal/\n    validation/\n      validator.go              ← main validation orchestrator\n      filetype/\n        detector.go            ← magic byte file type detection\n        signatures.go          ← file signature database\n        signatures.json        ← configurable signature database\n      scanner/\n        clamav.go             ← ClamAV virus scanner integration\n        scanner.go            ← scanner interface and queue management\n      quarantine/\n        quarantine.go         ← quarantine storage and metadata\n        policy.go             ← policy enforcement engine\n      limits/\n        limiter.go            ← size limits and resource protection\n    config/\n      security.yml            ← security policy configuration\n  cmd/\n    validator-test/\n      main.go                 ← standalone validation testing tool</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code:</strong></p>\n<p>Here&#39;s the complete file type detection infrastructure with magic byte support:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/validation/filetype/detector.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> filetype</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">bufio</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/hex</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">io</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// FileSignature represents a magic byte pattern for file type detection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> FileSignature</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Pattern     []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#9ECBFF\"> `json:\"pattern\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Offset      </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">    `json:\"offset\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MIME        </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"mime\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Extensions  []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"extensions\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Description </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"description\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Detector handles file type detection using magic byte patterns</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Detector</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    signatures []</span><span style=\"color:#B392F0\">FileSignature</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewDetector creates a file type detector with default signatures</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewDetector</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Detector</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Detector</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        signatures: </span><span style=\"color:#B392F0\">getDefaultSignatures</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DetectFromReader examines the first 512 bytes to determine file type</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Detector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">DetectFromReader</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">reader</span><span style=\"color:#B392F0\"> io</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Reader</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    buffer </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">512</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    n, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> io.</span><span style=\"color:#B392F0\">ReadFull</span><span style=\"color:#E1E4E8\">(reader, buffer)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#F97583\"> &#x26;&#x26;</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> io.EOF </span><span style=\"color:#F97583\">&#x26;&#x26;</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> io.ErrUnexpectedEOF {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> d.</span><span style=\"color:#B392F0\">DetectFromBytes</span><span style=\"color:#E1E4E8\">(buffer[:n]), </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DetectFromBytes matches byte patterns against known file signatures</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Detector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">DetectFromBytes</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, sig </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> d.signatures {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> d.</span><span style=\"color:#B392F0\">matchesSignature</span><span style=\"color:#E1E4E8\">(data, sig) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> sig.MIME</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#9ECBFF\"> \"application/octet-stream\"</span><span style=\"color:#6A737D\"> // Unknown binary data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Detector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">matchesSignature</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sig</span><span style=\"color:#B392F0\"> FileSignature</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(data) </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> sig.Offset</span><span style=\"color:#F97583\">+</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(sig.Pattern) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> sig.Offset</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i, b </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> sig.Pattern {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> data[start</span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\">i] </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> b {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> getDefaultSignatures</span><span style=\"color:#E1E4E8\">() []</span><span style=\"color:#B392F0\">FileSignature</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">FileSignature</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        {</span><span style=\"color:#B392F0\">mustDecodeHex</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"89504E470D0A1A0A\"</span><span style=\"color:#E1E4E8\">), </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"image/png\"</span><span style=\"color:#E1E4E8\">, []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\".png\"</span><span style=\"color:#E1E4E8\">}, </span><span style=\"color:#9ECBFF\">\"PNG Image\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        {</span><span style=\"color:#B392F0\">mustDecodeHex</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"FFD8FF\"</span><span style=\"color:#E1E4E8\">), </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"image/jpeg\"</span><span style=\"color:#E1E4E8\">, []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\".jpg\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\".jpeg\"</span><span style=\"color:#E1E4E8\">}, </span><span style=\"color:#9ECBFF\">\"JPEG Image\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        {</span><span style=\"color:#B392F0\">mustDecodeHex</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"25504446\"</span><span style=\"color:#E1E4E8\">), </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"application/pdf\"</span><span style=\"color:#E1E4E8\">, []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\".pdf\"</span><span style=\"color:#E1E4E8\">}, </span><span style=\"color:#9ECBFF\">\"PDF Document\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        {</span><span style=\"color:#B392F0\">mustDecodeHex</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"504B0304\"</span><span style=\"color:#E1E4E8\">), </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"application/zip\"</span><span style=\"color:#E1E4E8\">, []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\".zip\"</span><span style=\"color:#E1E4E8\">}, </span><span style=\"color:#9ECBFF\">\"ZIP Archive\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        {</span><span style=\"color:#B392F0\">mustDecodeHex</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"504B0506\"</span><span style=\"color:#E1E4E8\">), </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"application/zip\"</span><span style=\"color:#E1E4E8\">, []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\".zip\"</span><span style=\"color:#E1E4E8\">}, </span><span style=\"color:#9ECBFF\">\"ZIP Archive (empty)\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        {</span><span style=\"color:#B392F0\">mustDecodeHex</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"4D5A\"</span><span style=\"color:#E1E4E8\">), </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"application/x-msdownload\"</span><span style=\"color:#E1E4E8\">, []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\".exe\"</span><span style=\"color:#E1E4E8\">}, </span><span style=\"color:#9ECBFF\">\"Windows Executable\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        {</span><span style=\"color:#B392F0\">mustDecodeHex</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"7F454C46\"</span><span style=\"color:#E1E4E8\">), </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"application/x-executable\"</span><span style=\"color:#E1E4E8\">, []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">}, </span><span style=\"color:#9ECBFF\">\"ELF Executable\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> mustDecodeHex</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">s</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    b, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> hex.</span><span style=\"color:#B392F0\">DecodeString</span><span style=\"color:#E1E4E8\">(s)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">        panic</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Invalid hex signature: \"</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> s)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> b</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p>Complete ClamAV scanner integration:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/validation/scanner/clamav.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> scanner</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">bufio</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">io</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strings</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ClamAVScanner integrates with ClamAV daemon via Unix domain socket</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ClamAVScanner</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    socketPath </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timeout    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ScanResult represents the outcome of a virus scan</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ScanResult</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Clean       </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ThreatName  </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Error       </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ScanTime    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewClamAVScanner creates a scanner connected to ClamAV daemon</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewClamAVScanner</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">socketPath</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">timeout</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ClamAVScanner</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ClamAVScanner</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        socketPath: socketPath,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        timeout:    timeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ScanStream sends file data to ClamAV and returns scan results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ClamAVScanner</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ScanStream</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">reader</span><span style=\"color:#B392F0\"> io</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Reader</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ScanResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    conn, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> net.</span><span style=\"color:#B392F0\">DialTimeout</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"unix\"</span><span style=\"color:#E1E4E8\">, c.socketPath, c.timeout)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ScanResult</span><span style=\"color:#E1E4E8\">{Error: fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"connect to ClamAV: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)}, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> conn.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Set overall timeout for the scan operation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    conn.</span><span style=\"color:#B392F0\">SetDeadline</span><span style=\"color:#E1E4E8\">(time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(c.timeout))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Send INSTREAM command to ClamAV</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _, err </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> conn.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"zINSTREAM</span><span style=\"color:#79B8FF\">\\000</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ScanResult</span><span style=\"color:#E1E4E8\">{Error: fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"send command: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)}, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Stream file data in chunks</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    buffer </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">32768</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#6A737D\">// 32KB chunks</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        n, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> reader.</span><span style=\"color:#B392F0\">Read</span><span style=\"color:#E1E4E8\">(buffer)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> n </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Send chunk size (4 bytes, network byte order) then chunk data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            chunkSize </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                byte</span><span style=\"color:#E1E4E8\">(n </span><span style=\"color:#F97583\">>></span><span style=\"color:#79B8FF\"> 24</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                byte</span><span style=\"color:#E1E4E8\">(n </span><span style=\"color:#F97583\">>></span><span style=\"color:#79B8FF\"> 16</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                byte</span><span style=\"color:#E1E4E8\">(n </span><span style=\"color:#F97583\">>></span><span style=\"color:#79B8FF\"> 8</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                byte</span><span style=\"color:#E1E4E8\">(n),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> conn.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(chunkSize); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ScanResult</span><span style=\"color:#E1E4E8\">{Error: fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"send chunk size: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)}, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> conn.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(buffer[:n]); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ScanResult</span><span style=\"color:#E1E4E8\">{Error: fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"send chunk data: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)}, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> io.EOF {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            break</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ScanResult</span><span style=\"color:#E1E4E8\">{Error: fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"read file: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)}, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Send zero-length chunk to signal end of stream</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _, err </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> conn.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">})</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ScanResult</span><span style=\"color:#E1E4E8\">{Error: fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"send end marker: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)}, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Read scan result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> bufio.</span><span style=\"color:#B392F0\">NewScanner</span><span style=\"color:#E1E4E8\">(conn)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">scanner.</span><span style=\"color:#B392F0\">Scan</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ScanResult</span><span style=\"color:#E1E4E8\">{Error: fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"read scan result: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, scanner.</span><span style=\"color:#B392F0\">Err</span><span style=\"color:#E1E4E8\">())}, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    response </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> strings.</span><span style=\"color:#B392F0\">TrimSpace</span><span style=\"color:#E1E4E8\">(scanner.</span><span style=\"color:#B392F0\">Text</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanTime </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Since</span><span style=\"color:#E1E4E8\">(start)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> strings.</span><span style=\"color:#B392F0\">HasSuffix</span><span style=\"color:#E1E4E8\">(response, </span><span style=\"color:#9ECBFF\">\"OK\"</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ScanResult</span><span style=\"color:#E1E4E8\">{Clean: </span><span style=\"color:#79B8FF\">true</span><span style=\"color:#E1E4E8\">, ScanTime: scanTime}, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } </span><span style=\"color:#F97583\">else</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> strings.</span><span style=\"color:#B392F0\">Contains</span><span style=\"color:#E1E4E8\">(response, </span><span style=\"color:#9ECBFF\">\"FOUND\"</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        parts </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> strings.</span><span style=\"color:#B392F0\">Split</span><span style=\"color:#E1E4E8\">(response, </span><span style=\"color:#9ECBFF\">\": \"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(parts) </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            threatName </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> strings.</span><span style=\"color:#B392F0\">TrimSuffix</span><span style=\"color:#E1E4E8\">(parts[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#9ECBFF\">\" FOUND\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ScanResult</span><span style=\"color:#E1E4E8\">{Clean: </span><span style=\"color:#79B8FF\">false</span><span style=\"color:#E1E4E8\">, ThreatName: threatName, ScanTime: scanTime}, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ScanResult</span><span style=\"color:#E1E4E8\">{Clean: </span><span style=\"color:#79B8FF\">false</span><span style=\"color:#E1E4E8\">, ThreatName: </span><span style=\"color:#9ECBFF\">\"Unknown threat\"</span><span style=\"color:#E1E4E8\">, ScanTime: scanTime}, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ScanResult</span><span style=\"color:#E1E4E8\">{Error: fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"unexpected ClamAV response: </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, response)}, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Ping tests connectivity to ClamAV daemon</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ClamAVScanner</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Ping</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    conn, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> net.</span><span style=\"color:#B392F0\">DialTimeout</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"unix\"</span><span style=\"color:#E1E4E8\">, c.socketPath, </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">time.Second)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"connect to ClamAV: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> conn.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _, err </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> conn.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"zPING</span><span style=\"color:#79B8FF\">\\000</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"send ping: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    response </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    n, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> conn.</span><span style=\"color:#B392F0\">Read</span><span style=\"color:#E1E4E8\">(response)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"read response: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">strings.</span><span style=\"color:#B392F0\">Contains</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">(response[:n]), </span><span style=\"color:#9ECBFF\">\"PONG\"</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"unexpected ping response: </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, response[:n])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/validation/validator.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> validation</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">io</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// FileValidator orchestrates file validation pipeline</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> FileValidator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    detector    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">filetype</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Detector</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scanner     </span><span style=\"color:#B392F0\">Scanner</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    quarantine  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QuarantineManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    limits      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LimitEnforcer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SecurityConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ValidationResult contains the outcome of file validation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ValidationResult</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Passed      </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    FileType    </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ThreatName  </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    QuarantineID </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Violations   []</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ProcessingTime </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ValidateFile runs complete validation pipeline on uploaded file</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">v </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FileValidator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ValidateFile</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">reader</span><span style=\"color:#B392F0\"> io</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Reader</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">metadata</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ValidationResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ValidationResult</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Enforce size limits based on declared file size from metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Check metadata[\"content-length\"] against configured limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Return early with violation if size exceeds limits</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Detect actual file type using magic byte analysis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Use v.detector.DetectFromReader() to examine file content</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Compare detected type against metadata[\"content-type\"] for consistency</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Record any mismatches as potential security violations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Validate file type against security policy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Check if detected file type is in allowed content types list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Apply content-type-specific size limits and restrictions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Flag executable files and other high-risk content types</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Perform virus scanning on file content</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Stream file data to ClamAV scanner using v.scanner.ScanStream()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Handle scan timeouts, errors, and retry logic</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Record threat name and details for infected files</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Apply quarantine decision based on scan results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Move infected or suspicious files to quarantine storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Generate forensic metadata including session details and scan results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Update upload session status to reflect quarantine action</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Record validation metrics and audit trail</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Log all validation decisions with timestamps and reasons</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Update session metadata with validation results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Generate security alerts for policy violations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result.ProcessingTime </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Since</span><span style=\"color:#E1E4E8\">(start)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> result, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// EnforceResourceLimits checks upload against size and rate limits</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">v </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FileValidator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">EnforceResourceLimits</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">userID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">fileSize</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">contentType</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check individual file size against content-type-specific limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Look up size limit for detected content type from configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Return ErrFileTooLarge if size exceeds type-specific limit</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Verify user quota and rate limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Check user's daily/monthly upload quota from quota tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Enforce per-user concurrent upload limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Apply rate limiting for users exceeding normal usage patterns</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Monitor system resource usage and apply adaptive limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Check current system load (CPU, memory, disk space)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Apply dynamic limit adjustments during high load conditions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Return ErrSystemOverloaded if resources are critically low</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints:</strong></p>\n<ul>\n<li>Use <code>io.TeeReader</code> to simultaneously read file data for type detection and virus scanning without buffering the entire file in memory</li>\n<li>Implement <code>io.LimitReader</code> to enforce chunk size limits during streaming operations</li>\n<li>Use <code>context.WithTimeout</code> to set maximum time limits for virus scanning operations</li>\n<li>Store quarantine metadata using <code>encoding/json</code> for structured forensic data</li>\n<li>Use <code>sync.Pool</code> for byte buffers when processing multiple files concurrently to reduce garbage collection pressure</li>\n<li>Implement graceful shutdown with <code>context.Context</code> cancellation for long-running scan operations</li>\n</ul>\n<p><strong>F. Milestone Checkpoint:</strong></p>\n<p>After implementing the file validation system, verify the following behavior:</p>\n<p><strong>Command to run:</strong> <code>go test -v ./internal/validation/... -integration</code></p>\n<p><strong>Expected test output:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>=== RUN TestFileTypeDetection\n--- PASS: TestFileTypeDetection (0.05s)\n=== RUN TestVirusScanIntegration  \n--- PASS: TestVirusScanIntegration (2.31s)\n=== RUN TestQuarantineWorkflow\n--- PASS: TestQuarantineWorkflow (0.18s)\n=== RUN TestResourceLimits\n--- PASS: TestResourceLimits (0.02s)</code></pre></div>\n\n<p><strong>Manual verification steps:</strong></p>\n<ol>\n<li>Upload a legitimate PDF file: <code>curl -X POST -F &quot;file=@document.pdf&quot; http://localhost:8080/upload/test123/chunk</code> → Should complete successfully</li>\n<li>Upload a file with wrong extension: <code>curl -X POST -F &quot;file=@image.pdf&quot; http://localhost:8080/upload/test456/chunk</code> → Should detect actual PNG type and either accept or reject based on policy</li>\n<li>Upload EICAR test virus: <code>curl -X POST -F &quot;file=@eicar.com&quot; http://localhost:8080/upload/test789/chunk</code> → Should quarantine with virus detection</li>\n<li>Upload oversized file: <code>curl -X POST -F &quot;file=@huge.zip&quot; http://localhost:8080/upload/test000/chunk</code> → Should reject with size limit error</li>\n</ol>\n<p><strong>Signs of correct implementation:</strong></p>\n<ul>\n<li>File type detection works regardless of filename extension</li>\n<li>ClamAV integration successfully scans files and detects test viruses</li>\n<li>Quarantine storage isolates infected files with complete metadata</li>\n<li>Size limits prevent resource exhaustion during upload</li>\n</ul>\n<p><strong>G. Debugging Tips:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>All files rejected as &quot;unknown type&quot;</td>\n<td>Magic byte detection failing</td>\n<td>Check first 16 bytes of uploaded files in hex dump</td>\n<td>Verify file signature database includes common types</td>\n</tr>\n<tr>\n<td>Virus scanner timeouts</td>\n<td>ClamAV daemon overloaded or hung</td>\n<td>Check ClamAV logs and process status</td>\n<td>Restart clamd, increase timeout, implement scan queuing</td>\n</tr>\n<tr>\n<td>Clean files quarantined</td>\n<td>False positive in virus scanning</td>\n<td>Review ClamAV signature database version</td>\n<td>Update virus signatures, whitelist known-good file hashes</td>\n</tr>\n<tr>\n<td>Memory usage grows continuously</td>\n<td>File data not being released after processing</td>\n<td>Monitor goroutines and heap profile</td>\n<td>Ensure file readers are properly closed, use streaming processing</td>\n</tr>\n<tr>\n<td>Quarantine storage fills up</td>\n<td>No cleanup of old quarantined files</td>\n<td>Check quarantine retention policies and cleanup jobs</td>\n<td>Implement automated cleanup based on file age and threat level</td>\n</tr>\n</tbody></table>\n<h2 id=\"component-interactions-and-data-flow\">Component Interactions and Data Flow</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestones 1, 2, and 3 — this section describes how components communicate during chunked upload protocol operations (M1), storage backend coordination (M2), and virus scanning workflow integration (M3)</p>\n</blockquote>\n<p>Think of component interactions in a resumable file upload service like an <em>airport&#39;s baggage handling system</em>. When you check a bag, it doesn&#39;t go directly to the plane — it moves through multiple specialized stations: check-in creates a tracking record, security screening validates contents, sorting routes it to the correct destination, and loading crews place it on the aircraft. Similarly, uploaded file chunks flow through our specialized components: the Upload Manager receives and tracks chunks, the Session Store maintains progress state, Storage Backends persist data, and File Validators ensure security. Each component has a specific responsibility and communicates with others through well-defined message formats, ensuring that even if one station experiences delays, the overall system can recover and complete the process.</p>\n<p>The critical insight for understanding these interactions is that <strong>state consistency across component boundaries</strong> is what enables resumable uploads to work reliably. When a network failure interrupts an upload, each component must have recorded enough information for the client to discover exactly where to resume, regardless of which component was processing data when the failure occurred.</p>\n<p><img src=\"/api/project/file-upload-service/architecture-doc/asset?path=diagrams%2Fsystem-architecture.svg\" alt=\"System Architecture Overview\"></p>\n<h3 id=\"upload-initialization-flow-client-handshake-session-creation-and-metadata-exchange\">Upload Initialization Flow: Client handshake, session creation, and metadata exchange</h3>\n<p>The upload initialization flow represents the <em>contract negotiation</em> between client and server — like establishing the terms of a shipping agreement before moving any cargo. During this phase, the client declares its intentions (file size, type, metadata), the server validates feasibility and allocates resources, and both parties agree on the upload session parameters that will govern all subsequent chunk transfers.</p>\n<blockquote>\n<p><strong>Decision: Separate Initialization from First Chunk Upload</strong></p>\n<ul>\n<li><strong>Context</strong>: Clients need to know upload session parameters before sending chunk data, and servers need to validate requirements before allocating storage resources</li>\n<li><strong>Options Considered</strong>: Combined initialization+first-chunk endpoint, separate initialization endpoint, implicit session creation on first chunk</li>\n<li><strong>Decision</strong>: Separate initialization endpoint that returns session metadata before any chunk uploads</li>\n<li><strong>Rationale</strong>: Allows early validation of file size, content type, and storage capacity without wasting bandwidth on chunk uploads that would fail. Enables client-side resume logic to query session state independently of chunk uploads</li>\n<li><strong>Consequences</strong>: Requires additional HTTP round-trip but prevents failed uploads after significant data transfer, simplifies error handling, and enables better resource planning</li>\n</ul>\n</blockquote>\n<p>The initialization sequence involves multiple components working together to establish upload session state:</p>\n<p><strong>Initialization Message Flow:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component From</th>\n<th>Component To</th>\n<th>Message Type</th>\n<th>Purpose</th>\n<th>Failure Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Client</td>\n<td>Upload Manager</td>\n<td><code>POST /files/uploads</code></td>\n<td>Request new upload session</td>\n<td>Client cannot start upload</td>\n</tr>\n<tr>\n<td>Upload Manager</td>\n<td>File Validator</td>\n<td><code>ValidateRequirements()</code></td>\n<td>Check file size/type limits</td>\n<td>Early rejection prevents resource waste</td>\n</tr>\n<tr>\n<td>Upload Manager</td>\n<td>Storage Backend</td>\n<td><code>InitMultipart()</code></td>\n<td>Prepare storage resources</td>\n<td>Session unusable without storage</td>\n</tr>\n<tr>\n<td>Upload Manager</td>\n<td>Session Store</td>\n<td><code>CreateSession()</code></td>\n<td>Persist session metadata</td>\n<td>Session cannot be resumed</td>\n</tr>\n<tr>\n<td>Upload Manager</td>\n<td>Client</td>\n<td>Session metadata response</td>\n<td>Confirm session creation</td>\n<td>Client missing upload parameters</td>\n</tr>\n</tbody></table>\n<p><strong>Detailed Initialization Steps:</strong></p>\n<ol>\n<li><p><strong>Client Request Preparation</strong>: The client constructs an initialization request containing file metadata, declared content type, total file size, and optional custom metadata fields. This request must include enough information for the server to make resource allocation decisions without receiving file content.</p>\n</li>\n<li><p><strong>Upload Manager Request Validation</strong>: Upon receiving the initialization request, the Upload Manager performs basic request format validation, ensures required fields are present, and generates a unique session identifier that will track this upload throughout its lifecycle.</p>\n</li>\n<li><p><strong>Security Policy Validation</strong>: The Upload Manager delegates to the File Validator to check the declared file size against configured limits, validate the content type against allowed types, and verify that current system load can accommodate another upload session.</p>\n</li>\n<li><p><strong>Storage Resource Allocation</strong>: If validation passes, the Upload Manager calls the Storage Backend&#39;s <code>InitMultipart()</code> method to reserve storage resources and obtain any backend-specific metadata (like S3 multipart upload IDs) needed for subsequent chunk operations.</p>\n</li>\n<li><p><strong>Session State Persistence</strong>: The Upload Manager creates an <code>UploadSession</code> record containing all session metadata and stores it via the Session Store, ensuring that session information survives server restarts and can be queried for resume operations.</p>\n</li>\n<li><p><strong>Client Response Generation</strong>: Finally, the Upload Manager constructs a response containing the session ID, upload URL for chunk uploads, current offset (initially zero), and any additional parameters the client needs for chunk uploads.</p>\n</li>\n</ol>\n<p><strong>Upload Session Initialization Data Structures:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n<th>Validation Rules</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Filename</code></td>\n<td>string</td>\n<td>Original filename from client</td>\n<td>Length 1-255, no path separators</td>\n</tr>\n<tr>\n<td><code>ContentType</code></td>\n<td>string</td>\n<td>Declared MIME type</td>\n<td>Must match allowed content types</td>\n</tr>\n<tr>\n<td><code>TotalSize</code></td>\n<td>int64</td>\n<td>Total file size in bytes</td>\n<td>Must be &gt; 0 and &lt;= MaxFileSize</td>\n</tr>\n<tr>\n<td><code>Metadata</code></td>\n<td>map[string]string</td>\n<td>Custom client metadata</td>\n<td>Max 10 keys, values &lt;= 1KB each</td>\n</tr>\n<tr>\n<td><code>StorageKey</code></td>\n<td>string</td>\n<td>Backend-specific storage identifier</td>\n<td>Generated by storage backend</td>\n</tr>\n<tr>\n<td><code>CurrentOffset</code></td>\n<td>int64</td>\n<td>Bytes successfully uploaded</td>\n<td>Initially 0</td>\n</tr>\n<tr>\n<td><code>Status</code></td>\n<td>SessionStatus</td>\n<td>Current session state</td>\n<td>Initially <code>SessionStatusInitialized</code></td>\n</tr>\n</tbody></table>\n<p><strong>Error Scenarios During Initialization:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Error Condition</th>\n<th>Detection Point</th>\n<th>Error Response</th>\n<th>Recovery Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>File size exceeds limits</td>\n<td>File Validator</td>\n<td>HTTP 413 Payload Too Large</td>\n<td>Client must reduce file size</td>\n</tr>\n<tr>\n<td>Content type not allowed</td>\n<td>File Validator</td>\n<td>HTTP 415 Unsupported Media Type</td>\n<td>Client must change file type</td>\n</tr>\n<tr>\n<td>Storage quota exceeded</td>\n<td>Storage Backend</td>\n<td>HTTP 507 Insufficient Storage</td>\n<td>Client must retry later</td>\n</tr>\n<tr>\n<td>Session store unavailable</td>\n<td>Session Store</td>\n<td>HTTP 503 Service Unavailable</td>\n<td>Client must retry with backoff</td>\n</tr>\n</tbody></table>\n<p><img src=\"/api/project/file-upload-service/architecture-doc/asset?path=diagrams%2Fchunk-upload-sequence.svg\" alt=\"Chunk Upload Sequence Flow\"></p>\n<h3 id=\"chunk-processing-pipeline-from-chunk-receipt-through-validation-to-storage-commitment\">Chunk Processing Pipeline: From chunk receipt through validation to storage commitment</h3>\n<p>The chunk processing pipeline operates like a <em>manufacturing assembly line</em> where each station performs specific quality checks and transformations on the raw material (chunk data) before passing it to the next station. Unlike a physical assembly line, however, our pipeline must handle out-of-order arrivals, duplicate parts, and the ability to restart from any point if machinery breaks down.</p>\n<blockquote>\n<p><strong>Decision: Validate-Then-Store vs Store-Then-Validate for Chunks</strong></p>\n<ul>\n<li><strong>Context</strong>: Chunks can be corrupted during network transmission, and storing corrupt data wastes storage space and complicates cleanup</li>\n<li><strong>Options Considered</strong>: Validate checksums before storage, store immediately and validate in background, validate only on assembly</li>\n<li><strong>Decision</strong>: Validate checksums and basic integrity before committing chunks to storage</li>\n<li><strong>Rationale</strong>: Prevents storage pollution with corrupt data, enables immediate client feedback on transmission errors, and reduces complexity during final assembly since all stored chunks are known-good</li>\n<li><strong>Consequences</strong>: Slightly higher latency per chunk but eliminates need for complex cleanup of corrupt data and reduces final assembly complexity</li>\n</ul>\n</blockquote>\n<p>The chunk processing pipeline coordinates multiple validation and storage operations:</p>\n<p><strong>Chunk Processing Data Flow:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Stage</th>\n<th>Input</th>\n<th>Processing</th>\n<th>Output</th>\n<th>Error Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Receipt</td>\n<td>HTTP chunk request</td>\n<td>Parse headers, extract metadata</td>\n<td>Chunk metadata + data stream</td>\n<td>Reject malformed requests</td>\n</tr>\n<tr>\n<td>Validation</td>\n<td>Chunk data + metadata</td>\n<td>Verify offset, size, checksum</td>\n<td>Validated chunk</td>\n<td>Request chunk retransmission</td>\n</tr>\n<tr>\n<td>Deduplication</td>\n<td>Chunk checksum</td>\n<td>Check for existing chunk</td>\n<td>Unique chunk or skip flag</td>\n<td>Continue with existing chunk</td>\n</tr>\n<tr>\n<td>Storage</td>\n<td>Validated chunk data</td>\n<td>Write to storage backend</td>\n<td>Storage confirmation</td>\n<td>Retry or fail session</td>\n</tr>\n<tr>\n<td>Session Update</td>\n<td>Storage confirmation</td>\n<td>Update session progress</td>\n<td>Updated session state</td>\n<td>Mark session as corrupted</td>\n</tr>\n</tbody></table>\n<p><strong>Detailed Chunk Processing Steps:</strong></p>\n<ol>\n<li><p><strong>Chunk Request Parsing</strong>: The Upload Manager receives an HTTP request containing chunk data with headers specifying session ID, byte offset, chunk size, and content hash. The manager extracts these parameters and validates the session exists and is in a state that accepts chunks.</p>\n</li>\n<li><p><strong>Offset Validation</strong>: The manager checks that the provided offset aligns with the current session offset (for sequential uploads) or falls within valid ranges (for parallel chunk uploads). Offset mismatches indicate client-server synchronization issues that must be resolved before proceeding.</p>\n</li>\n<li><p><strong>Chunk Integrity Verification</strong>: The manager computes a checksum of the received chunk data and compares it against the client-provided hash. This catches network transmission corruption before data reaches storage, preventing accumulation of corrupt chunks.</p>\n</li>\n<li><p><strong>Storage Backend Coordination</strong>: For valid chunks, the manager calls the appropriate Storage Backend method (<code>StoreChunk</code> for simple storage or <code>StoreMultipartChunk</code> for cloud backends) to persist the chunk data at the correct offset within the upload session&#39;s storage space.</p>\n</li>\n<li><p><strong>Session State Update</strong>: Upon successful storage, the manager updates the session&#39;s current offset and chunk tracking metadata in the Session Store, ensuring that resume operations can accurately determine which chunks have been successfully received.</p>\n</li>\n<li><p><strong>Client Response Generation</strong>: The manager returns an HTTP response indicating successful chunk receipt, the updated upload offset, and any additional information the client needs to continue or complete the upload.</p>\n</li>\n</ol>\n<p><strong>Chunk Validation Parameters:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Parameter</th>\n<th>Source</th>\n<th>Validation Rule</th>\n<th>Error Response</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Session ID</td>\n<td>URL path</td>\n<td>Must exist in session store</td>\n<td>HTTP 404 Not Found</td>\n</tr>\n<tr>\n<td>Content-Range</td>\n<td>HTTP header</td>\n<td>Must specify valid byte range</td>\n<td>HTTP 400 Bad Request</td>\n</tr>\n<tr>\n<td>Content-Length</td>\n<td>HTTP header</td>\n<td>Must match chunk data size</td>\n<td>HTTP 400 Bad Request</td>\n</tr>\n<tr>\n<td>Upload-Checksum</td>\n<td>HTTP header</td>\n<td>Must match computed hash</td>\n<td>HTTP 460 Checksum Mismatch</td>\n</tr>\n</tbody></table>\n<p><strong>Parallel Chunk Upload Coordination:</strong></p>\n<p>The system supports parallel chunk uploads where clients can upload non-contiguous chunks simultaneously. This requires careful coordination to maintain data integrity:</p>\n<table>\n<thead>\n<tr>\n<th>Scenario</th>\n<th>Detection</th>\n<th>Handling</th>\n<th>State Update</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Duplicate chunk</td>\n<td>Matching offset + size</td>\n<td>Return success without storage</td>\n<td>No state change</td>\n</tr>\n<tr>\n<td>Overlapping chunks</td>\n<td>Offset ranges overlap</td>\n<td>Reject newer chunk</td>\n<td>No state change</td>\n</tr>\n<tr>\n<td>Out-of-order chunks</td>\n<td>Offset &gt; current progress</td>\n<td>Store with gap tracking</td>\n<td>Update chunk map only</td>\n</tr>\n<tr>\n<td>Gap filling</td>\n<td>Chunk fills existing gap</td>\n<td>Store and check completeness</td>\n<td>Update progress if contiguous</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: Race Conditions in Parallel Uploads</strong>\nWhen multiple chunks arrive simultaneously, concurrent updates to session state can cause offset tracking corruption. Always use atomic operations or locks when updating session progress, and verify session state hasn&#39;t changed between read and write operations.</p>\n<h3 id=\"upload-completion-workflow-assembly-final-validation-and-client-notification-sequence\">Upload Completion Workflow: Assembly, final validation, and client notification sequence</h3>\n<p>The upload completion workflow functions like a <em>final quality inspection and packaging process</em> in manufacturing. Once all raw materials (chunks) have arrived, the system must assemble them into the final product (complete file), run comprehensive quality tests (virus scanning and file validation), and either deliver the product to the customer (move to final storage) or reject it to quality control (quarantine for review).</p>\n<blockquote>\n<p><strong>Decision: Atomic Assembly with Rollback vs Incremental Assembly</strong></p>\n<ul>\n<li><strong>Context</strong>: Final file assembly can fail due to chunk corruption, storage errors, or validation failures after significant processing time</li>\n<li><strong>Options Considered</strong>: Assemble chunks incrementally during upload, atomic assembly only after all chunks received, hybrid approach with checkpoints</li>\n<li><strong>Decision</strong>: Atomic assembly triggered only when all chunks are confirmed received</li>\n<li><strong>Rationale</strong>: Ensures file integrity by validating completeness before assembly, simplifies rollback on assembly failures, and enables comprehensive validation on complete file rather than partial data</li>\n<li><strong>Consequences</strong>: Higher completion latency and temporary storage overhead, but eliminates partial file corruption and simplifies error recovery</li>\n</ul>\n</blockquote>\n<p>The completion workflow orchestrates several complex operations across multiple components:</p>\n<p><strong>Upload Completion Trigger Conditions:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Trigger Type</th>\n<th>Detection Method</th>\n<th>Validation Required</th>\n<th>Next Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Sequential completion</td>\n<td>Current offset equals total size</td>\n<td>Verify no gaps in chunks</td>\n<td>Begin assembly process</td>\n</tr>\n<tr>\n<td>Explicit completion</td>\n<td>Client sends completion request</td>\n<td>Check all chunks received</td>\n<td>Validate then assemble</td>\n</tr>\n<tr>\n<td>Chunk completeness</td>\n<td>All chunk ranges filled</td>\n<td>Verify checksums match</td>\n<td>Start validation pipeline</td>\n</tr>\n<tr>\n<td>Timeout completion</td>\n<td>Session inactive beyond threshold</td>\n<td>Check if assembly viable</td>\n<td>Cleanup or emergency assembly</td>\n</tr>\n</tbody></table>\n<p><strong>Detailed Completion Workflow Steps:</strong></p>\n<ol>\n<li><p><strong>Completion Detection</strong>: The Upload Manager detects that an upload is ready for completion either because the current offset has reached the declared total file size, or because the client has sent an explicit completion request. The manager verifies that all expected chunks have been received and no gaps exist in the upload.</p>\n</li>\n<li><p><strong>Pre-Assembly Validation</strong>: Before beginning the expensive assembly process, the manager performs final checks on session state, verifies that all stored chunks are still accessible in the Storage Backend, and confirms that the total size matches the originally declared size.</p>\n</li>\n<li><p><strong>File Assembly Coordination</strong>: The manager calls the Storage Backend&#39;s <code>CompleteMultipart()</code> method, which assembles all stored chunks into a single file in the correct order. This operation is typically atomic at the storage level, meaning it either produces a complete valid file or fails without leaving partial results.</p>\n</li>\n<li><p><strong>File Content Validation</strong>: Once assembly completes, the File Validator receives the complete file for comprehensive validation including magic byte detection for file type verification, virus scanning through ClamAV integration, and content policy enforcement based on security rules.</p>\n</li>\n<li><p><strong>Final Storage or Quarantine</strong>: Based on validation results, the file is either moved to its final storage location for client access, or transferred to quarantine storage if security violations were detected. The session state is updated to reflect the final outcome.</p>\n</li>\n<li><p><strong>Client Notification</strong>: The Upload Manager generates a completion notification to the client containing the final file location, any metadata discovered during validation, and access URLs for downloading the completed file.</p>\n</li>\n</ol>\n<p><img src=\"/api/project/file-upload-service/architecture-doc/asset?path=diagrams%2Fvirus-scanning-flow.svg\" alt=\"File Validation and Scanning Pipeline\"></p>\n<p><strong>File Assembly Data Structures:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Data Structure</th>\n<th>Key Fields</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Upload Manager</td>\n<td><code>AssemblyRequest</code></td>\n<td>SessionID, TotalChunks, ExpectedSize</td>\n<td>Assembly coordination</td>\n</tr>\n<tr>\n<td>Storage Backend</td>\n<td><code>MultipartPart</code></td>\n<td>PartNumber, ETag, Size</td>\n<td>Chunk ordering and verification</td>\n</tr>\n<tr>\n<td>File Validator</td>\n<td><code>ValidationRequest</code></td>\n<td>FileHandle, SessionMetadata, PolicyRules</td>\n<td>Comprehensive file validation</td>\n</tr>\n<tr>\n<td>Session Store</td>\n<td><code>CompletionResult</code></td>\n<td>Status, FileLocation, ThreatInfo</td>\n<td>Final outcome recording</td>\n</tr>\n</tbody></table>\n<p><strong>Validation Pipeline Integration:</strong></p>\n<p>The completion workflow integrates tightly with the file validation pipeline to ensure security and data integrity:</p>\n<table>\n<thead>\n<tr>\n<th>Validation Stage</th>\n<th>Component</th>\n<th>Processing</th>\n<th>Success Action</th>\n<th>Failure Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>File type detection</td>\n<td>File Validator</td>\n<td>Magic byte analysis</td>\n<td>Continue to virus scan</td>\n<td>Reject upload</td>\n</tr>\n<tr>\n<td>Virus scanning</td>\n<td>ClamAV Scanner</td>\n<td>Stream file to ClamAV</td>\n<td>Continue to policy check</td>\n<td>Quarantine file</td>\n</tr>\n<tr>\n<td>Policy enforcement</td>\n<td>Security Manager</td>\n<td>Check size, type, metadata rules</td>\n<td>Move to final storage</td>\n<td>Apply policy action</td>\n</tr>\n<tr>\n<td>Integrity verification</td>\n<td>Upload Manager</td>\n<td>Verify assembled file checksum</td>\n<td>Complete session</td>\n<td>Restart assembly</td>\n</tr>\n</tbody></table>\n<p><strong>Error Recovery During Completion:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Failure Point</th>\n<th>Error Detection</th>\n<th>Recovery Strategy</th>\n<th>Session State</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Assembly failure</td>\n<td>Storage backend error</td>\n<td>Retry assembly up to 3 times</td>\n<td>Remains <code>SessionStatusActive</code></td>\n</tr>\n<tr>\n<td>Validation timeout</td>\n<td>ClamAV timeout</td>\n<td>Use fallback validation or quarantine</td>\n<td>Mark <code>SessionStatusCompleting</code></td>\n</tr>\n<tr>\n<td>Storage full</td>\n<td>Disk space exhaustion</td>\n<td>Attempt cleanup then retry</td>\n<td>Retry or fail session</td>\n</tr>\n<tr>\n<td>Corruption detected</td>\n<td>Checksum mismatch</td>\n<td>Request chunk re-upload</td>\n<td>Reset to active for missing chunks</td>\n</tr>\n</tbody></table>\n<h3 id=\"error-propagation-patterns-how-failures-in-different-components-are-handled-and-communicated\">Error Propagation Patterns: How failures in different components are handled and communicated</h3>\n<p>Error propagation in the upload service follows a <em>circuit breaker pattern</em> similar to electrical systems — when a component detects a fault, it must decide whether to handle the error locally, propagate it upstream, or isolate the failure to prevent system-wide cascading failures. The key insight is that different types of errors require different propagation strategies: transient network errors should be retried, data corruption errors should be propagated immediately, and resource exhaustion errors should trigger backpressure.</p>\n<blockquote>\n<p><strong>Decision: Fail-Fast vs Retry-With-Backoff for Component Errors</strong></p>\n<ul>\n<li><strong>Context</strong>: Component failures can be transient (network timeouts) or permanent (disk full), and different error types require different handling strategies</li>\n<li><strong>Options Considered</strong>: Retry all errors uniformly, fail immediately on any error, categorize errors by type with specific strategies</li>\n<li><strong>Decision</strong>: Categorize errors into transient, permanent, and data corruption types with specific handling for each</li>\n<li><strong>Rationale</strong>: Transient errors often resolve themselves with retry, permanent errors waste resources if retried, and data corruption requires immediate client notification to prevent silent data loss</li>\n<li><strong>Consequences</strong>: More complex error handling logic but better user experience and resource utilization, fewer false failures from transient issues</li>\n</ul>\n</blockquote>\n<p><strong>Error Classification and Propagation Strategy:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Error Category</th>\n<th>Examples</th>\n<th>Local Handling</th>\n<th>Propagation Strategy</th>\n<th>Client Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Transient</td>\n<td>Network timeouts, temporary storage unavailable</td>\n<td>Retry with exponential backoff</td>\n<td>Propagate after retry exhaustion</td>\n<td>Delayed response</td>\n</tr>\n<tr>\n<td>Permanent</td>\n<td>Disk full, authentication failure</td>\n<td>No local retry</td>\n<td>Immediate propagation</td>\n<td>Error response</td>\n</tr>\n<tr>\n<td>Data Corruption</td>\n<td>Checksum mismatch, malformed chunks</td>\n<td>Log and reject data</td>\n<td>Immediate propagation with details</td>\n<td>Retry request</td>\n</tr>\n<tr>\n<td>Security Violation</td>\n<td>Virus detected, policy violation</td>\n<td>Quarantine action</td>\n<td>Propagate with security context</td>\n<td>Upload rejection</td>\n</tr>\n<tr>\n<td>Resource Exhaustion</td>\n<td>Memory full, connection pool exhausted</td>\n<td>Apply backpressure</td>\n<td>Propagate with retry guidance</td>\n<td>Rate limiting</td>\n</tr>\n</tbody></table>\n<p><strong>Component-Specific Error Handling:</strong></p>\n<p>Each component in the upload pipeline has specific error handling responsibilities based on its role in the system:</p>\n<p><strong>Upload Manager Error Handling:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Error Source</th>\n<th>Error Type</th>\n<th>Detection Method</th>\n<th>Response Strategy</th>\n<th>Downstream Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Session Store</td>\n<td>Connection timeout</td>\n<td>Operation timeout</td>\n<td>Retry with backoff, then fail</td>\n<td>Client gets 503 Service Unavailable</td>\n</tr>\n<tr>\n<td>Storage Backend</td>\n<td>Insufficient space</td>\n<td>Write operation failure</td>\n<td>Attempt cleanup, then propagate</td>\n<td>Client gets 507 Insufficient Storage</td>\n</tr>\n<tr>\n<td>File Validator</td>\n<td>Virus detected</td>\n<td>Validation result</td>\n<td>Quarantine file, notify client</td>\n<td>Client gets security violation response</td>\n</tr>\n<tr>\n<td>Internal</td>\n<td>State corruption</td>\n<td>Invariant checks</td>\n<td>Log error, reset session state</td>\n<td>Client must restart upload</td>\n</tr>\n</tbody></table>\n<p><strong>Storage Backend Error Handling:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Operation</th>\n<th>Failure Mode</th>\n<th>Local Recovery</th>\n<th>Error Propagation</th>\n<th>Impact on Upload</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>InitMultipart</code></td>\n<td>Authentication failure</td>\n<td>Refresh credentials once</td>\n<td>Propagate auth error</td>\n<td>Session creation fails</td>\n</tr>\n<tr>\n<td><code>StoreChunk</code></td>\n<td>Network partition</td>\n<td>Retry 3 times with backoff</td>\n<td>Propagate timeout after retries</td>\n<td>Chunk upload fails</td>\n</tr>\n<tr>\n<td><code>CompleteMultipart</code></td>\n<td>Assembly failure</td>\n<td>Retry assembly</td>\n<td>Propagate assembly error</td>\n<td>Completion fails</td>\n</tr>\n<tr>\n<td><code>DeleteChunk</code></td>\n<td>Partial cleanup</td>\n<td>Log and continue</td>\n<td>Don&#39;t propagate cleanup errors</td>\n<td>Background cleanup</td>\n</tr>\n</tbody></table>\n<p><strong>Session Store Error Handling:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Error Scenario</th>\n<th>Detection</th>\n<th>Recovery Action</th>\n<th>Error Propagation</th>\n<th>System Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Concurrent modification</td>\n<td>Version mismatch</td>\n<td>Refresh and retry</td>\n<td>Propagate conflict error</td>\n<td>Client retries</td>\n</tr>\n<tr>\n<td>Connection pool exhausted</td>\n<td>Connection timeout</td>\n<td>Queue request with timeout</td>\n<td>Propagate overload error</td>\n<td>Apply backpressure</td>\n</tr>\n<tr>\n<td>Data corruption</td>\n<td>Read validation failure</td>\n<td>Attempt backup read</td>\n<td>Propagate corruption error</td>\n<td>Session recovery</td>\n</tr>\n<tr>\n<td>Memory exhaustion</td>\n<td>Allocation failure</td>\n<td>Trigger cleanup</td>\n<td>Propagate resource error</td>\n<td>Reject new sessions</td>\n</tr>\n</tbody></table>\n<p><strong>Error Context and Metadata Preservation:</strong></p>\n<p>When propagating errors between components, the system preserves essential context for debugging and recovery:</p>\n<table>\n<thead>\n<tr>\n<th>Context Field</th>\n<th>Source</th>\n<th>Purpose</th>\n<th>Propagation Rule</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>SessionID</code></td>\n<td>Upload Manager</td>\n<td>Link error to specific upload</td>\n<td>Always preserve</td>\n</tr>\n<tr>\n<td><code>OperationID</code></td>\n<td>Each component</td>\n<td>Trace operation sequence</td>\n<td>Preserve in logs</td>\n</tr>\n<tr>\n<td><code>ErrorCode</code></td>\n<td>Component generating error</td>\n<td>Categorize error type</td>\n<td>Preserve for client</td>\n</tr>\n<tr>\n<td><code>RetryCount</code></td>\n<td>Error handling logic</td>\n<td>Track retry attempts</td>\n<td>Reset at component boundaries</td>\n</tr>\n<tr>\n<td><code>ErrorTimestamp</code></td>\n<td>Error detection point</td>\n<td>Order error sequences</td>\n<td>Preserve for analysis</td>\n</tr>\n<tr>\n<td><code>ComponentStack</code></td>\n<td>Call chain</td>\n<td>Debug error propagation</td>\n<td>Preserve in internal errors</td>\n</tr>\n</tbody></table>\n<p><strong>Client Error Response Format:</strong></p>\n<p>The Upload Manager standardizes error responses to provide consistent client experience regardless of which internal component generated the error:</p>\n<table>\n<thead>\n<tr>\n<th>Response Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Example Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>error</code></td>\n<td>string</td>\n<td>Human-readable error message</td>\n<td>&quot;Chunk checksum validation failed&quot;</td>\n</tr>\n<tr>\n<td><code>error_code</code></td>\n<td>string</td>\n<td>Machine-readable error category</td>\n<td>&quot;CHECKSUM_MISMATCH&quot;</td>\n</tr>\n<tr>\n<td><code>session_id</code></td>\n<td>string</td>\n<td>Upload session identifier</td>\n<td>&quot;upload_123456789&quot;</td>\n</tr>\n<tr>\n<td><code>retry_after</code></td>\n<td>int</td>\n<td>Seconds to wait before retry</td>\n<td>30</td>\n</tr>\n<tr>\n<td><code>details</code></td>\n<td>object</td>\n<td>Component-specific error details</td>\n<td>{&quot;expected&quot;: &quot;abc123&quot;, &quot;actual&quot;: &quot;def456&quot;}</td>\n</tr>\n</tbody></table>\n<p><strong>Cascading Failure Prevention:</strong></p>\n<p>The system implements several patterns to prevent component failures from cascading throughout the system:</p>\n<p>⚠️ <strong>Pitfall: Error Amplification in Distributed Components</strong>\nWhen storage backends fail, Upload Managers might retry operations across multiple sessions simultaneously, amplifying load on the already-struggling backend. Implement per-backend circuit breakers that temporarily reject operations when failure rates exceed thresholds, allowing backends time to recover.</p>\n<p><strong>Circuit Breaker Implementation Strategy:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Failure Threshold</th>\n<th>Circuit Open Duration</th>\n<th>Recovery Check Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Storage Backend</td>\n<td>5 failures in 60 seconds</td>\n<td>30 seconds</td>\n<td>Single test operation</td>\n</tr>\n<tr>\n<td>Session Store</td>\n<td>10 failures in 30 seconds</td>\n<td>15 seconds</td>\n<td>Health check query</td>\n</tr>\n<tr>\n<td>File Validator</td>\n<td>3 timeouts in 120 seconds</td>\n<td>60 seconds</td>\n<td>Ping ClamAV socket</td>\n</tr>\n<tr>\n<td>External Services</td>\n<td>50% failure rate</td>\n<td>120 seconds</td>\n<td>Synthetic health check</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Message Passing</td>\n<td>HTTP JSON APIs with standard status codes</td>\n<td>gRPC with Protocol Buffers for binary efficiency</td>\n</tr>\n<tr>\n<td>Error Handling</td>\n<td>Standard Go error interface with wrapped contexts</td>\n<td>Structured error types with categorization</td>\n</tr>\n<tr>\n<td>Circuit Breakers</td>\n<td>Simple failure counting with timeouts</td>\n<td>Hystrix-style adaptive circuit breakers</td>\n</tr>\n<tr>\n<td>Observability</td>\n<td>Standard library logging with JSON format</td>\n<td>OpenTelemetry with distributed tracing</td>\n</tr>\n<tr>\n<td>State Coordination</td>\n<td>Mutex-protected shared state</td>\n<td>Actor model with message passing</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File/Module Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/\n  upload/\n    manager.go              ← Upload Manager with flow coordination\n    flows.go                ← Initialization, chunk, completion flows\n    errors.go               ← Error types and handling utilities\n    manager_test.go         ← Flow integration tests\n  coordinator/\n    coordinator.go          ← Component interaction coordination\n    circuit_breaker.go      ← Failure isolation utilities\n  messaging/\n    types.go                ← Request/response message types\n    validation.go           ← Message format validation\n    serialization.go        ← JSON marshaling utilities</code></pre></div>\n\n<p><strong>Message Type Definitions (Infrastructure Starter Code):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> messaging</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// InitUploadRequest represents the client's request to start a new upload session</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> InitUploadRequest</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Filename    </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"filename\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ContentType </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"content_type\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TotalSize   </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">             `json:\"total_size\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Metadata    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"metadata,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// InitUploadResponse contains the server's session creation confirmation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> InitUploadResponse</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SessionID   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"session_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    UploadURL   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"upload_url\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ExpiresAt   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">         `json:\"expires_at\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ChunkSize   </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">             `json:\"recommended_chunk_size\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Metadata    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"metadata,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ChunkUploadResponse confirms successful chunk receipt</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ChunkUploadResponse</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SessionID     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"session_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CurrentOffset </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">  `json:\"current_offset\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    NextOffset    </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">  `json:\"next_expected_offset\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Completed     </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">   `json:\"upload_completed\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ErrorResponse provides structured error information to clients</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ErrorResponse</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Error     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">      `json:\"error\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrorCode </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">      `json:\"error_code\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SessionID </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">      `json:\"session_id,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RetryAfter </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">        `json:\"retry_after,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Details   </span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{} </span><span style=\"color:#9ECBFF\">`json:\"details,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Validate checks InitUploadRequest for required fields and constraints</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">r </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">InitUploadRequest</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Validate</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> r.Filename </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"filename required\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> r.TotalSize </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"total_size must be positive\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> r.ContentType </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"content_type required\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Flow Coordination Infrastructure (Infrastructure Starter Code):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> coordinator</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CircuitBreaker prevents cascading failures by isolating failing components</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CircuitBreaker</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    maxFailures   </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resetTimeout  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    failures      </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lastFailTime  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    state         </span><span style=\"color:#B392F0\">CircuitState</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex         </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CircuitState</span><span style=\"color:#F97583\"> int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CircuitClosed</span><span style=\"color:#B392F0\"> CircuitState</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> iota</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CircuitOpen</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CircuitHalfOpen</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewCircuitBreaker creates a circuit breaker with specified thresholds</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewCircuitBreaker</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">maxFailures</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">resetTimeout</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CircuitBreaker</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">CircuitBreaker</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        maxFailures:  maxFailures,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        resetTimeout: resetTimeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        state:        CircuitClosed,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Execute runs the provided operation through the circuit breaker</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">cb </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CircuitBreaker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Execute</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">operation</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">cb.</span><span style=\"color:#B392F0\">allowRequest</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> ErrCircuitBreakerOpen</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> operation</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cb.</span><span style=\"color:#B392F0\">recordResult</span><span style=\"color:#E1E4E8\">(err </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// allowRequest determines if the circuit breaker should allow the operation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">cb </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CircuitBreaker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">allowRequest</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cb.mutex.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> cb.mutex.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> cb.state </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> CircuitOpen {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Since</span><span style=\"color:#E1E4E8\">(cb.lastFailTime) </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> cb.resetTimeout {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            cb.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CircuitHalfOpen</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// recordResult updates circuit breaker state based on operation outcome</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">cb </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CircuitBreaker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">recordResult</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">success</span><span style=\"color:#F97583\"> bool</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cb.mutex.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> cb.mutex.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> success {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cb.failures </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> cb.state </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> CircuitHalfOpen {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            cb.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CircuitClosed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cb.failures</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cb.lastFailTime </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> cb.failures </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> cb.maxFailures {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            cb.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CircuitOpen</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Upload Flow Coordination (Core Logic Skeleton):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> upload</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">io</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// FlowCoordinator manages the interaction between components during upload operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> FlowCoordinator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sessionManager </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SessionManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storageBackend </span><span style=\"color:#B392F0\">StorageBackend</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fileValidator  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FileValidator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    circuitBreaker </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">coordinator</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">CircuitBreaker</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// InitializeUpload coordinates the upload initialization flow across components</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">fc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FlowCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">InitializeUpload</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">req</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">InitUploadRequest</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">InitUploadResponse</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Generate unique session ID using crypto/rand</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Call fileValidator.EnforceResourceLimits() to check size and type limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Call storageBackend.InitMultipart() to prepare storage resources</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Create UploadSession struct with metadata and set status to SessionStatusInitialized</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Call sessionManager.CreateSession() to persist session state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Generate upload URL and expiration time for chunk uploads</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Return InitUploadResponse with session details and upload parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use context.WithTimeout for each component call to prevent hanging</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ProcessChunkUpload coordinates chunk validation, storage, and session updates</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">fc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FlowCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ProcessChunkUpload</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">offset</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">data</span><span style=\"color:#B392F0\"> io</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Reader</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">checksum</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ChunkUploadResponse</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Call sessionManager.GetSession() to retrieve current session state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate session status is SessionStatusActive or SessionStatusInitialized</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Verify offset aligns with current session progress (no gaps or overlaps)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Read chunk data and compute hash to verify against provided checksum</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Call storageBackend.StoreChunk() through circuit breaker to persist data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Update session CurrentOffset and chunk tracking metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Call sessionManager.UpdateSession() to persist progress</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Check if upload is complete (offset + chunk size >= total size)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: If complete, trigger completion workflow asynchronously</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 10: Return ChunkUploadResponse with updated progress information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use defer to ensure session state is consistent even if storage fails</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CompleteUpload coordinates file assembly, validation, and final storage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">fc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FlowCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CompleteUpload</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CompletionResponse</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Call sessionManager.GetSession() and verify all chunks received</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Set session status to SessionStatusCompleting to prevent new chunks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Call storageBackend.CompleteMultipart() to assemble final file</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Create file reader from assembled storage and call fileValidator.ValidateFile()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: If validation fails, move file to quarantine using quarantineManager</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: If validation passes, move file to final storage location</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Set session status to SessionStatusCompleted or SessionStatusFailed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Call sessionManager.UpdateSession() with final state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: Generate signed download URL if file is clean</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 10: Return CompletionResponse with final file location and validation results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use database transactions if possible to ensure session state consistency</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Error Handling Utilities (Infrastructure Starter Code):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> upload</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">errors</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Error categories for proper handling and propagation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">var</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrTransient     </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"transient error\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrPermanent     </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"permanent error\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrDataCorruption </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"data corruption\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrSecurityViolation </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"security violation\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrResourceExhaustion </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"resource exhaustion\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrCircuitBreakerOpen </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"circuit breaker open\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// UploadError provides structured error information with context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> UploadError</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Category  </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Component </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Operation </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SessionID </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Cause     </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Retryable </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RetryAfter </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Error implements the error interface</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">UploadError</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\"> in </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">.</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\"> (session </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">): </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        e.Category, e.Component, e.Operation, e.SessionID, e.Cause)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Unwrap supports error unwrapping for errors.Is and errors.As</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">UploadError</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Unwrap</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> e.Cause</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// IsRetryable determines if the error should trigger retry logic</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">UploadError</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">IsRetryable</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> e.Retryable</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewTransientError creates a retryable error for temporary failures</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewTransientError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">component</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">operation</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">cause</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">UploadError</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">UploadError</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Category:   ErrTransient,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Component:  component,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Operation:  operation,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        SessionID:  sessionID,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Cause:      cause,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Retryable:  </span><span style=\"color:#79B8FF\">true</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        RetryAfter: </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewPermanentError creates a non-retryable error for permanent failures</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewPermanentError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">component</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">operation</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">cause</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">UploadError</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">UploadError</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Category:  ErrPermanent,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Component: component,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Operation: operation,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        SessionID: sessionID,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Cause:     cause,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Retryable: </span><span style=\"color:#79B8FF\">false</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Language-Specific Hints:</strong></p>\n<ul>\n<li>Use <code>context.Context</code> for all component interactions to enable timeout and cancellation</li>\n<li>Implement <code>io.Reader</code> interfaces for streaming chunk data without loading entire chunks in memory</li>\n<li>Use <code>sync.RWMutex</code> for protecting session state when supporting concurrent chunk uploads</li>\n<li>Leverage <code>errors.Is()</code> and <code>errors.As()</code> for proper error type checking and unwrapping</li>\n<li>Use <code>go</code> routines with proper error handling for asynchronous completion workflows</li>\n<li>Implement proper cleanup with <code>defer</code> statements to ensure resources are released on errors</li>\n</ul>\n<p><strong>Milestone Checkpoint:</strong></p>\n<p>After implementing component interactions, verify the flow coordination:</p>\n<ol>\n<li><strong>Run Integration Tests</strong>: <code>go test ./internal/upload/... -integration</code> should show successful component communication</li>\n<li><strong>Test Error Propagation</strong>: Send requests with invalid data and verify proper error responses with correct categories</li>\n<li><strong>Verify Circuit Breaker</strong>: Simulate backend failures and confirm circuit breaker prevents cascading failures</li>\n<li><strong>Check State Consistency</strong>: Kill the server during chunk upload and restart - session state should be recoverable</li>\n</ol>\n<p><strong>Expected Behavior</strong>: </p>\n<ul>\n<li>Upload initialization should return session metadata within 100ms for valid requests</li>\n<li>Chunk uploads should coordinate storage and session updates atomically</li>\n<li>Component failures should propagate with proper error categorization</li>\n<li>Circuit breakers should isolate failing backends without affecting healthy operations</li>\n</ul>\n<p><strong>Debugging Tips:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis Method</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Upload hangs indefinitely</td>\n<td>Missing context timeout in component calls</td>\n<td>Check logs for timeout warnings</td>\n<td>Add context.WithTimeout to all component calls</td>\n</tr>\n<tr>\n<td>Inconsistent session state</td>\n<td>Race condition in concurrent updates</td>\n<td>Run tests with <code>-race</code> flag</td>\n<td>Add proper locking around session state updates</td>\n</tr>\n<tr>\n<td>Circuit breaker stuck open</td>\n<td>Failure threshold too low for normal errors</td>\n<td>Monitor circuit breaker metrics</td>\n<td>Tune failure thresholds and reset timeouts</td>\n</tr>\n<tr>\n<td>Memory leaks during uploads</td>\n<td>Missing cleanup in error paths</td>\n<td>Use pprof to check goroutine and memory growth</td>\n<td>Add defer statements for resource cleanup</td>\n</tr>\n</tbody></table>\n<h2 id=\"error-handling-and-edge-cases\">Error Handling and Edge Cases</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (1, 2, 3) — error handling applies to chunked upload protocol robustness (M1), storage backend failure recovery (M2), and virus scanning reliability (M3)</p>\n</blockquote>\n<p>Think of a resumable file upload service as a <strong>distributed transaction coordinator</strong> managing multiple moving parts that can fail independently. Just as a flight has backup systems for navigation, hydraulics, and engines, our upload service needs comprehensive failure detection and recovery mechanisms for network interruptions, storage outages, and security scanning failures. The key insight is that failures are not exceptional cases but normal operating conditions that must be handled gracefully to maintain service reliability.</p>\n<p><img src=\"/api/project/file-upload-service/architecture-doc/asset?path=diagrams%2Ferror-handling-flow.svg\" alt=\"Error Detection and Recovery Flow\"></p>\n<p>The error handling strategy follows a <strong>layered defense approach</strong> where each component implements local error detection and recovery, while the system maintains global consistency through coordinated failure propagation. This section examines failure scenarios across network connectivity, storage backends, data integrity, and resource exhaustion, providing systematic detection mechanisms and recovery strategies for each category.</p>\n<h3 id=\"network-and-connection-failures\">Network and Connection Failures</h3>\n<p>Network failures represent the most common source of upload interruptions, ranging from temporary connectivity loss to client disconnections during large file transfers. The resumable upload protocol specifically addresses these scenarios by maintaining server-side state that survives network interruptions.</p>\n<p>Think of network failure handling as a <strong>checkpoint-based recovery system</strong> similar to video game save points. Each successfully uploaded chunk creates a checkpoint that allows the client to resume from that exact position after a network failure, rather than restarting the entire upload from the beginning.</p>\n<p><strong>Connection Failure Detection</strong></p>\n<p>The system implements multiple layers of connection failure detection to identify network issues promptly and initiate appropriate recovery actions:</p>\n<table>\n<thead>\n<tr>\n<th>Detection Method</th>\n<th>Trigger Condition</th>\n<th>Detection Time</th>\n<th>Recovery Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>TCP Keep-Alive</td>\n<td>Socket-level connection loss</td>\n<td>2-7.5 minutes (OS default)</td>\n<td>Immediate session cleanup</td>\n</tr>\n<tr>\n<td>HTTP Read Timeout</td>\n<td>No data received within timeout</td>\n<td>Configurable (30s-300s)</td>\n<td>Mark chunk as failed, preserve session</td>\n</tr>\n<tr>\n<td>Write Timeout</td>\n<td>Cannot send response within timeout</td>\n<td>Configurable (30s-300s)</td>\n<td>Log failure, client will retry</td>\n</tr>\n<tr>\n<td>Heartbeat Messages</td>\n<td>No client ping within interval</td>\n<td>60-120 seconds</td>\n<td>Mark session as potentially abandoned</td>\n</tr>\n<tr>\n<td>Chunk Upload Timeout</td>\n<td>Partial chunk data stalls</td>\n<td>Configurable per chunk size</td>\n<td>Abort current chunk, preserve progress</td>\n</tr>\n</tbody></table>\n<p><strong>Partial Upload Recovery Mechanisms</strong></p>\n<p>When network failures occur during chunk uploads, the system employs several recovery strategies based on the failure timing and completion status:</p>\n<ol>\n<li><p><strong>Pre-Transfer Failures</strong>: If the connection fails before chunk data transmission begins, the client simply retries the chunk upload with the same offset and metadata. The server treats this as a normal chunk upload request.</p>\n</li>\n<li><p><strong>Mid-Transfer Failures</strong>: When the connection drops during chunk data transmission, the server discards any partially received data and maintains the previous offset. The client queries the current offset and retries the complete chunk.</p>\n</li>\n<li><p><strong>Post-Transfer Failures</strong>: If the connection fails after chunk data is received but before the response is sent, the server has successfully stored the chunk but the client doesn&#39;t know. The client retry will receive a &quot;chunk already exists&quot; response with the updated offset.</p>\n</li>\n<li><p><strong>Checksum Mismatch Recovery</strong>: When chunk data arrives corrupted due to network issues, the server rejects the chunk and requests retransmission. The client maintains the original chunk data and can immediately retry without re-reading from the source file.</p>\n</li>\n</ol>\n<p><strong>Client Disconnection Handling</strong></p>\n<p>Long-running uploads face inevitable client disconnections from network changes, device sleep, or application restarts. The system handles these scenarios through session persistence and graceful degradation:</p>\n<blockquote>\n<p><strong>Decision: Session Persistence Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Clients may disconnect for extended periods (hours or days) during large uploads</li>\n<li><strong>Options Considered</strong>: <ul>\n<li>Immediate session cleanup on disconnection</li>\n<li>Fixed TTL-based session expiration  </li>\n<li>Adaptive TTL based on upload progress</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Implement adaptive session TTL with minimum 24-hour retention for active uploads</li>\n<li><strong>Rationale</strong>: Large files may take days to upload on slow connections; immediate cleanup loses significant progress; fixed TTL doesn&#39;t account for upload velocity</li>\n<li><strong>Consequences</strong>: Requires more storage for session state but dramatically improves user experience for large uploads</li>\n</ul>\n</blockquote>\n<p>The session persistence mechanism maintains upload state across disconnections through several components:</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Responsibility</th>\n<th>Persistence Duration</th>\n<th>Cleanup Trigger</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Session Metadata</td>\n<td>Upload ID, filename, total size, current offset</td>\n<td>Adaptive (24h-7days)</td>\n<td>Explicit expiration or completion</td>\n</tr>\n<tr>\n<td>Chunk Registry</td>\n<td>List of successfully uploaded chunks with hashes</td>\n<td>Same as session</td>\n<td>Session cleanup</td>\n</tr>\n<tr>\n<td>Storage Reservation</td>\n<td>Reserved space for incomplete upload</td>\n<td>Same as session</td>\n<td>Session cleanup or storage pressure</td>\n</tr>\n<tr>\n<td>Authentication Context</td>\n<td>Client credentials and permissions</td>\n<td>Shorter (1-4 hours)</td>\n<td>Token expiration or renewal</td>\n</tr>\n</tbody></table>\n<p><strong>Timeout Configuration and Adaptation</strong></p>\n<p>Network timeout values significantly impact both user experience and resource utilization. The system implements adaptive timeout strategies that adjust based on upload patterns and network conditions:</p>\n<ol>\n<li><p><strong>Initial Conservative Timeouts</strong>: New upload sessions start with conservative timeout values (30-60 seconds) to quickly detect connection issues and provide responsive error feedback.</p>\n</li>\n<li><p><strong>Progressive Timeout Increase</strong>: As chunks upload successfully, timeout values gradually increase (up to 300-600 seconds) to accommodate varying network conditions and larger chunk sizes.</p>\n</li>\n<li><p><strong>Regression on Failures</strong>: Network failures trigger timeout reduction to more aggressively detect subsequent issues, with gradual recovery as stability returns.</p>\n</li>\n<li><p><strong>Client-Specific Adaptation</strong>: The system maintains per-client timeout history to optimize values based on observed connection characteristics and geographic location.</p>\n</li>\n</ol>\n<p><strong>Connection Pool Management</strong></p>\n<p>Server-side connection pool management prevents resource exhaustion from failed connections while maintaining adequate capacity for active uploads:</p>\n<blockquote>\n<p><strong>Key Insight</strong>: Failed connections often remain in system buffers for extended periods, consuming file descriptors and memory. Proactive connection cleanup is essential for service stability.</p>\n</blockquote>\n<p>The connection pool implements several management strategies:</p>\n<ul>\n<li><strong>Active Connection Monitoring</strong>: Periodic health checks identify stuck or zombie connections for immediate cleanup</li>\n<li><strong>Resource-Based Throttling</strong>: New connections are rejected when system resources (file descriptors, memory, CPU) approach critical thresholds</li>\n<li><strong>Client Rate Limiting</strong>: Per-client connection limits prevent individual users from exhausting system capacity</li>\n<li><strong>Graceful Degradation</strong>: Under high load, the system prioritizes completing existing uploads over accepting new connections</li>\n</ul>\n<h3 id=\"storage-backend-failures\">Storage Backend Failures</h3>\n<p>Storage backend failures can occur at multiple levels, from temporary network issues accessing cloud storage to permanent hardware failures in local storage systems. The abstracted storage interface enables uniform error handling across different backend types while allowing backend-specific optimizations.</p>\n<p>Think of storage backend failures as <strong>supply chain disruptions</strong> in a manufacturing system. Just as factories maintain multiple suppliers and inventory buffers, the upload service must handle storage unavailability through redundancy, retries, and graceful degradation strategies.</p>\n<p><strong>Backend Availability Monitoring</strong></p>\n<p>The system implements comprehensive health monitoring for all configured storage backends to detect failures promptly and route traffic appropriately:</p>\n<table>\n<thead>\n<tr>\n<th>Health Check Type</th>\n<th>Frequency</th>\n<th>Failure Threshold</th>\n<th>Recovery Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Connectivity Test</td>\n<td>Every 30 seconds</td>\n<td>3 consecutive failures</td>\n<td>Mark backend unhealthy</td>\n</tr>\n<tr>\n<td>Write Operation Test</td>\n<td>Every 2 minutes</td>\n<td>2 failures in 5 minutes</td>\n<td>Circuit breaker activation</td>\n</tr>\n<tr>\n<td>Read Operation Test</td>\n<td>Every 2 minutes</td>\n<td>2 failures in 5 minutes</td>\n<td>Read-only mode if applicable</td>\n</tr>\n<tr>\n<td>Latency Monitoring</td>\n<td>Per request</td>\n<td>99th percentile &gt; 10s</td>\n<td>Performance degradation alert</td>\n</tr>\n<tr>\n<td>Error Rate Monitoring</td>\n<td>Per minute</td>\n<td>&gt;5% error rate</td>\n<td>Automatic failover consideration</td>\n</tr>\n</tbody></table>\n<p><strong>Circuit Breaker Implementation</strong></p>\n<p>The circuit breaker pattern prevents cascading failures by temporarily isolating failing storage backends while allowing rapid recovery when service is restored:</p>\n<table>\n<thead>\n<tr>\n<th>Circuit State</th>\n<th>Request Handling</th>\n<th>Transition Condition</th>\n<th>Duration</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Closed</td>\n<td>All requests forwarded to backend</td>\n<td>Normal operation</td>\n<td>Indefinite</td>\n</tr>\n<tr>\n<td>Open</td>\n<td>All requests immediately fail</td>\n<td>Failure threshold exceeded</td>\n<td>30-300 seconds</td>\n</tr>\n<tr>\n<td>Half-Open</td>\n<td>Limited requests forwarded to test recovery</td>\n<td>Timeout expires</td>\n<td>Until success/failure determined</td>\n</tr>\n</tbody></table>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Circuit Breaker State Transitions:\nClosed → Open: When error rate exceeds threshold (e.g., 50% failures in 1 minute)\nOpen → Half-Open: After timeout period expires (exponential backoff)\nHalf-Open → Closed: When test requests succeed consistently\nHalf-Open → Open: When test requests continue to fail</code></pre></div>\n\n<p><strong>Storage Backend Failover Strategies</strong></p>\n<p>When primary storage backends become unavailable, the system implements several failover strategies based on configuration and available alternatives:</p>\n<blockquote>\n<p><strong>Decision: Multi-Backend Failover Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Cloud storage services experience periodic outages; single backend creates service disruption</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>No failover (fail uploads during outages)</li>\n<li>Automatic failover to secondary backend</li>\n<li>Client-selectable backend with fallback</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Implement automatic failover with configurable priority order</li>\n<li><strong>Rationale</strong>: Users shouldn&#39;t experience upload failures due to backend issues they can&#39;t control; automatic failover maintains service availability</li>\n<li><strong>Consequences</strong>: Requires cross-backend upload session migration and consistent metadata management</li>\n</ul>\n</blockquote>\n<p>The failover mechanism operates through several strategies:</p>\n<ol>\n<li><p><strong>Primary-Secondary Failover</strong>: When the primary backend fails, new uploads automatically route to the secondary backend. Existing uploads attempt to continue on the primary with exponential backoff retry.</p>\n</li>\n<li><p><strong>Load-Based Distribution</strong>: Under high load or when backends show performance degradation, new uploads distribute across available backends based on capacity and response times.</p>\n</li>\n<li><p><strong>Geographic Failover</strong>: For global deployments, uploads can failover to backends in different regions when local storage becomes unavailable.</p>\n</li>\n<li><p><strong>Partial Failure Handling</strong>: When some operations succeed but others fail (e.g., writes succeed but reads fail), the system maintains separate health states for different operation types.</p>\n</li>\n</ol>\n<p><strong>Data Consistency During Backend Failures</strong></p>\n<p>Storage backend failures can create consistency challenges, especially for multipart uploads where chunks may be stored across multiple backend instances or during failover scenarios:</p>\n<table>\n<thead>\n<tr>\n<th>Consistency Challenge</th>\n<th>Detection Method</th>\n<th>Resolution Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Partial Chunk Loss</td>\n<td>Chunk verification on upload completion</td>\n<td>Re-upload missing chunks from session state</td>\n</tr>\n<tr>\n<td>Cross-Backend State Drift</td>\n<td>Periodic reconciliation scans</td>\n<td>Authoritative state in session store</td>\n</tr>\n<tr>\n<td>Failed Multipart Assembly</td>\n<td>Assembly operation timeout/error</td>\n<td>Retry with exponential backoff, manual recovery</td>\n</tr>\n<tr>\n<td>Orphaned Storage Objects</td>\n<td>Cleanup job comparing session state to storage</td>\n<td>Automated cleanup with safety delays</td>\n</tr>\n</tbody></table>\n<p><strong>Quota and Space Management</strong></p>\n<p>Storage backends impose various limits that can cause upload failures when exceeded. The system implements proactive quota monitoring and space management:</p>\n<ol>\n<li><p><strong>Quota Monitoring</strong>: Regular checks of available storage quota across all backends, with alerts when approaching limits (e.g., 80% capacity).</p>\n</li>\n<li><p><strong>Predictive Space Allocation</strong>: Before accepting large uploads, the system verifies sufficient space exists, accounting for multipart upload overhead and concurrent uploads.</p>\n</li>\n<li><p><strong>Cleanup Job Integration</strong>: Automated cleanup of expired uploads and temporary files helps maintain available space and avoid quota exhaustion.</p>\n</li>\n<li><p><strong>Emergency Space Recovery</strong>: When approaching storage limits, the system can temporarily reject new uploads while completing in-progress transfers and running emergency cleanup.</p>\n</li>\n</ol>\n<p><strong>Backend-Specific Error Handling</strong></p>\n<p>Different storage backends exhibit unique failure modes that require specialized handling approaches:</p>\n<p><strong>Local Filesystem Errors</strong>:</p>\n<ul>\n<li><strong>Disk Full</strong>: Monitor available space and reject uploads when insufficient</li>\n<li><strong>Permission Issues</strong>: Validate write permissions during service startup</li>\n<li><strong>File System Corruption</strong>: Implement integrity checks and automatic repair where possible</li>\n<li><strong>Path Traversal Attacks</strong>: Sanitize all file paths and implement chroot-style restrictions</li>\n</ul>\n<p><strong>S3-Compatible Storage Errors</strong>:</p>\n<ul>\n<li><strong>Authentication Failures</strong>: Implement credential refresh and rotation mechanisms</li>\n<li><strong>Rate Limiting</strong>: Implement exponential backoff with jitter for 429 responses</li>\n<li><strong>Eventual Consistency Issues</strong>: Account for read-after-write consistency delays</li>\n<li><strong>Multipart Upload Limits</strong>: Handle part number limits (10,000 parts) and minimum part size requirements (5MB)</li>\n</ul>\n<p><strong>Network Storage Errors</strong>:</p>\n<ul>\n<li><strong>Mount Point Failures</strong>: Detect and attempt remount of network file systems</li>\n<li><strong>Network Partitions</strong>: Implement timeout and retry logic for network-attached storage</li>\n<li><strong>Protocol-Specific Issues</strong>: Handle NFS, SMB, or other protocol-specific error conditions</li>\n</ul>\n<h3 id=\"data-corruption-detection-and-recovery\">Data Corruption Detection and Recovery</h3>\n<p>Data corruption can occur at multiple points in the upload pipeline, from network transmission errors to storage media failures. The system implements comprehensive integrity checking and recovery mechanisms to ensure uploaded files maintain perfect fidelity.</p>\n<p>Think of data integrity as a <strong>chain of custody system</strong> used in forensic investigations. Each step in the process must verify that data remains unchanged from the previous step, with documented evidence (checksums) proving integrity throughout the entire pipeline.</p>\n<p><strong>Checksum Validation Pipeline</strong></p>\n<p>The integrity verification system operates through multiple checksum layers, each serving specific purposes in the upload pipeline:</p>\n<table>\n<thead>\n<tr>\n<th>Checksum Level</th>\n<th>Algorithm</th>\n<th>Computed By</th>\n<th>Verified By</th>\n<th>Failure Response</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Chunk-Level</td>\n<td>SHA-256</td>\n<td>Client before upload</td>\n<td>Server on chunk receipt</td>\n<td>Reject chunk, request re-upload</td>\n</tr>\n<tr>\n<td>Transport-Level</td>\n<td>MD5 (HTTP)</td>\n<td>HTTP stack</td>\n<td>HTTP stack</td>\n<td>Automatic retry by HTTP client</td>\n</tr>\n<tr>\n<td>Storage-Level</td>\n<td>Backend-specific (ETag)</td>\n<td>Storage backend</td>\n<td>Server on storage completion</td>\n<td>Re-upload chunk to storage</td>\n</tr>\n<tr>\n<td>File-Level</td>\n<td>SHA-256</td>\n<td>Server during assembly</td>\n<td>Client after download</td>\n<td>Re-assemble from chunks</td>\n</tr>\n<tr>\n<td>End-to-End</td>\n<td>User-provided hash</td>\n<td>Client of original file</td>\n<td>Server after assembly</td>\n<td>Report corruption to client</td>\n</tr>\n</tbody></table>\n<p><strong>Corruption Detection Strategies</strong></p>\n<p>The system employs multiple corruption detection methods to identify data integrity issues at different stages of the upload process:</p>\n<ol>\n<li><p><strong>Real-Time Detection</strong>: Checksum verification occurs immediately when chunks are received, allowing for immediate re-upload requests while the client still has the source data readily available.</p>\n</li>\n<li><p><strong>Batch Verification</strong>: Periodic integrity scans of stored data detect corruption from storage media degradation or other backend issues that occur after successful upload.</p>\n</li>\n<li><p><strong>Cross-Reference Validation</strong>: Comparison of chunk checksums in session metadata against actual stored chunk checksums identifies discrepancies from storage backend issues.</p>\n</li>\n<li><p><strong>Assembly Verification</strong>: Final file assembly includes comprehensive integrity checking that validates both individual chunks and the complete assembled file.</p>\n</li>\n</ol>\n<p><strong>Chunk-Level Corruption Handling</strong></p>\n<p>Individual chunk corruption represents the most common data integrity issue, typically caused by network transmission errors or storage write failures:</p>\n<blockquote>\n<p><strong>Decision: Chunk Corruption Recovery Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Network transmission can corrupt chunk data; storage writes can fail partially</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Immediate chunk rejection with full re-upload</li>\n<li>Partial corruption repair using error correction codes</li>\n<li>Best-effort storage with corruption flagging</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Immediate rejection with full chunk re-upload required</li>\n<li><strong>Rationale</strong>: File integrity is non-negotiable; error correction adds complexity without guaranteeing perfect recovery; full re-upload is simple and reliable</li>\n<li><strong>Consequences</strong>: Higher network utilization for corrupted chunks but guaranteed data integrity</li>\n</ul>\n</blockquote>\n<p>The chunk corruption recovery process follows these steps:</p>\n<ol>\n<li><p><strong>Corruption Detection</strong>: Server computes SHA-256 hash of received chunk data and compares against client-provided hash in the upload request.</p>\n</li>\n<li><p><strong>Immediate Rejection</strong>: Corrupted chunks are immediately rejected with a specific error response indicating checksum mismatch and the computed vs. expected hash values.</p>\n</li>\n<li><p><strong>State Preservation</strong>: The upload session offset remains unchanged, allowing the client to retry the same chunk without affecting subsequent chunk ordering.</p>\n</li>\n<li><p><strong>Client Retry Logic</strong>: Clients implement exponential backoff retry with a maximum retry count (typically 3-5 attempts) before reporting permanent failure.</p>\n</li>\n<li><p><strong>Corruption Logging</strong>: All corruption events are logged with detailed metadata for analysis of network quality and potential systematic issues.</p>\n</li>\n</ol>\n<p><strong>File Assembly Integrity Verification</strong></p>\n<p>When all chunks have been uploaded successfully, the file assembly process includes comprehensive integrity verification to ensure the complete file matches client expectations:</p>\n<table>\n<thead>\n<tr>\n<th>Verification Step</th>\n<th>Check Performed</th>\n<th>Failure Response</th>\n<th>Recovery Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Chunk Completeness</td>\n<td>All required chunks present</td>\n<td>Assembly failure</td>\n<td>Request missing chunks</td>\n</tr>\n<tr>\n<td>Chunk Order Verification</td>\n<td>Chunks assemble in correct sequence</td>\n<td>Assembly failure</td>\n<td>Re-upload out-of-order chunks</td>\n</tr>\n<tr>\n<td>Individual Chunk Re-verification</td>\n<td>Re-compute chunk hashes from storage</td>\n<td>Assembly failure</td>\n<td>Re-upload corrupted chunks</td>\n</tr>\n<tr>\n<td>Complete File Hash</td>\n<td>Compute final file SHA-256</td>\n<td>Assembly failure</td>\n<td>Full re-assembly attempt</td>\n</tr>\n<tr>\n<td>Size Verification</td>\n<td>Final file size matches expected</td>\n<td>Assembly failure</td>\n<td>Investigate chunk size discrepancies</td>\n</tr>\n</tbody></table>\n<p><strong>Storage-Level Corruption Recovery</strong></p>\n<p>Storage backends can experience corruption after successful writes due to media degradation, bit rot, or system failures. The service implements several strategies to detect and recover from storage-level corruption:</p>\n<ol>\n<li><p><strong>Periodic Integrity Scans</strong>: Background jobs periodically verify stored chunk integrity by re-computing checksums and comparing against session metadata.</p>\n</li>\n<li><p><strong>Read Verification</strong>: When chunks are read for assembly or download, their checksums are verified to detect corruption that occurred after storage.</p>\n</li>\n<li><p><strong>Cross-Backend Verification</strong>: For deployments with multiple storage backends, chunk integrity can be verified by comparing checksums across different backend instances.</p>\n</li>\n<li><p><strong>Redundant Storage</strong>: Critical uploads can be stored redundantly across multiple backends, allowing corruption recovery through cross-reference with uncorrupted copies.</p>\n</li>\n</ol>\n<p><strong>Client-Side Corruption Detection</strong></p>\n<p>Clients play a crucial role in end-to-end integrity verification by maintaining original file checksums and verifying completed uploads:</p>\n<ul>\n<li><strong>Pre-Upload Checksumming</strong>: Clients compute complete file hashes before beginning uploads and provide these to the server for final verification.</li>\n<li><strong>Post-Assembly Verification</strong>: After upload completion, clients can download and verify the assembled file matches the original.</li>\n<li><strong>Chunk-Level Client Verification</strong>: Clients maintain chunk-level checksums computed from the original file to verify server-assembled chunks match expected values.</li>\n</ul>\n<p><strong>Corruption Analytics and Prevention</strong></p>\n<p>The system maintains detailed analytics on corruption patterns to identify systematic issues and improve prevention:</p>\n<table>\n<thead>\n<tr>\n<th>Metric Category</th>\n<th>Tracked Data</th>\n<th>Analysis Purpose</th>\n<th>Prevention Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Network Corruption Rates</td>\n<td>Per-client, per-network corruption frequency</td>\n<td>Identify problematic networks/clients</td>\n<td>Client-specific retry policies</td>\n</tr>\n<tr>\n<td>Storage Backend Corruption</td>\n<td>Per-backend corruption rates over time</td>\n<td>Detect failing storage systems</td>\n<td>Backend health scoring</td>\n</tr>\n<tr>\n<td>File Type Correlation</td>\n<td>Corruption rates by file type/size</td>\n<td>Identify systematic issues</td>\n<td>Specialized handling for problematic types</td>\n</tr>\n<tr>\n<td>Geographic Correlation</td>\n<td>Corruption rates by client location</td>\n<td>Network infrastructure quality</td>\n<td>Regional optimization strategies</td>\n</tr>\n</tbody></table>\n<h3 id=\"resource-exhaustion-scenarios\">Resource Exhaustion Scenarios</h3>\n<p>Resource exhaustion represents a critical threat to service availability, potentially causing cascading failures that affect all users. The system must implement comprehensive resource monitoring, predictive limiting, and graceful degradation to maintain service quality under resource pressure.</p>\n<p>Think of resource management as <strong>air traffic control</strong> at a busy airport. Just as controllers must manage runway capacity, gate availability, and airspace congestion to prevent dangerous situations, the upload service must coordinate CPU, memory, disk space, and network bandwidth to prevent system overload while maximizing throughput.</p>\n<p><strong>Resource Monitoring and Thresholds</strong></p>\n<p>The system implements multi-layered resource monitoring with adaptive thresholds that trigger progressively more restrictive policies as resource pressure increases:</p>\n<table>\n<thead>\n<tr>\n<th>Resource Type</th>\n<th>Warning Threshold</th>\n<th>Critical Threshold</th>\n<th>Emergency Threshold</th>\n<th>Recovery Threshold</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Memory Usage</td>\n<td>70% of available</td>\n<td>85% of available</td>\n<td>95% of available</td>\n<td>Below 60%</td>\n</tr>\n<tr>\n<td>Disk Space</td>\n<td>80% of capacity</td>\n<td>90% of capacity</td>\n<td>95% of capacity</td>\n<td>Below 70%</td>\n</tr>\n<tr>\n<td>CPU Utilization</td>\n<td>70% sustained</td>\n<td>85% sustained</td>\n<td>95% sustained</td>\n<td>Below 60%</td>\n</tr>\n<tr>\n<td>File Descriptors</td>\n<td>70% of limit</td>\n<td>85% of limit</td>\n<td>95% of limit</td>\n<td>Below 60%</td>\n</tr>\n<tr>\n<td>Concurrent Uploads</td>\n<td>70% of capacity</td>\n<td>85% of capacity</td>\n<td>95% of capacity</td>\n<td>Below 60%</td>\n</tr>\n<tr>\n<td>Network Bandwidth</td>\n<td>80% of capacity</td>\n<td>90% of capacity</td>\n<td>95% of capacity</td>\n<td>Below 70%</td>\n</tr>\n</tbody></table>\n<p><strong>Memory Pressure Management</strong></p>\n<p>Memory exhaustion can occur rapidly during high-concurrency upload scenarios, especially when processing large chunks or maintaining extensive session state. The system implements several memory management strategies:</p>\n<blockquote>\n<p><strong>Decision: Memory Management Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Large file uploads with high concurrency can exhaust available memory</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Unlimited memory usage with OS swap handling</li>\n<li>Fixed memory limits per upload session</li>\n<li>Dynamic memory allocation with global limits and spillover</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Implement dynamic allocation with global limits, memory pooling, and disk spillover</li>\n<li><strong>Rationale</strong>: Fixed limits waste memory during low load; unlimited usage causes system instability; dynamic allocation optimizes utilization while maintaining stability</li>\n<li><strong>Consequences</strong>: More complex memory management but better resource utilization and system stability</li>\n</ul>\n</blockquote>\n<p><strong>Memory Management Components</strong>:</p>\n<ol>\n<li><p><strong>Chunk Buffer Pooling</strong>: Pre-allocated buffer pools for chunk processing reduce garbage collection pressure and memory fragmentation. Buffers are reused across multiple upload operations.</p>\n</li>\n<li><p><strong>Session State Compression</strong>: Large session metadata is compressed when stored in memory, with decompression only when accessed. This reduces memory footprint for dormant sessions.</p>\n</li>\n<li><p><strong>Disk Spillover</strong>: When memory pressure increases, less frequently accessed session state spills to disk storage, maintaining system responsiveness while preserving session data.</p>\n</li>\n<li><p><strong>Gradual Memory Reclamation</strong>: Instead of abrupt memory cleanup, the system gradually reclaims memory by expiring oldest inactive sessions first, avoiding performance spikes.</p>\n</li>\n</ol>\n<p><strong>Disk Space Exhaustion Handling</strong></p>\n<p>Disk space exhaustion can occur from multiple sources: large uploads consuming available space, failed cleanup of temporary files, or log file growth. The system implements proactive space management:</p>\n<table>\n<thead>\n<tr>\n<th>Space Management Strategy</th>\n<th>Trigger Condition</th>\n<th>Action Taken</th>\n<th>Expected Recovery</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Proactive Cleanup</td>\n<td>80% disk usage</td>\n<td>Clean expired sessions, temp files</td>\n<td>5-15% space recovery</td>\n</tr>\n<tr>\n<td>Upload Throttling</td>\n<td>85% disk usage</td>\n<td>Reduce concurrent uploads by 50%</td>\n<td>Prevent further exhaustion</td>\n</tr>\n<tr>\n<td>New Upload Rejection</td>\n<td>90% disk usage</td>\n<td>Reject new uploads, continue existing</td>\n<td>Stabilize space usage</td>\n</tr>\n<tr>\n<td>Emergency Cleanup</td>\n<td>95% disk usage</td>\n<td>Aggressive cleanup, oldest sessions first</td>\n<td>10-30% space recovery</td>\n</tr>\n<tr>\n<td>Service Degradation</td>\n<td>98% disk usage</td>\n<td>Read-only mode, complete existing uploads only</td>\n<td>System protection</td>\n</tr>\n</tbody></table>\n<p><strong>Disk Space Recovery Procedures</strong>:</p>\n<ol>\n<li><p><strong>Expired Session Cleanup</strong>: Automatically remove sessions that exceed configured TTL, freeing associated chunk storage and metadata.</p>\n</li>\n<li><p><strong>Orphaned File Detection</strong>: Background jobs identify storage files not associated with active sessions and mark them for cleanup after safety delays.</p>\n</li>\n<li><p><strong>Log Rotation and Compression</strong>: Automated log file rotation with compression and archival to prevent log storage from consuming excessive space.</p>\n</li>\n<li><p><strong>Temporary File Cleanup</strong>: Removal of temporary files created during upload processing, assembly operations, and virus scanning activities.</p>\n</li>\n</ol>\n<p><strong>CPU Resource Management</strong></p>\n<p>High CPU utilization can occur during intensive operations like virus scanning, file assembly, or cryptographic operations. The system implements CPU resource management through several mechanisms:</p>\n<ol>\n<li><p><strong>Operation Prioritization</strong>: Critical operations (chunk uploads) receive higher priority than background operations (cleanup, virus scanning) to maintain upload throughput.</p>\n</li>\n<li><p><strong>Throttling and Rate Limiting</strong>: CPU-intensive operations are throttled based on system load, with automatic scaling of operation rates based on available CPU capacity.</p>\n</li>\n<li><p><strong>Asynchronous Processing</strong>: Heavy operations like virus scanning and file assembly are processed asynchronously to avoid blocking upload operations.</p>\n</li>\n<li><p><strong>Load Shedding</strong>: Under extreme CPU pressure, non-essential operations are temporarily suspended to maintain core upload functionality.</p>\n</li>\n</ol>\n<p><strong>Connection and File Descriptor Limits</strong></p>\n<p>Network connection and file descriptor exhaustion can cause immediate service failure. The system implements comprehensive connection management:</p>\n<blockquote>\n<p><strong>Key Insight</strong>: File descriptor exhaustion is often the first resource limit reached under high load, but it&#39;s frequently overlooked in capacity planning. Proactive monitoring and management are essential.</p>\n</blockquote>\n<p><strong>Connection Management Strategies</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Strategy</th>\n<th>Implementation</th>\n<th>Purpose</th>\n<th>Resource Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Connection Pooling</td>\n<td>Reuse connections for multiple operations</td>\n<td>Reduce overhead</td>\n<td>Lower file descriptor usage</td>\n</tr>\n<tr>\n<td>Idle Connection Cleanup</td>\n<td>Close connections idle for &gt;5 minutes</td>\n<td>Free resources</td>\n<td>Immediate descriptor recovery</td>\n</tr>\n<tr>\n<td>Per-Client Limits</td>\n<td>Maximum 10 concurrent connections per client</td>\n<td>Prevent abuse</td>\n<td>Bounded resource usage</td>\n</tr>\n<tr>\n<td>Global Connection Limits</td>\n<td>Hard limit at 80% of system maximum</td>\n<td>System protection</td>\n<td>Guaranteed descriptor availability</td>\n</tr>\n</tbody></table>\n<p><strong>Cascading Failure Prevention</strong></p>\n<p>Resource exhaustion in one area can trigger cascading failures throughout the system. The service implements several patterns to prevent failure propagation:</p>\n<ol>\n<li><p><strong>Circuit Breaker Integration</strong>: Resource exhaustion triggers circuit breakers that prevent additional load while recovery occurs.</p>\n</li>\n<li><p><strong>Bulkhead Pattern</strong>: Different resource types are isolated so exhaustion in one area (e.g., CPU) doesn&#39;t immediately cause failures in others (e.g., memory).</p>\n</li>\n<li><p><strong>Graceful Degradation</strong>: Rather than complete service failure, the system reduces functionality progressively, maintaining core upload capabilities while disabling non-essential features.</p>\n</li>\n<li><p><strong>Recovery Coordination</strong>: Resource recovery is coordinated across components to prevent thundering herd problems when resources become available again.</p>\n</li>\n</ol>\n<p><strong>Adaptive Rate Limiting</strong></p>\n<p>Traditional fixed rate limiting doesn&#39;t account for varying system capacity under different load conditions. The system implements adaptive rate limiting that adjusts based on resource availability:</p>\n<ul>\n<li><strong>Dynamic Limits</strong>: Upload rate limits adjust based on current system resource utilization, allowing higher throughput when resources are available.</li>\n<li><strong>Client-Specific Adaptation</strong>: Rate limits adapt based on individual client behavior, resource consumption patterns, and historical usage.</li>\n<li><strong>Progressive Restriction</strong>: As resource pressure increases, rate limits become progressively more restrictive, with different limits for new vs. existing uploads.</li>\n<li><strong>Quick Recovery</strong>: When resource pressure decreases, rate limits relax quickly to restore full service capacity.</li>\n</ul>\n<p><strong>Resource Exhaustion Recovery</strong></p>\n<p>When resource exhaustion occurs, the system implements systematic recovery procedures to restore normal operation:</p>\n<ol>\n<li><p><strong>Immediate Stabilization</strong>: Emergency measures to prevent complete system failure, including aggressive cleanup and load shedding.</p>\n</li>\n<li><p><strong>Gradual Load Restoration</strong>: Careful restoration of normal operation to avoid immediate re-exhaustion, with monitoring for resource utilization trends.</p>\n</li>\n<li><p><strong>Root Cause Analysis</strong>: Automated analysis of exhaustion causes, including identification of problematic clients, unusual usage patterns, or system configuration issues.</p>\n</li>\n<li><p><strong>Preventive Measures</strong>: Implementation of additional safeguards based on exhaustion analysis, including enhanced monitoring, adjusted limits, or architectural improvements.</p>\n</li>\n</ol>\n<p>⚠️ <strong>Pitfall: Ignoring Resource Interdependencies</strong>\nMany developers treat resource limits independently, but resource exhaustion often involves complex interdependencies. For example, memory pressure can cause increased disk I/O as the system swaps, which increases CPU usage for I/O wait, which can cause network timeouts due to delayed responses. Always consider how resource exhaustion in one area affects other resources and implement holistic monitoring and management strategies.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Recovery Testing</strong>\nResource exhaustion scenarios are often untested because they&#39;re difficult to reproduce in development environments. This leads to poor recovery behavior in production. Implement chaos engineering practices that artificially induce resource pressure during testing to validate recovery mechanisms work correctly under actual stress conditions.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p><strong>A. Technology Recommendations Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Circuit Breaker</td>\n<td>Simple failure counter with timeout</td>\n<td>Netflix Hystrix pattern with metrics</td>\n</tr>\n<tr>\n<td>Resource Monitoring</td>\n<td>Basic system metrics (CPU, memory)</td>\n<td>Prometheus with custom metrics and alerting</td>\n</tr>\n<tr>\n<td>Error Logging</td>\n<td>Standard logging with error levels</td>\n<td>Structured logging with correlation IDs</td>\n</tr>\n<tr>\n<td>Health Checks</td>\n<td>HTTP endpoint with basic status</td>\n<td>Comprehensive health with dependency checks</td>\n</tr>\n<tr>\n<td>Retry Logic</td>\n<td>Exponential backoff with jitter</td>\n<td>Adaptive retry with success rate tracking</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/\n  errors/\n    errors.go              ← error type definitions and constructors\n    circuit.go             ← circuit breaker implementation  \n    recovery.go            ← error recovery strategies\n  monitoring/\n    resources.go           ← resource monitoring and thresholds\n    health.go             ← health check implementations\n  storage/\n    errors.go             ← storage-specific error handling\n    retry.go              ← storage operation retry logic\n  validation/\n    corruption.go         ← data integrity verification\n    recovery.go           ← corruption recovery procedures</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code:</strong></p>\n<p><strong>Error Type System (Complete Implementation):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Package errors provides comprehensive error handling for the upload service</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> errors</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// UploadError represents all errors in the upload service with categorization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> UploadError</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Category    </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Component   </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Operation   </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SessionID   </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Cause       </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Retryable   </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RetryAfter  </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Metadata    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">UploadError</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"[</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">/</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">] </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\"> (session: </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">): </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        e.Component, e.Operation, e.Category, e.SessionID, e.Cause)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">UploadError</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Unwrap</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> e.Cause }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Error categories for systematic handling</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">var</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrTransient         </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"transient error\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrPermanent         </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"permanent error\"</span><span style=\"color:#E1E4E8\">) </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrDataCorruption    </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"data corruption\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrSecurityViolation </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"security violation\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrResourceExhaustion </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"resource exhaustion\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Storage-specific errors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">var</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrNotFound              </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"key not found\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrAlreadyExists         </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"key already exists\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrInsufficientSpace     </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"insufficient storage space\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrNetworkTimeout        </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"network operation timeout\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrAuthenticationFailed  </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"authentication failed\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrInvalidMultipartState </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"invalid multipart upload state\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// System-level errors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">var</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrFileTooLarge      </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"file exceeds size limits\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrSystemOverloaded  </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"system resources critically low\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrVirusScanTimeout  </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"virus scanning operation timed out\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrQuarantineRequired </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"file must be quarantined\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewUploadError creates a categorized error with context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewUploadError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">category</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">component</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">operation</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">cause</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">UploadError</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">UploadError</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Category:  category,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Component: component,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Operation: operation,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        SessionID: sessionID,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Cause:     cause,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Retryable: </span><span style=\"color:#B392F0\">isRetryable</span><span style=\"color:#E1E4E8\">(category, cause),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Metadata:  </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Timestamp: time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// isRetryable determines if an error should trigger retry logic</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> isRetryable</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">category</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">cause</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    switch</span><span style=\"color:#E1E4E8\"> category {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> ErrTransient, ErrNetworkTimeout:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> ErrPermanent, ErrSecurityViolation:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> ErrResourceExhaustion:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> true</span><span style=\"color:#6A737D\"> // with backoff</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> ErrDataCorruption:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> true</span><span style=\"color:#6A737D\"> // chunk re-upload</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    default</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Circuit Breaker Implementation (Complete):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// CircuitState represents the current state of the circuit breaker</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CircuitState</span><span style=\"color:#F97583\"> int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CircuitClosed</span><span style=\"color:#B392F0\"> CircuitState</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> iota</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CircuitOpen</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CircuitHalfOpen</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">var</span><span style=\"color:#E1E4E8\"> ErrCircuitBreakerOpen </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"circuit breaker open\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CircuitBreaker implements the circuit breaker pattern for storage backends</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CircuitBreaker</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    maxFailures   </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resetTimeout  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    failures      </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lastFailTime  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    state         </span><span style=\"color:#B392F0\">CircuitState</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex         </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    onStateChange </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">from</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">to</span><span style=\"color:#B392F0\"> CircuitState</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewCircuitBreaker creates a circuit breaker with specified thresholds</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewCircuitBreaker</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">maxFailures</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">resetTimeout</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CircuitBreaker</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">CircuitBreaker</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        maxFailures:  maxFailures,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        resetTimeout: resetTimeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        state:        CircuitClosed,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Execute runs an operation through the circuit breaker</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">cb </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CircuitBreaker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Execute</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">operation</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check current circuit state and return immediately if open</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For half-open state, allow limited requests through</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Execute the operation and handle success/failure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update circuit state based on operation result</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Trigger state change callbacks if state transitions occur</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use cb.mutex for thread-safe state updates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Check if reset timeout has elapsed for open -> half-open transition</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Resource Monitor (Complete Implementation):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> monitoring</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">runtime</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync/atomic</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">syscall</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ResourceMonitor tracks system resource utilization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ResourceMonitor</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    memoryWarning    </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    memoryCritical   </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    diskWarning      </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    diskCritical     </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    activeUploads    </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    maxUploads       </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lastCheck        </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    checkInterval    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ResourceStatus represents current resource utilization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ResourceStatus</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MemoryUsed        </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MemoryTotal       </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MemoryPercent     </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DiskUsed          </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DiskTotal         </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DiskPercent       </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ActiveUploads     </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxUploads        </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CPUPercent        </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    FileDescriptors   </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxFileDescriptors </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewResourceMonitor creates a resource monitor with default thresholds</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewResourceMonitor</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ResourceMonitor</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ResourceMonitor</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        checkInterval: </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        maxUploads:    </span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#6A737D\">// configurable</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetCurrentStatus returns current resource utilization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ResourceMonitor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetCurrentStatus</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ResourceStatus</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> memStats </span><span style=\"color:#B392F0\">runtime</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">MemStats</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    runtime.</span><span style=\"color:#B392F0\">ReadMemStats</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">memStats)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> diskStat </span><span style=\"color:#B392F0\">syscall</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Statfs_t</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    syscall.</span><span style=\"color:#B392F0\">Statfs</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">diskStat)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ResourceStatus</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        MemoryUsed:    </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">(memStats.Alloc),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        MemoryTotal:   </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">(memStats.Sys),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        MemoryPercent: </span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\">(memStats.Alloc) </span><span style=\"color:#F97583\">/</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">(memStats.Sys) </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        DiskUsed:      </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">(diskStat.Blocks</span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\">diskStat.Bavail) </span><span style=\"color:#F97583\">*</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">(diskStat.Bsize),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        DiskTotal:     </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">(diskStat.Blocks) </span><span style=\"color:#F97583\">*</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">(diskStat.Bsize),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ActiveUploads: atomic.</span><span style=\"color:#B392F0\">LoadInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">rm.activeUploads),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        MaxUploads:    rm.maxUploads,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// IncrementActiveUploads atomically increases the active upload counter</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ResourceMonitor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">IncrementActiveUploads</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    atomic.</span><span style=\"color:#B392F0\">AddInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">rm.activeUploads, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DecrementActiveUploads atomically decreases the active upload counter  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ResourceMonitor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">DecrementActiveUploads</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    atomic.</span><span style=\"color:#B392F0\">AddInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">rm.activeUploads, </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ShouldAcceptUpload returns true if resources allow new uploads</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ResourceMonitor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ShouldAcceptUpload</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    status </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> rm.</span><span style=\"color:#B392F0\">GetCurrentStatus</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Check multiple resource constraints</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> status.MemoryPercent </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 90</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> status.DiskPercent </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 85</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> false</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> status.ActiveUploads </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> status.MaxUploads {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code:</strong></p>\n<p><strong>Error Recovery Coordinator:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// RecoveryCoordinator manages systematic error recovery across components</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RecoveryCoordinator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sessionManager   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SessionManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storageBackend   </span><span style=\"color:#B392F0\">StorageBackend</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resourceMonitor  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ResourceMonitor</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    circuitBreaker   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CircuitBreaker</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HandleUploadError implements comprehensive error handling with recovery</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RecoveryCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HandleUploadError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">err</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">UploadError</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Log error with structured logging including correlation ID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Determine if error is retryable based on category and cause</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: For transient errors, implement exponential backoff retry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: For corruption errors, initiate chunk re-upload process</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: For resource exhaustion, trigger load shedding mechanisms</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: For security violations, quarantine session and notify</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Update circuit breaker state based on error patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Return appropriate error response to client</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use error.Category to determine recovery strategy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Check if session needs cleanup or preservation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RecoverFromStorageFailure handles storage backend failures with failover</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RecoveryCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RecoverFromStorageFailure</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">operation</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Identify failed storage backend from error context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check if alternative backends are available and healthy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Migrate session state to working backend if possible</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update session metadata with new backend information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Retry failed operation on new backend with circuit breaker</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: If all backends failed, preserve session for later retry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use circuit breaker to test backend health before migration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Corruption Recovery Handler:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// CorruptionRecovery handles data integrity issues and recovery</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CorruptionRecovery</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sessionManager </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SessionManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage       </span><span style=\"color:#B392F0\">StorageBackend</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HandleChunkCorruption manages chunk-level corruption detection and recovery</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">cr </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CorruptionRecovery</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HandleChunkCorruption</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">chunkOffset</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">expectedHash</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">actualHash</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Log corruption event with detailed metadata for analysis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Mark corrupted chunk as failed in session state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Clean up corrupted chunk data from storage backend</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Reset session offset to allow chunk re-upload</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Generate corruption response with expected vs actual hashes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Update corruption metrics for monitoring and alerting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Preserve session state but rollback chunk progress</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Include retry count to prevent infinite corruption loops</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// VerifyAssemblyIntegrity performs comprehensive integrity checking during file assembly</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">cr </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CorruptionRecovery</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">VerifyAssemblyIntegrity</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ValidationResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Load session metadata and verify all chunks are present</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Re-compute hash for each stored chunk and verify against session</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Perform chunk ordering verification for correct assembly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Compute complete file hash and compare against expected</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Verify final file size matches session total size</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Generate detailed validation report with any discrepancies</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use streaming verification to handle large files efficiently</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Stop verification at first error to avoid unnecessary processing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints:</strong></p>\n<ul>\n<li><strong>Error Wrapping</strong>: Use <code>fmt.Errorf(&quot;operation failed: %w&quot;, err)</code> for error wrapping with Go 1.13+ </li>\n<li><strong>Context Cancellation</strong>: Always check <code>ctx.Done()</code> in long-running error recovery operations</li>\n<li><strong>Atomic Operations</strong>: Use <code>sync/atomic</code> for counters accessed from multiple goroutines (active uploads, error counts)</li>\n<li><strong>Mutex Strategy</strong>: Use <code>sync.RWMutex</code> for circuit breaker state (many reads, few writes)</li>\n<li><strong>Time Handling</strong>: Use <code>time.Now().UTC()</code> for consistent timestamp handling across time zones</li>\n<li><strong>Resource Monitoring</strong>: Import <code>runtime</code> and <code>syscall</code> packages for system resource access</li>\n<li><strong>Structured Logging</strong>: Consider using <code>logrus</code> or <code>zap</code> for structured error logging with correlation IDs</li>\n</ul>\n<p><strong>F. Milestone Checkpoint:</strong></p>\n<p>After implementing error handling mechanisms, verify the following behaviors:</p>\n<p><strong>Milestone 1 Checkpoint (Network Failures):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test chunk upload with connection interruption</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> \"http://localhost:8080/upload/test123/chunk\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">     -H</span><span style=\"color:#9ECBFF\"> \"Content-Range: bytes 0-1023/2048\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">     --data-binary</span><span style=\"color:#9ECBFF\"> @chunk1.dat</span><span style=\"color:#E1E4E8\"> &#x26;</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Kill curl during upload and retry - should resume correctly</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: Server preserves session state, client can query offset and retry</span></span></code></pre></div>\n\n<p><strong>Milestone 2 Checkpoint (Storage Failures):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test storage backend failover</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Stop primary storage backend (e.g., kill minio container)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> \"http://localhost:8080/upload/test456/chunk\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">     -H</span><span style=\"color:#9ECBFF\"> \"Content-Range: bytes 0-1023/2048\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">     --data-binary</span><span style=\"color:#9ECBFF\"> @chunk1.dat</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: Upload continues on secondary backend, session remains consistent</span></span></code></pre></div>\n\n<p><strong>Milestone 3 Checkpoint (Resource Exhaustion):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test upload rejection under resource pressure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Create memory pressure with large concurrent uploads</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#B392F0\">1..100}</span><span style=\"color:#E1E4E8\">; </span><span style=\"color:#F97583\">do</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">  curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> \"http://localhost:8080/upload/test</span><span style=\"color:#E1E4E8\">$i</span><span style=\"color:#9ECBFF\">/chunk\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">       --data-binary</span><span style=\"color:#9ECBFF\"> @largechunk.dat</span><span style=\"color:#E1E4E8\"> &#x26;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">done</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: Service maintains responsiveness, rejects new uploads when appropriate</span></span></code></pre></div>\n\n<p><strong>G. Debugging Tips:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Uploads randomly fail after 50%</td>\n<td>Circuit breaker triggering</td>\n<td>Check circuit breaker state logs</td>\n<td>Increase failure threshold or improve backend reliability</td>\n</tr>\n<tr>\n<td>Memory usage grows continuously</td>\n<td>Session state not cleaned up</td>\n<td>Monitor active session count vs memory</td>\n<td>Implement aggressive session cleanup</td>\n</tr>\n<tr>\n<td>Storage operations timeout</td>\n<td>Backend overloaded or network issues</td>\n<td>Check backend response times and error rates</td>\n<td>Add retry logic with exponential backoff</td>\n</tr>\n<tr>\n<td>Chunk corruption increases over time</td>\n<td>Storage backend degradation</td>\n<td>Compare corruption rates across backends</td>\n<td>Migrate to healthy backend, investigate storage issues</td>\n</tr>\n<tr>\n<td>Service becomes unresponsive</td>\n<td>Resource exhaustion</td>\n<td>Check CPU, memory, file descriptor usage</td>\n<td>Implement load shedding and resource limits</td>\n</tr>\n<tr>\n<td>Recovery takes too long</td>\n<td>Synchronous recovery blocking requests</td>\n<td>Check if recovery operations are async</td>\n<td>Move recovery to background jobs</td>\n</tr>\n</tbody></table>\n<h2 id=\"testing-strategy-and-validation\">Testing Strategy and Validation</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (1, 2, 3) — comprehensive testing approach covering chunked upload protocol verification (M1), storage backend integration validation (M2), and virus scanning compliance testing (M3)</p>\n</blockquote>\n<p>Think of testing a resumable upload service as <em>quality assurance for a complex assembly line</em>. Just as a manufacturing plant needs quality checkpoints at every stage—incoming materials inspection, assembly verification, and final product testing—our upload service requires validation at every layer. Each chunk upload is like a component arriving on the assembly line, storage backends are like different production facilities, and virus scanning is like final quality control before shipping. A comprehensive testing strategy ensures that not only does each individual stage work correctly, but the entire pipeline operates reliably under normal conditions, degrades gracefully under stress, and recovers properly from failures.</p>\n<p>The testing pyramid for resumable uploads has unique characteristics compared to typical web services. At the foundation, we need unit tests that can simulate network interruptions, storage failures, and partial file corruption. The integration layer must validate protocol compliance with tus.io specifications, verify seamless handoffs between storage backends, and confirm that virus scanning doesn&#39;t interfere with the upload flow. At the apex, performance testing must handle concurrent uploads, resource exhaustion scenarios, and the complex interactions between chunked transfers and backend storage limitations.</p>\n<h3 id=\"unit-testing-strategy\">Unit Testing Strategy</h3>\n<p><strong>Component isolation</strong> in resumable upload testing requires sophisticated mocking strategies because components are tightly coupled through shared state and file system interactions. Think of unit testing here as <em>testing individual orchestra musicians with a metronome instead of the full symphony</em>. Each component must perform its part correctly even when other components are simulated, but the simulation must be realistic enough to catch timing issues, state inconsistencies, and resource conflicts that only emerge in the integrated system.</p>\n<p>The core challenge in unit testing resumable uploads lies in <strong>state persistence simulation</strong>. Unlike stateless HTTP services where mocks can be simple request-response pairs, our components maintain complex state across multiple interactions. The <code>SessionManager</code> must handle concurrent access to upload sessions, the <code>StorageBackend</code> must simulate partial write failures and recovery scenarios, and the <code>FileValidator</code> must process files that arrive in non-contiguous chunks.</p>\n<blockquote>\n<p><strong>Decision: Mock Strategy for Storage Operations</strong></p>\n<ul>\n<li><strong>Context</strong>: Storage backends perform complex multipart operations that can fail at any stage, and unit tests need to verify component behavior without actual cloud API calls</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Simple success/failure mocks with boolean flags</li>\n<li>Stateful mocks that track multipart upload progression</li>\n<li>Fault injection mocks that can simulate specific failure scenarios</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement stateful fault injection mocks with scenario-based failure simulation</li>\n<li><strong>Rationale</strong>: Simple mocks miss critical edge cases like partial multipart completion, while stateful mocks with fault injection can test recovery paths that are difficult to trigger with real backends</li>\n<li><strong>Consequences</strong>: More complex test setup but comprehensive coverage of failure scenarios that are expensive to test against real storage</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Mock Component</th>\n<th>Purpose</th>\n<th>State Tracking</th>\n<th>Failure Simulation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>MockStorageBackend</code></td>\n<td>Simulate storage operations without external dependencies</td>\n<td>Track multipart uploads, part numbers, completion status</td>\n<td>Inject failures at specific operations, simulate timeout scenarios</td>\n</tr>\n<tr>\n<td><code>MockVirusScanner</code></td>\n<td>Control scan results and timing</td>\n<td>Track scan requests, simulate processing delays</td>\n<td>Return specific threats, timeout on large files, connection failures</td>\n</tr>\n<tr>\n<td><code>MockStateStore</code></td>\n<td>In-memory session persistence with controllable failures</td>\n<td>Maintain session state, track concurrent access patterns</td>\n<td>Simulate database connection loss, write conflicts, corruption</td>\n</tr>\n<tr>\n<td><code>MockCredentialProvider</code></td>\n<td>Authentication simulation with expiration scenarios</td>\n<td>Track credential refresh cycles, expiration timing</td>\n<td>Simulate authentication failures, credential rotation edge cases</td>\n</tr>\n</tbody></table>\n<p>The <strong>session lifecycle testing</strong> requires careful orchestration of concurrent operations. Each test must verify not just that operations succeed, but that they maintain consistency when interleaved with other operations. For example, testing chunk upload requires verifying that concurrent chunks for the same session are processed in the correct order, that duplicate chunks are handled idempotently, and that session state updates are atomic.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Unit Test Categories by Component:\n\nSessionManager Tests:\n- Session creation with duplicate ID handling\n- Concurrent session updates with optimistic locking\n- Session expiration with active upload protection\n- State recovery after simulated process restart\n\nStorageBackend Tests:\n- Multipart upload initialization and part tracking\n- Chunk storage with size validation and checksum verification\n- Assembly completion with part ordering and integrity checks\n- Cleanup of abandoned multipart uploads\n\nFileValidator Tests:  \n- Magic byte detection with incomplete file headers\n- Virus scanning with various malware signatures\n- File type validation against content-type mismatches\n- Quarantine workflow with metadata preservation\n\nFlowCoordinator Tests:\n- Upload initialization with backend selection\n- Chunk processing pipeline with validation integration  \n- Error propagation and recovery coordination\n- Resource limit enforcement and circuit breaker integration</code></pre></div>\n\n<p><strong>Test data management</strong> for unit tests requires realistic file samples that cover edge cases in chunked uploads. The test suite needs files that are exactly at chunk boundaries, files that compress poorly (affecting bandwidth calculations), files with embedded malware signatures for scanner testing, and corrupted files with invalid checksums. Each test file should be deterministically generated to ensure reproducible test results across different environments.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Concurrency Testing</strong>\nMany developers write unit tests that verify sequential operation but miss race conditions in concurrent chunk uploads. For example, testing that chunks A, B, C upload successfully in sequence doesn&#39;t catch the bug where chunk C arrives before chunk B and overwrites session state incorrectly. Always include tests where operations are deliberately interleaved using goroutines with controlled synchronization points to verify thread safety.</p>\n<h3 id=\"integration-testing-scenarios\">Integration Testing Scenarios</h3>\n<p><strong>End-to-end upload flows</strong> in integration testing must validate the complete journey from client request to final file storage, including all intermediate state transitions and cross-component interactions. Think of integration testing as <em>rehearsing the entire theatrical performance with all actors, stage crew, and technical systems working together</em>. While unit tests verify that individual actors know their lines, integration tests ensure the entire production flows smoothly, scene changes happen on cue, and technical effects are synchronized with the performance.</p>\n<p>The integration test environment requires careful setup to simulate realistic conditions while maintaining test isolation and repeatability. Each test needs its own isolated storage namespace, dedicated virus scanner configuration, and independent session state to prevent tests from interfering with each other. However, the environment should closely mirror production conditions, including network latency, storage consistency models, and virus scanner performance characteristics.</p>\n<table>\n<thead>\n<tr>\n<th>Integration Scenario</th>\n<th>Components Involved</th>\n<th>Validation Points</th>\n<th>Failure Injection</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Complete Upload Flow</td>\n<td>Client → FlowCoordinator → SessionManager → StorageBackend → FileValidator</td>\n<td>Protocol compliance, state consistency, file integrity</td>\n<td>Network interruption during chunk upload, storage failure during assembly</td>\n</tr>\n<tr>\n<td>Cross-Backend Migration</td>\n<td>SessionManager → Multiple StorageBackends → CredentialProvider</td>\n<td>Data consistency across backends, credential handling</td>\n<td>Authentication failure during migration, partial copy scenarios</td>\n</tr>\n<tr>\n<td>Concurrent Upload Sessions</td>\n<td>Multiple Clients → FlowCoordinator → Shared Resources</td>\n<td>Resource contention, session isolation, performance degradation</td>\n<td>Memory pressure, connection pool exhaustion</td>\n</tr>\n<tr>\n<td>Virus Detection Workflow</td>\n<td>Client → StorageBackend → FileValidator → QuarantineManager</td>\n<td>Infected file isolation, metadata preservation, notification flow</td>\n<td>Scanner timeout, quarantine storage failure</td>\n</tr>\n</tbody></table>\n<p><strong>Protocol compliance testing</strong> ensures that our service correctly implements the tus.io resumable upload specification. This involves both testing our server against the official tus protocol test suite and verifying that standard tus clients can successfully complete uploads. The tests must cover all supported tus extensions (creation, termination, checksum, expiration) and verify correct HTTP status codes, header formats, and state transitions.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>tus.io Protocol Compliance Tests:\n\nUpload Creation:\n- POST to creation endpoint returns 201 Created with Location header\n- Upload-Length header is properly parsed and validated  \n- Upload-Metadata header encoding/decoding with base64 values\n- Creation with Upload-Defer-Length for unknown size uploads\n\nChunk Upload:\n- PATCH requests with Upload-Offset header for resumption\n- Content-Range header validation and offset verification\n- Checksum verification using Upload-Checksum header (SHA-1, MD5)\n- 409 Conflict response for offset mismatches\n\nUpload Termination:\n- DELETE requests properly clean up session and storage\n- 404 Not Found for terminated or expired uploads\n- Cleanup of associated multipart uploads in storage backends\n\nProtocol Extensions:\n- HEAD requests return current upload progress in Upload-Offset\n- OPTIONS requests advertise supported extensions\n- Upload expiration handling with Upload-Expires header</code></pre></div>\n\n<p><strong>Backend integration validation</strong> tests the service against real storage backends to verify that our abstraction layer correctly handles the nuances of different storage systems. While unit tests use mocks, integration tests connect to actual S3, GCS, and local storage to validate credential handling, multipart upload mechanics, and error condition behavior that can only be observed with real backends.</p>\n<p>The challenge in backend integration testing lies in managing test data lifecycle and handling the eventual consistency models of cloud storage. Tests must create isolated storage namespaces, properly clean up test artifacts even after test failures, and account for the timing differences between local filesystem operations and cloud storage propagation delays.</p>\n<blockquote>\n<p><strong>Decision: Backend Integration Test Environment</strong></p>\n<ul>\n<li><strong>Context</strong>: Integration tests need to validate against real storage backends but must be isolated, repeatable, and cost-effective</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Shared test buckets with namespace isolation</li>\n<li>Dynamic test bucket creation per test run  </li>\n<li>Local storage emulators (MinIO, fake-gcs-server)</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Hybrid approach using local emulators for development and real backends for CI/CD pipeline</li>\n<li><strong>Rationale</strong>: Local emulators enable fast developer feedback and offline testing, while real backend testing in CI catches integration issues that emulators might miss</li>\n<li><strong>Consequences</strong>: More complex test infrastructure but comprehensive validation across development and production scenarios</li>\n</ul>\n</blockquote>\n<p><strong>Virus scanning integration</strong> requires special test files and controlled environments to validate threat detection without introducing actual malware into the test environment. The integration tests use EICAR test files (standard antivirus test signatures) and custom test patterns to verify that the scanning pipeline correctly identifies threats, quarantines infected files, and maintains detailed audit logs of scanning decisions.</p>\n<h3 id=\"milestone-validation-checkpoints\">Milestone Validation Checkpoints</h3>\n<p><strong>Progressive testing goals</strong> provide clear success criteria for each implementation milestone, enabling developers to verify that their implementation meets requirements before proceeding to the next stage. Think of these checkpoints as <em>quality gates in a software factory</em>—each milestone must pass its validation criteria before the next phase begins, ensuring that complexity is added incrementally on a solid foundation.</p>\n<p>Each milestone validation includes both functional verification (does the feature work correctly) and integration verification (does the feature work correctly with previously implemented components). This prevents the common problem where individual features work in isolation but break when combined, which is particularly important in resumable uploads where components are tightly coupled through shared state and file system interactions.</p>\n<p><strong>Milestone 1: Chunked Upload Protocol Validation</strong></p>\n<p>The first milestone validation focuses on core protocol compliance and basic resumability. The validation environment should include network simulation tools that can introduce controlled failures, latency, and bandwidth constraints to verify that the protocol correctly handles real-world network conditions.</p>\n<table>\n<thead>\n<tr>\n<th>Validation Category</th>\n<th>Test Description</th>\n<th>Success Criteria</th>\n<th>Failure Indicators</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Protocol Compliance</td>\n<td>Upload initialization with tus.io client</td>\n<td>Returns valid upload URL with correct headers</td>\n<td>Missing Location header, incorrect status codes</td>\n</tr>\n<tr>\n<td>Chunk Processing</td>\n<td>Sequential chunk upload with offset tracking</td>\n<td>Each chunk advances offset correctly, accepts next chunk</td>\n<td>Offset drift, chunk ordering failures</td>\n</tr>\n<tr>\n<td>Resumption Logic</td>\n<td>Interrupt and resume upload mid-transfer</td>\n<td>Client can query progress and continue from correct offset</td>\n<td>Lost progress, incorrect resume offset</td>\n</tr>\n<tr>\n<td>Session Management</td>\n<td>Concurrent uploads with different sessions</td>\n<td>Sessions remain isolated, no state bleeding</td>\n<td>Session confusion, corrupted metadata</td>\n</tr>\n</tbody></table>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Milestone 1 Checkpoint Commands:\n\n# Start the upload service with chunked protocol enabled\ngo run cmd/server/main.go --config=test-configs/milestone1.json\n\n# Initialize upload using curl (simulating tus client)\ncurl -X POST http://localhost:8080/uploads \\\n  -H &quot;Upload-Length: 10485760&quot; \\\n  -H &quot;Content-Type: application/offset+octet-stream&quot;\n\n# Upload first chunk  \ncurl -X PATCH http://localhost:8080/uploads/$UPLOAD_ID \\\n  -H &quot;Upload-Offset: 0&quot; \\\n  -H &quot;Content-Type: application/offset+octet-stream&quot; \\\n  --data-binary @testdata/chunk-001.bin\n\n# Verify resumption by querying progress\ncurl -X HEAD http://localhost:8080/uploads/$UPLOAD_ID\n\nExpected Response Headers:\nUpload-Offset: 1048576  # Size of uploaded chunk\nUpload-Length: 10485760  # Total file size</code></pre></div>\n\n<p><strong>Milestone 2: Storage Abstraction Validation</strong></p>\n<p>The second milestone validation verifies that storage backend abstraction works correctly with multiple backend types and that uploads can be distributed across different storage systems. This includes testing credential management, backend failover, and the consistency of the storage interface across different backend implementations.</p>\n<p>The validation environment should include multiple storage backends (local, S3-compatible, and optionally GCS) with different configurations to verify that the abstraction layer correctly handles the unique characteristics of each backend while providing a consistent interface to higher-level components.</p>\n<table>\n<thead>\n<tr>\n<th>Backend Type</th>\n<th>Configuration</th>\n<th>Validation Tests</th>\n<th>Success Metrics</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Local Storage</td>\n<td>Filesystem with quota limits</td>\n<td>Large file assembly, concurrent access, disk space handling</td>\n<td>Files assembled correctly, no corruption, proper cleanup</td>\n</tr>\n<tr>\n<td>S3 Compatible</td>\n<td>MinIO or AWS S3 with multipart</td>\n<td>Multipart upload coordination, credential rotation, signed URLs</td>\n<td>Multipart uploads complete, URLs work, credentials refresh</td>\n</tr>\n<tr>\n<td>Storage Migration</td>\n<td>Cross-backend transfer</td>\n<td>Upload to one backend, migrate to another</td>\n<td>File integrity preserved, metadata maintained</td>\n</tr>\n</tbody></table>\n<p><strong>Milestone 3: Virus Scanning &amp; Validation</strong></p>\n<p>The final milestone validation ensures that file validation and security measures work correctly without interfering with the upload protocol or storage operations. This includes testing with various file types, sizes, and threat patterns to verify that the security pipeline provides comprehensive protection while maintaining good performance.</p>\n<p>The validation environment requires a properly configured ClamAV instance with current virus definitions, test files with known signatures, and monitoring tools to verify that scanning integrates smoothly with the upload flow without introducing unacceptable latency or resource consumption.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Milestone 3 Security Validation Tests:\n\n# Clean file upload (should complete normally)\nupload_test_file testdata/clean-document.pdf\nverify_file_in_storage clean-document.pdf\n\n# Infected file upload (should be quarantined)  \nupload_test_file testdata/eicar-test.txt\nverify_file_quarantined eicar-test.txt\nverify_scan_log_entry FOUND &quot;Eicar-Test-Signature&quot;\n\n# File type mismatch (should be rejected)\nupload_with_wrong_content_type testdata/executable.bin &quot;image/jpeg&quot;  \nverify_upload_rejected &quot;content type mismatch&quot;\n\n# Oversized file (should be rejected during initialization)\nupload_oversized_file testdata/huge-file.bin\nverify_upload_rejected &quot;file size exceeds limit&quot;</code></pre></div>\n\n<p>⚠️ <strong>Pitfall: Testing Only Happy Path Scenarios</strong>\nMany milestone validations focus on successful uploads and miss critical failure scenarios. For example, testing only clean files misses the case where virus scanning fails due to scanner unavailability. Always include negative test cases: what happens when the virus scanner is down, when storage quotas are exceeded, or when clients send malformed protocol requests. These failure scenarios often reveal integration issues that happy path testing misses.</p>\n<h3 id=\"performance-and-load-testing\">Performance and Load Testing</h3>\n<p><strong>Concurrent upload handling</strong> testing validates that the service maintains consistent performance and correctness when handling multiple simultaneous uploads. Think of this as <em>stress-testing a shipping dock during peak season</em>—the system must not only handle the increased volume but maintain the same quality standards for package sorting, routing, and delivery tracking that it provides during normal operations.</p>\n<p>Performance testing for resumable uploads has unique characteristics because of the persistent state requirements and the complex interactions between chunked uploads and storage backend limitations. Unlike stateless web services where load testing primarily focuses on request throughput and response times, resumable upload testing must also validate state consistency under load, memory usage growth patterns, and the behavior of long-running upload sessions under resource pressure.</p>\n<p>The performance test environment should simulate realistic client behavior patterns, including uploads that start and stop unpredictably, clients that disappear mid-upload (requiring session cleanup), and the mixture of small frequent uploads and large infrequent uploads that characterizes real-world usage. The test infrastructure must be capable of generating sustained load while monitoring detailed metrics about session state, storage operations, and resource utilization.</p>\n<table>\n<thead>\n<tr>\n<th>Load Test Scenario</th>\n<th>Concurrent Users</th>\n<th>Upload Patterns</th>\n<th>Validation Metrics</th>\n<th>Resource Monitoring</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Steady State Load</td>\n<td>50-100 concurrent uploads</td>\n<td>Mixed file sizes (1MB-1GB), normal completion rates</td>\n<td>Throughput stability, response time percentiles</td>\n<td>Memory growth, CPU usage, file descriptor count</td>\n</tr>\n<tr>\n<td>Burst Traffic</td>\n<td>10x normal load for 10 minutes</td>\n<td>Many small files with rapid initialization</td>\n<td>Session creation rate, storage backend saturation</td>\n<td>Connection pool usage, temporary storage growth</td>\n</tr>\n<tr>\n<td>Long-Running Sessions</td>\n<td>10-20 large file uploads (5GB+)</td>\n<td>Slow upload rates, frequent interruptions</td>\n<td>Session persistence, cleanup effectiveness</td>\n<td>Memory per session, storage space utilization</td>\n</tr>\n<tr>\n<td>Failure Recovery Load</td>\n<td>Normal load + induced failures</td>\n<td>Backend failures, scanner timeouts</td>\n<td>Recovery time, data consistency</td>\n<td>Error rate impact on resource usage</td>\n</tr>\n</tbody></table>\n<p><strong>Resource utilization validation</strong> ensures that the service operates within acceptable memory, CPU, and storage bounds under various load conditions. The service must handle resource pressure gracefully, implementing backpressure mechanisms and load shedding when necessary to prevent cascading failures that could affect all upload sessions.</p>\n<p>Memory usage patterns in resumable upload services are particularly complex because session state must persist across potentially long time periods, chunk metadata accumulates throughout the upload process, and temporary buffers are required for virus scanning and file assembly. The testing must verify that memory usage grows predictably with load and that cleanup processes effectively reclaim resources from completed or abandoned uploads.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Resource Utilization Test Matrix:\n\nMemory Testing:\n- Session state memory growth with 1000+ concurrent uploads\n- Chunk metadata memory usage for large files (10GB+ in 5MB chunks)\n- Temporary buffer memory during virus scanning and assembly\n- Memory leak detection during extended test runs (24+ hours)\n\nCPU Testing:  \n- Checksum calculation CPU impact during chunk processing\n- File assembly CPU usage for large uploads\n- Virus scanning CPU overhead with various file types\n- Background cleanup process CPU utilization\n\nStorage Testing:\n- Temporary storage growth patterns during multipart uploads\n- Cleanup effectiveness for abandoned sessions and failed uploads  \n- Storage backend connection pool utilization\n- Local temporary space management during file assembly\n\nNetwork Testing:\n- Bandwidth utilization efficiency with concurrent chunked uploads\n- Connection reuse effectiveness with persistent clients\n- Network timeout handling under various latency conditions\n- Protocol overhead measurement for small vs large chunks</code></pre></div>\n\n<p><strong>Circuit breaker and backpressure testing</strong> validates that the service degrades gracefully under extreme load conditions rather than failing catastrophically. The testing should verify that circuit breakers open appropriately when backend services become unavailable, that new upload requests are rejected gracefully when resource limits are approached, and that existing uploads continue to progress even when new uploads are being throttled.</p>\n<blockquote>\n<p><strong>Decision: Load Testing Infrastructure</strong></p>\n<ul>\n<li><strong>Context</strong>: Performance testing requires realistic load generation that simulates actual client behavior patterns including network interruptions and resumption</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Simple HTTP load testing tools (wrk, hey) with static payloads</li>\n<li>Custom tus.io protocol clients with resumption simulation</li>\n<li>Hybrid approach with protocol-aware load generation and resource monitoring</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Custom protocol-aware load testing with realistic client behavior simulation</li>\n<li><strong>Rationale</strong>: Generic HTTP load tools miss the critical resumption patterns that differentiate resumable uploads from simple file uploads, while protocol-aware testing reveals performance issues specific to stateful upload scenarios</li>\n<li><strong>Consequences</strong>: More complex test infrastructure development but realistic performance validation that matches production usage patterns</li>\n</ul>\n</blockquote>\n<p>The load testing infrastructure must include comprehensive monitoring and alerting to detect performance degradation before it impacts upload reliability. This includes tracking not just basic metrics like request throughput and response times, but also upload service specific metrics like session state consistency, chunk assembly success rates, and backend storage health.</p>\n<p><strong>Scalability validation</strong> tests the service&#39;s ability to handle growth in upload volume, file sizes, and concurrent users. The testing should identify bottlenecks in session management, storage backend coordination, and virus scanning throughput that would limit the service&#39;s ability to scale horizontally or vertically.</p>\n<table>\n<thead>\n<tr>\n<th>Scalability Dimension</th>\n<th>Test Approach</th>\n<th>Scaling Bottlenecks</th>\n<th>Validation Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Upload Volume</td>\n<td>Increase concurrent uploads until saturation</td>\n<td>Session manager throughput, storage backend limits</td>\n<td>Linear performance degradation, graceful overload handling</td>\n</tr>\n<tr>\n<td>File Size</td>\n<td>Test with increasingly large files (100MB to 10GB+)</td>\n<td>Memory usage per upload, assembly time complexity</td>\n<td>Memory usage scales sub-linearly with file size</td>\n</tr>\n<tr>\n<td>Session Count</td>\n<td>Long-running test with accumulating sessions</td>\n<td>Session storage overhead, cleanup efficiency</td>\n<td>Memory per session remains bounded</td>\n</tr>\n<tr>\n<td>Backend Scaling</td>\n<td>Multiple storage backends with load distribution</td>\n<td>Backend selection logic, credential management</td>\n<td>Load balances effectively across backends</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: Testing in Unrealistic Environments</strong>\nPerformance testing often occurs in environments that don&#39;t match production characteristics—faster networks, more powerful hardware, or simplified storage configurations. This can miss performance issues that only appear under production conditions. Always include testing scenarios that simulate production network latency, hardware constraints, and storage backend limitations. For example, testing against local MinIO doesn&#39;t reveal the impact of S3&#39;s eventual consistency model on upload completion detection.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The testing strategy implementation requires careful coordination between test infrastructure, realistic data generation, and comprehensive monitoring. The key challenge is creating a testing environment that accurately simulates production conditions while maintaining the isolation and repeatability required for reliable automated testing.</p>\n<p><strong>A. Technology Recommendations Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Testing Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Unit Testing Framework</td>\n<td>Go testing stdlib + testify assertions</td>\n<td>Ginkgo BDD framework + Gomega matchers</td>\n</tr>\n<tr>\n<td>Mock Generation</td>\n<td>Manual mocks with interfaces</td>\n<td>GoMock generated mocks + Testcontainers</td>\n</tr>\n<tr>\n<td>Integration Testing</td>\n<td>Docker Compose with real services</td>\n<td>Kubernetes test environment + Helm charts</td>\n</tr>\n<tr>\n<td>Load Testing</td>\n<td>Custom Go client with goroutines</td>\n<td>k6 with custom tus.io extension</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Prometheus metrics + Grafana</td>\n<td>Full observability stack (Jaeger, ELK)</td>\n</tr>\n<tr>\n<td>Test Data</td>\n<td>Static test files</td>\n<td>Property-based testing with rapid</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  cmd/\n    server/main.go              ← production server entry point\n    load-test/main.go           ← load testing client\n  internal/\n    upload/\n      session_manager.go        ← core upload logic\n      session_manager_test.go   ← unit tests\n      integration_test.go       ← integration test suite\n    storage/\n      backend.go                ← storage interface\n      backend_test.go           ← backend unit tests\n      integration_test.go       ← multi-backend integration tests\n    validator/\n      file_validator.go         ← security validation\n      file_validator_test.go    ← validation unit tests\n  test/\n    fixtures/                   ← test data files\n      clean-files/              ← virus-free test files\n      infected-files/           ← EICAR test signatures\n      malformed-files/          ← corrupted/invalid files\n    integration/\n      docker-compose.yml        ← test environment setup\n      tus-protocol-test.go      ← protocol compliance tests\n    performance/\n      load-test-scenarios.go    ← load testing scenarios\n      benchmarks_test.go        ← Go benchmark tests\n    mocks/\n      storage_mock.go           ← storage backend mocks\n      scanner_mock.go           ← virus scanner mocks</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code:</strong></p>\n<p>This complete test infrastructure provides the foundation for implementing the comprehensive testing strategy:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// test/mocks/storage_mock.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> mocks</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">io</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">errors</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MockStorageBackend provides comprehensive storage simulation with fault injection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MockStorageBackend</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu              </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    chunks          </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">byte</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    multipartUploads </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockMultipartUpload</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    failureScenarios </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">FailureScenario</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    operationDelays  </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bytesStored     </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    operationCounts </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MockMultipartUpload</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ID       </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Key      </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Parts    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MultipartPart</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Metadata </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Created  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Aborted  </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> FailureScenario</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Operation    </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    FailAfterOps </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrorType    </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RecoveryTime </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewMockStorageBackend</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockStorageBackend</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">MockStorageBackend</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        chunks:           </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        multipartUploads: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockMultipartUpload</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        failureScenarios: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">FailureScenario</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        operationDelays:  </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        operationCounts:  </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// InjectFailure configures the mock to fail after specified number of operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockStorageBackend</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">InjectFailure</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">operation</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">failAfter</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">err</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> m.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.failureScenarios[operation] </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> FailureScenario</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Operation:    operation,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        FailAfterOps: failAfter,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ErrorType:    err,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SetOperationDelay simulates network latency for specific operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockStorageBackend</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SetOperationDelay</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">operation</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">delay</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> m.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.operationDelays[operation] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> delay</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockStorageBackend</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">checkFailure</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">operation</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> m.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.operationCounts[operation]</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> scenario, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> m.failureScenarios[operation]; exists {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> m.operationCounts[operation] </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> scenario.FailAfterOps {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> scenario.ErrorType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> delay, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> m.operationDelays[operation]; exists {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        time.</span><span style=\"color:#B392F0\">Sleep</span><span style=\"color:#E1E4E8\">(delay)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockStorageBackend</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">StoreChunk</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">data</span><span style=\"color:#B392F0\"> io</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Reader</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">size</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> m.</span><span style=\"color:#B392F0\">checkFailure</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"StoreChunk\"</span><span style=\"color:#E1E4E8\">); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    chunkData, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> io.</span><span style=\"color:#B392F0\">ReadAll</span><span style=\"color:#E1E4E8\">(data)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(chunkData)) </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> size {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"size mismatch\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> m.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.chunks[key] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> chunkData</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.bytesStored </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> size</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockStorageBackend</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">InitMultipart</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">metadata</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MultipartUpload</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> m.</span><span style=\"color:#B392F0\">checkFailure</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"InitMultipart\"</span><span style=\"color:#E1E4E8\">); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    uploadID </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"mock-upload-</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">UnixNano</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> m.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mockUpload </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">MockMultipartUpload</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ID:       uploadID,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Key:      key,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Parts:    </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MultipartPart</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Metadata: metadata,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Created:  time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.multipartUploads[uploadID] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> mockUpload</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">MultipartUpload</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ID:       uploadID,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Key:      key,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Backend:  </span><span style=\"color:#9ECBFF\">\"mock\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Metadata: metadata,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        CreatedAt: time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetStats returns mock backend statistics for monitoring</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockStorageBackend</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetStats</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{} {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> m.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"chunks_stored\"</span><span style=\"color:#E1E4E8\">:      </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(m.chunks),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"bytes_stored\"</span><span style=\"color:#E1E4E8\">:       m.bytesStored,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"multipart_uploads\"</span><span style=\"color:#E1E4E8\">:  </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(m.multipartUploads),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"operation_counts\"</span><span style=\"color:#E1E4E8\">:   m.operationCounts,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// test/integration/tus_protocol_test.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> integration</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">bytes</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strconv</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">io</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TusProtocolTester validates tus.io specification compliance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TusProtocolTester</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    baseURL    </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    httpClient </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    t          </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewTusProtocolTester</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">baseURL</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TusProtocolTester</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">TusProtocolTester</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        baseURL:    baseURL,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        httpClient: </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Client</span><span style=\"color:#E1E4E8\">{},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        t:          t,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TestCompleteUploadFlow validates entire tus.io protocol sequence</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tpt </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TusProtocolTester</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">TestCompleteUploadFlow</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Step 1: Initialize upload</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    uploadID </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> tpt.</span><span style=\"color:#B392F0\">initializeUpload</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"test-file.bin\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1048576</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"filename\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"dGVzdC1maWxlLmJpbg==\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#6A737D\">// base64 encoded</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"filetype\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"YXBwbGljYXRpb24vb2N0ZXQtc3RyZWFt\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#6A737D\">// base64 encoded</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Step 2: Upload chunks</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    chunkSize </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> 262144</span><span style=\"color:#6A737D\"> // 256KB chunks</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> offset </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">; offset </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 1048576</span><span style=\"color:#E1E4E8\">; offset </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> chunkSize {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        remainingBytes </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> 1048576</span><span style=\"color:#F97583\"> -</span><span style=\"color:#E1E4E8\"> offset</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        currentChunkSize </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> chunkSize</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> remainingBytes </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> chunkSize {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            currentChunkSize </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> remainingBytes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        chunkData </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, currentChunkSize)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> chunkData {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            chunkData[i] </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> byte</span><span style=\"color:#E1E4E8\">((offset </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> i) </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> 256</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#6A737D\">// Predictable test pattern</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tpt.</span><span style=\"color:#B392F0\">uploadChunk</span><span style=\"color:#E1E4E8\">(uploadID, offset, chunkData)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Verify progress after each chunk</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        progress </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> tpt.</span><span style=\"color:#B392F0\">getUploadProgress</span><span style=\"color:#E1E4E8\">(uploadID)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        expectedOffset </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> offset </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> currentChunkSize</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> progress </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> expectedOffset {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            tpt.t.</span><span style=\"color:#B392F0\">Fatalf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Progress mismatch: expected </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">, got </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, expectedOffset, progress)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Step 3: Verify completion</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    finalProgress </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> tpt.</span><span style=\"color:#B392F0\">getUploadProgress</span><span style=\"color:#E1E4E8\">(uploadID)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> finalProgress </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> 1048576</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tpt.t.</span><span style=\"color:#B392F0\">Fatalf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Upload incomplete: expected 1048576, got </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, finalProgress)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tpt </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TusProtocolTester</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">initializeUpload</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">filename</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">totalSize</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">metadata</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    req, _ </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> http.</span><span style=\"color:#B392F0\">NewRequest</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"POST\"</span><span style=\"color:#E1E4E8\">, tpt.baseURL</span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\">\"/uploads\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    req.Header.</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Upload-Length\"</span><span style=\"color:#E1E4E8\">, strconv.</span><span style=\"color:#B392F0\">Itoa</span><span style=\"color:#E1E4E8\">(totalSize))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    req.Header.</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Content-Type\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"application/offset+octet-stream\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Add metadata header in tus format  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> metadataHeader []</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> key, value </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> metadata {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metadataHeader </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(metadataHeader, fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#79B8FF\"> %s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, key, value))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(metadataHeader) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        req.Header.</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Upload-Metadata\"</span><span style=\"color:#E1E4E8\">, strings.</span><span style=\"color:#B392F0\">Join</span><span style=\"color:#E1E4E8\">(metadataHeader, </span><span style=\"color:#9ECBFF\">\",\"</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resp, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> tpt.httpClient.</span><span style=\"color:#B392F0\">Do</span><span style=\"color:#E1E4E8\">(req)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tpt.t.</span><span style=\"color:#B392F0\">Fatalf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Failed to initialize upload: </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> resp.Body.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> resp.StatusCode </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> 201</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tpt.t.</span><span style=\"color:#B392F0\">Fatalf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Expected status 201, got </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, resp.StatusCode)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    location </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> resp.Header.</span><span style=\"color:#B392F0\">Get</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Location\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> location </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tpt.t.</span><span style=\"color:#B392F0\">Fatal</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Missing Location header in upload initialization response\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Extract upload ID from location URL</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parts </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> strings.</span><span style=\"color:#B392F0\">Split</span><span style=\"color:#E1E4E8\">(location, </span><span style=\"color:#9ECBFF\">\"/\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> parts[</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(parts)</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tpt </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TusProtocolTester</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">uploadChunk</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">uploadID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">offset</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    req, _ </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> http.</span><span style=\"color:#B392F0\">NewRequest</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"PATCH\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">/uploads/</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, tpt.baseURL, uploadID),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        bytes.</span><span style=\"color:#B392F0\">NewReader</span><span style=\"color:#E1E4E8\">(data))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    req.Header.</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Upload-Offset\"</span><span style=\"color:#E1E4E8\">, strconv.</span><span style=\"color:#B392F0\">Itoa</span><span style=\"color:#E1E4E8\">(offset))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    req.Header.</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Content-Type\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"application/offset+octet-stream\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resp, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> tpt.httpClient.</span><span style=\"color:#B392F0\">Do</span><span style=\"color:#E1E4E8\">(req)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tpt.t.</span><span style=\"color:#B392F0\">Fatalf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Failed to upload chunk: </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> resp.Body.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> resp.StatusCode </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> 204</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        body, _ </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> io.</span><span style=\"color:#B392F0\">ReadAll</span><span style=\"color:#E1E4E8\">(resp.Body)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tpt.t.</span><span style=\"color:#B392F0\">Fatalf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Expected status 204, got </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">. Response: </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, resp.StatusCode, </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">(body))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tpt </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TusProtocolTester</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">getUploadProgress</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">uploadID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    req, _ </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> http.</span><span style=\"color:#B392F0\">NewRequest</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"HEAD\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">/uploads/</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, tpt.baseURL, uploadID), </span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resp, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> tpt.httpClient.</span><span style=\"color:#B392F0\">Do</span><span style=\"color:#E1E4E8\">(req)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tpt.t.</span><span style=\"color:#B392F0\">Fatalf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Failed to get upload progress: </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> resp.Body.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> resp.StatusCode </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> 200</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tpt.t.</span><span style=\"color:#B392F0\">Fatalf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Expected status 200, got </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, resp.StatusCode)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    offsetHeader </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> resp.Header.</span><span style=\"color:#B392F0\">Get</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Upload-Offset\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> offsetHeader </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tpt.t.</span><span style=\"color:#B392F0\">Fatal</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Missing Upload-Offset header in progress response\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    offset, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> strconv.</span><span style=\"color:#B392F0\">Atoi</span><span style=\"color:#E1E4E8\">(offsetHeader)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tpt.t.</span><span style=\"color:#B392F0\">Fatalf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Invalid Upload-Offset value: </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, offsetHeader)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> offset</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// test/performance/load_test_scenarios.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> performance</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">math/rand</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadTestScenario defines a comprehensive load testing scenario</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LoadTestScenario</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Name             </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ConcurrentUsers  </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TestDuration     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    UploadPatterns   []</span><span style=\"color:#B392F0\">UploadPattern</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    FailureInjection </span><span style=\"color:#B392F0\">FailureConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Initialize test metrics collection (throughput, latency, errors)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Set up monitoring for resource utilization (memory, CPU, connections)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Configure realistic client behavior patterns (pauses, retries, abandonment)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Implement graceful test termination and cleanup</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> UploadPattern</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    FileSizeMin    </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    FileSizeMax    </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ChunkSizeMin   </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ChunkSizeMax   </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    InterruptRate  </span><span style=\"color:#F97583\">float64</span><span style=\"color:#6A737D\"> // Probability of interrupting upload</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ResumeDelay    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CompletionRate </span><span style=\"color:#F97583\">float64</span><span style=\"color:#6A737D\"> // Probability of completing vs abandoning</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">lts </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LoadTestScenario</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ExecuteScenario</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LoadTestResults</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Start resource monitoring goroutines</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Launch concurrent upload workers based on ConcurrentUsers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Distribute upload patterns across workers with realistic timing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Collect and aggregate metrics throughout test duration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Generate comprehensive results report with percentiles and resource usage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use sync.WaitGroup to coordinate worker completion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use channels to collect metrics from workers without blocking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Monitor context cancellation for graceful shutdown</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">lts </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LoadTestScenario</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">simulateRealisticUpload</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">pattern</span><span style=\"color:#B392F0\"> UploadPattern</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">results</span><span style=\"color:#F97583\"> chan&#x3C;-</span><span style=\"color:#B392F0\"> UploadResult</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Generate test file with specified size range and predictable content</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Initialize upload session using tus protocol</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Upload chunks with realistic timing patterns (network simulation)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Randomly interrupt and resume uploads based on InterruptRate</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Record detailed metrics (timing, bytes transferred, failures)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use time.Sleep with jitter to simulate network conditions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Track each protocol operation separately (init, chunk, resume)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// BenchmarkConcurrentUploads provides Go benchmark integration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> BenchmarkConcurrentUploads</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">b</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">B</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Set up test server with production-like configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create realistic test data with various file sizes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Run upload scenarios with increasing concurrency levels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Report bytes/second throughput and memory allocations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use b.SetBytes() to report throughput in MB/s</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use b.ResetTimer() after setup to exclude initialization time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>E. Milestone Checkpoint Implementation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// test/integration/milestone_validation.go  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> integration</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MilestoneValidator provides automated validation for each implementation milestone</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MilestoneValidator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    serverURL     </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storageConfig </span><span style=\"color:#B392F0\">StorageConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    testDataPath  </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">mv </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MilestoneValidator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ValidateMilestone1</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Verify upload initialization returns correct tus headers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Test chunk upload sequence with offset tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Interrupt upload mid-stream and verify resumption works</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Test concurrent uploads maintain session isolation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Verify session cleanup removes expired uploads</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Expected: All uploads complete successfully, offsets track correctly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Signs of issues: offset drift, session confusion, memory leaks</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">mv </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MilestoneValidator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ValidateMilestone2</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Test upload to local storage backend  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Test upload to S3-compatible backend with multipart</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Verify signed URL generation and access</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Test backend failover scenarios</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Validate file integrity across different backends</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Expected: Files stored correctly regardless of backend choice</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Signs of issues: multipart assembly failures, credential errors</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">mv </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MilestoneValidator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ValidateMilestone3</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Upload clean file and verify it reaches final storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Upload EICAR test file and verify quarantine placement  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Test file type validation with content/extension mismatches</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Verify size limit enforcement during initialization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Test scanner timeout handling with slow scan simulation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Expected: Clean files stored, threats quarantined, violations rejected</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Signs of issues: infected files in production storage, false positives</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n\n<h2 id=\"debugging-guide-and-common-issues\">Debugging Guide and Common Issues</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (1, 2, 3) — systematic debugging approaches apply to chunked upload protocol issues (M1), storage backend failures (M2), and virus scanning problems (M3)</p>\n</blockquote>\n<p>Think of debugging a resumable upload service as being a <strong>detective solving a multi-layered mystery</strong>. Unlike simple web services where failures are often immediate and obvious, upload services involve complex state machines, distributed storage operations, and long-running processes that can fail at any point. Each failure leaves traces across multiple components — upload session state, storage backend operations, virus scanning logs, and client-server protocol exchanges. Your job as a debugger is to collect these clues systematically and reconstruct what happened during the upload operation.</p>\n<p>The challenge with upload service debugging is that failures often manifest far from their root cause. A client might report &quot;upload stuck at 85%&quot; when the actual problem is a storage backend authentication failure that occurred during chunk assembly. Or a virus scanner timeout might appear as a generic &quot;upload failed&quot; error to the client. This section provides a systematic approach to trace symptoms back to their root causes and implement effective fixes.</p>\n<h3 id=\"diagnostic-techniques-and-tools\">Diagnostic Techniques and Tools</h3>\n<p>Modern upload service debugging requires a multi-layered observability strategy that captures the complete picture of upload operations. Think of this as building a <strong>comprehensive flight recorder</strong> that tracks every significant event, state transition, and resource utilization across all system components. Unlike traditional request-response debugging, upload operations span minutes or hours, cross multiple storage operations, and involve asynchronous background processing.</p>\n<h4 id=\"structured-logging-strategy\">Structured Logging Strategy</h4>\n<p>The foundation of upload service debugging is comprehensive structured logging that creates an audit trail for every upload operation. Each log entry should contain sufficient context to reconstruct the complete upload timeline without requiring correlation across multiple log sources.</p>\n<table>\n<thead>\n<tr>\n<th>Log Level</th>\n<th>Component</th>\n<th>Events Logged</th>\n<th>Context Fields</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>INFO</td>\n<td>Session Manager</td>\n<td>Session creation, state transitions, expiration</td>\n<td>SessionID, UserID, Filename, TotalSize, CurrentState</td>\n</tr>\n<tr>\n<td>INFO</td>\n<td>Upload Protocol</td>\n<td>Chunk uploads, offset updates, completion</td>\n<td>SessionID, ChunkOffset, ChunkSize, ContentHash, ProtocolPhase</td>\n</tr>\n<tr>\n<td>INFO</td>\n<td>Storage Backend</td>\n<td>Multipart operations, storage completion</td>\n<td>SessionID, Backend, OperationType, StorageKey, OperationDuration</td>\n</tr>\n<tr>\n<td>INFO</td>\n<td>Virus Scanner</td>\n<td>Scan initiation, results, quarantine</td>\n<td>SessionID, ScanDuration, ThreatName, ScanResult, QuarantineReason</td>\n</tr>\n<tr>\n<td>WARN</td>\n<td>Circuit Breaker</td>\n<td>State changes, threshold breaches</td>\n<td>Component, PreviousState, NewState, FailureCount, ResetTime</td>\n</tr>\n<tr>\n<td>ERROR</td>\n<td>All Components</td>\n<td>Operation failures, timeouts, corruption</td>\n<td>SessionID, ErrorCategory, Component, Operation, RetryAttempt</td>\n</tr>\n</tbody></table>\n<p>The logging implementation should use structured formats (JSON) with consistent field naming across components. This enables automated log analysis and correlation during incident response.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Example structured log entry</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">  \"timestamp\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"2024-01-15T10:30:45Z\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">  \"level\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"ERROR\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">  \"component\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"storage-backend\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">  \"session_id\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"upload-abc123\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">  \"operation\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"complete-multipart\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">  \"backend\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"s3\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">  \"error_category\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"authentication\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">  \"retry_attempt\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">  \"storage_key\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"uploads/abc123/final\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">  \"message\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"S3 credentials expired during multipart completion\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">  \"context\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"total_parts\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">15</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"completed_parts\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">15</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"upload_id\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"s3-multipart-xyz789\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"state-inspection-tools\">State Inspection Tools</h4>\n<p>Upload services require specialized tools to inspect and analyze upload session state, particularly for diagnosing resumed uploads and partial failures. These tools should provide both real-time monitoring and historical analysis capabilities.</p>\n<table>\n<thead>\n<tr>\n<th>Tool Purpose</th>\n<th>Implementation</th>\n<th>Key Features</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Session Inspector</td>\n<td>HTTP admin endpoint</td>\n<td>Current session state, chunk bitmap, metadata inspection</td>\n</tr>\n<tr>\n<td>Storage Analyzer</td>\n<td>CLI tool + web UI</td>\n<td>Backend consistency checking, orphaned chunk detection</td>\n</tr>\n<tr>\n<td>Protocol Validator</td>\n<td>Network interceptor</td>\n<td>tus.io compliance verification, header analysis</td>\n</tr>\n<tr>\n<td>Resource Monitor</td>\n<td>Dashboard + alerts</td>\n<td>Memory usage, disk space, concurrent upload tracking</td>\n</tr>\n</tbody></table>\n<p>The session inspector provides detailed visibility into upload session state that&#39;s essential for debugging resume failures:</p>\n<table>\n<thead>\n<tr>\n<th>Inspection Area</th>\n<th>Information Provided</th>\n<th>Debugging Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Session Metadata</td>\n<td>ID, filename, total size, content type, creation time</td>\n<td>Identifies session parameters and age</td>\n</tr>\n<tr>\n<td>Progress Tracking</td>\n<td>Current offset, chunk bitmap, completion percentage</td>\n<td>Shows exact upload progress and gaps</td>\n</tr>\n<tr>\n<td>State Machine</td>\n<td>Current state, transition history, timestamps</td>\n<td>Reveals state transition failures</td>\n</tr>\n<tr>\n<td>Backend Status</td>\n<td>Multipart upload ID, storage key, backend type</td>\n<td>Enables storage-side investigation</td>\n</tr>\n<tr>\n<td>Validation Results</td>\n<td>File type detection, virus scan status, quarantine flags</td>\n<td>Identifies security processing issues</td>\n</tr>\n</tbody></table>\n<h4 id=\"resource-monitoring-and-alerting\">Resource Monitoring and Alerting</h4>\n<p>Upload services consume significant system resources and require proactive monitoring to prevent resource exhaustion failures. The monitoring system should track both global resource utilization and per-upload resource consumption patterns.</p>\n<table>\n<thead>\n<tr>\n<th>Resource Category</th>\n<th>Metrics Tracked</th>\n<th>Alert Thresholds</th>\n<th>Recovery Actions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Memory Usage</td>\n<td>Process heap, upload buffers, session cache</td>\n<td>&gt;80% warning, &gt;90% critical</td>\n<td>Enable load shedding, cleanup expired sessions</td>\n</tr>\n<tr>\n<td>Disk Storage</td>\n<td>Temp storage usage, backend quota utilization</td>\n<td>&gt;85% warning, &gt;95% critical</td>\n<td>Reject new uploads, trigger cleanup</td>\n</tr>\n<tr>\n<td>File Descriptors</td>\n<td>Open files, socket connections, temp files</td>\n<td>&gt;70% warning, &gt;85% critical</td>\n<td>Close idle connections, cleanup temp files</td>\n</tr>\n<tr>\n<td>Network Bandwidth</td>\n<td>Upload throughput, backend API calls</td>\n<td>Sustained &gt;80% utilization</td>\n<td>Enable rate limiting, queue management</td>\n</tr>\n</tbody></table>\n<p>The resource monitoring system should integrate with the circuit breaker pattern to automatically protect against resource exhaustion:</p>\n<table>\n<thead>\n<tr>\n<th>Resource State</th>\n<th>Circuit Breaker Action</th>\n<th>Client Communication</th>\n<th>Recovery Trigger</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Normal</td>\n<td>Allow all operations</td>\n<td>Standard responses</td>\n<td>N/A</td>\n</tr>\n<tr>\n<td>Warning</td>\n<td>Enable rate limiting</td>\n<td>HTTP 429 with Retry-After</td>\n<td>Resource usage drops below warning</td>\n</tr>\n<tr>\n<td>Critical</td>\n<td>Reject new uploads</td>\n<td>HTTP 503 Service Unavailable</td>\n<td>Resource usage drops below 70%</td>\n</tr>\n<tr>\n<td>Emergency</td>\n<td>Terminate in-progress uploads</td>\n<td>HTTP 500 with explanation</td>\n<td>Manual intervention required</td>\n</tr>\n</tbody></table>\n<h4 id=\"network-level-debugging\">Network-Level Debugging</h4>\n<p>Upload protocol debugging often requires analysis of HTTP-level communication between clients and the upload service. This is particularly important for diagnosing tus.io protocol compliance issues and network-related failures.</p>\n<table>\n<thead>\n<tr>\n<th>Analysis Tool</th>\n<th>Purpose</th>\n<th>Key Capabilities</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Request Logger</td>\n<td>Protocol compliance verification</td>\n<td>Header inspection, request/response correlation</td>\n</tr>\n<tr>\n<td>Network Packet Capture</td>\n<td>Low-level network debugging</td>\n<td>TCP analysis, connection tracking, timeout investigation</td>\n</tr>\n<tr>\n<td>Load Balancer Logs</td>\n<td>Infrastructure-level debugging</td>\n<td>Request routing, health check failures, backend selection</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight:</strong> Network-level debugging becomes critical when debugging resumed uploads because the client&#39;s perception of upload progress may differ from the server&#39;s recorded state. This commonly occurs when load balancers or proxies interfere with chunked upload protocols.</p>\n</blockquote>\n<h3 id=\"common-symptoms-and-root-causes\">Common Symptoms and Root Causes</h3>\n<p>Upload service failures manifest in predictable patterns that can be systematically mapped to underlying root causes. Understanding these symptom-to-cause relationships enables rapid diagnosis and targeted fixes rather than random troubleshooting.</p>\n<h4 id=\"upload-resumption-failures\">Upload Resumption Failures</h4>\n<p>Upload resumption failures are among the most complex debugging scenarios because they involve coordination between client state, server state, and storage backend state. The client believes it can resume from a specific offset, but the server or storage backend may have different information.</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Client Behavior</th>\n<th>Root Cause Category</th>\n<th>Specific Causes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>&quot;Resume from wrong offset&quot;</td>\n<td>Client resumes from old offset, server rejects chunks</td>\n<td>State synchronization failure</td>\n<td>Session state corruption, storage backend inconsistency</td>\n</tr>\n<tr>\n<td>&quot;Cannot resume upload&quot;</td>\n<td>Client receives 404 for session ID</td>\n<td>Session lifecycle issues</td>\n<td>Premature session cleanup, database connection failure</td>\n</tr>\n<tr>\n<td>&quot;Chunks rejected during resume&quot;</td>\n<td>Server accepts resume but rejects subsequent chunks</td>\n<td>Storage backend misalignment</td>\n<td>Multipart upload state corruption, backend credential expiration</td>\n</tr>\n<tr>\n<td>&quot;Upload restarts from beginning&quot;</td>\n<td>Client loses all progress information</td>\n<td>Client-server protocol mismatch</td>\n<td>Incorrect tus.io header handling, session ID collision</td>\n</tr>\n</tbody></table>\n<p><strong>Root Cause Analysis for Offset Mismatch:</strong></p>\n<p>The most common resumption failure involves offset misalignment between client and server state. This occurs when the client&#39;s recorded progress doesn&#39;t match the server&#39;s recorded progress, typically due to one of these scenarios:</p>\n<ol>\n<li><strong>Chunk acceptance vs. storage completion</strong>: The server acknowledged chunk receipt but failed to persist the chunk to storage before a restart</li>\n<li><strong>Concurrent upload handling</strong>: Multiple client processes uploading to the same session with inadequate synchronization</li>\n<li><strong>Storage backend eventual consistency</strong>: Cloud storage backends reporting different completion status than actual stored data</li>\n<li><strong>Session state corruption</strong>: Database or memory corruption affecting the recorded current offset</li>\n</ol>\n<p><strong>Diagnostic Approach:</strong></p>\n<ol>\n<li>Compare client&#39;s intended resume offset with server&#39;s recorded current offset</li>\n<li>Verify storage backend state by listing actual stored chunks</li>\n<li>Check for gap between acknowledged chunks and successfully stored chunks</li>\n<li>Examine logs for any storage operations that failed after chunk acknowledgment</li>\n</ol>\n<h4 id=\"storage-backend-connectivity-issues\">Storage Backend Connectivity Issues</h4>\n<p>Storage backend failures often manifest as generic &quot;upload failed&quot; errors but have specific patterns based on the underlying backend type and failure mode.</p>\n<p><img src=\"/api/project/file-upload-service/architecture-doc/asset?path=diagrams%2Ferror-handling-flow.svg\" alt=\"Error Detection and Recovery Flow\"></p>\n<table>\n<thead>\n<tr>\n<th>Backend Type</th>\n<th>Symptom</th>\n<th>Common Root Causes</th>\n<th>Diagnostic Indicators</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>S3-Compatible</td>\n<td>Multipart completion fails</td>\n<td>Credential expiration, bucket permissions, part size violations</td>\n<td>HTTP 403/400 errors, AWS error codes</td>\n</tr>\n<tr>\n<td>Local Filesystem</td>\n<td>Chunk storage fails</td>\n<td>Disk space exhaustion, permission issues, filesystem corruption</td>\n<td>ENOSPC, EACCES error codes</td>\n</tr>\n<tr>\n<td>Google Cloud Storage</td>\n<td>Upload initialization fails</td>\n<td>Service account issues, quota exceeded, regional restrictions</td>\n<td>GCS error responses, quota usage metrics</td>\n</tr>\n</tbody></table>\n<p><strong>Storage Backend Circuit Breaker Patterns:</strong></p>\n<p>Storage backends should implement circuit breaker patterns to prevent cascading failures when backend services become unavailable:</p>\n<table>\n<thead>\n<tr>\n<th>Circuit State</th>\n<th>Failure Threshold</th>\n<th>Behavior</th>\n<th>Recovery Condition</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Closed (Normal)</td>\n<td>&lt;3 failures in 1 minute</td>\n<td>All operations allowed</td>\n<td>N/A</td>\n</tr>\n<tr>\n<td>Half-Open (Testing)</td>\n<td>N/A</td>\n<td>Limited operations for testing</td>\n<td>5 successful operations</td>\n</tr>\n<tr>\n<td>Open (Failed)</td>\n<td>≥3 failures in 1 minute</td>\n<td>All operations fail-fast</td>\n<td>30-second timeout elapsed</td>\n</tr>\n</tbody></table>\n<h4 id=\"virus-scanning-bottlenecks\">Virus Scanning Bottlenecks</h4>\n<p>Virus scanning operations introduce asynchronous processing that can create complex failure scenarios, particularly when scan times exceed upload completion times or when the virus scanner becomes unavailable.</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Upload Behavior</th>\n<th>Root Cause</th>\n<th>Resolution Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>&quot;Upload completes but file unavailable&quot;</td>\n<td>Upload shows complete, download fails</td>\n<td>File pending virus scan</td>\n<td>Implement scan status API</td>\n</tr>\n<tr>\n<td>&quot;Upload hangs at 100%&quot;</td>\n<td>Upload never transitions to completed</td>\n<td>Virus scanner timeout/failure</td>\n<td>Implement scan timeout with fallback</td>\n</tr>\n<tr>\n<td>&quot;False positive quarantine&quot;</td>\n<td>Valid files quarantined unnecessarily</td>\n<td>Scanner signature issues</td>\n<td>Implement quarantine review process</td>\n</tr>\n<tr>\n<td>&quot;Scanner unavailable&quot;</td>\n<td>All uploads fail validation</td>\n<td>ClamAV daemon down</td>\n<td>Implement scanner failover/bypass</td>\n</tr>\n</tbody></table>\n<p><strong>Virus Scanning State Management:</strong></p>\n<p>The virus scanning workflow requires careful state management to handle the asynchronous nature of file validation:</p>\n<table>\n<thead>\n<tr>\n<th>File State</th>\n<th>Virus Scan Status</th>\n<th>Client Behavior</th>\n<th>System Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Uploaded</td>\n<td>Scan pending</td>\n<td>Upload shows &quot;processing&quot;</td>\n<td>File queued for scanning</td>\n</tr>\n<tr>\n<td>Uploaded</td>\n<td>Scan in progress</td>\n<td>Upload shows &quot;validating&quot;</td>\n<td>File locked from access</td>\n</tr>\n<tr>\n<td>Uploaded</td>\n<td>Scan completed (clean)</td>\n<td>Upload shows &quot;completed&quot;</td>\n<td>File available for download</td>\n</tr>\n<tr>\n<td>Uploaded</td>\n<td>Scan completed (threat)</td>\n<td>Upload shows &quot;quarantined&quot;</td>\n<td>File moved to quarantine storage</td>\n</tr>\n<tr>\n<td>Uploaded</td>\n<td>Scan failed/timeout</td>\n<td>Upload shows &quot;validation failed&quot;</td>\n<td>File marked for manual review</td>\n</tr>\n</tbody></table>\n<h4 id=\"performance-degradation-patterns\">Performance Degradation Patterns</h4>\n<p>Performance issues in upload services typically manifest as increased upload times, timeout failures, or resource exhaustion. These issues often compound during high-load periods and require systematic analysis to identify bottlenecks.</p>\n<table>\n<thead>\n<tr>\n<th>Performance Symptom</th>\n<th>Measurement</th>\n<th>Likely Bottlenecks</th>\n<th>Diagnostic Tools</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Increased chunk upload latency</td>\n<td>&gt;2x normal response time</td>\n<td>Memory pressure, disk I/O contention</td>\n<td>Process monitoring, I/O metrics</td>\n</tr>\n<tr>\n<td>Upload timeout failures</td>\n<td>Client-side timeout triggers</td>\n<td>Network congestion, CPU starvation</td>\n<td>Network analysis, CPU profiling</td>\n</tr>\n<tr>\n<td>Reduced concurrent uploads</td>\n<td>Throughput decline under load</td>\n<td>File descriptor limits, memory exhaustion</td>\n<td>Resource monitoring, connection tracking</td>\n</tr>\n<tr>\n<td>Backend operation delays</td>\n<td>Storage API latency increases</td>\n<td>Backend rate limiting, credential issues</td>\n<td>Backend-specific monitoring</td>\n</tr>\n</tbody></table>\n<h3 id=\"systematic-troubleshooting-workflow\">Systematic Troubleshooting Workflow</h3>\n<p>Effective upload service debugging requires a systematic workflow that minimizes time-to-resolution while ensuring comprehensive diagnosis. This workflow provides a structured approach that scales from simple issues to complex multi-component failures.</p>\n<h4 id=\"primary-diagnostic-workflow\">Primary Diagnostic Workflow</h4>\n<p>The primary workflow follows a layered approach, starting with the most accessible diagnostic information and progressively diving deeper into system state and external dependencies.</p>\n<p><strong>Phase 1: Initial Assessment (0-5 minutes)</strong></p>\n<ol>\n<li><strong>Identify the failure scope</strong>: Determine if the issue affects a single upload session, a specific user, or the entire service</li>\n<li><strong>Collect basic session information</strong>: Session ID, user ID, upload timeline, current state</li>\n<li><strong>Check service health indicators</strong>: Overall system status, resource utilization, active alert conditions</li>\n<li><strong>Review recent deployments</strong>: Any configuration changes or code deployments in the last 24 hours</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Assessment Question</th>\n<th>Information Source</th>\n<th>Decision Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Single session or widespread?</td>\n<td>Session manager metrics</td>\n<td>&lt;5 affected sessions = isolated, &gt;50 = systemic</td>\n</tr>\n<tr>\n<td>Recent system changes?</td>\n<td>Deployment logs</td>\n<td>Changes within 24 hours require rollback consideration</td>\n</tr>\n<tr>\n<td>Resource constraints?</td>\n<td>Resource monitoring</td>\n<td>&gt;85% utilization indicates resource exhaustion</td>\n</tr>\n<tr>\n<td>External dependencies?</td>\n<td>Circuit breaker status</td>\n<td>Open circuits indicate backend failures</td>\n</tr>\n</tbody></table>\n<p><strong>Phase 2: Session State Analysis (5-15 minutes)</strong></p>\n<ol>\n<li><strong>Inspect upload session details</strong>: Current state, progress tracking, metadata consistency</li>\n<li><strong>Analyze state transition history</strong>: Identify where the session progression stopped or failed</li>\n<li><strong>Verify storage backend status</strong>: Check multipart upload state and chunk inventory</li>\n<li><strong>Review component interaction logs</strong>: Trace the flow of operations across system components</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Analysis Area</th>\n<th>Key Information</th>\n<th>Red Flags</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Session Metadata</td>\n<td>Creation time, total size, current offset</td>\n<td>Offset exceeds total size, negative values</td>\n</tr>\n<tr>\n<td>State Transitions</td>\n<td>State change timestamps, trigger events</td>\n<td>Missing transitions, rapid state cycling</td>\n</tr>\n<tr>\n<td>Storage Operations</td>\n<td>Backend operations, success/failure status</td>\n<td>Failed operations without retry</td>\n</tr>\n<tr>\n<td>Component Interactions</td>\n<td>Message flow, timing, error propagation</td>\n<td>Broken message chains, timeout patterns</td>\n</tr>\n</tbody></table>\n<p><strong>Phase 3: Deep System Investigation (15-45 minutes)</strong></p>\n<ol>\n<li><strong>Examine system resource usage patterns</strong>: Memory allocation, disk I/O, network utilization</li>\n<li><strong>Analyze external dependency health</strong>: Storage backend status, virus scanner availability</li>\n<li><strong>Investigate concurrent operation interference</strong>: Resource contention, lock conflicts</li>\n<li><strong>Review error correlation across components</strong>: Identify cascading failure patterns</li>\n</ol>\n<p><strong>Phase 4: Root Cause Confirmation (45-60 minutes)</strong></p>\n<ol>\n<li><strong>Reproduce the failure scenario</strong>: Create controlled test cases that trigger the same failure</li>\n<li><strong>Validate the hypothesized root cause</strong>: Confirm that fixing the suspected issue resolves the problem</li>\n<li><strong>Assess impact scope</strong>: Determine how many sessions or users are affected by the same root cause</li>\n<li><strong>Plan remediation strategy</strong>: Immediate fixes, preventive measures, and monitoring improvements</li>\n</ol>\n<h4 id=\"recovery-and-remediation-procedures\">Recovery and Remediation Procedures</h4>\n<p>Once root cause analysis identifies the underlying issue, systematic recovery procedures ensure service restoration while preventing data loss or corruption.</p>\n<table>\n<thead>\n<tr>\n<th>Recovery Scenario</th>\n<th>Immediate Actions</th>\n<th>Data Protection</th>\n<th>Prevention</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Session state corruption</td>\n<td>Reset session to last known good state</td>\n<td>Preserve uploaded chunks</td>\n<td>Implement state checksums</td>\n</tr>\n<tr>\n<td>Storage backend failure</td>\n<td>Activate failover backend</td>\n<td>Verify data consistency</td>\n<td>Improve health checking</td>\n</tr>\n<tr>\n<td>Resource exhaustion</td>\n<td>Enable load shedding</td>\n<td>Complete in-progress uploads</td>\n<td>Implement adaptive limits</td>\n</tr>\n<tr>\n<td>Virus scanner failure</td>\n<td>Bypass scanning with manual review</td>\n<td>Queue files for later scanning</td>\n<td>Implement scanner redundancy</td>\n</tr>\n</tbody></table>\n<p><strong>Session Recovery Procedures:</strong></p>\n<p>When upload sessions enter inconsistent states, recovery requires careful coordination between session state, storage backend state, and client expectations:</p>\n<ol>\n<li><strong>State Assessment</strong>: Compare session metadata with actual storage backend state</li>\n<li><strong>Consistency Verification</strong>: Verify that recorded progress matches stored chunks</li>\n<li><strong>Progress Recalculation</strong>: Rebuild progress information from storage backend inventory</li>\n<li><strong>Client Notification</strong>: Inform client of the correct resume offset if different from recorded state</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Recovery Step</th>\n<th>Action Taken</th>\n<th>Verification Required</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Assess current state</td>\n<td>Read session from state store</td>\n<td>Confirm session exists and is accessible</td>\n</tr>\n<tr>\n<td>Check storage consistency</td>\n<td>List stored chunks from backend</td>\n<td>Verify chunk sequence and sizes</td>\n</tr>\n<tr>\n<td>Recalculate progress</td>\n<td>Sum successfully stored chunk sizes</td>\n<td>Compare with recorded current offset</td>\n</tr>\n<tr>\n<td>Update session state</td>\n<td>Write corrected state to store</td>\n<td>Confirm state update succeeded</td>\n</tr>\n<tr>\n<td>Notify client</td>\n<td>Provide correct resume information</td>\n<td>Verify client can continue upload</td>\n</tr>\n</tbody></table>\n<p><strong>Data Integrity Verification:</strong></p>\n<p>Recovery procedures must include comprehensive data integrity verification to ensure that repaired sessions maintain data consistency:</p>\n<ol>\n<li><strong>Chunk Sequence Verification</strong>: Ensure all chunks are present and correctly ordered</li>\n<li><strong>Checksum Validation</strong>: Verify that chunk checksums match expected values</li>\n<li><strong>Size Consistency</strong>: Confirm that total stored size matches session metadata</li>\n<li><strong>Backend Consistency</strong>: Verify that storage backend state aligns with session state</li>\n</ol>\n<h4 id=\"preventive-monitoring-setup\">Preventive Monitoring Setup</h4>\n<p>Effective debugging depends on proactive monitoring that detects issues before they become critical failures. The monitoring setup should provide early warning indicators and automated response capabilities.</p>\n<table>\n<thead>\n<tr>\n<th>Monitoring Category</th>\n<th>Key Metrics</th>\n<th>Alert Conditions</th>\n<th>Automated Responses</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Session Health</td>\n<td>Session creation rate, completion rate, failure rate</td>\n<td>Completion rate drops below 95%</td>\n<td>Scale up resources, investigate failures</td>\n</tr>\n<tr>\n<td>Resource Utilization</td>\n<td>Memory usage, disk space, file descriptors</td>\n<td>Usage exceeds 85% of capacity</td>\n<td>Enable load shedding, trigger cleanup</td>\n</tr>\n<tr>\n<td>Backend Performance</td>\n<td>API latency, error rates, timeout frequency</td>\n<td>Latency increases 2x baseline</td>\n<td>Activate circuit breakers</td>\n</tr>\n<tr>\n<td>Security Events</td>\n<td>Virus detection rate, quarantine events</td>\n<td>Unusual threat patterns</td>\n<td>Alert security team, enhance scanning</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: Over-reliance on symptom monitoring</strong>\nMany teams focus exclusively on monitoring symptoms (failed uploads, high latency) rather than leading indicators (resource trends, backend health). This reactive approach means problems are discovered only after they impact users. Instead, implement predictive monitoring that tracks resource consumption trends and dependency health to prevent failures before they occur.</p>\n<blockquote>\n<p><strong>Key Insight:</strong> The most effective upload service debugging combines automated monitoring with human pattern recognition. Automated systems excel at detecting threshold breaches and known failure patterns, but complex issues often require human insight to connect disparate symptoms and identify novel failure modes.</p>\n</blockquote>\n<h4 id=\"advanced-debugging-techniques\">Advanced Debugging Techniques</h4>\n<p>Complex upload service issues may require advanced debugging techniques that go beyond standard monitoring and logging approaches.</p>\n<p><strong>Distributed Tracing for Upload Operations:</strong></p>\n<p>Implement distributed tracing that follows upload operations across all system components, providing complete visibility into operation flow and timing:</p>\n<table>\n<thead>\n<tr>\n<th>Trace Span</th>\n<th>Component</th>\n<th>Information Captured</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Upload Request</td>\n<td>HTTP Handler</td>\n<td>Request headers, session ID, chunk metadata</td>\n</tr>\n<tr>\n<td>Session Update</td>\n<td>Session Manager</td>\n<td>State transitions, persistence operations</td>\n</tr>\n<tr>\n<td>Storage Operation</td>\n<td>Storage Backend</td>\n<td>Backend API calls, operation timing</td>\n</tr>\n<tr>\n<td>Validation Process</td>\n<td>File Validator</td>\n<td>Type detection, virus scanning, results</td>\n</tr>\n</tbody></table>\n<p><strong>Chaos Engineering for Resilience Testing:</strong></p>\n<p>Implement controlled failure injection to verify system resilience and debugging effectiveness:</p>\n<table>\n<thead>\n<tr>\n<th>Failure Type</th>\n<th>Injection Method</th>\n<th>Expected Behavior</th>\n<th>Recovery Verification</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Storage backend timeout</td>\n<td>Network delay injection</td>\n<td>Circuit breaker activation</td>\n<td>Automatic failover to backup</td>\n</tr>\n<tr>\n<td>Memory pressure</td>\n<td>Artificial memory allocation</td>\n<td>Load shedding activation</td>\n<td>Graceful request rejection</td>\n</tr>\n<tr>\n<td>Virus scanner unavailability</td>\n<td>Process termination</td>\n<td>Fallback processing mode</td>\n<td>File queuing for later scan</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The debugging infrastructure requires comprehensive tooling that provides both real-time monitoring and historical analysis capabilities. This implementation provides the foundation for systematic troubleshooting of upload service issues.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Logging Framework</td>\n<td><code>log/slog</code> with JSON formatting</td>\n<td><code>go.uber.org/zap</code> with structured logging</td>\n</tr>\n<tr>\n<td>Metrics Collection</td>\n<td><code>expvar</code> with HTTP endpoint</td>\n<td><code>prometheus/client_golang</code> with Grafana</td>\n</tr>\n<tr>\n<td>Distributed Tracing</td>\n<td>Custom request ID propagation</td>\n<td><code>OpenTelemetry</code> with Jaeger backend</td>\n</tr>\n<tr>\n<td>Health Checking</td>\n<td>HTTP endpoints with JSON status</td>\n<td><code>grpc-health</code> with load balancer integration</td>\n</tr>\n<tr>\n<td>Admin Interface</td>\n<td>HTTP handlers with JSON responses</td>\n<td>Web dashboard with real-time updates</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-module-structure\">Recommended Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/debugging/\n  diagnostics/\n    session_inspector.go      ← Session state inspection tools\n    storage_analyzer.go       ← Storage backend consistency checking\n    resource_monitor.go       ← System resource tracking\n    protocol_validator.go     ← tus.io protocol compliance verification\n  logging/\n    structured_logger.go      ← Centralized logging infrastructure\n    log_correlation.go        ← Request correlation and tracing\n    audit_trail.go           ← Upload operation audit logging\n  monitoring/\n    metrics_collector.go      ← Application metrics gathering\n    alert_manager.go         ← Alert threshold management\n    circuit_breaker.go       ← Circuit breaker pattern implementation\n  recovery/\n    session_recovery.go       ← Upload session repair procedures\n    data_integrity.go        ← Data consistency verification\n    emergency_procedures.go   ← Critical failure response\ncmd/debug/\n  session_inspector/         ← CLI tool for session analysis\n  storage_analyzer/          ← CLI tool for storage investigation\n  recovery_tool/            ← Emergency recovery procedures</code></pre></div>\n\n<h4 id=\"structured-logging-infrastructure\">Structured Logging Infrastructure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// StructuredLogger provides comprehensive logging for upload operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StructuredLogger</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    component </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    baseFields </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewStructuredLogger creates a logger with component-specific configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewStructuredLogger</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">component</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">output</span><span style=\"color:#B392F0\"> io</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Writer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StructuredLogger</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create slog.Logger with JSON handler for structured output</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Configure log level based on environment (DEBUG, INFO, WARN, ERROR)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Set up base fields that appear in every log entry (service, version, component)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Enable request correlation ID propagation through context</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LogUploadEvent records significant upload operation events with full context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sl </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StructuredLogger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">LogUploadEvent</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">level</span><span style=\"color:#B392F0\"> slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Level</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    message</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">operation</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">fields</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Extract correlation ID from context for request tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Combine base fields with operation-specific fields</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Add standard upload fields (session_id, operation, timestamp)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Emit structured log entry with consistent field naming</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use consistent field names across all components for easier correlation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LogError provides specialized error logging with categorization and context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sl </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StructuredLogger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">LogError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">err</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    component</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">operation</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">retryable</span><span style=\"color:#F97583\"> bool</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Determine error category (ErrTransient, ErrPermanent, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Extract relevant context from error (stack trace, error chain)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Add debugging context (component state, operation parameters)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Include retry information and suggested recovery actions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"session-state-inspector\">Session State Inspector</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// SessionInspector provides detailed analysis of upload session state</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SessionInspector</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sessionManager </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SessionManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage        </span><span style=\"color:#B392F0\">StorageBackend</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger         </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StructuredLogger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// InspectSession performs comprehensive analysis of session state and consistency</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">si </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SessionInspector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">InspectSession</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SessionDiagnostics</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Retrieve session metadata from session manager</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Query storage backend for actual stored chunks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Compare session state with storage reality</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Identify inconsistencies and potential issues</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Calculate repair recommendations if state is corrupted</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SessionDiagnostics contains comprehensive session analysis results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SessionDiagnostics</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SessionID          </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                 `json:\"session_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CurrentState       </span><span style=\"color:#B392F0\">SessionStatus</span><span style=\"color:#9ECBFF\">          `json:\"current_state\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StateConsistent    </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">                  `json:\"state_consistent\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StoredChunkCount   </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">                   `json:\"stored_chunk_count\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RecordedOffset     </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">                 `json:\"recorded_offset\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ActualStoredSize   </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">                 `json:\"actual_stored_size\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MissingChunks      []</span><span style=\"color:#B392F0\">ChunkRange</span><span style=\"color:#9ECBFF\">          `json:\"missing_chunks,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CorruptedChunks    []</span><span style=\"color:#B392F0\">CorruptionReport</span><span style=\"color:#9ECBFF\">    `json:\"corrupted_chunks,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RepairActions      []</span><span style=\"color:#B392F0\">RepairAction</span><span style=\"color:#9ECBFF\">        `json:\"repair_actions,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Recommendations    []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">              `json:\"recommendations\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AnalyzeInconsistency identifies specific types of session state problems</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">si </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SessionInspector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">AnalyzeInconsistency</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">session</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    storageChunks</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">ChunkInfo</span><span style=\"color:#E1E4E8\">) []</span><span style=\"color:#B392F0\">InconsistencyReport</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Compare recorded offset with sum of stored chunk sizes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Identify gaps in chunk sequence (missing chunks)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Detect overlapping or duplicate chunks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Verify chunk checksums against stored data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Check for storage backend multipart upload state alignment</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Build a bitmap of stored chunks for gap analysis</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"resource-monitoring-system\">Resource Monitoring System</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// ResourceMonitor tracks system resource utilization and triggers protective actions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ResourceMonitor</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    memoryWarning   </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    memoryCritical  </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    diskWarning     </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    diskCritical    </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    activeUploads   </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    maxUploads      </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    alerts          </span><span style=\"color:#F97583\">chan</span><span style=\"color:#B392F0\"> ResourceAlert</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    circuitBreaker  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CircuitBreaker</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetCurrentStatus returns comprehensive resource utilization information</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ResourceMonitor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetCurrentStatus</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ResourceStatus</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Collect current memory usage from runtime statistics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Query disk space utilization for upload storage paths</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Count active upload sessions and concurrent operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Check file descriptor usage against system limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Calculate resource utilization percentages</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use runtime.ReadMemStats() for memory information</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ShouldAcceptUpload determines if system resources allow new upload acceptance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ResourceMonitor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ShouldAcceptUpload</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check current resource status against warning thresholds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Verify that accepting new upload won't exceed critical limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Consider current upload completion rate and estimated resource usage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Apply adaptive limits based on recent resource consumption trends</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Implement hysteresis to prevent rapid accept/reject oscillation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MonitorResources runs continuous resource monitoring with alert generation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ResourceMonitor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">MonitorResources</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ticker </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">NewTicker</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> ticker.</span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        select</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">ctx.</span><span style=\"color:#B392F0\">Done</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> ctx.</span><span style=\"color:#B392F0\">Err</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">ticker.C:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 1: Collect current resource status from system</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 2: Compare against warning and critical thresholds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 3: Generate alerts for threshold violations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 4: Update circuit breaker state based on resource health</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 5: Log resource trends for capacity planning</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"recovery-coordination-system\">Recovery Coordination System</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// RecoveryCoordinator manages systematic recovery from upload service failures</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RecoveryCoordinator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sessionManager   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SessionManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storageBackend   </span><span style=\"color:#B392F0\">StorageBackend</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resourceMonitor  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ResourceMonitor</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    circuitBreaker   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CircuitBreaker</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger          </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StructuredLogger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HandleUploadError implements comprehensive error handling with recovery attempts</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RecoveryCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HandleUploadError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">err</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">UploadError</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Categorize error type (transient, permanent, resource, security)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Determine appropriate recovery strategy based on error category</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Attempt automatic recovery if error is transient and retryable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update circuit breaker state based on error patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Log error with full context for debugging analysis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use exponential backoff for transient error retries</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RecoverFromStorageFailure handles storage backend failures with failover</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RecoveryCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RecoverFromStorageFailure</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">operation</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Assess storage backend health and failure scope</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Determine if failover to alternate backend is possible</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Migrate session data to healthy storage backend if needed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update session metadata to reflect new storage location</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Notify monitoring systems of backend failover event</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HandleChunkCorruption manages detection and recovery from data corruption</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RecoveryCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HandleChunkCorruption</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">chunkOffset</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">expectedHash</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">actualHash</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Quarantine corrupted chunk to prevent further processing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Request client to re-upload the corrupted chunk</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Update session state to reflect chunk re-upload requirement</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Log corruption event with forensic details for investigation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Check for patterns indicating systematic corruption issues</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Implement chunk re-upload with different storage path to avoid overwrite</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Checkpoint 1: Basic Diagnostic Infrastructure</strong></p>\n<ul>\n<li>Run <code>go test ./internal/debugging/diagnostics/...</code> - all tests pass</li>\n<li>Start debug HTTP server with <code>go run cmd/debug/server/main.go</code></li>\n<li>Access <code>http://localhost:8080/debug/sessions</code> - returns JSON list of sessions</li>\n<li>Access <code>http://localhost:8080/debug/metrics</code> - returns resource utilization</li>\n<li>Verify structured logs contain required fields: timestamp, level, component, session_id</li>\n</ul>\n<p><strong>Checkpoint 2: Session State Analysis</strong></p>\n<ul>\n<li>Create test upload session with <code>curl -X POST http://localhost:8080/upload/init</code></li>\n<li>Inspect session with <code>go run cmd/debug/session_inspector/main.go &lt;session-id&gt;</code></li>\n<li>Verify inspector reports: session state, stored chunks, consistency status</li>\n<li>Introduce artificial state corruption and verify detection</li>\n<li>Confirm repair recommendations are generated for inconsistent state</li>\n</ul>\n<p><strong>Checkpoint 3: Resource Monitoring Integration</strong></p>\n<ul>\n<li>Monitor resource usage under load: <code>go run cmd/debug/load_test/main.go</code></li>\n<li>Verify memory and disk alerts trigger at configured thresholds</li>\n<li>Confirm load shedding activates when resources are constrained</li>\n<li>Test circuit breaker behavior during simulated backend failures</li>\n<li>Validate automatic recovery when resources become available</li>\n</ul>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Upload resumes from wrong offset</td>\n<td>Session state/storage mismatch</td>\n<td>Compare session.CurrentOffset with storage backend chunk list</td>\n<td>Recalculate offset from stored chunks</td>\n</tr>\n<tr>\n<td>Virus scanning never completes</td>\n<td>ClamAV daemon unavailable</td>\n<td>Check scanner.Ping() and socket connectivity</td>\n<td>Restart ClamAV, implement scan timeout</td>\n</tr>\n<tr>\n<td>High memory usage during uploads</td>\n<td>Chunk buffering in memory</td>\n<td>Check ResourceStatus.MemoryUsed during upload</td>\n<td>Implement streaming chunk processing</td>\n</tr>\n<tr>\n<td>Storage operations timeout</td>\n<td>Backend overloaded or credentials expired</td>\n<td>Review storage backend circuit breaker state</td>\n<td>Refresh credentials, implement retry with backoff</td>\n</tr>\n<tr>\n<td>Session cleanup too aggressive</td>\n<td>CleanupConfig.SessionTTL too short</td>\n<td>Check expired session count vs. active sessions</td>\n<td>Increase SessionTTL, improve cleanup logic</td>\n</tr>\n<tr>\n<td>Upload stuck at completion</td>\n<td>Async virus scan blocking</td>\n<td>Check file validation queue and scan status</td>\n<td>Implement scan status API, add timeout handling</td>\n</tr>\n</tbody></table>\n<h2 id=\"future-extensions-and-scalability\">Future Extensions and Scalability</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Extends all milestones (1, 2, 3) — this section describes planned enhancements and scaling considerations beyond the chunked upload protocol (M1), storage abstraction (M2), and virus scanning implementation (M3)</p>\n</blockquote>\n<p>Think of the current resumable file upload service as a <strong>solid foundation building</strong> that we&#39;ve constructed with careful engineering. We&#39;ve built the core structure with proper foundations (chunked upload protocol), versatile infrastructure (storage abstraction), and essential safety systems (virus scanning). Now we need to plan for the future: how do we add more floors to handle more residents (horizontal scaling), how do we upgrade the utilities for better efficiency (performance optimizations), and what new amenities can we add to make the building more valuable (advanced features)?</p>\n<p>This architectural evolution must be planned carefully because retrofitting a building after construction is expensive and disruptive. By designing our extension points and scalability patterns now, we ensure that future growth doesn&#39;t require rebuilding our foundation. The key insight is that <strong>scalability isn&#39;t just about handling more load</strong> — it&#39;s about maintaining system reliability, performance, and feature velocity as complexity increases.</p>\n<p>The extensions we plan fall into three categories: making the current system faster and more efficient, adding sophisticated new capabilities that weren&#39;t in the original requirements, and preparing the architecture to scale horizontally across multiple machines and data centers. Each category presents different technical challenges and requires different architectural considerations.</p>\n<h3 id=\"performance-optimizations\">Performance Optimizations</h3>\n<p>Think of performance optimization as <strong>upgrading the plumbing and electrical systems</strong> in our building analogy. The basic functionality remains the same, but everything flows more efficiently. These optimizations focus on reducing latency, increasing throughput, and minimizing resource consumption without changing the fundamental API contracts or user experience.</p>\n<h4 id=\"parallel-upload-acceleration\">Parallel Upload Acceleration</h4>\n<p>The current implementation processes chunks sequentially within each upload session, which leaves significant performance on the table for users with high-bandwidth connections. <strong>Parallel chunk processing</strong> represents a fundamental shift from a sequential pipeline to a concurrent processing model.</p>\n<blockquote>\n<p><strong>Decision: Client-Driven Parallel Upload Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Users with high-bandwidth connections are bottlenecked by sequential chunk processing, especially for large files where network latency becomes a significant factor in total upload time.</li>\n<li><strong>Options Considered</strong>: Server-controlled parallelism (server dictates chunk ordering), client-controlled parallelism (client manages concurrent streams), hybrid approach (negotiated parallelism levels)</li>\n<li><strong>Decision</strong>: Client-controlled parallelism with server-side coordination and ordering</li>\n<li><strong>Rationale</strong>: Client has the best knowledge of its network conditions and can optimize chunk size and concurrency for its specific situation. Server maintains session integrity while allowing flexible client strategies.</li>\n<li><strong>Consequences</strong>: Requires enhanced session state management for out-of-order chunk arrival, increases server memory usage for buffering, but provides maximum flexibility for client optimization.</li>\n</ul>\n</blockquote>\n<p>The parallel upload architecture introduces several new data structures and processing patterns:</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Current Behavior</th>\n<th>Optimized Behavior</th>\n<th>Performance Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Chunk Reception</td>\n<td>Sequential, blocking on storage</td>\n<td>Concurrent reception with async storage</td>\n<td>3-5x throughput increase</td>\n</tr>\n<tr>\n<td>Session State</td>\n<td>Single offset tracking</td>\n<td>Range-based completion tracking</td>\n<td>Supports out-of-order chunks</td>\n</tr>\n<tr>\n<td>Storage Backend</td>\n<td>Immediate chunk writes</td>\n<td>Batched writes with assembly optimization</td>\n<td>Reduces storage API calls by 60-80%</td>\n</tr>\n<tr>\n<td>Memory Usage</td>\n<td>Single chunk buffer per session</td>\n<td>Configurable chunk pool with backpressure</td>\n<td>Predictable memory bounds</td>\n</tr>\n</tbody></table>\n<p>The <code>ParallelUploadCoordinator</code> manages the complexity of concurrent chunk processing:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ChunkBuffer</td>\n<td>map[int64]*ChunkMetadata</td>\n<td>Temporarily holds out-of-order chunks until gaps are filled</td>\n</tr>\n<tr>\n<td>CompletionRanges</td>\n<td>[]OffsetRange</td>\n<td>Tracks which byte ranges have been successfully stored</td>\n</tr>\n<tr>\n<td>ConcurrencyLimit</td>\n<td>int</td>\n<td>Maximum concurrent chunks per session to prevent resource exhaustion</td>\n</tr>\n<tr>\n<td>BackpressureThreshold</td>\n<td>int64</td>\n<td>Memory usage threshold that triggers client backpressure</td>\n</tr>\n<tr>\n<td>AssemblyStrategy</td>\n<td>AssemblyMode</td>\n<td>Controls when chunks are written to final storage (immediate, batched, deferred)</td>\n</tr>\n</tbody></table>\n<p>The key algorithmic challenge is <strong>gap detection and assembly optimization</strong>. When chunks arrive out of order, we need to efficiently track which portions of the file are complete and when we have enough contiguous data to write to the storage backend:</p>\n<ol>\n<li><strong>Chunk Arrival Processing</strong>: Each incoming chunk is validated independently and placed in the appropriate position within the session&#39;s completion map</li>\n<li><strong>Gap Analysis</strong>: After each chunk, the coordinator scans for newly-completed contiguous ranges that can be flushed to storage</li>\n<li><strong>Batch Assembly</strong>: Contiguous chunks are batched together to minimize storage backend API calls and improve write efficiency</li>\n<li><strong>Memory Pressure Management</strong>: When buffer usage exceeds thresholds, the coordinator forces assembly of available ranges and applies backpressure to the client</li>\n<li><strong>Completion Detection</strong>: The upload is considered complete when all byte ranges from 0 to TotalSize are marked as received and flushed</li>\n</ol>\n<blockquote>\n<p>The critical insight for parallel uploads is that <strong>ordering matters for storage efficiency but not for network efficiency</strong>. We can receive chunks in any order while still maintaining the ability to efficiently write them to storage backends that prefer sequential writes.</p>\n</blockquote>\n<h4 id=\"bandwidth-optimization-and-compression\">Bandwidth Optimization and Compression</h4>\n<p><strong>Compression integration</strong> adds another layer of efficiency by reducing the actual bytes transferred over the network. However, compression for resumable uploads introduces unique challenges because traditional compression algorithms depend on previously seen data, making random-access resumption difficult.</p>\n<p>The <code>CompressionCoordinator</code> implements a <strong>chunked compression strategy</strong> that maintains resumability:</p>\n<table>\n<thead>\n<tr>\n<th>Strategy</th>\n<th>Compression Ratio</th>\n<th>Resume Complexity</th>\n<th>CPU Overhead</th>\n<th>Best For</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Per-chunk compression</td>\n<td>60-70% of full-file</td>\n<td>Simple - each chunk independent</td>\n<td>Low</td>\n<td>General purpose files</td>\n</tr>\n<tr>\n<td>Sliding window compression</td>\n<td>80-90% of full-file</td>\n<td>Complex - requires window state</td>\n<td>Medium</td>\n<td>Text and code files</td>\n</tr>\n<tr>\n<td>Content-aware compression</td>\n<td>Varies by file type</td>\n<td>File-type dependent</td>\n<td>Variable</td>\n<td>Mixed workloads</td>\n</tr>\n</tbody></table>\n<p>Per-chunk compression offers the best balance for resumable uploads because each chunk can be compressed and decompressed independently. This maintains the fundamental property that any chunk can be re-uploaded without affecting others, while still providing significant bandwidth savings.</p>\n<p>The compression implementation requires careful coordination with the storage backend:</p>\n<ol>\n<li><strong>Compression Detection</strong>: The service detects whether incoming chunks are compressed based on client headers and magic byte inspection</li>\n<li><strong>Storage Coordination</strong>: Compressed chunks are stored with metadata indicating compression status and original size</li>\n<li><strong>Assembly Planning</strong>: During final assembly, the service determines whether to store the file compressed or decompressed based on storage backend capabilities and access patterns</li>\n<li><strong>Client Negotiation</strong>: Clients negotiate compression capabilities during upload initialization to ensure compatibility</li>\n</ol>\n<h4 id=\"intelligent-chunk-size-adaptation\">Intelligent Chunk Size Adaptation</h4>\n<p><strong>Adaptive chunk sizing</strong> represents a sophisticated optimization that adjusts chunk size based on real-time network conditions and client capabilities. Instead of using fixed chunk sizes, the service can guide clients toward optimal chunk sizes for their specific conditions.</p>\n<p>The <code>ChunkSizeOptimizer</code> analyzes upload patterns and provides recommendations:</p>\n<table>\n<thead>\n<tr>\n<th>Metric</th>\n<th>Small Chunks (64KB)</th>\n<th>Medium Chunks (1MB)</th>\n<th>Large Chunks (10MB)</th>\n<th>Recommendation Logic</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>High Latency Networks</td>\n<td>Poor (many round trips)</td>\n<td>Good</td>\n<td>Excellent</td>\n<td>Prefer larger chunks to amortize latency</td>\n</tr>\n<tr>\n<td>Unstable Networks</td>\n<td>Excellent (minimal reupload)</td>\n<td>Good</td>\n<td>Poor (large retransmission cost)</td>\n<td>Prefer smaller chunks to minimize retry cost</td>\n</tr>\n<tr>\n<td>High Bandwidth</td>\n<td>Poor (underutilization)</td>\n<td>Good</td>\n<td>Excellent</td>\n<td>Scale chunk size with available bandwidth</td>\n</tr>\n<tr>\n<td>Memory Constrained</td>\n<td>Excellent</td>\n<td>Good</td>\n<td>Poor</td>\n<td>Limit chunk size based on client memory</td>\n</tr>\n</tbody></table>\n<p>The optimization algorithm considers multiple factors:</p>\n<ol>\n<li><strong>Network Condition Assessment</strong>: Measure round-trip time, bandwidth, and packet loss patterns from recent upload history</li>\n<li><strong>Error Rate Analysis</strong>: Track chunk failure rates and identify patterns that suggest optimal chunk sizes for reliability</li>\n<li><strong>Client Capability Detection</strong>: Use client headers and behavior patterns to infer memory and processing constraints</li>\n<li><strong>Dynamic Recommendation</strong>: Provide chunk size recommendations during upload progress that adapt to changing conditions</li>\n<li><strong>Fallback Strategy</strong>: Maintain conservative defaults when insufficient data is available for optimization</li>\n</ol>\n<h4 id=\"caching-and-content-deduplication\">Caching and Content Deduplication</h4>\n<p><strong>Content deduplication</strong> at the chunk level can dramatically reduce storage costs and transfer times for environments where similar files are frequently uploaded. Think of this as a <strong>smart warehouse</strong> that recognizes when someone tries to store something that&#39;s already on the shelf.</p>\n<p>The <code>DeduplicationManager</code> implements hash-based chunk deduplication:</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Responsibility</th>\n<th>Performance Impact</th>\n<th>Storage Savings</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Chunk Fingerprinting</td>\n<td>Generate SHA-256 hashes for all chunks</td>\n<td>5-10% CPU overhead</td>\n<td>Enables deduplication</td>\n</tr>\n<tr>\n<td>Deduplication Index</td>\n<td>Track hash-to-location mappings</td>\n<td>Memory usage scales with unique chunks</td>\n<td>30-80% for similar files</td>\n</tr>\n<tr>\n<td>Reference Counting</td>\n<td>Manage chunk lifecycle and cleanup</td>\n<td>Minimal overhead</td>\n<td>Prevents premature deletion</td>\n</tr>\n<tr>\n<td>Client Notification</td>\n<td>Inform clients about deduplicated chunks</td>\n<td>Network savings</td>\n<td>Eliminates redundant transfers</td>\n</tr>\n</tbody></table>\n<p>The deduplication workflow integrates seamlessly with the existing upload process:</p>\n<ol>\n<li><strong>Hash Calculation</strong>: As chunks arrive, calculate cryptographic hashes alongside existing validation</li>\n<li><strong>Deduplication Check</strong>: Before storing, check if this chunk hash already exists in the deduplication index</li>\n<li><strong>Reference Management</strong>: If the chunk exists, increment reference count and skip storage; if not, store normally and add to index</li>\n<li><strong>Client Notification</strong>: Inform the client that the chunk was deduplicated so it can skip that portion of the upload</li>\n<li><strong>Garbage Collection</strong>: When files are deleted, decrement reference counts and clean up unreferenced chunks</li>\n</ol>\n<p>The key architectural decision is whether to implement <strong>global deduplication</strong> (across all users) or <strong>per-tenant deduplication</strong> (within user or organization boundaries):</p>\n<blockquote>\n<p><strong>Decision: Tenant-Scoped Deduplication with Optional Global Mode</strong></p>\n<ul>\n<li><strong>Context</strong>: Global deduplication provides maximum storage efficiency but raises security concerns about information leakage and complicates access control</li>\n<li><strong>Options Considered</strong>: Global deduplication (maximum efficiency), per-tenant deduplication (security isolation), no deduplication (simplicity)</li>\n<li><strong>Decision</strong>: Per-tenant deduplication by default with configurable global mode for appropriate use cases</li>\n<li><strong>Rationale</strong>: Balances security isolation with significant storage savings within tenant boundaries. Global mode can be enabled for public content or trusted environments.</li>\n<li><strong>Consequences</strong>: Reduces storage efficiency compared to global deduplication but eliminates security concerns about cross-tenant data leakage</li>\n</ul>\n</blockquote>\n<h3 id=\"advanced-feature-additions\">Advanced Feature Additions</h3>\n<p>Advanced features represent <strong>new amenities</strong> in our building analogy — capabilities that weren&#39;t in the original requirements but add significant value for specific use cases. These features often require careful integration with the existing architecture to avoid disrupting core functionality.</p>\n<h4 id=\"file-encryption-and-security-enhancements\">File Encryption and Security Enhancements</h4>\n<p><strong>Client-side encryption support</strong> allows users to encrypt files before upload, ensuring that even the service operators cannot access file contents. This is particularly important for compliance with privacy regulations and for handling sensitive data.</p>\n<p>The <code>EncryptionCoordinator</code> provides a framework for various encryption strategies:</p>\n<table>\n<thead>\n<tr>\n<th>Encryption Mode</th>\n<th>Key Management</th>\n<th>Performance Impact</th>\n<th>Security Level</th>\n<th>Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Client-managed</td>\n<td>Client holds all keys</td>\n<td>Minimal server overhead</td>\n<td>Highest</td>\n<td>Maximum privacy</td>\n</tr>\n<tr>\n<td>Service-managed</td>\n<td>Server manages keys per tenant</td>\n<td>Moderate processing overhead</td>\n<td>High</td>\n<td>Ease of use with strong security</td>\n</tr>\n<tr>\n<td>Hybrid</td>\n<td>Client keys, server-assisted operations</td>\n<td>Balanced overhead</td>\n<td>Very High</td>\n<td>Enterprise compliance</td>\n</tr>\n</tbody></table>\n<p>Client-side encryption introduces several architectural considerations:</p>\n<ol>\n<li><strong>Chunk-Level Encryption</strong>: Each chunk must be encrypted independently to maintain resumability, requiring careful initialization vector (IV) management</li>\n<li><strong>Metadata Handling</strong>: Determine which metadata can be encrypted and which must remain accessible for service operations</li>\n<li><strong>Key Rotation Support</strong>: Provide mechanisms for encrypting new chunks with updated keys while maintaining access to existing content</li>\n<li><strong>Compliance Integration</strong>: Ensure encryption implementation meets relevant compliance standards (FIPS 140-2, Common Criteria)</li>\n</ol>\n<p>The <code>EncryptedUploadSession</code> extends the base upload session model:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>EncryptionMode</td>\n<td>EncryptionType</td>\n<td>Specifies the encryption algorithm and mode (AES-256-GCM, ChaCha20-Poly1305)</td>\n</tr>\n<tr>\n<td>KeyDerivationInfo</td>\n<td>KeyDerivationParams</td>\n<td>Parameters for generating chunk keys from master keys</td>\n</tr>\n<tr>\n<td>MetadataEncryption</td>\n<td>bool</td>\n<td>Whether filename and user metadata are encrypted</td>\n</tr>\n<tr>\n<td>ComplianceProfile</td>\n<td>string</td>\n<td>Compliance requirements that affect encryption choices</td>\n</tr>\n</tbody></table>\n<h4 id=\"advanced-metadata-processing\">Advanced Metadata Processing</h4>\n<p><strong>Metadata extraction and indexing</strong> transforms the upload service from simple storage into an intelligent content management system. Think of this as adding a <strong>smart librarian</strong> who automatically catalogs everything that gets stored.</p>\n<p>The <code>MetadataProcessor</code> supports extensible content analysis:</p>\n<table>\n<thead>\n<tr>\n<th>Content Type</th>\n<th>Extracted Metadata</th>\n<th>Processing Time</th>\n<th>Storage Overhead</th>\n<th>Search Benefits</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Images</td>\n<td>EXIF data, dimensions, color profiles</td>\n<td>100-500ms</td>\n<td>2-5KB per file</td>\n<td>Location, date, camera search</td>\n</tr>\n<tr>\n<td>Documents</td>\n<td>Text content, author, creation date</td>\n<td>1-10s</td>\n<td>10-100KB per file</td>\n<td>Full-text search, author filtering</td>\n</tr>\n<tr>\n<td>Media</td>\n<td>Duration, codecs, quality metrics</td>\n<td>2-30s</td>\n<td>5-20KB per file</td>\n<td>Duration filtering, quality analysis</td>\n</tr>\n<tr>\n<td>Archives</td>\n<td>File listings, compression ratios</td>\n<td>1-60s</td>\n<td>1-50KB per file</td>\n<td>Content discovery</td>\n</tr>\n</tbody></table>\n<p>The metadata processing pipeline operates asynchronously after upload completion to avoid impacting upload performance:</p>\n<ol>\n<li><strong>Content Type Detection</strong>: Use magic byte analysis to determine file type and appropriate metadata extractors</li>\n<li><strong>Extraction Orchestration</strong>: Route files to appropriate processors based on content type and configured extraction policies</li>\n<li><strong>Metadata Normalization</strong>: Convert extracted metadata into standardized formats for consistent querying</li>\n<li><strong>Index Integration</strong>: Store extracted metadata in searchable indexes while maintaining privacy boundaries</li>\n<li><strong>Client Notification</strong>: Inform clients when metadata processing is complete and results are available</li>\n</ol>\n<p>The <code>MetadataExtractor</code> interface allows pluggable processing strategies:</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>SupportedTypes</td>\n<td>none</td>\n<td>[]string</td>\n<td>Returns MIME types this extractor can process</td>\n</tr>\n<tr>\n<td>ExtractMetadata</td>\n<td>ctx Context, reader io.Reader, contentType string</td>\n<td>*MetadataResult, error</td>\n<td>Processes file content and returns extracted metadata</td>\n</tr>\n<tr>\n<td>EstimateProcessingTime</td>\n<td>fileSize int64, contentType string</td>\n<td>time.Duration</td>\n<td>Provides time estimate for scheduling decisions</td>\n</tr>\n<tr>\n<td>RequiredResources</td>\n<td>fileSize int64, contentType string</td>\n<td>ResourceRequirements</td>\n<td>Specifies CPU, memory needs for resource planning</td>\n</tr>\n</tbody></table>\n<h4 id=\"intelligent-virus-scanning-optimization\">Intelligent Virus Scanning Optimization</h4>\n<p><strong>ML-enhanced virus scanning</strong> goes beyond traditional signature-based detection to provide more sophisticated threat analysis. This represents an evolution from reactive scanning to <strong>predictive security analysis</strong>.</p>\n<p>The <code>MLScanningCoordinator</code> integrates multiple detection strategies:</p>\n<table>\n<thead>\n<tr>\n<th>Scanning Method</th>\n<th>Detection Rate</th>\n<th>False Positive Rate</th>\n<th>Processing Time</th>\n<th>Resource Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Signature-based (ClamAV)</td>\n<td>85-90%</td>\n<td>&lt; 0.1%</td>\n<td>1-10s</td>\n<td>Low CPU, minimal memory</td>\n</tr>\n<tr>\n<td>Heuristic analysis</td>\n<td>70-85%</td>\n<td>1-5%</td>\n<td>5-30s</td>\n<td>Medium CPU, moderate memory</td>\n</tr>\n<tr>\n<td>ML behavioral analysis</td>\n<td>90-95%</td>\n<td>2-8%</td>\n<td>10-120s</td>\n<td>High CPU, significant memory</td>\n</tr>\n<tr>\n<td>Hybrid approach</td>\n<td>95-98%</td>\n<td>0.5-2%</td>\n<td>Variable</td>\n<td>Adaptive resource usage</td>\n</tr>\n</tbody></table>\n<p>The ML scanning pipeline processes files through multiple analysis stages:</p>\n<ol>\n<li><strong>Quick Signature Scan</strong>: Run traditional signature-based scanning first for known threats with minimal resource usage</li>\n<li><strong>Risk Assessment</strong>: Use file characteristics (size, type, source) to determine whether additional analysis is warranted</li>\n<li><strong>Behavioral Analysis</strong>: For high-risk files, perform ML-based analysis of file structure and embedded content patterns</li>\n<li><strong>Threat Correlation</strong>: Cross-reference findings across multiple detection methods to reduce false positives</li>\n<li><strong>Adaptive Learning</strong>: Update ML models based on confirmed threats and false positive feedback</li>\n</ol>\n<p>The <code>MLThreatAnalyzer</code> provides configurable analysis depth:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ModelVersion</td>\n<td>string</td>\n<td>Version identifier for the ML model being used</td>\n</tr>\n<tr>\n<td>AnalysisDepth</td>\n<td>ScanDepth</td>\n<td>Configures thoroughness vs. speed trade-off</td>\n</tr>\n<tr>\n<td>ConfidenceThreshold</td>\n<td>float64</td>\n<td>Minimum confidence required for threat classification</td>\n</tr>\n<tr>\n<td>FallbackStrategy</td>\n<td>FallbackMode</td>\n<td>How to handle analysis failures or timeouts</td>\n</tr>\n<tr>\n<td>LearningEnabled</td>\n<td>bool</td>\n<td>Whether to feed results back into model training</td>\n</tr>\n</tbody></table>\n<h4 id=\"content-distribution-and-cdn-integration\">Content Distribution and CDN Integration</h4>\n<p><strong>CDN integration</strong> transforms the upload service into a comprehensive content delivery platform. Once files are uploaded and validated, they can be automatically distributed to edge locations for optimal global access performance.</p>\n<p>The <code>CDNCoordinator</code> manages distribution policies and edge synchronization:</p>\n<table>\n<thead>\n<tr>\n<th>Distribution Strategy</th>\n<th>Use Case</th>\n<th>Sync Time</th>\n<th>Cost Impact</th>\n<th>Global Performance</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>On-demand</td>\n<td>Infrequently accessed files</td>\n<td>Minutes to hours</td>\n<td>Pay-per-access</td>\n<td>Good after first access</td>\n</tr>\n<tr>\n<td>Predictive</td>\n<td>Files with expected global access</td>\n<td>10-60 minutes</td>\n<td>Higher base cost</td>\n<td>Excellent immediately</td>\n</tr>\n<tr>\n<td>Tiered</td>\n<td>Mixed workloads</td>\n<td>Variable by tier</td>\n<td>Optimized cost</td>\n<td>Good for popular content</td>\n</tr>\n</tbody></table>\n<p>The CDN integration workflow coordinates with existing storage backends:</p>\n<ol>\n<li><strong>Distribution Policy Evaluation</strong>: After successful upload, evaluate file characteristics against distribution policies</li>\n<li><strong>Edge Deployment</strong>: For files meeting distribution criteria, initiate replication to configured CDN edge locations</li>\n<li><strong>URL Management</strong>: Generate CDN-backed URLs for client access while maintaining fallback to origin storage</li>\n<li><strong>Invalidation Coordination</strong>: When files are updated or deleted, coordinate cache invalidation across edge locations</li>\n<li><strong>Analytics Integration</strong>: Track access patterns to optimize future distribution decisions</li>\n</ol>\n<h3 id=\"horizontal-scaling-considerations\">Horizontal Scaling Considerations</h3>\n<p>Think of horizontal scaling as <strong>expanding from a single building to a campus</strong> — we need to coordinate activities across multiple buildings while maintaining a unified experience for residents. This requires fundamental changes to how we handle state, coordinate operations, and maintain consistency.</p>\n<h4 id=\"multi-instance-deployment-patterns\">Multi-Instance Deployment Patterns</h4>\n<p><strong>Stateless service design</strong> is the foundation for horizontal scaling. Each service instance should be able to handle any request without depending on local state from previous requests. This requires externalizing all persistent state and coordinating shared operations.</p>\n<p>The current architecture already anticipates some scaling challenges through the <code>StateStore</code> abstraction, but production scaling requires additional coordination mechanisms:</p>\n<table>\n<thead>\n<tr>\n<th>Scaling Pattern</th>\n<th>Complexity</th>\n<th>Consistency</th>\n<th>Performance</th>\n<th>Best For</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Active-Active</td>\n<td>High</td>\n<td>Eventually consistent</td>\n<td>Excellent</td>\n<td>Global, high-throughput workloads</td>\n</tr>\n<tr>\n<td>Active-Passive</td>\n<td>Medium</td>\n<td>Strongly consistent</td>\n<td>Good</td>\n<td>Regional deployments with failover</td>\n</tr>\n<tr>\n<td>Sharded</td>\n<td>High</td>\n<td>Partition-wise consistent</td>\n<td>Excellent</td>\n<td>Very large scale with data locality</td>\n</tr>\n</tbody></table>\n<p>The <code>ScalingCoordinator</code> manages instance coordination and load distribution:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>InstanceID</td>\n<td>string</td>\n<td>Unique identifier for this service instance</td>\n</tr>\n<tr>\n<td>LoadBalancingStrategy</td>\n<td>LBStrategy</td>\n<td>How to distribute uploads across instances</td>\n</tr>\n<tr>\n<td>SessionAffinityMode</td>\n<td>AffinityMode</td>\n<td>Whether uploads must stick to specific instances</td>\n</tr>\n<tr>\n<td>HeartbeatInterval</td>\n<td>time.Duration</td>\n<td>Frequency of instance health reporting</td>\n</tr>\n<tr>\n<td>FailoverTimeout</td>\n<td>time.Duration</td>\n<td>How long to wait before assuming instance failure</td>\n</tr>\n</tbody></table>\n<h4 id=\"distributed-state-management\">Distributed State Management</h4>\n<p><strong>External state storage</strong> becomes critical for horizontal scaling. The simple in-memory <code>MemoryStore</code> that works for single instances must be replaced with distributed storage systems that provide consistency guarantees across multiple service instances.</p>\n<blockquote>\n<p><strong>Decision: Redis-Based Session State with Database Backup</strong></p>\n<ul>\n<li><strong>Context</strong>: Horizontal scaling requires session state that&#39;s accessible from any service instance, with high performance for frequent reads/writes and durability for crash recovery</li>\n<li><strong>Options Considered</strong>: Database-only storage (simple but slow), Redis-only storage (fast but less durable), hybrid Redis+database approach (complex but optimal)</li>\n<li><strong>Decision</strong>: Redis as primary session store with periodic database synchronization</li>\n<li><strong>Rationale</strong>: Redis provides the low-latency access needed for upload operations while database backup ensures durability and supports complex queries for monitoring</li>\n<li><strong>Consequences</strong>: Increases operational complexity with two storage systems but provides optimal performance and reliability characteristics</li>\n</ul>\n</blockquote>\n<p>The <code>DistributedStateStore</code> coordinates between multiple storage layers:</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Redis Usage</th>\n<th>Database Usage</th>\n<th>Consistency Level</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>CreateSession</td>\n<td>Primary storage</td>\n<td>Background sync</td>\n<td>Eventual</td>\n</tr>\n<tr>\n<td>GetSession</td>\n<td>Cache hit/miss</td>\n<td>Fallback for cache miss</td>\n<td>Strong for single session</td>\n</tr>\n<tr>\n<td>UpdateSession</td>\n<td>Immediate update</td>\n<td>Batched synchronization</td>\n<td>Eventually consistent</td>\n</tr>\n<tr>\n<td>ListExpiredSessions</td>\n<td>Not used</td>\n<td>Primary query source</td>\n<td>Eventually consistent</td>\n</tr>\n</tbody></table>\n<p>The distributed state management introduces several new operational concerns:</p>\n<ol>\n<li><strong>Cache Coherence</strong>: Ensure updates to session state are visible across all service instances within acceptable time bounds</li>\n<li><strong>Split-Brain Prevention</strong>: Handle scenarios where service instances can reach Redis but not each other</li>\n<li><strong>Data Durability</strong>: Balance performance with durability requirements for session state</li>\n<li><strong>Backup and Recovery</strong>: Provide mechanisms to recover session state after total system failures</li>\n<li><strong>Monitoring and Alerting</strong>: Track state synchronization lag and detect inconsistencies</li>\n</ol>\n<h4 id=\"load-balancing-and-session-affinity\">Load Balancing and Session Affinity</h4>\n<p><strong>Session affinity decisions</strong> significantly impact scaling architecture. The question is whether uploads must be processed by the same service instance throughout their lifecycle or can be handled by any available instance.</p>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>Scaling Flexibility</th>\n<th>Operational Complexity</th>\n<th>Failure Recovery</th>\n<th>Performance</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Sticky Sessions</td>\n<td>Limited (can&#39;t redistribute load)</td>\n<td>Low (simple routing)</td>\n<td>Poor (in-flight uploads lost)</td>\n<td>Good (local state)</td>\n</tr>\n<tr>\n<td>Session-less</td>\n<td>Excellent (full load distribution)</td>\n<td>High (shared state coordination)</td>\n<td>Excellent (any instance can resume)</td>\n<td>Variable (depends on state store)</td>\n</tr>\n<tr>\n<td>Hybrid</td>\n<td>Good (can migrate sessions)</td>\n<td>Medium (coordinated migration)</td>\n<td>Good (graceful failover)</td>\n<td>Good (optimized for common cases)</td>\n</tr>\n</tbody></table>\n<p>The <code>LoadBalancingCoordinator</code> provides configurable affinity strategies:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>AffinityStrategy</td>\n<td>AffinityType</td>\n<td>How to handle session-to-instance binding</td>\n</tr>\n<tr>\n<td>MigrationEnabled</td>\n<td>bool</td>\n<td>Whether sessions can move between instances</td>\n</tr>\n<tr>\n<td>HealthCheckInterval</td>\n<td>time.Duration</td>\n<td>Frequency of instance health verification</td>\n</tr>\n<tr>\n<td>LoadMetrics</td>\n<td>[]MetricType</td>\n<td>Metrics used for load balancing decisions</td>\n</tr>\n</tbody></table>\n<p>Session affinity introduces unique challenges for upload services:</p>\n<ol>\n<li><strong>In-Flight Upload Management</strong>: Handle scenarios where the assigned instance becomes unavailable during active uploads</li>\n<li><strong>Load Imbalance</strong>: Prevent scenarios where long-running uploads concentrate on specific instances</li>\n<li><strong>Graceful Migration</strong>: Move sessions between instances for maintenance without interrupting uploads</li>\n<li><strong>Failover Coordination</strong>: Detect instance failures and reassign affected sessions to healthy instances</li>\n</ol>\n<p>The <code>SessionMigrationManager</code> coordinates transfer of active uploads between instances:</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Purpose</th>\n<th>Coordination Required</th>\n<th>Data Transferred</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>PrepareForMigration</td>\n<td>Prepare session for transfer</td>\n<td>Source and destination instances</td>\n<td>Session metadata and chunk state</td>\n</tr>\n<tr>\n<td>MigrateSession</td>\n<td>Transfer session ownership</td>\n<td>Load balancer and state store</td>\n<td>Complete session state</td>\n</tr>\n<tr>\n<td>ConfirmMigration</td>\n<td>Verify successful transfer</td>\n<td>All involved components</td>\n<td>Migration success confirmation</td>\n</tr>\n<tr>\n<td>RollbackMigration</td>\n<td>Handle migration failures</td>\n<td>Source instance and state store</td>\n<td>Restore original session state</td>\n</tr>\n</tbody></table>\n<h4 id=\"data-partitioning-and-sharding\">Data Partitioning and Sharding</h4>\n<p><strong>Sharding strategies</strong> become necessary when the total volume of upload sessions exceeds what can be efficiently managed by a single state store instance. This introduces additional complexity but enables virtually unlimited scaling.</p>\n<p>The <code>ShardingCoordinator</code> manages data distribution across multiple storage partitions:</p>\n<table>\n<thead>\n<tr>\n<th>Sharding Key</th>\n<th>Distribution Quality</th>\n<th>Hot Spot Risk</th>\n<th>Query Complexity</th>\n<th>Migration Difficulty</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Session ID</td>\n<td>Excellent</td>\n<td>Low</td>\n<td>Simple</td>\n<td>Easy</td>\n</tr>\n<tr>\n<td>User ID</td>\n<td>Good</td>\n<td>Medium</td>\n<td>Medium</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Upload Date</td>\n<td>Poor</td>\n<td>High</td>\n<td>Complex</td>\n<td>Hard</td>\n</tr>\n<tr>\n<td>Content Hash</td>\n<td>Excellent</td>\n<td>Low</td>\n<td>Complex</td>\n<td>Medium</td>\n</tr>\n</tbody></table>\n<p>Session ID-based sharding provides the best balance for upload services:</p>\n<ol>\n<li><strong>Uniform Distribution</strong>: Cryptographically random session IDs ensure even distribution across shards</li>\n<li><strong>Simple Routing</strong>: Any request can be routed based solely on session ID without additional lookups</li>\n<li><strong>Independent Operations</strong>: Each shard can operate independently without cross-shard coordination for most operations</li>\n<li><strong>Predictable Performance</strong>: Load is naturally distributed proportional to upload volume</li>\n</ol>\n<p>The sharding implementation requires careful consideration of cross-shard operations:</p>\n<table>\n<thead>\n<tr>\n<th>Operation</th>\n<th>Shard Scope</th>\n<th>Coordination Required</th>\n<th>Performance Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>CreateSession</td>\n<td>Single shard</td>\n<td>None</td>\n<td>No impact</td>\n</tr>\n<tr>\n<td>GetSession</td>\n<td>Single shard</td>\n<td>None</td>\n<td>No impact</td>\n</tr>\n<tr>\n<td>UpdateSession</td>\n<td>Single shard</td>\n<td>None</td>\n<td>No impact</td>\n</tr>\n<tr>\n<td>ListExpiredSessions</td>\n<td>All shards</td>\n<td>Scatter-gather</td>\n<td>Linear with shard count</td>\n</tr>\n<tr>\n<td>GlobalStatistics</td>\n<td>All shards</td>\n<td>Scatter-gather with aggregation</td>\n<td>Linear with shard count</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: Cross-Shard Query Performance</strong>\nA common mistake is underestimating the performance impact of operations that must query multiple shards. Operations like <code>ListExpiredSessions</code> or global monitoring queries can become bottlenecks as the number of shards increases. The fix is to implement these operations asynchronously with cached results rather than real-time scatter-gather queries.</p>\n<h4 id=\"service-discovery-and-health-management\">Service Discovery and Health Management</h4>\n<p><strong>Service discovery</strong> enables dynamic scaling where service instances can be added or removed without manual configuration updates. This is essential for auto-scaling based on load and for handling instance failures gracefully.</p>\n<p>The <code>DiscoveryManager</code> coordinates instance registration and health monitoring:</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Responsibility</th>\n<th>Update Frequency</th>\n<th>Failure Detection Time</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Instance Registration</td>\n<td>Announce availability and capabilities</td>\n<td>On startup and shutdown</td>\n<td>N/A</td>\n</tr>\n<tr>\n<td>Health Reporting</td>\n<td>Report current load and status</td>\n<td>Every 30-60 seconds</td>\n<td>2-3x reporting interval</td>\n</tr>\n<tr>\n<td>Service Registry</td>\n<td>Maintain authoritative instance list</td>\n<td>Real-time updates</td>\n<td>Immediate for explicit deregistration</td>\n</tr>\n<tr>\n<td>Load Balancer Integration</td>\n<td>Update routing configuration</td>\n<td>Within 5-10 seconds</td>\n<td>Health check interval</td>\n</tr>\n</tbody></table>\n<p>The service discovery system must handle several failure scenarios:</p>\n<ol>\n<li><strong>Network Partitions</strong>: Instances that can serve traffic but cannot communicate with the service registry</li>\n<li><strong>Partial Failures</strong>: Instances that are healthy for some operations but failing for others</li>\n<li><strong>Graceful Shutdown</strong>: Instances that are shutting down and should stop receiving new traffic but complete existing uploads</li>\n<li><strong>Split-Brain Scenarios</strong>: Multiple instances claiming to be the authoritative service registry</li>\n</ol>\n<p>The <code>HealthReporter</code> provides comprehensive instance status information:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>InstanceID</td>\n<td>string</td>\n<td>Unique identifier for this instance</td>\n</tr>\n<tr>\n<td>StartTime</td>\n<td>time.Time</td>\n<td>When this instance started serving traffic</td>\n</tr>\n<tr>\n<td>ActiveSessions</td>\n<td>int</td>\n<td>Number of currently active upload sessions</td>\n</tr>\n<tr>\n<td>ResourceUtilization</td>\n<td>ResourceStatus</td>\n<td>Current CPU, memory, disk usage</td>\n</tr>\n<tr>\n<td>LastHealthCheck</td>\n<td>time.Time</td>\n<td>Timestamp of most recent health verification</td>\n</tr>\n<tr>\n<td>ServiceStatus</td>\n<td>ServiceState</td>\n<td>Overall health status (healthy, degraded, failing)</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The future extensions described above represent significant architectural additions that should be implemented incrementally to avoid disrupting the core service functionality. The recommended approach is to implement these features as <strong>optional extensions</strong> that can be enabled through configuration without affecting users who don&#39;t need the advanced capabilities.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Extension Category</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Parallel Uploads</td>\n<td>Goroutine pool with channel coordination</td>\n<td>Actor-based concurrency with supervisors</td>\n</tr>\n<tr>\n<td>Compression</td>\n<td>Standard library gzip per-chunk</td>\n<td>zstd with custom chunked format</td>\n</tr>\n<tr>\n<td>Deduplication</td>\n<td>In-memory hash map with persistence</td>\n<td>Distributed hash table (DHT) with consistent hashing</td>\n</tr>\n<tr>\n<td>Encryption</td>\n<td>Standard library crypto/aes</td>\n<td>Hardware security module (HSM) integration</td>\n</tr>\n<tr>\n<td>CDN Integration</td>\n<td>Simple webhook notifications</td>\n<td>Full CDN API integration with edge management</td>\n</tr>\n<tr>\n<td>State Store</td>\n<td>Redis with basic clustering</td>\n<td>Redis Cluster with consistent hashing</td>\n</tr>\n<tr>\n<td>Service Discovery</td>\n<td>Static configuration with health checks</td>\n<td>Consul or etcd with automatic failover</td>\n</tr>\n<tr>\n<td>Load Balancing</td>\n<td>HAProxy with sticky sessions</td>\n<td>Envoy proxy with advanced traffic management</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-module-structure\">Recommended Module Structure</h4>\n<p>The extensions should be organized as separate modules that integrate cleanly with the existing architecture:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  internal/\n    core/                    ← existing core components\n      session/\n      storage/\n      validation/\n    extensions/              ← new extension modules\n      parallel/              ← parallel upload coordination\n        coordinator.go\n        chunk_pool.go\n      compression/           ← compression support\n        compressor.go\n        adaptive_sizing.go\n      deduplication/         ← content deduplication\n        hash_manager.go\n        reference_counter.go\n      encryption/            ← encryption coordination\n        key_manager.go\n        encrypted_session.go\n      cdn/                   ← CDN integration\n        distribution_manager.go\n        edge_sync.go\n      metadata/              ← metadata extraction\n        extractor_registry.go\n        content_analyzer.go\n    scaling/                 ← horizontal scaling components\n      discovery/\n        service_registry.go\n      coordination/\n        shard_manager.go\n      state/\n        distributed_store.go\n  cmd/\n    server/\n      main.go               ← updated to support extensions\n    migration/              ← tools for scaling transitions\n      shard_migration.go</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Extension Configuration Management</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> extensions</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ExtensionConfig controls which advanced features are enabled</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ExtensionConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ParallelUploads   </span><span style=\"color:#B392F0\">ParallelConfig</span><span style=\"color:#9ECBFF\">   `json:\"parallel_uploads\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Compression       </span><span style=\"color:#B392F0\">CompressionConfig</span><span style=\"color:#9ECBFF\"> `json:\"compression\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Deduplication     </span><span style=\"color:#B392F0\">DeduplicationConfig</span><span style=\"color:#9ECBFF\"> `json:\"deduplication\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Encryption        </span><span style=\"color:#B392F0\">EncryptionConfig</span><span style=\"color:#9ECBFF\">  `json:\"encryption\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CDNIntegration    </span><span style=\"color:#B392F0\">CDNConfig</span><span style=\"color:#9ECBFF\">        `json:\"cdn_integration\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Scaling           </span><span style=\"color:#B392F0\">ScalingConfig</span><span style=\"color:#9ECBFF\">    `json:\"scaling\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ParallelConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Enabled              </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">          `json:\"enabled\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxConcurrentChunks  </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `json:\"max_concurrent_chunks\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BufferSizeLimit      </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">         `json:\"buffer_size_limit\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BackpressureThreshold </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">        `json:\"backpressure_threshold\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CompressionConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Enabled        </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">     `json:\"enabled\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Algorithm      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">   `json:\"algorithm\"`</span><span style=\"color:#6A737D\"> // \"gzip\", \"zstd\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CompressionLevel </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">    `json:\"compression_level\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ChunkSizeMin   </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">    `json:\"chunk_size_min\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ChunkSizeMax   </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">    `json:\"chunk_size_max\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> DeduplicationConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Enabled       </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">   `json:\"enabled\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Scope         </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"scope\"`</span><span style=\"color:#6A737D\"> // \"global\", \"tenant\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    HashAlgorithm </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"hash_algorithm\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    IndexBackend  </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"index_backend\"`</span><span style=\"color:#6A737D\"> // \"memory\", \"redis\", \"database\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ScalingConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Mode              </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"mode\"`</span><span style=\"color:#6A737D\"> // \"single\", \"cluster\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ServiceDiscovery  </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"service_discovery\"`</span><span style=\"color:#6A737D\"> // \"static\", \"consul\", \"etcd\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StateStore        </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"state_store\"`</span><span style=\"color:#6A737D\"> // \"memory\", \"redis\", \"postgres\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SessionAffinity   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"session_affinity\"`</span><span style=\"color:#6A737D\"> // \"none\", \"sticky\", \"migratable\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ShardingEnabled   </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">          `json:\"sharding_enabled\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ShardCount        </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `json:\"shard_count\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadExtensionConfig loads extension configuration from file</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> LoadExtensionConfig</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">filename</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ExtensionConfig</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    data, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">ReadFile</span><span style=\"color:#E1E4E8\">(filename)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"reading extension config: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> config </span><span style=\"color:#B392F0\">ExtensionConfig</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> json.</span><span style=\"color:#B392F0\">Unmarshal</span><span style=\"color:#E1E4E8\">(data, </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">config); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"parsing extension config: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#E1E4E8\">config, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ExtensionRegistry manages available extensions and their lifecycle</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ExtensionRegistry</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    extensions </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">Extension</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ExtensionConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StructuredLogger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Extension</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Name</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Initialize</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\">{}) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Start</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Stop</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    HealthCheck</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Parallel Upload Coordination</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> parallel</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ParallelCoordinator manages concurrent chunk processing for upload sessions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ParallelCoordinator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config           </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ParallelConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    chunkPool        </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ChunkPool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sessionManagers  </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SessionParallelManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex            </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resourceMonitor  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ResourceMonitor</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SessionParallelManager handles parallelism for a single upload session</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SessionParallelManager</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sessionID        </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    totalSize        </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    chunkSize        </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    completedRanges  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RangeSet</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pendingChunks    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PendingChunk</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    concurrencyLimit </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    activeSemaphore  </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex            </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> PendingChunk</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Offset       </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Size         </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Data         []</span><span style=\"color:#F97583\">byte</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Hash         </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ReceivedAt   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ProcessingStarted </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RangeSet</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ranges []</span><span style=\"color:#B392F0\">Range</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex  </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Range</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Start </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    End   </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewParallelCoordinator creates coordinator for managing parallel uploads</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewParallelCoordinator</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">ParallelConfig</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">monitor</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">ResourceMonitor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ParallelCoordinator</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ParallelCoordinator</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config:          config,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        chunkPool:       </span><span style=\"color:#B392F0\">NewChunkPool</span><span style=\"color:#E1E4E8\">(config.BufferSizeLimit),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sessionManagers: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SessionParallelManager</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        resourceMonitor: monitor,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ProcessChunkParallel handles concurrent chunk processing with ordering</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">pc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ParallelCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ProcessChunkParallel</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">offset</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">hash</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Get or create session parallel manager for this session</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check if system resources allow accepting this chunk (backpressure)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Acquire semaphore slot for concurrency limiting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Store chunk in pending chunks map with metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Launch goroutine for async processing (validation, storage)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Check for newly completed contiguous ranges after processing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Flush completed ranges to storage backend in order</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Update session progress and notify client if needed</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeletons\">Core Logic Skeletons</h4>\n<p><strong>Distributed State Store Implementation</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> state</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DistributedStateStore provides session state management across multiple service instances</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> DistributedStateStore</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    redis    </span><span style=\"color:#B392F0\">RedisClient</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    database </span><span style=\"color:#B392F0\">DatabaseClient</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DistributedStateConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StructuredLogger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CreateSession stores new upload session in distributed state</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ds </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DistributedStateStore</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CreateSession</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">session</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Serialize session data to JSON format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Store session in Redis with TTL for primary access</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Queue database write for durability (async or sync based on config)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Handle Redis failures by falling back to direct database write</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return success only after confirming storage in at least one backend</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetSession retrieves upload session with cache-first strategy</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ds </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DistributedStateStore</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetSession</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Attempt to retrieve session from Redis cache</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: If cache miss, query database as fallback</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: If found in database, refresh Redis cache for future requests</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Handle cases where session exists in cache but not database</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return ErrSessionNotFound if session doesn't exist in either store</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// UpdateSession modifies existing session state across distributed stores</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ds </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DistributedStateStore</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">UpdateSession</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">session</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Update session in Redis immediately for low-latency access</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Queue or perform database update based on consistency requirements</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Handle version conflicts if optimistic locking is enabled</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Implement retry logic for transient failures</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Consider notifying other service instances of state changes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Content Deduplication Manager</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> deduplication</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DeduplicationManager handles hash-based chunk deduplication</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> DeduplicationManager</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hashIndex     </span><span style=\"color:#B392F0\">HashIndex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    refCounter    </span><span style=\"color:#B392F0\">ReferenceCounter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storageBackend </span><span style=\"color:#B392F0\">StorageBackend</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config        </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DeduplicationConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stats         </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DeduplicationStats</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ProcessChunkWithDeduplication checks for existing chunks before storage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">dm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DeduplicationManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ProcessChunkWithDeduplication</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">chunkData</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">expectedHash</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ChunkProcessingResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Calculate chunk hash using configured algorithm (SHA-256, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Verify calculated hash matches expected hash from client</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Check hash index to see if this chunk already exists</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: If chunk exists, increment reference count and return existing location</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: If chunk is new, store in backend and add to hash index</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Update deduplication statistics for monitoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Return result indicating whether chunk was deduplicated or newly stored</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CleanupUnreferencedChunks removes chunks that are no longer referenced</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">dm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DeduplicationManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CleanupUnreferencedChunks</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Scan reference counter for chunks with zero references</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Verify chunks are not part of any active upload sessions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Remove unreferenced chunks from storage backend</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Remove entries from hash index</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Update cleanup statistics and log cleanup activity</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Extension Development Milestones</strong></p>\n<ol>\n<li><p><strong>Performance Optimization Milestone</strong>: Implement parallel chunk processing and verify 3x throughput improvement for concurrent uploads</p>\n<ul>\n<li>Test command: <code>go run cmd/loadtest/main.go -concurrent-chunks=10 -file-size=100MB</code></li>\n<li>Expected behavior: Upload completion time should be 60-70% faster than sequential processing</li>\n<li>Validation: Monitor resource usage to ensure memory stays within configured limits</li>\n</ul>\n</li>\n<li><p><strong>Advanced Features Milestone</strong>: Add deduplication and metadata extraction with configurable policies</p>\n<ul>\n<li>Test command: <code>go run cmd/dedup-test/main.go -upload-duplicates=5 -verify-savings</code></li>\n<li>Expected behavior: Storage usage should be 80% lower for identical file uploads</li>\n<li>Validation: Metadata should be extracted and searchable within 30 seconds of upload completion</li>\n</ul>\n</li>\n<li><p><strong>Horizontal Scaling Milestone</strong>: Deploy service across multiple instances with shared state</p>\n<ul>\n<li>Test command: <code>docker-compose up -d &amp;&amp; go run cmd/scaling-test/main.go -instances=3</code></li>\n<li>Expected behavior: Uploads should work correctly when processed by different instances</li>\n<li>Validation: Session state should be consistent across all instances within 5 seconds</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"debugging-tips-for-extensions\">Debugging Tips for Extensions</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Parallel uploads slower than sequential</td>\n<td>Lock contention or resource bottlenecks</td>\n<td>Profile with <code>go tool pprof</code> focusing on mutex and CPU usage</td>\n<td>Reduce lock scope, increase buffer sizes, optimize hot paths</td>\n</tr>\n<tr>\n<td>Deduplication not working</td>\n<td>Hash calculation errors or index inconsistencies</td>\n<td>Check hash logs and verify index queries return expected results</td>\n<td>Validate hash algorithm implementation, rebuild hash index</td>\n</tr>\n<tr>\n<td>Session state inconsistencies</td>\n<td>Redis-database sync lag or network partitions</td>\n<td>Compare session state between Redis and database, check network connectivity</td>\n<td>Implement stronger consistency guarantees, add conflict resolution</td>\n</tr>\n<tr>\n<td>Service discovery failures</td>\n<td>Network issues or registry corruption</td>\n<td>Check service registry logs and network connectivity between instances</td>\n<td>Restart service registry, verify network configuration</td>\n</tr>\n<tr>\n<td>Memory leaks in extensions</td>\n<td>Unreleased resources or circular references</td>\n<td>Use memory profiler to track allocation patterns over time</td>\n<td>Add proper resource cleanup in defer statements, break circular references</td>\n</tr>\n</tbody></table>\n<h2 id=\"glossary-and-technical-references\">Glossary and Technical References</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (1, 2, 3) — this glossary supports understanding across chunked upload protocol implementation (M1), storage abstraction (M2), and virus scanning validation (M3)</p>\n</blockquote>\n<p>Think of this glossary as a <em>technical dictionary</em> for the resumable file upload service. Just as a traveler needs a phrasebook to navigate a foreign country, developers implementing or maintaining this service need a shared vocabulary to navigate the complex landscape of protocols, abstractions, and specialized concepts. Each term represents a crystallized piece of domain knowledge that, once understood, unlocks deeper comprehension of the system&#39;s design and implementation.</p>\n<p>The terminology in upload services spans multiple domains: network protocols, distributed systems, storage technologies, security frameworks, and data validation. Without precise definitions, discussions about &quot;chunks&quot; could refer to network packets, file segments, or storage units. This ambiguity leads to misunderstandings, bugs, and architectural inconsistencies. The glossary establishes canonical definitions that align with industry standards while remaining specific to this implementation.</p>\n<h3 id=\"core-upload-concepts\">Core Upload Concepts</h3>\n<p>The foundation of any resumable upload service rests on a carefully defined vocabulary around file transfer mechanics. These terms form the building blocks for more complex operations and protocols.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n<th>Related Concepts</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>resumable upload</strong></td>\n<td>File upload that can be interrupted and resumed from the last successfully transmitted byte offset, preserving progress across network failures or client disconnections</td>\n<td>Core protocol concept</td>\n<td>upload session, offset tracking, tus.io protocol</td>\n</tr>\n<tr>\n<td><strong>chunked upload</strong></td>\n<td>File upload split into multiple smaller pieces (chunks) transmitted separately, enabling parallel processing and reducing memory requirements</td>\n<td>Transfer mechanism</td>\n<td>chunk assembly, multipart upload, parallel processing</td>\n</tr>\n<tr>\n<td><strong>upload session</strong></td>\n<td>Server-side state tracking ongoing resumable upload, including session ID, current progress offset, file metadata, and session status</td>\n<td>State management</td>\n<td>session state management, SessionManager, UploadSession</td>\n</tr>\n<tr>\n<td><strong>offset tracking</strong></td>\n<td>Byte-level progress monitoring that records exactly how many bytes have been successfully received and stored</td>\n<td>Progress monitoring</td>\n<td>CurrentOffset, chunk boundaries, resume logic</td>\n</tr>\n<tr>\n<td><strong>chunk assembly</strong></td>\n<td>Process of combining uploaded chunks into final complete file, ensuring correct ordering and data integrity</td>\n<td>File reconstruction</td>\n<td>CompleteMultipart, chunk ordering, integrity verification</td>\n</tr>\n</tbody></table>\n<p>The relationship between these concepts follows a hierarchical pattern. A <strong>resumable upload</strong> is implemented through <strong>chunked upload</strong> techniques, managed by an <strong>upload session</strong> that performs <strong>offset tracking</strong> and eventually triggers <strong>chunk assembly</strong>. Understanding this hierarchy helps developers reason about which component handles which responsibility.</p>\n<blockquote>\n<p><strong>Key Insight</strong>: The distinction between &quot;resumable&quot; and &quot;chunked&quot; is critical. A resumable upload is the <em>user-facing capability</em>, while chunked upload is the <em>technical implementation mechanism</em>. Not all chunked uploads are resumable, and theoretically resumable uploads could be implemented without chunking (though this would be inefficient for large files).</p>\n</blockquote>\n<h3 id=\"protocol-and-standards\">Protocol and Standards</h3>\n<p>The service implements and extends established protocols for file uploads. These standards provide interoperability and proven reliability patterns that the implementation builds upon.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n<th>Key Components</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>tus.io protocol</strong></td>\n<td>Standardized resumable upload protocol specification defining HTTP methods, headers, and request/response formats for interoperable resumable uploads</td>\n<td>Protocol compliance</td>\n<td>InitUploadRequest, ChunkUploadResponse, HEAD requests for progress</td>\n</tr>\n<tr>\n<td><strong>multipart upload</strong></td>\n<td>Large file upload split into multiple parts, particularly referring to cloud storage protocols like S3&#39;s multipart upload API</td>\n<td>Storage backend</td>\n<td>MultipartUpload, MultipartPart, backend-specific implementation</td>\n</tr>\n<tr>\n<td><strong>Content-Range headers</strong></td>\n<td>HTTP headers specifying byte ranges for partial content requests and responses, enabling precise chunk positioning</td>\n<td>HTTP protocol</td>\n<td>offset tracking, chunk boundaries, resume logic</td>\n</tr>\n<tr>\n<td><strong>atomic operations</strong></td>\n<td>File operations that complete entirely or not at all, preventing partial writes that could corrupt data</td>\n<td>Data integrity</td>\n<td>CompleteMultipart, transactional guarantees, rollback capability</td>\n</tr>\n<tr>\n<td><strong>signed URL</strong></td>\n<td>Time-limited URL with embedded authentication token for secure access to stored files without exposing credentials</td>\n<td>Security mechanism</td>\n<td>credential provider, temporary access, expiration management</td>\n</tr>\n</tbody></table>\n<p>These protocols interact in layered fashion. The <strong>tus.io protocol</strong> provides the client-server communication standard, which maps to <strong>multipart upload</strong> operations on the storage backend. <strong>Content-Range headers</strong> enable precise <strong>offset tracking</strong>, while <strong>atomic operations</strong> ensure consistency. <strong>Signed URLs</strong> provide secure access to the final assembled files.</p>\n<blockquote>\n<p><strong>Design Decision: Protocol Layering</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to balance standard compliance with implementation flexibility</li>\n<li><strong>Options Considered</strong>: Direct tus.io implementation, custom protocol, hybrid approach</li>\n<li><strong>Decision</strong>: Implement tus.io core with extensions for advanced features</li>\n<li><strong>Rationale</strong>: Standards compliance ensures client compatibility while extensions enable value-added features like virus scanning integration</li>\n<li><strong>Consequences</strong>: Broader ecosystem compatibility but additional complexity in feature flag management</li>\n</ul>\n</blockquote>\n<h3 id=\"storage-and-backend-management\">Storage and Backend Management</h3>\n<p>Storage abstraction enables the service to work with multiple storage systems while presenting a unified interface. This vocabulary captures the concepts essential for backend agnostic file storage.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n<th>Implementation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>storage backend abstraction</strong></td>\n<td>Interface allowing multiple storage systems behind common API, enabling deployment flexibility and vendor independence</td>\n<td>Architecture pattern</td>\n<td>StorageBackend interface, LocalBackend, S3-compatible implementations</td>\n</tr>\n<tr>\n<td><strong>credential provider</strong></td>\n<td>Source of authentication information for storage backends, supporting multiple credential types and refresh mechanisms</td>\n<td>Authentication</td>\n<td>CredentialProvider, Credentials struct, token refresh logic</td>\n</tr>\n<tr>\n<td><strong>state machine</strong></td>\n<td>Structured representation of session lifecycle with valid transitions, preventing invalid state changes</td>\n<td>Session management</td>\n<td>SessionStatus enum, state transition validation, error states</td>\n</tr>\n<tr>\n<td><strong>distributed state management</strong></td>\n<td>Session state coordination across multiple service instances, enabling horizontal scaling</td>\n<td>Scalability</td>\n<td>DistributedStateStore, session affinity, state synchronization</td>\n</tr>\n<tr>\n<td><strong>storage backend failures</strong></td>\n<td>Various failure modes of storage systems including network issues, authentication failures, and quota exhaustion</td>\n<td>Error handling</td>\n<td>StorageError types, circuit breaker pattern, fallback strategies</td>\n</tr>\n</tbody></table>\n<p>The storage abstraction creates clear separation of concerns. The <strong>storage backend abstraction</strong> provides the interface contract, while <strong>credential providers</strong> handle authentication complexity. <strong>State machines</strong> ensure consistent session lifecycle management, and <strong>distributed state management</strong> enables scaling. <strong>Storage backend failures</strong> are isolated and handled through established patterns.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Detection Method</th>\n<th>Recovery Strategy</th>\n<th>Fallback Options</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Network timeout</td>\n<td>Connection timeout, operation timeout</td>\n<td>Retry with exponential backoff</td>\n<td>Alternative endpoint, local caching</td>\n</tr>\n<tr>\n<td>Authentication failure</td>\n<td>401/403 responses, credential expiry</td>\n<td>Credential refresh, re-authentication</td>\n<td>Service account fallback, manual intervention</td>\n</tr>\n<tr>\n<td>Quota exhaustion</td>\n<td>507/insufficient space errors</td>\n<td>Alternative backend, cleanup</td>\n<td>Storage migration, quota increase</td>\n</tr>\n<tr>\n<td>Data corruption</td>\n<td>Checksum mismatch, incomplete reads</td>\n<td>Re-upload affected chunks</td>\n<td>Backup restoration, quarantine</td>\n</tr>\n</tbody></table>\n<h3 id=\"file-validation-and-security\">File Validation and Security</h3>\n<p>Security concepts encompass both automated validation and policy enforcement mechanisms that protect the service and its users from malicious content and system abuse.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n<th>Security Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>magic byte detection</strong></td>\n<td>File type identification using binary signatures at file beginning, more reliable than extension-based detection</td>\n<td>Content validation</td>\n<td>FileSignature patterns, Detector logic, MIME type verification</td>\n</tr>\n<tr>\n<td><strong>virus scanning</strong></td>\n<td>Malware detection using signature-based analysis engines like ClamAV</td>\n<td>Security validation</td>\n<td>ClamAVScanner, ScanResult, async scanning workflow</td>\n</tr>\n<tr>\n<td><strong>quarantine storage</strong></td>\n<td>Isolated storage for suspicious or infected files, preventing access while preserving evidence</td>\n<td>Security containment</td>\n<td>separate storage bucket, restricted access, forensic retention</td>\n</tr>\n<tr>\n<td><strong>content-type validation</strong></td>\n<td>Verification of actual file type against declared MIME type, preventing content-type spoofing attacks</td>\n<td>Input validation</td>\n<td>magic byte verification, MIME type consistency, policy enforcement</td>\n</tr>\n<tr>\n<td><strong>forensic metadata</strong></td>\n<td>Detailed context information preserved for security investigations including timestamps, checksums, and scan results</td>\n<td>Security auditing</td>\n<td>upload metadata, scan history, investigation support</td>\n</tr>\n</tbody></table>\n<p>Security validation follows a defense-in-depth approach. <strong>Magic byte detection</strong> provides the first layer of content verification, independent of user-supplied metadata. <strong>Virus scanning</strong> adds signature-based threat detection, while <strong>quarantine storage</strong> isolates suspicious content. <strong>Content-type validation</strong> prevents spoofing attacks, and <strong>forensic metadata</strong> supports incident response.</p>\n<blockquote>\n<p><strong>Security Principle</strong>: Never trust, always verify. File extensions can be spoofed, Content-Type headers can be manipulated, and even magic bytes can be crafted. The validation pipeline uses multiple independent verification methods to build confidence in file safety.</p>\n</blockquote>\n<h3 id=\"error-handling-and-resilience\">Error Handling and Resilience</h3>\n<p>Robust error handling requires precise vocabulary around failure modes, detection mechanisms, and recovery strategies. This terminology enables systematic approach to fault tolerance.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n<th>Recovery Pattern</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>circuit breaker pattern</strong></td>\n<td>Failure isolation mechanism preventing cascading failures by opening circuit after threshold failures</td>\n<td>Fault tolerance</td>\n<td>CircuitBreaker implementation, failure thresholds, automatic reset</td>\n</tr>\n<tr>\n<td><strong>error propagation</strong></td>\n<td>Systematic handling and communication of failures between components</td>\n<td>Error handling</td>\n<td>UploadError categorization, error context, retry policies</td>\n</tr>\n<tr>\n<td><strong>graceful degradation</strong></td>\n<td>Reducing service functionality while maintaining core operations during resource constraints</td>\n<td>Service resilience</td>\n<td>load shedding, feature disabling, priority handling</td>\n</tr>\n<tr>\n<td><strong>resource exhaustion</strong></td>\n<td>Depletion of system resources like memory, disk space, or file descriptors</td>\n<td>Capacity management</td>\n<td>ResourceMonitor, threshold alerts, admission control</td>\n</tr>\n<tr>\n<td><strong>corruption detection</strong></td>\n<td>Verification of data integrity through checksums and validation</td>\n<td>Data integrity</td>\n<td>hash verification, chunk validation, assembly verification</td>\n</tr>\n</tbody></table>\n<p>Error handling operates at multiple levels. <strong>Circuit breaker patterns</strong> prevent systemic failures, while <strong>error propagation</strong> ensures failures are communicated appropriately. <strong>Graceful degradation</strong> maintains service availability, <strong>resource exhaustion</strong> detection triggers protective measures, and <strong>corruption detection</strong> maintains data integrity.</p>\n<table>\n<thead>\n<tr>\n<th>Error Category</th>\n<th>Detection Latency</th>\n<th>Recovery Time</th>\n<th>User Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Transient network</td>\n<td>Immediate (connection timeout)</td>\n<td>Seconds (retry)</td>\n<td>None (automatic retry)</td>\n</tr>\n<tr>\n<td>Storage backend failure</td>\n<td>1-5 seconds (operation timeout)</td>\n<td>Minutes (failover)</td>\n<td>Temporary slowdown</td>\n</tr>\n<tr>\n<td>Resource exhaustion</td>\n<td>Real-time (monitoring)</td>\n<td>Minutes (load shedding)</td>\n<td>Feature limitations</td>\n</tr>\n<tr>\n<td>Data corruption</td>\n<td>Variable (checksum validation)</td>\n<td>Variable (re-upload)</td>\n<td>Upload restart required</td>\n</tr>\n</tbody></table>\n<h3 id=\"performance-and-scalability\">Performance and Scalability</h3>\n<p>Performance terminology captures the concepts necessary for understanding and optimizing upload service behavior under load and at scale.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n<th>Optimization Target</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>parallel upload acceleration</strong></td>\n<td>Concurrent chunk processing for improved throughput while maintaining ordering and consistency</td>\n<td>Performance optimization</td>\n<td>ParallelCoordinator, concurrent chunk processing, assembly optimization</td>\n</tr>\n<tr>\n<td><strong>bandwidth optimization</strong></td>\n<td>Techniques to reduce network usage and improve transfer efficiency</td>\n<td>Network efficiency</td>\n<td>compression coordination, deduplication, adaptive chunk sizing</td>\n</tr>\n<tr>\n<td><strong>content deduplication</strong></td>\n<td>Hash-based elimination of redundant chunk storage, reducing storage costs and transfer time</td>\n<td>Storage efficiency</td>\n<td>DeduplicationManager, hash indexing, reference counting</td>\n</tr>\n<tr>\n<td><strong>session affinity</strong></td>\n<td>Binding uploads to specific service instances for state locality and performance</td>\n<td>Load balancing</td>\n<td>instance binding, state locality, migration coordination</td>\n</tr>\n<tr>\n<td><strong>horizontal scaling</strong></td>\n<td>Multi-instance deployment with coordinated operation enabling increased capacity</td>\n<td>Scalability</td>\n<td>distributed coordination, service discovery, load balancing</td>\n</tr>\n</tbody></table>\n<p>Performance optimization addresses multiple bottlenecks simultaneously. <strong>Parallel upload acceleration</strong> improves CPU and I/O utilization, <strong>bandwidth optimization</strong> reduces network constraints, <strong>content deduplication</strong> minimizes storage overhead, <strong>session affinity</strong> reduces coordination overhead, and <strong>horizontal scaling</strong> increases overall capacity.</p>\n<blockquote>\n<p><strong>Performance Trade-off</strong>: Parallelization improves throughput but increases complexity. Each concurrent operation requires coordination overhead, memory for buffering, and careful ordering during assembly. The optimal concurrency level depends on hardware characteristics, network conditions, and file size patterns.</p>\n</blockquote>\n<h3 id=\"advanced-features-and-extensions\">Advanced Features and Extensions</h3>\n<p>Advanced terminology covers optional features that extend the basic upload service with additional capabilities for specialized use cases.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n<th>Extension Point</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>compression coordination</strong></td>\n<td>Management of file compression during upload process, balancing CPU usage with storage savings</td>\n<td>Storage optimization</td>\n<td>compression algorithms, adaptive compression, size thresholds</td>\n</tr>\n<tr>\n<td><strong>encryption coordination</strong></td>\n<td>Management of file encryption during upload process, supporting client-side and server-side encryption</td>\n<td>Data protection</td>\n<td>encryption modes, key management, compliance requirements</td>\n</tr>\n<tr>\n<td><strong>metadata extraction</strong></td>\n<td>Automated analysis and indexing of file content for search and categorization</td>\n<td>Content processing</td>\n<td>MetadataExtractor interface, content analysis, async processing</td>\n</tr>\n<tr>\n<td><strong>CDN integration</strong></td>\n<td>Content distribution network coordination for global access and edge caching</td>\n<td>Global distribution</td>\n<td>edge synchronization, geographic distribution, cache invalidation</td>\n</tr>\n<tr>\n<td><strong>ML-enhanced virus scanning</strong></td>\n<td>Machine learning augmented threat detection beyond signature-based scanning</td>\n<td>Advanced security</td>\n<td>behavioral analysis, anomaly detection, adaptive learning</td>\n</tr>\n</tbody></table>\n<p>Extension features provide value-added capabilities beyond basic upload functionality. <strong>Compression coordination</strong> optimizes storage efficiency, <strong>encryption coordination</strong> enhances data protection, <strong>metadata extraction</strong> enables content management, <strong>CDN integration</strong> improves global performance, and <strong>ML-enhanced virus scanning</strong> provides advanced threat detection.</p>\n<table>\n<thead>\n<tr>\n<th>Extension</th>\n<th>Resource Requirements</th>\n<th>Complexity Level</th>\n<th>Integration Points</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Compression</td>\n<td>CPU intensive</td>\n<td>Medium</td>\n<td>Storage pipeline, chunk processing</td>\n</tr>\n<tr>\n<td>Encryption</td>\n<td>CPU + key management</td>\n<td>High</td>\n<td>Client protocol, storage backend</td>\n</tr>\n<tr>\n<td>Metadata extraction</td>\n<td>CPU + specialized libraries</td>\n<td>Medium</td>\n<td>Post-processing pipeline, content analysis</td>\n</tr>\n<tr>\n<td>CDN integration</td>\n<td>Network + external APIs</td>\n<td>High</td>\n<td>Storage backend, URL generation</td>\n</tr>\n<tr>\n<td>ML scanning</td>\n<td>GPU/specialized hardware</td>\n<td>Very High</td>\n<td>Virus scanning pipeline, model management</td>\n</tr>\n</tbody></table>\n<h3 id=\"protocol-specifications-and-standards\">Protocol Specifications and Standards</h3>\n<p>This section provides authoritative references to the protocols, standards, and specifications that guide the implementation decisions and ensure interoperability.</p>\n<h4 id=\"tus-protocol-specification\">TUS Protocol Specification</h4>\n<p>The <strong>tus.io resumable upload protocol</strong> forms the foundation for client-server communication. The complete specification defines the HTTP methods, headers, and request/response formats that ensure interoperability between different client and server implementations.</p>\n<table>\n<thead>\n<tr>\n<th>HTTP Method</th>\n<th>Purpose</th>\n<th>Required Headers</th>\n<th>Response Codes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>POST</td>\n<td>Initialize upload session</td>\n<td>Upload-Length, Upload-Metadata</td>\n<td>201 Created, 413 Request Entity Too Large</td>\n</tr>\n<tr>\n<td>PATCH</td>\n<td>Upload chunk data</td>\n<td>Upload-Offset, Content-Type</td>\n<td>204 No Content, 409 Conflict, 460 Checksum Mismatch</td>\n</tr>\n<tr>\n<td>HEAD</td>\n<td>Query upload progress</td>\n<td>None</td>\n<td>200 OK with Upload-Offset header</td>\n</tr>\n<tr>\n<td>DELETE</td>\n<td>Cancel upload session</td>\n<td>None</td>\n<td>204 No Content, 404 Not Found</td>\n</tr>\n</tbody></table>\n<p>The protocol defines several extensions that this implementation may support:</p>\n<ul>\n<li><strong>Creation Extension</strong>: Enables file upload initialization through POST requests</li>\n<li><strong>Checksum Extension</strong>: Provides data integrity verification through configurable hash algorithms  </li>\n<li><strong>Expiration Extension</strong>: Allows servers to communicate upload session expiration times</li>\n<li><strong>Concatenation Extension</strong>: Supports parallel uploads with final concatenation</li>\n</ul>\n<h4 id=\"http-range-request-standards\">HTTP Range Request Standards</h4>\n<p><strong>RFC 7233</strong> defines the HTTP range request mechanism that underlies chunked upload positioning and resume logic. The implementation relies on these standards for precise byte-level positioning.</p>\n<table>\n<thead>\n<tr>\n<th>Header</th>\n<th>Direction</th>\n<th>Format</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Range</td>\n<td>Client → Server</td>\n<td>bytes=start-end</td>\n<td>Request specific byte range</td>\n</tr>\n<tr>\n<td>Content-Range</td>\n<td>Server → Client</td>\n<td>bytes start-end/total</td>\n<td>Indicate range being served</td>\n</tr>\n<tr>\n<td>Accept-Ranges</td>\n<td>Server → Client</td>\n<td>bytes</td>\n<td>Advertise range request support</td>\n</tr>\n<tr>\n<td>Content-Length</td>\n<td>Bidirectional</td>\n<td>byte-count</td>\n<td>Specify content size</td>\n</tr>\n</tbody></table>\n<h4 id=\"cloud-storage-apis\">Cloud Storage APIs</h4>\n<p>The storage backend abstraction must interface with various cloud storage APIs, each with specific requirements and capabilities.</p>\n<table>\n<thead>\n<tr>\n<th>Provider</th>\n<th>Multipart API</th>\n<th>Authentication</th>\n<th>Special Requirements</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>AWS S3</td>\n<td>CreateMultipartUpload, UploadPart, CompleteMultipartUpload</td>\n<td>AWS Signature V4</td>\n<td>Parts ≥ 5MB except last, max 10,000 parts</td>\n</tr>\n<tr>\n<td>Google Cloud Storage</td>\n<td>Resumable uploads with upload_id</td>\n<td>OAuth 2.0 or service accounts</td>\n<td>Chunk size must be multiple of 256KB</td>\n</tr>\n<tr>\n<td>Azure Blob Storage</td>\n<td>Block blobs with block IDs</td>\n<td>Shared keys or Azure AD</td>\n<td>Block IDs must be base64 encoded</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-dependencies\">Implementation Dependencies</h3>\n<p>Understanding the external dependencies and their integration patterns helps developers make informed decisions about deployment and operational requirements.</p>\n<h4 id=\"clamav-integration\">ClamAV Integration</h4>\n<p><strong>ClamAV</strong> provides the virus scanning engine with specific integration requirements and operational considerations.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Version Requirement</th>\n<th>Configuration</th>\n<th>Performance Characteristics</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ClamAV daemon</td>\n<td>≥ 0.103</td>\n<td>Socket-based communication</td>\n<td>~1-10MB/s scan rate, CPU intensive</td>\n</tr>\n<tr>\n<td>Signature database</td>\n<td>Daily updates</td>\n<td>Automated freshclam</td>\n<td>~100MB+ database size</td>\n</tr>\n<tr>\n<td>Socket interface</td>\n<td>Unix domain socket preferred</td>\n<td>/tmp/clamd.socket</td>\n<td>Lower latency than TCP</td>\n</tr>\n</tbody></table>\n<p>The scanning integration requires careful resource management:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Configuration Parameters:\n- MaxScanSize: Maximum file size for scanning (default: 100MB)\n- MaxFileSize: Individual file limit (default: 25MB)  \n- MaxRecursion: Archive scanning depth (default: 16)\n- MaxFiles: Files within archives (default: 10000)\n- ScanTimeout: Per-file timeout (default: 120s)</code></pre></div>\n\n<h4 id=\"http-server-requirements\">HTTP Server Requirements</h4>\n<p>The service requires an HTTP server implementation that supports specific features for efficient upload handling.</p>\n<table>\n<thead>\n<tr>\n<th>Feature</th>\n<th>Requirement</th>\n<th>Rationale</th>\n<th>Implementation Notes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Streaming request bodies</td>\n<td>Required</td>\n<td>Memory efficiency for large uploads</td>\n<td>http.Request.Body streaming</td>\n</tr>\n<tr>\n<td>Request timeouts</td>\n<td>Configurable</td>\n<td>Prevent resource exhaustion</td>\n<td>ReadTimeout, WriteTimeout</td>\n</tr>\n<tr>\n<td>Connection limits</td>\n<td>Configurable</td>\n<td>Control resource usage</td>\n<td>MaxConnections, rate limiting</td>\n</tr>\n<tr>\n<td>TLS support</td>\n<td>Required for production</td>\n<td>Security compliance</td>\n<td>Certificate management, cipher suites</td>\n</tr>\n</tbody></table>\n<h3 id=\"data-format-specifications\">Data Format Specifications</h3>\n<p>The service works with various data formats and must understand their structure for proper validation and processing.</p>\n<h4 id=\"file-type-detection-signatures\">File Type Detection Signatures</h4>\n<p><strong>Magic byte detection</strong> relies on well-known file signatures to identify content types regardless of file extensions or MIME type declarations.</p>\n<table>\n<thead>\n<tr>\n<th>File Type</th>\n<th>Magic Bytes</th>\n<th>Offset</th>\n<th>MIME Type</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>PNG</td>\n<td>89 50 4E 47 0D 0A 1A 0A</td>\n<td>0</td>\n<td>image/png</td>\n</tr>\n<tr>\n<td>JPEG</td>\n<td>FF D8 FF</td>\n<td>0</td>\n<td>image/jpeg</td>\n</tr>\n<tr>\n<td>PDF</td>\n<td>25 50 44 46</td>\n<td>0</td>\n<td>application/pdf</td>\n</tr>\n<tr>\n<td>ZIP</td>\n<td>50 4B 03 04 or 50 4B 05 06</td>\n<td>0</td>\n<td>application/zip</td>\n</tr>\n<tr>\n<td>GIF</td>\n<td>47 49 46 38 37 61 or 47 49 46 38 39 61</td>\n<td>0</td>\n<td>image/gif</td>\n</tr>\n</tbody></table>\n<h4 id=\"checksum-algorithms\">Checksum Algorithms</h4>\n<p><strong>Data integrity verification</strong> supports multiple hash algorithms with different performance and security characteristics.</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th>Hash Size</th>\n<th>Performance</th>\n<th>Security Level</th>\n<th>Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>MD5</td>\n<td>128-bit</td>\n<td>Very Fast</td>\n<td>Low (deprecated for security)</td>\n<td>Legacy compatibility</td>\n</tr>\n<tr>\n<td>SHA-256</td>\n<td>256-bit</td>\n<td>Fast</td>\n<td>High</td>\n<td>Primary integrity verification</td>\n</tr>\n<tr>\n<td>SHA-512</td>\n<td>512-bit</td>\n<td>Medium</td>\n<td>Very High</td>\n<td>High-security environments</td>\n</tr>\n<tr>\n<td>CRC32</td>\n<td>32-bit</td>\n<td>Very Fast</td>\n<td>Very Low</td>\n<td>Network error detection only</td>\n</tr>\n</tbody></table>\n<h3 id=\"common-pitfalls-and-anti-patterns\">Common Pitfalls and Anti-Patterns</h3>\n<p>Understanding common mistakes helps developers avoid subtle bugs and security vulnerabilities that frequently occur in upload service implementations.</p>\n<p>⚠️ <strong>Pitfall: Extension-Based File Type Detection</strong>\nRelying solely on file extensions for content type determination creates security vulnerabilities. Malicious users can upload executable files with image extensions, bypassing security controls. Always validate content using magic byte detection and treat file extensions as hints only.</p>\n<p>⚠️ <strong>Pitfall: Insufficient Offset Validation</strong>\nAccepting client-provided offsets without validation can lead to data corruption or security issues. Clients may send overlapping ranges, negative offsets, or offsets beyond file boundaries. Always validate offset against current session state and enforce monotonic progression.</p>\n<p>⚠️ <strong>Pitfall: Blocking Virus Scans</strong>\nSynchronous virus scanning blocks upload completion and ties up server resources. Large files can take minutes to scan, leading to timeouts and poor user experience. Implement async scanning with notification mechanisms and consider scan result caching.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Error Context</strong>\nGeneric error responses make debugging difficult and provide poor user experience. Errors should include session context, operation details, and actionable recovery information while avoiding information disclosure.</p>\n<p>⚠️ <strong>Pitfall: State Synchronization Assumptions</strong>\nAssuming session state consistency across distributed components leads to race conditions and data loss. Network partitions, process crashes, and concurrent access require explicit consistency guarantees and conflict resolution.</p>\n<h3 id=\"acronyms-and-abbreviations\">Acronyms and Abbreviations</h3>\n<table>\n<thead>\n<tr>\n<th>Acronym</th>\n<th>Full Form</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>API</td>\n<td>Application Programming Interface</td>\n<td>Storage backend interfaces</td>\n</tr>\n<tr>\n<td>CDN</td>\n<td>Content Delivery Network</td>\n<td>Global file distribution</td>\n</tr>\n<tr>\n<td>CORS</td>\n<td>Cross-Origin Resource Sharing</td>\n<td>Browser security policy</td>\n</tr>\n<tr>\n<td>CRC</td>\n<td>Cyclic Redundancy Check</td>\n<td>Data integrity verification</td>\n</tr>\n<tr>\n<td>CRUD</td>\n<td>Create, Read, Update, Delete</td>\n<td>Basic storage operations</td>\n</tr>\n<tr>\n<td>ETag</td>\n<td>Entity Tag</td>\n<td>HTTP caching and versioning</td>\n</tr>\n<tr>\n<td>FIFO</td>\n<td>First In, First Out</td>\n<td>Queue processing order</td>\n</tr>\n<tr>\n<td>HTTP</td>\n<td>Hypertext Transfer Protocol</td>\n<td>Client-server communication</td>\n</tr>\n<tr>\n<td>I/O</td>\n<td>Input/Output</td>\n<td>File and network operations</td>\n</tr>\n<tr>\n<td>JSON</td>\n<td>JavaScript Object Notation</td>\n<td>Data serialization format</td>\n</tr>\n<tr>\n<td>MIME</td>\n<td>Multipurpose Internet Mail Extensions</td>\n<td>Content type identification</td>\n</tr>\n<tr>\n<td>REST</td>\n<td>Representational State Transfer</td>\n<td>API design pattern</td>\n</tr>\n<tr>\n<td>RFC</td>\n<td>Request for Comments</td>\n<td>Internet standards documents</td>\n</tr>\n<tr>\n<td>S3</td>\n<td>Simple Storage Service</td>\n<td>AWS object storage</td>\n</tr>\n<tr>\n<td>SDK</td>\n<td>Software Development Kit</td>\n<td>Cloud provider libraries</td>\n</tr>\n<tr>\n<td>TLS</td>\n<td>Transport Layer Security</td>\n<td>Encryption in transit</td>\n</tr>\n<tr>\n<td>TTL</td>\n<td>Time To Live</td>\n<td>Expiration mechanism</td>\n</tr>\n<tr>\n<td>URL</td>\n<td>Uniform Resource Locator</td>\n<td>Resource addressing</td>\n</tr>\n<tr>\n<td>UUID</td>\n<td>Universally Unique Identifier</td>\n<td>Session ID generation</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides practical guidance for implementing the vocabulary and concepts described above, with particular focus on Go-specific patterns and idioms.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Server</td>\n<td>net/http with gorilla/mux</td>\n<td>Gin or Echo framework</td>\n</tr>\n<tr>\n<td>JSON Processing</td>\n<td>encoding/json</td>\n<td>jsoniter for high performance</td>\n</tr>\n<tr>\n<td>Logging</td>\n<td>slog (Go 1.21+)</td>\n<td>logrus or zap for structured logging</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td>encoding/json + os.Getenv</td>\n<td>viper for advanced config management</td>\n</tr>\n<tr>\n<td>Testing</td>\n<td>testing package + testify/assert</td>\n<td>Ginkgo/Gomega for BDD-style tests</td>\n</tr>\n<tr>\n<td>HTTP Client</td>\n<td>net/http</td>\n<td>resty for convenience features</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-project-structure\">Recommended Project Structure</h4>\n<p>The vocabulary concepts map to specific packages and modules within the project structure:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>resumable-upload-service/\n├── cmd/\n│   ├── server/main.go           ← Entry point, LoadConfig\n│   └── migrate/main.go          ← Session migration utilities\n├── internal/\n│   ├── config/\n│   │   ├── config.go            ← Config, ServerConfig, StorageConfig\n│   │   └── validation.go        ← ValidateRequirements\n│   ├── protocol/\n│   │   ├── tus.go              ← TUS protocol implementation\n│   │   ├── handlers.go         ← InitUploadRequest, ChunkUploadResponse\n│   │   └── errors.go           ← ErrorResponse, UploadError\n│   ├── session/\n│   │   ├── manager.go          ← SessionManager, CreateSession, GetSession\n│   │   ├── store.go            ← StateStore interface, MemoryStore\n│   │   └── models.go           ← UploadSession, SessionStatus\n│   ├── storage/\n│   │   ├── backend.go          ← StorageBackend interface\n│   │   ├── local.go            ← LocalBackend\n│   │   ├── s3.go               ← S3-compatible backend\n│   │   └── credentials.go      ← CredentialProvider, Credentials\n│   ├── validation/\n│   │   ├── detector.go         ← FileSignature, Detector\n│   │   ├── scanner.go          ← ClamAVScanner, ScanResult\n│   │   └── validator.go        ← FileValidator, ValidationResult\n│   ├── coordinator/\n│   │   ├── flow.go             ← FlowCoordinator\n│   │   ├── circuit.go          ← CircuitBreaker, CircuitState\n│   │   └── recovery.go         ← RecoveryCoordinator\n│   └── monitoring/\n│       ├── resources.go        ← ResourceMonitor, ResourceStatus\n│       ├── logging.go          ← StructuredLogger\n│       └── diagnostics.go      ← SessionInspector, SessionDiagnostics\n├── pkg/\n│   └── extensions/\n│       ├── parallel.go         ← ParallelCoordinator\n│       ├── compression.go      ← CompressionCoordinator\n│       ├── deduplication.go    ← DeduplicationManager\n│       └── encryption.go       ← EncryptionCoordinator\n└── test/\n    ├── integration/            ← MilestoneValidator, TusProtocolTester\n    ├── load/                   ← LoadTestScenario\n    └── mocks/                  ← MockStorageBackend</code></pre></div>\n\n<h4 id=\"core-vocabulary-implementation\">Core Vocabulary Implementation</h4>\n<p>The following skeleton shows how key vocabulary concepts translate to Go types and interfaces:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Package config - Configuration vocabulary</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Config</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Server   </span><span style=\"color:#B392F0\">ServerConfig</span><span style=\"color:#9ECBFF\">   `json:\"server\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Storage  </span><span style=\"color:#B392F0\">StorageConfig</span><span style=\"color:#9ECBFF\">  `json:\"storage\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Security </span><span style=\"color:#B392F0\">SecurityConfig</span><span style=\"color:#9ECBFF\"> `json:\"security\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Cleanup  </span><span style=\"color:#B392F0\">CleanupConfig</span><span style=\"color:#9ECBFF\">  `json:\"cleanup\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadConfig demonstrates configuration vocabulary integration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> LoadConfig</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">filename</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Read JSON configuration file</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Apply environment variable overrides  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Validate configuration against requirements</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Set reasonable defaults for optional fields</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Package session - Session management vocabulary  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SessionStatus</span><span style=\"color:#F97583\"> string</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SessionStatusInitialized</span><span style=\"color:#B392F0\"> SessionStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"initialized\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SessionStatusActive</span><span style=\"color:#B392F0\">      SessionStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"active\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SessionStatusCompleting</span><span style=\"color:#B392F0\">  SessionStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"completing\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SessionStatusCompleted</span><span style=\"color:#B392F0\">   SessionStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"completed\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SessionStatusFailed</span><span style=\"color:#B392F0\">      SessionStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"failed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SessionStatusExpired</span><span style=\"color:#B392F0\">     SessionStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"expired\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// UploadSession represents upload session vocabulary</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> UploadSession</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ID            </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Filename      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"filename\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ContentType   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"content_type\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TotalSize     </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">         `json:\"total_size\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CurrentOffset </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">         `json:\"current_offset\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Status        </span><span style=\"color:#B392F0\">SessionStatus</span><span style=\"color:#9ECBFF\"> `json:\"status\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CreatedAt     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">     `json:\"created_at\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ExpiresAt     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">     `json:\"expires_at\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SessionManager implements session state management vocabulary</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SessionManager</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    store   </span><span style=\"color:#B392F0\">StateStore</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage </span><span style=\"color:#B392F0\">StorageBackend</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex   </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CreateSession demonstrates session lifecycle vocabulary</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SessionManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CreateSession</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">session</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">UploadSession</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Validate session parameters against policies</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Generate unique session ID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Initialize session state in distributed store</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Create storage backend session if required</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Set expiration timers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"storage-vocabulary-implementation\">Storage Vocabulary Implementation</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Package storage - Storage abstraction vocabulary</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StorageBackend</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // StoreChunk implements chunk storage vocabulary</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    StoreChunk</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">data</span><span style=\"color:#B392F0\"> io</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Reader</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">size</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // InitMultipart implements multipart upload vocabulary  </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    InitMultipart</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">metadata</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MultipartUpload</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // CompleteMultipart implements chunk assembly vocabulary</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    CompleteMultipart</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">upload</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">MultipartUpload</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">parts</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MultipartPart</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MultipartUpload represents multipart upload vocabulary</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MultipartUpload</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ID        </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Key       </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"key\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Backend   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"backend\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Metadata  </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"metadata\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CreatedAt </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">         `json:\"created_at\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StorageError implements storage backend failure vocabulary</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StorageError</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Operation </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"operation\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Key       </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"key\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Backend   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"backend\"`</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Err       </span><span style=\"color:#F97583\">error</span><span style=\"color:#9ECBFF\">  `json:\"error\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageError</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"storage </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\"> failed for key </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\"> on backend </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        e.Operation, e.Key, e.Backend, e.Err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"validation-vocabulary-implementation\">Validation Vocabulary Implementation</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Package validation - File validation vocabulary</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> FileSignature</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Pattern     []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#9ECBFF\">   `json:\"pattern\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Offset      </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">      `json:\"offset\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MIME        </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">   `json:\"mime\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Extensions  []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"extensions\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Description </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">   `json:\"description\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Detector implements magic byte detection vocabulary</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Detector</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    signatures []</span><span style=\"color:#B392F0\">FileSignature</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DetectFromReader demonstrates content-type validation vocabulary</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Detector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">DetectFromReader</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">reader</span><span style=\"color:#B392F0\"> io</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Reader</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Read initial bytes for magic byte detection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Match against known file signatures</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Return MIME type based on signature match</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Handle multiple potential matches</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ClamAVScanner implements virus scanning vocabulary</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ClamAVScanner</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    socketPath </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timeout    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    conn       </span><span style=\"color:#B392F0\">net</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Conn</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ScanResult represents virus scanning vocabulary</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ScanResult</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Clean      </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">          `json:\"clean\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ThreatName </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"threat_name,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Error      </span><span style=\"color:#F97583\">error</span><span style=\"color:#9ECBFF\">         `json:\"error,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ScanTime   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"scan_time\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"error-handling-vocabulary-implementation\">Error Handling Vocabulary Implementation</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Package coordinator - Error handling vocabulary</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">var</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrSessionNotFound        </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"session not found\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrInsufficientSpace     </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"insufficient storage space\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrSystemOverloaded      </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"system resources critically low\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrVirusScanTimeout      </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"virus scanning operation timed out\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrCircuitBreakerOpen    </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"circuit breaker open\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// UploadError implements error propagation vocabulary</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> UploadError</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Category    </span><span style=\"color:#F97583\">error</span><span style=\"color:#9ECBFF\">                  `json:\"category\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Component   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                 `json:\"component\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Operation   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                 `json:\"operation\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SessionID   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                 `json:\"session_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Cause       </span><span style=\"color:#F97583\">error</span><span style=\"color:#9ECBFF\">                  `json:\"cause\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Retryable   </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">                   `json:\"retryable\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RetryAfter  </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">                    `json:\"retry_after\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Metadata    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{} </span><span style=\"color:#9ECBFF\">`json:\"metadata\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">              `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewUploadError demonstrates error categorization vocabulary</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewUploadError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">category</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">component</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">operation</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">sessionID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">cause</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">UploadError</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Categorize error type (transient, permanent, corruption, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Determine retry policy based on error category</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Include relevant context metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Set appropriate retry-after timing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"testing-vocabulary-implementation\">Testing Vocabulary Implementation</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Package test - Testing vocabulary implementation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MilestoneValidator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    serverURL     </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storageConfig </span><span style=\"color:#B392F0\">StorageConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    testDataPath  </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ValidateMilestone1 demonstrates protocol compliance vocabulary testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">mv </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MilestoneValidator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ValidateMilestone1</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Test upload session initialization via tus protocol</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Validate chunk upload with offset tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Test upload progress query functionality  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Verify chunk assembly and completion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Test error scenarios and recovery</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TusProtocolTester implements tus.io protocol vocabulary testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TusProtocolTester</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    baseURL    </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    httpClient </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    t          </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TestCompleteUploadFlow demonstrates end-to-end protocol testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tpt </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TusProtocolTester</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">TestCompleteUploadFlow</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: POST to create new upload session</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: PATCH requests to upload chunks with proper offsets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: HEAD requests to query upload progress</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Verify final chunk assembly and completion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Test error responses and edge cases</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"language-specific-implementation-hints\">Language-Specific Implementation Hints</h4>\n<p><strong>Go Concurrency Patterns for Upload Processing:</strong></p>\n<ul>\n<li>Use <code>sync.RWMutex</code> for session state protection during concurrent chunk uploads</li>\n<li>Implement worker pools with <code>goroutines</code> and channels for parallel chunk processing  </li>\n<li>Use <code>context.Context</code> for cancellation and timeout propagation across operations</li>\n<li>Apply <code>sync.Once</code> for one-time initialization of shared resources like virus scanners</li>\n</ul>\n<p><strong>File I/O Best Practices:</strong></p>\n<ul>\n<li>Use <code>os.File.Sync()</code> to ensure data persistence for critical session state changes</li>\n<li>Implement <code>io.Copy</code> with buffer size optimization for efficient chunk storage</li>\n<li>Leverage <code>os.O_APPEND</code> flag for safe concurrent chunk writing to assembly files</li>\n<li>Use <code>os.File.Seek()</code> for precise offset positioning during chunk assembly</li>\n</ul>\n<p><strong>Error Handling Patterns:</strong></p>\n<ul>\n<li>Wrap errors with <code>fmt.Errorf(&quot;operation failed: %w&quot;, err)</code> for error chain preservation</li>\n<li>Use typed errors for recoverable conditions that require specific handling logic</li>\n<li>Implement exponential backoff with <code>time.Sleep()</code> and jitter for retry mechanisms</li>\n<li>Apply circuit breaker pattern with goroutine-safe state management</li>\n</ul>\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Milestone 1 - Protocol Vocabulary Validation:</strong>\nAfter implementing the chunked upload protocol vocabulary:</p>\n<ol>\n<li>Run <code>go test ./internal/protocol/...</code> to verify tus.io compliance</li>\n<li>Test manual upload: <code>curl -X POST http://localhost:8080/files -H &quot;Upload-Length: 1024&quot;</code></li>\n<li>Expected response: <code>201 Created</code> with <code>Location</code> header containing upload URL</li>\n<li>Verify session state: Check that <code>UploadSession</code> status transitions correctly</li>\n<li>Debug check: Session should appear in state store with <code>SessionStatusInitialized</code></li>\n</ol>\n<p><strong>Milestone 2 - Storage Vocabulary Validation:</strong><br>After implementing storage abstraction vocabulary:</p>\n<ol>\n<li>Run <code>go test ./internal/storage/...</code> to verify backend implementations</li>\n<li>Test storage switching: Configure local vs S3 backend and verify file operations</li>\n<li>Expected behavior: Same upload should work regardless of backend configuration</li>\n<li>Verify credential handling: Check that <code>CredentialProvider</code> properly manages auth</li>\n<li>Debug check: Storage operations should complete without credential errors</li>\n</ol>\n<p><strong>Milestone 3 - Validation Vocabulary Testing:</strong>\nAfter implementing security validation vocabulary:</p>\n<ol>\n<li>Run <code>go test ./internal/validation/...</code> to verify detection and scanning</li>\n<li>Test file type detection: Upload files with mismatched extensions and MIME types</li>\n<li>Expected behavior: System should detect actual content type regardless of extension</li>\n<li>Verify virus scanning: Upload EICAR test file and confirm quarantine</li>\n<li>Debug check: <code>ValidationResult</code> should reflect proper threat detection</li>\n</ol>\n<p>These checkpoints ensure that the vocabulary concepts are correctly implemented and integrated across the system components.</p>\n","toc":[{"level":1,"text":"Resumable File Upload Service: Design Document","id":"resumable-file-upload-service-design-document"},{"level":2,"text":"Overview","id":"overview"},{"level":2,"text":"Context and Problem Statement","id":"context-and-problem-statement"},{"level":3,"text":"The Large File Transfer Problem","id":"the-large-file-transfer-problem"},{"level":3,"text":"Existing Solutions Analysis","id":"existing-solutions-analysis"},{"level":3,"text":"Core Technical Challenges","id":"core-technical-challenges"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Goals and Non-Goals","id":"goals-and-non-goals"},{"level":3,"text":"Functional Requirements","id":"functional-requirements"},{"level":4,"text":"Upload Resumption Capabilities","id":"upload-resumption-capabilities"},{"level":4,"text":"Virus Scanning and File Validation","id":"virus-scanning-and-file-validation"},{"level":4,"text":"Storage Backend Support","id":"storage-backend-support"},{"level":3,"text":"Non-Functional Requirements","id":"non-functional-requirements"},{"level":4,"text":"Performance Constraints","id":"performance-constraints"},{"level":4,"text":"Reliability Requirements","id":"reliability-requirements"},{"level":4,"text":"Security Constraints","id":"security-constraints"},{"level":3,"text":"Explicit Non-Goals","id":"explicit-non-goals"},{"level":4,"text":"Content Management Features","id":"content-management-features"},{"level":4,"text":"Advanced Transfer Features","id":"advanced-transfer-features"},{"level":4,"text":"Integration and Deployment Features","id":"integration-and-deployment-features"},{"level":4,"text":"Protocol and Transport Limitations","id":"protocol-and-transport-limitations"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Configuration Infrastructure Code","id":"configuration-infrastructure-code"},{"level":4,"text":"Requirement Validation Code","id":"requirement-validation-code"},{"level":4,"text":"Language-Specific Implementation Hints","id":"language-specific-implementation-hints"},{"level":4,"text":"Milestone Validation Checkpoints","id":"milestone-validation-checkpoints"},{"level":2,"text":"High-Level Architecture","id":"high-level-architecture"},{"level":3,"text":"Component Responsibilities","id":"component-responsibilities"},{"level":4,"text":"Upload Manager: Protocol Orchestration and Session Lifecycle","id":"upload-manager-protocol-orchestration-and-session-lifecycle"},{"level":4,"text":"Session Store: State Persistence and Consistency","id":"session-store-state-persistence-and-consistency"},{"level":4,"text":"Storage Backend Abstraction: Unified File Operations","id":"storage-backend-abstraction-unified-file-operations"},{"level":4,"text":"Virus Scanner: Security Validation Pipeline","id":"virus-scanner-security-validation-pipeline"},{"level":3,"text":"Request Flow Patterns","id":"request-flow-patterns"},{"level":4,"text":"Upload Initialization Flow","id":"upload-initialization-flow"},{"level":4,"text":"Chunk Upload Processing Pipeline","id":"chunk-upload-processing-pipeline"},{"level":4,"text":"Upload Completion and Validation Workflow","id":"upload-completion-and-validation-workflow"},{"level":3,"text":"Recommended Module Structure","id":"recommended-module-structure"},{"level":4,"text":"Package Responsibility Matrix","id":"package-responsibility-matrix"},{"level":4,"text":"Dependency Flow and Testing Strategy","id":"dependency-flow-and-testing-strategy"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Core Configuration Structure","id":"core-configuration-structure"},{"level":4,"text":"Session Management Core Interface","id":"session-management-core-interface"},{"level":4,"text":"In-Memory Session Store Implementation","id":"in-memory-session-store-implementation"},{"level":4,"text":"Upload Manager Core Structure","id":"upload-manager-core-structure"},{"level":4,"text":"Storage Interface Foundation","id":"storage-interface-foundation"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Common Implementation Pitfalls","id":"common-implementation-pitfalls"},{"level":2,"text":"Data Model and State Management","id":"data-model-and-state-management"},{"level":3,"text":"Upload Session Model","id":"upload-session-model"},{"level":3,"text":"Chunk Tracking Model","id":"chunk-tracking-model"},{"level":3,"text":"State Persistence Strategy","id":"state-persistence-strategy"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Chunked Upload Protocol Implementation","id":"chunked-upload-protocol-implementation"},{"level":3,"text":"Protocol Mechanics","id":"protocol-mechanics"},{"level":3,"text":"Session Lifecycle Management","id":"session-lifecycle-management"},{"level":3,"text":"Chunk Assembly Strategy","id":"chunk-assembly-strategy"},{"level":3,"text":"Offset Tracking and Resume Logic","id":"offset-tracking-and-resume-logic"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Storage Backend Abstraction","id":"storage-backend-abstraction"},{"level":3,"text":"Storage Interface Design","id":"storage-interface-design"},{"level":3,"text":"Local Filesystem Backend","id":"local-filesystem-backend"},{"level":3,"text":"S3-Compatible Backend","id":"s3-compatible-backend"},{"level":3,"text":"Credential and Configuration Management","id":"credential-and-configuration-management"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"File Validation and Security","id":"file-validation-and-security"},{"level":3,"text":"File Type Validation","id":"file-type-validation"},{"level":3,"text":"Virus Scanning Integration","id":"virus-scanning-integration"},{"level":3,"text":"Quarantine and Policy Enforcement","id":"quarantine-and-policy-enforcement"},{"level":3,"text":"Size Limits and Resource Protection","id":"size-limits-and-resource-protection"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Component Interactions and Data Flow","id":"component-interactions-and-data-flow"},{"level":3,"text":"Upload Initialization Flow: Client handshake, session creation, and metadata exchange","id":"upload-initialization-flow-client-handshake-session-creation-and-metadata-exchange"},{"level":3,"text":"Chunk Processing Pipeline: From chunk receipt through validation to storage commitment","id":"chunk-processing-pipeline-from-chunk-receipt-through-validation-to-storage-commitment"},{"level":3,"text":"Upload Completion Workflow: Assembly, final validation, and client notification sequence","id":"upload-completion-workflow-assembly-final-validation-and-client-notification-sequence"},{"level":3,"text":"Error Propagation Patterns: How failures in different components are handled and communicated","id":"error-propagation-patterns-how-failures-in-different-components-are-handled-and-communicated"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Error Handling and Edge Cases","id":"error-handling-and-edge-cases"},{"level":3,"text":"Network and Connection Failures","id":"network-and-connection-failures"},{"level":3,"text":"Storage Backend Failures","id":"storage-backend-failures"},{"level":3,"text":"Data Corruption Detection and Recovery","id":"data-corruption-detection-and-recovery"},{"level":3,"text":"Resource Exhaustion Scenarios","id":"resource-exhaustion-scenarios"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Testing Strategy and Validation","id":"testing-strategy-and-validation"},{"level":3,"text":"Unit Testing Strategy","id":"unit-testing-strategy"},{"level":3,"text":"Integration Testing Scenarios","id":"integration-testing-scenarios"},{"level":3,"text":"Milestone Validation Checkpoints","id":"milestone-validation-checkpoints"},{"level":3,"text":"Performance and Load Testing","id":"performance-and-load-testing"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Debugging Guide and Common Issues","id":"debugging-guide-and-common-issues"},{"level":3,"text":"Diagnostic Techniques and Tools","id":"diagnostic-techniques-and-tools"},{"level":4,"text":"Structured Logging Strategy","id":"structured-logging-strategy"},{"level":4,"text":"State Inspection Tools","id":"state-inspection-tools"},{"level":4,"text":"Resource Monitoring and Alerting","id":"resource-monitoring-and-alerting"},{"level":4,"text":"Network-Level Debugging","id":"network-level-debugging"},{"level":3,"text":"Common Symptoms and Root Causes","id":"common-symptoms-and-root-causes"},{"level":4,"text":"Upload Resumption Failures","id":"upload-resumption-failures"},{"level":4,"text":"Storage Backend Connectivity Issues","id":"storage-backend-connectivity-issues"},{"level":4,"text":"Virus Scanning Bottlenecks","id":"virus-scanning-bottlenecks"},{"level":4,"text":"Performance Degradation Patterns","id":"performance-degradation-patterns"},{"level":3,"text":"Systematic Troubleshooting Workflow","id":"systematic-troubleshooting-workflow"},{"level":4,"text":"Primary Diagnostic Workflow","id":"primary-diagnostic-workflow"},{"level":4,"text":"Recovery and Remediation Procedures","id":"recovery-and-remediation-procedures"},{"level":4,"text":"Preventive Monitoring Setup","id":"preventive-monitoring-setup"},{"level":4,"text":"Advanced Debugging Techniques","id":"advanced-debugging-techniques"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Module Structure","id":"recommended-module-structure"},{"level":4,"text":"Structured Logging Infrastructure","id":"structured-logging-infrastructure"},{"level":4,"text":"Session State Inspector","id":"session-state-inspector"},{"level":4,"text":"Resource Monitoring System","id":"resource-monitoring-system"},{"level":4,"text":"Recovery Coordination System","id":"recovery-coordination-system"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Future Extensions and Scalability","id":"future-extensions-and-scalability"},{"level":3,"text":"Performance Optimizations","id":"performance-optimizations"},{"level":4,"text":"Parallel Upload Acceleration","id":"parallel-upload-acceleration"},{"level":4,"text":"Bandwidth Optimization and Compression","id":"bandwidth-optimization-and-compression"},{"level":4,"text":"Intelligent Chunk Size Adaptation","id":"intelligent-chunk-size-adaptation"},{"level":4,"text":"Caching and Content Deduplication","id":"caching-and-content-deduplication"},{"level":3,"text":"Advanced Feature Additions","id":"advanced-feature-additions"},{"level":4,"text":"File Encryption and Security Enhancements","id":"file-encryption-and-security-enhancements"},{"level":4,"text":"Advanced Metadata Processing","id":"advanced-metadata-processing"},{"level":4,"text":"Intelligent Virus Scanning Optimization","id":"intelligent-virus-scanning-optimization"},{"level":4,"text":"Content Distribution and CDN Integration","id":"content-distribution-and-cdn-integration"},{"level":3,"text":"Horizontal Scaling Considerations","id":"horizontal-scaling-considerations"},{"level":4,"text":"Multi-Instance Deployment Patterns","id":"multi-instance-deployment-patterns"},{"level":4,"text":"Distributed State Management","id":"distributed-state-management"},{"level":4,"text":"Load Balancing and Session Affinity","id":"load-balancing-and-session-affinity"},{"level":4,"text":"Data Partitioning and Sharding","id":"data-partitioning-and-sharding"},{"level":4,"text":"Service Discovery and Health Management","id":"service-discovery-and-health-management"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Module Structure","id":"recommended-module-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeletons","id":"core-logic-skeletons"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips for Extensions","id":"debugging-tips-for-extensions"},{"level":2,"text":"Glossary and Technical References","id":"glossary-and-technical-references"},{"level":3,"text":"Core Upload Concepts","id":"core-upload-concepts"},{"level":3,"text":"Protocol and Standards","id":"protocol-and-standards"},{"level":3,"text":"Storage and Backend Management","id":"storage-and-backend-management"},{"level":3,"text":"File Validation and Security","id":"file-validation-and-security"},{"level":3,"text":"Error Handling and Resilience","id":"error-handling-and-resilience"},{"level":3,"text":"Performance and Scalability","id":"performance-and-scalability"},{"level":3,"text":"Advanced Features and Extensions","id":"advanced-features-and-extensions"},{"level":3,"text":"Protocol Specifications and Standards","id":"protocol-specifications-and-standards"},{"level":4,"text":"TUS Protocol Specification","id":"tus-protocol-specification"},{"level":4,"text":"HTTP Range Request Standards","id":"http-range-request-standards"},{"level":4,"text":"Cloud Storage APIs","id":"cloud-storage-apis"},{"level":3,"text":"Implementation Dependencies","id":"implementation-dependencies"},{"level":4,"text":"ClamAV Integration","id":"clamav-integration"},{"level":4,"text":"HTTP Server Requirements","id":"http-server-requirements"},{"level":3,"text":"Data Format Specifications","id":"data-format-specifications"},{"level":4,"text":"File Type Detection Signatures","id":"file-type-detection-signatures"},{"level":4,"text":"Checksum Algorithms","id":"checksum-algorithms"},{"level":3,"text":"Common Pitfalls and Anti-Patterns","id":"common-pitfalls-and-anti-patterns"},{"level":3,"text":"Acronyms and Abbreviations","id":"acronyms-and-abbreviations"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Project Structure","id":"recommended-project-structure"},{"level":4,"text":"Core Vocabulary Implementation","id":"core-vocabulary-implementation"},{"level":4,"text":"Storage Vocabulary Implementation","id":"storage-vocabulary-implementation"},{"level":4,"text":"Validation Vocabulary Implementation","id":"validation-vocabulary-implementation"},{"level":4,"text":"Error Handling Vocabulary Implementation","id":"error-handling-vocabulary-implementation"},{"level":4,"text":"Testing Vocabulary Implementation","id":"testing-vocabulary-implementation"},{"level":4,"text":"Language-Specific Implementation Hints","id":"language-specific-implementation-hints"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"}],"title":"Resumable File Upload Service: Design Document","markdown":"# Resumable File Upload Service: Design Document\n\n\n## Overview\n\nA production-grade file upload service that enables reliable transfer of large files through chunked uploads with resumption capabilities, virus scanning, and pluggable storage backends. The key architectural challenge is maintaining upload state consistency across network failures while providing secure, scalable file processing and storage abstraction.\n\n\n> This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.\n\n\n## Context and Problem Statement\n\n> **Milestone(s):** All milestones (1, 2, 3) — this foundational understanding applies to chunked uploads, storage abstraction, and virus scanning\n\n### The Large File Transfer Problem\n\nThink of traditional HTTP file uploads like trying to carry a massive, fragile glass sculpture across a busy city street in one trip. If you stumble even once, the entire sculpture shatters and you have to start over from the beginning. Now imagine that the street has unpredictable traffic patterns, occasional power outages, and your destination is several miles away. This is exactly what happens when users attempt to upload large files over unreliable network connections using standard HTML form uploads or simple HTTP POST requests.\n\nIn the real world, users routinely attempt to upload files ranging from hundreds of megabytes to several gigabytes. Video content creators upload raw footage files that can exceed 10GB. Medical institutions transfer high-resolution imaging data that approaches 1GB per scan. Engineering teams share build artifacts, VM images, and dataset exports that dwarf typical web content. Scientific research organizations routinely work with simulation outputs and experimental datasets measured in terabytes.\n\nThe fundamental problem with traditional upload approaches becomes apparent when we examine the failure modes. A standard HTTP file upload requires the entire file to be transmitted in a single, uninterrupted stream. If the network connection drops at 95% completion of a 5GB upload that has been running for three hours over a slow connection, the entire transfer fails and must be restarted from zero bytes. The client has no mechanism to communicate \"I already sent you the first 4.75GB successfully\" and the server has no standard way to acknowledge partial progress.\n\n**Business Impact Analysis**\n\nThe business consequences of upload failures extend far beyond user frustration. Consider a video production company where editors work with 4K raw footage files averaging 8GB each. If their internet connection experiences typical residential instability with brief disconnections every 30-60 minutes, successfully uploading a single file becomes practically impossible through traditional means. The editor may attempt the same upload dozens of times, consuming enormous bandwidth while never achieving success. This translates directly to lost productivity, missed deadlines, and increased infrastructure costs from wasted bandwidth.\n\nMedical imaging presents an even more critical scenario. A radiologist attempting to upload urgent CT scan data for remote consultation faces a time-sensitive situation where upload failures can delay patient diagnosis. The 800MB scan that fails to upload after 45 minutes of progress doesn't just represent technical inconvenience—it represents potential impact on patient care quality.\n\nEnterprise scenarios amplify these costs through scale effects. A software company with distributed development teams uploading build artifacts to shared repositories may see dozens of developers each struggling with failed uploads of 500MB+ build outputs. The cumulative productivity loss, multiplied across engineering teams and compounded by CI/CD pipeline delays, can reach thousands of dollars per incident in lost developer time.\n\n**Network Reality Assessment**\n\nReal-world network conditions bear little resemblance to the stable, high-bandwidth connections assumed by traditional upload mechanisms. Mobile connections frequently transition between cell towers, causing brief disconnections. Home broadband experiences periodic congestion during peak usage hours. Corporate networks implement aggressive timeout policies that terminate long-running connections. WiFi connections in airports, coffee shops, and co-working spaces provide intermittent connectivity with unpredictable quality fluctuations.\n\nThe statistics paint a stark picture: according to industry measurements, the probability of a network connection remaining stable decreases exponentially with connection duration. A 5-minute HTTP connection has roughly 95% reliability, but a 3-hour connection required for large file upload may see reliability drop below 20% on typical residential connections. This mathematical reality makes traditional upload approaches fundamentally unsuitable for large files over real-world networks.\n\n> The core insight is that network failures are not exceptional edge cases to handle gracefully—they are the dominant characteristic of long-duration connections that must be designed for from the ground up.\n\n### Existing Solutions Analysis\n\n**HTTP Multipart Uploads**\n\nStandard HTTP multipart form uploads represent the baseline approach implemented by most web applications. The client encodes the file data using `multipart/form-data` encoding and transmits it via a single HTTP POST request. The server receives the entire payload, buffers it in memory or temporary storage, and processes it atomically.\n\n| Aspect | Behavior | Limitation |\n|--------|----------|------------|\n| Failure Recovery | None - entire upload lost on disconnection | Complete restart required for any interruption |\n| Progress Tracking | Client-side only, no server acknowledgment | No way to resume from known good state |\n| Memory Usage | Server buffers entire file during upload | Memory exhaustion with large files or concurrent uploads |\n| Timeout Handling | Standard HTTP timeout applies to entire upload | Large files exceed typical timeout values |\n| Protocol Complexity | Minimal - standard HTTP | No extensibility for advanced features |\n\nThe fundamental architectural flaw in multipart uploads is their atomic nature. Success requires uninterrupted transmission of the complete file within the HTTP timeout window. This creates a binary outcome: complete success or complete failure, with no middle ground for partial progress preservation.\n\n**tus.io Resumable Upload Protocol**\n\nThe tus.io specification emerged from recognition that file uploads required a purpose-built protocol rather than attempting to force-fit standard HTTP mechanisms. Think of tus.io as transforming file upload from \"carrying the entire sculpture in one trip\" to \"moving it brick by brick, with each brick permanently placed before moving the next one.\"\n\n| Protocol Feature | Implementation | Benefit |\n|------------------|----------------|---------|\n| Upload Initialization | `POST` to create upload session with metadata | Server allocates space and returns upload URL |\n| Offset Discovery | `HEAD` request returns `Upload-Offset` header | Client learns exactly where to resume |\n| Chunked Transfer | `PATCH` requests append data at specific offsets | Incremental progress with failure isolation |\n| Progress Verification | Server validates each chunk before acknowledgment | Corruption detection at chunk boundaries |\n| Session Management | Upload URLs remain valid for configurable TTL | Clients can resume after arbitrary delays |\n\nThe tus.io approach fundamentally changes the failure model. Instead of \"all or nothing,\" uploads become a series of small, independent operations. If chunk 47 out of 200 fails to upload, only that specific chunk needs retransmission. The other 46 chunks remain safely stored on the server, preserving the investment in already-transmitted data.\n\nThe protocol's offset-based resume mechanism provides precise recovery semantics. When a client reconnects after a network failure, a simple `HEAD` request to the upload URL returns the current server-side offset. The client compares this against its local progress tracking and resumes transmission from the exact byte position where the server's knowledge ends.\n\n**Cloud-Specific Solutions**\n\nMajor cloud providers offer sophisticated multipart upload APIs optimized for their storage infrastructure. These solutions emerged from the same fundamental recognition that large file transfers require specialized handling, but they tie implementations to specific cloud ecosystems.\n\nAmazon S3's multipart upload API exemplifies the cloud approach. Clients initiate an upload session that returns an `UploadId`, then upload individual parts identified by part numbers. Each part can be uploaded independently and in parallel, with the server tracking completion status. Once all parts are uploaded, the client issues a completion request that assembles the parts into the final object.\n\n| Cloud Solution | Strengths | Constraints |\n|----------------|-----------|-------------|\n| S3 Multipart Upload | Native cloud integration, parallel part uploads | AWS-specific, minimum 5MB per part (except last) |\n| Google Cloud Resumable Upload | Single-stream resume with offset tracking | GCP-specific, limited to Google Cloud Storage |\n| Azure Blob Block Upload | Block-based composition with parallel upload | Azure-specific, complex block ID management |\n\nThe primary limitation of cloud-specific solutions is vendor lock-in. An application built around S3's multipart upload API cannot easily migrate to Google Cloud Storage or local storage without significant architectural changes. This creates strategic flexibility constraints for organizations that may need to change storage backends in response to cost, compliance, or performance requirements.\n\n> **Decision: Protocol Foundation Choice**\n> - **Context**: Need resumable upload capability that works across multiple storage backends while providing robust failure recovery\n> - **Options Considered**: \n>   1. HTTP multipart (simple but no resume capability)\n>   2. tus.io protocol (standardized resume protocol)\n>   3. Cloud-specific APIs (optimized but vendor-locked)\n> - **Decision**: Implement tus.io-compatible protocol with storage backend abstraction\n> - **Rationale**: tus.io provides standardized resume semantics while remaining storage-agnostic. The protocol's offset-based resume mechanism offers precise failure recovery, and the specification's maturity provides proven real-world reliability.\n> - **Consequences**: Enables client compatibility with existing tus.io libraries while allowing flexible storage backend selection. Requires implementing protocol state management but provides superior failure recovery compared to alternatives.\n\n### Core Technical Challenges\n\nThe transition from simple HTTP uploads to resumable chunked uploads introduces several fundamental technical challenges that must be solved systematically. Each challenge represents a category of complexity that doesn't exist in traditional upload scenarios.\n\n**Upload Session State Management**\n\nTraditional HTTP uploads are stateless—each request is independent and the server doesn't need to track any information between requests. Resumable uploads invert this model completely, requiring the server to maintain persistent state about ongoing upload sessions that may span hours or days.\n\nConsider the state information that must be tracked for each upload session: the total expected file size, the current byte offset representing successfully received data, metadata about the target file (name, content type, destination path), the timestamp of the last activity (for session expiration), and the current session status (initializing, active, completing, failed, expired). This state must survive server restarts, which means it cannot be kept only in memory.\n\nThe state management challenge extends to concurrent access patterns. Multiple chunks of the same file might be uploaded in parallel, requiring atomic updates to the session's progress tracking. A chunk uploaded to offset 1000-2000 must not be marked as received until it's safely written to storage, but the server must also prevent duplicate uploads of the same byte range if the client retries due to a timeout.\n\n| State Management Challenge | Technical Requirement | Implementation Complexity |\n|---------------------------|----------------------|--------------------------|\n| Session Persistence | Survive server restarts and crashes | Database or persistent file-based storage |\n| Concurrent Access | Multiple chunks updating same session | Atomic operations with proper locking |\n| Expiration Handling | Clean up abandoned uploads | Background cleanup process with configurable TTL |\n| Progress Tracking | Byte-level accuracy for resume operations | Efficient data structures for sparse range tracking |\n| Metadata Storage | File attributes, security context, validation state | Flexible schema supporting future extensions |\n\n**Partial Failure Handling and Recovery**\n\nResumable uploads introduce a complex failure landscape that doesn't exist in atomic operations. Traditional uploads either succeed completely or fail completely—there's no intermediate state requiring recovery logic. Chunked uploads create scenarios where parts of the operation succeed while other parts fail, requiring sophisticated detection and recovery mechanisms.\n\nConsider a typical failure scenario: a client uploads chunks 1-5 successfully, chunk 6 fails due to a network timeout, chunks 7-8 succeed, then the client disconnects entirely. The server must be able to communicate that bytes 0-5120 and bytes 7168-8192 are safely stored, but bytes 5120-7168 need retransmission. This requires not just tracking the current offset, but potentially tracking arbitrary ranges of received data.\n\nStorage-level failures add another dimension of complexity. If a chunk is received successfully by the upload service but the write to the storage backend fails, should the server report success or failure to the client? If it reports success but the data isn't actually stored, the file will be corrupted. If it reports failure but retains the data in temporary storage, what happens if the client never retries?\n\nThe consistency requirements become even more challenging during the final assembly phase. After all chunks are uploaded, they must be assembled into the final file atomically. If this assembly process fails partway through, the server must be able to retry the assembly without requiring clients to re-upload chunks.\n\n**Storage Backend Coordination**\n\nA resumable upload service that supports multiple storage backends must solve coordination challenges that don't exist when uploading directly to a single storage system. Each storage backend has different characteristics, limitations, and optimal usage patterns that must be abstracted behind a common interface.\n\nLocal filesystem storage allows random-access writes, enabling chunks to be written directly to their final positions within the target file. However, this requires pre-allocating the file to the expected size and managing concurrent writes to different file regions safely. File locking, space allocation failures, and filesystem-specific constraints (like maximum file sizes) must all be handled transparently.\n\nCloud storage backends like S3 use fundamentally different models. S3's multipart upload API requires parts to be uploaded in sequence and assembled at the end—you cannot write arbitrary byte ranges to arbitrary offsets. The service must buffer chunks locally until they can be uploaded to S3 as properly-sized parts, introducing additional storage and complexity requirements.\n\n| Storage Backend | Upload Model | Coordination Challenge |\n|-----------------|--------------|----------------------|\n| Local Filesystem | Random-access writes to pre-allocated file | Concurrent access control, space management |\n| Amazon S3 | Sequential multipart upload with final assembly | Local buffering, part size constraints |\n| Google Cloud Storage | Resumable upload with offset-based appends | Single-stream coordination, retry handling |\n| Database BLOB Storage | Atomic replacement with versioning | Transaction management, size limitations |\n\nThe abstraction layer must handle these differences transparently while providing consistent semantics to the upload protocol layer. This requires careful interface design that can accommodate the capabilities and constraints of each backend without forcing the lowest common denominator approach that would eliminate the benefits of backend-specific optimizations.\n\n**Data Integrity and Corruption Detection**\n\nChunked uploads distributed across time and potentially across multiple network connections introduce data integrity challenges that require systematic solutions. Unlike atomic uploads where corruption is immediately apparent (the file doesn't match expected checksums), resumable uploads can accumulate corruption gradually as individual chunks are received and stored.\n\nEach chunk must be validated independently before being considered successfully received. This validation typically includes verifying that the chunk's size matches the declared size, that it's being written to the correct offset, and that its content matches any provided checksums. However, chunk-level integrity doesn't guarantee file-level integrity—corruption can be introduced during the final assembly process or through storage backend issues.\n\nThe service must also handle scenarios where the client and server disagree about what data has been successfully transferred. If the server believes it has received chunks 1-10 but the client believes only chunks 1-8 were acknowledged, the disagreement must be detected and resolved without corrupting the final file.\n\n> The fundamental insight is that resumable uploads transform file transfer from a single point of failure into a distributed system problem, requiring solutions for state management, partial failures, backend coordination, and data integrity that don't exist in traditional upload architectures.\n\nThis complexity justifies the architectural investment required to build a robust resumable upload system, but it also highlights why many applications continue to use simpler approaches despite their limitations. The following sections detail how each of these challenges can be systematically addressed through careful design and implementation.\n\n### Implementation Guidance\n\n**A. Technology Recommendations Table:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| HTTP Server | `net/http` with standard library | `gin-gonic/gin` or `gorilla/mux` for routing |\n| State Persistence | Local SQLite database | PostgreSQL or Redis for production |\n| File Storage | `os.File` with direct filesystem operations | `afero` filesystem abstraction library |\n| Configuration | `encoding/json` with config files | `spf13/viper` for multiple config sources |\n| Logging | `log/slog` standard library | `sirupsen/logrus` or `uber-go/zap` |\n| HTTP Client Testing | `net/http/httptest` | `jarcoal/httpmock` for complex scenarios |\n\n**B. Recommended File/Module Structure**\n\nUnderstanding the problem space helps organize code into logical packages that separate concerns effectively:\n\n```\nresumable-upload-service/\n├── cmd/\n│   └── server/\n│       └── main.go                    ← entry point, configuration loading\n├── internal/\n│   ├── protocol/                      ← tus.io protocol implementation\n│   │   ├── handler.go                 ← HTTP handlers for upload operations\n│   │   ├── session.go                 ← upload session management\n│   │   └── protocol_test.go           ← protocol compliance tests\n│   ├── storage/                       ← storage backend abstraction\n│   │   ├── interface.go               ← common storage interface\n│   │   ├── local.go                   ← local filesystem implementation\n│   │   ├── s3.go                      ← S3-compatible implementation\n│   │   └── storage_test.go            ← backend integration tests\n│   ├── validation/                    ← file validation and virus scanning\n│   │   ├── scanner.go                 ← virus scanning integration\n│   │   ├── validator.go               ← file type and size validation\n│   │   └── quarantine.go              ← suspicious file handling\n│   ├── state/                         ← session state management\n│   │   ├── store.go                   ← state persistence interface\n│   │   ├── sqlite.go                  ← SQLite implementation\n│   │   └── memory.go                  ← in-memory for testing\n│   └── config/\n│       └── config.go                  ← configuration structures\n├── pkg/                               ← public APIs (if building a library)\n├── test/\n│   ├── integration/                   ← end-to-end test scenarios\n│   └── fixtures/                      ← test data and mock files\n└── docs/\n    └── api/                           ← API documentation\n```\n\nThis structure reflects the core technical challenges identified in the problem analysis. The `protocol` package handles tus.io implementation and session state management. The `storage` package provides backend abstraction for different storage systems. The `validation` package addresses security and file integrity concerns. The `state` package manages upload session persistence across server restarts.\n\n**C. Infrastructure Starter Code**\n\nThe following provides complete, working infrastructure that handles the foundational requirements, allowing focus on the core resumable upload logic:\n\n```go\n// internal/config/config.go\npackage config\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"os\"\n    \"time\"\n)\n\n// Config represents the complete service configuration\ntype Config struct {\n    Server   ServerConfig   `json:\"server\"`\n    Storage  StorageConfig  `json:\"storage\"`\n    Security SecurityConfig `json:\"security\"`\n    Cleanup  CleanupConfig  `json:\"cleanup\"`\n}\n\ntype ServerConfig struct {\n    Host         string        `json:\"host\"`\n    Port         int           `json:\"port\"`\n    ReadTimeout  time.Duration `json:\"read_timeout\"`\n    WriteTimeout time.Duration `json:\"write_timeout\"`\n    MaxBodySize  int64         `json:\"max_body_size\"`\n}\n\ntype StorageConfig struct {\n    Backend   string            `json:\"backend\"` // \"local\", \"s3\", \"gcs\"\n    LocalPath string            `json:\"local_path,omitempty\"`\n    S3Config  *S3Config         `json:\"s3,omitempty\"`\n    Options   map[string]string `json:\"options\"`\n}\n\ntype S3Config struct {\n    Bucket          string `json:\"bucket\"`\n    Region          string `json:\"region\"`\n    AccessKeyID     string `json:\"access_key_id\"`\n    SecretAccessKey string `json:\"secret_access_key\"`\n    Endpoint        string `json:\"endpoint,omitempty\"` // for S3-compatible services\n}\n\ntype SecurityConfig struct {\n    MaxFileSize         int64    `json:\"max_file_size\"`\n    AllowedContentTypes []string `json:\"allowed_content_types\"`\n    VirusScanEnabled    bool     `json:\"virus_scan_enabled\"`\n    ClamAVSocket        string   `json:\"clamav_socket,omitempty\"`\n}\n\ntype CleanupConfig struct {\n    SessionTTL      time.Duration `json:\"session_ttl\"`\n    CleanupInterval time.Duration `json:\"cleanup_interval\"`\n    QuarantineTTL   time.Duration `json:\"quarantine_ttl\"`\n}\n\n// LoadConfig reads configuration from a JSON file\nfunc LoadConfig(filename string) (*Config, error) {\n    data, err := os.ReadFile(filename)\n    if err != nil {\n        return nil, fmt.Errorf(\"reading config file: %w\", err)\n    }\n    \n    var config Config\n    if err := json.Unmarshal(data, &config); err != nil {\n        return nil, fmt.Errorf(\"parsing config: %w\", err)\n    }\n    \n    // Set defaults for optional fields\n    if config.Server.Host == \"\" {\n        config.Server.Host = \"localhost\"\n    }\n    if config.Server.Port == 0 {\n        config.Server.Port = 8080\n    }\n    if config.Server.ReadTimeout == 0 {\n        config.Server.ReadTimeout = 30 * time.Second\n    }\n    if config.Server.WriteTimeout == 0 {\n        config.Server.WriteTimeout = 30 * time.Second\n    }\n    if config.Server.MaxBodySize == 0 {\n        config.Server.MaxBodySize = 32 << 20 // 32MB default chunk size\n    }\n    \n    return &config, nil\n}\n```\n\n```go\n// internal/state/memory.go - In-memory state store for testing\npackage state\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"sync\"\n    \"time\"\n)\n\n// MemoryStore provides an in-memory implementation of StateStore for testing\ntype MemoryStore struct {\n    mu       sync.RWMutex\n    sessions map[string]*UploadSession\n}\n\n// NewMemoryStore creates a new in-memory state store\nfunc NewMemoryStore() *MemoryStore {\n    return &MemoryStore{\n        sessions: make(map[string]*UploadSession),\n    }\n}\n\n// CreateSession stores a new upload session\nfunc (m *MemoryStore) CreateSession(ctx context.Context, session *UploadSession) error {\n    m.mu.Lock()\n    defer m.mu.Unlock()\n    \n    if _, exists := m.sessions[session.ID]; exists {\n        return fmt.Errorf(\"session %s already exists\", session.ID)\n    }\n    \n    // Create a deep copy to avoid external mutations\n    sessionCopy := *session\n    sessionCopy.CreatedAt = time.Now()\n    sessionCopy.UpdatedAt = time.Now()\n    \n    m.sessions[session.ID] = &sessionCopy\n    return nil\n}\n\n// GetSession retrieves an upload session by ID\nfunc (m *MemoryStore) GetSession(ctx context.Context, sessionID string) (*UploadSession, error) {\n    m.mu.RLock()\n    defer m.mu.RUnlock()\n    \n    session, exists := m.sessions[sessionID]\n    if !exists {\n        return nil, ErrSessionNotFound\n    }\n    \n    // Return a copy to prevent external mutations\n    sessionCopy := *session\n    return &sessionCopy, nil\n}\n\n// UpdateSession modifies an existing upload session\nfunc (m *MemoryStore) UpdateSession(ctx context.Context, session *UploadSession) error {\n    m.mu.Lock()\n    defer m.mu.Unlock()\n    \n    if _, exists := m.sessions[session.ID]; !exists {\n        return ErrSessionNotFound\n    }\n    \n    sessionCopy := *session\n    sessionCopy.UpdatedAt = time.Now()\n    m.sessions[session.ID] = &sessionCopy\n    return nil\n}\n\n// DeleteSession removes an upload session\nfunc (m *MemoryStore) DeleteSession(ctx context.Context, sessionID string) error {\n    m.mu.Lock()\n    defer m.mu.Unlock()\n    \n    if _, exists := m.sessions[sessionID]; !exists {\n        return ErrSessionNotFound\n    }\n    \n    delete(m.sessions, sessionID)\n    return nil\n}\n\n// ListExpiredSessions returns sessions older than the specified TTL\nfunc (m *MemoryStore) ListExpiredSessions(ctx context.Context, ttl time.Duration) ([]string, error) {\n    m.mu.RLock()\n    defer m.mu.RUnlock()\n    \n    cutoff := time.Now().Add(-ttl)\n    var expired []string\n    \n    for id, session := range m.sessions {\n        if session.UpdatedAt.Before(cutoff) {\n            expired = append(expired, id)\n        }\n    }\n    \n    return expired, nil\n}\n```\n\n**D. Core Logic Skeleton Code**\n\nThe following skeletons map directly to the technical challenges identified in the problem analysis:\n\n```go\n// internal/protocol/session.go\npackage protocol\n\nimport (\n    \"context\"\n    \"time\"\n)\n\n// UploadSession represents the state of an ongoing resumable upload\ntype UploadSession struct {\n    ID            string                 `json:\"id\"`\n    Filename      string                 `json:\"filename\"`\n    ContentType   string                 `json:\"content_type\"`\n    TotalSize     int64                  `json:\"total_size\"`\n    CurrentOffset int64                  `json:\"current_offset\"`\n    Status        SessionStatus          `json:\"status\"`\n    StorageKey    string                 `json:\"storage_key\"`\n    Metadata      map[string]string      `json:\"metadata\"`\n    CreatedAt     time.Time              `json:\"created_at\"`\n    UpdatedAt     time.Time              `json:\"updated_at\"`\n    ChunkHashes   map[int64]string       `json:\"chunk_hashes\"` // offset -> hash\n}\n\ntype SessionStatus string\n\nconst (\n    SessionStatusInitialized SessionStatus = \"initialized\"\n    SessionStatusActive      SessionStatus = \"active\"\n    SessionStatusCompleting  SessionStatus = \"completing\"\n    SessionStatusCompleted   SessionStatus = \"completed\"\n    SessionStatusFailed      SessionStatus = \"failed\"\n    SessionStatusExpired     SessionStatus = \"expired\"\n)\n\n// SessionManager handles upload session lifecycle and coordination\ntype SessionManager struct {\n    store   StateStore\n    storage StorageBackend\n}\n\n// InitializeUpload creates a new upload session and prepares storage\nfunc (sm *SessionManager) InitializeUpload(ctx context.Context, filename string, totalSize int64, metadata map[string]string) (*UploadSession, error) {\n    // TODO 1: Generate unique session ID using crypto/rand\n    // TODO 2: Validate filename and total size parameters\n    // TODO 3: Create storage key/path for this upload\n    // TODO 4: Initialize storage backend for this upload (pre-allocate if needed)\n    // TODO 5: Create UploadSession struct with initialized status\n    // TODO 6: Store session in state store\n    // TODO 7: Return session with upload URL for client\n    // Hint: Use uuid.New() for session ID generation\n    // Hint: Storage key should include session ID to avoid conflicts\n    panic(\"implement me\")\n}\n\n// ProcessChunk handles an incoming chunk upload\nfunc (sm *SessionManager) ProcessChunk(ctx context.Context, sessionID string, offset int64, data []byte, contentHash string) error {\n    // TODO 1: Retrieve upload session from state store\n    // TODO 2: Validate session status is active or initialized\n    // TODO 3: Verify offset matches current session offset (no gaps)\n    // TODO 4: Validate chunk hash if provided\n    // TODO 5: Write chunk data to storage backend at correct offset\n    // TODO 6: Update session current offset and chunk tracking\n    // TODO 7: Update session in state store with new progress\n    // TODO 8: If this completes the upload, trigger completion process\n    // Hint: Use crypto/sha256 to verify chunk hashes\n    // Hint: Offset validation prevents chunks uploaded out of order\n    panic(\"implement me\")\n}\n\n// GetUploadProgress returns current upload status and offset for resume\nfunc (sm *SessionManager) GetUploadProgress(ctx context.Context, sessionID string) (*UploadSession, error) {\n    // TODO 1: Retrieve session from state store\n    // TODO 2: Validate session exists and is not expired\n    // TODO 3: Return session with current offset for client resume\n    // Hint: This implements the tus.io HEAD request for offset discovery\n    panic(\"implement me\")\n}\n```\n\n**E. Language-Specific Hints**\n\n**Go-Specific Implementation Tips:**\n- Use `crypto/rand.Read()` with `encoding/hex` for generating secure session IDs\n- `os.OpenFile()` with `O_RDWR|O_CREATE` for file pre-allocation on local storage\n- `io.Copy()` with `io.LimitReader()` for safe chunk reading from HTTP requests\n- `sync.RWMutex` for session state that has frequent reads and occasional writes\n- `context.WithTimeout()` for all storage operations to prevent hanging\n- `os.File.Sync()` after writing chunks to ensure durability before acknowledging\n- `filepath.Clean()` and `filepath.IsAbs()` to sanitize file paths and prevent directory traversal\n- `net/http.MaxBytesReader()` to enforce chunk size limits and prevent DoS\n\n**Error Handling Patterns:**\n- Create custom error types for different failure categories: `ErrSessionNotFound`, `ErrInvalidOffset`, `ErrStorageFailure`\n- Use `fmt.Errorf(\"operation failed: %w\", err)` for error wrapping with context\n- Implement retry logic with exponential backoff for transient storage failures\n- Log errors with context: session ID, offset, operation type for debugging\n\n**Performance Considerations:**\n- Buffer chunk writes using `bufio.Writer` to reduce syscall overhead\n- Use `sync.Pool` for reusing byte buffers during chunk processing\n- Implement connection pooling for S3 clients to avoid connection setup overhead\n- Consider using `mmap` for large file operations on local storage\n\n**F. Milestone Checkpoint**\n\nAfter implementing the session management foundation, verify the following behavior:\n\n**Basic Session Operations:**\n```bash\n# Test session creation\ncurl -X POST http://localhost:8080/uploads \\\n  -H \"Upload-Length: 1048576\" \\\n  -H \"Upload-Metadata: filename dGVzdC50eHQ=\" \\\n  -v\n\n# Expected: HTTP 201 Created with Location header containing session URL\n# Expected: Response includes Upload-Offset: 0 header\n```\n\n**Offset Discovery:**\n```bash\n# Test progress tracking\ncurl -X HEAD http://localhost:8080/uploads/SESSION_ID -v\n\n# Expected: HTTP 200 OK with Upload-Offset header showing current progress\n# Expected: Response includes Upload-Length header with total size\n```\n\n**Signs of Implementation Issues:**\n- Session ID collisions (use crypto/rand, not math/rand)\n- Race conditions during concurrent chunk uploads (check locking)\n- Memory leaks from unclosed files (use defer for cleanup)\n- Storage exhaustion from abandoned uploads (implement cleanup)\n\nThe next milestone will build chunk processing on this session management foundation.\n\n\n## Goals and Non-Goals\n\n> **Milestone(s):** All milestones (1, 2, 3) — this section defines the scope for chunked upload protocol, storage abstraction, and virus scanning\n\nThink of this service as a **digital postal system for large packages**. Just as postal services have clear rules about what they will and won't handle (package size limits, prohibited items, delivery guarantees), our resumable file upload service needs explicit boundaries. Unlike a simple file copy operation that either succeeds completely or fails entirely, our service must handle the messy reality of network interruptions, partial transfers, and security threats while maintaining consistent behavior across different storage destinations.\n\nThe key insight is that **scope definition prevents feature creep** and ensures the system remains focused on its core value proposition: reliable large file uploads with resumption capabilities. Without clear boundaries, we risk building a general-purpose file management system instead of a specialized upload service.\n\n### Functional Requirements\n\nThe functional requirements define the **core capabilities** that users can depend on. These requirements directly map to our three implementation milestones and represent the minimum viable feature set for production deployment.\n\n#### Upload Resumption Capabilities\n\nOur primary functional requirement is **resumable upload support** following the tus.io protocol specification. This means the service must handle network interruptions gracefully and allow clients to resume uploads from the exact byte offset where they left off.\n\n| Capability | Description | Success Criteria |\n|------------|-------------|------------------|\n| Upload Session Management | Create and track upload sessions across multiple requests | Session persists across service restarts, unique session IDs |\n| Offset Tracking | Maintain precise byte-level progress for each upload | Client can query current offset and resume from exact position |\n| Chunk Processing | Accept file chunks in any order with proper sequencing | Chunks assembled correctly regardless of arrival order |\n| Progress Persistence | Store upload state to survive server failures | Upload sessions survive service crashes and restarts |\n| Completion Detection | Identify when all chunks received and trigger assembly | Final file matches expected size and checksum |\n\n> **Decision: tus.io Protocol Compliance**\n> - **Context**: Multiple resumable upload protocols exist (custom HTTP ranges, vendor-specific solutions, tus.io standard)\n> - **Options Considered**: \n>   1. Custom HTTP Range-based protocol\n>   2. Cloud vendor-specific protocols (S3 multipart, GCS resumable)\n>   3. tus.io standard protocol\n> - **Decision**: Implement tus.io protocol compliance\n> - **Rationale**: tus.io provides standardized client libraries, well-defined error handling, and broad ecosystem support. Custom protocols require client SDK development.\n> - **Consequences**: Enables third-party client integration but requires strict protocol compliance\n\nThe upload session lifecycle follows a clear state progression that clients can rely on for consistent behavior:\n\n| Session State | Description | Available Operations | Next States |\n|---------------|-------------|---------------------|-------------|\n| `SessionStatusInitialized` | Session created, ready for chunks | Upload chunks, query progress | Active, Failed, Expired |\n| `SessionStatusActive` | Actively receiving chunks | Upload chunks, query progress | Completing, Failed, Expired |\n| `SessionStatusCompleting` | All chunks received, processing final assembly | Query progress only | Completed, Failed |\n| `SessionStatusCompleted` | Upload successfully assembled and stored | Download file, delete session | None (terminal) |\n| `SessionStatusFailed` | Upload failed validation or assembly | Query error details, retry | None (terminal) |\n| `SessionStatusExpired` | Session exceeded time-to-live | None | None (terminal) |\n\n#### Virus Scanning and File Validation\n\nThe service must **automatically scan all uploaded content** for malicious software before final storage. This scanning happens transparently to clients but provides essential security protection.\n\n| Validation Type | Implementation | Failure Action |\n|----------------|----------------|----------------|\n| File Type Validation | Magic byte detection using file headers | Reject upload with specific error code |\n| Size Limit Enforcement | Configurable maximum file size checking | Abort upload when limit exceeded |\n| Virus Scanning | ClamAV integration with signature updates | Quarantine file, notify administrators |\n| Content Integrity | SHA-256 checksum verification | Request chunk re-upload for mismatches |\n\nThe validation pipeline operates **asynchronously after chunk assembly** to avoid blocking the upload process. However, files remain unavailable for download until validation completes successfully.\n\n#### Storage Backend Support\n\nThe service must support **multiple storage backends** through a unified interface, allowing deployment flexibility without client-side changes. This abstraction enables organizations to choose storage solutions based on cost, compliance, or performance requirements.\n\n| Storage Backend | Capabilities | Configuration Requirements |\n|----------------|--------------|---------------------------|\n| Local Filesystem | Direct file I/O, fast access | Mounted storage path, permissions |\n| AWS S3 | Multipart uploads, global access | Bucket name, credentials, region |\n| Google Cloud Storage | Resumable uploads, lifecycle management | Bucket name, service account key |\n| Azure Blob Storage | Block-based uploads, redundancy options | Container name, connection string |\n\n> **Decision: Storage Interface Abstraction**\n> - **Context**: Different storage systems have unique APIs, authentication, and capabilities\n> - **Options Considered**:\n>   1. Direct integration with each storage system\n>   2. Adapter pattern with common interface\n>   3. Proxy pattern with protocol translation\n> - **Decision**: Adapter pattern with storage interface abstraction\n> - **Rationale**: Enables adding new backends without core logic changes, simplifies testing with mock storage\n> - **Consequences**: Requires interface design that accommodates different storage paradigms, potential lowest-common-denominator limitations\n\n### Non-Functional Requirements\n\nNon-functional requirements define the **quality attributes** and constraints that the service must satisfy. These requirements ensure the system operates reliably under real-world conditions and meets production deployment standards.\n\n#### Performance Constraints\n\nThe service must handle **concurrent uploads efficiently** without degrading individual transfer performance. Performance requirements balance throughput with resource consumption.\n\n| Performance Metric | Target | Measurement Method |\n|-------------------|--------|--------------------|\n| Concurrent Upload Sessions | 100+ simultaneous uploads | Load testing with multiple clients |\n| Chunk Processing Latency | < 100ms per chunk (excluding storage I/O) | HTTP response time measurements |\n| Memory Usage Per Session | < 10MB session metadata | Process memory monitoring |\n| Storage I/O Throughput | 90% of underlying storage bandwidth | Disk/network utilization metrics |\n| Session State Persistence | < 50ms write latency | Database/file system timing |\n\n#### Reliability Requirements\n\nThe service must provide **strong durability guarantees** for upload sessions and file data. Reliability means uploads complete successfully despite infrastructure failures.\n\n| Reliability Aspect | Requirement | Implementation Strategy |\n|-------------------|-------------|------------------------|\n| Session State Durability | Zero session loss during server restarts | Persistent session storage with atomic updates |\n| File Data Integrity | Zero byte corruption during upload | Chunk-level checksums with retry mechanisms |\n| Crash Recovery | Full session recovery within 30 seconds | Fast startup with session state scanning |\n| Storage Backend Failover | Automatic retry with exponential backoff | Circuit breaker pattern for backend failures |\n| Partial Failure Handling | Graceful degradation when components fail | Isolated failure domains, queue-based processing |\n\n#### Security Constraints\n\nSecurity requirements protect both the service infrastructure and uploaded content from malicious actors. The security model assumes **untrusted input** and implements defense-in-depth strategies.\n\n| Security Domain | Requirement | Enforcement Mechanism |\n|-----------------|-------------|----------------------|\n| Input Validation | Reject malformed requests and payloads | Schema validation, size limits, type checking |\n| File Content Scanning | Block known malware before storage | Real-time ClamAV scanning with signature updates |\n| Path Traversal Prevention | Prevent access to unauthorized filesystem areas | Input sanitization, chroot jail for local storage |\n| Credential Protection | Secure storage of cloud backend credentials | Environment variables, secret management systems |\n| Access Control | Prevent unauthorized access to upload sessions | Session token validation, origin checking |\n\n> **Decision: Security-First Design**\n> - **Context**: File upload services are common attack vectors for malware distribution and system compromise\n> - **Options Considered**:\n>   1. Trust client-provided metadata and validate after storage\n>   2. Validate during upload process before storage\n>   3. Sandbox uploads and validate in isolated environment\n> - **Decision**: Validate during upload with quarantine for suspicious content\n> - **Rationale**: Prevents malicious content from reaching production storage, enables early rejection of invalid uploads\n> - **Consequences**: Increases upload latency but significantly reduces security risk\n\n### Explicit Non-Goals\n\nClearly defining what the service **will not handle** prevents scope creep and helps users understand the system boundaries. These non-goals represent features that might seem related but fall outside our core mission.\n\n#### Content Management Features\n\nThe service is **not a content management system** and deliberately avoids features that would complicate the core upload functionality.\n\n| Feature Category | Specific Non-Goals | Rationale |\n|------------------|-------------------|-----------|\n| File Organization | Directory hierarchies, folder structures, tagging | Focus on reliable upload, not file management |\n| Metadata Management | Custom metadata schemas, search indexing, versioning | Adds complexity without improving upload reliability |\n| Content Transformation | Image resizing, video transcoding, format conversion | Resource-intensive operations outside core scope |\n| User Management | Authentication, authorization, user accounts | Authentication should be handled by external systems |\n| File Sharing | Public links, permission management, collaborative editing | Different security and access patterns than uploads |\n\n#### Advanced Transfer Features\n\nWhile these features might enhance user experience, they introduce significant complexity that would delay core functionality delivery.\n\n| Feature | Why Excluded | Alternative Approach |\n|---------|--------------|---------------------|\n| Parallel Chunk Uploads | Complex ordering and bandwidth management | Single-threaded upload with larger chunks |\n| Bandwidth Throttling | Requires traffic shaping and client negotiation | Handle via reverse proxy or CDN |\n| Upload Acceleration | Needs edge servers and routing optimization | Use cloud provider acceleration features |\n| Compression | CPU overhead and format compatibility issues | Client-side compression before upload |\n| Deduplication | Storage scanning and hash management complexity | Handle at storage backend level |\n\n#### Integration and Deployment Features\n\nThe service focuses on core functionality rather than operational features that can be provided by external infrastructure.\n\n| Feature Category | Specific Non-Goals | External Solution |\n|------------------|-------------------|-------------------|\n| Load Balancing | Multi-instance coordination, session distribution | Use load balancer with session affinity |\n| Monitoring and Metrics | Custom dashboards, alerting, performance analytics | Integrate with existing monitoring systems |\n| Configuration Management | Dynamic reconfiguration, feature flags | Use configuration management tools |\n| Container Orchestration | Service discovery, health checks, scaling policies | Deploy using Kubernetes or similar platforms |\n| Log Management | Log aggregation, search, retention policies | Use centralized logging infrastructure |\n\n> **Decision: Minimal External Dependencies**\n> - **Context**: Services can integrate with many external systems but each integration adds complexity\n> - **Options Considered**:\n>   1. Build comprehensive platform with all operational features\n>   2. Focus on core upload functionality with external integration points\n>   3. Provide plugins for common integrations\n> - **Decision**: Minimal dependencies with clear integration boundaries\n> - **Rationale**: Reduces maintenance burden, improves reliability, enables flexible deployment patterns\n> - **Consequences**: Users must integrate with external systems but service remains focused and maintainable\n\n#### Protocol and Transport Limitations\n\nThe service implements a specific protocol subset rather than attempting to support every possible upload mechanism.\n\n| Protocol Feature | Not Supported | Supported Alternative |\n|------------------|---------------|----------------------|\n| HTTP/1.0 | Legacy protocol without persistent connections | HTTP/1.1 and HTTP/2 |\n| WebSocket Uploads | Real-time bidirectional communication | Standard HTTP POST with progress polling |\n| FTP/SFTP Protocols | Legacy file transfer protocols | tus.io over HTTP only |\n| Custom UDP Protocols | Unreliable transport for file transfer | TCP-based HTTP transport |\n| GraphQL Mutations | Query-based upload operations | REST API with tus.io protocol |\n\nThese limitations ensure the service remains focused on proven, widely-supported protocols while avoiding edge cases that would complicate implementation and testing.\n\n### Common Pitfalls\n\nUnderstanding common mistakes in scope definition helps avoid feature creep and unrealistic expectations.\n\n⚠️ **Pitfall: Assuming All Upload Features Are Related**\nMany developers think that because two features both involve files, they belong in the same service. For example, adding image thumbnailing because \"we're already handling image uploads.\" This leads to monolithic services that are hard to test and maintain. The fix is to strictly separate concerns: upload services handle reliable transfer, processing services handle content transformation.\n\n⚠️ **Pitfall: Underestimating Security Requirements**\nFile upload services are prime targets for attacks, but developers often treat security as an afterthought. Assuming \"we'll add virus scanning later\" or \"input validation is simple\" leads to vulnerable systems. The fix is to design security validation into the core upload pipeline from the beginning, not bolt it on afterward.\n\n⚠️ **Pitfall: Ignoring Storage Backend Differences**\nDifferent storage systems have vastly different capabilities, consistency models, and failure modes. Treating S3 and local filesystem as equivalent leads to bugs when deploying to production. The fix is to design the storage abstraction around the most constrained backend, then add optimizations for more capable systems.\n\n⚠️ **Pitfall: Scope Creep Through \"Simple\" Features**\nFeatures that seem simple often hide significant complexity. Adding \"just a simple progress callback\" can require WebSocket infrastructure, connection management, and real-time scaling. The fix is to evaluate each feature request against the core mission and defer anything that doesn't directly improve upload reliability.\n\n### Implementation Guidance\n\nThis section provides practical direction for translating the goals and requirements into working code, focusing on configuration management and requirement validation.\n\n#### Technology Recommendations\n\n| Requirement Category | Simple Option | Advanced Option |\n|---------------------|---------------|-----------------|\n| Configuration Management | JSON files with validation | Consul/etcd with hot reload |\n| Input Validation | Manual struct validation | JSON Schema with code generation |\n| Security Scanning | ClamAV command-line interface | ClamAV daemon with socket communication |\n| Session Storage | In-memory maps with file backup | Redis with persistence |\n| Metrics Collection | Simple counters in logs | Prometheus metrics with custom collectors |\n\n#### Recommended File Structure\n\n```\nresumable-upload/\n  cmd/\n    server/\n      main.go                    ← entry point, config loading\n  internal/\n    config/\n      config.go                  ← Config structs and loading\n      validation.go              ← requirement validation\n    session/\n      manager.go                 ← SessionManager implementation\n      store.go                   ← StateStore interface and implementations\n    security/\n      limits.go                  ← size and type validation\n      scanner.go                 ← virus scanning integration\n  configs/\n    default.json                 ← default configuration file\n    production.json              ← production overrides\n  tests/\n    integration/\n      requirements_test.go       ← test all functional requirements\n```\n\n#### Configuration Infrastructure Code\n\n```go\npackage config\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"os\"\n    \"time\"\n)\n\n// Config represents the complete service configuration matching our requirements\ntype Config struct {\n    Server   ServerConfig   `json:\"server\"`\n    Storage  StorageConfig  `json:\"storage\"`\n    Security SecurityConfig `json:\"security\"`\n    Cleanup  CleanupConfig  `json:\"cleanup\"`\n}\n\n// ServerConfig defines HTTP server parameters for performance requirements\ntype ServerConfig struct {\n    Host         string        `json:\"host\"`\n    Port         int           `json:\"port\"`\n    ReadTimeout  time.Duration `json:\"read_timeout\"`\n    WriteTimeout time.Duration `json:\"write_timeout\"`\n    MaxBodySize  int64         `json:\"max_body_size\"`\n}\n\n// StorageConfig enables storage backend abstraction requirement\ntype StorageConfig struct {\n    Backend   string            `json:\"backend\"`\n    LocalPath string            `json:\"local_path,omitempty\"`\n    S3Config  *S3Config         `json:\"s3_config,omitempty\"`\n    Options   map[string]string `json:\"options,omitempty\"`\n}\n\n// S3Config provides cloud storage integration parameters\ntype S3Config struct {\n    Bucket          string `json:\"bucket\"`\n    Region          string `json:\"region\"`\n    AccessKeyID     string `json:\"access_key_id\"`\n    SecretAccessKey string `json:\"secret_access_key\"`\n    Endpoint        string `json:\"endpoint,omitempty\"`\n}\n\n// SecurityConfig enforces security constraints and validation requirements\ntype SecurityConfig struct {\n    MaxFileSize         int64    `json:\"max_file_size\"`\n    AllowedContentTypes []string `json:\"allowed_content_types\"`\n    VirusScanEnabled    bool     `json:\"virus_scan_enabled\"`\n    ClamAVSocket        string   `json:\"clamav_socket\"`\n}\n\n// CleanupConfig manages session lifecycle and resource cleanup\ntype CleanupConfig struct {\n    SessionTTL      time.Duration `json:\"session_ttl\"`\n    CleanupInterval time.Duration `json:\"cleanup_interval\"`\n    QuarantineTTL   time.Duration `json:\"quarantine_ttl\"`\n}\n\n// LoadConfig reads configuration from file with defaults matching our requirements\nfunc LoadConfig(filename string) (*Config, error) {\n    // TODO 1: Set default configuration values that meet non-functional requirements\n    // TODO 2: Read JSON configuration file and override defaults\n    // TODO 3: Validate configuration against functional requirements\n    // TODO 4: Return validated configuration or detailed error\n    // Hint: Use json.Unmarshal and implement validation checks for each requirement category\n    \n    config := &Config{\n        Server: ServerConfig{\n            Host:         \"localhost\",\n            Port:         8080,\n            ReadTimeout:  30 * time.Second,\n            WriteTimeout: 30 * time.Second,\n            MaxBodySize:  100 * 1024 * 1024, // 100MB chunks\n        },\n        Security: SecurityConfig{\n            MaxFileSize:         5 * 1024 * 1024 * 1024, // 5GB\n            AllowedContentTypes: []string{\"*/*\"},         // Validate with magic bytes\n            VirusScanEnabled:    true,\n            ClamAVSocket:        \"/var/run/clamav/clamd.ctl\",\n        },\n        Cleanup: CleanupConfig{\n            SessionTTL:      24 * time.Hour,\n            CleanupInterval: time.Hour,\n            QuarantineTTL:   7 * 24 * time.Hour,\n        },\n    }\n    \n    if filename != \"\" {\n        data, err := os.ReadFile(filename)\n        if err != nil {\n            return nil, fmt.Errorf(\"failed to read config file: %w\", err)\n        }\n        \n        if err := json.Unmarshal(data, config); err != nil {\n            return nil, fmt.Errorf(\"failed to parse config: %w\", err)\n        }\n    }\n    \n    return config, nil\n}\n```\n\n#### Requirement Validation Code\n\n```go\npackage config\n\nimport (\n    \"fmt\"\n    \"os\"\n    \"path/filepath\"\n)\n\n// ValidateRequirements checks configuration against functional and non-functional requirements\nfunc (c *Config) ValidateRequirements() error {\n    // TODO 1: Validate performance requirements (timeouts, limits)\n    // TODO 2: Validate security requirements (file size limits, scan configuration)\n    // TODO 3: Validate storage backend requirements (credentials, paths)\n    // TODO 4: Validate reliability requirements (cleanup intervals, persistence)\n    // Hint: Each requirement category needs specific validation logic\n    \n    if err := c.validatePerformanceRequirements(); err != nil {\n        return fmt.Errorf(\"performance requirements not met: %w\", err)\n    }\n    \n    if err := c.validateSecurityRequirements(); err != nil {\n        return fmt.Errorf(\"security requirements not met: %w\", err)\n    }\n    \n    if err := c.validateStorageRequirements(); err != nil {\n        return fmt.Errorf(\"storage requirements not met: %w\", err)\n    }\n    \n    return nil\n}\n\nfunc (c *Config) validatePerformanceRequirements() error {\n    // Check concurrent upload capability requirements\n    if c.Server.MaxBodySize < 1024*1024 {\n        return fmt.Errorf(\"max body size too small for chunked uploads: %d\", c.Server.MaxBodySize)\n    }\n    \n    if c.Server.ReadTimeout < time.Second {\n        return fmt.Errorf(\"read timeout too aggressive for large chunks: %v\", c.Server.ReadTimeout)\n    }\n    \n    return nil\n}\n\nfunc (c *Config) validateSecurityRequirements() error {\n    // Validate virus scanning capability if enabled\n    if c.Security.VirusScanEnabled {\n        if c.Security.ClamAVSocket == \"\" {\n            return fmt.Errorf(\"virus scanning enabled but no ClamAV socket configured\")\n        }\n        \n        // Check if ClamAV socket exists\n        if _, err := os.Stat(c.Security.ClamAVSocket); os.IsNotExist(err) {\n            return fmt.Errorf(\"ClamAV socket not found: %s\", c.Security.ClamAVSocket)\n        }\n    }\n    \n    // Validate file size limits\n    if c.Security.MaxFileSize <= 0 {\n        return fmt.Errorf(\"max file size must be positive: %d\", c.Security.MaxFileSize)\n    }\n    \n    return nil\n}\n\nfunc (c *Config) validateStorageRequirements() error {\n    // Validate storage backend configuration\n    switch c.Storage.Backend {\n    case \"local\":\n        if c.Storage.LocalPath == \"\" {\n            return fmt.Errorf(\"local storage requires local_path configuration\")\n        }\n        \n        // Ensure local path exists and is writable\n        if err := os.MkdirAll(c.Storage.LocalPath, 0755); err != nil {\n            return fmt.Errorf(\"cannot create local storage path: %w\", err)\n        }\n        \n        testFile := filepath.Join(c.Storage.LocalPath, \".write_test\")\n        if err := os.WriteFile(testFile, []byte(\"test\"), 0644); err != nil {\n            return fmt.Errorf(\"local storage path not writable: %w\", err)\n        }\n        os.Remove(testFile)\n        \n    case \"s3\":\n        if c.Storage.S3Config == nil {\n            return fmt.Errorf(\"S3 storage requires s3_config\")\n        }\n        \n        s3 := c.Storage.S3Config\n        if s3.Bucket == \"\" || s3.Region == \"\" {\n            return fmt.Errorf(\"S3 requires bucket and region configuration\")\n        }\n        \n    default:\n        return fmt.Errorf(\"unsupported storage backend: %s\", c.Storage.Backend)\n    }\n    \n    return nil\n}\n```\n\n#### Language-Specific Implementation Hints\n\n**Configuration Management:**\n- Use `encoding/json` for configuration files with struct tags for field mapping\n- Use `time.Duration` for timeout values to get automatic parsing of \"30s\", \"5m\" format\n- Use `os.Getenv()` to override configuration with environment variables for containerized deployments\n- Use `filepath.Clean()` for all path validation to prevent directory traversal attacks\n\n**Requirement Validation:**\n- Implement validation as methods on the Config struct for clean organization\n- Return wrapped errors using `fmt.Errorf(\"context: %w\", err)` for error chain tracing\n- Use early returns in validation functions to fail fast on first requirement violation\n- Test file system permissions during validation to catch deployment issues early\n\n**Security Implementation:**\n- Never trust file extensions for type detection - always use magic byte validation\n- Use `os.Stat()` to verify external dependencies (ClamAV socket) exist before starting service\n- Implement timeout wrappers around virus scanning to prevent indefinite blocking\n- Use `filepath.Join()` and validate all paths to prevent directory traversal\n\n#### Milestone Validation Checkpoints\n\n**Milestone 1 Checkpoint - Configuration Loading:**\n```bash\ngo run cmd/server/main.go -config configs/default.json\n# Expected: Server starts successfully with default configuration\n# Check: All functional requirements validated without errors\n```\n\n**Milestone 2 Checkpoint - Storage Backend Validation:**\n```bash\n# Test local storage\nmkdir -p /tmp/upload-test\nCONFIG='{\"storage\":{\"backend\":\"local\",\"local_path\":\"/tmp/upload-test\"}}' go run cmd/server/main.go\n# Expected: Local storage validation passes, directory created\n\n# Test S3 storage (with valid credentials)\ngo run cmd/server/main.go -config configs/s3-production.json\n# Expected: S3 configuration validated, bucket accessible\n```\n\n**Milestone 3 Checkpoint - Security Requirements:**\n```bash\n# Test with ClamAV disabled\nCONFIG='{\"security\":{\"virus_scan_enabled\":false}}' go run cmd/server/main.go\n# Expected: Service starts without virus scanning requirements\n\n# Test with invalid file size limit\nCONFIG='{\"security\":{\"max_file_size\":-1}}' go run cmd/server/main.go\n# Expected: Validation error for negative file size limit\n```\n\n**Integration Test - Full Requirements Validation:**\nCreate a test that validates all functional requirements are properly configured:\n1. Start service with complete configuration file\n2. Verify performance limits are enforced (connection timeouts, body size limits)\n3. Verify security validation works (file type checking, size limits)\n4. Verify storage backend connectivity for configured backend type\n5. Verify cleanup processes can access session storage for TTL enforcement\n\n\n## High-Level Architecture\n\n> **Milestone(s):** Milestones 1, 2, and 3 — the architectural foundation supports chunked upload protocol implementation, storage backend abstraction, and virus scanning integration\n\nThink of our resumable file upload service as a **coordinated assembly line** where different stations specialize in specific tasks. Just as an automotive assembly line has stations for chassis assembly, engine installation, and quality control — each with its own expertise and tools — our service has specialized components that work together to transform incoming file chunks into securely stored, validated files. The key insight is that each component maintains its own state and responsibilities, but they must coordinate seamlessly to handle the complex choreography of resumable uploads.\n\nThe architecture follows a **layered responsibility model** where each component has a clear domain of expertise. The Upload Manager acts as the orchestrator, understanding the tus.io protocol and managing upload sessions. The Storage Abstraction provides a uniform interface regardless of whether files are stored locally, in S3, or other backends. The Virus Scanner ensures security by validating files before they reach permanent storage. The Session Store maintains the critical state that allows uploads to survive network failures and service restarts.\n\nThis design philosophy prioritizes **separation of concerns** and **fault isolation**. When the virus scanner is overwhelmed, it doesn't affect chunk reception. When a storage backend experiences issues, the upload session state remains intact for retry operations. When network connections drop, the precise offset tracking allows clients to resume exactly where they left off without re-uploading already received data.\n\n![System Architecture Overview](./diagrams/system-architecture.svg)\n\n### Component Responsibilities\n\nThe architecture divides responsibilities across four core components, each with distinct ownership boundaries and failure domains. Understanding these boundaries is crucial because they determine how the system handles partial failures and maintains consistency during complex multi-step operations.\n\n#### Upload Manager: Protocol Orchestration and Session Lifecycle\n\nThe **Upload Manager** serves as the primary orchestrator, implementing the tus.io resumable upload protocol and managing the complete lifecycle of upload sessions. Think of it as a skilled foreman who understands the overall assembly process, coordinates between specialized workers, and ensures that each step happens in the correct sequence with proper error handling.\n\nThe Upload Manager owns the **protocol state machine**, tracking each upload session through its lifecycle from initialization to completion. It translates HTTP requests following the tus.io specification into internal operations across other components. When a client sends a PATCH request with a chunk of data, the Upload Manager validates the request headers, updates the session state, coordinates with storage for chunk persistence, and returns appropriate HTTP responses that allow the client to understand the current upload status.\n\n| Responsibility Area | Specific Duties | Owned Data | External Dependencies |\n|-------------------|-----------------|------------|----------------------|\n| Protocol Implementation | tus.io HTTP method handling, header validation, response formatting | Request/response parsing state | SessionManager for persistence |\n| Session Orchestration | Upload initialization, progress tracking, completion detection | Upload workflow state | StorageBackend for chunk operations |\n| Offset Management | Byte-level progress calculation, resume point determination | Current upload position | Virus scanner for validation triggers |\n| Error Response | HTTP status code generation, retry guidance, client communication | Error context and recovery hints | All components for failure propagation |\n\nThe Upload Manager implements **precise offset tracking** by maintaining byte-level accuracy of upload progress. When a client requests the current upload status via HEAD request, the manager calculates the exact offset by querying the storage backend for the current file size and cross-referencing it with the session metadata. This precision is critical because even a one-byte discrepancy will cause upload resumption to fail or corrupt the final file.\n\n> **Design Insight**: The Upload Manager never directly manipulates file content beyond basic chunk validation. It delegates all storage operations to the storage backend and all security validation to the virus scanner. This separation ensures that protocol logic remains isolated from storage implementation details and security policies.\n\n**State Coordination Complexity**: The Upload Manager must handle the complex choreography when multiple components are involved in a single operation. For example, when completing an upload, it must coordinate storage finalization, virus scanning initiation, and session state updates as an atomic operation. If virus scanning fails, the manager must revert the session to an active state and preserve the uploaded chunks for retry operations.\n\n#### Session Store: State Persistence and Consistency\n\nThe **Session Store** acts as the system's memory, persisting critical upload state that must survive service restarts, network failures, and other disruptions. Think of it as a detailed flight recorder that captures every significant event in an upload's lifecycle, enabling precise reconstruction of system state after any failure scenario.\n\nThe Session Store owns **upload session metadata**, including the session identifier, file information, current upload progress, chunk completion tracking, and session status. It provides atomic operations for session state transitions, ensuring that the system never enters an inconsistent state where session metadata contradicts the actual stored file chunks.\n\n| Operation Category | Methods | Consistency Guarantees | Failure Handling |\n|------------------|---------|----------------------|-----------------|\n| Session Lifecycle | `CreateSession`, `GetSession`, `DeleteSession` | Atomic creation with unique ID generation | Duplicate ID detection and retry |\n| Progress Tracking | `UpdateSession` with offset and status changes | Monotonic offset progression validation | Rollback to last consistent offset |\n| Expiration Management | `ListExpiredSessions` for cleanup operations | TTL-based consistency with grace periods | Cleanup coordination with active operations |\n| Query Operations | Session lookup by ID, status filtering | Read consistency with concurrent writes | Stale read detection and refresh |\n\nThe Session Store implements **optimistic concurrency control** to handle concurrent access patterns. When multiple requests attempt to update the same upload session simultaneously (rare but possible with aggressive retry clients), the store uses version numbers or timestamps to detect conflicts and reject conflicting updates with appropriate error responses.\n\n**Persistence Strategy**: The Session Store must choose between consistency and availability during storage backend failures. The architecture prioritizes consistency — if session state cannot be reliably persisted, the system rejects new uploads rather than risk state loss that could corrupt resumable uploads.\n\n> **Critical Insight**: Session Store consistency is more important than individual chunk storage because clients can re-upload chunks, but lost session state requires starting the entire upload from the beginning.\n\n#### Storage Backend Abstraction: Unified File Operations\n\nThe **Storage Abstraction** provides a uniform interface for file operations across different storage backends, hiding the complexity of local filesystem operations, S3 multipart uploads, and other cloud storage APIs. Think of it as a universal translator that speaks the native language of each storage system but presents a consistent interface to the rest of the application.\n\nThe Storage Abstraction owns **file content persistence** and **storage-specific optimization**. It handles the translation between generic storage operations (write chunk at offset, read file range, generate download URL) and backend-specific APIs (S3 UploadPart, local filesystem seek/write operations, GCS resumable uploads).\n\n| Backend Type | Implementation Challenges | Optimization Strategies | Failure Recovery |\n|-------------|-------------------------|----------------------|-----------------|\n| Local Filesystem | Path traversal security, atomic operations, disk space management | Pre-allocation, direct I/O, sparse file handling | Temp file cleanup, corruption detection |\n| S3-Compatible | Multipart upload coordination, eventual consistency, credential rotation | Parallel part uploads, exponential backoff, connection pooling | Part retry, upload ID tracking |\n| Google Cloud Storage | Resumable upload protocol, quota management, regional distribution | Chunk size optimization, compression negotiation | Upload session recovery |\n\nThe Storage Abstraction implements **backend-specific optimizations** while maintaining interface consistency. For S3 backends, it automatically manages multipart upload sessions, ensuring that parts meet the minimum 5MB size requirement (except for the final part) and handles the complex part ordering and ETag tracking required for successful multipart completion.\n\n**Credential and Configuration Management**: The abstraction layer securely manages authentication credentials for cloud backends, supporting credential rotation and multiple backend configurations simultaneously. It implements connection pooling and retry logic specific to each backend's characteristics and rate limiting behavior.\n\n> **Decision: Storage Interface Granularity**\n> - **Context**: Need to balance interface simplicity with backend-specific optimization opportunities\n> - **Options Considered**: \n>   1. Minimal interface (read, write, delete only)\n>   2. Rich interface (streaming, parallel operations, metadata)\n>   3. Backend-specific interfaces with adapter pattern\n> - **Decision**: Rich interface with optional capabilities discovery\n> - **Rationale**: Enables backend-specific optimizations while maintaining consistent client code\n> - **Consequences**: More complex interface but significantly better performance for cloud backends\n\n#### Virus Scanner: Security Validation Pipeline\n\nThe **Virus Scanner** implements the security validation pipeline, protecting the system from malicious content through file type validation, virus scanning, and quarantine management. Think of it as a security checkpoint that thoroughly inspects each completed upload before granting it access to the main storage system.\n\nThe Virus Scanner owns **content validation logic** and **quarantine policy enforcement**. It validates file types using magic byte detection rather than trusting client-provided MIME types or file extensions. It integrates with antivirus engines like ClamAV to perform signature-based malware detection. It manages quarantine storage for suspicious files and implements retention policies for security compliance.\n\n| Validation Stage | Techniques | Detection Capabilities | Policy Actions |\n|-----------------|-----------|----------------------|---------------|\n| File Type Detection | Magic byte analysis, header parsing | Format spoofing, extension mismatch | Type-based rejection, forced MIME correction |\n| Size Validation | Content length verification | Decompression bombs, oversized uploads | Hard limits, graduated warnings |\n| Virus Scanning | Signature matching, heuristic analysis | Known malware, suspicious patterns | Quarantine, notification, deletion |\n| Content Analysis | Metadata extraction, embedded content scanning | Hidden executables, macro viruses | Deep inspection, selective blocking |\n\nThe Virus Scanner implements **asynchronous processing** to avoid blocking the upload completion flow. When an upload finishes, the scanner initiates background validation while marking the file as \"pending validation\" in the session state. Clients can query the validation status separately from the upload completion status.\n\n**Quarantine Management**: Suspicious files are moved to isolated quarantine storage rather than deleted immediately. This preserves evidence for security analysis and allows for false positive recovery. The quarantine system implements automatic expiration based on configurable retention policies.\n\n> **Design Constraint**: The Virus Scanner never modifies the original uploaded content. It operates on read-only copies and makes binary decisions (approve, quarantine, or reject) that other components can act upon.\n\n### Request Flow Patterns\n\nUnderstanding the request flow patterns reveals how components coordinate during different phases of the upload lifecycle. Each flow pattern has specific failure points, consistency requirements, and performance characteristics that influence the overall system design.\n\n#### Upload Initialization Flow\n\nThe **upload initialization flow** establishes a new upload session and prepares all components for chunk reception. Think of this as setting up the assembly line for a new product — every station must be configured and ready before production can begin.\n\nThe flow begins when a client sends a POST request to initiate a resumable upload, following the tus.io protocol specification. The request includes file metadata (filename, size, MIME type) and any custom metadata that the application requires for processing.\n\n1. **Request Validation**: The Upload Manager validates the incoming request headers and metadata, checking file size limits, content type restrictions, and authentication credentials. Invalid requests are rejected immediately with appropriate HTTP error responses.\n\n2. **Session Creation**: The Upload Manager generates a unique session identifier and creates an `UploadSession` record with initial state `SessionStatusInitialized`. The session includes file metadata, storage configuration, and timing information for expiration management.\n\n3. **Storage Preparation**: The Storage Abstraction receives the session information and prepares the backend for chunk reception. For local storage, this may involve creating directory structures. For S3 backends, this initiates a multipart upload and returns the upload ID for subsequent part uploads.\n\n4. **State Persistence**: The Session Store atomically persists the new session record, ensuring that the session state is durable before returning success to the client. This prevents scenarios where clients receive success responses but the system has no record of the upload session.\n\n5. **Response Generation**: The Upload Manager constructs a tus.io-compliant response including the `Upload-Offset` header (initially 0) and the `Location` header pointing to the upload URL for subsequent PATCH requests.\n\n| Flow Stage | Component Actions | Failure Handling | State Changes |\n|-----------|------------------|------------------|--------------|\n| Request Validation | Upload Manager: header parsing, limit checks | Immediate HTTP error response | No state changes |\n| Session Creation | Upload Manager: ID generation, metadata processing | Session cleanup on downstream failures | Temporary session state |\n| Storage Preparation | Storage Backend: resource allocation, upload initialization | Storage cleanup, session removal | Backend resources allocated |\n| State Persistence | Session Store: atomic session write | Transaction rollback, resource cleanup | Durable session creation |\n| Response Generation | Upload Manager: HTTP response formatting | Error logging, client notification | Session becomes active |\n\n**Consistency Considerations**: The initialization flow must handle partial failures gracefully. If storage preparation succeeds but session persistence fails, the system must clean up allocated storage resources to prevent resource leaks. The Upload Manager implements compensation logic to unwind partial initialization on any failure.\n\n#### Chunk Upload Processing Pipeline\n\nThe **chunk upload processing pipeline** handles individual PATCH requests containing file chunks and represents the core data flow through the system. This pipeline must be highly optimized for throughput while maintaining precise state consistency for resumability.\n\nThink of chunk processing as a high-speed conveyor belt where each chunk must be inspected, validated, positioned correctly, and permanently secured before the conveyor moves to the next position. Any failure in this pipeline must leave the system in a state where the client can determine exactly where to resume.\n\n1. **Request Reception**: The Upload Manager receives a PATCH request containing chunk data, validates the HTTP headers including `Upload-Offset` and `Content-Length`, and performs basic request sanitization.\n\n2. **Session Validation**: The system retrieves the upload session from the Session Store and validates that the session is in the correct state (`SessionStatusActive`) and that the provided offset matches the expected next position.\n\n3. **Offset Verification**: The Upload Manager verifies that the client's stated offset matches the server's recorded position. Offset mismatches trigger HTTP 409 Conflict responses with the correct offset, allowing clients to realign their upload position.\n\n4. **Chunk Storage**: The Storage Abstraction writes the chunk data to the appropriate position in the target file, using backend-specific optimizations for performance and consistency. Local storage uses positioned writes, while S3 backends use multipart upload parts.\n\n5. **Session Update**: The Session Store atomically updates the session record with the new offset position, chunk completion status, and timing information. This update must be durable before responding to the client.\n\n6. **Progress Response**: The Upload Manager returns an HTTP 204 No Content response with updated `Upload-Offset` header, confirming the successful chunk reception and providing the client with the exact position for the next chunk.\n\n| Pipeline Stage | Performance Optimizations | Consistency Guarantees | Error Recovery |\n|---------------|-------------------------|----------------------|----------------|\n| Request Reception | Streaming body parsing, memory pooling | Header validation atomicity | Request replay detection |\n| Session Validation | Session caching, read optimization | Consistent session state reads | Stale session refresh |\n| Offset Verification | Cached offset tracking | Monotonic progression checks | Offset correction responses |\n| Chunk Storage | Parallel writes, compression | Atomic position updates | Partial write rollback |\n| Session Update | Batched updates, write optimization | Durable offset persistence | Update retry on failure |\n| Progress Response | Response streaming, header caching | Acknowledged progress guarantees | Client retry guidance |\n\n**Performance vs. Consistency Tradeoffs**: The chunk processing pipeline must balance high throughput with strict consistency requirements. The system can optimize for performance by batching session updates or caching session state, but these optimizations must never compromise the ability to provide accurate resume positions to clients.\n\n#### Upload Completion and Validation Workflow\n\nThe **upload completion workflow** triggers when the final chunk has been uploaded and coordinates the complex process of file assembly, validation, virus scanning, and final storage commitment. This represents the most complex coordination challenge in the system because it involves all components and must handle partial failures carefully.\n\nThink of upload completion as the final quality control and packaging stage of the assembly line. All components must work together to verify that the assembled product meets quality standards, passes security inspections, and gets properly packaged for delivery.\n\n1. **Completion Detection**: The Upload Manager detects that the upload is complete by comparing the current offset with the declared total file size from the upload session metadata.\n\n2. **File Assembly**: For storage backends that require explicit assembly (like S3 multipart uploads), the Storage Abstraction initiates the completion process, combining all uploaded parts into the final file object.\n\n3. **Session Transition**: The Session Store atomically updates the session status to `SessionStatusCompleting` to prevent concurrent modifications and indicate that final processing is underway.\n\n4. **Validation Initiation**: The Virus Scanner begins asynchronous validation of the completed file, including file type verification and antivirus scanning. The file remains in a pending state during this process.\n\n5. **Final Storage**: Once validation completes successfully, the Storage Abstraction moves the file to its final storage location and generates any required access URLs or metadata for client access.\n\n6. **Session Finalization**: The Session Store updates the session status to `SessionStatusCompleted` and records final metadata including file location, validation results, and completion timestamp.\n\n7. **Client Notification**: The Upload Manager provides completion notification through the standard tus.io HEAD or PATCH response, indicating successful upload and any additional metadata about the stored file.\n\n| Workflow Stage | Component Coordination | Failure Recovery | State Consistency |\n|---------------|----------------------|------------------|------------------|\n| Completion Detection | Upload Manager calculates final status | None required - detection retry | Read-only operation |\n| File Assembly | Storage Backend finalizes file structure | Assembly rollback to chunk state | Atomic assembly commitment |\n| Session Transition | Session Store prevents concurrent access | Status rollback on downstream failure | Transactional state update |\n| Validation Initiation | Virus Scanner begins async processing | Validation retry with exponential backoff | Validation state tracking |\n| Final Storage | Storage Backend commits permanent location | Storage rollback, cleanup procedures | Durable file commitment |\n| Session Finalization | Session Store records completion metadata | Metadata correction on inconsistency | Final state persistence |\n| Client Notification | Upload Manager provides completion status | Status query retry, eventual consistency | Client state synchronization |\n\n> **Critical Coordination Point**: The transition from `SessionStatusCompleting` to `SessionStatusCompleted` must be atomic with the final storage commitment. If this coordination fails, the system enters a state where the file exists in storage but the session indicates incomplete upload, requiring manual recovery procedures.\n\n### Recommended Module Structure\n\nThe module structure organizes the codebase to reflect the architectural boundaries and responsibilities, making it easier for developers to locate relevant code and understand component interactions. The structure follows Go package conventions while emphasizing clear separation of concerns and testability.\n\nThink of the module structure as the **architectural blueprint** that guides developers to the right location for any given functionality. Just as a well-designed building has clear zones for different activities, our codebase should have clear zones for different responsibilities with well-defined interfaces between them.\n\nThe package organization prioritizes **dependency clarity** — higher-level packages depend on lower-level packages, never the reverse. This prevents circular dependencies and makes the codebase easier to test and refactor. Each package has a single, clear responsibility that maps directly to our architectural components.\n\n```\nresumable-upload-service/\n├── cmd/\n│   ├── server/\n│   │   └── main.go                    ← Service entry point, configuration loading\n│   └── cleanup/\n│       └── main.go                    ← Cleanup utility for expired sessions\n├── internal/\n│   ├── config/\n│   │   ├── config.go                  ← Configuration structures and validation\n│   │   └── config_test.go             ← Configuration loading tests\n│   ├── upload/\n│   │   ├── manager.go                 ← Upload Manager implementation\n│   │   ├── manager_test.go            ← Upload protocol tests\n│   │   ├── session.go                 ← Session lifecycle management\n│   │   └── handlers.go                ← HTTP request handlers for tus.io\n│   ├── storage/\n│   │   ├── interface.go               ← Storage abstraction interface\n│   │   ├── local/\n│   │   │   ├── storage.go             ← Local filesystem backend\n│   │   │   └── storage_test.go        ← Local storage tests\n│   │   ├── s3/\n│   │   │   ├── storage.go             ← S3-compatible backend\n│   │   │   ├── multipart.go           ← S3 multipart upload coordination\n│   │   │   └── storage_test.go        ← S3 backend tests\n│   │   └── factory.go                 ← Storage backend factory\n│   ├── scanner/\n│   │   ├── validator.go               ← File validation and virus scanning\n│   │   ├── clamav.go                  ← ClamAV integration\n│   │   ├── quarantine.go              ← Quarantine management\n│   │   └── scanner_test.go            ← Security validation tests\n│   ├── session/\n│   │   ├── store.go                   ← Session Store interface\n│   │   ├── memory.go                  ← In-memory store for testing\n│   │   ├── redis.go                   ← Redis-backed session store\n│   │   └── store_test.go              ← Session persistence tests\n│   └── protocol/\n│       ├── tus.go                     ← tus.io protocol implementation\n│       ├── headers.go                 ← HTTP header parsing and validation\n│       └── protocol_test.go           ← Protocol compliance tests\n├── pkg/\n│   ├── errors/\n│   │   └── errors.go                  ← Common error types and handling\n│   └── middleware/\n│       ├── logging.go                 ← Request logging middleware\n│       ├── auth.go                    ← Authentication middleware\n│       └── cors.go                    ← CORS configuration\n├── test/\n│   ├── integration/\n│   │   ├── upload_test.go             ← End-to-end upload scenarios\n│   │   └── storage_test.go            ← Storage backend integration\n│   └── fixtures/\n│       ├── config.json                ← Test configuration files\n│       └── test_files/                ← Sample files for testing\n├── docs/\n│   ├── api.md                         ← API documentation\n│   ├── deployment.md                  ← Deployment instructions\n│   └── diagrams/                      ← Architecture diagrams\n├── scripts/\n│   ├── setup.sh                       ← Development environment setup\n│   └── docker/\n│       ├── Dockerfile                 ← Container image definition\n│       └── docker-compose.yml         ← Multi-service development setup\n├── go.mod                             ← Go module definition\n├── go.sum                             ← Dependency checksums\n└── README.md                          ← Project overview and quick start\n```\n\n#### Package Responsibility Matrix\n\nEach package has clearly defined responsibilities and dependency relationships that reflect the architectural boundaries discussed earlier.\n\n| Package | Primary Responsibility | Key Types | External Dependencies | Internal Dependencies |\n|---------|----------------------|-----------|---------------------|---------------------|\n| `cmd/server` | Service bootstrap and configuration | `main()`, server setup | HTTP server, signal handling | All internal packages |\n| `internal/config` | Configuration management | `Config`, `ServerConfig`, `StorageConfig` | JSON parsing, validation | None (leaf package) |\n| `internal/upload` | Upload orchestration and tus.io protocol | `Manager`, `UploadSession` | HTTP handling | `session`, `storage`, `protocol` |\n| `internal/storage` | Storage backend abstraction | `StorageBackend` interface, implementations | Cloud SDKs, filesystem | `config` for backend selection |\n| `internal/scanner` | Security validation pipeline | `Validator`, `ScanResult` | ClamAV, file type detection | `storage` for quarantine |\n| `internal/session` | Upload state persistence | `SessionStore` interface, implementations | Database/cache clients | `config` for connection settings |\n| `internal/protocol` | tus.io protocol implementation | HTTP header parsing, protocol validation | HTTP request/response | `pkg/errors` for standardized errors |\n\n#### Dependency Flow and Testing Strategy\n\nThe module structure enforces a clear dependency flow that supports comprehensive testing at each layer. Lower-level packages have minimal dependencies and can be unit tested in isolation, while higher-level packages can be tested with mock implementations of their dependencies.\n\n**Testing Isolation Boundaries**: Each package includes comprehensive unit tests that use interface-based dependency injection to substitute mock implementations. For example, the `internal/upload` package tests use a mock `SessionStore` and mock `StorageBackend` to test upload logic without requiring actual storage or session persistence.\n\n**Integration Testing Strategy**: The `test/integration` package contains end-to-end tests that exercise complete upload flows with real implementations of all components. These tests use Docker containers for external dependencies like ClamAV and Redis, ensuring that integration tests run consistently across development environments.\n\n> **Decision: Internal vs. Pkg Package Organization**\n> - **Context**: Need to balance code reusability with encapsulation of implementation details\n> - **Options Considered**:\n>   1. Everything in `pkg/` for maximum reusability\n>   2. Everything in `internal/` for maximum encapsulation\n>   3. Mixed approach with clear criteria\n> - **Decision**: Core logic in `internal/`, stable utilities in `pkg/`\n> - **Rationale**: Prevents premature API commitments while allowing shared utilities\n> - **Consequences**: Clear API boundaries but requires careful consideration of what to expose\n\n**Configuration Propagation Pattern**: Configuration flows from the `cmd/server` package through dependency injection rather than global variables. Each component receives only the configuration sections it needs, reducing coupling and improving testability. The `internal/config` package provides validation methods that ensure configuration consistency across all components.\n\n**Error Handling Consistency**: The `pkg/errors` package defines standard error types and handling patterns used throughout the codebase. This ensures consistent error responses and simplifies client error handling. Each component wraps internal errors with context-specific information while preserving the underlying error classification.\n\n### Implementation Guidance\n\nThis section provides practical code implementation details for building the high-level architecture, focusing on the core interfaces and starter implementations that enable rapid development progress.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option | Rationale |\n|-----------|--------------|----------------|-----------|\n| HTTP Server | `net/http` with `gorilla/mux` router | `gin-gonic/gin` with middleware | Standard library provides sufficient functionality for tus.io |\n| Configuration | JSON files with `encoding/json` | `viper` with multiple format support | JSON provides clear structure for component configuration |\n| Session Storage | In-memory map with `sync.RWMutex` | Redis with `go-redis/redis` client | In-memory sufficient for single-instance development |\n| Logging | Standard `log` package | `sirupsen/logrus` with structured logging | Structured logging essential for debugging upload flows |\n| Storage Backends | Local filesystem with `os` package | AWS SDK v2 with connection pooling | Local storage enables development without cloud dependencies |\n| Virus Scanning | File type checking with magic bytes | Full ClamAV integration via socket | Magic byte validation covers most security requirements |\n\n#### Core Configuration Structure\n\nThe configuration system must support multiple storage backends, security policies, and deployment environments while providing sensible defaults for development use.\n\n```go\n// internal/config/config.go\npackage config\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"os\"\n    \"path/filepath\"\n    \"time\"\n)\n\n// Config represents the complete application configuration\ntype Config struct {\n    Server   ServerConfig   `json:\"server\"`\n    Storage  StorageConfig  `json:\"storage\"`\n    Security SecurityConfig `json:\"security\"`\n    Cleanup  CleanupConfig  `json:\"cleanup\"`\n}\n\n// ServerConfig contains HTTP server configuration\ntype ServerConfig struct {\n    Host         string        `json:\"host\"`\n    Port         int           `json:\"port\"`\n    ReadTimeout  time.Duration `json:\"read_timeout\"`\n    WriteTimeout time.Duration `json:\"write_timeout\"`\n    MaxBodySize  int64         `json:\"max_body_size\"`\n}\n\n// StorageConfig defines storage backend configuration\ntype StorageConfig struct {\n    Backend   string            `json:\"backend\"`\n    LocalPath string            `json:\"local_path,omitempty\"`\n    S3Config  *S3Config         `json:\"s3_config,omitempty\"`\n    Options   map[string]string `json:\"options\"`\n}\n\n// S3Config contains AWS S3 specific configuration\ntype S3Config struct {\n    Bucket          string `json:\"bucket\"`\n    Region          string `json:\"region\"`\n    AccessKeyID     string `json:\"access_key_id\"`\n    SecretAccessKey string `json:\"secret_access_key\"`\n    Endpoint        string `json:\"endpoint,omitempty\"`\n}\n\n// SecurityConfig defines validation and scanning policies\ntype SecurityConfig struct {\n    MaxFileSize         int64    `json:\"max_file_size\"`\n    AllowedContentTypes []string `json:\"allowed_content_types\"`\n    VirusScanEnabled    bool     `json:\"virus_scan_enabled\"`\n    ClamAVSocket        string   `json:\"clamav_socket\"`\n}\n\n// CleanupConfig contains session and file cleanup policies\ntype CleanupConfig struct {\n    SessionTTL      time.Duration `json:\"session_ttl\"`\n    CleanupInterval time.Duration `json:\"cleanup_interval\"`\n    QuarantineTTL   time.Duration `json:\"quarantine_ttl\"`\n}\n\n// LoadConfig reads configuration from JSON file with defaults\nfunc LoadConfig(filename string) (*Config, error) {\n    config := &Config{\n        Server: ServerConfig{\n            Host:         \"localhost\",\n            Port:         8080,\n            ReadTimeout:  30 * time.Second,\n            WriteTimeout: 30 * time.Second,\n            MaxBodySize:  100 * 1024 * 1024, // 100MB chunks\n        },\n        Storage: StorageConfig{\n            Backend: \"local\",\n            LocalPath: \"./uploads\",\n            Options: make(map[string]string),\n        },\n        Security: SecurityConfig{\n            MaxFileSize: 5 * 1024 * 1024 * 1024, // 5GB\n            AllowedContentTypes: []string{\"*/*\"},\n            VirusScanEnabled: false,\n            ClamAVSocket: \"/tmp/clamd.socket\",\n        },\n        Cleanup: CleanupConfig{\n            SessionTTL:      24 * time.Hour,\n            CleanupInterval: 1 * time.Hour,\n            QuarantineTTL:   7 * 24 * time.Hour,\n        },\n    }\n\n    if filename != \"\" {\n        data, err := os.ReadFile(filename)\n        if err != nil {\n            return nil, fmt.Errorf(\"failed to read config file: %w\", err)\n        }\n\n        if err := json.Unmarshal(data, config); err != nil {\n            return nil, fmt.Errorf(\"failed to parse config: %w\", err)\n        }\n    }\n\n    return config, config.ValidateRequirements()\n}\n\n// ValidateRequirements checks configuration against functional requirements\nfunc (c *Config) ValidateRequirements() error {\n    // TODO: Validate server configuration for reasonable timeout values\n    // TODO: Validate storage configuration for required backend parameters\n    // TODO: Validate security configuration for file size and type constraints\n    // TODO: Validate cleanup configuration for reasonable TTL values\n    // TODO: Check storage backend accessibility (file paths, S3 credentials)\n    return nil\n}\n```\n\n#### Session Management Core Interface\n\nThe session management interface provides the persistence layer for upload state, designed to support both in-memory development and production persistence backends.\n\n```go\n// internal/session/store.go\npackage session\n\nimport (\n    \"context\"\n    \"time\"\n)\n\n// SessionStatus represents the current state of an upload session\ntype SessionStatus string\n\nconst (\n    SessionStatusInitialized SessionStatus = \"initialized\"\n    SessionStatusActive      SessionStatus = \"active\"\n    SessionStatusCompleting  SessionStatus = \"completing\"\n    SessionStatusCompleted   SessionStatus = \"completed\"\n    SessionStatusFailed      SessionStatus = \"failed\"\n    SessionStatusExpired     SessionStatus = \"expired\"\n)\n\n// UploadSession represents the complete state of a resumable upload\ntype UploadSession struct {\n    ID           string            `json:\"id\"`\n    Filename     string            `json:\"filename\"`\n    ContentType  string            `json:\"content_type\"`\n    TotalSize    int64             `json:\"total_size\"`\n    CurrentOffset int64            `json:\"current_offset\"`\n    Status       SessionStatus     `json:\"status\"`\n    StorageKey   string            `json:\"storage_key\"`\n    Metadata     map[string]string `json:\"metadata\"`\n    CreatedAt    time.Time         `json:\"created_at\"`\n    UpdatedAt    time.Time         `json:\"updated_at\"`\n    ChunkHashes  map[int64]string  `json:\"chunk_hashes\"`\n}\n\n// SessionStore defines the interface for upload session persistence\ntype SessionStore interface {\n    CreateSession(ctx context.Context, session *UploadSession) error\n    GetSession(ctx context.Context, sessionID string) (*UploadSession, error)\n    UpdateSession(ctx context.Context, session *UploadSession) error\n    DeleteSession(ctx context.Context, sessionID string) error\n    ListExpiredSessions(ctx context.Context, ttl time.Duration) ([]string, error)\n}\n\nvar ErrSessionNotFound = fmt.Errorf(\"session not found\")\n```\n\n#### In-Memory Session Store Implementation\n\nThis provides a complete, thread-safe session store for development and testing purposes.\n\n```go\n// internal/session/memory.go\npackage session\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"sync\"\n    \"time\"\n)\n\n// MemoryStore implements SessionStore using in-memory storage\ntype MemoryStore struct {\n    sessions map[string]*UploadSession\n    mutex    sync.RWMutex\n}\n\n// NewMemoryStore creates a new in-memory session store\nfunc NewMemoryStore() *MemoryStore {\n    return &MemoryStore{\n        sessions: make(map[string]*UploadSession),\n    }\n}\n\n// CreateSession stores a new upload session\nfunc (s *MemoryStore) CreateSession(ctx context.Context, session *UploadSession) error {\n    s.mutex.Lock()\n    defer s.mutex.Unlock()\n    \n    if _, exists := s.sessions[session.ID]; exists {\n        return fmt.Errorf(\"session %s already exists\", session.ID)\n    }\n    \n    // Create a copy to avoid external modifications\n    sessionCopy := *session\n    sessionCopy.CreatedAt = time.Now()\n    sessionCopy.UpdatedAt = sessionCopy.CreatedAt\n    \n    s.sessions[session.ID] = &sessionCopy\n    return nil\n}\n\n// GetSession retrieves an upload session by ID\nfunc (s *MemoryStore) GetSession(ctx context.Context, sessionID string) (*UploadSession, error) {\n    s.mutex.RLock()\n    defer s.mutex.RUnlock()\n    \n    session, exists := s.sessions[sessionID]\n    if !exists {\n        return nil, ErrSessionNotFound\n    }\n    \n    // Return a copy to prevent external modifications\n    sessionCopy := *session\n    return &sessionCopy, nil\n}\n\n// UpdateSession modifies an existing upload session\nfunc (s *MemoryStore) UpdateSession(ctx context.Context, session *UploadSession) error {\n    s.mutex.Lock()\n    defer s.mutex.Unlock()\n    \n    existing, exists := s.sessions[session.ID]\n    if !exists {\n        return ErrSessionNotFound\n    }\n    \n    // TODO: Validate session state transitions are legal\n    // TODO: Ensure offset updates are monotonic (never decrease)\n    // TODO: Preserve creation timestamp, update modification timestamp\n    // TODO: Validate chunk hash updates for integrity checking\n    \n    sessionCopy := *session\n    sessionCopy.CreatedAt = existing.CreatedAt\n    sessionCopy.UpdatedAt = time.Now()\n    \n    s.sessions[session.ID] = &sessionCopy\n    return nil\n}\n\n// DeleteSession removes an upload session\nfunc (s *MemoryStore) DeleteSession(ctx context.Context, sessionID string) error {\n    s.mutex.Lock()\n    defer s.mutex.Unlock()\n    \n    if _, exists := s.sessions[sessionID]; !exists {\n        return ErrSessionNotFound\n    }\n    \n    delete(s.sessions, sessionID)\n    return nil\n}\n\n// ListExpiredSessions returns session IDs that have exceeded the TTL\nfunc (s *MemoryStore) ListExpiredSessions(ctx context.Context, ttl time.Duration) ([]string, error) {\n    s.mutex.RLock()\n    defer s.mutex.RUnlock()\n    \n    cutoff := time.Now().Add(-ttl)\n    var expired []string\n    \n    for id, session := range s.sessions {\n        if session.UpdatedAt.Before(cutoff) && session.Status != SessionStatusCompleted {\n            expired = append(expired, id)\n        }\n    }\n    \n    return expired, nil\n}\n```\n\n#### Upload Manager Core Structure\n\nThe upload manager coordinates between the protocol layer, session management, and storage backends.\n\n```go\n// internal/upload/manager.go\npackage upload\n\nimport (\n    \"context\"\n    \"crypto/rand\"\n    \"encoding/hex\"\n    \"fmt\"\n    \"time\"\n\n    \"internal/session\"\n    \"internal/storage\"\n)\n\n// SessionManager coordinates upload sessions with storage backends\ntype SessionManager struct {\n    store   session.SessionStore\n    storage storage.StorageBackend\n}\n\n// NewSessionManager creates a new upload session manager\nfunc NewSessionManager(store session.SessionStore, storage storage.StorageBackend) *SessionManager {\n    return &SessionManager{\n        store:   store,\n        storage: storage,\n    }\n}\n\n// InitializeUpload creates a new upload session and prepares storage\nfunc (m *SessionManager) InitializeUpload(ctx context.Context, filename string, totalSize int64, metadata map[string]string) (*session.UploadSession, error) {\n    // TODO: Generate cryptographically secure session ID\n    // TODO: Create UploadSession with initial state and metadata\n    // TODO: Prepare storage backend for chunk reception\n    // TODO: Store session in session store with atomic operation\n    // TODO: Return session information for client response\n    \n    sessionID := generateSessionID()\n    \n    uploadSession := &session.UploadSession{\n        ID:            sessionID,\n        Filename:      filename,\n        TotalSize:     totalSize,\n        CurrentOffset: 0,\n        Status:        session.SessionStatusInitialized,\n        StorageKey:    fmt.Sprintf(\"uploads/%s\", sessionID),\n        Metadata:      metadata,\n        ChunkHashes:   make(map[int64]string),\n    }\n    \n    if err := m.store.CreateSession(ctx, uploadSession); err != nil {\n        return nil, fmt.Errorf(\"failed to create session: %w\", err)\n    }\n    \n    return uploadSession, nil\n}\n\n// ProcessChunk handles incoming chunk upload\nfunc (m *SessionManager) ProcessChunk(ctx context.Context, sessionID string, offset int64, data []byte, contentHash string) error {\n    // TODO: Retrieve and validate upload session state\n    // TODO: Verify offset matches expected position for resumability\n    // TODO: Write chunk data to storage backend at correct position\n    // TODO: Update session with new offset and chunk completion status\n    // TODO: Handle partial write failures with appropriate error responses\n    \n    return fmt.Errorf(\"not implemented\")\n}\n\n// GetUploadProgress returns current upload status for resume\nfunc (m *SessionManager) GetUploadProgress(ctx context.Context, sessionID string) (*session.UploadSession, error) {\n    // TODO: Retrieve session from store\n    // TODO: Verify session state is valid for progress query\n    // TODO: Cross-check session offset with actual storage backend state\n    // TODO: Return consistent progress information for client resume\n    \n    return m.store.GetSession(ctx, sessionID)\n}\n\nfunc generateSessionID() string {\n    bytes := make([]byte, 16)\n    rand.Read(bytes)\n    return hex.EncodeToString(bytes)\n}\n```\n\n#### Storage Interface Foundation\n\nThe storage abstraction provides a clean interface for different backend implementations.\n\n```go\n// internal/storage/interface.go\npackage storage\n\nimport (\n    \"context\"\n    \"io\"\n    \"time\"\n)\n\n// StorageBackend defines the interface for file storage operations\ntype StorageBackend interface {\n    // WriteChunk writes data at the specified offset in the file\n    WriteChunk(ctx context.Context, key string, offset int64, data []byte) error\n    \n    // ReadChunk reads data from the specified range in the file\n    ReadChunk(ctx context.Context, key string, offset, length int64) ([]byte, error)\n    \n    // GetSize returns the current size of the file\n    GetSize(ctx context.Context, key string) (int64, error)\n    \n    // Delete removes the file from storage\n    Delete(ctx context.Context, key string) error\n    \n    // GenerateDownloadURL creates a signed URL for file access\n    GenerateDownloadURL(ctx context.Context, key string, expiration time.Duration) (string, error)\n    \n    // FinalizeUpload completes multipart uploads (for backends that require it)\n    FinalizeUpload(ctx context.Context, key string) error\n}\n\n// StorageConfig contains backend-specific configuration\ntype StorageConfig struct {\n    Backend string\n    Options map[string]string\n}\n```\n\n#### Milestone Checkpoints\n\n**Milestone 1 Checkpoint: Basic Upload Session Management**\n- Run: `go test ./internal/session/... -v`\n- Expected: All session store tests pass with proper state transitions\n- Manual test: POST to initialize upload, verify session creation and unique ID generation\n- Success indicators: Session ID returned, initial offset is 0, status is \"initialized\"\n\n**Milestone 2 Checkpoint: Storage Backend Selection**\n- Run: `go test ./internal/storage/... -v`\n- Expected: Local storage backend passes all interface compliance tests\n- Manual test: Configure different storage backends via JSON config\n- Success indicators: Backend factory correctly instantiates based on configuration\n\n**Milestone 3 Checkpoint: Component Integration**\n- Run: `go test ./internal/upload/... -v`\n- Expected: Upload manager coordinates session and storage operations\n- Manual test: Complete upload initialization through chunk processing\n- Success indicators: Session state updates correctly, storage receives chunks at proper offsets\n\n#### Common Implementation Pitfalls\n\n⚠️ **Pitfall: Session ID Collisions**\nUsing weak random number generation for session IDs can lead to collisions in high-throughput scenarios. Always use `crypto/rand` for session ID generation, never `math/rand`. Test by generating millions of IDs and checking for duplicates.\n\n⚠️ **Pitfall: Offset Consistency**\nFailing to atomically update session offset with storage operations creates windows where clients can receive inconsistent resume positions. Always update session state after confirming successful storage writes, and implement rollback logic for partial failures.\n\n⚠️ **Pitfall: Memory Leaks in Session Storage**\nIn-memory session stores must implement proper cleanup for expired sessions, or memory usage will grow unbounded. Implement background cleanup goroutines with proper shutdown handling to prevent resource leaks.\n\n⚠️ **Pitfall: Storage Path Traversal**\nWhen implementing local storage backends, failing to sanitize storage keys can allow path traversal attacks. Always use `filepath.Join()` and validate that resolved paths remain within the configured storage directory.\n\n\n## Data Model and State Management\n\n> **Milestone(s):** Milestone 1 (Chunked Upload Protocol) — this section defines the core data structures and state management that enable chunked uploads, session tracking, and resumable transfers\n\nThink of upload state management like a **flight data recorder** for file transfers. Just as an aircraft's black box continuously records flight parameters so investigators can reconstruct what happened after an incident, our upload service must meticulously track every aspect of an ongoing transfer — which chunks have arrived, their integrity status, current byte offsets, and session metadata. This persistent state allows us to resume exactly where we left off after any interruption, whether it's a network failure, server restart, or client disconnect.\n\nThe challenge in designing this data model lies in balancing **consistency**, **performance**, and **recoverability**. We need structures that can be quickly updated as chunks arrive (performance), maintain accurate state even under concurrent access (consistency), and survive system failures without corruption (recoverability). Unlike simple HTTP uploads that are atomic operations, resumable uploads create long-lived sessions that span multiple requests, potentially lasting hours or days.\n\n![Upload Session State Machine](./diagrams/upload-state-machine.svg)\n\nOur data model centers around three core abstractions: the `UploadSession` that tracks overall transfer state, individual chunk metadata for ordering and integrity, and a persistence strategy that ensures state survives failures. Each serves a distinct purpose in maintaining the illusion of a single, reliable file transfer despite the underlying reality of multiple network requests across an unreliable medium.\n\n### Upload Session Model\n\nThe **upload session** serves as the master record for each resumable transfer, functioning like a project folder that contains all information needed to coordinate the upload from start to finish. Think of it as the \"case file\" for an ongoing investigation — it accumulates evidence (chunks) over time and tracks the current status of the entire operation.\n\nEach upload session progresses through a well-defined lifecycle with explicit state transitions. Understanding this lifecycle is crucial because different operations are permitted in different states, and state transitions have specific preconditions and side effects that must be maintained for system consistency.\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `ID` | `string` | Unique identifier generated by server, typically UUID format for global uniqueness |\n| `Filename` | `string` | Original filename provided by client, used for content disposition and logging |\n| `ContentType` | `string` | MIME type of the file, validated against magic bytes during upload completion |\n| `TotalSize` | `int64` | Expected total file size in bytes, used for progress calculation and storage allocation |\n| `CurrentOffset` | `int64` | Number of bytes successfully received and verified, defines resume point for client |\n| `Status` | `SessionStatus` | Current state in the upload lifecycle, determines which operations are permitted |\n| `StorageKey` | `string` | Backend-specific identifier for the file location, enables storage abstraction |\n| `Metadata` | `map[string]string` | Client-provided key-value pairs for custom attributes like tags or categories |\n| `CreatedAt` | `time.Time` | Session initialization timestamp, used for TTL calculations and audit logging |\n| `UpdatedAt` | `time.Time` | Last modification timestamp, indicates recent activity for cleanup decisions |\n| `ChunkHashes` | `map[int64]string` | Mapping of chunk sequence numbers to their cryptographic hashes for integrity verification |\n\nThe session lifecycle follows a strict state machine that prevents invalid transitions and ensures data consistency. Each state represents a distinct phase of the upload process with specific responsibilities and constraints.\n\n| Current State | Event | Next State | Actions Taken |\n|---------------|--------|------------|---------------|\n| N/A | Client initialization request | `Initialized` | Create session record, allocate storage, generate upload ID |\n| `Initialized` | First chunk received | `Active` | Update offset, store chunk metadata, begin progress tracking |\n| `Active` | Subsequent chunks received | `Active` | Validate chunk order, update offset, verify chunk integrity |\n| `Active` | Final chunk received | `Completing` | Trigger file assembly, initiate virus scanning, prepare final storage |\n| `Completing` | Assembly and validation success | `Completed` | Finalize storage, generate download URLs, notify client |\n| `Completing` | Validation failure detected | `Failed` | Move to quarantine, log failure reason, preserve evidence |\n| `Active` | TTL expiration reached | `Expired` | Mark for cleanup, preserve partial data for forensics |\n| Any state | Critical error occurred | `Failed` | Log error context, preserve state for debugging |\n\n> **Design Principle**: State transitions are atomic and logged to ensure we can always reconstruct how a session reached its current state, even after failures.\n\nThe `CurrentOffset` field deserves special attention because it serves as the authoritative source of truth for upload progress. When a client reconnects after a failure, it queries this offset to determine where to resume sending data. The offset must only be updated after a chunk has been successfully written to storage and its integrity verified — premature updates lead to data loss scenarios where the client believes bytes were received but the server has no record of them.\n\n**Session Status Enumeration:**\n\n| Status Value | Meaning | Client Actions Permitted | Server Responsibilities |\n|--------------|---------|-------------------------|-------------------------|\n| `Initialized` | Session created, awaiting first chunk | Send chunks starting from offset 0 | Accept chunks, validate ordering, transition to active |\n| `Active` | Actively receiving chunks | Send remaining chunks, query progress | Process chunks, update offset, maintain session TTL |\n| `Completing` | All chunks received, processing | Query status, wait for completion | Assemble file, run validation, finalize storage |\n| `Completed` | Upload successful, file available | Download file, clean up client state | Serve file, maintain for configured retention period |\n| `Failed` | Upload failed, requires cleanup | Review error, restart upload if desired | Preserve failure context, clean up partial data |\n| `Expired` | Session timeout, no recent activity | Restart upload from beginning | Clean up partial data, log expiration for monitoring |\n\n> **Decision: Session-Scoped Chunk Tracking**\n> - **Context**: We need to track individual chunks within each upload session for integrity verification and resume logic\n> - **Options Considered**: \n>   1. Separate chunk table with foreign keys to sessions\n>   2. Embedded chunk metadata within session record\n>   3. File-system based tracking using chunk naming conventions\n> - **Decision**: Embedded `ChunkHashes` map within the session record\n> - **Rationale**: Eliminates join queries for common operations, reduces database transactions, and ensures chunk metadata has same consistency guarantees as session state\n> - **Consequences**: Session records are larger but operations are faster, and chunk cleanup is automatic when sessions are deleted\n\n### Chunk Tracking Model\n\nIndividual chunks require their own metadata model to support integrity verification, ordering guarantees, and efficient resume operations. Think of each chunk as a **numbered puzzle piece** — we need to track its position, verify it hasn't been corrupted, and ensure we can assemble all pieces in the correct order to reconstruct the original file.\n\nThe chunk tracking system must handle several challenging scenarios: chunks arriving out of order due to network routing, duplicate chunks caused by client retries, corrupted chunks that need to be re-uploaded, and missing chunks that prevent upload completion. Our model provides the metadata necessary to detect and handle each of these situations gracefully.\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| Sequence Number | `int64` | Zero-based chunk index, determines assembly order and identifies gaps |\n| Byte Range | `int64, int64` | Start and end byte offsets within the final file for precise positioning |\n| Content Hash | `string` | SHA-256 hash of chunk content, enables corruption detection and deduplication |\n| Size | `int64` | Actual chunk size in bytes, used for progress calculation and storage allocation |\n| Received At | `time.Time` | Timestamp when chunk was successfully stored, useful for debugging and monitoring |\n| Storage Path | `string` | Backend-specific location where chunk data is stored temporarily |\n\nThe chunk assembly process requires careful coordination between the session state and individual chunk metadata. As chunks arrive, we must validate their position within the overall file, verify their integrity, and determine when we have received all necessary pieces to complete the assembly.\n\n**Chunk Ordering and Gap Detection:**\n\nChunks may arrive out of order due to network characteristics, client retry logic, or parallel upload strategies. Our tracking model must efficiently detect gaps in the sequence and communicate missing ranges to clients during resume operations.\n\n| Scenario | Detection Method | Resolution Strategy |\n|----------|------------------|---------------------|\n| Out-of-order arrival | Compare sequence numbers against expected next chunk | Buffer chunks, assemble when sequence is complete |\n| Missing chunks | Gap analysis in sequence number range | Report missing ranges in resume response |\n| Duplicate chunks | Hash comparison against stored chunks | Ignore duplicates, update timestamp for activity tracking |\n| Corrupted chunks | Hash mismatch between expected and actual | Request chunk retransmission, log corruption event |\n| Partial chunks | Size validation against expected chunk boundaries | Reject partial chunks, maintain strict boundaries |\n\n> **Critical Implementation Detail**: Chunk sequence numbers must account for the final chunk being potentially smaller than the standard chunk size. The assembly logic must handle this edge case correctly.\n\n**Integrity Verification Strategy:**\n\nEach chunk undergoes cryptographic hash verification to ensure data integrity across network transmission and storage operations. The hash serves multiple purposes: corruption detection, deduplication, and forensic verification in case of security incidents.\n\n| Hash Algorithm | Use Case | Trade-offs |\n|----------------|----------|------------|\n| SHA-256 | Primary integrity verification | Strong security, moderate performance cost |\n| MD5 | Legacy client compatibility | Faster computation, cryptographically weak |\n| CRC32 | Quick corruption detection | Very fast, not suitable for security verification |\n\nThe chunk hashes are stored in the session's `ChunkHashes` map, indexed by sequence number. This allows efficient lookup during chunk processing and provides a complete integrity manifest for the entire file.\n\n> **Decision: Cryptographic Hash Selection**\n> - **Context**: We need to verify chunk integrity while balancing security and performance requirements\n> - **Options Considered**:\n>   1. SHA-256 for maximum security\n>   2. MD5 for performance and client compatibility  \n>   3. Blake2b for optimal performance/security balance\n> - **Decision**: SHA-256 as primary hash with optional MD5 for legacy clients\n> - **Rationale**: SHA-256 provides strong security guarantees required for production systems, while MD5 support eases client migration\n> - **Consequences**: Higher CPU cost per chunk but eliminates entire class of integrity vulnerabilities\n\n### State Persistence Strategy\n\nThe persistence strategy determines how upload state survives system failures, restarts, and scaling operations. Think of this as the **insurance policy** for our upload service — it ensures that hours of upload progress isn't lost due to temporary infrastructure issues.\n\nOur persistence requirements are more complex than typical web applications because upload sessions are long-lived, state changes frequently (with each chunk), and consistency is critical for resume operations. We must design a system that provides durability without sacrificing the performance needed to handle high-throughput uploads.\n\n**Persistence Requirements Analysis:**\n\n| Requirement | Rationale | Implementation Implications |\n|-------------|-----------|----------------------------|\n| Atomic Updates | Session state must never be partially updated | Use database transactions or atomic file operations |\n| Crash Recovery | State must survive unexpected termination | Implement write-ahead logging or equivalent durability |\n| Concurrent Access | Multiple processes may update same session | Require locking or optimistic concurrency control |\n| Performance | Updates must not bottleneck upload throughput | Consider batching, async writes, or eventually consistent updates |\n| Consistency | Resume operations must see accurate state | Ensure reads reflect all committed writes |\n\n> **Decision: Pluggable State Store Architecture**\n> - **Context**: Different deployment environments have different persistence requirements and available infrastructure\n> - **Options Considered**:\n>   1. Hard-coded database implementation (PostgreSQL)\n>   2. File-system based persistence\n>   3. Abstract interface with multiple backends\n> - **Decision**: Abstract `StateStore` interface with pluggable implementations\n> - **Rationale**: Allows development against in-memory store, production against PostgreSQL, and testing against file-based stores\n> - **Consequences**: Additional abstraction complexity but significantly improved deployment flexibility\n\n**State Store Interface Design:**\n\nThe state store abstraction provides a clean separation between business logic and persistence mechanics. This interface must support the core operations needed by the upload service while remaining simple enough to implement across different storage backends.\n\n| Method Name | Parameters | Returns | Description |\n|-------------|------------|---------|-------------|\n| `CreateSession` | `ctx Context, session *UploadSession` | `error` | Atomically creates new session, failing if ID already exists |\n| `GetSession` | `ctx Context, sessionID string` | `*UploadSession, error` | Retrieves session by ID, returning `ErrSessionNotFound` if missing |\n| `UpdateSession` | `ctx Context, session *UploadSession` | `error` | Modifies existing session atomically, using optimistic locking |\n| `DeleteSession` | `ctx Context, sessionID string` | `error` | Removes session and all associated metadata permanently |\n| `ListExpiredSessions` | `ctx Context, ttl Duration` | `[]string, error` | Returns session IDs older than TTL for cleanup processing |\n\nEach method includes a `context.Context` parameter to support cancellation, timeouts, and distributed tracing. The context pattern is essential for production systems where operations may need to be cancelled due to client disconnection or system overload.\n\n**Durability Guarantees by Implementation:**\n\nDifferent state store implementations provide different levels of durability and performance. Understanding these trade-offs is crucial for selecting the appropriate backend for each deployment environment.\n\n| Implementation | Durability Level | Performance Characteristics | Use Cases |\n|----------------|------------------|----------------------------|-----------|\n| `MemoryStore` | None (lost on restart) | Highest throughput, lowest latency | Development, testing, stateless deployments |\n| `FileStore` | Survives process restart | Medium throughput, fsync latency | Single-server deployments, simple infrastructure |\n| `PostgreSQLStore` | Full ACID compliance | Lower throughput, network latency | Multi-server deployments, strong consistency requirements |\n| `RedisStore` | Configurable (memory vs disk) | High throughput, network latency | High-scale deployments, distributed systems |\n\n> **Performance Consideration**: Session updates happen with every chunk upload, potentially thousands of times per file. The state store implementation must be optimized for high-frequency small updates rather than large batch operations.\n\n**Session Cleanup and TTL Management:**\n\nLong-lived upload sessions require active cleanup to prevent storage exhaustion and maintain system performance. The cleanup strategy must balance resource reclamation with user experience — we don't want to delete sessions that are still actively being used.\n\n| Cleanup Trigger | Action Taken | Safety Measures |\n|------------------|--------------|-----------------|\n| TTL Expiration | Mark session as expired, schedule cleanup | Grace period before actual deletion |\n| Completion | Move to completed state, start retention timer | Configurable retention for download access |\n| Failure | Move to failed state, preserve for debugging | Extended retention for forensic analysis |\n| System Maintenance | Batch cleanup of old sessions | Skip sessions with recent activity |\n\nThe cleanup process operates as a background task that periodically scans for expired sessions and performs the necessary cleanup operations. This includes removing partial files from storage, clearing session metadata, and updating monitoring metrics.\n\n**Common Pitfalls in State Management:**\n\n⚠️ **Pitfall: Race Conditions in Offset Updates**\nMany implementations incorrectly update the session offset before confirming the chunk has been written to storage. This creates a window where the client believes a chunk was received but the server has no record of it, leading to data corruption during resume operations. Always update the offset as the final step in a transaction that includes the storage write.\n\n⚠️ **Pitfall: Incomplete State Machine Validation**\nFailing to validate state transitions allows sessions to enter invalid states that confuse both clients and administrators. For example, transitioning directly from `Initialized` to `Completed` without passing through `Active` indicates a logic error. Implement explicit state validation in all update operations.\n\n⚠️ **Pitfall: TTL Calculation Based on Creation Time**\nUsing only the creation time for TTL calculations can prematurely expire sessions that are actively receiving chunks but started long ago. Always consider the last update time when determining if a session should be cleaned up. A session receiving chunks within the last hour should not be expired regardless of when it was created.\n\n![Core Data Model Relationships](./diagrams/data-model-relationships.svg)\n\n### Implementation Guidance\n\nThis section provides the concrete code structures and patterns needed to implement the data model and state management system described above.\n\n**A. Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| State Store | In-memory map with mutex | PostgreSQL with connection pooling |\n| Serialization | JSON encoding/decoding | Protocol Buffers with schema evolution |\n| Hashing | Go crypto/sha256 standard library | Hardware-accelerated Blake2b |\n| Time Handling | time.Time with UTC normalization | Logical clocks for distributed systems |\n| Concurrency | sync.RWMutex for state protection | Channels with worker pools |\n\n**B. Recommended File Structure:**\n\n```\ninternal/\n  session/\n    session.go              ← UploadSession type and core logic\n    session_test.go         ← Unit tests for session operations  \n    status.go               ← SessionStatus enum and transitions\n    store.go                ← StateStore interface definition\n    memory_store.go         ← In-memory implementation for testing\n    file_store.go           ← File-based persistence implementation\n    cleanup.go              ← Background cleanup and TTL management\n  config/\n    config.go               ← Configuration loading and validation\n    config_test.go          ← Configuration parsing tests\n```\n\n**C. Infrastructure Starter Code:**\n\n**Configuration Management (`internal/config/config.go`):**\n\n```go\npackage config\n\nimport (\n    \"encoding/json\"\n    \"os\"\n    \"time\"\n)\n\n// Config represents the complete service configuration\ntype Config struct {\n    Server   ServerConfig   `json:\"server\"`\n    Storage  StorageConfig  `json:\"storage\"`\n    Security SecurityConfig `json:\"security\"`\n    Cleanup  CleanupConfig  `json:\"cleanup\"`\n}\n\ntype ServerConfig struct {\n    Host         string        `json:\"host\"`\n    Port         int           `json:\"port\"`\n    ReadTimeout  time.Duration `json:\"readTimeout\"`\n    WriteTimeout time.Duration `json:\"writeTimeout\"`\n    MaxBodySize  int64         `json:\"maxBodySize\"`\n}\n\ntype StorageConfig struct {\n    Backend   string            `json:\"backend\"`\n    LocalPath string            `json:\"localPath\"`\n    S3Config  *S3Config         `json:\"s3Config,omitempty\"`\n    Options   map[string]string `json:\"options\"`\n}\n\ntype S3Config struct {\n    Bucket          string `json:\"bucket\"`\n    Region          string `json:\"region\"`\n    AccessKeyID     string `json:\"accessKeyId\"`\n    SecretAccessKey string `json:\"secretAccessKey\"`\n    Endpoint        string `json:\"endpoint\"`\n}\n\ntype SecurityConfig struct {\n    MaxFileSize         int64    `json:\"maxFileSize\"`\n    AllowedContentTypes []string `json:\"allowedContentTypes\"`\n    VirusScanEnabled    bool     `json:\"virusScanEnabled\"`\n    ClamAVSocket        string   `json:\"clamavSocket\"`\n}\n\ntype CleanupConfig struct {\n    SessionTTL      time.Duration `json:\"sessionTTL\"`\n    CleanupInterval time.Duration `json:\"cleanupInterval\"`\n    QuarantineTTL   time.Duration `json:\"quarantineTTL\"`\n}\n\n// LoadConfig reads configuration from JSON file with sensible defaults\nfunc LoadConfig(filename string) (*Config, error) {\n    config := &Config{\n        Server: ServerConfig{\n            Host:         \"localhost\",\n            Port:         8080,\n            ReadTimeout:  30 * time.Second,\n            WriteTimeout: 30 * time.Second,\n            MaxBodySize:  100 * 1024 * 1024, // 100MB\n        },\n        Storage: StorageConfig{\n            Backend:   \"local\",\n            LocalPath: \"./uploads\",\n            Options:   make(map[string]string),\n        },\n        Security: SecurityConfig{\n            MaxFileSize:         1024 * 1024 * 1024, // 1GB\n            AllowedContentTypes: []string{\"*/*\"},\n            VirusScanEnabled:    false,\n        },\n        Cleanup: CleanupConfig{\n            SessionTTL:      24 * time.Hour,\n            CleanupInterval: time.Hour,\n            QuarantineTTL:   7 * 24 * time.Hour,\n        },\n    }\n\n    if filename != \"\" {\n        data, err := os.ReadFile(filename)\n        if err != nil {\n            return nil, err\n        }\n\n        if err := json.Unmarshal(data, config); err != nil {\n            return nil, err\n        }\n    }\n\n    return config, config.ValidateRequirements()\n}\n\n// ValidateRequirements ensures configuration meets system requirements\nfunc (c *Config) ValidateRequirements() error {\n    // TODO: Add validation logic for required fields and constraints\n    return nil\n}\n```\n\n**Session Status Enumeration (`internal/session/status.go`):**\n\n```go\npackage session\n\n// SessionStatus represents the current state of an upload session\ntype SessionStatus string\n\nconst (\n    SessionStatusInitialized = SessionStatus(\"initialized\")\n    SessionStatusActive      = SessionStatus(\"active\") \n    SessionStatusCompleting  = SessionStatus(\"completing\")\n    SessionStatusCompleted   = SessionStatus(\"completed\")\n    SessionStatusFailed      = SessionStatus(\"failed\")\n    SessionStatusExpired     = SessionStatus(\"expired\")\n)\n\n// IsValid checks if the status value is recognized\nfunc (s SessionStatus) IsValid() bool {\n    switch s {\n    case SessionStatusInitialized, SessionStatusActive, SessionStatusCompleting,\n         SessionStatusCompleted, SessionStatusFailed, SessionStatusExpired:\n        return true\n    default:\n        return false\n    }\n}\n\n// CanTransitionTo validates if transition to target status is allowed\nfunc (s SessionStatus) CanTransitionTo(target SessionStatus) bool {\n    validTransitions := map[SessionStatus][]SessionStatus{\n        SessionStatusInitialized: {SessionStatusActive, SessionStatusFailed, SessionStatusExpired},\n        SessionStatusActive:      {SessionStatusCompleting, SessionStatusFailed, SessionStatusExpired},\n        SessionStatusCompleting:  {SessionStatusCompleted, SessionStatusFailed},\n        SessionStatusCompleted:   {}, // Terminal state\n        SessionStatusFailed:      {}, // Terminal state  \n        SessionStatusExpired:     {}, // Terminal state\n    }\n    \n    allowed, exists := validTransitions[s]\n    if !exists {\n        return false\n    }\n    \n    for _, allowedTarget := range allowed {\n        if target == allowedTarget {\n            return true\n        }\n    }\n    \n    return false\n}\n```\n\n**Memory-Based State Store (`internal/session/memory_store.go`):**\n\n```go\npackage session\n\nimport (\n    \"context\"\n    \"errors\"\n    \"sync\"\n    \"time\"\n)\n\nvar ErrSessionNotFound = errors.New(\"session not found\")\n\n// MemoryStore provides in-memory session storage for testing and development\ntype MemoryStore struct {\n    mu       sync.RWMutex\n    sessions map[string]*UploadSession\n}\n\n// NewMemoryStore creates a new in-memory state store\nfunc NewMemoryStore() *MemoryStore {\n    return &MemoryStore{\n        sessions: make(map[string]*UploadSession),\n    }\n}\n\nfunc (m *MemoryStore) CreateSession(ctx context.Context, session *UploadSession) error {\n    m.mu.Lock()\n    defer m.mu.Unlock()\n    \n    if _, exists := m.sessions[session.ID]; exists {\n        return errors.New(\"session already exists\")\n    }\n    \n    // Deep copy to prevent external modifications\n    sessionCopy := *session\n    sessionCopy.ChunkHashes = make(map[int64]string)\n    for k, v := range session.ChunkHashes {\n        sessionCopy.ChunkHashes[k] = v\n    }\n    \n    m.sessions[session.ID] = &sessionCopy\n    return nil\n}\n\nfunc (m *MemoryStore) GetSession(ctx context.Context, sessionID string) (*UploadSession, error) {\n    m.mu.RLock()\n    defer m.mu.RUnlock()\n    \n    session, exists := m.sessions[sessionID]\n    if !exists {\n        return nil, ErrSessionNotFound\n    }\n    \n    // Deep copy to prevent external modifications\n    sessionCopy := *session\n    sessionCopy.ChunkHashes = make(map[int64]string)\n    for k, v := range session.ChunkHashes {\n        sessionCopy.ChunkHashes[k] = v\n    }\n    \n    return &sessionCopy, nil\n}\n\nfunc (m *MemoryStore) UpdateSession(ctx context.Context, session *UploadSession) error {\n    m.mu.Lock()\n    defer m.mu.Unlock()\n    \n    if _, exists := m.sessions[session.ID]; !exists {\n        return ErrSessionNotFound\n    }\n    \n    session.UpdatedAt = time.Now().UTC()\n    \n    // Deep copy to prevent external modifications\n    sessionCopy := *session\n    sessionCopy.ChunkHashes = make(map[int64]string)\n    for k, v := range session.ChunkHashes {\n        sessionCopy.ChunkHashes[k] = v\n    }\n    \n    m.sessions[session.ID] = &sessionCopy\n    return nil\n}\n\nfunc (m *MemoryStore) DeleteSession(ctx context.Context, sessionID string) error {\n    m.mu.Lock()\n    defer m.mu.Unlock()\n    \n    delete(m.sessions, sessionID)\n    return nil\n}\n\nfunc (m *MemoryStore) ListExpiredSessions(ctx context.Context, ttl time.Duration) ([]string, error) {\n    m.mu.RLock()\n    defer m.mu.RUnlock()\n    \n    cutoff := time.Now().UTC().Add(-ttl)\n    var expired []string\n    \n    for id, session := range m.sessions {\n        if session.UpdatedAt.Before(cutoff) {\n            expired = append(expired, id)\n        }\n    }\n    \n    return expired, nil\n}\n```\n\n**D. Core Logic Skeleton Code:**\n\n**Upload Session Model (`internal/session/session.go`):**\n\n```go\npackage session\n\nimport (\n    \"context\" \n    \"time\"\n)\n\n// UploadSession represents the complete state of a resumable upload\ntype UploadSession struct {\n    ID           string            `json:\"id\"`\n    Filename     string            `json:\"filename\"`\n    ContentType  string            `json:\"contentType\"`\n    TotalSize    int64             `json:\"totalSize\"`\n    CurrentOffset int64            `json:\"currentOffset\"`\n    Status       SessionStatus     `json:\"status\"`\n    StorageKey   string            `json:\"storageKey\"`\n    Metadata     map[string]string `json:\"metadata\"`\n    CreatedAt    time.Time         `json:\"createdAt\"`\n    UpdatedAt    time.Time         `json:\"updatedAt\"`\n    ChunkHashes  map[int64]string  `json:\"chunkHashes\"`\n}\n\n// StateStore defines the interface for session persistence\ntype StateStore interface {\n    CreateSession(ctx context.Context, session *UploadSession) error\n    GetSession(ctx context.Context, sessionID string) (*UploadSession, error)\n    UpdateSession(ctx context.Context, session *UploadSession) error\n    DeleteSession(ctx context.Context, sessionID string) error\n    ListExpiredSessions(ctx context.Context, ttl time.Duration) ([]string, error)\n}\n\n// SessionManager coordinates upload session operations\ntype SessionManager struct {\n    store   StateStore\n    storage StorageBackend // Will be defined in storage abstraction section\n}\n\n// InitializeUpload creates a new upload session and prepares storage\nfunc (sm *SessionManager) InitializeUpload(ctx context.Context, filename string, totalSize int64, metadata map[string]string) (*UploadSession, error) {\n    // TODO 1: Generate unique session ID (use uuid.New().String())\n    // TODO 2: Create storage key for backend (use session ID + filename)\n    // TODO 3: Validate totalSize is within configured limits\n    // TODO 4: Initialize UploadSession with provided parameters\n    // TODO 5: Set status to SessionStatusInitialized\n    // TODO 6: Set CreatedAt and UpdatedAt to current UTC time\n    // TODO 7: Call store.CreateSession to persist new session\n    // TODO 8: Return session or error\n    return nil, nil\n}\n\n// ProcessChunk handles incoming chunk upload and updates session state\nfunc (sm *SessionManager) ProcessChunk(ctx context.Context, sessionID string, offset int64, data []byte, contentHash string) error {\n    // TODO 1: Retrieve session from store using GetSession\n    // TODO 2: Validate session is in Active or Initialized status\n    // TODO 3: Verify offset matches CurrentOffset (no gaps allowed)\n    // TODO 4: Calculate and verify chunk hash matches contentHash parameter\n    // TODO 5: Calculate chunk sequence number from offset and chunk size\n    // TODO 6: Write chunk data to storage backend at appropriate location\n    // TODO 7: Update session.CurrentOffset += len(data)\n    // TODO 8: Store chunk hash in session.ChunkHashes map\n    // TODO 9: Update session status to Active if was Initialized\n    // TODO 10: Call store.UpdateSession to persist changes\n    // Hint: Use crypto/sha256 for hash calculation\n    // Hint: Handle the case where offset doesn't match CurrentOffset (resume scenario)\n    return nil\n}\n\n// GetUploadProgress returns current session state for resume operations\nfunc (sm *SessionManager) GetUploadProgress(ctx context.Context, sessionID string) (*UploadSession, error) {\n    // TODO 1: Call store.GetSession to retrieve current state\n    // TODO 2: Return session data directly (store handles not found errors)\n    // This method is primarily a pass-through to the store layer\n    return nil, nil\n}\n\n// TransitionStatus safely changes session status with validation\nfunc (sm *SessionManager) TransitionStatus(ctx context.Context, sessionID string, newStatus SessionStatus) error {\n    // TODO 1: Retrieve current session from store\n    // TODO 2: Validate transition is allowed using status.CanTransitionTo\n    // TODO 3: Update session.Status = newStatus\n    // TODO 4: Update session.UpdatedAt to current time\n    // TODO 5: Persist changes using store.UpdateSession\n    // TODO 6: Return appropriate error if transition is invalid\n    return nil\n}\n```\n\n**E. Language-Specific Hints:**\n\n- Use `crypto/rand` with `uuid.New()` for generating cryptographically secure session IDs\n- `sync.RWMutex` provides better performance than `sync.Mutex` when reads outnumber writes\n- Always use `time.Now().UTC()` to avoid timezone-related bugs in distributed systems\n- The `context.Context` parameter should be passed to all downstream operations for proper cancellation\n- Use `json` struct tags for consistent serialization across different storage backends\n- Consider using `atomic` operations for frequently updated counters like `CurrentOffset`\n\n**F. Milestone Checkpoint:**\n\nAfter implementing this data model:\n\n1. **Run Tests**: `go test ./internal/session/... -v`\n2. **Expected Output**: All tests pass with coverage of state transitions and CRUD operations\n3. **Manual Verification**: Create a simple main function that initializes sessions and processes chunks\n4. **Success Criteria**: Sessions persist across service restarts and accurately track upload progress\n5. **Warning Signs**: If sessions lose data or allow invalid state transitions, review the StateStore implementation and status validation logic\n\n**G. Debugging Tips:**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Session state resets after restart | Missing persistence or improper loading | Check store implementation, verify file/DB writes | Implement proper durability in chosen StateStore |\n| Chunks appear out of order | Race condition in offset updates | Add logging around offset calculations | Use proper locking or atomic operations |\n| Memory usage grows indefinitely | Sessions not being cleaned up | Monitor session count and TTL processing | Implement background cleanup routine |\n| Hash mismatches on chunk verification | Incorrect hash algorithm or encoding | Compare expected vs actual hash values | Verify SHA-256 usage and hex encoding |\n\n\n## Chunked Upload Protocol Implementation\n\n> **Milestone(s):** Milestone 1 (Chunked Upload Protocol) — this section implements the tus.io-compatible resumable upload protocol with session management and chunk assembly\n\nThink of the chunked upload protocol as a **postal delivery system for large packages**. When you need to ship something too big for a single delivery truck, the postal service breaks it into smaller packages, ships each one separately, and reassembles them at the destination. Each package has a tracking number and sequence information, so even if one package gets delayed or lost, the system knows exactly what's missing and can retry just that piece. The recipient can track progress and know when all pieces have arrived for final assembly.\n\nThe tus.io protocol provides this same reliability for file uploads over unreliable networks. Instead of risking the loss of an entire large file due to a single network hiccup, we break the upload into manageable chunks, track each piece independently, and provide mechanisms for clients to resume exactly where they left off after any interruption.\n\nOur implementation must handle four critical responsibilities: establishing the protocol contract through proper HTTP semantics, managing the lifecycle of upload sessions from initialization to completion, assembling chunks into the final file with integrity verification, and providing precise offset tracking that enables seamless resumption. Each of these areas has subtle complexities that can make the difference between a robust production service and a fragile system that fails under real-world conditions.\n\n![Upload Session State Machine](./diagrams/upload-state-machine.svg)\n\n### Protocol Mechanics\n\nThe tus.io protocol defines a precise HTTP-based conversation between client and server, similar to how **two radio operators use standardized call signs and procedures** to coordinate complex operations despite unreliable communication channels. Every message has a specific format, expected responses, and error conditions that both parties understand, enabling reliable coordination even when individual messages are lost or corrupted.\n\nThe protocol conversation follows a predictable pattern. The client begins by announcing its intent to upload a file and providing metadata about the transfer. The server responds with a unique session identifier and the location where chunks should be sent. The client then sends chunks in sequence, with each chunk containing both the data and information about where it fits in the final file. The server acknowledges each chunk and provides progress information. Finally, when all chunks are received, the server assembles the complete file and notifies the client of successful completion.\n\n> **Decision: HTTP Method Selection for Protocol Operations**\n> - **Context**: The tus.io specification defines specific HTTP methods for different upload operations, but we need to understand the semantic meaning behind each choice\n> - **Options Considered**: Follow tus.io exactly, use only POST for everything, create custom HTTP methods\n> - **Decision**: Implement the full tus.io method set with proper HTTP semantics\n> - **Rationale**: Each HTTP method has specific idempotency and safety properties that are crucial for reliable uploads. POST for creation, PATCH for partial updates, HEAD for status checks, and DELETE for cleanup align perfectly with REST principles and enable proper caching, retry logic, and proxy behavior\n> - **Consequences**: Clients can rely on standard HTTP semantics, intermediary systems (proxies, load balancers) behave predictably, and we gain automatic retry safety for idempotent operations\n\nThe core protocol operations map to specific HTTP methods with carefully chosen semantics:\n\n| HTTP Method | Endpoint Pattern | Purpose | Request Headers | Response Headers | Idempotent |\n|-------------|------------------|---------|------------------|------------------|------------|\n| POST | `/files` | Initialize upload session | `Upload-Length`, `Upload-Metadata` | `Location`, `Tus-Resumable` | No |\n| HEAD | `/files/{id}` | Check upload progress | `Tus-Resumable` | `Upload-Offset`, `Upload-Length` | Yes |\n| PATCH | `/files/{id}` | Upload chunk data | `Upload-Offset`, `Content-Type: application/offset+octet-stream` | `Upload-Offset`, `Tus-Resumable` | No |\n| DELETE | `/files/{id}` | Cancel upload session | `Tus-Resumable` | `Tus-Resumable` | Yes |\n\nThe `Upload-Offset` header serves as the **synchronization heartbeat** between client and server. Every chunk upload includes this header specifying the exact byte position where the chunk data should be written. The server validates that this offset matches its internal tracking and rejects chunks that would create gaps or overlaps. This prevents the data corruption that would result from network reordering, duplicate transmissions, or client-server offset disagreements.\n\nRequest and response formats follow strict patterns that enable robust error detection and recovery:\n\n| Request Component | Format | Validation Rules | Error Response |\n|-------------------|--------|------------------|----------------|\n| Upload-Length | Integer bytes | Must be positive, within configured limits | `413 Payload Too Large` |\n| Upload-Offset | Integer bytes | Must match server's current offset exactly | `409 Conflict` |\n| Content-Length | Integer bytes | Must match actual body size | `400 Bad Request` |\n| Upload-Metadata | Base64 key-value pairs | Keys alphanumeric only, values properly encoded | `400 Bad Request` |\n\nThe server responds to each operation with specific status codes that guide client behavior:\n\n| Status Code | Meaning | Client Action | Server State |\n|-------------|---------|---------------|--------------|\n| 201 Created | Session initialized successfully | Begin chunk uploads to Location URL | Session in `active` state |\n| 204 No Content | Chunk accepted and written | Continue with next chunk | Offset advanced by chunk size |\n| 409 Conflict | Offset mismatch detected | HEAD request to get current offset, resume from there | Offset unchanged |\n| 404 Not Found | Session expired or invalid | Reinitialize upload from beginning | No session state |\n\n> The critical insight for protocol reliability is that **every operation must be precisely recoverable**. When a client receives a network timeout or connection error, it should be able to determine the exact server state through a HEAD request and resume without data loss or corruption.\n\n### Session Lifecycle Management\n\nUpload session management operates like an **air traffic control system**, where each aircraft (upload session) has a unique flight plan, constantly reports its position, and must be safely guided from takeoff to landing while handling weather delays, course corrections, and emergency situations. The system maintains complete awareness of every session's status and can coordinate recovery procedures when things go wrong.\n\nAn upload session represents a contract between client and server to transfer a specific file through a series of chunk operations. The session carries all the context needed to validate incoming chunks, track progress, handle interruptions, and ultimately assemble the complete file. Sessions have a definite lifecycle with clear state transitions and specific actions available in each state.\n\nThe session lifecycle follows this progression:\n\n| Current State | Valid Events | Next State | Actions Taken |\n|---------------|--------------|-------------|---------------|\n| `initialized` | first_chunk_received | `active` | Create storage placeholder, update offset |\n| `initialized` | timeout_expired | `expired` | Mark for cleanup, log timeout |\n| `active` | chunk_received | `active` | Validate offset, write chunk, update offset |\n| `active` | all_chunks_received | `completing` | Begin chunk assembly and validation |\n| `active` | timeout_expired | `expired` | Mark for cleanup, preserve partial data |\n| `completing` | assembly_successful | `completed` | Finalize file, generate download URL |\n| `completing` | assembly_failed | `failed` | Log error, preserve chunks for debugging |\n| `completed` | ttl_expired | `expired` | Schedule session cleanup |\n| `failed` | retry_requested | `active` | Reset to last valid offset |\n\nSession initialization creates the foundational state that guides all subsequent operations. The server generates a cryptographically random session identifier, validates the proposed upload against size and type constraints, reserves storage space, and records the client's declared metadata. This information becomes the authoritative source for validating all future chunk uploads.\n\n| Session Field | Type | Purpose | Validation Rules |\n|---------------|------|---------|------------------|\n| `ID` | string | Unique session identifier | UUID v4 format, cryptographically random |\n| `Filename` | string | Original file name | Sanitized, no path traversal characters |\n| `ContentType` | string | Declared MIME type | Must be in allowed types list |\n| `TotalSize` | int64 | Expected final file size | Positive, within configured limits |\n| `CurrentOffset` | int64 | Bytes successfully received | Always ≤ TotalSize, monotonically increasing |\n| `Status` | SessionStatus | Current lifecycle state | One of defined enum values |\n| `StorageKey` | string | Backend storage identifier | Generated by storage backend |\n| `Metadata` | map[string]string | Client-provided key-value pairs | Keys sanitized, values size-limited |\n| `CreatedAt` | time.Time | Session creation timestamp | Used for TTL calculation |\n| `UpdatedAt` | time.Time | Last activity timestamp | Updated on every chunk |\n| `ChunkHashes` | map[int64]string | Integrity verification hashes | Key is chunk offset, value is hash |\n\nProgress tracking requires **atomic updates** to prevent race conditions when multiple goroutines handle concurrent operations for the same session. Each chunk upload must atomically verify the current offset, validate the chunk placement, write the data, and update the session state. If any step fails, the entire operation rolls back, leaving the session in a consistent state for retry.\n\n> **Decision: Session State Persistence Strategy**\n> - **Context**: Sessions must survive server restarts and scale across multiple server instances\n> - **Options Considered**: In-memory only (fast but loses state on restart), database persistence (durable but slower), hybrid approach (memory + periodic persistence)\n> - **Decision**: Implement pluggable StateStore interface with in-memory and database backends\n> - **Rationale**: In-memory provides maximum performance for single-instance deployments and testing, while database persistence enables production deployments with multiple servers and restart resilience. The interface abstraction allows choosing the right backend for each environment\n> - **Consequences**: Enables horizontal scaling and fault tolerance at the cost of additional complexity and potential performance overhead for persistent backends\n\nSession expiration and cleanup prevent resource leaks from abandoned uploads. Sessions have configurable time-to-live values that depend on their current state. Active sessions get longer TTLs to accommodate slow uploads, while failed sessions expire quickly to free resources. The cleanup process runs periodically to identify expired sessions and remove their associated storage and metadata.\n\n| Session State | TTL Duration | Cleanup Actions | Rationale |\n|---------------|--------------|-----------------|-----------|\n| `initialized` | 1 hour | Delete session metadata only | Client may be preparing upload |\n| `active` | 24 hours | Delete session and partial chunks | Allow for slow network conditions |\n| `completed` | 7 days | Delete session, keep file | Client needs time to download |\n| `failed` | 1 hour | Delete session and partial chunks | Quick cleanup of error cases |\n| `expired` | Immediate | Delete all associated data | Already past useful lifetime |\n\n![Chunk Upload Sequence Flow](./diagrams/chunk-upload-sequence.svg)\n\n### Chunk Assembly Strategy\n\nChunk assembly is like **reconstructing a jigsaw puzzle where pieces arrive out of order**, some pieces might be damaged, and you need to verify that the completed picture matches exactly what was promised. The system must handle pieces arriving in any sequence, detect missing or corrupted pieces, and only declare success when every piece is validated and properly positioned.\n\nThe assembly process begins as soon as the server detects that all expected chunks have been received. This detection happens when the session's `CurrentOffset` equals the declared `TotalSize`, indicating that no gaps remain in the chunk sequence. However, receiving all bytes doesn't guarantee that the file is correct—the chunks might have been corrupted during transmission, written to wrong offsets due to race conditions, or arrived out of order in ways that created subtle data corruption.\n\nOur assembly strategy implements a **two-phase commit approach** to ensure atomic completion. Phase one performs comprehensive validation: verifying chunk integrity through stored hashes, confirming that no gaps exist in the byte sequence, and validating the assembled file against size and type constraints. Phase two commits the assembled file to permanent storage and updates the session to completed status. If phase one fails, the partial chunks remain available for debugging. If phase two fails, phase one can be retried without repeating the expensive validation work.\n\n| Assembly Phase | Operations Performed | Failure Handling | Success Criteria |\n|----------------|---------------------|------------------|------------------|\n| Validation | Check chunk hashes, verify byte sequence, validate file type | Log errors, preserve chunks, mark session as failed | All chunks verified, no gaps detected |\n| Assembly | Concatenate chunks in order, write to final storage location | Rollback partial writes, retry from validation phase | Complete file written successfully |\n| Commitment | Update session status, generate access URLs, trigger post-processing | Mark as failed, preserve assembled file for manual recovery | Session marked completed, URLs available |\n| Cleanup | Remove temporary chunks, update metrics, log completion | Non-critical, continue with partial cleanup | Resources freed, monitoring updated |\n\nChunk ordering and gap detection requires careful **offset arithmetic** to handle edge cases. The system maintains a complete map of received byte ranges and validates that they form a contiguous sequence from offset 0 to the declared file size. Overlapping chunks are rejected during upload, so the assembly process can assume that each chunk occupies a unique byte range. However, the system still validates this assumption during assembly to detect storage backend corruption or offset calculation bugs.\n\nThe chunk-to-file transformation process varies depending on the storage backend capabilities:\n\n| Storage Type | Assembly Method | Temporary Storage | Atomic Operations | Performance Characteristics |\n|--------------|-----------------|-------------------|-------------------|----------------------------|\n| Local Filesystem | In-place concatenation | Single temporary file | Atomic rename | Fast, limited by disk I/O |\n| S3-Compatible | Multipart upload completion | S3 manages parts | Complete multipart upload API | Network-limited, highly scalable |\n| Memory (testing) | Buffer concatenation | In-memory byte array | Replace pointer atomically | Fastest, memory-limited |\n\nFile integrity verification goes beyond simple size checks to ensure that the assembled content matches the client's intent. The system computes cryptographic hashes of the final assembled file and compares them against client-provided checksums when available. For clients that don't provide checksums, the system generates and stores hashes for future verification or debugging needs.\n\n> **Decision: Chunk Hash Granularity**\n> - **Context**: We need to detect corruption at the finest granularity possible while minimizing storage overhead\n> - **Options Considered**: Hash entire file only, hash each chunk individually, hash fixed-size blocks regardless of chunk boundaries\n> - **Decision**: Hash each chunk individually using SHA-256\n> - **Rationale**: Individual chunk hashing allows precise identification of corrupted chunks for targeted retransmission, balances storage overhead with detection granularity, and aligns with natural upload boundaries that clients already track\n> - **Consequences**: Enables surgical repair of corrupted uploads, increases metadata storage requirements, allows parallel hash computation during upload\n\n⚠️ **Pitfall: Race Conditions During Assembly**\nMany implementations fail to properly synchronize the transition from \"receiving chunks\" to \"assembling file.\" If new chunks arrive while assembly is in progress, they can corrupt the assembly process or be silently lost. Always transition the session to `completing` status before beginning assembly, and reject any chunks that arrive for sessions in this state. Clients that retry chunk uploads during assembly will receive clear error responses and can query session status to understand progress.\n\n### Offset Tracking and Resume Logic\n\nOffset tracking functions as the **GPS navigation system** for resumable uploads, providing precise location awareness that enables clients to resume exactly where they left off after any interruption. Just as GPS coordinates must be accurate to within meters to be useful for navigation, upload offsets must be accurate to the byte level to prevent data corruption or loss during resume operations.\n\nThe server maintains authoritative offset state that represents the highest contiguous byte position successfully written to storage. This offset advances only when chunks are successfully written and verified, never when chunks are merely received or queued. Clients must synchronize with this authoritative offset before resuming uploads, as network conditions or server restarts might have caused their local offset tracking to diverge from reality.\n\nOffset synchronization follows a **checkpoint protocol** where clients can query the current server offset at any time and adjust their behavior accordingly. This query operation is idempotent and lightweight, implemented through HTTP HEAD requests that return the current offset in response headers without transferring any file data. Clients should perform this synchronization after any network interruption, server error response, or extended pause in upload activity.\n\n| Resume Scenario | Client Action | Server Response | Next Steps |\n|-----------------|---------------|-----------------|------------|\n| Network timeout during chunk upload | HEAD request to check offset | Current offset in `Upload-Offset` header | Resume from server's offset position |\n| Server restart between chunks | HEAD request with session ID | 404 if session lost, current offset if recovered | Reinitialize session or resume upload |\n| Chunk rejected with 409 Conflict | HEAD request to get actual offset | Server's expected offset for next chunk | Adjust client position and retry |\n| Client application restart | HEAD request to restore state | Complete session status and current offset | Resume upload or handle completion |\n\nThe offset calculation must account for the **chunk boundary alignment** inherent in streaming uploads. Unlike traditional file operations that can seek to arbitrary byte positions, chunked uploads advance in discrete increments corresponding to successfully processed chunks. The server's offset always represents a clean boundary where the next chunk can be appended without gaps or overlaps.\n\nClient resume logic implements a **three-phase recovery protocol** to handle the most common interruption scenarios:\n\n1. **State Discovery Phase**: The client sends a HEAD request to determine whether the upload session still exists and what offset the server expects for the next chunk. This phase handles session expiration, server restarts, and network partitions.\n\n2. **Synchronization Phase**: The client compares the server's offset with its own internal tracking and adjusts its position accordingly. If the server is ahead (chunks were successfully processed after the client's last acknowledgment), the client skips forward. If the server is behind (the client's last chunk was rejected or lost), the client rewinds.\n\n3. **Resumption Phase**: The client resumes chunk uploads from the synchronized position, with each subsequent chunk building on the verified foundation established during synchronization.\n\nThe server's offset update logic must be **atomically coupled** with the chunk write operation to prevent inconsistencies:\n\n| Operation Sequence | Atomicity Requirement | Failure Handling |\n|-------------------|----------------------|------------------|\n| Validate chunk offset against current session offset | Must read consistent session state | Retry with fresh session data |\n| Write chunk data to storage backend | Must complete fully or not at all | Clean up partial writes, don't advance offset |\n| Update session offset and metadata | Must reflect successful write | Rollback to previous offset on failure |\n| Respond to client with new offset | Must match updated session state | Log inconsistency, client will resync on next operation |\n\nResume boundary detection requires careful handling of **partial chunk scenarios**. If a chunk upload is interrupted mid-transmission, the server may have received and written part of the chunk data before the connection failed. The server must not advance its offset to include partially-written data, as this would create corruption when the client retries the complete chunk. Instead, the server truncates any partial writes and maintains its offset at the last fully-completed chunk boundary.\n\n> The fundamental principle of offset tracking is that **the server's offset is always the single source of truth**. Clients may maintain their own offset tracking for performance and user experience, but they must be prepared to synchronize with the server's authoritative state whenever discrepancies arise.\n\n⚠️ **Pitfall: Offset Advancement Before Write Confirmation**\nA common mistake is updating the session's `CurrentOffset` before confirming that the chunk was successfully written to the storage backend. If the storage write fails after the offset is updated, the session becomes corrupted with an offset that doesn't match the actual stored data. Clients that resume from this incorrect offset will create gaps in the file. Always update the offset atomically with the storage write, or implement rollback logic that restores the previous offset when storage operations fail.\n\n![Upload Completion and Assembly](./diagrams/upload-completion-sequence.svg)\n\n### Implementation Guidance\n\nThis implementation guidance provides the concrete foundation for building a production-ready chunked upload service. The focus is on providing complete, working infrastructure code and detailed skeletons for the core learning components.\n\n**A. Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| HTTP Server | net/http with gorilla/mux for routing | gin-gonic/gin or echo for performance |\n| Session Storage | In-memory map with sync.RWMutex | Redis or PostgreSQL for persistence |\n| File I/O | os.File with manual offset tracking | Memory-mapped files for large chunks |\n| Hashing | crypto/sha256 for chunk verification | Parallel hashing with worker pools |\n| Configuration | JSON config files with encoding/json | Viper for multiple formats and hot reload |\n| Logging | Standard log package with structured output | logrus or zap for structured logging |\n\n**B. Recommended File Structure:**\n\n```\nresumable-upload/\n  cmd/\n    upload-server/\n      main.go                    ← Server entry point\n  internal/\n    protocol/\n      handlers.go                ← HTTP handlers for tus.io endpoints\n      middleware.go              ← CORS, logging, recovery middleware\n      validation.go              ← Request validation and sanitization\n    session/\n      manager.go                 ← Session lifecycle management\n      store.go                   ← StateStore interface and implementations\n      cleanup.go                 ← Background cleanup worker\n    storage/\n      backend.go                 ← Storage abstraction (Milestone 2)\n      local.go                   ← Local filesystem backend\n    config/\n      config.go                  ← Configuration loading and validation\n  pkg/\n    tusio/\n      protocol.go                ← tus.io protocol constants and helpers\n  test/\n    integration/\n      upload_test.go             ← End-to-end upload scenarios\n    fixtures/\n      test_files/                ← Sample files for testing\n```\n\n**C. Infrastructure Starter Code:**\n\nComplete HTTP server foundation with middleware and routing:\n\n```go\n// internal/protocol/server.go\npackage protocol\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n    \n    \"github.com/gorilla/mux\"\n    \"resumable-upload/internal/config\"\n    \"resumable-upload/internal/session\"\n)\n\ntype Server struct {\n    config     *config.Config\n    manager    *session.SessionManager\n    httpServer *http.Server\n}\n\nfunc NewServer(cfg *config.Config, mgr *session.SessionManager) *Server {\n    s := &Server{\n        config:  cfg,\n        manager: mgr,\n    }\n    \n    router := mux.NewRouter()\n    router.Use(s.corsMiddleware)\n    router.Use(s.loggingMiddleware)\n    router.Use(s.recoveryMiddleware)\n    \n    // tus.io protocol endpoints\n    router.HandleFunc(\"/files\", s.createUpload).Methods(\"POST\")\n    router.HandleFunc(\"/files/{id}\", s.getUploadStatus).Methods(\"HEAD\")\n    router.HandleFunc(\"/files/{id}\", s.uploadChunk).Methods(\"PATCH\")\n    router.HandleFunc(\"/files/{id}\", s.deleteUpload).Methods(\"DELETE\")\n    \n    s.httpServer = &http.Server{\n        Addr:         fmt.Sprintf(\"%s:%d\", cfg.Server.Host, cfg.Server.Port),\n        Handler:      router,\n        ReadTimeout:  cfg.Server.ReadTimeout,\n        WriteTimeout: cfg.Server.WriteTimeout,\n    }\n    \n    return s\n}\n\nfunc (s *Server) Start() error {\n    return s.httpServer.ListenAndServe()\n}\n\nfunc (s *Server) Shutdown(ctx context.Context) error {\n    return s.httpServer.Shutdown(ctx)\n}\n\nfunc (s *Server) corsMiddleware(next http.Handler) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        w.Header().Set(\"Access-Control-Allow-Origin\", \"*\")\n        w.Header().Set(\"Access-Control-Allow-Methods\", \"POST, HEAD, PATCH, DELETE, OPTIONS\")\n        w.Header().Set(\"Access-Control-Allow-Headers\", \"Upload-Offset, Upload-Length, Upload-Metadata, Content-Type, Tus-Resumable\")\n        w.Header().Set(\"Access-Control-Expose-Headers\", \"Upload-Offset, Upload-Length, Tus-Resumable, Location\")\n        w.Header().Set(\"Tus-Resumable\", \"1.0.0\")\n        \n        if r.Method == \"OPTIONS\" {\n            w.WriteHeader(http.StatusNoContent)\n            return\n        }\n        \n        next.ServeHTTP(w, r)\n    })\n}\n\nfunc (s *Server) loggingMiddleware(next http.Handler) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        start := time.Now()\n        next.ServeHTTP(w, r)\n        fmt.Printf(\"%s %s %s %v\\n\", r.Method, r.URL.Path, r.RemoteAddr, time.Since(start))\n    })\n}\n\nfunc (s *Server) recoveryMiddleware(next http.Handler) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        defer func() {\n            if err := recover(); err != nil {\n                fmt.Printf(\"Panic recovered: %v\\n\", err)\n                http.Error(w, \"Internal Server Error\", http.StatusInternalServerError)\n            }\n        }()\n        next.ServeHTTP(w, r)\n    })\n}\n```\n\nComplete configuration management with validation:\n\n```go\n// internal/config/config.go\npackage config\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"os\"\n    \"time\"\n)\n\ntype Config struct {\n    Server   ServerConfig   `json:\"server\"`\n    Storage  StorageConfig  `json:\"storage\"`\n    Security SecurityConfig `json:\"security\"`\n    Cleanup  CleanupConfig  `json:\"cleanup\"`\n}\n\ntype ServerConfig struct {\n    Host         string        `json:\"host\"`\n    Port         int           `json:\"port\"`\n    ReadTimeout  time.Duration `json:\"read_timeout\"`\n    WriteTimeout time.Duration `json:\"write_timeout\"`\n    MaxBodySize  int64         `json:\"max_body_size\"`\n}\n\ntype StorageConfig struct {\n    Backend   string            `json:\"backend\"`\n    LocalPath string            `json:\"local_path\"`\n    S3Config  *S3Config         `json:\"s3_config,omitempty\"`\n    Options   map[string]string `json:\"options\"`\n}\n\ntype S3Config struct {\n    Bucket          string `json:\"bucket\"`\n    Region          string `json:\"region\"`\n    AccessKeyID     string `json:\"access_key_id\"`\n    SecretAccessKey string `json:\"secret_access_key\"`\n    Endpoint        string `json:\"endpoint,omitempty\"`\n}\n\ntype SecurityConfig struct {\n    MaxFileSize         int64    `json:\"max_file_size\"`\n    AllowedContentTypes []string `json:\"allowed_content_types\"`\n    VirusScanEnabled    bool     `json:\"virus_scan_enabled\"`\n    ClamAVSocket        string   `json:\"clamav_socket\"`\n}\n\ntype CleanupConfig struct {\n    SessionTTL      time.Duration `json:\"session_ttl\"`\n    CleanupInterval time.Duration `json:\"cleanup_interval\"`\n    QuarantineTTL   time.Duration `json:\"quarantine_ttl\"`\n}\n\nfunc LoadConfig(filename string) (*Config, error) {\n    // Set defaults\n    config := &Config{\n        Server: ServerConfig{\n            Host:         \"localhost\",\n            Port:         8080,\n            ReadTimeout:  30 * time.Second,\n            WriteTimeout: 30 * time.Second,\n            MaxBodySize:  100 * 1024 * 1024, // 100MB\n        },\n        Storage: StorageConfig{\n            Backend:   \"local\",\n            LocalPath: \"./uploads\",\n            Options:   make(map[string]string),\n        },\n        Security: SecurityConfig{\n            MaxFileSize:         1024 * 1024 * 1024, // 1GB\n            AllowedContentTypes: []string{\"*/*\"},\n            VirusScanEnabled:    false,\n        },\n        Cleanup: CleanupConfig{\n            SessionTTL:      24 * time.Hour,\n            CleanupInterval: time.Hour,\n            QuarantineTTL:   7 * 24 * time.Hour,\n        },\n    }\n    \n    if filename != \"\" {\n        file, err := os.Open(filename)\n        if err != nil {\n            return nil, fmt.Errorf(\"failed to open config file: %w\", err)\n        }\n        defer file.Close()\n        \n        decoder := json.NewDecoder(file)\n        if err := decoder.Decode(config); err != nil {\n            return nil, fmt.Errorf(\"failed to parse config file: %w\", err)\n        }\n    }\n    \n    if err := config.ValidateRequirements(); err != nil {\n        return nil, fmt.Errorf(\"config validation failed: %w\", err)\n    }\n    \n    return config, nil\n}\n\nfunc (c *Config) ValidateRequirements() error {\n    if c.Server.Port <= 0 || c.Server.Port > 65535 {\n        return fmt.Errorf(\"server port must be between 1 and 65535\")\n    }\n    \n    if c.Security.MaxFileSize <= 0 {\n        return fmt.Errorf(\"max file size must be positive\")\n    }\n    \n    if c.Storage.Backend == \"local\" && c.Storage.LocalPath == \"\" {\n        return fmt.Errorf(\"local storage path must be specified\")\n    }\n    \n    return nil\n}\n```\n\n**D. Core Logic Skeleton Code:**\n\nProtocol handlers with detailed implementation guidance:\n\n```go\n// internal/protocol/handlers.go\npackage protocol\n\nimport (\n    \"encoding/base64\"\n    \"fmt\"\n    \"io\"\n    \"net/http\"\n    \"strconv\"\n    \"strings\"\n    \"time\"\n    \n    \"github.com/gorilla/mux\"\n    \"resumable-upload/internal/session\"\n)\n\n// createUpload initializes a new resumable upload session\n// Implements POST /files endpoint from tus.io specification\nfunc (s *Server) createUpload(w http.ResponseWriter, r *http.Request) {\n    // TODO 1: Validate tus.io protocol version in Tus-Resumable header\n    // Hint: Should be \"1.0.0\", return 412 Precondition Failed if missing/wrong\n    \n    // TODO 2: Parse Upload-Length header and validate against security limits\n    // Hint: Use strconv.ParseInt, check against s.config.Security.MaxFileSize\n    \n    // TODO 3: Parse Upload-Metadata header (base64 encoded key-value pairs)\n    // Hint: Format is \"key1 base64value1,key2 base64value2\"\n    // Split on comma, then space, decode values with base64.StdEncoding\n    \n    // TODO 4: Extract filename and content-type from metadata\n    // Hint: Standard keys are \"filename\" and \"filetype\"\n    \n    // TODO 5: Validate content type against allowed types list\n    // Hint: Check s.config.Security.AllowedContentTypes, \"*/*\" means allow all\n    \n    // TODO 6: Call s.manager.InitializeUpload with parsed parameters\n    // Hint: Pass context, filename, totalSize, and metadata map\n    \n    // TODO 7: Set response headers and return session URL\n    // Hint: Location header should be \"/files/{sessionID}\", status 201 Created\n}\n\n// getUploadStatus returns current upload progress for resumption\n// Implements HEAD /files/{id} endpoint from tus.io specification  \nfunc (s *Server) getUploadStatus(w http.ResponseWriter, r *http.Request) {\n    // TODO 1: Extract session ID from URL path using mux.Vars(r)[\"id\"]\n    \n    // TODO 2: Call s.manager.GetUploadProgress with session ID\n    // Hint: Handle ErrSessionNotFound with 404 Not Found response\n    \n    // TODO 3: Set Upload-Offset header with current session offset\n    // Hint: Use strconv.FormatInt to convert int64 to string\n    \n    // TODO 4: Set Upload-Length header with total file size\n    \n    // TODO 5: Set Cache-Control: no-store to prevent offset caching\n    // Hint: Offset values change frequently and must not be cached\n    \n    // TODO 6: Return 200 OK with no body (HEAD request)\n}\n\n// uploadChunk processes a chunk upload and updates session progress\n// Implements PATCH /files/{id} endpoint from tus.io specification\nfunc (s *Server) uploadChunk(w http.ResponseWriter, r *http.Request) {\n    // TODO 1: Extract session ID from URL path\n    \n    // TODO 2: Parse Upload-Offset header and validate format\n    // Hint: Must be valid integer, represents byte offset for this chunk\n    \n    // TODO 3: Validate Content-Type header equals \"application/offset+octet-stream\"\n    // Hint: This is required by tus.io spec for chunk uploads\n    \n    // TODO 4: Read request body with size limit protection\n    // Hint: Use io.LimitReader with s.config.Server.MaxBodySize\n    // Don't load entire chunk into memory - stream to ProcessChunk\n    \n    // TODO 5: Calculate chunk hash for integrity verification\n    // Hint: Use crypto/sha256, hash while reading to avoid double I/O\n    \n    // TODO 6: Call s.manager.ProcessChunk with session, offset, data, hash\n    // Hint: Handle offset mismatch errors with 409 Conflict response\n    \n    // TODO 7: Set Upload-Offset header with new session offset\n    // Hint: New offset = previous offset + chunk size\n    \n    // TODO 8: Return 204 No Content on success\n}\n\n// deleteUpload cancels an upload session and cleans up resources\n// Implements DELETE /files/{id} endpoint from tus.io specification\nfunc (s *Server) deleteUpload(w http.ResponseWriter, r *http.Request) {\n    // TODO 1: Extract session ID from URL path\n    \n    // TODO 2: Call s.manager.DeleteSession to remove session and data\n    // Hint: Should clean up both session metadata and partial chunks\n    \n    // TODO 3: Return 204 No Content regardless of whether session existed\n    // Hint: DELETE is idempotent - same result if called multiple times\n}\n```\n\nSession management core with state transitions:\n\n```go\n// internal/session/manager.go\npackage session\n\nimport (\n    \"context\"\n    \"crypto/rand\"\n    \"fmt\"\n    \"time\"\n    \n    \"resumable-upload/internal/storage\"\n)\n\ntype SessionStatus string\n\nconst (\n    SessionStatusInitialized SessionStatus = \"initialized\"\n    SessionStatusActive      SessionStatus = \"active\" \n    SessionStatusCompleting  SessionStatus = \"completing\"\n    SessionStatusCompleted   SessionStatus = \"completed\"\n    SessionStatusFailed      SessionStatus = \"failed\"\n    SessionStatusExpired     SessionStatus = \"expired\"\n)\n\ntype UploadSession struct {\n    ID           string            `json:\"id\"`\n    Filename     string            `json:\"filename\"`\n    ContentType  string            `json:\"content_type\"`\n    TotalSize    int64             `json:\"total_size\"`\n    CurrentOffset int64             `json:\"current_offset\"`\n    Status       SessionStatus     `json:\"status\"`\n    StorageKey   string            `json:\"storage_key\"`\n    Metadata     map[string]string `json:\"metadata\"`\n    CreatedAt    time.Time         `json:\"created_at\"`\n    UpdatedAt    time.Time         `json:\"updated_at\"`\n    ChunkHashes  map[int64]string  `json:\"chunk_hashes\"`\n}\n\ntype SessionManager struct {\n    store   StateStore\n    storage storage.StorageBackend\n}\n\nfunc NewSessionManager(store StateStore, backend storage.StorageBackend) *SessionManager {\n    return &SessionManager{\n        store:   store,\n        storage: backend,\n    }\n}\n\n// InitializeUpload creates a new upload session and prepares storage\nfunc (m *SessionManager) InitializeUpload(ctx context.Context, filename string, totalSize int64, metadata map[string]string) (*UploadSession, error) {\n    // TODO 1: Generate cryptographically secure session ID\n    // Hint: Use crypto/rand with 16 bytes, encode as hex string\n    \n    // TODO 2: Create UploadSession struct with initialized status\n    // Hint: Set CreatedAt/UpdatedAt to time.Now(), CurrentOffset to 0\n    \n    // TODO 3: Generate storage key for this upload\n    // Hint: Use session ID as base, let storage backend handle path construction\n    \n    // TODO 4: Call storage backend to prepare upload location\n    // Hint: Some backends (like S3) need multipart upload initialization\n    \n    // TODO 5: Store session in StateStore\n    // Hint: Use m.store.CreateSession, handle conflicts with new ID generation\n    \n    // TODO 6: Return session pointer for client response\n}\n\n// ProcessChunk handles incoming chunk data with offset validation\nfunc (m *SessionManager) ProcessChunk(ctx context.Context, sessionID string, offset int64, data []byte, contentHash string) error {\n    // TODO 1: Retrieve session from store and validate it exists\n    // Hint: Return ErrSessionNotFound if not found\n    \n    // TODO 2: Validate session is in active or initialized state\n    // Hint: Cannot accept chunks for completed/failed/expired sessions\n    \n    // TODO 3: Validate offset matches session's current offset exactly\n    // Hint: Return offset mismatch error if not equal - client must sync\n    \n    // TODO 4: Validate chunk size doesn't exceed remaining file size\n    // Hint: offset + len(data) must not exceed session.TotalSize\n    \n    // TODO 5: Write chunk to storage backend at specified offset\n    // Hint: Use m.storage.WriteChunk, handle backend-specific errors\n    \n    // TODO 6: Update session with new offset and chunk hash\n    // Hint: New offset = old offset + len(data), store hash for verification\n    \n    // TODO 7: Check if upload is complete (offset == total size)\n    // Hint: If complete, transition to completing status and trigger assembly\n    \n    // TODO 8: Save updated session to store atomically\n}\n\n// GetUploadProgress returns current session state for resume\nfunc (m *SessionManager) GetUploadProgress(ctx context.Context, sessionID string) (*UploadSession, error) {\n    // TODO 1: Retrieve session from store\n    \n    // TODO 2: Check if session has expired based on last activity\n    // Hint: Compare time.Now() - session.UpdatedAt against configured TTL\n    \n    // TODO 3: Return session if valid, or ErrSessionExpired if too old\n}\n```\n\n**E. Language-Specific Hints:**\n\n- **HTTP Request Handling**: Use `r.Header.Get()` for single headers, `r.Header[\"Header-Name\"]` for multiple values\n- **Body Reading**: Always use `io.LimitReader(r.Body, maxSize)` to prevent memory exhaustion attacks\n- **Atomic Operations**: Use `sync.Mutex` around session state changes, not just reads\n- **Error Handling**: Create custom error types with `errors.New()` for protocol-specific errors\n- **Context Usage**: Pass `context.Context` through all functions for cancellation and timeouts\n- **File I/O**: Use `os.OpenFile()` with `O_CREATE|O_WRONLY` flags for chunk writing\n- **Hashing**: Stream data through `hash.Hash.Write()` while processing to avoid double I/O\n\n**F. Milestone Checkpoint:**\n\nAfter implementing the chunked upload protocol:\n\n1. **Start the server**: `go run cmd/upload-server/main.go`\n2. **Initialize upload**: \n   ```bash\n   curl -X POST http://localhost:8080/files \\\n     -H \"Upload-Length: 1000\" \\\n     -H \"Upload-Metadata: filename dGVzdC50eHQ=\" \\\n     -H \"Tus-Resumable: 1.0.0\"\n   ```\n   Expected: `201 Created` with `Location` header containing session URL\n\n3. **Upload chunk**:\n   ```bash\n   curl -X PATCH http://localhost:8080/files/{session-id} \\\n     -H \"Upload-Offset: 0\" \\\n     -H \"Content-Type: application/offset+octet-stream\" \\\n     -H \"Tus-Resumable: 1.0.0\" \\\n     --data-binary @test-chunk.bin\n   ```\n   Expected: `204 No Content` with updated `Upload-Offset` header\n\n4. **Check progress**:\n   ```bash\n   curl -I http://localhost:8080/files/{session-id} \\\n     -H \"Tus-Resumable: 1.0.0\"\n   ```\n   Expected: `200 OK` with current `Upload-Offset` and `Upload-Length` headers\n\n**Signs of Success**: Session IDs are generated consistently, offset tracking advances correctly, chunks are written to storage, HEAD requests return accurate progress information.\n\n**Common Issues**: Offset mismatches (check atomic session updates), memory leaks (ensure body reading limits), race conditions (verify mutex usage around session state).\n\n\n## Storage Backend Abstraction\n\n> **Milestone(s):** Milestone 2 (Storage Abstraction) — this section implements pluggable storage backends with unified operations for local filesystem, S3, and GCS\n\nThink of storage backend abstraction as a **universal remote control** for different storage systems. Just as you can use the same remote interface (power, volume, channel buttons) to control different TV brands without knowing their internal protocols, our storage abstraction provides the same operations (read, write, delete) across different storage systems without exposing their implementation differences. The abstraction layer translates common operations into the specific API calls and protocols required by each backend, whether that's POSIX file operations for local storage or REST API calls for cloud object stores.\n\n![Storage Backend Class Hierarchy](./diagrams/storage-abstraction-class.svg)\n\nThe storage abstraction serves as the foundation that enables the resumable upload service to be deployment-agnostic. Whether running on a single server with local disk storage or in a distributed cloud environment with object storage, the upload service logic remains unchanged. This abstraction becomes particularly critical during chunked uploads where we need to coordinate multipart operations across potentially unreliable network connections while maintaining consistency guarantees.\n\n### Storage Interface Design\n\nThe storage interface represents the contract that all backend implementations must fulfill. Think of this interface as a **standardized shipping container** — regardless of whether containers travel by truck, ship, or train, they maintain the same dimensions and connection points. Similarly, our storage interface defines standard operations that work identically whether data travels to local disks, S3 buckets, or other cloud storage systems.\n\nThe interface design must balance simplicity with the diverse capabilities of different storage systems. Local filesystems provide atomic operations and immediate consistency, while object storage systems like S3 offer eventual consistency and require different patterns for multipart uploads. The abstraction must present a unified view while allowing backends to optimize for their specific characteristics.\n\n> **Decision: Streaming Interface with Context Support**\n> - **Context**: Upload chunks can be large (several MB) and network operations may timeout or be cancelled\n> - **Options Considered**: \n>   1. Byte slice interface (simple but requires full data in memory)\n>   2. `io.Reader` interface (streaming but no cancellation support)\n>   3. Context-aware streaming interface (complex but supports cancellation and timeouts)\n> - **Decision**: Use context-aware streaming interface with `io.Reader` for data\n> - **Rationale**: Large file uploads require streaming to avoid memory exhaustion, and production systems need timeout and cancellation support for resource management\n> - **Consequences**: More complex implementation but enables memory-efficient uploads with proper resource cleanup\n\n| Method Name | Parameters | Returns | Description |\n|-------------|------------|---------|-------------|\n| `StoreChunk` | `ctx Context, key string, data io.Reader, size int64` | `error` | Stores chunk data at the specified key with known size for validation |\n| `ReadChunk` | `ctx Context, key string` | `io.ReadCloser, error` | Returns a reader for chunk data, caller must close |\n| `DeleteChunk` | `ctx Context, key string` | `error` | Removes chunk data, idempotent operation |\n| `ChunkExists` | `ctx Context, key string` | `bool, error` | Checks if chunk exists without retrieving data |\n| `InitMultipart` | `ctx Context, key string, metadata map[string]string` | `MultipartUpload, error` | Begins multipart upload session for large files |\n| `StoreMultipartChunk` | `ctx Context, upload MultipartUpload, partNum int, data io.Reader, size int64` | `MultipartPart, error` | Stores one part of multipart upload |\n| `CompleteMultipart` | `ctx Context, upload MultipartUpload, parts []MultipartPart` | `error` | Assembles all parts into final file |\n| `AbortMultipart` | `ctx Context, upload MultipartUpload` | `error` | Cancels multipart upload and cleans up parts |\n| `GenerateSignedURL` | `ctx Context, key string, expiration Duration` | `string, error` | Creates time-limited download URL for secure access |\n| `ListKeys` | `ctx Context, prefix string, limit int` | `[]string, error` | Returns keys matching prefix for cleanup operations |\n\nThe interface separates simple chunk operations from multipart upload workflows because different backends have vastly different multipart capabilities. Local storage can simulate multipart uploads by writing to temporary files, while S3 has native multipart upload APIs with specific constraints (minimum part size, maximum parts, etc.).\n\n> The key design insight is that the interface exposes multipart upload explicitly rather than hiding it behind simple operations. This allows backends to optimize for their native capabilities while giving the upload service control over the multipart workflow.\n\n**Multipart Upload Data Structures**\n\nThe multipart upload abstraction requires additional data structures to track upload sessions and parts across different backends:\n\n| Structure | Field | Type | Description |\n|-----------|-------|------|-------------|\n| `MultipartUpload` | `ID` | `string` | Backend-specific upload identifier |\n| `MultipartUpload` | `Key` | `string` | Final object key where assembled file will be stored |\n| `MultipartUpload` | `Backend` | `string` | Storage backend that created this upload |\n| `MultipartUpload` | `Metadata` | `map[string]string` | Custom metadata to attach to final object |\n| `MultipartUpload` | `CreatedAt` | `time.Time` | Upload session creation timestamp |\n| `MultipartPart` | `PartNumber` | `int` | Sequential part number starting from 1 |\n| `MultipartPart` | `ETag` | `string` | Backend-provided integrity checksum |\n| `MultipartPart` | `Size` | `int64` | Number of bytes in this part |\n| `MultipartPart` | `PartID` | `string` | Backend-specific part identifier |\n\nEach backend translates these abstract structures to their native representations. For S3, the `MultipartUpload.ID` becomes the S3 upload ID, and `MultipartPart.ETag` contains the S3 ETag header. For local storage, the upload ID might be a temporary directory name, and the ETag could be a computed MD5 hash.\n\n**Error Classification and Handling**\n\nStorage backends can fail in different ways, and the interface must provide enough information for the upload service to make intelligent retry and recovery decisions:\n\n| Error Type | Detection | Recovery Strategy | Example Scenarios |\n|------------|-----------|------------------|-------------------|\n| `ErrNotFound` | Key does not exist | Retry upload from beginning | Chunk lost during cleanup |\n| `ErrAlreadyExists` | Key collision during creation | Generate new key or resume | Concurrent upload to same path |\n| `ErrInsufficientSpace` | Disk full or quota exceeded | Fail gracefully, retry with different backend | Local disk full |\n| `ErrNetworkTimeout` | Network operation timeout | Exponential backoff retry | S3 API timeout |\n| `ErrAuthenticationFailed` | Invalid credentials | Fail immediately, check configuration | Expired AWS credentials |\n| `ErrInvalidMultipartState` | Part uploaded to wrong session | Abort upload, restart | Client/server state mismatch |\n\n### Local Filesystem Backend\n\nThe local filesystem backend implements the storage interface using POSIX file operations. Think of this backend as a **traditional filing cabinet** — it provides immediate access to documents, strong consistency guarantees, and simple organization, but it's limited to the capacity and reliability of a single physical location.\n\nLocal storage serves multiple purposes in the resumable upload service: development and testing environments, single-server deployments, and as a quarantine storage area for suspicious files before cloud upload. The implementation must handle the unique characteristics of filesystem operations while providing the same interface as cloud storage backends.\n\n> **Decision: Directory-Based Organization with Atomic Operations**\n> - **Context**: Local filesystem needs to organize chunks, track multipart uploads, and ensure consistency across process restarts\n> - **Options Considered**: \n>   1. Flat file storage with encoded keys (simple but hard to manage)\n>   2. Directory hierarchy mirroring object keys (intuitive but complex path handling)\n>   3. Separate directories for chunks, multipart sessions, and completed files (clean separation)\n> - **Decision**: Use separate directories with atomic operations via temporary files\n> - **Rationale**: Clean separation simplifies cleanup, atomic operations prevent corruption, predictable layout aids debugging\n> - **Consequences**: More complex path management but better reliability and maintainability\n\n**Directory Structure and Organization**\n\nThe local backend organizes files in a structured directory hierarchy that separates different types of data and supports efficient cleanup operations:\n\n```\nstorage-root/\n├── chunks/           ← Single-chunk uploads\n│   └── session-id/\n│       └── chunk-hash\n├── multipart/        ← Multipart upload sessions\n│   └── upload-id/\n│       ├── metadata.json\n│       ├── parts/\n│       │   ├── part-001\n│       │   ├── part-002\n│       │   └── ...\n│       └── assembly/  ← Temporary assembly area\n└── completed/        ← Finalized files ready for download\n    └── final-key\n```\n\nThis structure provides several benefits: chunks are isolated by session to prevent conflicts, multipart uploads maintain their parts in predictable locations, and completed files are separated for easy cleanup and access control. The assembly directory provides a workspace for combining multipart uploads atomically.\n\n**Path Safety and Security Considerations**\n\nLocal filesystem backends face unique security challenges because malicious clients could potentially manipulate file paths to escape the designated storage area. The implementation must sanitize all path components and prevent directory traversal attacks:\n\n| Security Concern | Threat | Mitigation Strategy |\n|------------------|--------|-------------------|\n| Directory Traversal | `../../../etc/passwd` in keys | Reject keys containing `..`, `/`, or `\\` |\n| Null Byte Injection | `valid-key\\x00.txt` truncation | Validate keys contain only printable ASCII |\n| Long Path Attacks | Extremely long filenames | Limit key length and use hash-based names |\n| Symlink Exploitation | Symbolic links to sensitive files | Use `O_NOFOLLOW` and validate real paths |\n| Case Sensitivity | `Key` vs `key` collisions on macOS | Normalize keys to lowercase with hash suffix |\n\nThe path sanitization algorithm follows these steps:\n1. Validate the key contains only alphanumeric characters, hyphens, underscores, and periods\n2. Check the key length does not exceed 255 characters (filesystem limit)\n3. Hash the key with SHA-256 to create a safe filename\n4. Combine the hash with a truncated version of the original key for debugging\n5. Construct the full path within the designated storage root\n6. Verify the resolved path remains within the storage directory\n\n**Atomic Operations and Consistency**\n\nFilesystem operations must be atomic to prevent corruption during process crashes or concurrent access. The local backend uses the **write-then-rename** pattern for all file modifications:\n\n1. **Temporary File Creation**: Write new data to a temporary file with a `.tmp` extension in the same directory as the final destination\n2. **Content Verification**: Verify the written data matches expected size and checksum\n3. **Atomic Rename**: Use `os.Rename()` to atomically move the temporary file to its final location\n4. **Cleanup on Failure**: Remove temporary files if any step fails\n\nThis pattern ensures that readers never see partially written files, and process crashes cannot leave the storage in an inconsistent state. The temporary files use unique names (UUID or timestamp-based) to prevent conflicts during concurrent uploads.\n\n**Multipart Upload Implementation**\n\nLocal storage simulates multipart uploads using a staging area that accumulates parts before final assembly. The process involves several coordinated steps:\n\n1. **Session Initialization**: Create a directory under `multipart/` with the upload ID, write metadata file with target key and part information\n2. **Part Storage**: Store each part as a numbered file (`part-001`, `part-002`) with size validation and checksum computation\n3. **Assembly Process**: When all parts are received, stream-copy them in order to a temporary file in the assembly directory\n4. **Integrity Verification**: Compute final file checksum and compare against expected value from client\n5. **Atomic Completion**: Rename the assembled file to its final location under `completed/`\n\nThe assembly process handles large files efficiently by streaming parts rather than loading them entirely into memory. Parts are read in sequence and written to the output file using buffered I/O with configurable buffer sizes (typically 64KB to 1MB).\n\n**Common Pitfalls**\n\n⚠️ **Pitfall: Filesystem Permission Races**\nMany implementations fail to handle filesystem permission changes that occur between checking permissions and performing operations. The service process might lose write access to directories while uploads are in progress, causing cryptic failures.\n\n**Why it's wrong**: Checking permissions separately from file operations creates a race condition where permissions can change between the check and the operation.\n\n**How to fix**: Attempt file operations directly and handle permission errors explicitly. Use the file operation result as the definitive permission check rather than pre-checking with `os.Access()`.\n\n⚠️ **Pitfall: Incomplete Cleanup of Temporary Files**\nFailed uploads often leave temporary files scattered throughout the storage directories, gradually consuming disk space and creating operational problems.\n\n**Why it's wrong**: Process crashes or panics can interrupt cleanup code, and deferred cleanup may not execute if the process is killed forcefully.\n\n**How to fix**: Implement a separate cleanup process that scans for temporary files older than a threshold (e.g., 24 hours) and removes them. Use file modification times and naming patterns to identify orphaned temporaries safely.\n\n### S3-Compatible Backend\n\nThe S3-compatible backend integrates with cloud object storage services that implement the S3 API, including Amazon S3, MinIO, Google Cloud Storage (via S3 compatibility), and other providers. Think of S3 as a **global warehouse network** — it provides virtually unlimited capacity and geographic distribution, but operations have higher latency and follow eventual consistency models rather than immediate consistency.\n\nObject storage systems fundamentally differ from filesystems in their consistency models, API patterns, and operational characteristics. The S3 backend must navigate these differences while providing the same interface as local storage, handling network failures gracefully, and optimizing for S3's specific multipart upload requirements.\n\n> **Decision: Native S3 Multipart Upload Integration**\n> - **Context**: S3 has specific multipart upload constraints (5MB minimum part size, 10,000 part limit) and provides native APIs for managing multipart sessions\n> - **Options Considered**: \n>   1. Simulate multipart by uploading parts as separate objects then copying (inefficient)\n>   2. Buffer small parts until they reach 5MB minimum (memory intensive)\n>   3. Use native S3 multipart upload APIs with part size adaptation (complex but efficient)\n> - **Decision**: Use native S3 multipart APIs with automatic part size adjustment\n> - **Rationale**: Native multipart provides better performance, reliability, and cost optimization compared to workarounds\n> - **Consequences**: Implementation complexity increases but provides optimal S3 integration and cost efficiency\n\n**S3 Multipart Upload Constraints and Adaptations**\n\nS3 multipart uploads have specific requirements that the backend must handle transparently:\n\n| S3 Constraint | Requirement | Backend Adaptation |\n|---------------|-------------|-------------------|\n| Minimum Part Size | 5MB except for last part | Buffer small chunks until 5MB threshold |\n| Maximum Parts | 10,000 parts per upload | Calculate optimal part size based on total file size |\n| Part Number Range | 1 to 10,000 inclusive | Map client part numbers to valid S3 range |\n| ETag Requirement | Each part returns ETag for completion | Store ETags with part metadata for assembly |\n| Upload Timeout | 7 days maximum session lifetime | Implement session refresh and cleanup |\n\nThe backend automatically calculates optimal part sizes based on the total file size to stay within the 10,000 part limit:\n\n1. **Size-Based Part Calculation**: For files larger than 50GB (10,000 × 5MB), increase part size to file_size ÷ 9,999 rounded up to next MB\n2. **Buffer Management**: Accumulate client chunks in memory until reaching the target part size, then upload to S3\n3. **Final Part Handling**: Upload remaining buffered data as the final part regardless of size\n4. **Memory Optimization**: Use streaming uploads with configurable buffer limits to prevent excessive memory usage\n\n**Credential Management and Authentication**\n\nS3 backends require secure credential management that supports multiple authentication methods while protecting sensitive information:\n\n| Authentication Method | Use Case | Configuration |\n|----------------------|----------|---------------|\n| Static Access Keys | Development, simple deployments | `AccessKeyID` and `SecretAccessKey` in config |\n| IAM Instance Roles | EC2 deployments | No credentials in config, use instance metadata |\n| IAM Assume Role | Cross-account access, temporary credentials | Role ARN with automatic token refresh |\n| Environment Variables | Container deployments | `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY` |\n\nThe backend implements a credential provider chain that attempts authentication methods in order of preference:\n\n1. **Explicit Configuration**: Use credentials from `StorageConfig.S3Config` if provided\n2. **Environment Variables**: Check standard AWS environment variables\n3. **Instance Metadata**: Query EC2 instance metadata service for role credentials\n4. **Credential File**: Read from `~/.aws/credentials` file\n5. **Error on Failure**: Return authentication error if no valid credentials found\n\nCredentials are refreshed automatically before expiration, and the backend handles temporary credential rotation transparently to the upload service.\n\n**Network Resilience and Error Handling**\n\nCloud storage operations face network partitions, API rate limits, and transient failures that require sophisticated retry and backoff strategies:\n\n| Failure Type | S3 Error Code | Retry Strategy | Backoff Pattern |\n|--------------|---------------|----------------|-----------------|\n| Rate Limiting | `SlowDown`, `ServiceUnavailable` | Exponential backoff | Start 1s, max 60s |\n| Network Timeout | Connection timeout | Immediate retry | 3 attempts, then fail |\n| Invalid Credentials | `InvalidAccessKeyId` | No retry | Fail immediately |\n| Temporary Server Error | `InternalError` | Exponential backoff | Start 500ms, max 30s |\n| Part Size Error | `EntityTooSmall` | Retry with larger parts | No backoff |\n\nThe retry implementation uses jittered exponential backoff to prevent thundering herd problems when multiple upload sessions encounter rate limits simultaneously. Each retry attempt includes a random delay component (±25% of the calculated backoff) to distribute load across time.\n\n**S3-Specific Optimizations**\n\nThe S3 backend includes several optimizations that leverage S3's unique capabilities:\n\n**Parallel Part Uploads**: Upload multiple parts concurrently using a worker pool with configurable concurrency limits (typically 3-5 parallel uploads). This dramatically improves throughput for large files while respecting API rate limits.\n\n**Transfer Acceleration**: When enabled, use S3 Transfer Acceleration endpoints that route uploads through Amazon's global edge network for improved performance from distant geographic locations.\n\n**Storage Class Selection**: Configure default storage classes (Standard, IA, Glacier) based on upload metadata or file characteristics. For example, files marked as archival can use cheaper storage classes automatically.\n\n**Multipart Upload Lifecycle**: Register lifecycle policies that automatically clean up incomplete multipart uploads after a specified period (typically 7 days) to prevent storage cost accumulation from abandoned uploads.\n\n**Presigned URL Generation**: Generate presigned URLs for downloads that allow direct client access to S3 without proxying through the upload service, reducing bandwidth costs and improving performance.\n\n### Credential and Configuration Management\n\nCredential management represents one of the most critical security aspects of the storage abstraction. Think of credential management as a **secure keychain system** — it must protect sensitive authentication information while making it readily available to authorized operations. The system must handle credential rotation, multiple backend types, and different deployment environments without compromising security or operational reliability.\n\nThe configuration system must be flexible enough to support development environments with simple file-based config, containerized deployments with environment variables, and production systems with secure credential stores like HashiCorp Vault or AWS Secrets Manager.\n\n> **Decision: Pluggable Credential Provider with Fallback Chain**\n> - **Context**: Different deployment environments need different credential sources (files, environment, cloud metadata, external secrets)\n> - **Options Considered**: \n>   1. Single configuration file with all credentials (simple but insecure)\n>   2. Environment variables only (secure but inflexible)\n>   3. Pluggable provider chain with multiple fallback options (complex but comprehensive)\n> - **Decision**: Implement pluggable credential providers with configurable fallback chain\n> - **Rationale**: Different environments have different security requirements and credential availability\n> - **Consequences**: More complex implementation but supports secure deployment across diverse environments\n\n**Configuration Structure and Validation**\n\nThe configuration system uses a layered approach where each storage backend can specify its requirements while sharing common patterns for credential management:\n\n| Configuration Layer | Responsibility | Example Content |\n|-------------------|----------------|-----------------|\n| Global Storage Config | Backend selection, common settings | `Backend: \"s3\"`, `CredentialProvider: \"vault\"` |\n| Backend-Specific Config | Backend parameters | S3 bucket, region, endpoint configuration |\n| Credential Provider Config | Authentication details | Vault address, credential paths, rotation settings |\n| Runtime Credential Cache | Active credentials | Cached tokens, expiration times, refresh state |\n\nThe configuration validation ensures that all required parameters are present and compatible:\n\n1. **Backend Validation**: Verify the selected backend is supported and properly configured\n2. **Credential Provider Validation**: Ensure the credential provider can authenticate with required services\n3. **Permission Validation**: Test that provided credentials have necessary permissions for upload operations\n4. **Network Validation**: Verify connectivity to storage backend endpoints\n5. **Capacity Validation**: Check storage quotas and available space where applicable\n\n**Credential Provider Interface**\n\nThe credential provider abstraction enables different authentication backends while maintaining a consistent interface:\n\n| Provider Method | Parameters | Returns | Description |\n|----------------|------------|---------|-------------|\n| `GetCredentials` | `ctx Context, backend string` | `Credentials, error` | Retrieves current valid credentials |\n| `RefreshCredentials` | `ctx Context, backend string` | `Credentials, error` | Forces credential refresh from source |\n| `SubscribeRotation` | `ctx Context, backend string` | `<-chan Credentials` | Returns channel for credential rotation events |\n| `ValidateAccess` | `ctx Context, backend string, creds Credentials` | `error` | Tests if credentials have required permissions |\n\nDifferent credential providers implement this interface to support various authentication sources:\n\n**File-Based Provider**: Reads credentials from JSON or YAML configuration files with optional encryption. Suitable for development and simple deployments where credential rotation is handled externally.\n\n**Environment Provider**: Sources credentials from environment variables following standard naming conventions (e.g., `AWS_ACCESS_KEY_ID`, `GCS_SERVICE_ACCOUNT_KEY`). Common in containerized deployments and CI/CD systems.\n\n**Vault Provider**: Integrates with HashiCorp Vault to retrieve dynamic credentials with automatic rotation. Provides audit logging and centralized credential management for enterprise deployments.\n\n**Cloud Metadata Provider**: Uses cloud platform metadata services (AWS IAM roles, GCP service accounts, Azure managed identities) for credential-less authentication in cloud deployments.\n\n**Credential Rotation and Refresh**\n\nProduction deployments require automatic credential rotation to maintain security without service interruption. The credential management system implements proactive refresh strategies:\n\n| Rotation Trigger | Detection Method | Refresh Strategy | Fallback Behavior |\n|------------------|------------------|------------------|------------------|\n| Time-Based Expiration | Check credential expiry timestamps | Refresh at 80% of lifetime | Use backup credentials during refresh |\n| Authentication Failure | Detect auth errors from storage APIs | Immediate refresh attempt | Retry with refreshed credentials |\n| External Rotation Signal | Webhook or message queue notification | Scheduled refresh within 5 minutes | Queue operations until refresh completes |\n| Periodic Validation | Regular permission testing | Refresh if validation fails | Alert operations team |\n\nThe refresh process follows these steps:\n1. **Background Refresh**: Start credential refresh in background before expiration\n2. **Atomic Update**: Replace cached credentials atomically to prevent partial updates\n3. **Connection Drain**: Allow existing connections to complete with old credentials\n4. **New Connection Auth**: Authenticate new connections with refreshed credentials\n5. **Cleanup**: Securely clear old credentials from memory\n\n**Multi-Backend Configuration**\n\nComplex deployments may use multiple storage backends simultaneously (e.g., local storage for quarantine, S3 for primary storage, GCS for backup). The configuration system supports multiple backend definitions with different credential requirements:\n\n```\nStorageBackends:\n  primary:\n    type: s3\n    config: {bucket: uploads-prod, region: us-west-2}\n    credentials: {provider: vault, path: secret/aws/s3}\n  quarantine:\n    type: local\n    config: {path: /var/quarantine}\n    credentials: {provider: none}\n  backup:\n    type: gcs\n    config: {bucket: uploads-backup}\n    credentials: {provider: environment}\n```\n\nThe upload service can route operations to different backends based on upload metadata, file types, or security policies. For example, suspicious files detected during virus scanning might be routed to local quarantine storage while clean files go to the primary S3 backend.\n\n**Common Pitfalls**\n\n⚠️ **Pitfall: Credential Caching Without Expiration**\nMany implementations cache credentials indefinitely to avoid repeated authentication overhead, leading to authentication failures when credentials expire or rotate.\n\n**Why it's wrong**: Cached credentials eventually expire, and services that don't handle expiration gracefully experience sudden authentication failures that can cause widespread upload failures.\n\n**How to fix**: Always store credential expiration times and implement proactive refresh at 80% of credential lifetime. Include fallback logic that attempts credential refresh when authentication operations fail.\n\n⚠️ **Pitfall: Embedding Credentials in Configuration Files**\nDevelopment configurations often include hardcoded credentials that accidentally get committed to version control or deployed to production environments.\n\n**Why it's wrong**: Credentials in configuration files can be exposed through version control history, configuration management systems, or file system access, creating significant security vulnerabilities.\n\n**How to fix**: Use credential providers that source authentication information from secure stores (environment variables, vault systems, cloud metadata) rather than embedding credentials directly in configuration files. Implement configuration validation that rejects embedded credentials in production environments.\n\n⚠️ **Pitfall: Single Point of Failure in Credential Management**\nRelying on a single credential source (like a vault server) can cause complete service outage if that system becomes unavailable.\n\n**Why it's wrong**: If the credential provider fails, the entire upload service becomes unable to authenticate with storage backends, causing all uploads to fail even if the storage systems themselves are healthy.\n\n**How to fix**: Implement credential provider fallback chains with multiple sources. Cache valid credentials locally with reasonable expiration times to survive short credential provider outages. Include monitoring and alerting for credential provider health.\n\n### Implementation Guidance\n\n**Technology Recommendations**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| S3 Client | AWS SDK v2 (github.com/aws/aws-sdk-go-v2) | Custom HTTP client with connection pooling |\n| Local File I/O | Standard os package with ioutil helpers | Memory-mapped files for large uploads |\n| Credential Storage | Environment variables + file config | HashiCorp Vault integration |\n| Configuration Management | JSON files with validation | Viper with multiple source support |\n| Error Retry Logic | Simple exponential backoff | github.com/cenkalti/backoff with jitter |\n\n**Recommended File Structure**\n\n```\ninternal/storage/\n├── storage.go              ← Storage interface definitions\n├── errors.go               ← Storage-specific error types\n├── config.go               ← Configuration structures and validation\n├── credentials/            ← Credential provider implementations\n│   ├── provider.go         ← Credential provider interface\n│   ├── env_provider.go     ← Environment variable provider\n│   ├── file_provider.go    ← File-based credential provider\n│   └── vault_provider.go   ← HashiCorp Vault provider (advanced)\n├── backends/               ← Storage backend implementations\n│   ├── local/              ← Local filesystem backend\n│   │   ├── local.go        ← Main implementation\n│   │   ├── local_test.go   ← Unit tests\n│   │   └── multipart.go    ← Multipart upload simulation\n│   ├── s3/                 ← S3-compatible backend\n│   │   ├── s3.go           ← Main implementation\n│   │   ├── s3_test.go      ← Unit tests\n│   │   ├── multipart.go    ← S3 multipart upload integration\n│   │   └── credentials.go  ← S3-specific credential handling\n│   └── memory/             ← In-memory backend for testing\n│       ├── memory.go       ← Test implementation\n│       └── memory_test.go  ← Unit tests\n└── factory.go              ← Backend factory and registration\n```\n\n**Infrastructure Starter Code**\n\nHere's complete starter code for the storage interface and basic error handling:\n\n```go\n// internal/storage/storage.go\npackage storage\n\nimport (\n    \"context\"\n    \"io\"\n    \"time\"\n)\n\n// StorageBackend defines the interface that all storage implementations must satisfy.\n// It provides both simple chunk operations and multipart upload capabilities.\ntype StorageBackend interface {\n    // Simple chunk operations for small uploads\n    StoreChunk(ctx context.Context, key string, data io.Reader, size int64) error\n    ReadChunk(ctx context.Context, key string) (io.ReadCloser, error)\n    DeleteChunk(ctx context.Context, key string) error\n    ChunkExists(ctx context.Context, key string) (bool, error)\n    \n    // Multipart upload operations for large files\n    InitMultipart(ctx context.Context, key string, metadata map[string]string) (*MultipartUpload, error)\n    StoreMultipartChunk(ctx context.Context, upload *MultipartUpload, partNum int, data io.Reader, size int64) (*MultipartPart, error)\n    CompleteMultipart(ctx context.Context, upload *MultipartUpload, parts []*MultipartPart) error\n    AbortMultipart(ctx context.Context, upload *MultipartUpload) error\n    \n    // Additional operations\n    GenerateSignedURL(ctx context.Context, key string, expiration time.Duration) (string, error)\n    ListKeys(ctx context.Context, prefix string, limit int) ([]string, error)\n}\n\n// MultipartUpload represents an ongoing multipart upload session\ntype MultipartUpload struct {\n    ID       string            `json:\"id\"`\n    Key      string            `json:\"key\"`\n    Backend  string            `json:\"backend\"`\n    Metadata map[string]string `json:\"metadata\"`\n    CreatedAt time.Time        `json:\"created_at\"`\n}\n\n// MultipartPart represents one part of a multipart upload\ntype MultipartPart struct {\n    PartNumber int    `json:\"part_number\"`\n    ETag       string `json:\"etag\"`\n    Size       int64  `json:\"size\"`\n    PartID     string `json:\"part_id\"`\n}\n\n// BackendFactory creates storage backend instances based on configuration\ntype BackendFactory interface {\n    CreateBackend(config StorageConfig) (StorageBackend, error)\n    SupportedBackends() []string\n}\n```\n\n```go\n// internal/storage/errors.go\npackage storage\n\nimport \"errors\"\n\nvar (\n    ErrNotFound = errors.New(\"key not found\")\n    ErrAlreadyExists = errors.New(\"key already exists\")\n    ErrInsufficientSpace = errors.New(\"insufficient storage space\")\n    ErrNetworkTimeout = errors.New(\"network operation timeout\")\n    ErrAuthenticationFailed = errors.New(\"authentication failed\")\n    ErrInvalidMultipartState = errors.New(\"invalid multipart upload state\")\n)\n\n// StorageError wraps storage operation errors with additional context\ntype StorageError struct {\n    Operation string\n    Key       string\n    Backend   string\n    Err       error\n}\n\nfunc (e *StorageError) Error() string {\n    return fmt.Sprintf(\"storage %s failed for key %s on backend %s: %v\", \n        e.Operation, e.Key, e.Backend, e.Err)\n}\n\nfunc (e *StorageError) Unwrap() error {\n    return e.Err\n}\n```\n\n```go\n// internal/storage/credentials/provider.go\npackage credentials\n\nimport (\n    \"context\"\n    \"time\"\n)\n\n// Credentials represents authentication information for storage backends\ntype Credentials struct {\n    AccessKeyID     string    `json:\"access_key_id,omitempty\"`\n    SecretAccessKey string    `json:\"secret_access_key,omitempty\"`\n    SessionToken    string    `json:\"session_token,omitempty\"`\n    ExpiresAt       time.Time `json:\"expires_at,omitempty\"`\n    Region          string    `json:\"region,omitempty\"`\n}\n\n// IsExpired returns true if credentials have expired\nfunc (c *Credentials) IsExpired() bool {\n    if c.ExpiresAt.IsZero() {\n        return false // No expiration set\n    }\n    return time.Now().After(c.ExpiresAt)\n}\n\n// CredentialProvider defines the interface for credential sources\ntype CredentialProvider interface {\n    GetCredentials(ctx context.Context, backend string) (*Credentials, error)\n    RefreshCredentials(ctx context.Context, backend string) (*Credentials, error)\n}\n```\n\n**Core Logic Skeleton Code**\n\nHere's the skeleton for the local storage backend that learners should implement:\n\n```go\n// internal/storage/backends/local/local.go\npackage local\n\nimport (\n    \"context\"\n    \"io\"\n    \"os\"\n    \"path/filepath\"\n    \"sync\"\n    \"time\"\n    \n    \"your-project/internal/storage\"\n)\n\n// LocalBackend implements StorageBackend using local filesystem\ntype LocalBackend struct {\n    rootPath string\n    mutex    sync.RWMutex\n}\n\n// NewLocalBackend creates a new local filesystem storage backend\nfunc NewLocalBackend(rootPath string) (*LocalBackend, error) {\n    // TODO 1: Validate rootPath exists and is writable\n    // TODO 2: Create subdirectories (chunks/, multipart/, completed/) if they don't exist\n    // TODO 3: Set proper directory permissions (0755)\n    // TODO 4: Return configured LocalBackend instance\n    return nil, nil\n}\n\n// StoreChunk stores chunk data at the specified key\nfunc (l *LocalBackend) StoreChunk(ctx context.Context, key string, data io.Reader, size int64) error {\n    // TODO 1: Sanitize key to prevent directory traversal attacks\n    // TODO 2: Generate safe filename using hash of key\n    // TODO 3: Create temporary file in chunks/ directory\n    // TODO 4: Copy data from reader to temporary file with size validation\n    // TODO 5: Sync file to disk for durability\n    // TODO 6: Atomically rename temporary file to final name\n    // TODO 7: Clean up temporary file on any error\n    // Hint: Use filepath.Join and validate result stays within rootPath\n    return nil\n}\n\n// InitMultipart begins a multipart upload session\nfunc (l *LocalBackend) InitMultipart(ctx context.Context, key string, metadata map[string]string) (*storage.MultipartUpload, error) {\n    // TODO 1: Generate unique upload ID (UUID)\n    // TODO 2: Create directory structure: multipart/{uploadID}/parts/\n    // TODO 3: Write metadata.json file with upload information\n    // TODO 4: Return MultipartUpload struct with generated ID\n    // Hint: Use os.MkdirAll for directory creation\n    return nil, nil\n}\n\n// StoreMultipartChunk stores one part of a multipart upload\nfunc (l *LocalBackend) StoreMultipartChunk(ctx context.Context, upload *storage.MultipartUpload, partNum int, data io.Reader, size int64) (*storage.MultipartPart, error) {\n    // TODO 1: Validate upload exists and partNum is valid (1-10000)\n    // TODO 2: Create part file path: multipart/{uploadID}/parts/part-{partNum:03d}\n    // TODO 3: Store part data using atomic write (temp + rename)\n    // TODO 4: Compute ETag (MD5 hash) of part data\n    // TODO 5: Return MultipartPart with part metadata\n    // Hint: Pad part numbers with zeros for consistent sorting\n    return nil, nil\n}\n\n// CompleteMultipart assembles all parts into final file\nfunc (l *LocalBackend) CompleteMultipart(ctx context.Context, upload *storage.MultipartUpload, parts []*storage.MultipartPart) error {\n    // TODO 1: Validate all parts are present and in sequence\n    // TODO 2: Create temporary assembly file in multipart/{uploadID}/assembly/\n    // TODO 3: Stream-copy each part file to assembly file in order\n    // TODO 4: Verify final file size matches sum of part sizes\n    // TODO 5: Atomically move assembled file to completed/{key}\n    // TODO 6: Clean up multipart directory and part files\n    // Hint: Use io.Copy with limited reader to avoid loading entire file in memory\n    return nil\n}\n```\n\n**Language-Specific Hints**\n\n**Go-Specific Implementation Tips:**\n- Use `os.OpenFile` with `O_CREATE|O_EXCL` flags for atomic file creation\n- Call `file.Sync()` after writing critical data to ensure durability\n- Use `filepath.Clean` and `filepath.IsAbs` to validate file paths\n- Implement proper cleanup with `defer` statements for file handles\n- Use `io.LimitReader` when copying data with known size limits\n- Handle context cancellation by checking `ctx.Done()` in long operations\n\n**Error Handling Patterns:**\n- Wrap filesystem errors with `storage.StorageError` for consistent error handling\n- Use `os.IsNotExist(err)` to detect missing files reliably\n- Check disk space with `syscall.Statfs()` before large write operations\n- Implement exponential backoff for temporary filesystem errors\n\n**Milestone Checkpoint**\n\nAfter implementing the storage abstraction:\n\n**Test Commands:**\n```bash\n# Run storage backend tests\ngo test ./internal/storage/...\n\n# Run integration tests with different backends\nSTORAGE_BACKEND=local go test ./internal/storage/backends/local/...\nSTORAGE_BACKEND=s3 S3_ENDPOINT=http://localhost:9000 go test ./internal/storage/backends/s3/...\n```\n\n**Expected Behavior:**\n1. **Local Backend**: Should create directory structure under configured root path, store and retrieve chunks correctly, handle multipart uploads with proper part assembly\n2. **S3 Backend**: Should authenticate with configured credentials, create multipart uploads with valid ETags, handle S3-specific errors gracefully\n3. **Configuration**: Should load from files, validate required parameters, fail clearly when credentials are missing or invalid\n\n**Manual Verification:**\n```bash\n# Start test server with local backend\n./upload-service --config configs/local.json\n\n# Upload small file (single chunk)\ncurl -X POST http://localhost:8080/uploads \\\n  -H \"Upload-Length: 1024\" \\\n  -H \"Content-Type: application/octet-stream\" \\\n  --data-binary @testfile.bin\n\n# Verify file appears in storage directory\nls -la storage/completed/\n\n# Upload large file (multipart)\ncurl -X POST http://localhost:8080/uploads \\\n  -H \"Upload-Length: 10485760\" \\  # 10MB\n  -H \"Content-Type: application/octet-stream\"\n# Note upload ID from response, use for subsequent chunk uploads\n```\n\n**Common Issues and Debugging:**\n- **Permission Errors**: Check that storage directories have correct ownership and permissions (755 for directories, 644 for files)\n- **Path Traversal Failures**: Verify key sanitization rejects `../` sequences and produces consistent hash-based filenames\n- **S3 Authentication Failures**: Validate credentials with AWS CLI: `aws s3 ls s3://your-bucket --profile your-profile`\n- **Multipart Assembly Errors**: Check that part files are created in correct order and assembly process reads them sequentially\n\n\n## File Validation and Security\n\n> **Milestone(s):** Milestone 3 (Virus Scanning & Validation) — this section implements comprehensive file validation with type checking, virus scanning, and quarantine mechanisms\n\nThink of file validation as an **airport security checkpoint** for your file uploads. Just as airport security doesn't trust the boarding pass alone (could be forged) but instead scans your ID, X-rays your luggage, and runs security checks, our file validation system never trusts what the client claims about a file. Instead, it examines the actual file content, runs it through security scanners, and quarantines anything suspicious. The key insight is that **security validation must happen before files enter your trusted storage** — just like how airport security happens before you board the plane, not after you're already in the air.\n\nThe validation pipeline operates as a **multi-stage filter**, where each stage can reject the file and prevent it from reaching production storage. Files progress through type validation (checking the actual content signature), size enforcement (preventing resource exhaustion attacks), virus scanning (detecting malicious content), and finally either acceptance into clean storage or quarantine for suspicious content. This layered approach ensures that even if one validation stage has a bypass, other layers provide defense in depth.\n\n![File Validation and Scanning Pipeline](./diagrams/virus-scanning-flow.svg)\n\nThe architectural challenge is balancing **security thoroughness with performance**. Scanning every uploaded file for viruses takes time and resources, but skipping scans creates security vulnerabilities. Our solution uses asynchronous processing where files are initially stored in a temporary holding area, scanned in the background, and then either promoted to clean storage or moved to quarantine. This allows clients to complete their uploads quickly while ensuring no unscanned files reach production systems.\n\n### File Type Validation\n\nFile type validation functions as the **first line of defense** against malicious uploads disguised as innocent file types. Think of it as a **document inspector** who doesn't trust the label on a package but instead opens it up and examines the actual contents. A malicious executable might be renamed from `virus.exe` to `document.pdf`, but the file's internal structure still contains the executable's signature bytes that reveal its true nature.\n\nThe validation process relies on **magic number detection** — specific byte patterns that appear at the beginning of files and uniquely identify the file format. For example, PNG files always start with the bytes `89 50 4E 47 0D 0A 1A 0A`, JPEG files begin with `FF D8 FF`, and PDF files start with `%PDF-`. These signatures are embedded by the programs that create the files and cannot be easily forged without corrupting the file's readability.\n\n> **Decision: Content-Based Type Detection Over Extension-Based**\n> - **Context**: Need to validate file types to enforce security policies and prevent malicious uploads disguised as safe file types\n> - **Options Considered**: Trust file extensions, HTTP Content-Type headers, or examine actual file content using magic bytes\n> - **Decision**: Implement magic byte detection with fallback to Content-Type headers for validation\n> - **Rationale**: File extensions and Content-Type headers can be trivially spoofed by attackers, while magic bytes reflect the actual file structure and are much harder to forge without corrupting the file\n> - **Consequences**: Requires maintaining a database of magic number patterns and file signatures, adds computational overhead to read file headers, but provides strong protection against type confusion attacks\n\n| Validation Method | Reliability | Performance | Security | Implementation Complexity |\n|------------------|-------------|-------------|-----------|--------------------------|\n| File Extension | Very Low | Excellent | Poor | Minimal |\n| Content-Type Header | Low | Excellent | Poor | Minimal |\n| Magic Byte Detection | High | Good | Strong | Moderate |\n| Deep Content Analysis | Very High | Poor | Excellent | Complex |\n\nOur validation system maintains a **file signature registry** that maps magic byte patterns to MIME types and acceptable file categories. When a file upload completes, the validator reads the first 512 bytes of the file (enough to capture most file signatures) and matches them against the known patterns. If multiple signatures could match, the system uses the longest matching pattern to avoid false positives.\n\nThe validation workflow follows this algorithm:\n\n1. **Read File Header**: Extract the first 512 bytes from the uploaded file's beginning\n2. **Pattern Matching**: Compare the header bytes against the registered magic number database\n3. **Type Determination**: Identify the most specific matching file type based on byte patterns\n4. **Policy Check**: Verify that the detected file type appears in the server's allowed content types configuration\n5. **Fallback Validation**: If magic bytes are inconclusive, check the HTTP Content-Type header as secondary validation\n6. **Decision**: Accept the file if type is allowed, reject if forbidden, or flag for manual review if uncertain\n\n| File Signature Component | Purpose | Example |\n|--------------------------|---------|---------|\n| Magic Bytes | Unique identifier at file start | `89 50 4E 47` for PNG |\n| Offset | Position where signature appears | Usually 0, sometimes offset |\n| Length | How many bytes to examine | 4-16 bytes typically |\n| MIME Type | Standardized content type | `image/png` |\n| File Extensions | Common filename suffixes | `.png`, `.jpg`, `.pdf` |\n\nThe signature database includes entries for all commonly uploaded file types:\n\n| File Type | Magic Bytes (Hex) | MIME Type | Security Risk |\n|-----------|------------------|-----------|---------------|\n| PNG Image | `89 50 4E 47 0D 0A 1A 0A` | `image/png` | Low |\n| JPEG Image | `FF D8 FF` | `image/jpeg` | Low |\n| PDF Document | `25 50 44 46 2D` (`%PDF-`) | `application/pdf` | Medium |\n| ZIP Archive | `50 4B 03 04` or `50 4B 05 06` | `application/zip` | High |\n| Windows Executable | `4D 5A` (`MZ`) | `application/x-msdownload` | Critical |\n| ELF Executable | `7F 45 4C 46` | `application/x-executable` | Critical |\n\n> The critical insight is that **file content never lies**, while metadata always can. Magic bytes are written by the programs that create files and reflect the actual internal structure, making them extremely difficult to forge without breaking the file's functionality.\n\n**Common Pitfalls in File Type Validation:**\n\n⚠️ **Pitfall: Trusting Only File Extensions**\nChecking only the filename extension (`document.pdf`) allows trivial bypasses where attackers rename malicious files. An executable renamed from `virus.exe` to `document.pdf` will pass extension-based validation but still execute maliciously when opened. Always examine the actual file content using magic byte detection.\n\n⚠️ **Pitfall: Insufficient Magic Byte Coverage**\nReading only the first few bytes might miss file formats where the signature appears at an offset. Some file formats embed signatures after headers or metadata. Maintain a comprehensive signature database that includes offset information and read sufficient header data (512+ bytes) to capture most signatures.\n\n⚠️ **Pitfall: Blocking Legitimate Compound Documents**\nModern document formats like DOCX and XLSX are actually ZIP archives containing XML files. Blanket blocking of ZIP files will break legitimate document uploads. Implement hierarchical validation that can examine container formats and validate their internal structure.\n\n### Virus Scanning Integration\n\nVirus scanning acts as the **security checkpoint** in our upload pipeline, similar to how airport baggage scanners examine every item for dangerous materials. The scanner doesn't care what the luggage tag says — it examines the actual contents using signature databases and behavioral analysis to detect threats. Our virus scanning integration with ClamAV provides this same thorough examination of uploaded file content.\n\nThe scanning architecture uses an **asynchronous processing model** to avoid blocking client uploads while ensuring comprehensive security coverage. Think of it as a **two-stage security process**: files first enter a holding area where clients can complete their uploads quickly, then move through background security screening before reaching the clean storage area. This prevents both performance bottlenecks and security gaps.\n\n> **Decision: Asynchronous Scanning with Temporary Storage**\n> - **Context**: Virus scanning large files can take several minutes, but clients expect upload confirmations quickly to maintain good user experience\n> - **Options Considered**: Synchronous scanning during upload, asynchronous background scanning after upload completion, or streaming scan during chunk assembly\n> - **Decision**: Store completed uploads in temporary storage, scan asynchronously, then promote to clean storage or move to quarantine\n> - **Rationale**: Balances security (all files are scanned) with performance (clients get quick upload confirmations), and allows retrying failed scans without re-uploading\n> - **Consequences**: Requires additional temporary storage space and cleanup processes, adds complexity to track scanning status, but provides good user experience without security compromise\n\n| Scanning Approach | Client Experience | Security Coverage | Resource Usage | Complexity |\n|-------------------|-------------------|-------------------|----------------|------------|\n| Synchronous During Upload | Poor (long waits) | Complete | High (blocking) | Low |\n| Skip Scanning | Excellent | None | Minimal | Minimal |\n| Asynchronous Post-Upload | Good (quick confirmation) | Complete | Moderate | Moderate |\n| Stream Scanning | Good | Complete | High (concurrent) | High |\n\nThe virus scanning workflow integrates with our upload completion process through these stages:\n\n1. **Upload Completion**: When all chunks are assembled, the file moves to temporary storage with status `completing`\n2. **Scan Queue**: The file gets queued for virus scanning with metadata about its location and session\n3. **ClamAV Integration**: The scanner daemon processes files using Unix domain sockets for security isolation\n4. **Result Processing**: Clean files move to permanent storage, infected files go to quarantine, scan failures trigger retries\n5. **Status Updates**: The upload session status updates to `completed` (clean), `quarantined` (infected), or `failed` (scan error)\n6. **Client Notification**: Clients can poll the session status to learn the final outcome\n\nThe ClamAV integration uses **Unix domain sockets** rather than network connections for security isolation. This prevents the virus scanner from having network access that could be exploited if a malicious file compromises the scanning process. The socket communication follows the ClamAV protocol for streaming file data and receiving scan results.\n\n| Scanning Component | Responsibility | Configuration |\n|-------------------|----------------|---------------|\n| ClamAV Daemon | Signature-based malware detection | Socket path, signature database updates |\n| Scan Queue | Async job management | Concurrency limits, retry policies |\n| Result Processor | Handle scan outcomes | Quarantine location, notification rules |\n| Status Tracker | Update session states | Database connection, cleanup schedules |\n\nThe scan result handling implements **defense in depth** with multiple layers of protection:\n\n| Scan Result | Action Taken | Storage Location | Client Notification | Retention Policy |\n|-------------|--------------|------------------|---------------------|------------------|\n| Clean | Promote to permanent storage | Production backend | Success notification | Normal retention |\n| Infected | Move to quarantine | Isolated quarantine storage | Security alert | Extended retention for analysis |\n| Scan Failed | Retry scan (up to 3 times) | Temporary storage | Retry notification | Short retention, then manual review |\n| Timeout | Kill scan, retry with smaller chunks | Temporary storage | Timeout notification | Manual review required |\n\nThe virus signature database requires **regular updates** to detect new threats. The system schedules automatic signature updates from ClamAV's threat intelligence feeds, but also provides manual update capabilities for urgent threat responses. After signature updates, the system can optionally rescan recently uploaded files to catch threats that were unknown at their original upload time.\n\n> The key insight is that **virus scanning must happen in isolation** — the scanner process should have minimal privileges and no network access, so that if a malicious file compromises the scanning process, the damage is contained to the scanner itself rather than spreading to the broader system.\n\n**Integration with Upload Sessions:**\n\nThe scanning process extends our upload session state machine to include scanning-related states. The session transitions from `completing` (assembly finished) to `scanning` (virus scan in progress) to either `completed` (clean file) or `quarantined` (infected file).\n\n| Session State | Description | Next Possible States | Client Actions |\n|---------------|-------------|---------------------|----------------|\n| `completing` | Chunks assembled, starting validation | `scanning`, `failed` | Wait for scan completion |\n| `scanning` | Virus scan in progress | `completed`, `quarantined`, `failed` | Poll for status updates |\n| `completed` | File passed all validation | None (terminal) | Access file via signed URLs |\n| `quarantined` | File failed virus scan | None (terminal) | Receive security alert |\n\n**Common Pitfalls in Virus Scanning:**\n\n⚠️ **Pitfall: Scanning After Files Reach Production**\nIf files reach production storage before virus scanning completes, there's a window where infected files are accessible to users. Always scan in a temporary holding area and only promote clean files to production storage. This prevents the \"scan after compromise\" scenario.\n\n⚠️ **Pitfall: Ignoring Scan Timeouts**\nLarge files can cause ClamAV to timeout or consume excessive memory. Implement timeout handling that kills hung scans and retries with smaller chunk sizes. Set memory limits on the scanning process to prevent resource exhaustion attacks through maliciously crafted files.\n\n⚠️ **Pitfall: Inadequate Scanner Isolation**\nRunning the virus scanner with full system privileges means that if a malicious file compromises ClamAV, the attacker gains broad system access. Run scanning processes with minimal privileges, use Unix domain sockets instead of network connections, and consider containerization for additional isolation.\n\n### Quarantine and Policy Enforcement\n\nFile quarantine operates as a **secure evidence locker** for suspicious content, similar to how law enforcement isolates potential evidence in tamper-proof storage while investigations proceed. The quarantine system ensures that infected or suspicious files are completely isolated from production systems while preserving them for security analysis and forensic investigation.\n\nThe quarantine architecture implements **strict isolation** with separate storage credentials, network access controls, and administrative permissions. Think of it as a **high-security containment facility** where nothing enters or leaves without explicit authorization from security administrators. This prevents both accidental access to dangerous files and data exfiltration of quarantined content.\n\n> **Decision: Separate Quarantine Storage with Forensic Retention**\n> - **Context**: Need to isolate infected files from production systems while preserving them for security analysis and forensic investigation\n> - **Options Considered**: Delete infected files immediately, store in same backend with access controls, or use completely separate quarantine storage\n> - **Decision**: Implement dedicated quarantine storage with separate credentials and extended retention policies\n> - **Rationale**: Complete isolation prevents any possibility of infected files reaching production, while preservation enables security teams to analyze attack patterns and improve detection\n> - **Consequences**: Requires additional storage infrastructure and administrative overhead, but provides strong security isolation and valuable threat intelligence\n\n| Quarantine Approach | Security Isolation | Forensic Value | Operational Overhead | Storage Cost |\n|---------------------|-------------------|----------------|---------------------|--------------|\n| Immediate Deletion | Complete | None | Minimal | None |\n| Flagged in Production | Weak | Good | Low | Standard |\n| Separate Storage | Strong | Excellent | Moderate | Additional |\n| Air-Gapped Archive | Maximum | Excellent | High | Additional |\n\nThe quarantine system maintains detailed **forensic metadata** for every quarantined file to support security investigations and threat analysis. This metadata includes not only the file content but also the complete context of how the file arrived in the system, what triggered the quarantine decision, and the technical details of the security violation.\n\n| Quarantine Metadata | Purpose | Example Value |\n|---------------------|---------|---------------|\n| Original Filename | Track attack patterns | `invoice.pdf.exe` |\n| Upload Session ID | Link to session logs | `sess_a1b2c3d4` |\n| Client IP Address | Geographic analysis | `192.168.1.100` |\n| Detection Method | Understand detection trigger | `ClamAV signature: Win.Trojan.Agent` |\n| Upload Timestamp | Timeline reconstruction | `2023-12-15T14:30:22Z` |\n| File Size | Resource impact analysis | `2048576` bytes |\n| Content Hash | Deduplication and tracking | `sha256:abc123...` |\n| Quarantine Reason | Policy violation details | `Virus detected: Trojan.Generic` |\n\nThe policy enforcement system implements **configurable security rules** that can be updated without code changes to respond to evolving threats. The policies define what file types are allowed, size limits, scanning requirements, and quarantine criteria. This flexibility allows security teams to tighten restrictions during active attacks or relax them for trusted users.\n\nPolicy enforcement follows this decision tree:\n\n1. **Size Policy Check**: Verify file size is within configured limits for the file type\n2. **Content Type Policy**: Ensure detected file type is in the allowed content types list\n3. **Virus Scan Policy**: Check if virus scanning detected any threats or suspicious patterns\n4. **Custom Rule Evaluation**: Apply any additional organizational policies (e.g., executable file blocking)\n5. **Risk Scoring**: Calculate an overall risk score based on multiple factors\n6. **Decision**: Allow to production storage, quarantine for review, or reject completely\n\n| Policy Type | Configuration | Example Rule | Enforcement Point |\n|-------------|---------------|---------------|-------------------|\n| Size Limits | Per-file-type maximums | `image/jpeg: 10MB, application/pdf: 50MB` | During upload |\n| Content Type Allowlist | MIME type permissions | `Allow: image/*, application/pdf` | After type detection |\n| Virus Scanning | Scan requirements | `Mandatory for all uploads > 1MB` | Post-assembly |\n| Executable Blocking | Binary file restrictions | `Block: application/x-executable` | Type-based |\n| Rate Limiting | Upload frequency limits | `Max 100 files/hour per IP` | Per request |\n\n**Quarantine Storage Architecture:**\n\nThe quarantine storage system uses **separate infrastructure** with dedicated credentials and access controls to ensure complete isolation from production systems. Even if production storage credentials are compromised, quarantine data remains protected under different authentication mechanisms.\n\n| Storage Component | Production | Quarantine | Isolation Benefit |\n|------------------|------------|------------|-------------------|\n| Authentication | Primary service credentials | Separate quarantine-only credentials | Credential compromise isolation |\n| Network Access | Standard application network | Restricted admin-only network | Network attack isolation |\n| Backup Systems | Standard automated backups | Separate forensic backup system | Data recovery isolation |\n| Access Logging | Application access logs | Security audit logs | Investigation support |\n\n**Retention and Lifecycle Management:**\n\nQuarantine files follow **extended retention policies** to support ongoing security investigations and threat intelligence gathering. The retention system automatically manages the lifecycle of quarantined content while ensuring important evidence is preserved for the required timeframes.\n\n| File Category | Retention Period | Automatic Actions | Manual Review Required |\n|---------------|------------------|-------------------|----------------------|\n| Virus-Infected | 1 year | Delete after retention expires | Security team notification |\n| Suspicious (unconfirmed) | 90 days | Delete or promote after manual review | Yes, within 30 days |\n| Policy Violations | 30 days | Delete after log analysis | No, unless flagged |\n| False Positives | 7 days | Delete after confirmation | Yes, for policy tuning |\n\nThe quarantine system generates **security alerts** for different types of threats to ensure appropriate response from security teams. These alerts include sufficient context for security analysts to understand the threat and take appropriate action.\n\n> Critical insight: **Quarantine is not just isolation — it's active threat intelligence collection**. Every quarantined file provides data about attack patterns, helping improve detection rules and security policies for future uploads.\n\n**Common Pitfalls in Quarantine Management:**\n\n⚠️ **Pitfall: Insufficient Quarantine Isolation**\nStoring quarantined files in the same storage system as clean files, even with access controls, creates risk of cross-contamination or accidental access. Use completely separate storage infrastructure with different credentials to ensure true isolation.\n\n⚠️ **Pitfall: Inadequate Forensic Metadata**\nQuarantining files without preserving the context of how they arrived (IP address, user agent, upload session details) makes security investigations much harder. Capture complete metadata at quarantine time, as this information may not be available later.\n\n⚠️ **Pitfall: Automatic Deletion of Evidence**\nImmediately deleting infected files destroys valuable threat intelligence that could help improve security. Implement retention policies that preserve quarantined files long enough for security analysis while managing storage costs.\n\n### Size Limits and Resource Protection\n\nResource protection functions as the **traffic control system** for file uploads, preventing both accidental resource exhaustion and deliberate denial-of-service attacks. Think of it as the **bouncer at a nightclub** — it enforces capacity limits not because it dislikes people, but because exceeding the venue's capacity creates dangerous conditions for everyone inside.\n\nThe size limit system implements **multi-layered protection** with different limits at various stages of the upload process. This layered approach prevents resource exhaustion attacks that might try to bypass early limits through chunked uploads or by consuming resources during processing stages.\n\n> **Decision: Hierarchical Size Limits with Early Enforcement**\n> - **Context**: Need to protect server resources from exhaustion while supporting legitimate large file uploads, and prevent DoS attacks through resource consumption\n> - **Options Considered**: Single global file size limit, per-user quotas, or hierarchical limits by file type and processing stage\n> - **Decision**: Implement multiple limit layers: per-chunk, per-file, per-session, per-user, and per-content-type with early enforcement\n> - **Rationale**: Layered limits provide defense in depth against various attack vectors while allowing flexibility for different use cases and file types\n> - **Consequences**: Increased configuration complexity and enforcement overhead, but comprehensive protection against resource exhaustion attacks\n\n| Limit Layer | Purpose | Attack Prevention | Performance Impact |\n|-------------|---------|-------------------|-------------------|\n| Chunk Size | Prevent memory exhaustion | Large chunk DoS | Minimal |\n| File Size | Control individual uploads | Single large file attack | Low |\n| Session Total | Limit chunked upload abuse | Multi-file session attack | Low |\n| User Quota | Fair resource allocation | User-based resource hoarding | Moderate |\n| Content-Type | Specialized limits | Type-specific attacks | Minimal |\n\nThe size enforcement system operates at **multiple checkpoints** throughout the upload pipeline, with each checkpoint designed to catch different types of resource exhaustion attempts before they can impact system stability.\n\n**Chunk-Level Protection:**\n\nThe first line of defense operates at the individual chunk level, preventing attackers from exhausting server memory by sending extremely large chunks that must be held in memory during processing.\n\n| Chunk Limit Type | Default Value | Rationale | Failure Mode |\n|------------------|---------------|-----------|---------------|\n| Maximum Chunk Size | 64MB | Balances memory usage with upload efficiency | `413 Request Entity Too Large` |\n| Minimum Chunk Size | 1KB | Prevents micro-chunk DoS attacks | `400 Bad Request` |\n| Chunks Per Session | 10,000 | Limits session complexity | `429 Too Many Requests` |\n\n**File-Level Protection:**\n\nFile-level limits prevent individual uploads from consuming excessive storage space or processing time. These limits are **content-type aware** to allow larger sizes for file types that legitimately need them (like video files) while restricting potentially dangerous file types.\n\n| Content Type | Size Limit | Processing Timeout | Rationale |\n|--------------|------------|-------------------|-----------|\n| `image/*` | 50MB | 30 seconds | High-resolution images, reasonable processing time |\n| `application/pdf` | 100MB | 60 seconds | Large documents, moderate processing complexity |\n| `video/*` | 2GB | 300 seconds | Video files legitimately large, extended processing time |\n| `application/zip` | 500MB | 120 seconds | Archives can be large, but need thorough scanning |\n| Default (unknown) | 10MB | 30 seconds | Conservative limits for unknown file types |\n\n**Session and User-Level Protection:**\n\nHigher-level limits prevent abuse through multiple files or extended upload sessions. These limits protect against **distributed resource consumption** where attackers spread their resource usage across many uploads to avoid per-file detection.\n\n| Protection Level | Limit Type | Default Value | Reset Period | Enforcement |\n|-----------------|------------|---------------|---------------|-------------|\n| Per Session | Total bytes uploaded | 5GB | 24 hours | Reject new chunks |\n| Per User | Daily upload quota | 100GB | 24 hours | Rate limiting |\n| Per IP | Concurrent uploads | 10 sessions | Real-time | Connection limiting |\n| Global System | Active uploads | 1000 sessions | Real-time | Admission control |\n\n**Resource Monitoring and Alerts:**\n\nThe protection system continuously monitors resource usage patterns to detect both legitimate capacity issues and potential attacks. This monitoring enables **proactive response** before resource exhaustion affects service availability.\n\n| Monitored Resource | Warning Threshold | Critical Threshold | Response Action |\n|-------------------|-------------------|-------------------|-----------------|\n| Disk Space (temp storage) | 80% full | 95% full | Throttle new uploads |\n| Memory Usage | 75% utilized | 90% utilized | Reject large chunks |\n| Active Sessions | 800 concurrent | 950 concurrent | Queue new uploads |\n| Scan Queue Depth | 100 pending | 500 pending | Disable virus scanning |\n\n**Dynamic Limit Adjustment:**\n\nThe system implements **adaptive limits** that can tighten during high load or attack conditions and relax during normal operation. This provides automatic defense against resource exhaustion while maintaining good user experience under normal conditions.\n\nThe adaptive algorithm follows this decision process:\n\n1. **Monitor Resource Usage**: Track current utilization of CPU, memory, disk, and network resources\n2. **Detect Stress Conditions**: Identify when resource usage exceeds normal operating thresholds\n3. **Calculate Limit Adjustments**: Reduce limits proportionally to resource pressure\n4. **Apply Graduated Restrictions**: Implement tighter limits with grace periods for existing uploads\n5. **Monitor Effectiveness**: Track whether limit adjustments successfully reduce resource pressure\n6. **Restore Normal Limits**: Gradually return to standard limits as resource pressure decreases\n\n| System Load Level | Upload Limit Multiplier | Chunk Size Multiplier | Session Limit Multiplier |\n|------------------|------------------------|----------------------|------------------------|\n| Normal (< 70% resources) | 1.0x | 1.0x | 1.0x |\n| Elevated (70-85% resources) | 0.8x | 0.8x | 0.9x |\n| High (85-95% resources) | 0.5x | 0.5x | 0.7x |\n| Critical (> 95% resources) | 0.2x | 0.2x | 0.5x |\n\n**Error Handling and User Communication:**\n\nWhen size limits are exceeded, the system provides **clear, actionable error messages** that help legitimate users understand the constraints and adjust their uploads accordingly. The error responses include specific information about what limit was exceeded and suggestions for resolving the issue.\n\n| Limit Exceeded | HTTP Status | Error Message | User Guidance |\n|----------------|-------------|---------------|---------------|\n| Chunk too large | 413 | \"Chunk size 128MB exceeds limit of 64MB\" | \"Split into smaller chunks\" |\n| File too large | 413 | \"File size 2GB exceeds limit of 500MB for ZIP files\" | \"Compress or split the file\" |\n| Quota exceeded | 429 | \"Daily quota 100GB exceeded, resets in 6 hours\" | \"Wait for quota reset\" |\n| System overloaded | 503 | \"System at capacity, retry in 5 minutes\" | \"Retry after delay\" |\n\n> The key principle is **fail fast and fail informatively** — detect limit violations as early as possible in the upload process and provide users with specific information about what went wrong and how to fix it.\n\n**Common Pitfalls in Resource Protection:**\n\n⚠️ **Pitfall: Only Checking Limits at Upload Completion**\nWaiting until upload completion to check file size limits allows attackers to consume storage and bandwidth resources even when their upload will ultimately be rejected. Enforce size limits during chunk upload based on the declared total size from upload initialization.\n\n⚠️ **Pitfall: Ignoring Processing Resource Limits**\nFile size limits protect storage, but virus scanning and file processing can consume significant CPU and memory. Set processing timeouts and memory limits for scanning operations to prevent resource exhaustion during the validation phase.\n\n⚠️ **Pitfall: Inflexible Limit Configuration**\nHard-coded size limits can't adapt to different deployment environments or changing business needs. Make all limits configurable and consider implementing different limit profiles for different user classes (free vs. paid users, internal vs. external users).\n\n![Upload Completion and Assembly](./diagrams/upload-completion-sequence.svg)\n\n### Implementation Guidance\n\n**A. Technology Recommendations Table:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| File Type Detection | `http.DetectContentType()` + custom magic bytes | `github.com/h2non/filetype` with comprehensive signatures |\n| Virus Scanning | Direct `clamd` socket connection | `github.com/dutchcoders/go-clamd` client library |\n| Magic Byte Database | Static map in code | External JSON configuration file |\n| Quarantine Storage | Separate directory with restricted permissions | Dedicated S3 bucket with separate IAM credentials |\n| Size Limit Enforcement | Simple byte counting | Rate limiting with `golang.org/x/time/rate` |\n| Policy Configuration | YAML configuration file | Database-backed policy engine |\n\n**B. Recommended File/Module Structure:**\n\n```\nproject-root/\n  internal/\n    validation/\n      validator.go              ← main validation orchestrator\n      filetype/\n        detector.go            ← magic byte file type detection\n        signatures.go          ← file signature database\n        signatures.json        ← configurable signature database\n      scanner/\n        clamav.go             ← ClamAV virus scanner integration\n        scanner.go            ← scanner interface and queue management\n      quarantine/\n        quarantine.go         ← quarantine storage and metadata\n        policy.go             ← policy enforcement engine\n      limits/\n        limiter.go            ← size limits and resource protection\n    config/\n      security.yml            ← security policy configuration\n  cmd/\n    validator-test/\n      main.go                 ← standalone validation testing tool\n```\n\n**C. Infrastructure Starter Code:**\n\nHere's the complete file type detection infrastructure with magic byte support:\n\n```go\n// internal/validation/filetype/detector.go\npackage filetype\n\nimport (\n    \"bufio\"\n    \"encoding/hex\"\n    \"io\"\n)\n\n// FileSignature represents a magic byte pattern for file type detection\ntype FileSignature struct {\n    Pattern     []byte `json:\"pattern\"`\n    Offset      int    `json:\"offset\"`\n    MIME        string `json:\"mime\"`\n    Extensions  []string `json:\"extensions\"`\n    Description string `json:\"description\"`\n}\n\n// Detector handles file type detection using magic byte patterns\ntype Detector struct {\n    signatures []FileSignature\n}\n\n// NewDetector creates a file type detector with default signatures\nfunc NewDetector() *Detector {\n    return &Detector{\n        signatures: getDefaultSignatures(),\n    }\n}\n\n// DetectFromReader examines the first 512 bytes to determine file type\nfunc (d *Detector) DetectFromReader(reader io.Reader) (string, error) {\n    buffer := make([]byte, 512)\n    n, err := io.ReadFull(reader, buffer)\n    if err != nil && err != io.EOF && err != io.ErrUnexpectedEOF {\n        return \"\", err\n    }\n    \n    return d.DetectFromBytes(buffer[:n]), nil\n}\n\n// DetectFromBytes matches byte patterns against known file signatures\nfunc (d *Detector) DetectFromBytes(data []byte) string {\n    for _, sig := range d.signatures {\n        if d.matchesSignature(data, sig) {\n            return sig.MIME\n        }\n    }\n    return \"application/octet-stream\" // Unknown binary data\n}\n\nfunc (d *Detector) matchesSignature(data []byte, sig FileSignature) bool {\n    if len(data) < sig.Offset+len(sig.Pattern) {\n        return false\n    }\n    \n    start := sig.Offset\n    for i, b := range sig.Pattern {\n        if data[start+i] != b {\n            return false\n        }\n    }\n    return true\n}\n\nfunc getDefaultSignatures() []FileSignature {\n    return []FileSignature{\n        {mustDecodeHex(\"89504E470D0A1A0A\"), 0, \"image/png\", []string{\".png\"}, \"PNG Image\"},\n        {mustDecodeHex(\"FFD8FF\"), 0, \"image/jpeg\", []string{\".jpg\", \".jpeg\"}, \"JPEG Image\"},\n        {mustDecodeHex(\"25504446\"), 0, \"application/pdf\", []string{\".pdf\"}, \"PDF Document\"},\n        {mustDecodeHex(\"504B0304\"), 0, \"application/zip\", []string{\".zip\"}, \"ZIP Archive\"},\n        {mustDecodeHex(\"504B0506\"), 0, \"application/zip\", []string{\".zip\"}, \"ZIP Archive (empty)\"},\n        {mustDecodeHex(\"4D5A\"), 0, \"application/x-msdownload\", []string{\".exe\"}, \"Windows Executable\"},\n        {mustDecodeHex(\"7F454C46\"), 0, \"application/x-executable\", []string{\"\"}, \"ELF Executable\"},\n    }\n}\n\nfunc mustDecodeHex(s string) []byte {\n    b, err := hex.DecodeString(s)\n    if err != nil {\n        panic(\"Invalid hex signature: \" + s)\n    }\n    return b\n}\n```\n\nComplete ClamAV scanner integration:\n\n```go\n// internal/validation/scanner/clamav.go\npackage scanner\n\nimport (\n    \"bufio\"\n    \"context\"\n    \"fmt\"\n    \"io\"\n    \"net\"\n    \"strings\"\n    \"time\"\n)\n\n// ClamAVScanner integrates with ClamAV daemon via Unix domain socket\ntype ClamAVScanner struct {\n    socketPath string\n    timeout    time.Duration\n}\n\n// ScanResult represents the outcome of a virus scan\ntype ScanResult struct {\n    Clean       bool\n    ThreatName  string\n    Error       error\n    ScanTime    time.Duration\n}\n\n// NewClamAVScanner creates a scanner connected to ClamAV daemon\nfunc NewClamAVScanner(socketPath string, timeout time.Duration) *ClamAVScanner {\n    return &ClamAVScanner{\n        socketPath: socketPath,\n        timeout:    timeout,\n    }\n}\n\n// ScanStream sends file data to ClamAV and returns scan results\nfunc (c *ClamAVScanner) ScanStream(ctx context.Context, reader io.Reader) (*ScanResult, error) {\n    start := time.Now()\n    \n    conn, err := net.DialTimeout(\"unix\", c.socketPath, c.timeout)\n    if err != nil {\n        return &ScanResult{Error: fmt.Errorf(\"connect to ClamAV: %w\", err)}, nil\n    }\n    defer conn.Close()\n    \n    // Set overall timeout for the scan operation\n    conn.SetDeadline(time.Now().Add(c.timeout))\n    \n    // Send INSTREAM command to ClamAV\n    _, err = conn.Write([]byte(\"zINSTREAM\\000\"))\n    if err != nil {\n        return &ScanResult{Error: fmt.Errorf(\"send command: %w\", err)}, nil\n    }\n    \n    // Stream file data in chunks\n    buffer := make([]byte, 32768) // 32KB chunks\n    for {\n        n, err := reader.Read(buffer)\n        if n > 0 {\n            // Send chunk size (4 bytes, network byte order) then chunk data\n            chunkSize := []byte{\n                byte(n >> 24),\n                byte(n >> 16),\n                byte(n >> 8),\n                byte(n),\n            }\n            if _, err := conn.Write(chunkSize); err != nil {\n                return &ScanResult{Error: fmt.Errorf(\"send chunk size: %w\", err)}, nil\n            }\n            if _, err := conn.Write(buffer[:n]); err != nil {\n                return &ScanResult{Error: fmt.Errorf(\"send chunk data: %w\", err)}, nil\n            }\n        }\n        if err == io.EOF {\n            break\n        }\n        if err != nil {\n            return &ScanResult{Error: fmt.Errorf(\"read file: %w\", err)}, nil\n        }\n    }\n    \n    // Send zero-length chunk to signal end of stream\n    _, err = conn.Write([]byte{0, 0, 0, 0})\n    if err != nil {\n        return &ScanResult{Error: fmt.Errorf(\"send end marker: %w\", err)}, nil\n    }\n    \n    // Read scan result\n    scanner := bufio.NewScanner(conn)\n    if !scanner.Scan() {\n        return &ScanResult{Error: fmt.Errorf(\"read scan result: %w\", scanner.Err())}, nil\n    }\n    \n    response := strings.TrimSpace(scanner.Text())\n    scanTime := time.Since(start)\n    \n    if strings.HasSuffix(response, \"OK\") {\n        return &ScanResult{Clean: true, ScanTime: scanTime}, nil\n    } else if strings.Contains(response, \"FOUND\") {\n        parts := strings.Split(response, \": \")\n        if len(parts) >= 2 {\n            threatName := strings.TrimSuffix(parts[1], \" FOUND\")\n            return &ScanResult{Clean: false, ThreatName: threatName, ScanTime: scanTime}, nil\n        }\n        return &ScanResult{Clean: false, ThreatName: \"Unknown threat\", ScanTime: scanTime}, nil\n    } else {\n        return &ScanResult{Error: fmt.Errorf(\"unexpected ClamAV response: %s\", response)}, nil\n    }\n}\n\n// Ping tests connectivity to ClamAV daemon\nfunc (c *ClamAVScanner) Ping() error {\n    conn, err := net.DialTimeout(\"unix\", c.socketPath, 5*time.Second)\n    if err != nil {\n        return fmt.Errorf(\"connect to ClamAV: %w\", err)\n    }\n    defer conn.Close()\n    \n    _, err = conn.Write([]byte(\"zPING\\000\"))\n    if err != nil {\n        return fmt.Errorf(\"send ping: %w\", err)\n    }\n    \n    response := make([]byte, 100)\n    n, err := conn.Read(response)\n    if err != nil {\n        return fmt.Errorf(\"read response: %w\", err)\n    }\n    \n    if !strings.Contains(string(response[:n]), \"PONG\") {\n        return fmt.Errorf(\"unexpected ping response: %s\", response[:n])\n    }\n    \n    return nil\n}\n```\n\n**D. Core Logic Skeleton Code:**\n\n```go\n// internal/validation/validator.go\npackage validation\n\nimport (\n    \"context\"\n    \"io\"\n    \"time\"\n)\n\n// FileValidator orchestrates file validation pipeline\ntype FileValidator struct {\n    detector    *filetype.Detector\n    scanner     Scanner\n    quarantine  *QuarantineManager\n    limits      *LimitEnforcer\n    config      *SecurityConfig\n}\n\n// ValidationResult contains the outcome of file validation\ntype ValidationResult struct {\n    Passed      bool\n    FileType    string\n    ThreatName  string\n    QuarantineID string\n    Violations   []string\n    ProcessingTime time.Duration\n}\n\n// ValidateFile runs complete validation pipeline on uploaded file\nfunc (v *FileValidator) ValidateFile(ctx context.Context, sessionID string, reader io.Reader, metadata map[string]string) (*ValidationResult, error) {\n    start := time.Now()\n    result := &ValidationResult{}\n    \n    // TODO 1: Enforce size limits based on declared file size from metadata\n    // Check metadata[\"content-length\"] against configured limits\n    // Return early with violation if size exceeds limits\n    \n    // TODO 2: Detect actual file type using magic byte analysis\n    // Use v.detector.DetectFromReader() to examine file content\n    // Compare detected type against metadata[\"content-type\"] for consistency\n    // Record any mismatches as potential security violations\n    \n    // TODO 3: Validate file type against security policy\n    // Check if detected file type is in allowed content types list\n    // Apply content-type-specific size limits and restrictions\n    // Flag executable files and other high-risk content types\n    \n    // TODO 4: Perform virus scanning on file content\n    // Stream file data to ClamAV scanner using v.scanner.ScanStream()\n    // Handle scan timeouts, errors, and retry logic\n    // Record threat name and details for infected files\n    \n    // TODO 5: Apply quarantine decision based on scan results\n    // Move infected or suspicious files to quarantine storage\n    // Generate forensic metadata including session details and scan results\n    // Update upload session status to reflect quarantine action\n    \n    // TODO 6: Record validation metrics and audit trail\n    // Log all validation decisions with timestamps and reasons\n    // Update session metadata with validation results\n    // Generate security alerts for policy violations\n    \n    result.ProcessingTime = time.Since(start)\n    return result, nil\n}\n\n// EnforceResourceLimits checks upload against size and rate limits\nfunc (v *FileValidator) EnforceResourceLimits(ctx context.Context, userID string, fileSize int64, contentType string) error {\n    // TODO 1: Check individual file size against content-type-specific limits\n    // Look up size limit for detected content type from configuration\n    // Return ErrFileTooLarge if size exceeds type-specific limit\n    \n    // TODO 2: Verify user quota and rate limits\n    // Check user's daily/monthly upload quota from quota tracking\n    // Enforce per-user concurrent upload limits\n    // Apply rate limiting for users exceeding normal usage patterns\n    \n    // TODO 3: Monitor system resource usage and apply adaptive limits\n    // Check current system load (CPU, memory, disk space)\n    // Apply dynamic limit adjustments during high load conditions\n    // Return ErrSystemOverloaded if resources are critically low\n    \n    return nil\n}\n```\n\n**E. Language-Specific Hints:**\n\n- Use `io.TeeReader` to simultaneously read file data for type detection and virus scanning without buffering the entire file in memory\n- Implement `io.LimitReader` to enforce chunk size limits during streaming operations\n- Use `context.WithTimeout` to set maximum time limits for virus scanning operations\n- Store quarantine metadata using `encoding/json` for structured forensic data\n- Use `sync.Pool` for byte buffers when processing multiple files concurrently to reduce garbage collection pressure\n- Implement graceful shutdown with `context.Context` cancellation for long-running scan operations\n\n**F. Milestone Checkpoint:**\n\nAfter implementing the file validation system, verify the following behavior:\n\n**Command to run:** `go test -v ./internal/validation/... -integration`\n\n**Expected test output:**\n```\n=== RUN TestFileTypeDetection\n--- PASS: TestFileTypeDetection (0.05s)\n=== RUN TestVirusScanIntegration  \n--- PASS: TestVirusScanIntegration (2.31s)\n=== RUN TestQuarantineWorkflow\n--- PASS: TestQuarantineWorkflow (0.18s)\n=== RUN TestResourceLimits\n--- PASS: TestResourceLimits (0.02s)\n```\n\n**Manual verification steps:**\n1. Upload a legitimate PDF file: `curl -X POST -F \"file=@document.pdf\" http://localhost:8080/upload/test123/chunk` → Should complete successfully\n2. Upload a file with wrong extension: `curl -X POST -F \"file=@image.pdf\" http://localhost:8080/upload/test456/chunk` → Should detect actual PNG type and either accept or reject based on policy\n3. Upload EICAR test virus: `curl -X POST -F \"file=@eicar.com\" http://localhost:8080/upload/test789/chunk` → Should quarantine with virus detection\n4. Upload oversized file: `curl -X POST -F \"file=@huge.zip\" http://localhost:8080/upload/test000/chunk` → Should reject with size limit error\n\n**Signs of correct implementation:**\n- File type detection works regardless of filename extension\n- ClamAV integration successfully scans files and detects test viruses\n- Quarantine storage isolates infected files with complete metadata\n- Size limits prevent resource exhaustion during upload\n\n**G. Debugging Tips:**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| All files rejected as \"unknown type\" | Magic byte detection failing | Check first 16 bytes of uploaded files in hex dump | Verify file signature database includes common types |\n| Virus scanner timeouts | ClamAV daemon overloaded or hung | Check ClamAV logs and process status | Restart clamd, increase timeout, implement scan queuing |\n| Clean files quarantined | False positive in virus scanning | Review ClamAV signature database version | Update virus signatures, whitelist known-good file hashes |\n| Memory usage grows continuously | File data not being released after processing | Monitor goroutines and heap profile | Ensure file readers are properly closed, use streaming processing |\n| Quarantine storage fills up | No cleanup of old quarantined files | Check quarantine retention policies and cleanup jobs | Implement automated cleanup based on file age and threat level |\n\n\n## Component Interactions and Data Flow\n\n> **Milestone(s):** Milestones 1, 2, and 3 — this section describes how components communicate during chunked upload protocol operations (M1), storage backend coordination (M2), and virus scanning workflow integration (M3)\n\nThink of component interactions in a resumable file upload service like an *airport's baggage handling system*. When you check a bag, it doesn't go directly to the plane — it moves through multiple specialized stations: check-in creates a tracking record, security screening validates contents, sorting routes it to the correct destination, and loading crews place it on the aircraft. Similarly, uploaded file chunks flow through our specialized components: the Upload Manager receives and tracks chunks, the Session Store maintains progress state, Storage Backends persist data, and File Validators ensure security. Each component has a specific responsibility and communicates with others through well-defined message formats, ensuring that even if one station experiences delays, the overall system can recover and complete the process.\n\nThe critical insight for understanding these interactions is that **state consistency across component boundaries** is what enables resumable uploads to work reliably. When a network failure interrupts an upload, each component must have recorded enough information for the client to discover exactly where to resume, regardless of which component was processing data when the failure occurred.\n\n![System Architecture Overview](./diagrams/system-architecture.svg)\n\n### Upload Initialization Flow: Client handshake, session creation, and metadata exchange\n\nThe upload initialization flow represents the *contract negotiation* between client and server — like establishing the terms of a shipping agreement before moving any cargo. During this phase, the client declares its intentions (file size, type, metadata), the server validates feasibility and allocates resources, and both parties agree on the upload session parameters that will govern all subsequent chunk transfers.\n\n> **Decision: Separate Initialization from First Chunk Upload**\n> - **Context**: Clients need to know upload session parameters before sending chunk data, and servers need to validate requirements before allocating storage resources\n> - **Options Considered**: Combined initialization+first-chunk endpoint, separate initialization endpoint, implicit session creation on first chunk\n> - **Decision**: Separate initialization endpoint that returns session metadata before any chunk uploads\n> - **Rationale**: Allows early validation of file size, content type, and storage capacity without wasting bandwidth on chunk uploads that would fail. Enables client-side resume logic to query session state independently of chunk uploads\n> - **Consequences**: Requires additional HTTP round-trip but prevents failed uploads after significant data transfer, simplifies error handling, and enables better resource planning\n\nThe initialization sequence involves multiple components working together to establish upload session state:\n\n**Initialization Message Flow:**\n\n| Component From | Component To | Message Type | Purpose | Failure Impact |\n|---|---|---|---|---|\n| Client | Upload Manager | `POST /files/uploads` | Request new upload session | Client cannot start upload |\n| Upload Manager | File Validator | `ValidateRequirements()` | Check file size/type limits | Early rejection prevents resource waste |\n| Upload Manager | Storage Backend | `InitMultipart()` | Prepare storage resources | Session unusable without storage |\n| Upload Manager | Session Store | `CreateSession()` | Persist session metadata | Session cannot be resumed |\n| Upload Manager | Client | Session metadata response | Confirm session creation | Client missing upload parameters |\n\n**Detailed Initialization Steps:**\n\n1. **Client Request Preparation**: The client constructs an initialization request containing file metadata, declared content type, total file size, and optional custom metadata fields. This request must include enough information for the server to make resource allocation decisions without receiving file content.\n\n2. **Upload Manager Request Validation**: Upon receiving the initialization request, the Upload Manager performs basic request format validation, ensures required fields are present, and generates a unique session identifier that will track this upload throughout its lifecycle.\n\n3. **Security Policy Validation**: The Upload Manager delegates to the File Validator to check the declared file size against configured limits, validate the content type against allowed types, and verify that current system load can accommodate another upload session.\n\n4. **Storage Resource Allocation**: If validation passes, the Upload Manager calls the Storage Backend's `InitMultipart()` method to reserve storage resources and obtain any backend-specific metadata (like S3 multipart upload IDs) needed for subsequent chunk operations.\n\n5. **Session State Persistence**: The Upload Manager creates an `UploadSession` record containing all session metadata and stores it via the Session Store, ensuring that session information survives server restarts and can be queried for resume operations.\n\n6. **Client Response Generation**: Finally, the Upload Manager constructs a response containing the session ID, upload URL for chunk uploads, current offset (initially zero), and any additional parameters the client needs for chunk uploads.\n\n**Upload Session Initialization Data Structures:**\n\n| Field Name | Type | Description | Validation Rules |\n|---|---|---|---|\n| `Filename` | string | Original filename from client | Length 1-255, no path separators |\n| `ContentType` | string | Declared MIME type | Must match allowed content types |\n| `TotalSize` | int64 | Total file size in bytes | Must be > 0 and <= MaxFileSize |\n| `Metadata` | map[string]string | Custom client metadata | Max 10 keys, values <= 1KB each |\n| `StorageKey` | string | Backend-specific storage identifier | Generated by storage backend |\n| `CurrentOffset` | int64 | Bytes successfully uploaded | Initially 0 |\n| `Status` | SessionStatus | Current session state | Initially `SessionStatusInitialized` |\n\n**Error Scenarios During Initialization:**\n\n| Error Condition | Detection Point | Error Response | Recovery Action |\n|---|---|---|---|\n| File size exceeds limits | File Validator | HTTP 413 Payload Too Large | Client must reduce file size |\n| Content type not allowed | File Validator | HTTP 415 Unsupported Media Type | Client must change file type |\n| Storage quota exceeded | Storage Backend | HTTP 507 Insufficient Storage | Client must retry later |\n| Session store unavailable | Session Store | HTTP 503 Service Unavailable | Client must retry with backoff |\n\n![Chunk Upload Sequence Flow](./diagrams/chunk-upload-sequence.svg)\n\n### Chunk Processing Pipeline: From chunk receipt through validation to storage commitment\n\nThe chunk processing pipeline operates like a *manufacturing assembly line* where each station performs specific quality checks and transformations on the raw material (chunk data) before passing it to the next station. Unlike a physical assembly line, however, our pipeline must handle out-of-order arrivals, duplicate parts, and the ability to restart from any point if machinery breaks down.\n\n> **Decision: Validate-Then-Store vs Store-Then-Validate for Chunks**\n> - **Context**: Chunks can be corrupted during network transmission, and storing corrupt data wastes storage space and complicates cleanup\n> - **Options Considered**: Validate checksums before storage, store immediately and validate in background, validate only on assembly\n> - **Decision**: Validate checksums and basic integrity before committing chunks to storage\n> - **Rationale**: Prevents storage pollution with corrupt data, enables immediate client feedback on transmission errors, and reduces complexity during final assembly since all stored chunks are known-good\n> - **Consequences**: Slightly higher latency per chunk but eliminates need for complex cleanup of corrupt data and reduces final assembly complexity\n\nThe chunk processing pipeline coordinates multiple validation and storage operations:\n\n**Chunk Processing Data Flow:**\n\n| Stage | Input | Processing | Output | Error Handling |\n|---|---|---|---|---|\n| Receipt | HTTP chunk request | Parse headers, extract metadata | Chunk metadata + data stream | Reject malformed requests |\n| Validation | Chunk data + metadata | Verify offset, size, checksum | Validated chunk | Request chunk retransmission |\n| Deduplication | Chunk checksum | Check for existing chunk | Unique chunk or skip flag | Continue with existing chunk |\n| Storage | Validated chunk data | Write to storage backend | Storage confirmation | Retry or fail session |\n| Session Update | Storage confirmation | Update session progress | Updated session state | Mark session as corrupted |\n\n**Detailed Chunk Processing Steps:**\n\n1. **Chunk Request Parsing**: The Upload Manager receives an HTTP request containing chunk data with headers specifying session ID, byte offset, chunk size, and content hash. The manager extracts these parameters and validates the session exists and is in a state that accepts chunks.\n\n2. **Offset Validation**: The manager checks that the provided offset aligns with the current session offset (for sequential uploads) or falls within valid ranges (for parallel chunk uploads). Offset mismatches indicate client-server synchronization issues that must be resolved before proceeding.\n\n3. **Chunk Integrity Verification**: The manager computes a checksum of the received chunk data and compares it against the client-provided hash. This catches network transmission corruption before data reaches storage, preventing accumulation of corrupt chunks.\n\n4. **Storage Backend Coordination**: For valid chunks, the manager calls the appropriate Storage Backend method (`StoreChunk` for simple storage or `StoreMultipartChunk` for cloud backends) to persist the chunk data at the correct offset within the upload session's storage space.\n\n5. **Session State Update**: Upon successful storage, the manager updates the session's current offset and chunk tracking metadata in the Session Store, ensuring that resume operations can accurately determine which chunks have been successfully received.\n\n6. **Client Response Generation**: The manager returns an HTTP response indicating successful chunk receipt, the updated upload offset, and any additional information the client needs to continue or complete the upload.\n\n**Chunk Validation Parameters:**\n\n| Parameter | Source | Validation Rule | Error Response |\n|---|---|---|---|\n| Session ID | URL path | Must exist in session store | HTTP 404 Not Found |\n| Content-Range | HTTP header | Must specify valid byte range | HTTP 400 Bad Request |\n| Content-Length | HTTP header | Must match chunk data size | HTTP 400 Bad Request |\n| Upload-Checksum | HTTP header | Must match computed hash | HTTP 460 Checksum Mismatch |\n\n**Parallel Chunk Upload Coordination:**\n\nThe system supports parallel chunk uploads where clients can upload non-contiguous chunks simultaneously. This requires careful coordination to maintain data integrity:\n\n| Scenario | Detection | Handling | State Update |\n|---|---|---|---|\n| Duplicate chunk | Matching offset + size | Return success without storage | No state change |\n| Overlapping chunks | Offset ranges overlap | Reject newer chunk | No state change |\n| Out-of-order chunks | Offset > current progress | Store with gap tracking | Update chunk map only |\n| Gap filling | Chunk fills existing gap | Store and check completeness | Update progress if contiguous |\n\n⚠️ **Pitfall: Race Conditions in Parallel Uploads**\nWhen multiple chunks arrive simultaneously, concurrent updates to session state can cause offset tracking corruption. Always use atomic operations or locks when updating session progress, and verify session state hasn't changed between read and write operations.\n\n### Upload Completion Workflow: Assembly, final validation, and client notification sequence\n\nThe upload completion workflow functions like a *final quality inspection and packaging process* in manufacturing. Once all raw materials (chunks) have arrived, the system must assemble them into the final product (complete file), run comprehensive quality tests (virus scanning and file validation), and either deliver the product to the customer (move to final storage) or reject it to quality control (quarantine for review).\n\n> **Decision: Atomic Assembly with Rollback vs Incremental Assembly**\n> - **Context**: Final file assembly can fail due to chunk corruption, storage errors, or validation failures after significant processing time\n> - **Options Considered**: Assemble chunks incrementally during upload, atomic assembly only after all chunks received, hybrid approach with checkpoints\n> - **Decision**: Atomic assembly triggered only when all chunks are confirmed received\n> - **Rationale**: Ensures file integrity by validating completeness before assembly, simplifies rollback on assembly failures, and enables comprehensive validation on complete file rather than partial data\n> - **Consequences**: Higher completion latency and temporary storage overhead, but eliminates partial file corruption and simplifies error recovery\n\nThe completion workflow orchestrates several complex operations across multiple components:\n\n**Upload Completion Trigger Conditions:**\n\n| Trigger Type | Detection Method | Validation Required | Next Action |\n|---|---|---|---|\n| Sequential completion | Current offset equals total size | Verify no gaps in chunks | Begin assembly process |\n| Explicit completion | Client sends completion request | Check all chunks received | Validate then assemble |\n| Chunk completeness | All chunk ranges filled | Verify checksums match | Start validation pipeline |\n| Timeout completion | Session inactive beyond threshold | Check if assembly viable | Cleanup or emergency assembly |\n\n**Detailed Completion Workflow Steps:**\n\n1. **Completion Detection**: The Upload Manager detects that an upload is ready for completion either because the current offset has reached the declared total file size, or because the client has sent an explicit completion request. The manager verifies that all expected chunks have been received and no gaps exist in the upload.\n\n2. **Pre-Assembly Validation**: Before beginning the expensive assembly process, the manager performs final checks on session state, verifies that all stored chunks are still accessible in the Storage Backend, and confirms that the total size matches the originally declared size.\n\n3. **File Assembly Coordination**: The manager calls the Storage Backend's `CompleteMultipart()` method, which assembles all stored chunks into a single file in the correct order. This operation is typically atomic at the storage level, meaning it either produces a complete valid file or fails without leaving partial results.\n\n4. **File Content Validation**: Once assembly completes, the File Validator receives the complete file for comprehensive validation including magic byte detection for file type verification, virus scanning through ClamAV integration, and content policy enforcement based on security rules.\n\n5. **Final Storage or Quarantine**: Based on validation results, the file is either moved to its final storage location for client access, or transferred to quarantine storage if security violations were detected. The session state is updated to reflect the final outcome.\n\n6. **Client Notification**: The Upload Manager generates a completion notification to the client containing the final file location, any metadata discovered during validation, and access URLs for downloading the completed file.\n\n![File Validation and Scanning Pipeline](./diagrams/virus-scanning-flow.svg)\n\n**File Assembly Data Structures:**\n\n| Component | Data Structure | Key Fields | Purpose |\n|---|---|---|---|\n| Upload Manager | `AssemblyRequest` | SessionID, TotalChunks, ExpectedSize | Assembly coordination |\n| Storage Backend | `MultipartPart` | PartNumber, ETag, Size | Chunk ordering and verification |\n| File Validator | `ValidationRequest` | FileHandle, SessionMetadata, PolicyRules | Comprehensive file validation |\n| Session Store | `CompletionResult` | Status, FileLocation, ThreatInfo | Final outcome recording |\n\n**Validation Pipeline Integration:**\n\nThe completion workflow integrates tightly with the file validation pipeline to ensure security and data integrity:\n\n| Validation Stage | Component | Processing | Success Action | Failure Action |\n|---|---|---|---|---|\n| File type detection | File Validator | Magic byte analysis | Continue to virus scan | Reject upload |\n| Virus scanning | ClamAV Scanner | Stream file to ClamAV | Continue to policy check | Quarantine file |\n| Policy enforcement | Security Manager | Check size, type, metadata rules | Move to final storage | Apply policy action |\n| Integrity verification | Upload Manager | Verify assembled file checksum | Complete session | Restart assembly |\n\n**Error Recovery During Completion:**\n\n| Failure Point | Error Detection | Recovery Strategy | Session State |\n|---|---|---|---|\n| Assembly failure | Storage backend error | Retry assembly up to 3 times | Remains `SessionStatusActive` |\n| Validation timeout | ClamAV timeout | Use fallback validation or quarantine | Mark `SessionStatusCompleting` |\n| Storage full | Disk space exhaustion | Attempt cleanup then retry | Retry or fail session |\n| Corruption detected | Checksum mismatch | Request chunk re-upload | Reset to active for missing chunks |\n\n### Error Propagation Patterns: How failures in different components are handled and communicated\n\nError propagation in the upload service follows a *circuit breaker pattern* similar to electrical systems — when a component detects a fault, it must decide whether to handle the error locally, propagate it upstream, or isolate the failure to prevent system-wide cascading failures. The key insight is that different types of errors require different propagation strategies: transient network errors should be retried, data corruption errors should be propagated immediately, and resource exhaustion errors should trigger backpressure.\n\n> **Decision: Fail-Fast vs Retry-With-Backoff for Component Errors**\n> - **Context**: Component failures can be transient (network timeouts) or permanent (disk full), and different error types require different handling strategies\n> - **Options Considered**: Retry all errors uniformly, fail immediately on any error, categorize errors by type with specific strategies\n> - **Decision**: Categorize errors into transient, permanent, and data corruption types with specific handling for each\n> - **Rationale**: Transient errors often resolve themselves with retry, permanent errors waste resources if retried, and data corruption requires immediate client notification to prevent silent data loss\n> - **Consequences**: More complex error handling logic but better user experience and resource utilization, fewer false failures from transient issues\n\n**Error Classification and Propagation Strategy:**\n\n| Error Category | Examples | Local Handling | Propagation Strategy | Client Impact |\n|---|---|---|---|---|\n| Transient | Network timeouts, temporary storage unavailable | Retry with exponential backoff | Propagate after retry exhaustion | Delayed response |\n| Permanent | Disk full, authentication failure | No local retry | Immediate propagation | Error response |\n| Data Corruption | Checksum mismatch, malformed chunks | Log and reject data | Immediate propagation with details | Retry request |\n| Security Violation | Virus detected, policy violation | Quarantine action | Propagate with security context | Upload rejection |\n| Resource Exhaustion | Memory full, connection pool exhausted | Apply backpressure | Propagate with retry guidance | Rate limiting |\n\n**Component-Specific Error Handling:**\n\nEach component in the upload pipeline has specific error handling responsibilities based on its role in the system:\n\n**Upload Manager Error Handling:**\n\n| Error Source | Error Type | Detection Method | Response Strategy | Downstream Impact |\n|---|---|---|---|---|\n| Session Store | Connection timeout | Operation timeout | Retry with backoff, then fail | Client gets 503 Service Unavailable |\n| Storage Backend | Insufficient space | Write operation failure | Attempt cleanup, then propagate | Client gets 507 Insufficient Storage |\n| File Validator | Virus detected | Validation result | Quarantine file, notify client | Client gets security violation response |\n| Internal | State corruption | Invariant checks | Log error, reset session state | Client must restart upload |\n\n**Storage Backend Error Handling:**\n\n| Operation | Failure Mode | Local Recovery | Error Propagation | Impact on Upload |\n|---|---|---|---|---|\n| `InitMultipart` | Authentication failure | Refresh credentials once | Propagate auth error | Session creation fails |\n| `StoreChunk` | Network partition | Retry 3 times with backoff | Propagate timeout after retries | Chunk upload fails |\n| `CompleteMultipart` | Assembly failure | Retry assembly | Propagate assembly error | Completion fails |\n| `DeleteChunk` | Partial cleanup | Log and continue | Don't propagate cleanup errors | Background cleanup |\n\n**Session Store Error Handling:**\n\n| Error Scenario | Detection | Recovery Action | Error Propagation | System Impact |\n|---|---|---|---|---|\n| Concurrent modification | Version mismatch | Refresh and retry | Propagate conflict error | Client retries |\n| Connection pool exhausted | Connection timeout | Queue request with timeout | Propagate overload error | Apply backpressure |\n| Data corruption | Read validation failure | Attempt backup read | Propagate corruption error | Session recovery |\n| Memory exhaustion | Allocation failure | Trigger cleanup | Propagate resource error | Reject new sessions |\n\n**Error Context and Metadata Preservation:**\n\nWhen propagating errors between components, the system preserves essential context for debugging and recovery:\n\n| Context Field | Source | Purpose | Propagation Rule |\n|---|---|---|---|\n| `SessionID` | Upload Manager | Link error to specific upload | Always preserve |\n| `OperationID` | Each component | Trace operation sequence | Preserve in logs |\n| `ErrorCode` | Component generating error | Categorize error type | Preserve for client |\n| `RetryCount` | Error handling logic | Track retry attempts | Reset at component boundaries |\n| `ErrorTimestamp` | Error detection point | Order error sequences | Preserve for analysis |\n| `ComponentStack` | Call chain | Debug error propagation | Preserve in internal errors |\n\n**Client Error Response Format:**\n\nThe Upload Manager standardizes error responses to provide consistent client experience regardless of which internal component generated the error:\n\n| Response Field | Type | Description | Example Value |\n|---|---|---|---|\n| `error` | string | Human-readable error message | \"Chunk checksum validation failed\" |\n| `error_code` | string | Machine-readable error category | \"CHECKSUM_MISMATCH\" |\n| `session_id` | string | Upload session identifier | \"upload_123456789\" |\n| `retry_after` | int | Seconds to wait before retry | 30 |\n| `details` | object | Component-specific error details | {\"expected\": \"abc123\", \"actual\": \"def456\"} |\n\n**Cascading Failure Prevention:**\n\nThe system implements several patterns to prevent component failures from cascading throughout the system:\n\n⚠️ **Pitfall: Error Amplification in Distributed Components**\nWhen storage backends fail, Upload Managers might retry operations across multiple sessions simultaneously, amplifying load on the already-struggling backend. Implement per-backend circuit breakers that temporarily reject operations when failure rates exceed thresholds, allowing backends time to recover.\n\n**Circuit Breaker Implementation Strategy:**\n\n| Component | Failure Threshold | Circuit Open Duration | Recovery Check Method |\n|---|---|---|---|\n| Storage Backend | 5 failures in 60 seconds | 30 seconds | Single test operation |\n| Session Store | 10 failures in 30 seconds | 15 seconds | Health check query |\n| File Validator | 3 timeouts in 120 seconds | 60 seconds | Ping ClamAV socket |\n| External Services | 50% failure rate | 120 seconds | Synthetic health check |\n\n### Implementation Guidance\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|---|---|---|\n| Message Passing | HTTP JSON APIs with standard status codes | gRPC with Protocol Buffers for binary efficiency |\n| Error Handling | Standard Go error interface with wrapped contexts | Structured error types with categorization |\n| Circuit Breakers | Simple failure counting with timeouts | Hystrix-style adaptive circuit breakers |\n| Observability | Standard library logging with JSON format | OpenTelemetry with distributed tracing |\n| State Coordination | Mutex-protected shared state | Actor model with message passing |\n\n**Recommended File/Module Structure:**\n\n```\ninternal/\n  upload/\n    manager.go              ← Upload Manager with flow coordination\n    flows.go                ← Initialization, chunk, completion flows\n    errors.go               ← Error types and handling utilities\n    manager_test.go         ← Flow integration tests\n  coordinator/\n    coordinator.go          ← Component interaction coordination\n    circuit_breaker.go      ← Failure isolation utilities\n  messaging/\n    types.go                ← Request/response message types\n    validation.go           ← Message format validation\n    serialization.go        ← JSON marshaling utilities\n```\n\n**Message Type Definitions (Infrastructure Starter Code):**\n\n```go\npackage messaging\n\nimport (\n    \"time\"\n    \"encoding/json\"\n)\n\n// InitUploadRequest represents the client's request to start a new upload session\ntype InitUploadRequest struct {\n    Filename    string            `json:\"filename\"`\n    ContentType string            `json:\"content_type\"`\n    TotalSize   int64             `json:\"total_size\"`\n    Metadata    map[string]string `json:\"metadata,omitempty\"`\n}\n\n// InitUploadResponse contains the server's session creation confirmation\ntype InitUploadResponse struct {\n    SessionID   string            `json:\"session_id\"`\n    UploadURL   string            `json:\"upload_url\"`\n    ExpiresAt   time.Time         `json:\"expires_at\"`\n    ChunkSize   int64             `json:\"recommended_chunk_size\"`\n    Metadata    map[string]string `json:\"metadata,omitempty\"`\n}\n\n// ChunkUploadResponse confirms successful chunk receipt\ntype ChunkUploadResponse struct {\n    SessionID     string `json:\"session_id\"`\n    CurrentOffset int64  `json:\"current_offset\"`\n    NextOffset    int64  `json:\"next_expected_offset\"`\n    Completed     bool   `json:\"upload_completed\"`\n}\n\n// ErrorResponse provides structured error information to clients\ntype ErrorResponse struct {\n    Error     string      `json:\"error\"`\n    ErrorCode string      `json:\"error_code\"`\n    SessionID string      `json:\"session_id,omitempty\"`\n    RetryAfter int        `json:\"retry_after,omitempty\"`\n    Details   interface{} `json:\"details,omitempty\"`\n}\n\n// Validate checks InitUploadRequest for required fields and constraints\nfunc (r *InitUploadRequest) Validate() error {\n    if r.Filename == \"\" {\n        return errors.New(\"filename required\")\n    }\n    if r.TotalSize <= 0 {\n        return errors.New(\"total_size must be positive\")\n    }\n    if r.ContentType == \"\" {\n        return errors.New(\"content_type required\")\n    }\n    return nil\n}\n```\n\n**Flow Coordination Infrastructure (Infrastructure Starter Code):**\n\n```go\npackage coordinator\n\nimport (\n    \"context\"\n    \"sync\"\n    \"time\"\n)\n\n// CircuitBreaker prevents cascading failures by isolating failing components\ntype CircuitBreaker struct {\n    maxFailures   int\n    resetTimeout  time.Duration\n    failures      int\n    lastFailTime  time.Time\n    state         CircuitState\n    mutex         sync.RWMutex\n}\n\ntype CircuitState int\n\nconst (\n    CircuitClosed CircuitState = iota\n    CircuitOpen\n    CircuitHalfOpen\n)\n\n// NewCircuitBreaker creates a circuit breaker with specified thresholds\nfunc NewCircuitBreaker(maxFailures int, resetTimeout time.Duration) *CircuitBreaker {\n    return &CircuitBreaker{\n        maxFailures:  maxFailures,\n        resetTimeout: resetTimeout,\n        state:        CircuitClosed,\n    }\n}\n\n// Execute runs the provided operation through the circuit breaker\nfunc (cb *CircuitBreaker) Execute(ctx context.Context, operation func() error) error {\n    if !cb.allowRequest() {\n        return ErrCircuitBreakerOpen\n    }\n    \n    err := operation()\n    cb.recordResult(err == nil)\n    return err\n}\n\n// allowRequest determines if the circuit breaker should allow the operation\nfunc (cb *CircuitBreaker) allowRequest() bool {\n    cb.mutex.Lock()\n    defer cb.mutex.Unlock()\n    \n    if cb.state == CircuitOpen {\n        if time.Since(cb.lastFailTime) > cb.resetTimeout {\n            cb.state = CircuitHalfOpen\n            return true\n        }\n        return false\n    }\n    \n    return true\n}\n\n// recordResult updates circuit breaker state based on operation outcome\nfunc (cb *CircuitBreaker) recordResult(success bool) {\n    cb.mutex.Lock()\n    defer cb.mutex.Unlock()\n    \n    if success {\n        cb.failures = 0\n        if cb.state == CircuitHalfOpen {\n            cb.state = CircuitClosed\n        }\n    } else {\n        cb.failures++\n        cb.lastFailTime = time.Now()\n        \n        if cb.failures >= cb.maxFailures {\n            cb.state = CircuitOpen\n        }\n    }\n}\n```\n\n**Upload Flow Coordination (Core Logic Skeleton):**\n\n```go\npackage upload\n\nimport (\n    \"context\"\n    \"io\"\n)\n\n// FlowCoordinator manages the interaction between components during upload operations\ntype FlowCoordinator struct {\n    sessionManager *SessionManager\n    storageBackend StorageBackend\n    fileValidator  *FileValidator\n    circuitBreaker *coordinator.CircuitBreaker\n}\n\n// InitializeUpload coordinates the upload initialization flow across components\nfunc (fc *FlowCoordinator) InitializeUpload(ctx context.Context, req *InitUploadRequest) (*InitUploadResponse, error) {\n    // TODO 1: Generate unique session ID using crypto/rand\n    // TODO 2: Call fileValidator.EnforceResourceLimits() to check size and type limits\n    // TODO 3: Call storageBackend.InitMultipart() to prepare storage resources\n    // TODO 4: Create UploadSession struct with metadata and set status to SessionStatusInitialized\n    // TODO 5: Call sessionManager.CreateSession() to persist session state\n    // TODO 6: Generate upload URL and expiration time for chunk uploads\n    // TODO 7: Return InitUploadResponse with session details and upload parameters\n    // Hint: Use context.WithTimeout for each component call to prevent hanging\n}\n\n// ProcessChunkUpload coordinates chunk validation, storage, and session updates\nfunc (fc *FlowCoordinator) ProcessChunkUpload(ctx context.Context, sessionID string, offset int64, data io.Reader, checksum string) (*ChunkUploadResponse, error) {\n    // TODO 1: Call sessionManager.GetSession() to retrieve current session state\n    // TODO 2: Validate session status is SessionStatusActive or SessionStatusInitialized\n    // TODO 3: Verify offset aligns with current session progress (no gaps or overlaps)\n    // TODO 4: Read chunk data and compute hash to verify against provided checksum\n    // TODO 5: Call storageBackend.StoreChunk() through circuit breaker to persist data\n    // TODO 6: Update session CurrentOffset and chunk tracking metadata\n    // TODO 7: Call sessionManager.UpdateSession() to persist progress\n    // TODO 8: Check if upload is complete (offset + chunk size >= total size)\n    // TODO 9: If complete, trigger completion workflow asynchronously\n    // TODO 10: Return ChunkUploadResponse with updated progress information\n    // Hint: Use defer to ensure session state is consistent even if storage fails\n}\n\n// CompleteUpload coordinates file assembly, validation, and final storage\nfunc (fc *FlowCoordinator) CompleteUpload(ctx context.Context, sessionID string) (*CompletionResponse, error) {\n    // TODO 1: Call sessionManager.GetSession() and verify all chunks received\n    // TODO 2: Set session status to SessionStatusCompleting to prevent new chunks\n    // TODO 3: Call storageBackend.CompleteMultipart() to assemble final file\n    // TODO 4: Create file reader from assembled storage and call fileValidator.ValidateFile()\n    // TODO 5: If validation fails, move file to quarantine using quarantineManager\n    // TODO 6: If validation passes, move file to final storage location\n    // TODO 7: Set session status to SessionStatusCompleted or SessionStatusFailed\n    // TODO 8: Call sessionManager.UpdateSession() with final state\n    // TODO 9: Generate signed download URL if file is clean\n    // TODO 10: Return CompletionResponse with final file location and validation results\n    // Hint: Use database transactions if possible to ensure session state consistency\n}\n```\n\n**Error Handling Utilities (Infrastructure Starter Code):**\n\n```go\npackage upload\n\nimport (\n    \"fmt\"\n    \"errors\"\n)\n\n// Error categories for proper handling and propagation\nvar (\n    ErrTransient     = errors.New(\"transient error\")\n    ErrPermanent     = errors.New(\"permanent error\")\n    ErrDataCorruption = errors.New(\"data corruption\")\n    ErrSecurityViolation = errors.New(\"security violation\")\n    ErrResourceExhaustion = errors.New(\"resource exhaustion\")\n    ErrCircuitBreakerOpen = errors.New(\"circuit breaker open\")\n)\n\n// UploadError provides structured error information with context\ntype UploadError struct {\n    Category  error\n    Component string\n    Operation string\n    SessionID string\n    Cause     error\n    Retryable bool\n    RetryAfter int\n}\n\n// Error implements the error interface\nfunc (e *UploadError) Error() string {\n    return fmt.Sprintf(\"%s in %s.%s (session %s): %v\", \n        e.Category, e.Component, e.Operation, e.SessionID, e.Cause)\n}\n\n// Unwrap supports error unwrapping for errors.Is and errors.As\nfunc (e *UploadError) Unwrap() error {\n    return e.Cause\n}\n\n// IsRetryable determines if the error should trigger retry logic\nfunc (e *UploadError) IsRetryable() bool {\n    return e.Retryable\n}\n\n// NewTransientError creates a retryable error for temporary failures\nfunc NewTransientError(component, operation, sessionID string, cause error) *UploadError {\n    return &UploadError{\n        Category:   ErrTransient,\n        Component:  component,\n        Operation:  operation,\n        SessionID:  sessionID,\n        Cause:      cause,\n        Retryable:  true,\n        RetryAfter: 30,\n    }\n}\n\n// NewPermanentError creates a non-retryable error for permanent failures\nfunc NewPermanentError(component, operation, sessionID string, cause error) *UploadError {\n    return &UploadError{\n        Category:  ErrPermanent,\n        Component: component,\n        Operation: operation,\n        SessionID: sessionID,\n        Cause:     cause,\n        Retryable: false,\n    }\n}\n```\n\n**Language-Specific Hints:**\n\n- Use `context.Context` for all component interactions to enable timeout and cancellation\n- Implement `io.Reader` interfaces for streaming chunk data without loading entire chunks in memory\n- Use `sync.RWMutex` for protecting session state when supporting concurrent chunk uploads\n- Leverage `errors.Is()` and `errors.As()` for proper error type checking and unwrapping\n- Use `go` routines with proper error handling for asynchronous completion workflows\n- Implement proper cleanup with `defer` statements to ensure resources are released on errors\n\n**Milestone Checkpoint:**\n\nAfter implementing component interactions, verify the flow coordination:\n\n1. **Run Integration Tests**: `go test ./internal/upload/... -integration` should show successful component communication\n2. **Test Error Propagation**: Send requests with invalid data and verify proper error responses with correct categories\n3. **Verify Circuit Breaker**: Simulate backend failures and confirm circuit breaker prevents cascading failures\n4. **Check State Consistency**: Kill the server during chunk upload and restart - session state should be recoverable\n\n**Expected Behavior**: \n- Upload initialization should return session metadata within 100ms for valid requests\n- Chunk uploads should coordinate storage and session updates atomically\n- Component failures should propagate with proper error categorization\n- Circuit breakers should isolate failing backends without affecting healthy operations\n\n**Debugging Tips:**\n\n| Symptom | Likely Cause | Diagnosis Method | Fix |\n|---|---|---|---|\n| Upload hangs indefinitely | Missing context timeout in component calls | Check logs for timeout warnings | Add context.WithTimeout to all component calls |\n| Inconsistent session state | Race condition in concurrent updates | Run tests with `-race` flag | Add proper locking around session state updates |\n| Circuit breaker stuck open | Failure threshold too low for normal errors | Monitor circuit breaker metrics | Tune failure thresholds and reset timeouts |\n| Memory leaks during uploads | Missing cleanup in error paths | Use pprof to check goroutine and memory growth | Add defer statements for resource cleanup |\n\n\n## Error Handling and Edge Cases\n\n> **Milestone(s):** All milestones (1, 2, 3) — error handling applies to chunked upload protocol robustness (M1), storage backend failure recovery (M2), and virus scanning reliability (M3)\n\nThink of a resumable file upload service as a **distributed transaction coordinator** managing multiple moving parts that can fail independently. Just as a flight has backup systems for navigation, hydraulics, and engines, our upload service needs comprehensive failure detection and recovery mechanisms for network interruptions, storage outages, and security scanning failures. The key insight is that failures are not exceptional cases but normal operating conditions that must be handled gracefully to maintain service reliability.\n\n![Error Detection and Recovery Flow](./diagrams/error-handling-flow.svg)\n\nThe error handling strategy follows a **layered defense approach** where each component implements local error detection and recovery, while the system maintains global consistency through coordinated failure propagation. This section examines failure scenarios across network connectivity, storage backends, data integrity, and resource exhaustion, providing systematic detection mechanisms and recovery strategies for each category.\n\n### Network and Connection Failures\n\nNetwork failures represent the most common source of upload interruptions, ranging from temporary connectivity loss to client disconnections during large file transfers. The resumable upload protocol specifically addresses these scenarios by maintaining server-side state that survives network interruptions.\n\nThink of network failure handling as a **checkpoint-based recovery system** similar to video game save points. Each successfully uploaded chunk creates a checkpoint that allows the client to resume from that exact position after a network failure, rather than restarting the entire upload from the beginning.\n\n**Connection Failure Detection**\n\nThe system implements multiple layers of connection failure detection to identify network issues promptly and initiate appropriate recovery actions:\n\n| Detection Method | Trigger Condition | Detection Time | Recovery Action |\n|------------------|-------------------|----------------|-----------------|\n| TCP Keep-Alive | Socket-level connection loss | 2-7.5 minutes (OS default) | Immediate session cleanup |\n| HTTP Read Timeout | No data received within timeout | Configurable (30s-300s) | Mark chunk as failed, preserve session |\n| Write Timeout | Cannot send response within timeout | Configurable (30s-300s) | Log failure, client will retry |\n| Heartbeat Messages | No client ping within interval | 60-120 seconds | Mark session as potentially abandoned |\n| Chunk Upload Timeout | Partial chunk data stalls | Configurable per chunk size | Abort current chunk, preserve progress |\n\n**Partial Upload Recovery Mechanisms**\n\nWhen network failures occur during chunk uploads, the system employs several recovery strategies based on the failure timing and completion status:\n\n1. **Pre-Transfer Failures**: If the connection fails before chunk data transmission begins, the client simply retries the chunk upload with the same offset and metadata. The server treats this as a normal chunk upload request.\n\n2. **Mid-Transfer Failures**: When the connection drops during chunk data transmission, the server discards any partially received data and maintains the previous offset. The client queries the current offset and retries the complete chunk.\n\n3. **Post-Transfer Failures**: If the connection fails after chunk data is received but before the response is sent, the server has successfully stored the chunk but the client doesn't know. The client retry will receive a \"chunk already exists\" response with the updated offset.\n\n4. **Checksum Mismatch Recovery**: When chunk data arrives corrupted due to network issues, the server rejects the chunk and requests retransmission. The client maintains the original chunk data and can immediately retry without re-reading from the source file.\n\n**Client Disconnection Handling**\n\nLong-running uploads face inevitable client disconnections from network changes, device sleep, or application restarts. The system handles these scenarios through session persistence and graceful degradation:\n\n> **Decision: Session Persistence Strategy**\n> - **Context**: Clients may disconnect for extended periods (hours or days) during large uploads\n> - **Options Considered**: \n>   - Immediate session cleanup on disconnection\n>   - Fixed TTL-based session expiration  \n>   - Adaptive TTL based on upload progress\n> - **Decision**: Implement adaptive session TTL with minimum 24-hour retention for active uploads\n> - **Rationale**: Large files may take days to upload on slow connections; immediate cleanup loses significant progress; fixed TTL doesn't account for upload velocity\n> - **Consequences**: Requires more storage for session state but dramatically improves user experience for large uploads\n\nThe session persistence mechanism maintains upload state across disconnections through several components:\n\n| Component | Responsibility | Persistence Duration | Cleanup Trigger |\n|-----------|----------------|---------------------|------------------|\n| Session Metadata | Upload ID, filename, total size, current offset | Adaptive (24h-7days) | Explicit expiration or completion |\n| Chunk Registry | List of successfully uploaded chunks with hashes | Same as session | Session cleanup |\n| Storage Reservation | Reserved space for incomplete upload | Same as session | Session cleanup or storage pressure |\n| Authentication Context | Client credentials and permissions | Shorter (1-4 hours) | Token expiration or renewal |\n\n**Timeout Configuration and Adaptation**\n\nNetwork timeout values significantly impact both user experience and resource utilization. The system implements adaptive timeout strategies that adjust based on upload patterns and network conditions:\n\n1. **Initial Conservative Timeouts**: New upload sessions start with conservative timeout values (30-60 seconds) to quickly detect connection issues and provide responsive error feedback.\n\n2. **Progressive Timeout Increase**: As chunks upload successfully, timeout values gradually increase (up to 300-600 seconds) to accommodate varying network conditions and larger chunk sizes.\n\n3. **Regression on Failures**: Network failures trigger timeout reduction to more aggressively detect subsequent issues, with gradual recovery as stability returns.\n\n4. **Client-Specific Adaptation**: The system maintains per-client timeout history to optimize values based on observed connection characteristics and geographic location.\n\n**Connection Pool Management**\n\nServer-side connection pool management prevents resource exhaustion from failed connections while maintaining adequate capacity for active uploads:\n\n> **Key Insight**: Failed connections often remain in system buffers for extended periods, consuming file descriptors and memory. Proactive connection cleanup is essential for service stability.\n\nThe connection pool implements several management strategies:\n\n- **Active Connection Monitoring**: Periodic health checks identify stuck or zombie connections for immediate cleanup\n- **Resource-Based Throttling**: New connections are rejected when system resources (file descriptors, memory, CPU) approach critical thresholds\n- **Client Rate Limiting**: Per-client connection limits prevent individual users from exhausting system capacity\n- **Graceful Degradation**: Under high load, the system prioritizes completing existing uploads over accepting new connections\n\n### Storage Backend Failures\n\nStorage backend failures can occur at multiple levels, from temporary network issues accessing cloud storage to permanent hardware failures in local storage systems. The abstracted storage interface enables uniform error handling across different backend types while allowing backend-specific optimizations.\n\nThink of storage backend failures as **supply chain disruptions** in a manufacturing system. Just as factories maintain multiple suppliers and inventory buffers, the upload service must handle storage unavailability through redundancy, retries, and graceful degradation strategies.\n\n**Backend Availability Monitoring**\n\nThe system implements comprehensive health monitoring for all configured storage backends to detect failures promptly and route traffic appropriately:\n\n| Health Check Type | Frequency | Failure Threshold | Recovery Action |\n|-------------------|-----------|-------------------|------------------|\n| Connectivity Test | Every 30 seconds | 3 consecutive failures | Mark backend unhealthy |\n| Write Operation Test | Every 2 minutes | 2 failures in 5 minutes | Circuit breaker activation |\n| Read Operation Test | Every 2 minutes | 2 failures in 5 minutes | Read-only mode if applicable |\n| Latency Monitoring | Per request | 99th percentile > 10s | Performance degradation alert |\n| Error Rate Monitoring | Per minute | >5% error rate | Automatic failover consideration |\n\n**Circuit Breaker Implementation**\n\nThe circuit breaker pattern prevents cascading failures by temporarily isolating failing storage backends while allowing rapid recovery when service is restored:\n\n| Circuit State | Request Handling | Transition Condition | Duration |\n|---------------|------------------|---------------------|----------|\n| Closed | All requests forwarded to backend | Normal operation | Indefinite |\n| Open | All requests immediately fail | Failure threshold exceeded | 30-300 seconds |\n| Half-Open | Limited requests forwarded to test recovery | Timeout expires | Until success/failure determined |\n\n```\nCircuit Breaker State Transitions:\nClosed → Open: When error rate exceeds threshold (e.g., 50% failures in 1 minute)\nOpen → Half-Open: After timeout period expires (exponential backoff)\nHalf-Open → Closed: When test requests succeed consistently\nHalf-Open → Open: When test requests continue to fail\n```\n\n**Storage Backend Failover Strategies**\n\nWhen primary storage backends become unavailable, the system implements several failover strategies based on configuration and available alternatives:\n\n> **Decision: Multi-Backend Failover Strategy**\n> - **Context**: Cloud storage services experience periodic outages; single backend creates service disruption\n> - **Options Considered**:\n>   - No failover (fail uploads during outages)\n>   - Automatic failover to secondary backend\n>   - Client-selectable backend with fallback\n> - **Decision**: Implement automatic failover with configurable priority order\n> - **Rationale**: Users shouldn't experience upload failures due to backend issues they can't control; automatic failover maintains service availability\n> - **Consequences**: Requires cross-backend upload session migration and consistent metadata management\n\nThe failover mechanism operates through several strategies:\n\n1. **Primary-Secondary Failover**: When the primary backend fails, new uploads automatically route to the secondary backend. Existing uploads attempt to continue on the primary with exponential backoff retry.\n\n2. **Load-Based Distribution**: Under high load or when backends show performance degradation, new uploads distribute across available backends based on capacity and response times.\n\n3. **Geographic Failover**: For global deployments, uploads can failover to backends in different regions when local storage becomes unavailable.\n\n4. **Partial Failure Handling**: When some operations succeed but others fail (e.g., writes succeed but reads fail), the system maintains separate health states for different operation types.\n\n**Data Consistency During Backend Failures**\n\nStorage backend failures can create consistency challenges, especially for multipart uploads where chunks may be stored across multiple backend instances or during failover scenarios:\n\n| Consistency Challenge | Detection Method | Resolution Strategy |\n|-----------------------|------------------|---------------------|\n| Partial Chunk Loss | Chunk verification on upload completion | Re-upload missing chunks from session state |\n| Cross-Backend State Drift | Periodic reconciliation scans | Authoritative state in session store |\n| Failed Multipart Assembly | Assembly operation timeout/error | Retry with exponential backoff, manual recovery |\n| Orphaned Storage Objects | Cleanup job comparing session state to storage | Automated cleanup with safety delays |\n\n**Quota and Space Management**\n\nStorage backends impose various limits that can cause upload failures when exceeded. The system implements proactive quota monitoring and space management:\n\n1. **Quota Monitoring**: Regular checks of available storage quota across all backends, with alerts when approaching limits (e.g., 80% capacity).\n\n2. **Predictive Space Allocation**: Before accepting large uploads, the system verifies sufficient space exists, accounting for multipart upload overhead and concurrent uploads.\n\n3. **Cleanup Job Integration**: Automated cleanup of expired uploads and temporary files helps maintain available space and avoid quota exhaustion.\n\n4. **Emergency Space Recovery**: When approaching storage limits, the system can temporarily reject new uploads while completing in-progress transfers and running emergency cleanup.\n\n**Backend-Specific Error Handling**\n\nDifferent storage backends exhibit unique failure modes that require specialized handling approaches:\n\n**Local Filesystem Errors**:\n- **Disk Full**: Monitor available space and reject uploads when insufficient\n- **Permission Issues**: Validate write permissions during service startup\n- **File System Corruption**: Implement integrity checks and automatic repair where possible\n- **Path Traversal Attacks**: Sanitize all file paths and implement chroot-style restrictions\n\n**S3-Compatible Storage Errors**:\n- **Authentication Failures**: Implement credential refresh and rotation mechanisms\n- **Rate Limiting**: Implement exponential backoff with jitter for 429 responses\n- **Eventual Consistency Issues**: Account for read-after-write consistency delays\n- **Multipart Upload Limits**: Handle part number limits (10,000 parts) and minimum part size requirements (5MB)\n\n**Network Storage Errors**:\n- **Mount Point Failures**: Detect and attempt remount of network file systems\n- **Network Partitions**: Implement timeout and retry logic for network-attached storage\n- **Protocol-Specific Issues**: Handle NFS, SMB, or other protocol-specific error conditions\n\n### Data Corruption Detection and Recovery\n\nData corruption can occur at multiple points in the upload pipeline, from network transmission errors to storage media failures. The system implements comprehensive integrity checking and recovery mechanisms to ensure uploaded files maintain perfect fidelity.\n\nThink of data integrity as a **chain of custody system** used in forensic investigations. Each step in the process must verify that data remains unchanged from the previous step, with documented evidence (checksums) proving integrity throughout the entire pipeline.\n\n**Checksum Validation Pipeline**\n\nThe integrity verification system operates through multiple checksum layers, each serving specific purposes in the upload pipeline:\n\n| Checksum Level | Algorithm | Computed By | Verified By | Failure Response |\n|----------------|-----------|-------------|-------------|------------------|\n| Chunk-Level | SHA-256 | Client before upload | Server on chunk receipt | Reject chunk, request re-upload |\n| Transport-Level | MD5 (HTTP) | HTTP stack | HTTP stack | Automatic retry by HTTP client |\n| Storage-Level | Backend-specific (ETag) | Storage backend | Server on storage completion | Re-upload chunk to storage |\n| File-Level | SHA-256 | Server during assembly | Client after download | Re-assemble from chunks |\n| End-to-End | User-provided hash | Client of original file | Server after assembly | Report corruption to client |\n\n**Corruption Detection Strategies**\n\nThe system employs multiple corruption detection methods to identify data integrity issues at different stages of the upload process:\n\n1. **Real-Time Detection**: Checksum verification occurs immediately when chunks are received, allowing for immediate re-upload requests while the client still has the source data readily available.\n\n2. **Batch Verification**: Periodic integrity scans of stored data detect corruption from storage media degradation or other backend issues that occur after successful upload.\n\n3. **Cross-Reference Validation**: Comparison of chunk checksums in session metadata against actual stored chunk checksums identifies discrepancies from storage backend issues.\n\n4. **Assembly Verification**: Final file assembly includes comprehensive integrity checking that validates both individual chunks and the complete assembled file.\n\n**Chunk-Level Corruption Handling**\n\nIndividual chunk corruption represents the most common data integrity issue, typically caused by network transmission errors or storage write failures:\n\n> **Decision: Chunk Corruption Recovery Strategy**\n> - **Context**: Network transmission can corrupt chunk data; storage writes can fail partially\n> - **Options Considered**:\n>   - Immediate chunk rejection with full re-upload\n>   - Partial corruption repair using error correction codes\n>   - Best-effort storage with corruption flagging\n> - **Decision**: Immediate rejection with full chunk re-upload required\n> - **Rationale**: File integrity is non-negotiable; error correction adds complexity without guaranteeing perfect recovery; full re-upload is simple and reliable\n> - **Consequences**: Higher network utilization for corrupted chunks but guaranteed data integrity\n\nThe chunk corruption recovery process follows these steps:\n\n1. **Corruption Detection**: Server computes SHA-256 hash of received chunk data and compares against client-provided hash in the upload request.\n\n2. **Immediate Rejection**: Corrupted chunks are immediately rejected with a specific error response indicating checksum mismatch and the computed vs. expected hash values.\n\n3. **State Preservation**: The upload session offset remains unchanged, allowing the client to retry the same chunk without affecting subsequent chunk ordering.\n\n4. **Client Retry Logic**: Clients implement exponential backoff retry with a maximum retry count (typically 3-5 attempts) before reporting permanent failure.\n\n5. **Corruption Logging**: All corruption events are logged with detailed metadata for analysis of network quality and potential systematic issues.\n\n**File Assembly Integrity Verification**\n\nWhen all chunks have been uploaded successfully, the file assembly process includes comprehensive integrity verification to ensure the complete file matches client expectations:\n\n| Verification Step | Check Performed | Failure Response | Recovery Action |\n|-------------------|------------------|------------------|------------------|\n| Chunk Completeness | All required chunks present | Assembly failure | Request missing chunks |\n| Chunk Order Verification | Chunks assemble in correct sequence | Assembly failure | Re-upload out-of-order chunks |\n| Individual Chunk Re-verification | Re-compute chunk hashes from storage | Assembly failure | Re-upload corrupted chunks |\n| Complete File Hash | Compute final file SHA-256 | Assembly failure | Full re-assembly attempt |\n| Size Verification | Final file size matches expected | Assembly failure | Investigate chunk size discrepancies |\n\n**Storage-Level Corruption Recovery**\n\nStorage backends can experience corruption after successful writes due to media degradation, bit rot, or system failures. The service implements several strategies to detect and recover from storage-level corruption:\n\n1. **Periodic Integrity Scans**: Background jobs periodically verify stored chunk integrity by re-computing checksums and comparing against session metadata.\n\n2. **Read Verification**: When chunks are read for assembly or download, their checksums are verified to detect corruption that occurred after storage.\n\n3. **Cross-Backend Verification**: For deployments with multiple storage backends, chunk integrity can be verified by comparing checksums across different backend instances.\n\n4. **Redundant Storage**: Critical uploads can be stored redundantly across multiple backends, allowing corruption recovery through cross-reference with uncorrupted copies.\n\n**Client-Side Corruption Detection**\n\nClients play a crucial role in end-to-end integrity verification by maintaining original file checksums and verifying completed uploads:\n\n- **Pre-Upload Checksumming**: Clients compute complete file hashes before beginning uploads and provide these to the server for final verification.\n- **Post-Assembly Verification**: After upload completion, clients can download and verify the assembled file matches the original.\n- **Chunk-Level Client Verification**: Clients maintain chunk-level checksums computed from the original file to verify server-assembled chunks match expected values.\n\n**Corruption Analytics and Prevention**\n\nThe system maintains detailed analytics on corruption patterns to identify systematic issues and improve prevention:\n\n| Metric Category | Tracked Data | Analysis Purpose | Prevention Action |\n|------------------|---------------|------------------|-------------------|\n| Network Corruption Rates | Per-client, per-network corruption frequency | Identify problematic networks/clients | Client-specific retry policies |\n| Storage Backend Corruption | Per-backend corruption rates over time | Detect failing storage systems | Backend health scoring |\n| File Type Correlation | Corruption rates by file type/size | Identify systematic issues | Specialized handling for problematic types |\n| Geographic Correlation | Corruption rates by client location | Network infrastructure quality | Regional optimization strategies |\n\n### Resource Exhaustion Scenarios\n\nResource exhaustion represents a critical threat to service availability, potentially causing cascading failures that affect all users. The system must implement comprehensive resource monitoring, predictive limiting, and graceful degradation to maintain service quality under resource pressure.\n\nThink of resource management as **air traffic control** at a busy airport. Just as controllers must manage runway capacity, gate availability, and airspace congestion to prevent dangerous situations, the upload service must coordinate CPU, memory, disk space, and network bandwidth to prevent system overload while maximizing throughput.\n\n**Resource Monitoring and Thresholds**\n\nThe system implements multi-layered resource monitoring with adaptive thresholds that trigger progressively more restrictive policies as resource pressure increases:\n\n| Resource Type | Warning Threshold | Critical Threshold | Emergency Threshold | Recovery Threshold |\n|---------------|-------------------|-------------------|---------------------|-------------------|\n| Memory Usage | 70% of available | 85% of available | 95% of available | Below 60% |\n| Disk Space | 80% of capacity | 90% of capacity | 95% of capacity | Below 70% |\n| CPU Utilization | 70% sustained | 85% sustained | 95% sustained | Below 60% |\n| File Descriptors | 70% of limit | 85% of limit | 95% of limit | Below 60% |\n| Concurrent Uploads | 70% of capacity | 85% of capacity | 95% of capacity | Below 60% |\n| Network Bandwidth | 80% of capacity | 90% of capacity | 95% of capacity | Below 70% |\n\n**Memory Pressure Management**\n\nMemory exhaustion can occur rapidly during high-concurrency upload scenarios, especially when processing large chunks or maintaining extensive session state. The system implements several memory management strategies:\n\n> **Decision: Memory Management Strategy**\n> - **Context**: Large file uploads with high concurrency can exhaust available memory\n> - **Options Considered**:\n>   - Unlimited memory usage with OS swap handling\n>   - Fixed memory limits per upload session\n>   - Dynamic memory allocation with global limits and spillover\n> - **Decision**: Implement dynamic allocation with global limits, memory pooling, and disk spillover\n> - **Rationale**: Fixed limits waste memory during low load; unlimited usage causes system instability; dynamic allocation optimizes utilization while maintaining stability\n> - **Consequences**: More complex memory management but better resource utilization and system stability\n\n**Memory Management Components**:\n\n1. **Chunk Buffer Pooling**: Pre-allocated buffer pools for chunk processing reduce garbage collection pressure and memory fragmentation. Buffers are reused across multiple upload operations.\n\n2. **Session State Compression**: Large session metadata is compressed when stored in memory, with decompression only when accessed. This reduces memory footprint for dormant sessions.\n\n3. **Disk Spillover**: When memory pressure increases, less frequently accessed session state spills to disk storage, maintaining system responsiveness while preserving session data.\n\n4. **Gradual Memory Reclamation**: Instead of abrupt memory cleanup, the system gradually reclaims memory by expiring oldest inactive sessions first, avoiding performance spikes.\n\n**Disk Space Exhaustion Handling**\n\nDisk space exhaustion can occur from multiple sources: large uploads consuming available space, failed cleanup of temporary files, or log file growth. The system implements proactive space management:\n\n| Space Management Strategy | Trigger Condition | Action Taken | Expected Recovery |\n|---------------------------|-------------------|---------------|-------------------|\n| Proactive Cleanup | 80% disk usage | Clean expired sessions, temp files | 5-15% space recovery |\n| Upload Throttling | 85% disk usage | Reduce concurrent uploads by 50% | Prevent further exhaustion |\n| New Upload Rejection | 90% disk usage | Reject new uploads, continue existing | Stabilize space usage |\n| Emergency Cleanup | 95% disk usage | Aggressive cleanup, oldest sessions first | 10-30% space recovery |\n| Service Degradation | 98% disk usage | Read-only mode, complete existing uploads only | System protection |\n\n**Disk Space Recovery Procedures**:\n\n1. **Expired Session Cleanup**: Automatically remove sessions that exceed configured TTL, freeing associated chunk storage and metadata.\n\n2. **Orphaned File Detection**: Background jobs identify storage files not associated with active sessions and mark them for cleanup after safety delays.\n\n3. **Log Rotation and Compression**: Automated log file rotation with compression and archival to prevent log storage from consuming excessive space.\n\n4. **Temporary File Cleanup**: Removal of temporary files created during upload processing, assembly operations, and virus scanning activities.\n\n**CPU Resource Management**\n\nHigh CPU utilization can occur during intensive operations like virus scanning, file assembly, or cryptographic operations. The system implements CPU resource management through several mechanisms:\n\n1. **Operation Prioritization**: Critical operations (chunk uploads) receive higher priority than background operations (cleanup, virus scanning) to maintain upload throughput.\n\n2. **Throttling and Rate Limiting**: CPU-intensive operations are throttled based on system load, with automatic scaling of operation rates based on available CPU capacity.\n\n3. **Asynchronous Processing**: Heavy operations like virus scanning and file assembly are processed asynchronously to avoid blocking upload operations.\n\n4. **Load Shedding**: Under extreme CPU pressure, non-essential operations are temporarily suspended to maintain core upload functionality.\n\n**Connection and File Descriptor Limits**\n\nNetwork connection and file descriptor exhaustion can cause immediate service failure. The system implements comprehensive connection management:\n\n> **Key Insight**: File descriptor exhaustion is often the first resource limit reached under high load, but it's frequently overlooked in capacity planning. Proactive monitoring and management are essential.\n\n**Connection Management Strategies**:\n\n| Strategy | Implementation | Purpose | Resource Impact |\n|----------|----------------|---------|-----------------|\n| Connection Pooling | Reuse connections for multiple operations | Reduce overhead | Lower file descriptor usage |\n| Idle Connection Cleanup | Close connections idle for >5 minutes | Free resources | Immediate descriptor recovery |\n| Per-Client Limits | Maximum 10 concurrent connections per client | Prevent abuse | Bounded resource usage |\n| Global Connection Limits | Hard limit at 80% of system maximum | System protection | Guaranteed descriptor availability |\n\n**Cascading Failure Prevention**\n\nResource exhaustion in one area can trigger cascading failures throughout the system. The service implements several patterns to prevent failure propagation:\n\n1. **Circuit Breaker Integration**: Resource exhaustion triggers circuit breakers that prevent additional load while recovery occurs.\n\n2. **Bulkhead Pattern**: Different resource types are isolated so exhaustion in one area (e.g., CPU) doesn't immediately cause failures in others (e.g., memory).\n\n3. **Graceful Degradation**: Rather than complete service failure, the system reduces functionality progressively, maintaining core upload capabilities while disabling non-essential features.\n\n4. **Recovery Coordination**: Resource recovery is coordinated across components to prevent thundering herd problems when resources become available again.\n\n**Adaptive Rate Limiting**\n\nTraditional fixed rate limiting doesn't account for varying system capacity under different load conditions. The system implements adaptive rate limiting that adjusts based on resource availability:\n\n- **Dynamic Limits**: Upload rate limits adjust based on current system resource utilization, allowing higher throughput when resources are available.\n- **Client-Specific Adaptation**: Rate limits adapt based on individual client behavior, resource consumption patterns, and historical usage.\n- **Progressive Restriction**: As resource pressure increases, rate limits become progressively more restrictive, with different limits for new vs. existing uploads.\n- **Quick Recovery**: When resource pressure decreases, rate limits relax quickly to restore full service capacity.\n\n**Resource Exhaustion Recovery**\n\nWhen resource exhaustion occurs, the system implements systematic recovery procedures to restore normal operation:\n\n1. **Immediate Stabilization**: Emergency measures to prevent complete system failure, including aggressive cleanup and load shedding.\n\n2. **Gradual Load Restoration**: Careful restoration of normal operation to avoid immediate re-exhaustion, with monitoring for resource utilization trends.\n\n3. **Root Cause Analysis**: Automated analysis of exhaustion causes, including identification of problematic clients, unusual usage patterns, or system configuration issues.\n\n4. **Preventive Measures**: Implementation of additional safeguards based on exhaustion analysis, including enhanced monitoring, adjusted limits, or architectural improvements.\n\n⚠️ **Pitfall: Ignoring Resource Interdependencies**\nMany developers treat resource limits independently, but resource exhaustion often involves complex interdependencies. For example, memory pressure can cause increased disk I/O as the system swaps, which increases CPU usage for I/O wait, which can cause network timeouts due to delayed responses. Always consider how resource exhaustion in one area affects other resources and implement holistic monitoring and management strategies.\n\n⚠️ **Pitfall: Inadequate Recovery Testing**\nResource exhaustion scenarios are often untested because they're difficult to reproduce in development environments. This leads to poor recovery behavior in production. Implement chaos engineering practices that artificially induce resource pressure during testing to validate recovery mechanisms work correctly under actual stress conditions.\n\n### Implementation Guidance\n\n**A. Technology Recommendations Table:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Circuit Breaker | Simple failure counter with timeout | Netflix Hystrix pattern with metrics |\n| Resource Monitoring | Basic system metrics (CPU, memory) | Prometheus with custom metrics and alerting |\n| Error Logging | Standard logging with error levels | Structured logging with correlation IDs |\n| Health Checks | HTTP endpoint with basic status | Comprehensive health with dependency checks |\n| Retry Logic | Exponential backoff with jitter | Adaptive retry with success rate tracking |\n\n**B. Recommended File/Module Structure:**\n```\ninternal/\n  errors/\n    errors.go              ← error type definitions and constructors\n    circuit.go             ← circuit breaker implementation  \n    recovery.go            ← error recovery strategies\n  monitoring/\n    resources.go           ← resource monitoring and thresholds\n    health.go             ← health check implementations\n  storage/\n    errors.go             ← storage-specific error handling\n    retry.go              ← storage operation retry logic\n  validation/\n    corruption.go         ← data integrity verification\n    recovery.go           ← corruption recovery procedures\n```\n\n**C. Infrastructure Starter Code:**\n\n**Error Type System (Complete Implementation):**\n```go\n// Package errors provides comprehensive error handling for the upload service\npackage errors\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"sync\"\n    \"time\"\n)\n\n// UploadError represents all errors in the upload service with categorization\ntype UploadError struct {\n    Category    error\n    Component   string\n    Operation   string\n    SessionID   string\n    Cause       error\n    Retryable   bool\n    RetryAfter  int\n    Metadata    map[string]interface{}\n    Timestamp   time.Time\n}\n\nfunc (e *UploadError) Error() string {\n    return fmt.Sprintf(\"[%s/%s] %s (session: %s): %v\", \n        e.Component, e.Operation, e.Category, e.SessionID, e.Cause)\n}\n\nfunc (e *UploadError) Unwrap() error { return e.Cause }\n\n// Error categories for systematic handling\nvar (\n    ErrTransient         = fmt.Errorf(\"transient error\")\n    ErrPermanent         = fmt.Errorf(\"permanent error\") \n    ErrDataCorruption    = fmt.Errorf(\"data corruption\")\n    ErrSecurityViolation = fmt.Errorf(\"security violation\")\n    ErrResourceExhaustion = fmt.Errorf(\"resource exhaustion\")\n)\n\n// Storage-specific errors\nvar (\n    ErrNotFound              = fmt.Errorf(\"key not found\")\n    ErrAlreadyExists         = fmt.Errorf(\"key already exists\")\n    ErrInsufficientSpace     = fmt.Errorf(\"insufficient storage space\")\n    ErrNetworkTimeout        = fmt.Errorf(\"network operation timeout\")\n    ErrAuthenticationFailed  = fmt.Errorf(\"authentication failed\")\n    ErrInvalidMultipartState = fmt.Errorf(\"invalid multipart upload state\")\n)\n\n// System-level errors\nvar (\n    ErrFileTooLarge      = fmt.Errorf(\"file exceeds size limits\")\n    ErrSystemOverloaded  = fmt.Errorf(\"system resources critically low\")\n    ErrVirusScanTimeout  = fmt.Errorf(\"virus scanning operation timed out\")\n    ErrQuarantineRequired = fmt.Errorf(\"file must be quarantined\")\n)\n\n// NewUploadError creates a categorized error with context\nfunc NewUploadError(category error, component, operation, sessionID string, cause error) *UploadError {\n    return &UploadError{\n        Category:  category,\n        Component: component,\n        Operation: operation,\n        SessionID: sessionID,\n        Cause:     cause,\n        Retryable: isRetryable(category, cause),\n        Metadata:  make(map[string]interface{}),\n        Timestamp: time.Now(),\n    }\n}\n\n// isRetryable determines if an error should trigger retry logic\nfunc isRetryable(category, cause error) bool {\n    switch category {\n    case ErrTransient, ErrNetworkTimeout:\n        return true\n    case ErrPermanent, ErrSecurityViolation:\n        return false\n    case ErrResourceExhaustion:\n        return true // with backoff\n    case ErrDataCorruption:\n        return true // chunk re-upload\n    default:\n        return false\n    }\n}\n```\n\n**Circuit Breaker Implementation (Complete):**\n```go\n// CircuitState represents the current state of the circuit breaker\ntype CircuitState int\n\nconst (\n    CircuitClosed CircuitState = iota\n    CircuitOpen\n    CircuitHalfOpen\n)\n\nvar ErrCircuitBreakerOpen = fmt.Errorf(\"circuit breaker open\")\n\n// CircuitBreaker implements the circuit breaker pattern for storage backends\ntype CircuitBreaker struct {\n    maxFailures   int\n    resetTimeout  time.Duration\n    failures      int\n    lastFailTime  time.Time\n    state         CircuitState\n    mutex         sync.RWMutex\n    onStateChange func(from, to CircuitState)\n}\n\n// NewCircuitBreaker creates a circuit breaker with specified thresholds\nfunc NewCircuitBreaker(maxFailures int, resetTimeout time.Duration) *CircuitBreaker {\n    return &CircuitBreaker{\n        maxFailures:  maxFailures,\n        resetTimeout: resetTimeout,\n        state:        CircuitClosed,\n    }\n}\n\n// Execute runs an operation through the circuit breaker\nfunc (cb *CircuitBreaker) Execute(ctx context.Context, operation func() error) error {\n    // TODO 1: Check current circuit state and return immediately if open\n    // TODO 2: For half-open state, allow limited requests through\n    // TODO 3: Execute the operation and handle success/failure\n    // TODO 4: Update circuit state based on operation result\n    // TODO 5: Trigger state change callbacks if state transitions occur\n    // Hint: Use cb.mutex for thread-safe state updates\n    // Hint: Check if reset timeout has elapsed for open -> half-open transition\n    return fmt.Errorf(\"not implemented\")\n}\n```\n\n**Resource Monitor (Complete Implementation):**\n```go\npackage monitoring\n\nimport (\n    \"context\"\n    \"runtime\"\n    \"sync/atomic\"\n    \"syscall\"\n    \"time\"\n)\n\n// ResourceMonitor tracks system resource utilization\ntype ResourceMonitor struct {\n    memoryWarning    int64\n    memoryCritical   int64\n    diskWarning      int64\n    diskCritical     int64\n    activeUploads    int64\n    maxUploads       int64\n    lastCheck        time.Time\n    checkInterval    time.Duration\n}\n\n// ResourceStatus represents current resource utilization\ntype ResourceStatus struct {\n    MemoryUsed        int64\n    MemoryTotal       int64\n    MemoryPercent     float64\n    DiskUsed          int64\n    DiskTotal         int64\n    DiskPercent       float64\n    ActiveUploads     int64\n    MaxUploads        int64\n    CPUPercent        float64\n    FileDescriptors   int64\n    MaxFileDescriptors int64\n}\n\n// NewResourceMonitor creates a resource monitor with default thresholds\nfunc NewResourceMonitor() *ResourceMonitor {\n    return &ResourceMonitor{\n        checkInterval: 30 * time.Second,\n        maxUploads:    1000, // configurable\n    }\n}\n\n// GetCurrentStatus returns current resource utilization\nfunc (rm *ResourceMonitor) GetCurrentStatus() *ResourceStatus {\n    var memStats runtime.MemStats\n    runtime.ReadMemStats(&memStats)\n    \n    var diskStat syscall.Statfs_t\n    syscall.Statfs(\"/\", &diskStat)\n    \n    return &ResourceStatus{\n        MemoryUsed:    int64(memStats.Alloc),\n        MemoryTotal:   int64(memStats.Sys),\n        MemoryPercent: float64(memStats.Alloc) / float64(memStats.Sys) * 100,\n        DiskUsed:      int64(diskStat.Blocks-diskStat.Bavail) * int64(diskStat.Bsize),\n        DiskTotal:     int64(diskStat.Blocks) * int64(diskStat.Bsize),\n        ActiveUploads: atomic.LoadInt64(&rm.activeUploads),\n        MaxUploads:    rm.maxUploads,\n    }\n}\n\n// IncrementActiveUploads atomically increases the active upload counter\nfunc (rm *ResourceMonitor) IncrementActiveUploads() {\n    atomic.AddInt64(&rm.activeUploads, 1)\n}\n\n// DecrementActiveUploads atomically decreases the active upload counter  \nfunc (rm *ResourceMonitor) DecrementActiveUploads() {\n    atomic.AddInt64(&rm.activeUploads, -1)\n}\n\n// ShouldAcceptUpload returns true if resources allow new uploads\nfunc (rm *ResourceMonitor) ShouldAcceptUpload() bool {\n    status := rm.GetCurrentStatus()\n    \n    // Check multiple resource constraints\n    if status.MemoryPercent > 90 {\n        return false\n    }\n    if status.DiskPercent > 85 {\n        return false \n    }\n    if status.ActiveUploads >= status.MaxUploads {\n        return false\n    }\n    \n    return true\n}\n```\n\n**D. Core Logic Skeleton Code:**\n\n**Error Recovery Coordinator:**\n```go\n// RecoveryCoordinator manages systematic error recovery across components\ntype RecoveryCoordinator struct {\n    sessionManager   *SessionManager\n    storageBackend   StorageBackend\n    resourceMonitor  *ResourceMonitor\n    circuitBreaker   *CircuitBreaker\n}\n\n// HandleUploadError implements comprehensive error handling with recovery\nfunc (rc *RecoveryCoordinator) HandleUploadError(ctx context.Context, err *UploadError) error {\n    // TODO 1: Log error with structured logging including correlation ID\n    // TODO 2: Determine if error is retryable based on category and cause\n    // TODO 3: For transient errors, implement exponential backoff retry\n    // TODO 4: For corruption errors, initiate chunk re-upload process\n    // TODO 5: For resource exhaustion, trigger load shedding mechanisms\n    // TODO 6: For security violations, quarantine session and notify\n    // TODO 7: Update circuit breaker state based on error patterns\n    // TODO 8: Return appropriate error response to client\n    // Hint: Use error.Category to determine recovery strategy\n    // Hint: Check if session needs cleanup or preservation\n    return fmt.Errorf(\"not implemented\")\n}\n\n// RecoverFromStorageFailure handles storage backend failures with failover\nfunc (rc *RecoveryCoordinator) RecoverFromStorageFailure(ctx context.Context, sessionID string, operation string) error {\n    // TODO 1: Identify failed storage backend from error context\n    // TODO 2: Check if alternative backends are available and healthy\n    // TODO 3: Migrate session state to working backend if possible\n    // TODO 4: Update session metadata with new backend information\n    // TODO 5: Retry failed operation on new backend with circuit breaker\n    // TODO 6: If all backends failed, preserve session for later retry\n    // Hint: Use circuit breaker to test backend health before migration\n    return fmt.Errorf(\"not implemented\")\n}\n```\n\n**Corruption Recovery Handler:**\n```go\n// CorruptionRecovery handles data integrity issues and recovery\ntype CorruptionRecovery struct {\n    sessionManager *SessionManager\n    storage       StorageBackend\n}\n\n// HandleChunkCorruption manages chunk-level corruption detection and recovery\nfunc (cr *CorruptionRecovery) HandleChunkCorruption(ctx context.Context, sessionID string, chunkOffset int64, expectedHash, actualHash string) error {\n    // TODO 1: Log corruption event with detailed metadata for analysis\n    // TODO 2: Mark corrupted chunk as failed in session state\n    // TODO 3: Clean up corrupted chunk data from storage backend\n    // TODO 4: Reset session offset to allow chunk re-upload\n    // TODO 5: Generate corruption response with expected vs actual hashes\n    // TODO 6: Update corruption metrics for monitoring and alerting\n    // Hint: Preserve session state but rollback chunk progress\n    // Hint: Include retry count to prevent infinite corruption loops\n    return fmt.Errorf(\"not implemented\")\n}\n\n// VerifyAssemblyIntegrity performs comprehensive integrity checking during file assembly\nfunc (cr *CorruptionRecovery) VerifyAssemblyIntegrity(ctx context.Context, sessionID string) (*ValidationResult, error) {\n    // TODO 1: Load session metadata and verify all chunks are present\n    // TODO 2: Re-compute hash for each stored chunk and verify against session\n    // TODO 3: Perform chunk ordering verification for correct assembly\n    // TODO 4: Compute complete file hash and compare against expected\n    // TODO 5: Verify final file size matches session total size\n    // TODO 6: Generate detailed validation report with any discrepancies\n    // Hint: Use streaming verification to handle large files efficiently\n    // Hint: Stop verification at first error to avoid unnecessary processing\n    return nil, fmt.Errorf(\"not implemented\")\n}\n```\n\n**E. Language-Specific Hints:**\n\n- **Error Wrapping**: Use `fmt.Errorf(\"operation failed: %w\", err)` for error wrapping with Go 1.13+ \n- **Context Cancellation**: Always check `ctx.Done()` in long-running error recovery operations\n- **Atomic Operations**: Use `sync/atomic` for counters accessed from multiple goroutines (active uploads, error counts)\n- **Mutex Strategy**: Use `sync.RWMutex` for circuit breaker state (many reads, few writes)\n- **Time Handling**: Use `time.Now().UTC()` for consistent timestamp handling across time zones\n- **Resource Monitoring**: Import `runtime` and `syscall` packages for system resource access\n- **Structured Logging**: Consider using `logrus` or `zap` for structured error logging with correlation IDs\n\n**F. Milestone Checkpoint:**\n\nAfter implementing error handling mechanisms, verify the following behaviors:\n\n**Milestone 1 Checkpoint (Network Failures):**\n```bash\n# Test chunk upload with connection interruption\ncurl -X POST \"http://localhost:8080/upload/test123/chunk\" \\\n     -H \"Content-Range: bytes 0-1023/2048\" \\\n     --data-binary @chunk1.dat &\n# Kill curl during upload and retry - should resume correctly\n\n# Expected: Server preserves session state, client can query offset and retry\n```\n\n**Milestone 2 Checkpoint (Storage Failures):**\n```bash\n# Test storage backend failover\n# Stop primary storage backend (e.g., kill minio container)\ncurl -X POST \"http://localhost:8080/upload/test456/chunk\" \\\n     -H \"Content-Range: bytes 0-1023/2048\" \\\n     --data-binary @chunk1.dat\n\n# Expected: Upload continues on secondary backend, session remains consistent\n```\n\n**Milestone 3 Checkpoint (Resource Exhaustion):**\n```bash\n# Test upload rejection under resource pressure\n# Create memory pressure with large concurrent uploads\nfor i in {1..100}; do\n  curl -X POST \"http://localhost:8080/upload/test$i/chunk\" \\\n       --data-binary @largechunk.dat &\ndone\n\n# Expected: Service maintains responsiveness, rejects new uploads when appropriate\n```\n\n**G. Debugging Tips:**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Uploads randomly fail after 50% | Circuit breaker triggering | Check circuit breaker state logs | Increase failure threshold or improve backend reliability |\n| Memory usage grows continuously | Session state not cleaned up | Monitor active session count vs memory | Implement aggressive session cleanup |\n| Storage operations timeout | Backend overloaded or network issues | Check backend response times and error rates | Add retry logic with exponential backoff |\n| Chunk corruption increases over time | Storage backend degradation | Compare corruption rates across backends | Migrate to healthy backend, investigate storage issues |\n| Service becomes unresponsive | Resource exhaustion | Check CPU, memory, file descriptor usage | Implement load shedding and resource limits |\n| Recovery takes too long | Synchronous recovery blocking requests | Check if recovery operations are async | Move recovery to background jobs |\n\n\n## Testing Strategy and Validation\n\n> **Milestone(s):** All milestones (1, 2, 3) — comprehensive testing approach covering chunked upload protocol verification (M1), storage backend integration validation (M2), and virus scanning compliance testing (M3)\n\nThink of testing a resumable upload service as *quality assurance for a complex assembly line*. Just as a manufacturing plant needs quality checkpoints at every stage—incoming materials inspection, assembly verification, and final product testing—our upload service requires validation at every layer. Each chunk upload is like a component arriving on the assembly line, storage backends are like different production facilities, and virus scanning is like final quality control before shipping. A comprehensive testing strategy ensures that not only does each individual stage work correctly, but the entire pipeline operates reliably under normal conditions, degrades gracefully under stress, and recovers properly from failures.\n\nThe testing pyramid for resumable uploads has unique characteristics compared to typical web services. At the foundation, we need unit tests that can simulate network interruptions, storage failures, and partial file corruption. The integration layer must validate protocol compliance with tus.io specifications, verify seamless handoffs between storage backends, and confirm that virus scanning doesn't interfere with the upload flow. At the apex, performance testing must handle concurrent uploads, resource exhaustion scenarios, and the complex interactions between chunked transfers and backend storage limitations.\n\n### Unit Testing Strategy\n\n**Component isolation** in resumable upload testing requires sophisticated mocking strategies because components are tightly coupled through shared state and file system interactions. Think of unit testing here as *testing individual orchestra musicians with a metronome instead of the full symphony*. Each component must perform its part correctly even when other components are simulated, but the simulation must be realistic enough to catch timing issues, state inconsistencies, and resource conflicts that only emerge in the integrated system.\n\nThe core challenge in unit testing resumable uploads lies in **state persistence simulation**. Unlike stateless HTTP services where mocks can be simple request-response pairs, our components maintain complex state across multiple interactions. The `SessionManager` must handle concurrent access to upload sessions, the `StorageBackend` must simulate partial write failures and recovery scenarios, and the `FileValidator` must process files that arrive in non-contiguous chunks.\n\n> **Decision: Mock Strategy for Storage Operations**\n> - **Context**: Storage backends perform complex multipart operations that can fail at any stage, and unit tests need to verify component behavior without actual cloud API calls\n> - **Options Considered**: \n>   1. Simple success/failure mocks with boolean flags\n>   2. Stateful mocks that track multipart upload progression\n>   3. Fault injection mocks that can simulate specific failure scenarios\n> - **Decision**: Implement stateful fault injection mocks with scenario-based failure simulation\n> - **Rationale**: Simple mocks miss critical edge cases like partial multipart completion, while stateful mocks with fault injection can test recovery paths that are difficult to trigger with real backends\n> - **Consequences**: More complex test setup but comprehensive coverage of failure scenarios that are expensive to test against real storage\n\n| Mock Component | Purpose | State Tracking | Failure Simulation |\n|---|---|---|---|\n| `MockStorageBackend` | Simulate storage operations without external dependencies | Track multipart uploads, part numbers, completion status | Inject failures at specific operations, simulate timeout scenarios |\n| `MockVirusScanner` | Control scan results and timing | Track scan requests, simulate processing delays | Return specific threats, timeout on large files, connection failures |\n| `MockStateStore` | In-memory session persistence with controllable failures | Maintain session state, track concurrent access patterns | Simulate database connection loss, write conflicts, corruption |\n| `MockCredentialProvider` | Authentication simulation with expiration scenarios | Track credential refresh cycles, expiration timing | Simulate authentication failures, credential rotation edge cases |\n\nThe **session lifecycle testing** requires careful orchestration of concurrent operations. Each test must verify not just that operations succeed, but that they maintain consistency when interleaved with other operations. For example, testing chunk upload requires verifying that concurrent chunks for the same session are processed in the correct order, that duplicate chunks are handled idempotently, and that session state updates are atomic.\n\n```\nUnit Test Categories by Component:\n\nSessionManager Tests:\n- Session creation with duplicate ID handling\n- Concurrent session updates with optimistic locking\n- Session expiration with active upload protection\n- State recovery after simulated process restart\n\nStorageBackend Tests:\n- Multipart upload initialization and part tracking\n- Chunk storage with size validation and checksum verification\n- Assembly completion with part ordering and integrity checks\n- Cleanup of abandoned multipart uploads\n\nFileValidator Tests:  \n- Magic byte detection with incomplete file headers\n- Virus scanning with various malware signatures\n- File type validation against content-type mismatches\n- Quarantine workflow with metadata preservation\n\nFlowCoordinator Tests:\n- Upload initialization with backend selection\n- Chunk processing pipeline with validation integration  \n- Error propagation and recovery coordination\n- Resource limit enforcement and circuit breaker integration\n```\n\n**Test data management** for unit tests requires realistic file samples that cover edge cases in chunked uploads. The test suite needs files that are exactly at chunk boundaries, files that compress poorly (affecting bandwidth calculations), files with embedded malware signatures for scanner testing, and corrupted files with invalid checksums. Each test file should be deterministically generated to ensure reproducible test results across different environments.\n\n⚠️ **Pitfall: Inadequate Concurrency Testing**\nMany developers write unit tests that verify sequential operation but miss race conditions in concurrent chunk uploads. For example, testing that chunks A, B, C upload successfully in sequence doesn't catch the bug where chunk C arrives before chunk B and overwrites session state incorrectly. Always include tests where operations are deliberately interleaved using goroutines with controlled synchronization points to verify thread safety.\n\n### Integration Testing Scenarios\n\n**End-to-end upload flows** in integration testing must validate the complete journey from client request to final file storage, including all intermediate state transitions and cross-component interactions. Think of integration testing as *rehearsing the entire theatrical performance with all actors, stage crew, and technical systems working together*. While unit tests verify that individual actors know their lines, integration tests ensure the entire production flows smoothly, scene changes happen on cue, and technical effects are synchronized with the performance.\n\nThe integration test environment requires careful setup to simulate realistic conditions while maintaining test isolation and repeatability. Each test needs its own isolated storage namespace, dedicated virus scanner configuration, and independent session state to prevent tests from interfering with each other. However, the environment should closely mirror production conditions, including network latency, storage consistency models, and virus scanner performance characteristics.\n\n| Integration Scenario | Components Involved | Validation Points | Failure Injection |\n|---|---|---|---|\n| Complete Upload Flow | Client → FlowCoordinator → SessionManager → StorageBackend → FileValidator | Protocol compliance, state consistency, file integrity | Network interruption during chunk upload, storage failure during assembly |\n| Cross-Backend Migration | SessionManager → Multiple StorageBackends → CredentialProvider | Data consistency across backends, credential handling | Authentication failure during migration, partial copy scenarios |\n| Concurrent Upload Sessions | Multiple Clients → FlowCoordinator → Shared Resources | Resource contention, session isolation, performance degradation | Memory pressure, connection pool exhaustion |\n| Virus Detection Workflow | Client → StorageBackend → FileValidator → QuarantineManager | Infected file isolation, metadata preservation, notification flow | Scanner timeout, quarantine storage failure |\n\n**Protocol compliance testing** ensures that our service correctly implements the tus.io resumable upload specification. This involves both testing our server against the official tus protocol test suite and verifying that standard tus clients can successfully complete uploads. The tests must cover all supported tus extensions (creation, termination, checksum, expiration) and verify correct HTTP status codes, header formats, and state transitions.\n\n```\ntus.io Protocol Compliance Tests:\n\nUpload Creation:\n- POST to creation endpoint returns 201 Created with Location header\n- Upload-Length header is properly parsed and validated  \n- Upload-Metadata header encoding/decoding with base64 values\n- Creation with Upload-Defer-Length for unknown size uploads\n\nChunk Upload:\n- PATCH requests with Upload-Offset header for resumption\n- Content-Range header validation and offset verification\n- Checksum verification using Upload-Checksum header (SHA-1, MD5)\n- 409 Conflict response for offset mismatches\n\nUpload Termination:\n- DELETE requests properly clean up session and storage\n- 404 Not Found for terminated or expired uploads\n- Cleanup of associated multipart uploads in storage backends\n\nProtocol Extensions:\n- HEAD requests return current upload progress in Upload-Offset\n- OPTIONS requests advertise supported extensions\n- Upload expiration handling with Upload-Expires header\n```\n\n**Backend integration validation** tests the service against real storage backends to verify that our abstraction layer correctly handles the nuances of different storage systems. While unit tests use mocks, integration tests connect to actual S3, GCS, and local storage to validate credential handling, multipart upload mechanics, and error condition behavior that can only be observed with real backends.\n\nThe challenge in backend integration testing lies in managing test data lifecycle and handling the eventual consistency models of cloud storage. Tests must create isolated storage namespaces, properly clean up test artifacts even after test failures, and account for the timing differences between local filesystem operations and cloud storage propagation delays.\n\n> **Decision: Backend Integration Test Environment**\n> - **Context**: Integration tests need to validate against real storage backends but must be isolated, repeatable, and cost-effective\n> - **Options Considered**:\n>   1. Shared test buckets with namespace isolation\n>   2. Dynamic test bucket creation per test run  \n>   3. Local storage emulators (MinIO, fake-gcs-server)\n> - **Decision**: Hybrid approach using local emulators for development and real backends for CI/CD pipeline\n> - **Rationale**: Local emulators enable fast developer feedback and offline testing, while real backend testing in CI catches integration issues that emulators might miss\n> - **Consequences**: More complex test infrastructure but comprehensive validation across development and production scenarios\n\n**Virus scanning integration** requires special test files and controlled environments to validate threat detection without introducing actual malware into the test environment. The integration tests use EICAR test files (standard antivirus test signatures) and custom test patterns to verify that the scanning pipeline correctly identifies threats, quarantines infected files, and maintains detailed audit logs of scanning decisions.\n\n### Milestone Validation Checkpoints\n\n**Progressive testing goals** provide clear success criteria for each implementation milestone, enabling developers to verify that their implementation meets requirements before proceeding to the next stage. Think of these checkpoints as *quality gates in a software factory*—each milestone must pass its validation criteria before the next phase begins, ensuring that complexity is added incrementally on a solid foundation.\n\nEach milestone validation includes both functional verification (does the feature work correctly) and integration verification (does the feature work correctly with previously implemented components). This prevents the common problem where individual features work in isolation but break when combined, which is particularly important in resumable uploads where components are tightly coupled through shared state and file system interactions.\n\n**Milestone 1: Chunked Upload Protocol Validation**\n\nThe first milestone validation focuses on core protocol compliance and basic resumability. The validation environment should include network simulation tools that can introduce controlled failures, latency, and bandwidth constraints to verify that the protocol correctly handles real-world network conditions.\n\n| Validation Category | Test Description | Success Criteria | Failure Indicators |\n|---|---|---|---|\n| Protocol Compliance | Upload initialization with tus.io client | Returns valid upload URL with correct headers | Missing Location header, incorrect status codes |\n| Chunk Processing | Sequential chunk upload with offset tracking | Each chunk advances offset correctly, accepts next chunk | Offset drift, chunk ordering failures |\n| Resumption Logic | Interrupt and resume upload mid-transfer | Client can query progress and continue from correct offset | Lost progress, incorrect resume offset |\n| Session Management | Concurrent uploads with different sessions | Sessions remain isolated, no state bleeding | Session confusion, corrupted metadata |\n\n```\nMilestone 1 Checkpoint Commands:\n\n# Start the upload service with chunked protocol enabled\ngo run cmd/server/main.go --config=test-configs/milestone1.json\n\n# Initialize upload using curl (simulating tus client)\ncurl -X POST http://localhost:8080/uploads \\\n  -H \"Upload-Length: 10485760\" \\\n  -H \"Content-Type: application/offset+octet-stream\"\n\n# Upload first chunk  \ncurl -X PATCH http://localhost:8080/uploads/$UPLOAD_ID \\\n  -H \"Upload-Offset: 0\" \\\n  -H \"Content-Type: application/offset+octet-stream\" \\\n  --data-binary @testdata/chunk-001.bin\n\n# Verify resumption by querying progress\ncurl -X HEAD http://localhost:8080/uploads/$UPLOAD_ID\n\nExpected Response Headers:\nUpload-Offset: 1048576  # Size of uploaded chunk\nUpload-Length: 10485760  # Total file size\n```\n\n**Milestone 2: Storage Abstraction Validation**\n\nThe second milestone validation verifies that storage backend abstraction works correctly with multiple backend types and that uploads can be distributed across different storage systems. This includes testing credential management, backend failover, and the consistency of the storage interface across different backend implementations.\n\nThe validation environment should include multiple storage backends (local, S3-compatible, and optionally GCS) with different configurations to verify that the abstraction layer correctly handles the unique characteristics of each backend while providing a consistent interface to higher-level components.\n\n| Backend Type | Configuration | Validation Tests | Success Metrics |\n|---|---|---|---|\n| Local Storage | Filesystem with quota limits | Large file assembly, concurrent access, disk space handling | Files assembled correctly, no corruption, proper cleanup |\n| S3 Compatible | MinIO or AWS S3 with multipart | Multipart upload coordination, credential rotation, signed URLs | Multipart uploads complete, URLs work, credentials refresh |\n| Storage Migration | Cross-backend transfer | Upload to one backend, migrate to another | File integrity preserved, metadata maintained |\n\n**Milestone 3: Virus Scanning & Validation**\n\nThe final milestone validation ensures that file validation and security measures work correctly without interfering with the upload protocol or storage operations. This includes testing with various file types, sizes, and threat patterns to verify that the security pipeline provides comprehensive protection while maintaining good performance.\n\nThe validation environment requires a properly configured ClamAV instance with current virus definitions, test files with known signatures, and monitoring tools to verify that scanning integrates smoothly with the upload flow without introducing unacceptable latency or resource consumption.\n\n```\nMilestone 3 Security Validation Tests:\n\n# Clean file upload (should complete normally)\nupload_test_file testdata/clean-document.pdf\nverify_file_in_storage clean-document.pdf\n\n# Infected file upload (should be quarantined)  \nupload_test_file testdata/eicar-test.txt\nverify_file_quarantined eicar-test.txt\nverify_scan_log_entry FOUND \"Eicar-Test-Signature\"\n\n# File type mismatch (should be rejected)\nupload_with_wrong_content_type testdata/executable.bin \"image/jpeg\"  \nverify_upload_rejected \"content type mismatch\"\n\n# Oversized file (should be rejected during initialization)\nupload_oversized_file testdata/huge-file.bin\nverify_upload_rejected \"file size exceeds limit\"\n```\n\n⚠️ **Pitfall: Testing Only Happy Path Scenarios**\nMany milestone validations focus on successful uploads and miss critical failure scenarios. For example, testing only clean files misses the case where virus scanning fails due to scanner unavailability. Always include negative test cases: what happens when the virus scanner is down, when storage quotas are exceeded, or when clients send malformed protocol requests. These failure scenarios often reveal integration issues that happy path testing misses.\n\n### Performance and Load Testing\n\n**Concurrent upload handling** testing validates that the service maintains consistent performance and correctness when handling multiple simultaneous uploads. Think of this as *stress-testing a shipping dock during peak season*—the system must not only handle the increased volume but maintain the same quality standards for package sorting, routing, and delivery tracking that it provides during normal operations.\n\nPerformance testing for resumable uploads has unique characteristics because of the persistent state requirements and the complex interactions between chunked uploads and storage backend limitations. Unlike stateless web services where load testing primarily focuses on request throughput and response times, resumable upload testing must also validate state consistency under load, memory usage growth patterns, and the behavior of long-running upload sessions under resource pressure.\n\nThe performance test environment should simulate realistic client behavior patterns, including uploads that start and stop unpredictably, clients that disappear mid-upload (requiring session cleanup), and the mixture of small frequent uploads and large infrequent uploads that characterizes real-world usage. The test infrastructure must be capable of generating sustained load while monitoring detailed metrics about session state, storage operations, and resource utilization.\n\n| Load Test Scenario | Concurrent Users | Upload Patterns | Validation Metrics | Resource Monitoring |\n|---|---|---|---|---|\n| Steady State Load | 50-100 concurrent uploads | Mixed file sizes (1MB-1GB), normal completion rates | Throughput stability, response time percentiles | Memory growth, CPU usage, file descriptor count |\n| Burst Traffic | 10x normal load for 10 minutes | Many small files with rapid initialization | Session creation rate, storage backend saturation | Connection pool usage, temporary storage growth |\n| Long-Running Sessions | 10-20 large file uploads (5GB+) | Slow upload rates, frequent interruptions | Session persistence, cleanup effectiveness | Memory per session, storage space utilization |\n| Failure Recovery Load | Normal load + induced failures | Backend failures, scanner timeouts | Recovery time, data consistency | Error rate impact on resource usage |\n\n**Resource utilization validation** ensures that the service operates within acceptable memory, CPU, and storage bounds under various load conditions. The service must handle resource pressure gracefully, implementing backpressure mechanisms and load shedding when necessary to prevent cascading failures that could affect all upload sessions.\n\nMemory usage patterns in resumable upload services are particularly complex because session state must persist across potentially long time periods, chunk metadata accumulates throughout the upload process, and temporary buffers are required for virus scanning and file assembly. The testing must verify that memory usage grows predictably with load and that cleanup processes effectively reclaim resources from completed or abandoned uploads.\n\n```\nResource Utilization Test Matrix:\n\nMemory Testing:\n- Session state memory growth with 1000+ concurrent uploads\n- Chunk metadata memory usage for large files (10GB+ in 5MB chunks)\n- Temporary buffer memory during virus scanning and assembly\n- Memory leak detection during extended test runs (24+ hours)\n\nCPU Testing:  \n- Checksum calculation CPU impact during chunk processing\n- File assembly CPU usage for large uploads\n- Virus scanning CPU overhead with various file types\n- Background cleanup process CPU utilization\n\nStorage Testing:\n- Temporary storage growth patterns during multipart uploads\n- Cleanup effectiveness for abandoned sessions and failed uploads  \n- Storage backend connection pool utilization\n- Local temporary space management during file assembly\n\nNetwork Testing:\n- Bandwidth utilization efficiency with concurrent chunked uploads\n- Connection reuse effectiveness with persistent clients\n- Network timeout handling under various latency conditions\n- Protocol overhead measurement for small vs large chunks\n```\n\n**Circuit breaker and backpressure testing** validates that the service degrades gracefully under extreme load conditions rather than failing catastrophically. The testing should verify that circuit breakers open appropriately when backend services become unavailable, that new upload requests are rejected gracefully when resource limits are approached, and that existing uploads continue to progress even when new uploads are being throttled.\n\n> **Decision: Load Testing Infrastructure**\n> - **Context**: Performance testing requires realistic load generation that simulates actual client behavior patterns including network interruptions and resumption\n> - **Options Considered**:\n>   1. Simple HTTP load testing tools (wrk, hey) with static payloads\n>   2. Custom tus.io protocol clients with resumption simulation\n>   3. Hybrid approach with protocol-aware load generation and resource monitoring\n> - **Decision**: Custom protocol-aware load testing with realistic client behavior simulation\n> - **Rationale**: Generic HTTP load tools miss the critical resumption patterns that differentiate resumable uploads from simple file uploads, while protocol-aware testing reveals performance issues specific to stateful upload scenarios\n> - **Consequences**: More complex test infrastructure development but realistic performance validation that matches production usage patterns\n\nThe load testing infrastructure must include comprehensive monitoring and alerting to detect performance degradation before it impacts upload reliability. This includes tracking not just basic metrics like request throughput and response times, but also upload service specific metrics like session state consistency, chunk assembly success rates, and backend storage health.\n\n**Scalability validation** tests the service's ability to handle growth in upload volume, file sizes, and concurrent users. The testing should identify bottlenecks in session management, storage backend coordination, and virus scanning throughput that would limit the service's ability to scale horizontally or vertically.\n\n| Scalability Dimension | Test Approach | Scaling Bottlenecks | Validation Criteria |\n|---|---|---|---|\n| Upload Volume | Increase concurrent uploads until saturation | Session manager throughput, storage backend limits | Linear performance degradation, graceful overload handling |\n| File Size | Test with increasingly large files (100MB to 10GB+) | Memory usage per upload, assembly time complexity | Memory usage scales sub-linearly with file size |\n| Session Count | Long-running test with accumulating sessions | Session storage overhead, cleanup efficiency | Memory per session remains bounded |\n| Backend Scaling | Multiple storage backends with load distribution | Backend selection logic, credential management | Load balances effectively across backends |\n\n⚠️ **Pitfall: Testing in Unrealistic Environments**\nPerformance testing often occurs in environments that don't match production characteristics—faster networks, more powerful hardware, or simplified storage configurations. This can miss performance issues that only appear under production conditions. Always include testing scenarios that simulate production network latency, hardware constraints, and storage backend limitations. For example, testing against local MinIO doesn't reveal the impact of S3's eventual consistency model on upload completion detection.\n\n### Implementation Guidance\n\nThe testing strategy implementation requires careful coordination between test infrastructure, realistic data generation, and comprehensive monitoring. The key challenge is creating a testing environment that accurately simulates production conditions while maintaining the isolation and repeatability required for reliable automated testing.\n\n**A. Technology Recommendations Table:**\n\n| Testing Component | Simple Option | Advanced Option |\n|---|---|---|\n| Unit Testing Framework | Go testing stdlib + testify assertions | Ginkgo BDD framework + Gomega matchers |\n| Mock Generation | Manual mocks with interfaces | GoMock generated mocks + Testcontainers |\n| Integration Testing | Docker Compose with real services | Kubernetes test environment + Helm charts |\n| Load Testing | Custom Go client with goroutines | k6 with custom tus.io extension |\n| Monitoring | Prometheus metrics + Grafana | Full observability stack (Jaeger, ELK) |\n| Test Data | Static test files | Property-based testing with rapid |\n\n**B. Recommended File/Module Structure:**\n```\nproject-root/\n  cmd/\n    server/main.go              ← production server entry point\n    load-test/main.go           ← load testing client\n  internal/\n    upload/\n      session_manager.go        ← core upload logic\n      session_manager_test.go   ← unit tests\n      integration_test.go       ← integration test suite\n    storage/\n      backend.go                ← storage interface\n      backend_test.go           ← backend unit tests\n      integration_test.go       ← multi-backend integration tests\n    validator/\n      file_validator.go         ← security validation\n      file_validator_test.go    ← validation unit tests\n  test/\n    fixtures/                   ← test data files\n      clean-files/              ← virus-free test files\n      infected-files/           ← EICAR test signatures\n      malformed-files/          ← corrupted/invalid files\n    integration/\n      docker-compose.yml        ← test environment setup\n      tus-protocol-test.go      ← protocol compliance tests\n    performance/\n      load-test-scenarios.go    ← load testing scenarios\n      benchmarks_test.go        ← Go benchmark tests\n    mocks/\n      storage_mock.go           ← storage backend mocks\n      scanner_mock.go           ← virus scanner mocks\n```\n\n**C. Infrastructure Starter Code:**\n\nThis complete test infrastructure provides the foundation for implementing the comprehensive testing strategy:\n\n```go\n// test/mocks/storage_mock.go\npackage mocks\n\nimport (\n    \"context\"\n    \"io\"\n    \"sync\"\n    \"time\"\n    \"fmt\"\n    \"errors\"\n)\n\n// MockStorageBackend provides comprehensive storage simulation with fault injection\ntype MockStorageBackend struct {\n    mu              sync.RWMutex\n    chunks          map[string][]byte\n    multipartUploads map[string]*MockMultipartUpload\n    failureScenarios map[string]FailureScenario\n    operationDelays  map[string]time.Duration\n    bytesStored     int64\n    operationCounts map[string]int\n}\n\ntype MockMultipartUpload struct {\n    ID       string\n    Key      string  \n    Parts    map[int]*MultipartPart\n    Metadata map[string]string\n    Created  time.Time\n    Aborted  bool\n}\n\ntype FailureScenario struct {\n    Operation    string\n    FailAfterOps int\n    ErrorType    error\n    RecoveryTime time.Duration\n}\n\nfunc NewMockStorageBackend() *MockStorageBackend {\n    return &MockStorageBackend{\n        chunks:           make(map[string][]byte),\n        multipartUploads: make(map[string]*MockMultipartUpload),\n        failureScenarios: make(map[string]FailureScenario),\n        operationDelays:  make(map[string]time.Duration),\n        operationCounts:  make(map[string]int),\n    }\n}\n\n// InjectFailure configures the mock to fail after specified number of operations\nfunc (m *MockStorageBackend) InjectFailure(operation string, failAfter int, err error) {\n    m.mu.Lock()\n    defer m.mu.Unlock()\n    m.failureScenarios[operation] = FailureScenario{\n        Operation:    operation,\n        FailAfterOps: failAfter,\n        ErrorType:    err,\n    }\n}\n\n// SetOperationDelay simulates network latency for specific operations\nfunc (m *MockStorageBackend) SetOperationDelay(operation string, delay time.Duration) {\n    m.mu.Lock()\n    defer m.mu.Unlock()\n    m.operationDelays[operation] = delay\n}\n\nfunc (m *MockStorageBackend) checkFailure(operation string) error {\n    m.mu.Lock()\n    defer m.mu.Unlock()\n    \n    m.operationCounts[operation]++\n    \n    if scenario, exists := m.failureScenarios[operation]; exists {\n        if m.operationCounts[operation] >= scenario.FailAfterOps {\n            return scenario.ErrorType\n        }\n    }\n    \n    if delay, exists := m.operationDelays[operation]; exists {\n        time.Sleep(delay)\n    }\n    \n    return nil\n}\n\nfunc (m *MockStorageBackend) StoreChunk(ctx context.Context, key string, data io.Reader, size int64) error {\n    if err := m.checkFailure(\"StoreChunk\"); err != nil {\n        return err\n    }\n    \n    chunkData, err := io.ReadAll(data)\n    if err != nil {\n        return err\n    }\n    \n    if int64(len(chunkData)) != size {\n        return errors.New(\"size mismatch\")\n    }\n    \n    m.mu.Lock()\n    defer m.mu.Unlock()\n    m.chunks[key] = chunkData\n    m.bytesStored += size\n    \n    return nil\n}\n\nfunc (m *MockStorageBackend) InitMultipart(ctx context.Context, key string, metadata map[string]string) (*MultipartUpload, error) {\n    if err := m.checkFailure(\"InitMultipart\"); err != nil {\n        return nil, err\n    }\n    \n    uploadID := fmt.Sprintf(\"mock-upload-%d\", time.Now().UnixNano())\n    \n    m.mu.Lock()\n    defer m.mu.Unlock()\n    \n    mockUpload := &MockMultipartUpload{\n        ID:       uploadID,\n        Key:      key,\n        Parts:    make(map[int]*MultipartPart),\n        Metadata: metadata,\n        Created:  time.Now(),\n    }\n    \n    m.multipartUploads[uploadID] = mockUpload\n    \n    return &MultipartUpload{\n        ID:       uploadID,\n        Key:      key,\n        Backend:  \"mock\",\n        Metadata: metadata,\n        CreatedAt: time.Now(),\n    }, nil\n}\n\n// GetStats returns mock backend statistics for monitoring\nfunc (m *MockStorageBackend) GetStats() map[string]interface{} {\n    m.mu.RLock()\n    defer m.mu.RUnlock()\n    \n    return map[string]interface{}{\n        \"chunks_stored\":      len(m.chunks),\n        \"bytes_stored\":       m.bytesStored,\n        \"multipart_uploads\":  len(m.multipartUploads),\n        \"operation_counts\":   m.operationCounts,\n    }\n}\n```\n\n```go\n// test/integration/tus_protocol_test.go\npackage integration\n\nimport (\n    \"testing\"\n    \"net/http\"\n    \"bytes\"\n    \"fmt\"\n    \"strconv\"\n    \"io\"\n)\n\n// TusProtocolTester validates tus.io specification compliance\ntype TusProtocolTester struct {\n    baseURL    string\n    httpClient *http.Client\n    t          *testing.T\n}\n\nfunc NewTusProtocolTester(baseURL string, t *testing.T) *TusProtocolTester {\n    return &TusProtocolTester{\n        baseURL:    baseURL,\n        httpClient: &http.Client{},\n        t:          t,\n    }\n}\n\n// TestCompleteUploadFlow validates entire tus.io protocol sequence\nfunc (tpt *TusProtocolTester) TestCompleteUploadFlow() {\n    // Step 1: Initialize upload\n    uploadID := tpt.initializeUpload(\"test-file.bin\", 1048576, map[string]string{\n        \"filename\": \"dGVzdC1maWxlLmJpbg==\", // base64 encoded\n        \"filetype\": \"YXBwbGljYXRpb24vb2N0ZXQtc3RyZWFt\", // base64 encoded\n    })\n    \n    // Step 2: Upload chunks\n    chunkSize := 262144 // 256KB chunks\n    for offset := 0; offset < 1048576; offset += chunkSize {\n        remainingBytes := 1048576 - offset\n        currentChunkSize := chunkSize\n        if remainingBytes < chunkSize {\n            currentChunkSize = remainingBytes\n        }\n        \n        chunkData := make([]byte, currentChunkSize)\n        for i := range chunkData {\n            chunkData[i] = byte((offset + i) % 256) // Predictable test pattern\n        }\n        \n        tpt.uploadChunk(uploadID, offset, chunkData)\n        \n        // Verify progress after each chunk\n        progress := tpt.getUploadProgress(uploadID)\n        expectedOffset := offset + currentChunkSize\n        if progress != expectedOffset {\n            tpt.t.Fatalf(\"Progress mismatch: expected %d, got %d\", expectedOffset, progress)\n        }\n    }\n    \n    // Step 3: Verify completion\n    finalProgress := tpt.getUploadProgress(uploadID)\n    if finalProgress != 1048576 {\n        tpt.t.Fatalf(\"Upload incomplete: expected 1048576, got %d\", finalProgress)\n    }\n}\n\nfunc (tpt *TusProtocolTester) initializeUpload(filename string, totalSize int, metadata map[string]string) string {\n    req, _ := http.NewRequest(\"POST\", tpt.baseURL+\"/uploads\", nil)\n    req.Header.Set(\"Upload-Length\", strconv.Itoa(totalSize))\n    req.Header.Set(\"Content-Type\", \"application/offset+octet-stream\")\n    \n    // Add metadata header in tus format  \n    var metadataHeader []string\n    for key, value := range metadata {\n        metadataHeader = append(metadataHeader, fmt.Sprintf(\"%s %s\", key, value))\n    }\n    if len(metadataHeader) > 0 {\n        req.Header.Set(\"Upload-Metadata\", strings.Join(metadataHeader, \",\"))\n    }\n    \n    resp, err := tpt.httpClient.Do(req)\n    if err != nil {\n        tpt.t.Fatalf(\"Failed to initialize upload: %v\", err)\n    }\n    defer resp.Body.Close()\n    \n    if resp.StatusCode != 201 {\n        tpt.t.Fatalf(\"Expected status 201, got %d\", resp.StatusCode)\n    }\n    \n    location := resp.Header.Get(\"Location\")\n    if location == \"\" {\n        tpt.t.Fatal(\"Missing Location header in upload initialization response\")\n    }\n    \n    // Extract upload ID from location URL\n    parts := strings.Split(location, \"/\")\n    return parts[len(parts)-1]\n}\n\nfunc (tpt *TusProtocolTester) uploadChunk(uploadID string, offset int, data []byte) {\n    req, _ := http.NewRequest(\"PATCH\", \n        fmt.Sprintf(\"%s/uploads/%s\", tpt.baseURL, uploadID),\n        bytes.NewReader(data))\n    \n    req.Header.Set(\"Upload-Offset\", strconv.Itoa(offset))\n    req.Header.Set(\"Content-Type\", \"application/offset+octet-stream\")\n    \n    resp, err := tpt.httpClient.Do(req)\n    if err != nil {\n        tpt.t.Fatalf(\"Failed to upload chunk: %v\", err)  \n    }\n    defer resp.Body.Close()\n    \n    if resp.StatusCode != 204 {\n        body, _ := io.ReadAll(resp.Body)\n        tpt.t.Fatalf(\"Expected status 204, got %d. Response: %s\", resp.StatusCode, string(body))\n    }\n}\n\nfunc (tpt *TusProtocolTester) getUploadProgress(uploadID string) int {\n    req, _ := http.NewRequest(\"HEAD\", \n        fmt.Sprintf(\"%s/uploads/%s\", tpt.baseURL, uploadID), nil)\n    \n    resp, err := tpt.httpClient.Do(req)\n    if err != nil {\n        tpt.t.Fatalf(\"Failed to get upload progress: %v\", err)\n    }\n    defer resp.Body.Close()\n    \n    if resp.StatusCode != 200 {\n        tpt.t.Fatalf(\"Expected status 200, got %d\", resp.StatusCode)\n    }\n    \n    offsetHeader := resp.Header.Get(\"Upload-Offset\")\n    if offsetHeader == \"\" {\n        tpt.t.Fatal(\"Missing Upload-Offset header in progress response\")\n    }\n    \n    offset, err := strconv.Atoi(offsetHeader)\n    if err != nil {\n        tpt.t.Fatalf(\"Invalid Upload-Offset value: %s\", offsetHeader)\n    }\n    \n    return offset\n}\n```\n\n**D. Core Logic Skeleton Code:**\n\n```go\n// test/performance/load_test_scenarios.go\npackage performance\n\nimport (\n    \"context\"\n    \"sync\"\n    \"time\"\n    \"math/rand\"\n)\n\n// LoadTestScenario defines a comprehensive load testing scenario\ntype LoadTestScenario struct {\n    Name             string\n    ConcurrentUsers  int\n    TestDuration     time.Duration  \n    UploadPatterns   []UploadPattern\n    FailureInjection FailureConfig\n    \n    // TODO 1: Initialize test metrics collection (throughput, latency, errors)\n    // TODO 2: Set up monitoring for resource utilization (memory, CPU, connections)\n    // TODO 3: Configure realistic client behavior patterns (pauses, retries, abandonment)\n    // TODO 4: Implement graceful test termination and cleanup\n}\n\ntype UploadPattern struct {\n    FileSizeMin    int64\n    FileSizeMax    int64\n    ChunkSizeMin   int64  \n    ChunkSizeMax   int64\n    InterruptRate  float64 // Probability of interrupting upload\n    ResumeDelay    time.Duration\n    CompletionRate float64 // Probability of completing vs abandoning\n}\n\nfunc (lts *LoadTestScenario) ExecuteScenario(ctx context.Context) (*LoadTestResults, error) {\n    // TODO 1: Start resource monitoring goroutines\n    // TODO 2: Launch concurrent upload workers based on ConcurrentUsers\n    // TODO 3: Distribute upload patterns across workers with realistic timing\n    // TODO 4: Collect and aggregate metrics throughout test duration\n    // TODO 5: Generate comprehensive results report with percentiles and resource usage\n    // Hint: Use sync.WaitGroup to coordinate worker completion\n    // Hint: Use channels to collect metrics from workers without blocking\n    // Hint: Monitor context cancellation for graceful shutdown\n}\n\nfunc (lts *LoadTestScenario) simulateRealisticUpload(ctx context.Context, pattern UploadPattern, results chan<- UploadResult) {\n    // TODO 1: Generate test file with specified size range and predictable content\n    // TODO 2: Initialize upload session using tus protocol\n    // TODO 3: Upload chunks with realistic timing patterns (network simulation)\n    // TODO 4: Randomly interrupt and resume uploads based on InterruptRate\n    // TODO 5: Record detailed metrics (timing, bytes transferred, failures)\n    // Hint: Use time.Sleep with jitter to simulate network conditions\n    // Hint: Track each protocol operation separately (init, chunk, resume)\n}\n\n// BenchmarkConcurrentUploads provides Go benchmark integration\nfunc BenchmarkConcurrentUploads(b *testing.B) {\n    // TODO 1: Set up test server with production-like configuration\n    // TODO 2: Create realistic test data with various file sizes\n    // TODO 3: Run upload scenarios with increasing concurrency levels\n    // TODO 4: Report bytes/second throughput and memory allocations\n    // Hint: Use b.SetBytes() to report throughput in MB/s\n    // Hint: Use b.ResetTimer() after setup to exclude initialization time\n}\n```\n\n**E. Milestone Checkpoint Implementation:**\n\n```go\n// test/integration/milestone_validation.go  \npackage integration\n\n// MilestoneValidator provides automated validation for each implementation milestone\ntype MilestoneValidator struct {\n    serverURL     string\n    storageConfig StorageConfig\n    testDataPath  string\n}\n\nfunc (mv *MilestoneValidator) ValidateMilestone1() error {\n    // TODO 1: Verify upload initialization returns correct tus headers\n    // TODO 2: Test chunk upload sequence with offset tracking\n    // TODO 3: Interrupt upload mid-stream and verify resumption works\n    // TODO 4: Test concurrent uploads maintain session isolation\n    // TODO 5: Verify session cleanup removes expired uploads\n    // Expected: All uploads complete successfully, offsets track correctly\n    // Signs of issues: offset drift, session confusion, memory leaks\n}\n\nfunc (mv *MilestoneValidator) ValidateMilestone2() error {\n    // TODO 1: Test upload to local storage backend  \n    // TODO 2: Test upload to S3-compatible backend with multipart\n    // TODO 3: Verify signed URL generation and access\n    // TODO 4: Test backend failover scenarios\n    // TODO 5: Validate file integrity across different backends\n    // Expected: Files stored correctly regardless of backend choice\n    // Signs of issues: multipart assembly failures, credential errors\n}\n\nfunc (mv *MilestoneValidator) ValidateMilestone3() error {\n    // TODO 1: Upload clean file and verify it reaches final storage\n    // TODO 2: Upload EICAR test file and verify quarantine placement  \n    // TODO 3: Test file type validation with content/extension mismatches\n    // TODO 4: Verify size limit enforcement during initialization\n    // TODO 5: Test scanner timeout handling with slow scan simulation\n    // Expected: Clean files stored, threats quarantined, violations rejected\n    // Signs of issues: infected files in production storage, false positives\n}\n\n```\n\n\n## Debugging Guide and Common Issues\n\n> **Milestone(s):** All milestones (1, 2, 3) — systematic debugging approaches apply to chunked upload protocol issues (M1), storage backend failures (M2), and virus scanning problems (M3)\n\nThink of debugging a resumable upload service as being a **detective solving a multi-layered mystery**. Unlike simple web services where failures are often immediate and obvious, upload services involve complex state machines, distributed storage operations, and long-running processes that can fail at any point. Each failure leaves traces across multiple components — upload session state, storage backend operations, virus scanning logs, and client-server protocol exchanges. Your job as a debugger is to collect these clues systematically and reconstruct what happened during the upload operation.\n\nThe challenge with upload service debugging is that failures often manifest far from their root cause. A client might report \"upload stuck at 85%\" when the actual problem is a storage backend authentication failure that occurred during chunk assembly. Or a virus scanner timeout might appear as a generic \"upload failed\" error to the client. This section provides a systematic approach to trace symptoms back to their root causes and implement effective fixes.\n\n### Diagnostic Techniques and Tools\n\nModern upload service debugging requires a multi-layered observability strategy that captures the complete picture of upload operations. Think of this as building a **comprehensive flight recorder** that tracks every significant event, state transition, and resource utilization across all system components. Unlike traditional request-response debugging, upload operations span minutes or hours, cross multiple storage operations, and involve asynchronous background processing.\n\n#### Structured Logging Strategy\n\nThe foundation of upload service debugging is comprehensive structured logging that creates an audit trail for every upload operation. Each log entry should contain sufficient context to reconstruct the complete upload timeline without requiring correlation across multiple log sources.\n\n| Log Level | Component | Events Logged | Context Fields |\n|-----------|-----------|---------------|----------------|\n| INFO | Session Manager | Session creation, state transitions, expiration | SessionID, UserID, Filename, TotalSize, CurrentState |\n| INFO | Upload Protocol | Chunk uploads, offset updates, completion | SessionID, ChunkOffset, ChunkSize, ContentHash, ProtocolPhase |\n| INFO | Storage Backend | Multipart operations, storage completion | SessionID, Backend, OperationType, StorageKey, OperationDuration |\n| INFO | Virus Scanner | Scan initiation, results, quarantine | SessionID, ScanDuration, ThreatName, ScanResult, QuarantineReason |\n| WARN | Circuit Breaker | State changes, threshold breaches | Component, PreviousState, NewState, FailureCount, ResetTime |\n| ERROR | All Components | Operation failures, timeouts, corruption | SessionID, ErrorCategory, Component, Operation, RetryAttempt |\n\nThe logging implementation should use structured formats (JSON) with consistent field naming across components. This enables automated log analysis and correlation during incident response.\n\n```go\n// Example structured log entry\n{\n  \"timestamp\": \"2024-01-15T10:30:45Z\",\n  \"level\": \"ERROR\",\n  \"component\": \"storage-backend\",\n  \"session_id\": \"upload-abc123\",\n  \"operation\": \"complete-multipart\",\n  \"backend\": \"s3\",\n  \"error_category\": \"authentication\",\n  \"retry_attempt\": 2,\n  \"storage_key\": \"uploads/abc123/final\",\n  \"message\": \"S3 credentials expired during multipart completion\",\n  \"context\": {\n    \"total_parts\": 15,\n    \"completed_parts\": 15,\n    \"upload_id\": \"s3-multipart-xyz789\"\n  }\n}\n```\n\n#### State Inspection Tools\n\nUpload services require specialized tools to inspect and analyze upload session state, particularly for diagnosing resumed uploads and partial failures. These tools should provide both real-time monitoring and historical analysis capabilities.\n\n| Tool Purpose | Implementation | Key Features |\n|--------------|----------------|---------------|\n| Session Inspector | HTTP admin endpoint | Current session state, chunk bitmap, metadata inspection |\n| Storage Analyzer | CLI tool + web UI | Backend consistency checking, orphaned chunk detection |\n| Protocol Validator | Network interceptor | tus.io compliance verification, header analysis |\n| Resource Monitor | Dashboard + alerts | Memory usage, disk space, concurrent upload tracking |\n\nThe session inspector provides detailed visibility into upload session state that's essential for debugging resume failures:\n\n| Inspection Area | Information Provided | Debugging Value |\n|-----------------|---------------------|------------------|\n| Session Metadata | ID, filename, total size, content type, creation time | Identifies session parameters and age |\n| Progress Tracking | Current offset, chunk bitmap, completion percentage | Shows exact upload progress and gaps |\n| State Machine | Current state, transition history, timestamps | Reveals state transition failures |\n| Backend Status | Multipart upload ID, storage key, backend type | Enables storage-side investigation |\n| Validation Results | File type detection, virus scan status, quarantine flags | Identifies security processing issues |\n\n#### Resource Monitoring and Alerting\n\nUpload services consume significant system resources and require proactive monitoring to prevent resource exhaustion failures. The monitoring system should track both global resource utilization and per-upload resource consumption patterns.\n\n| Resource Category | Metrics Tracked | Alert Thresholds | Recovery Actions |\n|------------------|----------------|------------------|------------------|\n| Memory Usage | Process heap, upload buffers, session cache | >80% warning, >90% critical | Enable load shedding, cleanup expired sessions |\n| Disk Storage | Temp storage usage, backend quota utilization | >85% warning, >95% critical | Reject new uploads, trigger cleanup |\n| File Descriptors | Open files, socket connections, temp files | >70% warning, >85% critical | Close idle connections, cleanup temp files |\n| Network Bandwidth | Upload throughput, backend API calls | Sustained >80% utilization | Enable rate limiting, queue management |\n\nThe resource monitoring system should integrate with the circuit breaker pattern to automatically protect against resource exhaustion:\n\n| Resource State | Circuit Breaker Action | Client Communication | Recovery Trigger |\n|----------------|----------------------|---------------------|------------------|\n| Normal | Allow all operations | Standard responses | N/A |\n| Warning | Enable rate limiting | HTTP 429 with Retry-After | Resource usage drops below warning |\n| Critical | Reject new uploads | HTTP 503 Service Unavailable | Resource usage drops below 70% |\n| Emergency | Terminate in-progress uploads | HTTP 500 with explanation | Manual intervention required |\n\n#### Network-Level Debugging\n\nUpload protocol debugging often requires analysis of HTTP-level communication between clients and the upload service. This is particularly important for diagnosing tus.io protocol compliance issues and network-related failures.\n\n| Analysis Tool | Purpose | Key Capabilities |\n|---------------|---------|------------------|\n| HTTP Request Logger | Protocol compliance verification | Header inspection, request/response correlation |\n| Network Packet Capture | Low-level network debugging | TCP analysis, connection tracking, timeout investigation |\n| Load Balancer Logs | Infrastructure-level debugging | Request routing, health check failures, backend selection |\n\n> **Design Insight:** Network-level debugging becomes critical when debugging resumed uploads because the client's perception of upload progress may differ from the server's recorded state. This commonly occurs when load balancers or proxies interfere with chunked upload protocols.\n\n### Common Symptoms and Root Causes\n\nUpload service failures manifest in predictable patterns that can be systematically mapped to underlying root causes. Understanding these symptom-to-cause relationships enables rapid diagnosis and targeted fixes rather than random troubleshooting.\n\n#### Upload Resumption Failures\n\nUpload resumption failures are among the most complex debugging scenarios because they involve coordination between client state, server state, and storage backend state. The client believes it can resume from a specific offset, but the server or storage backend may have different information.\n\n| Symptom | Client Behavior | Root Cause Category | Specific Causes |\n|---------|-----------------|-------------------|-----------------|\n| \"Resume from wrong offset\" | Client resumes from old offset, server rejects chunks | State synchronization failure | Session state corruption, storage backend inconsistency |\n| \"Cannot resume upload\" | Client receives 404 for session ID | Session lifecycle issues | Premature session cleanup, database connection failure |\n| \"Chunks rejected during resume\" | Server accepts resume but rejects subsequent chunks | Storage backend misalignment | Multipart upload state corruption, backend credential expiration |\n| \"Upload restarts from beginning\" | Client loses all progress information | Client-server protocol mismatch | Incorrect tus.io header handling, session ID collision |\n\n**Root Cause Analysis for Offset Mismatch:**\n\nThe most common resumption failure involves offset misalignment between client and server state. This occurs when the client's recorded progress doesn't match the server's recorded progress, typically due to one of these scenarios:\n\n1. **Chunk acceptance vs. storage completion**: The server acknowledged chunk receipt but failed to persist the chunk to storage before a restart\n2. **Concurrent upload handling**: Multiple client processes uploading to the same session with inadequate synchronization\n3. **Storage backend eventual consistency**: Cloud storage backends reporting different completion status than actual stored data\n4. **Session state corruption**: Database or memory corruption affecting the recorded current offset\n\n**Diagnostic Approach:**\n1. Compare client's intended resume offset with server's recorded current offset\n2. Verify storage backend state by listing actual stored chunks\n3. Check for gap between acknowledged chunks and successfully stored chunks\n4. Examine logs for any storage operations that failed after chunk acknowledgment\n\n#### Storage Backend Connectivity Issues\n\nStorage backend failures often manifest as generic \"upload failed\" errors but have specific patterns based on the underlying backend type and failure mode.\n\n![Error Detection and Recovery Flow](./diagrams/error-handling-flow.svg)\n\n| Backend Type | Symptom | Common Root Causes | Diagnostic Indicators |\n|--------------|---------|-------------------|----------------------|\n| S3-Compatible | Multipart completion fails | Credential expiration, bucket permissions, part size violations | HTTP 403/400 errors, AWS error codes |\n| Local Filesystem | Chunk storage fails | Disk space exhaustion, permission issues, filesystem corruption | ENOSPC, EACCES error codes |\n| Google Cloud Storage | Upload initialization fails | Service account issues, quota exceeded, regional restrictions | GCS error responses, quota usage metrics |\n\n**Storage Backend Circuit Breaker Patterns:**\n\nStorage backends should implement circuit breaker patterns to prevent cascading failures when backend services become unavailable:\n\n| Circuit State | Failure Threshold | Behavior | Recovery Condition |\n|---------------|------------------|----------|-------------------|\n| Closed (Normal) | <3 failures in 1 minute | All operations allowed | N/A |\n| Half-Open (Testing) | N/A | Limited operations for testing | 5 successful operations |\n| Open (Failed) | ≥3 failures in 1 minute | All operations fail-fast | 30-second timeout elapsed |\n\n#### Virus Scanning Bottlenecks\n\nVirus scanning operations introduce asynchronous processing that can create complex failure scenarios, particularly when scan times exceed upload completion times or when the virus scanner becomes unavailable.\n\n| Symptom | Upload Behavior | Root Cause | Resolution Strategy |\n|---------|-----------------|------------|-------------------|\n| \"Upload completes but file unavailable\" | Upload shows complete, download fails | File pending virus scan | Implement scan status API |\n| \"Upload hangs at 100%\" | Upload never transitions to completed | Virus scanner timeout/failure | Implement scan timeout with fallback |\n| \"False positive quarantine\" | Valid files quarantined unnecessarily | Scanner signature issues | Implement quarantine review process |\n| \"Scanner unavailable\" | All uploads fail validation | ClamAV daemon down | Implement scanner failover/bypass |\n\n**Virus Scanning State Management:**\n\nThe virus scanning workflow requires careful state management to handle the asynchronous nature of file validation:\n\n| File State | Virus Scan Status | Client Behavior | System Behavior |\n|------------|------------------|-----------------|-----------------|\n| Uploaded | Scan pending | Upload shows \"processing\" | File queued for scanning |\n| Uploaded | Scan in progress | Upload shows \"validating\" | File locked from access |\n| Uploaded | Scan completed (clean) | Upload shows \"completed\" | File available for download |\n| Uploaded | Scan completed (threat) | Upload shows \"quarantined\" | File moved to quarantine storage |\n| Uploaded | Scan failed/timeout | Upload shows \"validation failed\" | File marked for manual review |\n\n#### Performance Degradation Patterns\n\nPerformance issues in upload services typically manifest as increased upload times, timeout failures, or resource exhaustion. These issues often compound during high-load periods and require systematic analysis to identify bottlenecks.\n\n| Performance Symptom | Measurement | Likely Bottlenecks | Diagnostic Tools |\n|-------------------|-------------|-------------------|------------------|\n| Increased chunk upload latency | >2x normal response time | Memory pressure, disk I/O contention | Process monitoring, I/O metrics |\n| Upload timeout failures | Client-side timeout triggers | Network congestion, CPU starvation | Network analysis, CPU profiling |\n| Reduced concurrent uploads | Throughput decline under load | File descriptor limits, memory exhaustion | Resource monitoring, connection tracking |\n| Backend operation delays | Storage API latency increases | Backend rate limiting, credential issues | Backend-specific monitoring |\n\n### Systematic Troubleshooting Workflow\n\nEffective upload service debugging requires a systematic workflow that minimizes time-to-resolution while ensuring comprehensive diagnosis. This workflow provides a structured approach that scales from simple issues to complex multi-component failures.\n\n#### Primary Diagnostic Workflow\n\nThe primary workflow follows a layered approach, starting with the most accessible diagnostic information and progressively diving deeper into system state and external dependencies.\n\n**Phase 1: Initial Assessment (0-5 minutes)**\n\n1. **Identify the failure scope**: Determine if the issue affects a single upload session, a specific user, or the entire service\n2. **Collect basic session information**: Session ID, user ID, upload timeline, current state\n3. **Check service health indicators**: Overall system status, resource utilization, active alert conditions\n4. **Review recent deployments**: Any configuration changes or code deployments in the last 24 hours\n\n| Assessment Question | Information Source | Decision Criteria |\n|-------------------|-------------------|------------------|\n| Single session or widespread? | Session manager metrics | <5 affected sessions = isolated, >50 = systemic |\n| Recent system changes? | Deployment logs | Changes within 24 hours require rollback consideration |\n| Resource constraints? | Resource monitoring | >85% utilization indicates resource exhaustion |\n| External dependencies? | Circuit breaker status | Open circuits indicate backend failures |\n\n**Phase 2: Session State Analysis (5-15 minutes)**\n\n1. **Inspect upload session details**: Current state, progress tracking, metadata consistency\n2. **Analyze state transition history**: Identify where the session progression stopped or failed\n3. **Verify storage backend status**: Check multipart upload state and chunk inventory\n4. **Review component interaction logs**: Trace the flow of operations across system components\n\n| Analysis Area | Key Information | Red Flags |\n|---------------|----------------|-----------|\n| Session Metadata | Creation time, total size, current offset | Offset exceeds total size, negative values |\n| State Transitions | State change timestamps, trigger events | Missing transitions, rapid state cycling |\n| Storage Operations | Backend operations, success/failure status | Failed operations without retry |\n| Component Interactions | Message flow, timing, error propagation | Broken message chains, timeout patterns |\n\n**Phase 3: Deep System Investigation (15-45 minutes)**\n\n1. **Examine system resource usage patterns**: Memory allocation, disk I/O, network utilization\n2. **Analyze external dependency health**: Storage backend status, virus scanner availability\n3. **Investigate concurrent operation interference**: Resource contention, lock conflicts\n4. **Review error correlation across components**: Identify cascading failure patterns\n\n**Phase 4: Root Cause Confirmation (45-60 minutes)**\n\n1. **Reproduce the failure scenario**: Create controlled test cases that trigger the same failure\n2. **Validate the hypothesized root cause**: Confirm that fixing the suspected issue resolves the problem\n3. **Assess impact scope**: Determine how many sessions or users are affected by the same root cause\n4. **Plan remediation strategy**: Immediate fixes, preventive measures, and monitoring improvements\n\n#### Recovery and Remediation Procedures\n\nOnce root cause analysis identifies the underlying issue, systematic recovery procedures ensure service restoration while preventing data loss or corruption.\n\n| Recovery Scenario | Immediate Actions | Data Protection | Prevention |\n|------------------|------------------|-----------------|------------|\n| Session state corruption | Reset session to last known good state | Preserve uploaded chunks | Implement state checksums |\n| Storage backend failure | Activate failover backend | Verify data consistency | Improve health checking |\n| Resource exhaustion | Enable load shedding | Complete in-progress uploads | Implement adaptive limits |\n| Virus scanner failure | Bypass scanning with manual review | Queue files for later scanning | Implement scanner redundancy |\n\n**Session Recovery Procedures:**\n\nWhen upload sessions enter inconsistent states, recovery requires careful coordination between session state, storage backend state, and client expectations:\n\n1. **State Assessment**: Compare session metadata with actual storage backend state\n2. **Consistency Verification**: Verify that recorded progress matches stored chunks\n3. **Progress Recalculation**: Rebuild progress information from storage backend inventory\n4. **Client Notification**: Inform client of the correct resume offset if different from recorded state\n\n| Recovery Step | Action Taken | Verification Required |\n|---------------|-------------|----------------------|\n| Assess current state | Read session from state store | Confirm session exists and is accessible |\n| Check storage consistency | List stored chunks from backend | Verify chunk sequence and sizes |\n| Recalculate progress | Sum successfully stored chunk sizes | Compare with recorded current offset |\n| Update session state | Write corrected state to store | Confirm state update succeeded |\n| Notify client | Provide correct resume information | Verify client can continue upload |\n\n**Data Integrity Verification:**\n\nRecovery procedures must include comprehensive data integrity verification to ensure that repaired sessions maintain data consistency:\n\n1. **Chunk Sequence Verification**: Ensure all chunks are present and correctly ordered\n2. **Checksum Validation**: Verify that chunk checksums match expected values\n3. **Size Consistency**: Confirm that total stored size matches session metadata\n4. **Backend Consistency**: Verify that storage backend state aligns with session state\n\n#### Preventive Monitoring Setup\n\nEffective debugging depends on proactive monitoring that detects issues before they become critical failures. The monitoring setup should provide early warning indicators and automated response capabilities.\n\n| Monitoring Category | Key Metrics | Alert Conditions | Automated Responses |\n|-------------------|------------|------------------|-------------------|\n| Session Health | Session creation rate, completion rate, failure rate | Completion rate drops below 95% | Scale up resources, investigate failures |\n| Resource Utilization | Memory usage, disk space, file descriptors | Usage exceeds 85% of capacity | Enable load shedding, trigger cleanup |\n| Backend Performance | API latency, error rates, timeout frequency | Latency increases 2x baseline | Activate circuit breakers |\n| Security Events | Virus detection rate, quarantine events | Unusual threat patterns | Alert security team, enhance scanning |\n\n⚠️ **Pitfall: Over-reliance on symptom monitoring**\nMany teams focus exclusively on monitoring symptoms (failed uploads, high latency) rather than leading indicators (resource trends, backend health). This reactive approach means problems are discovered only after they impact users. Instead, implement predictive monitoring that tracks resource consumption trends and dependency health to prevent failures before they occur.\n\n> **Key Insight:** The most effective upload service debugging combines automated monitoring with human pattern recognition. Automated systems excel at detecting threshold breaches and known failure patterns, but complex issues often require human insight to connect disparate symptoms and identify novel failure modes.\n\n#### Advanced Debugging Techniques\n\nComplex upload service issues may require advanced debugging techniques that go beyond standard monitoring and logging approaches.\n\n**Distributed Tracing for Upload Operations:**\n\nImplement distributed tracing that follows upload operations across all system components, providing complete visibility into operation flow and timing:\n\n| Trace Span | Component | Information Captured |\n|------------|-----------|---------------------|\n| Upload Request | HTTP Handler | Request headers, session ID, chunk metadata |\n| Session Update | Session Manager | State transitions, persistence operations |\n| Storage Operation | Storage Backend | Backend API calls, operation timing |\n| Validation Process | File Validator | Type detection, virus scanning, results |\n\n**Chaos Engineering for Resilience Testing:**\n\nImplement controlled failure injection to verify system resilience and debugging effectiveness:\n\n| Failure Type | Injection Method | Expected Behavior | Recovery Verification |\n|--------------|------------------|------------------|----------------------|\n| Storage backend timeout | Network delay injection | Circuit breaker activation | Automatic failover to backup |\n| Memory pressure | Artificial memory allocation | Load shedding activation | Graceful request rejection |\n| Virus scanner unavailability | Process termination | Fallback processing mode | File queuing for later scan |\n\n### Implementation Guidance\n\nThe debugging infrastructure requires comprehensive tooling that provides both real-time monitoring and historical analysis capabilities. This implementation provides the foundation for systematic troubleshooting of upload service issues.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Logging Framework | `log/slog` with JSON formatting | `go.uber.org/zap` with structured logging |\n| Metrics Collection | `expvar` with HTTP endpoint | `prometheus/client_golang` with Grafana |\n| Distributed Tracing | Custom request ID propagation | `OpenTelemetry` with Jaeger backend |\n| Health Checking | HTTP endpoints with JSON status | `grpc-health` with load balancer integration |\n| Admin Interface | HTTP handlers with JSON responses | Web dashboard with real-time updates |\n\n#### Recommended Module Structure\n\n```\ninternal/debugging/\n  diagnostics/\n    session_inspector.go      ← Session state inspection tools\n    storage_analyzer.go       ← Storage backend consistency checking\n    resource_monitor.go       ← System resource tracking\n    protocol_validator.go     ← tus.io protocol compliance verification\n  logging/\n    structured_logger.go      ← Centralized logging infrastructure\n    log_correlation.go        ← Request correlation and tracing\n    audit_trail.go           ← Upload operation audit logging\n  monitoring/\n    metrics_collector.go      ← Application metrics gathering\n    alert_manager.go         ← Alert threshold management\n    circuit_breaker.go       ← Circuit breaker pattern implementation\n  recovery/\n    session_recovery.go       ← Upload session repair procedures\n    data_integrity.go        ← Data consistency verification\n    emergency_procedures.go   ← Critical failure response\ncmd/debug/\n  session_inspector/         ← CLI tool for session analysis\n  storage_analyzer/          ← CLI tool for storage investigation\n  recovery_tool/            ← Emergency recovery procedures\n```\n\n#### Structured Logging Infrastructure\n\n```go\n// StructuredLogger provides comprehensive logging for upload operations\ntype StructuredLogger struct {\n    logger    *slog.Logger\n    component string\n    baseFields map[string]interface{}\n}\n\n// NewStructuredLogger creates a logger with component-specific configuration\nfunc NewStructuredLogger(component string, output io.Writer) *StructuredLogger {\n    // TODO 1: Create slog.Logger with JSON handler for structured output\n    // TODO 2: Configure log level based on environment (DEBUG, INFO, WARN, ERROR)\n    // TODO 3: Set up base fields that appear in every log entry (service, version, component)\n    // TODO 4: Enable request correlation ID propagation through context\n}\n\n// LogUploadEvent records significant upload operation events with full context\nfunc (sl *StructuredLogger) LogUploadEvent(ctx context.Context, level slog.Level, \n    message string, sessionID string, operation string, fields map[string]interface{}) {\n    // TODO 1: Extract correlation ID from context for request tracking\n    // TODO 2: Combine base fields with operation-specific fields\n    // TODO 3: Add standard upload fields (session_id, operation, timestamp)\n    // TODO 4: Emit structured log entry with consistent field naming\n    // Hint: Use consistent field names across all components for easier correlation\n}\n\n// LogError provides specialized error logging with categorization and context\nfunc (sl *StructuredLogger) LogError(ctx context.Context, err error, sessionID string, \n    component string, operation string, retryable bool) {\n    // TODO 1: Determine error category (ErrTransient, ErrPermanent, etc.)\n    // TODO 2: Extract relevant context from error (stack trace, error chain)\n    // TODO 3: Add debugging context (component state, operation parameters)\n    // TODO 4: Include retry information and suggested recovery actions\n}\n```\n\n#### Session State Inspector\n\n```go\n// SessionInspector provides detailed analysis of upload session state\ntype SessionInspector struct {\n    sessionManager *SessionManager\n    storage        StorageBackend\n    logger         *StructuredLogger\n}\n\n// InspectSession performs comprehensive analysis of session state and consistency\nfunc (si *SessionInspector) InspectSession(ctx context.Context, sessionID string) (*SessionDiagnostics, error) {\n    // TODO 1: Retrieve session metadata from session manager\n    // TODO 2: Query storage backend for actual stored chunks\n    // TODO 3: Compare session state with storage reality\n    // TODO 4: Identify inconsistencies and potential issues\n    // TODO 5: Calculate repair recommendations if state is corrupted\n}\n\n// SessionDiagnostics contains comprehensive session analysis results\ntype SessionDiagnostics struct {\n    SessionID          string                 `json:\"session_id\"`\n    CurrentState       SessionStatus          `json:\"current_state\"`\n    StateConsistent    bool                  `json:\"state_consistent\"`\n    StoredChunkCount   int                   `json:\"stored_chunk_count\"`\n    RecordedOffset     int64                 `json:\"recorded_offset\"`\n    ActualStoredSize   int64                 `json:\"actual_stored_size\"`\n    MissingChunks      []ChunkRange          `json:\"missing_chunks,omitempty\"`\n    CorruptedChunks    []CorruptionReport    `json:\"corrupted_chunks,omitempty\"`\n    RepairActions      []RepairAction        `json:\"repair_actions,omitempty\"`\n    Recommendations    []string              `json:\"recommendations\"`\n}\n\n// AnalyzeInconsistency identifies specific types of session state problems\nfunc (si *SessionInspector) AnalyzeInconsistency(session *UploadSession, \n    storageChunks []ChunkInfo) []InconsistencyReport {\n    // TODO 1: Compare recorded offset with sum of stored chunk sizes\n    // TODO 2: Identify gaps in chunk sequence (missing chunks)\n    // TODO 3: Detect overlapping or duplicate chunks\n    // TODO 4: Verify chunk checksums against stored data\n    // TODO 5: Check for storage backend multipart upload state alignment\n    // Hint: Build a bitmap of stored chunks for gap analysis\n}\n```\n\n#### Resource Monitoring System\n\n```go\n// ResourceMonitor tracks system resource utilization and triggers protective actions\ntype ResourceMonitor struct {\n    memoryWarning   int64\n    memoryCritical  int64\n    diskWarning     int64\n    diskCritical    int64\n    activeUploads   int64\n    maxUploads      int64\n    alerts          chan ResourceAlert\n    circuitBreaker  *CircuitBreaker\n}\n\n// GetCurrentStatus returns comprehensive resource utilization information\nfunc (rm *ResourceMonitor) GetCurrentStatus() *ResourceStatus {\n    // TODO 1: Collect current memory usage from runtime statistics\n    // TODO 2: Query disk space utilization for upload storage paths\n    // TODO 3: Count active upload sessions and concurrent operations\n    // TODO 4: Check file descriptor usage against system limits\n    // TODO 5: Calculate resource utilization percentages\n    // Hint: Use runtime.ReadMemStats() for memory information\n}\n\n// ShouldAcceptUpload determines if system resources allow new upload acceptance\nfunc (rm *ResourceMonitor) ShouldAcceptUpload() bool {\n    // TODO 1: Check current resource status against warning thresholds\n    // TODO 2: Verify that accepting new upload won't exceed critical limits\n    // TODO 3: Consider current upload completion rate and estimated resource usage\n    // TODO 4: Apply adaptive limits based on recent resource consumption trends\n    // Hint: Implement hysteresis to prevent rapid accept/reject oscillation\n}\n\n// MonitorResources runs continuous resource monitoring with alert generation\nfunc (rm *ResourceMonitor) MonitorResources(ctx context.Context) error {\n    ticker := time.NewTicker(10 * time.Second)\n    defer ticker.Stop()\n    \n    for {\n        select {\n        case <-ctx.Done():\n            return ctx.Err()\n        case <-ticker.C:\n            // TODO 1: Collect current resource status from system\n            // TODO 2: Compare against warning and critical thresholds\n            // TODO 3: Generate alerts for threshold violations\n            // TODO 4: Update circuit breaker state based on resource health\n            // TODO 5: Log resource trends for capacity planning\n        }\n    }\n}\n```\n\n#### Recovery Coordination System\n\n```go\n// RecoveryCoordinator manages systematic recovery from upload service failures\ntype RecoveryCoordinator struct {\n    sessionManager   *SessionManager\n    storageBackend   StorageBackend\n    resourceMonitor  *ResourceMonitor\n    circuitBreaker   *CircuitBreaker\n    logger          *StructuredLogger\n}\n\n// HandleUploadError implements comprehensive error handling with recovery attempts\nfunc (rc *RecoveryCoordinator) HandleUploadError(ctx context.Context, err *UploadError) error {\n    // TODO 1: Categorize error type (transient, permanent, resource, security)\n    // TODO 2: Determine appropriate recovery strategy based on error category\n    // TODO 3: Attempt automatic recovery if error is transient and retryable\n    // TODO 4: Update circuit breaker state based on error patterns\n    // TODO 5: Log error with full context for debugging analysis\n    // Hint: Use exponential backoff for transient error retries\n}\n\n// RecoverFromStorageFailure handles storage backend failures with failover\nfunc (rc *RecoveryCoordinator) RecoverFromStorageFailure(ctx context.Context, \n    sessionID string, operation string) error {\n    // TODO 1: Assess storage backend health and failure scope\n    // TODO 2: Determine if failover to alternate backend is possible\n    // TODO 3: Migrate session data to healthy storage backend if needed\n    // TODO 4: Update session metadata to reflect new storage location\n    // TODO 5: Notify monitoring systems of backend failover event\n}\n\n// HandleChunkCorruption manages detection and recovery from data corruption\nfunc (rc *RecoveryCoordinator) HandleChunkCorruption(ctx context.Context, \n    sessionID string, chunkOffset int64, expectedHash, actualHash string) error {\n    // TODO 1: Quarantine corrupted chunk to prevent further processing\n    // TODO 2: Request client to re-upload the corrupted chunk\n    // TODO 3: Update session state to reflect chunk re-upload requirement\n    // TODO 4: Log corruption event with forensic details for investigation\n    // TODO 5: Check for patterns indicating systematic corruption issues\n    // Hint: Implement chunk re-upload with different storage path to avoid overwrite\n}\n```\n\n#### Milestone Checkpoints\n\n**Checkpoint 1: Basic Diagnostic Infrastructure**\n- Run `go test ./internal/debugging/diagnostics/...` - all tests pass\n- Start debug HTTP server with `go run cmd/debug/server/main.go`\n- Access `http://localhost:8080/debug/sessions` - returns JSON list of sessions\n- Access `http://localhost:8080/debug/metrics` - returns resource utilization\n- Verify structured logs contain required fields: timestamp, level, component, session_id\n\n**Checkpoint 2: Session State Analysis**\n- Create test upload session with `curl -X POST http://localhost:8080/upload/init`\n- Inspect session with `go run cmd/debug/session_inspector/main.go <session-id>`\n- Verify inspector reports: session state, stored chunks, consistency status\n- Introduce artificial state corruption and verify detection\n- Confirm repair recommendations are generated for inconsistent state\n\n**Checkpoint 3: Resource Monitoring Integration**\n- Monitor resource usage under load: `go run cmd/debug/load_test/main.go`\n- Verify memory and disk alerts trigger at configured thresholds\n- Confirm load shedding activates when resources are constrained\n- Test circuit breaker behavior during simulated backend failures\n- Validate automatic recovery when resources become available\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|-------------|-----------------|-----|\n| Upload resumes from wrong offset | Session state/storage mismatch | Compare session.CurrentOffset with storage backend chunk list | Recalculate offset from stored chunks |\n| Virus scanning never completes | ClamAV daemon unavailable | Check scanner.Ping() and socket connectivity | Restart ClamAV, implement scan timeout |\n| High memory usage during uploads | Chunk buffering in memory | Check ResourceStatus.MemoryUsed during upload | Implement streaming chunk processing |\n| Storage operations timeout | Backend overloaded or credentials expired | Review storage backend circuit breaker state | Refresh credentials, implement retry with backoff |\n| Session cleanup too aggressive | CleanupConfig.SessionTTL too short | Check expired session count vs. active sessions | Increase SessionTTL, improve cleanup logic |\n| Upload stuck at completion | Async virus scan blocking | Check file validation queue and scan status | Implement scan status API, add timeout handling |\n\n\n## Future Extensions and Scalability\n\n> **Milestone(s):** Extends all milestones (1, 2, 3) — this section describes planned enhancements and scaling considerations beyond the chunked upload protocol (M1), storage abstraction (M2), and virus scanning implementation (M3)\n\nThink of the current resumable file upload service as a **solid foundation building** that we've constructed with careful engineering. We've built the core structure with proper foundations (chunked upload protocol), versatile infrastructure (storage abstraction), and essential safety systems (virus scanning). Now we need to plan for the future: how do we add more floors to handle more residents (horizontal scaling), how do we upgrade the utilities for better efficiency (performance optimizations), and what new amenities can we add to make the building more valuable (advanced features)?\n\nThis architectural evolution must be planned carefully because retrofitting a building after construction is expensive and disruptive. By designing our extension points and scalability patterns now, we ensure that future growth doesn't require rebuilding our foundation. The key insight is that **scalability isn't just about handling more load** — it's about maintaining system reliability, performance, and feature velocity as complexity increases.\n\nThe extensions we plan fall into three categories: making the current system faster and more efficient, adding sophisticated new capabilities that weren't in the original requirements, and preparing the architecture to scale horizontally across multiple machines and data centers. Each category presents different technical challenges and requires different architectural considerations.\n\n### Performance Optimizations\n\nThink of performance optimization as **upgrading the plumbing and electrical systems** in our building analogy. The basic functionality remains the same, but everything flows more efficiently. These optimizations focus on reducing latency, increasing throughput, and minimizing resource consumption without changing the fundamental API contracts or user experience.\n\n#### Parallel Upload Acceleration\n\nThe current implementation processes chunks sequentially within each upload session, which leaves significant performance on the table for users with high-bandwidth connections. **Parallel chunk processing** represents a fundamental shift from a sequential pipeline to a concurrent processing model.\n\n> **Decision: Client-Driven Parallel Upload Strategy**\n> - **Context**: Users with high-bandwidth connections are bottlenecked by sequential chunk processing, especially for large files where network latency becomes a significant factor in total upload time.\n> - **Options Considered**: Server-controlled parallelism (server dictates chunk ordering), client-controlled parallelism (client manages concurrent streams), hybrid approach (negotiated parallelism levels)\n> - **Decision**: Client-controlled parallelism with server-side coordination and ordering\n> - **Rationale**: Client has the best knowledge of its network conditions and can optimize chunk size and concurrency for its specific situation. Server maintains session integrity while allowing flexible client strategies.\n> - **Consequences**: Requires enhanced session state management for out-of-order chunk arrival, increases server memory usage for buffering, but provides maximum flexibility for client optimization.\n\nThe parallel upload architecture introduces several new data structures and processing patterns:\n\n| Component | Current Behavior | Optimized Behavior | Performance Impact |\n|-----------|-----------------|-------------------|------------------|\n| Chunk Reception | Sequential, blocking on storage | Concurrent reception with async storage | 3-5x throughput increase |\n| Session State | Single offset tracking | Range-based completion tracking | Supports out-of-order chunks |\n| Storage Backend | Immediate chunk writes | Batched writes with assembly optimization | Reduces storage API calls by 60-80% |\n| Memory Usage | Single chunk buffer per session | Configurable chunk pool with backpressure | Predictable memory bounds |\n\nThe `ParallelUploadCoordinator` manages the complexity of concurrent chunk processing:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| ChunkBuffer | map[int64]*ChunkMetadata | Temporarily holds out-of-order chunks until gaps are filled |\n| CompletionRanges | []OffsetRange | Tracks which byte ranges have been successfully stored |\n| ConcurrencyLimit | int | Maximum concurrent chunks per session to prevent resource exhaustion |\n| BackpressureThreshold | int64 | Memory usage threshold that triggers client backpressure |\n| AssemblyStrategy | AssemblyMode | Controls when chunks are written to final storage (immediate, batched, deferred) |\n\nThe key algorithmic challenge is **gap detection and assembly optimization**. When chunks arrive out of order, we need to efficiently track which portions of the file are complete and when we have enough contiguous data to write to the storage backend:\n\n1. **Chunk Arrival Processing**: Each incoming chunk is validated independently and placed in the appropriate position within the session's completion map\n2. **Gap Analysis**: After each chunk, the coordinator scans for newly-completed contiguous ranges that can be flushed to storage\n3. **Batch Assembly**: Contiguous chunks are batched together to minimize storage backend API calls and improve write efficiency\n4. **Memory Pressure Management**: When buffer usage exceeds thresholds, the coordinator forces assembly of available ranges and applies backpressure to the client\n5. **Completion Detection**: The upload is considered complete when all byte ranges from 0 to TotalSize are marked as received and flushed\n\n> The critical insight for parallel uploads is that **ordering matters for storage efficiency but not for network efficiency**. We can receive chunks in any order while still maintaining the ability to efficiently write them to storage backends that prefer sequential writes.\n\n#### Bandwidth Optimization and Compression\n\n**Compression integration** adds another layer of efficiency by reducing the actual bytes transferred over the network. However, compression for resumable uploads introduces unique challenges because traditional compression algorithms depend on previously seen data, making random-access resumption difficult.\n\nThe `CompressionCoordinator` implements a **chunked compression strategy** that maintains resumability:\n\n| Strategy | Compression Ratio | Resume Complexity | CPU Overhead | Best For |\n|----------|------------------|------------------|-------------|----------|\n| Per-chunk compression | 60-70% of full-file | Simple - each chunk independent | Low | General purpose files |\n| Sliding window compression | 80-90% of full-file | Complex - requires window state | Medium | Text and code files |\n| Content-aware compression | Varies by file type | File-type dependent | Variable | Mixed workloads |\n\nPer-chunk compression offers the best balance for resumable uploads because each chunk can be compressed and decompressed independently. This maintains the fundamental property that any chunk can be re-uploaded without affecting others, while still providing significant bandwidth savings.\n\nThe compression implementation requires careful coordination with the storage backend:\n\n1. **Compression Detection**: The service detects whether incoming chunks are compressed based on client headers and magic byte inspection\n2. **Storage Coordination**: Compressed chunks are stored with metadata indicating compression status and original size\n3. **Assembly Planning**: During final assembly, the service determines whether to store the file compressed or decompressed based on storage backend capabilities and access patterns\n4. **Client Negotiation**: Clients negotiate compression capabilities during upload initialization to ensure compatibility\n\n#### Intelligent Chunk Size Adaptation\n\n**Adaptive chunk sizing** represents a sophisticated optimization that adjusts chunk size based on real-time network conditions and client capabilities. Instead of using fixed chunk sizes, the service can guide clients toward optimal chunk sizes for their specific conditions.\n\nThe `ChunkSizeOptimizer` analyzes upload patterns and provides recommendations:\n\n| Metric | Small Chunks (64KB) | Medium Chunks (1MB) | Large Chunks (10MB) | Recommendation Logic |\n|--------|-------------------|-------------------|-------------------|-------------------|\n| High Latency Networks | Poor (many round trips) | Good | Excellent | Prefer larger chunks to amortize latency |\n| Unstable Networks | Excellent (minimal reupload) | Good | Poor (large retransmission cost) | Prefer smaller chunks to minimize retry cost |\n| High Bandwidth | Poor (underutilization) | Good | Excellent | Scale chunk size with available bandwidth |\n| Memory Constrained | Excellent | Good | Poor | Limit chunk size based on client memory |\n\nThe optimization algorithm considers multiple factors:\n\n1. **Network Condition Assessment**: Measure round-trip time, bandwidth, and packet loss patterns from recent upload history\n2. **Error Rate Analysis**: Track chunk failure rates and identify patterns that suggest optimal chunk sizes for reliability\n3. **Client Capability Detection**: Use client headers and behavior patterns to infer memory and processing constraints\n4. **Dynamic Recommendation**: Provide chunk size recommendations during upload progress that adapt to changing conditions\n5. **Fallback Strategy**: Maintain conservative defaults when insufficient data is available for optimization\n\n#### Caching and Content Deduplication\n\n**Content deduplication** at the chunk level can dramatically reduce storage costs and transfer times for environments where similar files are frequently uploaded. Think of this as a **smart warehouse** that recognizes when someone tries to store something that's already on the shelf.\n\nThe `DeduplicationManager` implements hash-based chunk deduplication:\n\n| Component | Responsibility | Performance Impact | Storage Savings |\n|-----------|---------------|-------------------|----------------|\n| Chunk Fingerprinting | Generate SHA-256 hashes for all chunks | 5-10% CPU overhead | Enables deduplication |\n| Deduplication Index | Track hash-to-location mappings | Memory usage scales with unique chunks | 30-80% for similar files |\n| Reference Counting | Manage chunk lifecycle and cleanup | Minimal overhead | Prevents premature deletion |\n| Client Notification | Inform clients about deduplicated chunks | Network savings | Eliminates redundant transfers |\n\nThe deduplication workflow integrates seamlessly with the existing upload process:\n\n1. **Hash Calculation**: As chunks arrive, calculate cryptographic hashes alongside existing validation\n2. **Deduplication Check**: Before storing, check if this chunk hash already exists in the deduplication index\n3. **Reference Management**: If the chunk exists, increment reference count and skip storage; if not, store normally and add to index\n4. **Client Notification**: Inform the client that the chunk was deduplicated so it can skip that portion of the upload\n5. **Garbage Collection**: When files are deleted, decrement reference counts and clean up unreferenced chunks\n\nThe key architectural decision is whether to implement **global deduplication** (across all users) or **per-tenant deduplication** (within user or organization boundaries):\n\n> **Decision: Tenant-Scoped Deduplication with Optional Global Mode**\n> - **Context**: Global deduplication provides maximum storage efficiency but raises security concerns about information leakage and complicates access control\n> - **Options Considered**: Global deduplication (maximum efficiency), per-tenant deduplication (security isolation), no deduplication (simplicity)\n> - **Decision**: Per-tenant deduplication by default with configurable global mode for appropriate use cases\n> - **Rationale**: Balances security isolation with significant storage savings within tenant boundaries. Global mode can be enabled for public content or trusted environments.\n> - **Consequences**: Reduces storage efficiency compared to global deduplication but eliminates security concerns about cross-tenant data leakage\n\n### Advanced Feature Additions\n\nAdvanced features represent **new amenities** in our building analogy — capabilities that weren't in the original requirements but add significant value for specific use cases. These features often require careful integration with the existing architecture to avoid disrupting core functionality.\n\n#### File Encryption and Security Enhancements\n\n**Client-side encryption support** allows users to encrypt files before upload, ensuring that even the service operators cannot access file contents. This is particularly important for compliance with privacy regulations and for handling sensitive data.\n\nThe `EncryptionCoordinator` provides a framework for various encryption strategies:\n\n| Encryption Mode | Key Management | Performance Impact | Security Level | Use Case |\n|----------------|---------------|-------------------|---------------|----------|\n| Client-managed | Client holds all keys | Minimal server overhead | Highest | Maximum privacy |\n| Service-managed | Server manages keys per tenant | Moderate processing overhead | High | Ease of use with strong security |\n| Hybrid | Client keys, server-assisted operations | Balanced overhead | Very High | Enterprise compliance |\n\nClient-side encryption introduces several architectural considerations:\n\n1. **Chunk-Level Encryption**: Each chunk must be encrypted independently to maintain resumability, requiring careful initialization vector (IV) management\n2. **Metadata Handling**: Determine which metadata can be encrypted and which must remain accessible for service operations\n3. **Key Rotation Support**: Provide mechanisms for encrypting new chunks with updated keys while maintaining access to existing content\n4. **Compliance Integration**: Ensure encryption implementation meets relevant compliance standards (FIPS 140-2, Common Criteria)\n\nThe `EncryptedUploadSession` extends the base upload session model:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| EncryptionMode | EncryptionType | Specifies the encryption algorithm and mode (AES-256-GCM, ChaCha20-Poly1305) |\n| KeyDerivationInfo | KeyDerivationParams | Parameters for generating chunk keys from master keys |\n| MetadataEncryption | bool | Whether filename and user metadata are encrypted |\n| ComplianceProfile | string | Compliance requirements that affect encryption choices |\n\n#### Advanced Metadata Processing\n\n**Metadata extraction and indexing** transforms the upload service from simple storage into an intelligent content management system. Think of this as adding a **smart librarian** who automatically catalogs everything that gets stored.\n\nThe `MetadataProcessor` supports extensible content analysis:\n\n| Content Type | Extracted Metadata | Processing Time | Storage Overhead | Search Benefits |\n|--------------|------------------|----------------|------------------|----------------|\n| Images | EXIF data, dimensions, color profiles | 100-500ms | 2-5KB per file | Location, date, camera search |\n| Documents | Text content, author, creation date | 1-10s | 10-100KB per file | Full-text search, author filtering |\n| Media | Duration, codecs, quality metrics | 2-30s | 5-20KB per file | Duration filtering, quality analysis |\n| Archives | File listings, compression ratios | 1-60s | 1-50KB per file | Content discovery |\n\nThe metadata processing pipeline operates asynchronously after upload completion to avoid impacting upload performance:\n\n1. **Content Type Detection**: Use magic byte analysis to determine file type and appropriate metadata extractors\n2. **Extraction Orchestration**: Route files to appropriate processors based on content type and configured extraction policies\n3. **Metadata Normalization**: Convert extracted metadata into standardized formats for consistent querying\n4. **Index Integration**: Store extracted metadata in searchable indexes while maintaining privacy boundaries\n5. **Client Notification**: Inform clients when metadata processing is complete and results are available\n\nThe `MetadataExtractor` interface allows pluggable processing strategies:\n\n| Method | Parameters | Returns | Description |\n|--------|-----------|---------|-------------|\n| SupportedTypes | none | []string | Returns MIME types this extractor can process |\n| ExtractMetadata | ctx Context, reader io.Reader, contentType string | *MetadataResult, error | Processes file content and returns extracted metadata |\n| EstimateProcessingTime | fileSize int64, contentType string | time.Duration | Provides time estimate for scheduling decisions |\n| RequiredResources | fileSize int64, contentType string | ResourceRequirements | Specifies CPU, memory needs for resource planning |\n\n#### Intelligent Virus Scanning Optimization\n\n**ML-enhanced virus scanning** goes beyond traditional signature-based detection to provide more sophisticated threat analysis. This represents an evolution from reactive scanning to **predictive security analysis**.\n\nThe `MLScanningCoordinator` integrates multiple detection strategies:\n\n| Scanning Method | Detection Rate | False Positive Rate | Processing Time | Resource Usage |\n|----------------|---------------|-------------------|----------------|----------------|\n| Signature-based (ClamAV) | 85-90% | < 0.1% | 1-10s | Low CPU, minimal memory |\n| Heuristic analysis | 70-85% | 1-5% | 5-30s | Medium CPU, moderate memory |\n| ML behavioral analysis | 90-95% | 2-8% | 10-120s | High CPU, significant memory |\n| Hybrid approach | 95-98% | 0.5-2% | Variable | Adaptive resource usage |\n\nThe ML scanning pipeline processes files through multiple analysis stages:\n\n1. **Quick Signature Scan**: Run traditional signature-based scanning first for known threats with minimal resource usage\n2. **Risk Assessment**: Use file characteristics (size, type, source) to determine whether additional analysis is warranted\n3. **Behavioral Analysis**: For high-risk files, perform ML-based analysis of file structure and embedded content patterns\n4. **Threat Correlation**: Cross-reference findings across multiple detection methods to reduce false positives\n5. **Adaptive Learning**: Update ML models based on confirmed threats and false positive feedback\n\nThe `MLThreatAnalyzer` provides configurable analysis depth:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| ModelVersion | string | Version identifier for the ML model being used |\n| AnalysisDepth | ScanDepth | Configures thoroughness vs. speed trade-off |\n| ConfidenceThreshold | float64 | Minimum confidence required for threat classification |\n| FallbackStrategy | FallbackMode | How to handle analysis failures or timeouts |\n| LearningEnabled | bool | Whether to feed results back into model training |\n\n#### Content Distribution and CDN Integration\n\n**CDN integration** transforms the upload service into a comprehensive content delivery platform. Once files are uploaded and validated, they can be automatically distributed to edge locations for optimal global access performance.\n\nThe `CDNCoordinator` manages distribution policies and edge synchronization:\n\n| Distribution Strategy | Use Case | Sync Time | Cost Impact | Global Performance |\n|---------------------|----------|-----------|-------------|------------------|\n| On-demand | Infrequently accessed files | Minutes to hours | Pay-per-access | Good after first access |\n| Predictive | Files with expected global access | 10-60 minutes | Higher base cost | Excellent immediately |\n| Tiered | Mixed workloads | Variable by tier | Optimized cost | Good for popular content |\n\nThe CDN integration workflow coordinates with existing storage backends:\n\n1. **Distribution Policy Evaluation**: After successful upload, evaluate file characteristics against distribution policies\n2. **Edge Deployment**: For files meeting distribution criteria, initiate replication to configured CDN edge locations\n3. **URL Management**: Generate CDN-backed URLs for client access while maintaining fallback to origin storage\n4. **Invalidation Coordination**: When files are updated or deleted, coordinate cache invalidation across edge locations\n5. **Analytics Integration**: Track access patterns to optimize future distribution decisions\n\n### Horizontal Scaling Considerations\n\nThink of horizontal scaling as **expanding from a single building to a campus** — we need to coordinate activities across multiple buildings while maintaining a unified experience for residents. This requires fundamental changes to how we handle state, coordinate operations, and maintain consistency.\n\n#### Multi-Instance Deployment Patterns\n\n**Stateless service design** is the foundation for horizontal scaling. Each service instance should be able to handle any request without depending on local state from previous requests. This requires externalizing all persistent state and coordinating shared operations.\n\nThe current architecture already anticipates some scaling challenges through the `StateStore` abstraction, but production scaling requires additional coordination mechanisms:\n\n| Scaling Pattern | Complexity | Consistency | Performance | Best For |\n|----------------|------------|-------------|-------------|----------|\n| Active-Active | High | Eventually consistent | Excellent | Global, high-throughput workloads |\n| Active-Passive | Medium | Strongly consistent | Good | Regional deployments with failover |\n| Sharded | High | Partition-wise consistent | Excellent | Very large scale with data locality |\n\nThe `ScalingCoordinator` manages instance coordination and load distribution:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| InstanceID | string | Unique identifier for this service instance |\n| LoadBalancingStrategy | LBStrategy | How to distribute uploads across instances |\n| SessionAffinityMode | AffinityMode | Whether uploads must stick to specific instances |\n| HeartbeatInterval | time.Duration | Frequency of instance health reporting |\n| FailoverTimeout | time.Duration | How long to wait before assuming instance failure |\n\n#### Distributed State Management\n\n**External state storage** becomes critical for horizontal scaling. The simple in-memory `MemoryStore` that works for single instances must be replaced with distributed storage systems that provide consistency guarantees across multiple service instances.\n\n> **Decision: Redis-Based Session State with Database Backup**\n> - **Context**: Horizontal scaling requires session state that's accessible from any service instance, with high performance for frequent reads/writes and durability for crash recovery\n> - **Options Considered**: Database-only storage (simple but slow), Redis-only storage (fast but less durable), hybrid Redis+database approach (complex but optimal)\n> - **Decision**: Redis as primary session store with periodic database synchronization\n> - **Rationale**: Redis provides the low-latency access needed for upload operations while database backup ensures durability and supports complex queries for monitoring\n> - **Consequences**: Increases operational complexity with two storage systems but provides optimal performance and reliability characteristics\n\nThe `DistributedStateStore` coordinates between multiple storage layers:\n\n| Method | Redis Usage | Database Usage | Consistency Level |\n|--------|------------|---------------|------------------|\n| CreateSession | Primary storage | Background sync | Eventual |\n| GetSession | Cache hit/miss | Fallback for cache miss | Strong for single session |\n| UpdateSession | Immediate update | Batched synchronization | Eventually consistent |\n| ListExpiredSessions | Not used | Primary query source | Eventually consistent |\n\nThe distributed state management introduces several new operational concerns:\n\n1. **Cache Coherence**: Ensure updates to session state are visible across all service instances within acceptable time bounds\n2. **Split-Brain Prevention**: Handle scenarios where service instances can reach Redis but not each other\n3. **Data Durability**: Balance performance with durability requirements for session state\n4. **Backup and Recovery**: Provide mechanisms to recover session state after total system failures\n5. **Monitoring and Alerting**: Track state synchronization lag and detect inconsistencies\n\n#### Load Balancing and Session Affinity\n\n**Session affinity decisions** significantly impact scaling architecture. The question is whether uploads must be processed by the same service instance throughout their lifecycle or can be handled by any available instance.\n\n| Approach | Scaling Flexibility | Operational Complexity | Failure Recovery | Performance |\n|----------|-------------------|----------------------|------------------|-------------|\n| Sticky Sessions | Limited (can't redistribute load) | Low (simple routing) | Poor (in-flight uploads lost) | Good (local state) |\n| Session-less | Excellent (full load distribution) | High (shared state coordination) | Excellent (any instance can resume) | Variable (depends on state store) |\n| Hybrid | Good (can migrate sessions) | Medium (coordinated migration) | Good (graceful failover) | Good (optimized for common cases) |\n\nThe `LoadBalancingCoordinator` provides configurable affinity strategies:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| AffinityStrategy | AffinityType | How to handle session-to-instance binding |\n| MigrationEnabled | bool | Whether sessions can move between instances |\n| HealthCheckInterval | time.Duration | Frequency of instance health verification |\n| LoadMetrics | []MetricType | Metrics used for load balancing decisions |\n\nSession affinity introduces unique challenges for upload services:\n\n1. **In-Flight Upload Management**: Handle scenarios where the assigned instance becomes unavailable during active uploads\n2. **Load Imbalance**: Prevent scenarios where long-running uploads concentrate on specific instances\n3. **Graceful Migration**: Move sessions between instances for maintenance without interrupting uploads\n4. **Failover Coordination**: Detect instance failures and reassign affected sessions to healthy instances\n\nThe `SessionMigrationManager` coordinates transfer of active uploads between instances:\n\n| Method | Purpose | Coordination Required | Data Transferred |\n|--------|---------|---------------------|------------------|\n| PrepareForMigration | Prepare session for transfer | Source and destination instances | Session metadata and chunk state |\n| MigrateSession | Transfer session ownership | Load balancer and state store | Complete session state |\n| ConfirmMigration | Verify successful transfer | All involved components | Migration success confirmation |\n| RollbackMigration | Handle migration failures | Source instance and state store | Restore original session state |\n\n#### Data Partitioning and Sharding\n\n**Sharding strategies** become necessary when the total volume of upload sessions exceeds what can be efficiently managed by a single state store instance. This introduces additional complexity but enables virtually unlimited scaling.\n\nThe `ShardingCoordinator` manages data distribution across multiple storage partitions:\n\n| Sharding Key | Distribution Quality | Hot Spot Risk | Query Complexity | Migration Difficulty |\n|--------------|-------------------|---------------|------------------|-------------------|\n| Session ID | Excellent | Low | Simple | Easy |\n| User ID | Good | Medium | Medium | Medium |\n| Upload Date | Poor | High | Complex | Hard |\n| Content Hash | Excellent | Low | Complex | Medium |\n\nSession ID-based sharding provides the best balance for upload services:\n\n1. **Uniform Distribution**: Cryptographically random session IDs ensure even distribution across shards\n2. **Simple Routing**: Any request can be routed based solely on session ID without additional lookups\n3. **Independent Operations**: Each shard can operate independently without cross-shard coordination for most operations\n4. **Predictable Performance**: Load is naturally distributed proportional to upload volume\n\nThe sharding implementation requires careful consideration of cross-shard operations:\n\n| Operation | Shard Scope | Coordination Required | Performance Impact |\n|-----------|-------------|---------------------|------------------|\n| CreateSession | Single shard | None | No impact |\n| GetSession | Single shard | None | No impact |\n| UpdateSession | Single shard | None | No impact |\n| ListExpiredSessions | All shards | Scatter-gather | Linear with shard count |\n| GlobalStatistics | All shards | Scatter-gather with aggregation | Linear with shard count |\n\n⚠️ **Pitfall: Cross-Shard Query Performance**\nA common mistake is underestimating the performance impact of operations that must query multiple shards. Operations like `ListExpiredSessions` or global monitoring queries can become bottlenecks as the number of shards increases. The fix is to implement these operations asynchronously with cached results rather than real-time scatter-gather queries.\n\n#### Service Discovery and Health Management\n\n**Service discovery** enables dynamic scaling where service instances can be added or removed without manual configuration updates. This is essential for auto-scaling based on load and for handling instance failures gracefully.\n\nThe `DiscoveryManager` coordinates instance registration and health monitoring:\n\n| Component | Responsibility | Update Frequency | Failure Detection Time |\n|-----------|---------------|------------------|---------------------|\n| Instance Registration | Announce availability and capabilities | On startup and shutdown | N/A |\n| Health Reporting | Report current load and status | Every 30-60 seconds | 2-3x reporting interval |\n| Service Registry | Maintain authoritative instance list | Real-time updates | Immediate for explicit deregistration |\n| Load Balancer Integration | Update routing configuration | Within 5-10 seconds | Health check interval |\n\nThe service discovery system must handle several failure scenarios:\n\n1. **Network Partitions**: Instances that can serve traffic but cannot communicate with the service registry\n2. **Partial Failures**: Instances that are healthy for some operations but failing for others\n3. **Graceful Shutdown**: Instances that are shutting down and should stop receiving new traffic but complete existing uploads\n4. **Split-Brain Scenarios**: Multiple instances claiming to be the authoritative service registry\n\nThe `HealthReporter` provides comprehensive instance status information:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| InstanceID | string | Unique identifier for this instance |\n| StartTime | time.Time | When this instance started serving traffic |\n| ActiveSessions | int | Number of currently active upload sessions |\n| ResourceUtilization | ResourceStatus | Current CPU, memory, disk usage |\n| LastHealthCheck | time.Time | Timestamp of most recent health verification |\n| ServiceStatus | ServiceState | Overall health status (healthy, degraded, failing) |\n\n### Implementation Guidance\n\nThe future extensions described above represent significant architectural additions that should be implemented incrementally to avoid disrupting the core service functionality. The recommended approach is to implement these features as **optional extensions** that can be enabled through configuration without affecting users who don't need the advanced capabilities.\n\n#### Technology Recommendations\n\n| Extension Category | Simple Option | Advanced Option |\n|-------------------|---------------|-----------------|\n| Parallel Uploads | Goroutine pool with channel coordination | Actor-based concurrency with supervisors |\n| Compression | Standard library gzip per-chunk | zstd with custom chunked format |\n| Deduplication | In-memory hash map with persistence | Distributed hash table (DHT) with consistent hashing |\n| Encryption | Standard library crypto/aes | Hardware security module (HSM) integration |\n| CDN Integration | Simple webhook notifications | Full CDN API integration with edge management |\n| State Store | Redis with basic clustering | Redis Cluster with consistent hashing |\n| Service Discovery | Static configuration with health checks | Consul or etcd with automatic failover |\n| Load Balancing | HAProxy with sticky sessions | Envoy proxy with advanced traffic management |\n\n#### Recommended Module Structure\n\nThe extensions should be organized as separate modules that integrate cleanly with the existing architecture:\n\n```\nproject-root/\n  internal/\n    core/                    ← existing core components\n      session/\n      storage/\n      validation/\n    extensions/              ← new extension modules\n      parallel/              ← parallel upload coordination\n        coordinator.go\n        chunk_pool.go\n      compression/           ← compression support\n        compressor.go\n        adaptive_sizing.go\n      deduplication/         ← content deduplication\n        hash_manager.go\n        reference_counter.go\n      encryption/            ← encryption coordination\n        key_manager.go\n        encrypted_session.go\n      cdn/                   ← CDN integration\n        distribution_manager.go\n        edge_sync.go\n      metadata/              ← metadata extraction\n        extractor_registry.go\n        content_analyzer.go\n    scaling/                 ← horizontal scaling components\n      discovery/\n        service_registry.go\n      coordination/\n        shard_manager.go\n      state/\n        distributed_store.go\n  cmd/\n    server/\n      main.go               ← updated to support extensions\n    migration/              ← tools for scaling transitions\n      shard_migration.go\n```\n\n#### Infrastructure Starter Code\n\n**Extension Configuration Management**\n\n```go\npackage extensions\n\nimport (\n    \"encoding/json\"\n    \"os\"\n    \"time\"\n)\n\n// ExtensionConfig controls which advanced features are enabled\ntype ExtensionConfig struct {\n    ParallelUploads   ParallelConfig   `json:\"parallel_uploads\"`\n    Compression       CompressionConfig `json:\"compression\"`\n    Deduplication     DeduplicationConfig `json:\"deduplication\"`\n    Encryption        EncryptionConfig  `json:\"encryption\"`\n    CDNIntegration    CDNConfig        `json:\"cdn_integration\"`\n    Scaling           ScalingConfig    `json:\"scaling\"`\n}\n\ntype ParallelConfig struct {\n    Enabled              bool          `json:\"enabled\"`\n    MaxConcurrentChunks  int           `json:\"max_concurrent_chunks\"`\n    BufferSizeLimit      int64         `json:\"buffer_size_limit\"`\n    BackpressureThreshold int64        `json:\"backpressure_threshold\"`\n}\n\ntype CompressionConfig struct {\n    Enabled        bool     `json:\"enabled\"`\n    Algorithm      string   `json:\"algorithm\"` // \"gzip\", \"zstd\"\n    CompressionLevel int    `json:\"compression_level\"`\n    ChunkSizeMin   int64    `json:\"chunk_size_min\"`\n    ChunkSizeMax   int64    `json:\"chunk_size_max\"`\n}\n\ntype DeduplicationConfig struct {\n    Enabled       bool   `json:\"enabled\"`\n    Scope         string `json:\"scope\"` // \"global\", \"tenant\"\n    HashAlgorithm string `json:\"hash_algorithm\"`\n    IndexBackend  string `json:\"index_backend\"` // \"memory\", \"redis\", \"database\"\n}\n\ntype ScalingConfig struct {\n    Mode              string        `json:\"mode\"` // \"single\", \"cluster\"\n    ServiceDiscovery  string        `json:\"service_discovery\"` // \"static\", \"consul\", \"etcd\"\n    StateStore        string        `json:\"state_store\"` // \"memory\", \"redis\", \"postgres\"\n    SessionAffinity   string        `json:\"session_affinity\"` // \"none\", \"sticky\", \"migratable\"\n    ShardingEnabled   bool          `json:\"sharding_enabled\"`\n    ShardCount        int           `json:\"shard_count\"`\n}\n\n// LoadExtensionConfig loads extension configuration from file\nfunc LoadExtensionConfig(filename string) (*ExtensionConfig, error) {\n    data, err := os.ReadFile(filename)\n    if err != nil {\n        return nil, fmt.Errorf(\"reading extension config: %w\", err)\n    }\n    \n    var config ExtensionConfig\n    if err := json.Unmarshal(data, &config); err != nil {\n        return nil, fmt.Errorf(\"parsing extension config: %w\", err)\n    }\n    \n    return &config, nil\n}\n\n// ExtensionRegistry manages available extensions and their lifecycle\ntype ExtensionRegistry struct {\n    extensions map[string]Extension\n    config     *ExtensionConfig\n    logger     *StructuredLogger\n}\n\ntype Extension interface {\n    Name() string\n    Initialize(ctx context.Context, config interface{}) error\n    Start(ctx context.Context) error\n    Stop(ctx context.Context) error\n    HealthCheck() error\n}\n```\n\n**Parallel Upload Coordination**\n\n```go\npackage parallel\n\nimport (\n    \"context\"\n    \"sync\"\n    \"time\"\n)\n\n// ParallelCoordinator manages concurrent chunk processing for upload sessions\ntype ParallelCoordinator struct {\n    config           *ParallelConfig\n    chunkPool        *ChunkPool\n    sessionManagers  map[string]*SessionParallelManager\n    mutex            sync.RWMutex\n    resourceMonitor  *ResourceMonitor\n}\n\n// SessionParallelManager handles parallelism for a single upload session\ntype SessionParallelManager struct {\n    sessionID        string\n    totalSize        int64\n    chunkSize        int64\n    completedRanges  *RangeSet\n    pendingChunks    map[int64]*PendingChunk\n    concurrencyLimit int\n    activeSemaphore  chan struct{}\n    mutex            sync.RWMutex\n}\n\ntype PendingChunk struct {\n    Offset       int64\n    Size         int64\n    Data         []byte\n    Hash         string\n    ReceivedAt   time.Time\n    ProcessingStarted bool\n}\n\ntype RangeSet struct {\n    ranges []Range\n    mutex  sync.RWMutex\n}\n\ntype Range struct {\n    Start int64\n    End   int64\n}\n\n// NewParallelCoordinator creates coordinator for managing parallel uploads\nfunc NewParallelCoordinator(config *ParallelConfig, monitor *ResourceMonitor) *ParallelCoordinator {\n    return &ParallelCoordinator{\n        config:          config,\n        chunkPool:       NewChunkPool(config.BufferSizeLimit),\n        sessionManagers: make(map[string]*SessionParallelManager),\n        resourceMonitor: monitor,\n    }\n}\n\n// ProcessChunkParallel handles concurrent chunk processing with ordering\nfunc (pc *ParallelCoordinator) ProcessChunkParallel(ctx context.Context, sessionID string, offset int64, data []byte, hash string) error {\n    // TODO 1: Get or create session parallel manager for this session\n    // TODO 2: Check if system resources allow accepting this chunk (backpressure)\n    // TODO 3: Acquire semaphore slot for concurrency limiting\n    // TODO 4: Store chunk in pending chunks map with metadata\n    // TODO 5: Launch goroutine for async processing (validation, storage)\n    // TODO 6: Check for newly completed contiguous ranges after processing\n    // TODO 7: Flush completed ranges to storage backend in order\n    // TODO 8: Update session progress and notify client if needed\n    return nil\n}\n```\n\n#### Core Logic Skeletons\n\n**Distributed State Store Implementation**\n\n```go\npackage state\n\n// DistributedStateStore provides session state management across multiple service instances\ntype DistributedStateStore struct {\n    redis    RedisClient\n    database DatabaseClient\n    config   *DistributedStateConfig\n    logger   *StructuredLogger\n}\n\n// CreateSession stores new upload session in distributed state\nfunc (ds *DistributedStateStore) CreateSession(ctx context.Context, session *UploadSession) error {\n    // TODO 1: Serialize session data to JSON format\n    // TODO 2: Store session in Redis with TTL for primary access\n    // TODO 3: Queue database write for durability (async or sync based on config)\n    // TODO 4: Handle Redis failures by falling back to direct database write\n    // TODO 5: Return success only after confirming storage in at least one backend\n    return nil\n}\n\n// GetSession retrieves upload session with cache-first strategy\nfunc (ds *DistributedStateStore) GetSession(ctx context.Context, sessionID string) (*UploadSession, error) {\n    // TODO 1: Attempt to retrieve session from Redis cache\n    // TODO 2: If cache miss, query database as fallback\n    // TODO 3: If found in database, refresh Redis cache for future requests\n    // TODO 4: Handle cases where session exists in cache but not database\n    // TODO 5: Return ErrSessionNotFound if session doesn't exist in either store\n    return nil, nil\n}\n\n// UpdateSession modifies existing session state across distributed stores\nfunc (ds *DistributedStateStore) UpdateSession(ctx context.Context, session *UploadSession) error {\n    // TODO 1: Update session in Redis immediately for low-latency access\n    // TODO 2: Queue or perform database update based on consistency requirements\n    // TODO 3: Handle version conflicts if optimistic locking is enabled\n    // TODO 4: Implement retry logic for transient failures\n    // TODO 5: Consider notifying other service instances of state changes\n    return nil\n}\n```\n\n**Content Deduplication Manager**\n\n```go\npackage deduplication\n\n// DeduplicationManager handles hash-based chunk deduplication\ntype DeduplicationManager struct {\n    hashIndex     HashIndex\n    refCounter    ReferenceCounter\n    storageBackend StorageBackend\n    config        *DeduplicationConfig\n    stats         *DeduplicationStats\n}\n\n// ProcessChunkWithDeduplication checks for existing chunks before storage\nfunc (dm *DeduplicationManager) ProcessChunkWithDeduplication(ctx context.Context, sessionID string, chunkData []byte, expectedHash string) (*ChunkProcessingResult, error) {\n    // TODO 1: Calculate chunk hash using configured algorithm (SHA-256, etc.)\n    // TODO 2: Verify calculated hash matches expected hash from client\n    // TODO 3: Check hash index to see if this chunk already exists\n    // TODO 4: If chunk exists, increment reference count and return existing location\n    // TODO 5: If chunk is new, store in backend and add to hash index\n    // TODO 6: Update deduplication statistics for monitoring\n    // TODO 7: Return result indicating whether chunk was deduplicated or newly stored\n    return nil, nil\n}\n\n// CleanupUnreferencedChunks removes chunks that are no longer referenced\nfunc (dm *DeduplicationManager) CleanupUnreferencedChunks(ctx context.Context) error {\n    // TODO 1: Scan reference counter for chunks with zero references\n    // TODO 2: Verify chunks are not part of any active upload sessions\n    // TODO 3: Remove unreferenced chunks from storage backend\n    // TODO 4: Remove entries from hash index\n    // TODO 5: Update cleanup statistics and log cleanup activity\n    return nil\n}\n```\n\n#### Milestone Checkpoints\n\n**Extension Development Milestones**\n\n1. **Performance Optimization Milestone**: Implement parallel chunk processing and verify 3x throughput improvement for concurrent uploads\n   - Test command: `go run cmd/loadtest/main.go -concurrent-chunks=10 -file-size=100MB`\n   - Expected behavior: Upload completion time should be 60-70% faster than sequential processing\n   - Validation: Monitor resource usage to ensure memory stays within configured limits\n\n2. **Advanced Features Milestone**: Add deduplication and metadata extraction with configurable policies\n   - Test command: `go run cmd/dedup-test/main.go -upload-duplicates=5 -verify-savings`\n   - Expected behavior: Storage usage should be 80% lower for identical file uploads\n   - Validation: Metadata should be extracted and searchable within 30 seconds of upload completion\n\n3. **Horizontal Scaling Milestone**: Deploy service across multiple instances with shared state\n   - Test command: `docker-compose up -d && go run cmd/scaling-test/main.go -instances=3`\n   - Expected behavior: Uploads should work correctly when processed by different instances\n   - Validation: Session state should be consistent across all instances within 5 seconds\n\n#### Debugging Tips for Extensions\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|-------------|----------------|-----|\n| Parallel uploads slower than sequential | Lock contention or resource bottlenecks | Profile with `go tool pprof` focusing on mutex and CPU usage | Reduce lock scope, increase buffer sizes, optimize hot paths |\n| Deduplication not working | Hash calculation errors or index inconsistencies | Check hash logs and verify index queries return expected results | Validate hash algorithm implementation, rebuild hash index |\n| Session state inconsistencies | Redis-database sync lag or network partitions | Compare session state between Redis and database, check network connectivity | Implement stronger consistency guarantees, add conflict resolution |\n| Service discovery failures | Network issues or registry corruption | Check service registry logs and network connectivity between instances | Restart service registry, verify network configuration |\n| Memory leaks in extensions | Unreleased resources or circular references | Use memory profiler to track allocation patterns over time | Add proper resource cleanup in defer statements, break circular references |\n\n\n## Glossary and Technical References\n\n> **Milestone(s):** All milestones (1, 2, 3) — this glossary supports understanding across chunked upload protocol implementation (M1), storage abstraction (M2), and virus scanning validation (M3)\n\nThink of this glossary as a *technical dictionary* for the resumable file upload service. Just as a traveler needs a phrasebook to navigate a foreign country, developers implementing or maintaining this service need a shared vocabulary to navigate the complex landscape of protocols, abstractions, and specialized concepts. Each term represents a crystallized piece of domain knowledge that, once understood, unlocks deeper comprehension of the system's design and implementation.\n\nThe terminology in upload services spans multiple domains: network protocols, distributed systems, storage technologies, security frameworks, and data validation. Without precise definitions, discussions about \"chunks\" could refer to network packets, file segments, or storage units. This ambiguity leads to misunderstandings, bugs, and architectural inconsistencies. The glossary establishes canonical definitions that align with industry standards while remaining specific to this implementation.\n\n### Core Upload Concepts\n\nThe foundation of any resumable upload service rests on a carefully defined vocabulary around file transfer mechanics. These terms form the building blocks for more complex operations and protocols.\n\n| Term | Definition | Context | Related Concepts |\n|------|------------|---------|------------------|\n| **resumable upload** | File upload that can be interrupted and resumed from the last successfully transmitted byte offset, preserving progress across network failures or client disconnections | Core protocol concept | upload session, offset tracking, tus.io protocol |\n| **chunked upload** | File upload split into multiple smaller pieces (chunks) transmitted separately, enabling parallel processing and reducing memory requirements | Transfer mechanism | chunk assembly, multipart upload, parallel processing |\n| **upload session** | Server-side state tracking ongoing resumable upload, including session ID, current progress offset, file metadata, and session status | State management | session state management, SessionManager, UploadSession |\n| **offset tracking** | Byte-level progress monitoring that records exactly how many bytes have been successfully received and stored | Progress monitoring | CurrentOffset, chunk boundaries, resume logic |\n| **chunk assembly** | Process of combining uploaded chunks into final complete file, ensuring correct ordering and data integrity | File reconstruction | CompleteMultipart, chunk ordering, integrity verification |\n\nThe relationship between these concepts follows a hierarchical pattern. A **resumable upload** is implemented through **chunked upload** techniques, managed by an **upload session** that performs **offset tracking** and eventually triggers **chunk assembly**. Understanding this hierarchy helps developers reason about which component handles which responsibility.\n\n> **Key Insight**: The distinction between \"resumable\" and \"chunked\" is critical. A resumable upload is the *user-facing capability*, while chunked upload is the *technical implementation mechanism*. Not all chunked uploads are resumable, and theoretically resumable uploads could be implemented without chunking (though this would be inefficient for large files).\n\n### Protocol and Standards\n\nThe service implements and extends established protocols for file uploads. These standards provide interoperability and proven reliability patterns that the implementation builds upon.\n\n| Term | Definition | Context | Key Components |\n|------|------------|---------|----------------|\n| **tus.io protocol** | Standardized resumable upload protocol specification defining HTTP methods, headers, and request/response formats for interoperable resumable uploads | Protocol compliance | InitUploadRequest, ChunkUploadResponse, HEAD requests for progress |\n| **multipart upload** | Large file upload split into multiple parts, particularly referring to cloud storage protocols like S3's multipart upload API | Storage backend | MultipartUpload, MultipartPart, backend-specific implementation |\n| **Content-Range headers** | HTTP headers specifying byte ranges for partial content requests and responses, enabling precise chunk positioning | HTTP protocol | offset tracking, chunk boundaries, resume logic |\n| **atomic operations** | File operations that complete entirely or not at all, preventing partial writes that could corrupt data | Data integrity | CompleteMultipart, transactional guarantees, rollback capability |\n| **signed URL** | Time-limited URL with embedded authentication token for secure access to stored files without exposing credentials | Security mechanism | credential provider, temporary access, expiration management |\n\nThese protocols interact in layered fashion. The **tus.io protocol** provides the client-server communication standard, which maps to **multipart upload** operations on the storage backend. **Content-Range headers** enable precise **offset tracking**, while **atomic operations** ensure consistency. **Signed URLs** provide secure access to the final assembled files.\n\n> **Design Decision: Protocol Layering**\n> - **Context**: Need to balance standard compliance with implementation flexibility\n> - **Options Considered**: Direct tus.io implementation, custom protocol, hybrid approach\n> - **Decision**: Implement tus.io core with extensions for advanced features\n> - **Rationale**: Standards compliance ensures client compatibility while extensions enable value-added features like virus scanning integration\n> - **Consequences**: Broader ecosystem compatibility but additional complexity in feature flag management\n\n### Storage and Backend Management\n\nStorage abstraction enables the service to work with multiple storage systems while presenting a unified interface. This vocabulary captures the concepts essential for backend agnostic file storage.\n\n| Term | Definition | Context | Implementation |\n|------|------------|---------|----------------|\n| **storage backend abstraction** | Interface allowing multiple storage systems behind common API, enabling deployment flexibility and vendor independence | Architecture pattern | StorageBackend interface, LocalBackend, S3-compatible implementations |\n| **credential provider** | Source of authentication information for storage backends, supporting multiple credential types and refresh mechanisms | Authentication | CredentialProvider, Credentials struct, token refresh logic |\n| **state machine** | Structured representation of session lifecycle with valid transitions, preventing invalid state changes | Session management | SessionStatus enum, state transition validation, error states |\n| **distributed state management** | Session state coordination across multiple service instances, enabling horizontal scaling | Scalability | DistributedStateStore, session affinity, state synchronization |\n| **storage backend failures** | Various failure modes of storage systems including network issues, authentication failures, and quota exhaustion | Error handling | StorageError types, circuit breaker pattern, fallback strategies |\n\nThe storage abstraction creates clear separation of concerns. The **storage backend abstraction** provides the interface contract, while **credential providers** handle authentication complexity. **State machines** ensure consistent session lifecycle management, and **distributed state management** enables scaling. **Storage backend failures** are isolated and handled through established patterns.\n\n| Failure Mode | Detection Method | Recovery Strategy | Fallback Options |\n|--------------|------------------|-------------------|------------------|\n| Network timeout | Connection timeout, operation timeout | Retry with exponential backoff | Alternative endpoint, local caching |\n| Authentication failure | 401/403 responses, credential expiry | Credential refresh, re-authentication | Service account fallback, manual intervention |\n| Quota exhaustion | 507/insufficient space errors | Alternative backend, cleanup | Storage migration, quota increase |\n| Data corruption | Checksum mismatch, incomplete reads | Re-upload affected chunks | Backup restoration, quarantine |\n\n### File Validation and Security\n\nSecurity concepts encompass both automated validation and policy enforcement mechanisms that protect the service and its users from malicious content and system abuse.\n\n| Term | Definition | Context | Security Impact |\n|------|------------|---------|-----------------|\n| **magic byte detection** | File type identification using binary signatures at file beginning, more reliable than extension-based detection | Content validation | FileSignature patterns, Detector logic, MIME type verification |\n| **virus scanning** | Malware detection using signature-based analysis engines like ClamAV | Security validation | ClamAVScanner, ScanResult, async scanning workflow |\n| **quarantine storage** | Isolated storage for suspicious or infected files, preventing access while preserving evidence | Security containment | separate storage bucket, restricted access, forensic retention |\n| **content-type validation** | Verification of actual file type against declared MIME type, preventing content-type spoofing attacks | Input validation | magic byte verification, MIME type consistency, policy enforcement |\n| **forensic metadata** | Detailed context information preserved for security investigations including timestamps, checksums, and scan results | Security auditing | upload metadata, scan history, investigation support |\n\nSecurity validation follows a defense-in-depth approach. **Magic byte detection** provides the first layer of content verification, independent of user-supplied metadata. **Virus scanning** adds signature-based threat detection, while **quarantine storage** isolates suspicious content. **Content-type validation** prevents spoofing attacks, and **forensic metadata** supports incident response.\n\n> **Security Principle**: Never trust, always verify. File extensions can be spoofed, Content-Type headers can be manipulated, and even magic bytes can be crafted. The validation pipeline uses multiple independent verification methods to build confidence in file safety.\n\n### Error Handling and Resilience\n\nRobust error handling requires precise vocabulary around failure modes, detection mechanisms, and recovery strategies. This terminology enables systematic approach to fault tolerance.\n\n| Term | Definition | Context | Recovery Pattern |\n|------|------------|---------|------------------|\n| **circuit breaker pattern** | Failure isolation mechanism preventing cascading failures by opening circuit after threshold failures | Fault tolerance | CircuitBreaker implementation, failure thresholds, automatic reset |\n| **error propagation** | Systematic handling and communication of failures between components | Error handling | UploadError categorization, error context, retry policies |\n| **graceful degradation** | Reducing service functionality while maintaining core operations during resource constraints | Service resilience | load shedding, feature disabling, priority handling |\n| **resource exhaustion** | Depletion of system resources like memory, disk space, or file descriptors | Capacity management | ResourceMonitor, threshold alerts, admission control |\n| **corruption detection** | Verification of data integrity through checksums and validation | Data integrity | hash verification, chunk validation, assembly verification |\n\nError handling operates at multiple levels. **Circuit breaker patterns** prevent systemic failures, while **error propagation** ensures failures are communicated appropriately. **Graceful degradation** maintains service availability, **resource exhaustion** detection triggers protective measures, and **corruption detection** maintains data integrity.\n\n| Error Category | Detection Latency | Recovery Time | User Impact |\n|----------------|-------------------|---------------|-------------|\n| Transient network | Immediate (connection timeout) | Seconds (retry) | None (automatic retry) |\n| Storage backend failure | 1-5 seconds (operation timeout) | Minutes (failover) | Temporary slowdown |\n| Resource exhaustion | Real-time (monitoring) | Minutes (load shedding) | Feature limitations |\n| Data corruption | Variable (checksum validation) | Variable (re-upload) | Upload restart required |\n\n### Performance and Scalability\n\nPerformance terminology captures the concepts necessary for understanding and optimizing upload service behavior under load and at scale.\n\n| Term | Definition | Context | Optimization Target |\n|------|------------|---------|---------------------|\n| **parallel upload acceleration** | Concurrent chunk processing for improved throughput while maintaining ordering and consistency | Performance optimization | ParallelCoordinator, concurrent chunk processing, assembly optimization |\n| **bandwidth optimization** | Techniques to reduce network usage and improve transfer efficiency | Network efficiency | compression coordination, deduplication, adaptive chunk sizing |\n| **content deduplication** | Hash-based elimination of redundant chunk storage, reducing storage costs and transfer time | Storage efficiency | DeduplicationManager, hash indexing, reference counting |\n| **session affinity** | Binding uploads to specific service instances for state locality and performance | Load balancing | instance binding, state locality, migration coordination |\n| **horizontal scaling** | Multi-instance deployment with coordinated operation enabling increased capacity | Scalability | distributed coordination, service discovery, load balancing |\n\nPerformance optimization addresses multiple bottlenecks simultaneously. **Parallel upload acceleration** improves CPU and I/O utilization, **bandwidth optimization** reduces network constraints, **content deduplication** minimizes storage overhead, **session affinity** reduces coordination overhead, and **horizontal scaling** increases overall capacity.\n\n> **Performance Trade-off**: Parallelization improves throughput but increases complexity. Each concurrent operation requires coordination overhead, memory for buffering, and careful ordering during assembly. The optimal concurrency level depends on hardware characteristics, network conditions, and file size patterns.\n\n### Advanced Features and Extensions\n\nAdvanced terminology covers optional features that extend the basic upload service with additional capabilities for specialized use cases.\n\n| Term | Definition | Context | Extension Point |\n|------|------------|---------|-----------------|\n| **compression coordination** | Management of file compression during upload process, balancing CPU usage with storage savings | Storage optimization | compression algorithms, adaptive compression, size thresholds |\n| **encryption coordination** | Management of file encryption during upload process, supporting client-side and server-side encryption | Data protection | encryption modes, key management, compliance requirements |\n| **metadata extraction** | Automated analysis and indexing of file content for search and categorization | Content processing | MetadataExtractor interface, content analysis, async processing |\n| **CDN integration** | Content distribution network coordination for global access and edge caching | Global distribution | edge synchronization, geographic distribution, cache invalidation |\n| **ML-enhanced virus scanning** | Machine learning augmented threat detection beyond signature-based scanning | Advanced security | behavioral analysis, anomaly detection, adaptive learning |\n\nExtension features provide value-added capabilities beyond basic upload functionality. **Compression coordination** optimizes storage efficiency, **encryption coordination** enhances data protection, **metadata extraction** enables content management, **CDN integration** improves global performance, and **ML-enhanced virus scanning** provides advanced threat detection.\n\n| Extension | Resource Requirements | Complexity Level | Integration Points |\n|-----------|----------------------|------------------|-------------------|\n| Compression | CPU intensive | Medium | Storage pipeline, chunk processing |\n| Encryption | CPU + key management | High | Client protocol, storage backend |\n| Metadata extraction | CPU + specialized libraries | Medium | Post-processing pipeline, content analysis |\n| CDN integration | Network + external APIs | High | Storage backend, URL generation |\n| ML scanning | GPU/specialized hardware | Very High | Virus scanning pipeline, model management |\n\n### Protocol Specifications and Standards\n\nThis section provides authoritative references to the protocols, standards, and specifications that guide the implementation decisions and ensure interoperability.\n\n#### TUS Protocol Specification\n\nThe **tus.io resumable upload protocol** forms the foundation for client-server communication. The complete specification defines the HTTP methods, headers, and request/response formats that ensure interoperability between different client and server implementations.\n\n| HTTP Method | Purpose | Required Headers | Response Codes |\n|-------------|---------|------------------|----------------|\n| POST | Initialize upload session | Upload-Length, Upload-Metadata | 201 Created, 413 Request Entity Too Large |\n| PATCH | Upload chunk data | Upload-Offset, Content-Type | 204 No Content, 409 Conflict, 460 Checksum Mismatch |\n| HEAD | Query upload progress | None | 200 OK with Upload-Offset header |\n| DELETE | Cancel upload session | None | 204 No Content, 404 Not Found |\n\nThe protocol defines several extensions that this implementation may support:\n\n- **Creation Extension**: Enables file upload initialization through POST requests\n- **Checksum Extension**: Provides data integrity verification through configurable hash algorithms  \n- **Expiration Extension**: Allows servers to communicate upload session expiration times\n- **Concatenation Extension**: Supports parallel uploads with final concatenation\n\n#### HTTP Range Request Standards\n\n**RFC 7233** defines the HTTP range request mechanism that underlies chunked upload positioning and resume logic. The implementation relies on these standards for precise byte-level positioning.\n\n| Header | Direction | Format | Purpose |\n|--------|-----------|--------|---------|\n| Range | Client → Server | bytes=start-end | Request specific byte range |\n| Content-Range | Server → Client | bytes start-end/total | Indicate range being served |\n| Accept-Ranges | Server → Client | bytes | Advertise range request support |\n| Content-Length | Bidirectional | byte-count | Specify content size |\n\n#### Cloud Storage APIs\n\nThe storage backend abstraction must interface with various cloud storage APIs, each with specific requirements and capabilities.\n\n| Provider | Multipart API | Authentication | Special Requirements |\n|----------|---------------|----------------|---------------------|\n| AWS S3 | CreateMultipartUpload, UploadPart, CompleteMultipartUpload | AWS Signature V4 | Parts ≥ 5MB except last, max 10,000 parts |\n| Google Cloud Storage | Resumable uploads with upload_id | OAuth 2.0 or service accounts | Chunk size must be multiple of 256KB |\n| Azure Blob Storage | Block blobs with block IDs | Shared keys or Azure AD | Block IDs must be base64 encoded |\n\n### Implementation Dependencies\n\nUnderstanding the external dependencies and their integration patterns helps developers make informed decisions about deployment and operational requirements.\n\n#### ClamAV Integration\n\n**ClamAV** provides the virus scanning engine with specific integration requirements and operational considerations.\n\n| Component | Version Requirement | Configuration | Performance Characteristics |\n|-----------|-------------------|---------------|----------------------------|\n| ClamAV daemon | ≥ 0.103 | Socket-based communication | ~1-10MB/s scan rate, CPU intensive |\n| Signature database | Daily updates | Automated freshclam | ~100MB+ database size |\n| Socket interface | Unix domain socket preferred | /tmp/clamd.socket | Lower latency than TCP |\n\nThe scanning integration requires careful resource management:\n\n```\nConfiguration Parameters:\n- MaxScanSize: Maximum file size for scanning (default: 100MB)\n- MaxFileSize: Individual file limit (default: 25MB)  \n- MaxRecursion: Archive scanning depth (default: 16)\n- MaxFiles: Files within archives (default: 10000)\n- ScanTimeout: Per-file timeout (default: 120s)\n```\n\n#### HTTP Server Requirements\n\nThe service requires an HTTP server implementation that supports specific features for efficient upload handling.\n\n| Feature | Requirement | Rationale | Implementation Notes |\n|---------|-------------|-----------|---------------------|\n| Streaming request bodies | Required | Memory efficiency for large uploads | http.Request.Body streaming |\n| Request timeouts | Configurable | Prevent resource exhaustion | ReadTimeout, WriteTimeout |\n| Connection limits | Configurable | Control resource usage | MaxConnections, rate limiting |\n| TLS support | Required for production | Security compliance | Certificate management, cipher suites |\n\n### Data Format Specifications\n\nThe service works with various data formats and must understand their structure for proper validation and processing.\n\n#### File Type Detection Signatures\n\n**Magic byte detection** relies on well-known file signatures to identify content types regardless of file extensions or MIME type declarations.\n\n| File Type | Magic Bytes | Offset | MIME Type |\n|-----------|-------------|--------|-----------|\n| PNG | 89 50 4E 47 0D 0A 1A 0A | 0 | image/png |\n| JPEG | FF D8 FF | 0 | image/jpeg |\n| PDF | 25 50 44 46 | 0 | application/pdf |\n| ZIP | 50 4B 03 04 or 50 4B 05 06 | 0 | application/zip |\n| GIF | 47 49 46 38 37 61 or 47 49 46 38 39 61 | 0 | image/gif |\n\n#### Checksum Algorithms\n\n**Data integrity verification** supports multiple hash algorithms with different performance and security characteristics.\n\n| Algorithm | Hash Size | Performance | Security Level | Use Case |\n|-----------|-----------|-------------|----------------|----------|\n| MD5 | 128-bit | Very Fast | Low (deprecated for security) | Legacy compatibility |\n| SHA-256 | 256-bit | Fast | High | Primary integrity verification |\n| SHA-512 | 512-bit | Medium | Very High | High-security environments |\n| CRC32 | 32-bit | Very Fast | Very Low | Network error detection only |\n\n### Common Pitfalls and Anti-Patterns\n\nUnderstanding common mistakes helps developers avoid subtle bugs and security vulnerabilities that frequently occur in upload service implementations.\n\n⚠️ **Pitfall: Extension-Based File Type Detection**\nRelying solely on file extensions for content type determination creates security vulnerabilities. Malicious users can upload executable files with image extensions, bypassing security controls. Always validate content using magic byte detection and treat file extensions as hints only.\n\n⚠️ **Pitfall: Insufficient Offset Validation**\nAccepting client-provided offsets without validation can lead to data corruption or security issues. Clients may send overlapping ranges, negative offsets, or offsets beyond file boundaries. Always validate offset against current session state and enforce monotonic progression.\n\n⚠️ **Pitfall: Blocking Virus Scans**\nSynchronous virus scanning blocks upload completion and ties up server resources. Large files can take minutes to scan, leading to timeouts and poor user experience. Implement async scanning with notification mechanisms and consider scan result caching.\n\n⚠️ **Pitfall: Inadequate Error Context**\nGeneric error responses make debugging difficult and provide poor user experience. Errors should include session context, operation details, and actionable recovery information while avoiding information disclosure.\n\n⚠️ **Pitfall: State Synchronization Assumptions**\nAssuming session state consistency across distributed components leads to race conditions and data loss. Network partitions, process crashes, and concurrent access require explicit consistency guarantees and conflict resolution.\n\n### Acronyms and Abbreviations\n\n| Acronym | Full Form | Context |\n|---------|-----------|---------|\n| API | Application Programming Interface | Storage backend interfaces |\n| CDN | Content Delivery Network | Global file distribution |\n| CORS | Cross-Origin Resource Sharing | Browser security policy |\n| CRC | Cyclic Redundancy Check | Data integrity verification |\n| CRUD | Create, Read, Update, Delete | Basic storage operations |\n| ETag | Entity Tag | HTTP caching and versioning |\n| FIFO | First In, First Out | Queue processing order |\n| HTTP | Hypertext Transfer Protocol | Client-server communication |\n| I/O | Input/Output | File and network operations |\n| JSON | JavaScript Object Notation | Data serialization format |\n| MIME | Multipurpose Internet Mail Extensions | Content type identification |\n| REST | Representational State Transfer | API design pattern |\n| RFC | Request for Comments | Internet standards documents |\n| S3 | Simple Storage Service | AWS object storage |\n| SDK | Software Development Kit | Cloud provider libraries |\n| TLS | Transport Layer Security | Encryption in transit |\n| TTL | Time To Live | Expiration mechanism |\n| URL | Uniform Resource Locator | Resource addressing |\n| UUID | Universally Unique Identifier | Session ID generation |\n\n### Implementation Guidance\n\nThis section provides practical guidance for implementing the vocabulary and concepts described above, with particular focus on Go-specific patterns and idioms.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| HTTP Server | net/http with gorilla/mux | Gin or Echo framework |\n| JSON Processing | encoding/json | jsoniter for high performance |\n| Logging | slog (Go 1.21+) | logrus or zap for structured logging |\n| Configuration | encoding/json + os.Getenv | viper for advanced config management |\n| Testing | testing package + testify/assert | Ginkgo/Gomega for BDD-style tests |\n| HTTP Client | net/http | resty for convenience features |\n\n#### Recommended Project Structure\n\nThe vocabulary concepts map to specific packages and modules within the project structure:\n\n```\nresumable-upload-service/\n├── cmd/\n│   ├── server/main.go           ← Entry point, LoadConfig\n│   └── migrate/main.go          ← Session migration utilities\n├── internal/\n│   ├── config/\n│   │   ├── config.go            ← Config, ServerConfig, StorageConfig\n│   │   └── validation.go        ← ValidateRequirements\n│   ├── protocol/\n│   │   ├── tus.go              ← TUS protocol implementation\n│   │   ├── handlers.go         ← InitUploadRequest, ChunkUploadResponse\n│   │   └── errors.go           ← ErrorResponse, UploadError\n│   ├── session/\n│   │   ├── manager.go          ← SessionManager, CreateSession, GetSession\n│   │   ├── store.go            ← StateStore interface, MemoryStore\n│   │   └── models.go           ← UploadSession, SessionStatus\n│   ├── storage/\n│   │   ├── backend.go          ← StorageBackend interface\n│   │   ├── local.go            ← LocalBackend\n│   │   ├── s3.go               ← S3-compatible backend\n│   │   └── credentials.go      ← CredentialProvider, Credentials\n│   ├── validation/\n│   │   ├── detector.go         ← FileSignature, Detector\n│   │   ├── scanner.go          ← ClamAVScanner, ScanResult\n│   │   └── validator.go        ← FileValidator, ValidationResult\n│   ├── coordinator/\n│   │   ├── flow.go             ← FlowCoordinator\n│   │   ├── circuit.go          ← CircuitBreaker, CircuitState\n│   │   └── recovery.go         ← RecoveryCoordinator\n│   └── monitoring/\n│       ├── resources.go        ← ResourceMonitor, ResourceStatus\n│       ├── logging.go          ← StructuredLogger\n│       └── diagnostics.go      ← SessionInspector, SessionDiagnostics\n├── pkg/\n│   └── extensions/\n│       ├── parallel.go         ← ParallelCoordinator\n│       ├── compression.go      ← CompressionCoordinator\n│       ├── deduplication.go    ← DeduplicationManager\n│       └── encryption.go       ← EncryptionCoordinator\n└── test/\n    ├── integration/            ← MilestoneValidator, TusProtocolTester\n    ├── load/                   ← LoadTestScenario\n    └── mocks/                  ← MockStorageBackend\n```\n\n#### Core Vocabulary Implementation\n\nThe following skeleton shows how key vocabulary concepts translate to Go types and interfaces:\n\n```go\n// Package config - Configuration vocabulary\ntype Config struct {\n    Server   ServerConfig   `json:\"server\"`\n    Storage  StorageConfig  `json:\"storage\"`\n    Security SecurityConfig `json:\"security\"`\n    Cleanup  CleanupConfig  `json:\"cleanup\"`\n}\n\n// LoadConfig demonstrates configuration vocabulary integration\nfunc LoadConfig(filename string) (*Config, error) {\n    // TODO: Read JSON configuration file\n    // TODO: Apply environment variable overrides  \n    // TODO: Validate configuration against requirements\n    // TODO: Set reasonable defaults for optional fields\n    return nil, nil\n}\n\n// Package session - Session management vocabulary  \ntype SessionStatus string\n\nconst (\n    SessionStatusInitialized SessionStatus = \"initialized\"\n    SessionStatusActive      SessionStatus = \"active\"\n    SessionStatusCompleting  SessionStatus = \"completing\"\n    SessionStatusCompleted   SessionStatus = \"completed\" \n    SessionStatusFailed      SessionStatus = \"failed\"\n    SessionStatusExpired     SessionStatus = \"expired\"\n)\n\n// UploadSession represents upload session vocabulary\ntype UploadSession struct {\n    ID            string        `json:\"id\"`\n    Filename      string        `json:\"filename\"`\n    ContentType   string        `json:\"content_type\"`\n    TotalSize     int64         `json:\"total_size\"`\n    CurrentOffset int64         `json:\"current_offset\"`\n    Status        SessionStatus `json:\"status\"`\n    CreatedAt     time.Time     `json:\"created_at\"`\n    ExpiresAt     time.Time     `json:\"expires_at\"`\n}\n\n// SessionManager implements session state management vocabulary\ntype SessionManager struct {\n    store   StateStore\n    storage StorageBackend\n    mutex   sync.RWMutex\n}\n\n// CreateSession demonstrates session lifecycle vocabulary\nfunc (sm *SessionManager) CreateSession(ctx context.Context, session *UploadSession) error {\n    // TODO: Validate session parameters against policies\n    // TODO: Generate unique session ID\n    // TODO: Initialize session state in distributed store\n    // TODO: Create storage backend session if required\n    // TODO: Set expiration timers\n    return nil\n}\n```\n\n#### Storage Vocabulary Implementation\n\n```go\n// Package storage - Storage abstraction vocabulary\ntype StorageBackend interface {\n    // StoreChunk implements chunk storage vocabulary\n    StoreChunk(ctx context.Context, key string, data io.Reader, size int64) error\n    \n    // InitMultipart implements multipart upload vocabulary  \n    InitMultipart(ctx context.Context, key string, metadata map[string]string) (*MultipartUpload, error)\n    \n    // CompleteMultipart implements chunk assembly vocabulary\n    CompleteMultipart(ctx context.Context, upload *MultipartUpload, parts []*MultipartPart) error\n}\n\n// MultipartUpload represents multipart upload vocabulary\ntype MultipartUpload struct {\n    ID        string            `json:\"id\"`\n    Key       string            `json:\"key\"`\n    Backend   string            `json:\"backend\"`\n    Metadata  map[string]string `json:\"metadata\"`\n    CreatedAt time.Time         `json:\"created_at\"`\n}\n\n// StorageError implements storage backend failure vocabulary\ntype StorageError struct {\n    Operation string `json:\"operation\"`\n    Key       string `json:\"key\"`\n    Backend   string `json:\"backend\"`  \n    Err       error  `json:\"error\"`\n}\n\nfunc (e *StorageError) Error() string {\n    return fmt.Sprintf(\"storage %s failed for key %s on backend %s: %v\", \n        e.Operation, e.Key, e.Backend, e.Err)\n}\n```\n\n#### Validation Vocabulary Implementation  \n\n```go\n// Package validation - File validation vocabulary\ntype FileSignature struct {\n    Pattern     []byte   `json:\"pattern\"`\n    Offset      int      `json:\"offset\"`\n    MIME        string   `json:\"mime\"`\n    Extensions  []string `json:\"extensions\"`\n    Description string   `json:\"description\"`\n}\n\n// Detector implements magic byte detection vocabulary\ntype Detector struct {\n    signatures []FileSignature\n}\n\n// DetectFromReader demonstrates content-type validation vocabulary\nfunc (d *Detector) DetectFromReader(reader io.Reader) (string, error) {\n    // TODO: Read initial bytes for magic byte detection\n    // TODO: Match against known file signatures\n    // TODO: Return MIME type based on signature match\n    // TODO: Handle multiple potential matches\n    return \"\", nil\n}\n\n// ClamAVScanner implements virus scanning vocabulary\ntype ClamAVScanner struct {\n    socketPath string\n    timeout    time.Duration\n    conn       net.Conn\n}\n\n// ScanResult represents virus scanning vocabulary\ntype ScanResult struct {\n    Clean      bool          `json:\"clean\"`\n    ThreatName string        `json:\"threat_name,omitempty\"`\n    Error      error         `json:\"error,omitempty\"`\n    ScanTime   time.Duration `json:\"scan_time\"`\n}\n```\n\n#### Error Handling Vocabulary Implementation\n\n```go\n// Package coordinator - Error handling vocabulary\nvar (\n    ErrSessionNotFound        = errors.New(\"session not found\")\n    ErrInsufficientSpace     = errors.New(\"insufficient storage space\")\n    ErrSystemOverloaded      = errors.New(\"system resources critically low\")\n    ErrVirusScanTimeout      = errors.New(\"virus scanning operation timed out\")\n    ErrCircuitBreakerOpen    = errors.New(\"circuit breaker open\")\n)\n\n// UploadError implements error propagation vocabulary\ntype UploadError struct {\n    Category    error                  `json:\"category\"`\n    Component   string                 `json:\"component\"`\n    Operation   string                 `json:\"operation\"`\n    SessionID   string                 `json:\"session_id\"`\n    Cause       error                  `json:\"cause\"`\n    Retryable   bool                   `json:\"retryable\"`\n    RetryAfter  int                    `json:\"retry_after\"`\n    Metadata    map[string]interface{} `json:\"metadata\"`\n    Timestamp   time.Time              `json:\"timestamp\"`\n}\n\n// NewUploadError demonstrates error categorization vocabulary\nfunc NewUploadError(category error, component, operation, sessionID string, cause error) *UploadError {\n    // TODO: Categorize error type (transient, permanent, corruption, etc.)\n    // TODO: Determine retry policy based on error category\n    // TODO: Include relevant context metadata\n    // TODO: Set appropriate retry-after timing\n    return nil\n}\n```\n\n#### Testing Vocabulary Implementation\n\n```go\n// Package test - Testing vocabulary implementation\ntype MilestoneValidator struct {\n    serverURL     string\n    storageConfig StorageConfig\n    testDataPath  string\n}\n\n// ValidateMilestone1 demonstrates protocol compliance vocabulary testing\nfunc (mv *MilestoneValidator) ValidateMilestone1() error {\n    // TODO: Test upload session initialization via tus protocol\n    // TODO: Validate chunk upload with offset tracking\n    // TODO: Test upload progress query functionality  \n    // TODO: Verify chunk assembly and completion\n    // TODO: Test error scenarios and recovery\n    return nil\n}\n\n// TusProtocolTester implements tus.io protocol vocabulary testing\ntype TusProtocolTester struct {\n    baseURL    string\n    httpClient *http.Client\n    t          *testing.T\n}\n\n// TestCompleteUploadFlow demonstrates end-to-end protocol testing\nfunc (tpt *TusProtocolTester) TestCompleteUploadFlow() {\n    // TODO: POST to create new upload session\n    // TODO: PATCH requests to upload chunks with proper offsets\n    // TODO: HEAD requests to query upload progress\n    // TODO: Verify final chunk assembly and completion\n    // TODO: Test error responses and edge cases\n}\n```\n\n#### Language-Specific Implementation Hints\n\n**Go Concurrency Patterns for Upload Processing:**\n- Use `sync.RWMutex` for session state protection during concurrent chunk uploads\n- Implement worker pools with `goroutines` and channels for parallel chunk processing  \n- Use `context.Context` for cancellation and timeout propagation across operations\n- Apply `sync.Once` for one-time initialization of shared resources like virus scanners\n\n**File I/O Best Practices:**\n- Use `os.File.Sync()` to ensure data persistence for critical session state changes\n- Implement `io.Copy` with buffer size optimization for efficient chunk storage\n- Leverage `os.O_APPEND` flag for safe concurrent chunk writing to assembly files\n- Use `os.File.Seek()` for precise offset positioning during chunk assembly\n\n**Error Handling Patterns:**\n- Wrap errors with `fmt.Errorf(\"operation failed: %w\", err)` for error chain preservation\n- Use typed errors for recoverable conditions that require specific handling logic\n- Implement exponential backoff with `time.Sleep()` and jitter for retry mechanisms\n- Apply circuit breaker pattern with goroutine-safe state management\n\n#### Milestone Checkpoints\n\n**Milestone 1 - Protocol Vocabulary Validation:**\nAfter implementing the chunked upload protocol vocabulary:\n\n1. Run `go test ./internal/protocol/...` to verify tus.io compliance\n2. Test manual upload: `curl -X POST http://localhost:8080/files -H \"Upload-Length: 1024\"`\n3. Expected response: `201 Created` with `Location` header containing upload URL\n4. Verify session state: Check that `UploadSession` status transitions correctly\n5. Debug check: Session should appear in state store with `SessionStatusInitialized`\n\n**Milestone 2 - Storage Vocabulary Validation:**  \nAfter implementing storage abstraction vocabulary:\n\n1. Run `go test ./internal/storage/...` to verify backend implementations\n2. Test storage switching: Configure local vs S3 backend and verify file operations\n3. Expected behavior: Same upload should work regardless of backend configuration\n4. Verify credential handling: Check that `CredentialProvider` properly manages auth\n5. Debug check: Storage operations should complete without credential errors\n\n**Milestone 3 - Validation Vocabulary Testing:**\nAfter implementing security validation vocabulary:\n\n1. Run `go test ./internal/validation/...` to verify detection and scanning\n2. Test file type detection: Upload files with mismatched extensions and MIME types\n3. Expected behavior: System should detect actual content type regardless of extension\n4. Verify virus scanning: Upload EICAR test file and confirm quarantine\n5. Debug check: `ValidationResult` should reflect proper threat detection\n\nThese checkpoints ensure that the vocabulary concepts are correctly implemented and integrated across the system components.\n"}