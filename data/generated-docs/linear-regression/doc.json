{"html":"<h1 id=\"linear-regression-from-scratch-design-document\">Linear Regression from Scratch: Design Document</h1>\n<h2 id=\"overview\">Overview</h2>\n<p>This system implements linear regression with gradient descent optimization from first principles, teaching the fundamental concepts of machine learning through hands-on implementation. The key architectural challenge is designing a flexible, educational framework that demonstrates both closed-form solutions and iterative optimization while handling single and multiple variable scenarios.</p>\n<blockquote>\n<p>This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.</p>\n</blockquote>\n<h2 id=\"context-and-problem-statement\">Context and Problem Statement</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (foundational understanding)</p>\n</blockquote>\n<h3 id=\"mental-model-the-line-fitting-detective\">Mental Model: The Line-Fitting Detective</h3>\n<p>Imagine you&#39;re a detective investigating a pattern in scattered clues. You have a collection of evidence points—each piece has two measurements: the time it was found (X-coordinate) and its importance level (Y-coordinate). Your job is to find the underlying relationship that connects these clues.</p>\n<p>As you plot these evidence points on a board, you notice they seem to follow a rough line. Some points are exactly on the line, others are scattered nearby. Your detective instincts tell you there&#39;s a pattern here, but how do you find the <strong>best possible line</strong> that captures this relationship? And once you find it, how can you use this line to predict the importance of future evidence based on when you find it?</p>\n<p>This is exactly what <strong>linear regression</strong> does—it&#39;s a mathematical detective that finds the best-fitting line through scattered data points. But unlike our human detective who might eyeball the pattern, linear regression uses precise mathematical methods to find the optimal line that minimizes the total distance between the line and all the evidence points.</p>\n<p>The detective has two main approaches available: the <strong>instant analysis method</strong> (closed-form solution) where they can immediately calculate the perfect line using a mathematical formula, and the <strong>iterative refinement method</strong> (gradient descent) where they start with a guess and gradually improve it until they find the best line. Both approaches lead to the same destination, but they teach us different fundamental concepts about how machines learn from data.</p>\n<p>Just as our detective needs to handle different types of cases—sometimes with just one type of evidence (simple linear regression), sometimes with multiple types of clues (multiple linear regression)—our implementation must be flexible enough to grow from basic line-fitting to complex pattern recognition while maintaining educational clarity about the underlying mathematical principles.</p>\n<h3 id=\"mathematical-foundation\">Mathematical Foundation</h3>\n<p>The mathematical foundation of linear regression rests on two fundamental concepts: <strong>least squares optimization</strong> and <strong>gradient-based parameter estimation</strong>. Understanding both approaches is crucial because they represent different philosophies in machine learning—analytical solutions versus iterative optimization—and each teaches distinct concepts that apply broadly across the field.</p>\n<p>The <strong>least squares principle</strong> provides the theoretical foundation for what we mean by &quot;best fit.&quot; When we have data points scattered around a potential line, we need a precise definition of &quot;best.&quot; Least squares defines this as the line that minimizes the sum of squared vertical distances between each point and the line. Mathematically, if we have points (x₁, y₁), (x₂, y₂), ..., (xₙ, yₙ), and our line is ŷ = mx + b, then we want to minimize the cost function:</p>\n<p><strong>Cost(m, b) = (1/2n) × Σ(yᵢ - (mxᵢ + b))²</strong></p>\n<p>The factor of 1/2n is included for mathematical convenience—it doesn&#39;t change the location of the minimum, but it simplifies derivative calculations. This cost function is <strong>convex</strong>, meaning it has a single global minimum with no local minima, which guarantees that any optimization method will find the same unique solution.</p>\n<p>For simple linear regression, this optimization problem has a <strong>closed-form analytical solution</strong> derived using calculus. By setting the partial derivatives of the cost function equal to zero and solving the resulting system of equations, we obtain the <strong>normal equations</strong>:</p>\n<ul>\n<li>Slope: <strong>m = Σ((xᵢ - x̄)(yᵢ - ȳ)) / Σ((xᵢ - x̄)²)</strong></li>\n<li>Intercept: <strong>b = ȳ - m × x̄</strong></li>\n</ul>\n<p>where x̄ and ȳ are the means of the input and output variables respectively. This closed-form solution is computationally efficient and provides exact results, making it ideal for educational purposes because learners can immediately see the direct relationship between the data and the fitted parameters.</p>\n<p>However, the closed-form approach becomes computationally intractable for multiple linear regression with many features, and it doesn&#39;t generalize to more complex machine learning models. This is where <strong>gradient descent optimization</strong> becomes essential. Gradient descent is an iterative algorithm that starts with initial parameter guesses and repeatedly updates them in the direction that most rapidly decreases the cost function.</p>\n<p>The gradient descent update rules are:</p>\n<ul>\n<li><strong>m_new = m_old - α × ∂Cost/∂m</strong></li>\n<li><strong>b_new = b_old - α × ∂Cost/∂b</strong></li>\n</ul>\n<p>where α is the <strong>learning rate</strong> that controls the step size. The partial derivatives (gradients) for linear regression are:</p>\n<ul>\n<li><strong>∂Cost/∂m = (1/n) × Σ((mx_i + b - y_i) × x_i)</strong></li>\n<li><strong>∂Cost/∂b = (1/n) × Σ(mx_i + b - y_i)</strong></li>\n</ul>\n<p>The learning rate α is a critical hyperparameter that balances convergence speed against stability. Too small, and the algorithm converges slowly; too large, and it may overshoot the minimum and diverge. Understanding this trade-off is fundamental to all iterative optimization in machine learning.</p>\n<p><strong>Convergence detection</strong> is another crucial concept. We stop iterating when the improvement in cost between consecutive iterations falls below a threshold ε, indicating we&#39;ve reached the minimum: <strong>|Cost_new - Cost_old| &lt; ε</strong>. This teaches learners about numerical precision, stopping criteria, and the practical aspects of optimization algorithms.</p>\n<p>The <strong>matrix formulation</strong> becomes essential for multiple linear regression. Instead of separate slope and intercept parameters, we work with a weight vector <strong>w</strong> and express predictions as <strong>ŷ = Xw</strong>, where X is the design matrix that includes a column of ones for the bias term. The gradient descent update becomes: <strong>w_new = w_old - α × X^T(Xw_old - y) / n</strong>, demonstrating vectorization and efficient computation.</p>\n<p>Both approaches—closed-form and iterative—solve the same mathematical problem but teach complementary skills. The closed-form solution teaches the mathematical foundations and provides insight into the statistical properties of linear regression. Gradient descent teaches optimization principles, numerical methods, and computational approaches that scale to complex machine learning models where closed-form solutions don&#39;t exist.</p>\n<h3 id=\"existing-approaches-comparison\">Existing Approaches Comparison</h3>\n<p>Understanding the landscape of existing approaches to linear regression helps learners appreciate why building from scratch provides educational value and how our implementation relates to production tools they&#39;ll encounter in their careers.</p>\n<p><strong>Closed-Form Analytical Solutions</strong> represent the classical statistical approach to linear regression. Libraries like NumPy&#39;s <code>numpy.linalg.lstsq</code> and the normal equation implementation in basic statistics packages fall into this category. The mathematical approach directly computes optimal parameters using matrix operations: <strong>w = (X^T X)^(-1) X^T y</strong> for the general case.</p>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>Advantages</th>\n<th>Disadvantages</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Computational Complexity</strong></td>\n<td>O(n) for simple regression, exact solution in one step</td>\n<td>O(n³) for matrix inversion in multiple regression, becomes intractable with many features</td>\n</tr>\n<tr>\n<td><strong>Numerical Stability</strong></td>\n<td>Deterministic, no hyperparameters to tune</td>\n<td>Susceptible to numerical issues when X^T X is near-singular</td>\n</tr>\n<tr>\n<td><strong>Educational Value</strong></td>\n<td>Clear mathematical foundation, direct connection between data and parameters</td>\n<td>Doesn&#39;t teach optimization concepts crucial for advanced ML</td>\n</tr>\n<tr>\n<td><strong>Scalability</strong></td>\n<td>Works well for small to medium datasets</td>\n<td>Memory requirements grow quadratically with feature count</td>\n</tr>\n<tr>\n<td><strong>Generalization</strong></td>\n<td>Limited to linear models with analytical solutions</td>\n<td>Doesn&#39;t extend to neural networks, logistic regression, or other iterative models</td>\n</tr>\n</tbody></table>\n<p><strong>Iterative Optimization Methods</strong> encompass gradient descent and its variants, which form the backbone of modern machine learning. This includes basic gradient descent, stochastic gradient descent (SGD), and advanced optimizers like Adam and RMSprop used in deep learning frameworks.</p>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>Advantages</th>\n<th>Disadvantages</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Computational Complexity</strong></td>\n<td>O(n) per iteration, scales linearly with data size</td>\n<td>Requires multiple iterations to converge, total complexity depends on convergence rate</td>\n</tr>\n<tr>\n<td><strong>Memory Efficiency</strong></td>\n<td>Constant memory usage regardless of feature count</td>\n<td>Requires storage for gradients and potentially momentum terms</td>\n</tr>\n<tr>\n<td><strong>Educational Value</strong></td>\n<td>Teaches fundamental optimization concepts used throughout ML</td>\n<td>More complex to implement and debug correctly</td>\n</tr>\n<tr>\n<td><strong>Extensibility</strong></td>\n<td>Generalizes to all differentiable models</td>\n<td>Requires understanding of calculus and numerical optimization</td>\n</tr>\n<tr>\n<td><strong>Hyperparameter Sensitivity</strong></td>\n<td>Flexible, can be tuned for different problems</td>\n<td>Requires careful tuning of learning rate and convergence criteria</td>\n</tr>\n</tbody></table>\n<p><strong>Black-Box Production Libraries</strong> like scikit-learn, TensorFlow, and PyTorch provide highly optimized implementations with extensive features, automatic hyperparameter tuning, and production-ready performance optimizations.</p>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>Advantages</th>\n<th>Disadvantages</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Development Speed</strong></td>\n<td>Single function call, extensive documentation</td>\n<td>No understanding of underlying mechanisms</td>\n</tr>\n<tr>\n<td><strong>Performance</strong></td>\n<td>Highly optimized, handles edge cases automatically</td>\n<td>Difficult to debug when things go wrong</td>\n</tr>\n<tr>\n<td><strong>Feature Completeness</strong></td>\n<td>Cross-validation, regularization, multiple solvers</td>\n<td>Overwhelming options for beginners</td>\n</tr>\n<tr>\n<td><strong>Production Readiness</strong></td>\n<td>Battle-tested, handles real-world data issues</td>\n<td>Hides important implementation details</td>\n</tr>\n<tr>\n<td><strong>Learning Curve</strong></td>\n<td>Easy to use for simple cases</td>\n<td>Steep learning curve for advanced features, black-box behavior</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Key Insight</strong>: The fundamental trade-off is between <strong>educational transparency</strong> and <strong>production efficiency</strong>. Our from-scratch implementation prioritizes understanding over performance, making explicit the concepts that production libraries abstract away.</p>\n</blockquote>\n<p>Our implementation strategy deliberately chooses educational value over performance optimization. We implement both closed-form and iterative approaches to demonstrate the conceptual bridge between classical statistics and modern machine learning optimization. This dual approach ensures learners understand when to apply each method and why both exist in the broader ecosystem.</p>\n<p>The <strong>progression from simple to complex</strong> mirrors how the field of machine learning evolved historically. Simple linear regression with closed-form solutions represents the foundation of statistical learning theory. Gradient descent represents the computational revolution that enabled complex models. Multiple linear regression with vectorization demonstrates the mathematical sophistication needed for high-dimensional problems. This progression prepares learners for advanced topics like neural networks, where gradient descent is the only viable optimization approach.</p>\n<p><strong>Architectural Philosophy</strong>: Rather than choosing one approach over another, our implementation demonstrates how different mathematical and computational strategies solve the same underlying problem. This comparative approach helps learners develop intuition about when to apply different techniques—use closed-form solutions when they exist and are computationally feasible, fall back to iterative optimization for complex models or large datasets.</p>\n<p>The implementation serves as a <strong>bridge between theory and practice</strong>. Unlike production libraries that hide complexity, and unlike pure mathematical treatments that avoid implementation details, our approach makes both the mathematics and the computational considerations explicit and understandable. This prepares learners to both understand the tools they&#39;ll use professionally and to implement novel approaches when existing tools don&#39;t meet their needs.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The implementation of linear regression from scratch serves as a foundation for understanding both classical statistics and modern machine learning. This section provides concrete guidance for building an educational system that demonstrates mathematical concepts through working code.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Core Computation</strong></td>\n<td>Pure Python with lists/loops</td>\n<td>NumPy arrays with vectorized operations</td>\n</tr>\n<tr>\n<td><strong>Data Handling</strong></td>\n<td>CSV reading with built-in <code>csv</code> module</td>\n<td>Pandas DataFrames with robust type handling</td>\n</tr>\n<tr>\n<td><strong>Visualization</strong></td>\n<td>Matplotlib with basic plotting</td>\n<td>Seaborn for statistical visualizations</td>\n</tr>\n<tr>\n<td><strong>Testing</strong></td>\n<td>Built-in <code>unittest</code> framework</td>\n<td>Pytest with parametrized tests</td>\n</tr>\n<tr>\n<td><strong>Numerical Precision</strong></td>\n<td>Python <code>float</code> (sufficient for learning)</td>\n<td>NumPy <code>float64</code> for numerical stability</td>\n</tr>\n</tbody></table>\n<p><strong>Recommendation</strong>: Start with NumPy as the core computational engine. While pure Python lists would be more educational for understanding loops and basic operations, NumPy&#39;s vectorized operations are essential for multiple linear regression and provide better numerical stability. The performance benefits also make experimentation more enjoyable.</p>\n<h4 id=\"recommended-project-structure\">Recommended Project Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>linear-regression-from-scratch/\n│\n├── src/\n│   ├── __init__.py\n│   ├── data_handler.py           ← Data loading, validation, preprocessing\n│   ├── simple_regression.py      ← Milestone 1: Simple linear regression\n│   ├── gradient_descent.py       ← Milestone 2: Optimization engine\n│   ├── multiple_regression.py    ← Milestone 3: Multiple features\n│   └── evaluation.py             ← R-squared, visualization utilities\n│\n├── tests/\n│   ├── __init__.py\n│   ├── test_data_handler.py      ← Data loading and preprocessing tests\n│   ├── test_simple_regression.py ← Simple regression validation\n│   ├── test_gradient_descent.py  ← Optimization algorithm tests\n│   └── test_integration.py       ← End-to-end workflow tests\n│\n├── examples/\n│   ├── synthetic_data.py         ← Generate test datasets\n│   ├── milestone_1_demo.py       ← Simple regression example\n│   ├── milestone_2_demo.py       ← Gradient descent example\n│   └── milestone_3_demo.py       ← Multiple regression example\n│\n├── data/\n│   ├── housing.csv               ← Real dataset for testing\n│   └── synthetic.csv             ← Generated data with known parameters\n│\n└── requirements.txt              ← numpy, matplotlib, pytest</code></pre></div>\n\n<p>This structure separates concerns clearly and mirrors the learning progression. Each milestone builds on previous components without modifying them, demonstrating good software engineering practices.</p>\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Data Loading and Validation Utilities</strong> (<code>src/data_handler.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> csv</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tuple, List, Optional, Union</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> load_csv_data</span><span style=\"color:#E1E4E8\">(filename: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, feature_columns: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                  target_column: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Tuple[np.ndarray, np.ndarray]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Load feature and target data from CSV file.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        filename: Path to CSV file</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        feature_columns: List of column names to use as features</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        target_column: Name of target variable column</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Tuple of (features, targets) as numpy arrays</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        FileNotFoundError: If CSV file doesn't exist</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        ValueError: If columns are missing or data is invalid</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    features </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    targets </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(filename, </span><span style=\"color:#9ECBFF\">'r'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">newline</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">''</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> csvfile:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        reader </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> csv.DictReader(csvfile)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Validate columns exist</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> all</span><span style=\"color:#E1E4E8\">(col </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> reader.fieldnames </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> col </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> feature_columns):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            missing </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [col </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> col </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> feature_columns </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> col </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> reader.fieldnames]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Missing feature columns: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">missing</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> target_column </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> reader.fieldnames:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Missing target column: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">target_column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Read data rows</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> row_num, row </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(reader, </span><span style=\"color:#FFAB70\">start</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">):  </span><span style=\"color:#6A737D\"># Start at 2 for header</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                feature_values </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">(row[col]) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> col </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> feature_columns]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                target_value </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> float</span><span style=\"color:#E1E4E8\">(row[target_column])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                features.append(feature_values)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                targets.append(target_value)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Invalid numeric data in row </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">row_num</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(features) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"No valid data rows found\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> np.array(features), np.array(targets)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_data</span><span style=\"color:#E1E4E8\">(features: np.ndarray, targets: np.ndarray) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Validate that feature and target arrays are compatible for regression.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        features: Feature matrix (n_samples, n_features)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        targets: Target vector (n_samples,)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        ValueError: If data has incompatible shapes or contains invalid values</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> features.ndim </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Features must be 2D array, got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">features.ndim</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">D\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> targets.ndim </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Targets must be 1D array, got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">targets.ndim</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">D\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> features.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> targets.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Feature count </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">features.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> != target count </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">targets.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> features.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Need at least 2 data points for regression\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> np.any(np.isnan(features)) </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> np.any(np.isnan(targets)):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Data contains NaN values\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> np.any(np.isinf(features)) </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> np.any(np.isinf(targets)):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Data contains infinite values\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> normalize_features</span><span style=\"color:#E1E4E8\">(features: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Normalize features to zero mean and unit variance (z-score normalization).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        features: Raw feature matrix (n_samples, n_features)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Tuple of (normalized_features, means, stds) where:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        - normalized_features: Features scaled to mean=0, std=1</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        - means: Original mean of each feature (for inverse transform)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        - stds: Original std of each feature (for inverse transform)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    means </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.mean(features, </span><span style=\"color:#FFAB70\">axis</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stds </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.std(features, </span><span style=\"color:#FFAB70\">axis</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Handle constant features (std = 0)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stds </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.where(stds </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">, stds)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    normalized </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (features </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> means) </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> stds</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> normalized, means, stds</span></span></code></pre></div>\n\n<p><strong>Synthetic Data Generation</strong> (<code>examples/synthetic_data.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tuple</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> generate_linear_data</span><span style=\"color:#E1E4E8\">(n_samples: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, slope: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, intercept: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        noise_std: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#E1E4E8\">, x_range: Tuple[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">)) -> Tuple[np.ndarray, np.ndarray]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Generate synthetic data following y = slope * x + intercept + noise.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        n_samples: Number of data points to generate</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        slope: True slope parameter</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        intercept: True intercept parameter  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        noise_std: Standard deviation of Gaussian noise</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        x_range: (min, max) range for x values</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Tuple of (x_values, y_values) as 1D numpy arrays</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    np.random.seed(</span><span style=\"color:#79B8FF\">42</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Reproducible results for testing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    x_min, x_max </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> x_range</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    x </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.random.uniform(x_min, x_max, n_samples)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Perfect linear relationship</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    y_perfect </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> slope </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> x </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> intercept</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Add Gaussian noise</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    noise </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.random.normal(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, noise_std, n_samples)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> y_perfect </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> noise</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> x, y</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> generate_multiple_linear_data</span><span style=\"color:#E1E4E8\">(n_samples: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, coefficients: np.ndarray, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                intercept: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, noise_std: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#E1E4E8\">) -> Tuple[np.ndarray, np.ndarray]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Generate synthetic data for multiple linear regression.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        n_samples: Number of data points</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        coefficients: True coefficient vector (one per feature)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        intercept: True intercept parameter</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        noise_std: Standard deviation of Gaussian noise</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Tuple of (feature_matrix, targets) where feature_matrix is (n_samples, n_features)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    np.random.seed(</span><span style=\"color:#79B8FF\">42</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    n_features </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(coefficients)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Generate random features (standardized)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    features </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.random.randn(n_samples, n_features)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Perfect linear relationship</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    y_perfect </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.dot(features, coefficients) </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> intercept</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Add noise</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    noise </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.random.normal(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, noise_std, n_samples)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> y_perfect </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> noise</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> features, y</span></span></code></pre></div>\n\n<h4 id=\"core-algorithm-skeletons\">Core Algorithm Skeletons</h4>\n<p><strong>Simple Linear Regression</strong> (<code>src/simple_regression.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tuple</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SimpleLinearRegression</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Simple linear regression using closed-form ordinary least squares solution.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.slope_ </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.intercept_ </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.is_fitted_ </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> fit</span><span style=\"color:#E1E4E8\">(self, x: np.ndarray, y: np.ndarray) -> </span><span style=\"color:#9ECBFF\">'SimpleLinearRegression'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Fit simple linear regression model using closed-form solution.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            x: Input features (1D array of shape n_samples)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            y: Target values (1D array of shape n_samples)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            self (fitted model)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate input arrays have same length and are 1D</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check for minimum number of samples (at least 2)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate mean of x and y using np.mean()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Calculate slope using formula: sum((x - x_mean) * (y - y_mean)) / sum((x - x_mean)^2)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Calculate intercept using formula: y_mean - slope * x_mean</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Set is_fitted_ to True and store parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use np.sum() for summations, be careful about division by zero</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> predict</span><span style=\"color:#E1E4E8\">(self, x: np.ndarray) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Make predictions using fitted linear model.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            x: Input values to predict on (1D array)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Predicted values (1D array same length as x)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            RuntimeError: If model hasn't been fitted yet</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if model is fitted, raise RuntimeError if not</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Apply linear equation: y = slope * x + intercept</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return predictions as numpy array</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> score</span><span style=\"color:#E1E4E8\">(self, x: np.ndarray, y: np.ndarray) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Calculate R-squared coefficient of determination.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            x: Input features</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            y: True target values</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            R-squared score (between 0 and 1 for reasonable models)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate predictions using self.predict(x)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate total sum of squares: sum((y - y_mean)^2)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate residual sum of squares: sum((y - predictions)^2)  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Calculate R-squared: 1 - (residual_ss / total_ss)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Handle edge case where total_ss = 0 (constant y values)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Gradient Descent Optimizer</strong> (<code>src/gradient_descent.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Tuple, Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> GradientDescentRegression</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Linear regression using gradient descent optimization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, learning_rate: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.01</span><span style=\"color:#E1E4E8\">, max_iterations: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 tolerance: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-6</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.learning_rate </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> learning_rate</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_iterations </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_iterations</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tolerance </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tolerance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Parameters (set during fitting)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.slope_ </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.intercept_ </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.is_fitted_ </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Training history (for debugging and visualization)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cost_history_: List[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.parameter_history_: List[Tuple[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _compute_cost</span><span style=\"color:#E1E4E8\">(self, x: np.ndarray, y: np.ndarray, slope: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, intercept: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Compute mean squared error cost function.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            x: Input features</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            y: Target values  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            slope: Current slope parameter</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            intercept: Current intercept parameter</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Mean squared error cost</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Calculate predictions using current parameters: y_pred = slope * x + intercept</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate residuals: residuals = y - y_pred</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate mean squared error: (1/2n) * sum(residuals^2)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return cost as float</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Factor of 1/2 simplifies gradients, n is len(x)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _compute_gradients</span><span style=\"color:#E1E4E8\">(self, x: np.ndarray, y: np.ndarray, slope: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, intercept: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> Tuple[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Compute gradients of cost function with respect to parameters.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            x: Input features</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            y: Target values</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            slope: Current slope parameter  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            intercept: Current intercept parameter</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Tuple of (gradient_slope, gradient_intercept)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Calculate predictions: y_pred = slope * x + intercept</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate prediction errors: errors = y_pred - y</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate gradient w.r.t slope: (1/n) * sum(errors * x)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Calculate gradient w.r.t intercept: (1/n) * sum(errors)  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return both gradients as tuple</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: These are partial derivatives of MSE cost function</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> fit</span><span style=\"color:#E1E4E8\">(self, x: np.ndarray, y: np.ndarray) -> </span><span style=\"color:#9ECBFF\">'GradientDescentRegression'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Fit linear regression using gradient descent optimization.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            x: Input features (1D array)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            y: Target values (1D array)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            self (fitted model)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate inputs (same as simple regression)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize parameters - slope=0, intercept=0 (or random small values)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Clear history lists for this training run</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Main gradient descent loop:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   a. Compute current cost and store in history</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   b. Compute gradients for both parameters  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   c. Update parameters: param = param - learning_rate * gradient</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   d. Store current parameters in history</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   e. Check convergence: if cost improvement &#x3C; tolerance, break</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   f. Check max iterations to prevent infinite loops</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Set is_fitted_ = True</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return self</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> predict</span><span style=\"color:#E1E4E8\">(self, x: np.ndarray) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Make predictions (same interface as SimpleLinearRegression).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Same implementation as SimpleLinearRegression.predict()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> score</span><span style=\"color:#E1E4E8\">(self, x: np.ndarray, y: np.ndarray) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate R-squared (same interface as SimpleLinearRegression).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Same implementation as SimpleLinearRegression.score()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Milestone 1 Checkpoint</strong>: After implementing SimpleLinearRegression, run this validation:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Create synthetic data with known parameters</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">x, y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> generate_linear_data(</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">slope</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2.5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">intercept</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">noise_std</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Fit model and check parameters are close to true values</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SimpleLinearRegression()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">model.fit(x, y)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Fitted slope: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">model.slope_</span><span style=\"color:#F97583\">:.3f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> (true: 2.5)\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Fitted intercept: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">model.intercept_</span><span style=\"color:#F97583\">:.3f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> (true: 1.0)\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"R-squared: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">model.score(x, y)</span><span style=\"color:#F97583\">:.3f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected output:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Fitted slope: ~2.5 (within 0.1)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Fitted intercept: ~1.0 (within 0.1)  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># R-squared: >0.95</span></span></code></pre></div>\n\n<p><strong>Milestone 2 Checkpoint</strong>: After implementing GradientDescentRegression:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Same synthetic data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">model_gd </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> GradientDescentRegression(</span><span style=\"color:#FFAB70\">learning_rate</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.01</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">max_iterations</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">model_gd.fit(x, y)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should match closed-form solution</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">model_closed </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SimpleLinearRegression()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">model_closed.fit(x, y)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Gradient descent slope: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">model_gd.slope_</span><span style=\"color:#F97583\">:.3f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Closed-form slope: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">model_closed.slope_</span><span style=\"color:#F97583\">:.3f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Cost decreased: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">model_gd.cost_history_[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">:.3f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> -> </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">model_gd.cost_history_[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">:.3f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: Parameters match within 0.01, cost decreases monotonically</span></span></code></pre></div>\n\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Division by zero error</td>\n<td>All x values are the same</td>\n<td>Print <code>np.var(x)</code> - should be &gt; 0</td>\n<td>Check data loading, ensure x has variation</td>\n</tr>\n<tr>\n<td>Gradient descent diverges</td>\n<td>Learning rate too high</td>\n<td>Plot cost history - should decrease</td>\n<td>Reduce learning rate by factor of 10</td>\n</tr>\n<tr>\n<td>R-squared is negative</td>\n<td>Predictions worse than mean</td>\n<td>Compare predictions vs actual values</td>\n<td>Check model fitting, data quality</td>\n</tr>\n<tr>\n<td>Parameters don&#39;t match expected</td>\n<td>Data scaling issues</td>\n<td>Print data statistics (min, max, std)</td>\n<td>Normalize features before fitting</td>\n</tr>\n<tr>\n<td>Slow convergence</td>\n<td>Learning rate too small</td>\n<td>Check number of iterations to converge</td>\n<td>Increase learning rate gradually</td>\n</tr>\n</tbody></table>\n<h2 id=\"goals-and-non-goals\">Goals and Non-Goals</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (defines the scope and learning objectives for the entire project)</p>\n</blockquote>\n<p>This section establishes the educational foundation and scope boundaries for our linear regression implementation. Unlike production machine learning systems that optimize for performance and feature completeness, this project prioritizes understanding and pedagogical clarity. We explicitly define what concepts learners should master, what capabilities the system should demonstrate, and which advanced topics remain outside our scope to maintain focus on fundamental principles.</p>\n<h3 id=\"learning-goals\">Learning Goals</h3>\n<p>The primary educational objective is to demystify machine learning by implementing linear regression from mathematical first principles. Rather than treating machine learning as a black box where data goes in and predictions come out, learners will understand every step of the process, from the underlying mathematical relationships to the computational algorithms that optimize model parameters.</p>\n<p><strong>Mathematical Foundation Mastery</strong> represents the cornerstone of our learning objectives. Learners should develop an intuitive understanding of what it means to &quot;fit a line to data&quot; beyond simply calling a library function. This includes grasping the least squares principle—that we seek parameters minimizing the sum of squared prediction errors—and understanding why this particular loss function makes mathematical and practical sense. The relationship between the closed-form normal equation solution and gradient descent optimization should become clear, with learners appreciating that both approaches solve the same underlying mathematical problem through different computational strategies.</p>\n<table>\n<thead>\n<tr>\n<th>Learning Concept</th>\n<th>Concrete Understanding</th>\n<th>Assessment Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Least Squares Principle</td>\n<td>Can explain why we minimize squared errors rather than absolute errors</td>\n<td>Describes mathematical properties: differentiability, unique solutions, statistical interpretation</td>\n</tr>\n<tr>\n<td>Gradient Descent Intuition</td>\n<td>Visualizes parameter space as a landscape with cost function as elevation</td>\n<td>Can predict behavior: learning rate too high causes oscillation, too low causes slow convergence</td>\n</tr>\n<tr>\n<td>Feature Scaling Impact</td>\n<td>Understands why features need similar scales for gradient descent</td>\n<td>Recognizes when normalization is needed and can implement z-score standardization</td>\n</tr>\n<tr>\n<td>Overfitting Recognition</td>\n<td>Identifies when models memorize training data vs learning patterns</td>\n<td>Explains bias-variance tradeoff and role of regularization</td>\n</tr>\n<tr>\n<td>R-squared Interpretation</td>\n<td>Interprets coefficient of determination as explained variance proportion</td>\n<td>Distinguishes between correlation and causation, recognizes R-squared limitations</td>\n</tr>\n</tbody></table>\n<p><strong>Algorithmic Thinking Development</strong> forms the second pillar of our educational approach. Machine learning involves systematic problem-solving procedures, and learners should understand these algorithms as step-by-step processes rather than magical transformations. The gradient descent algorithm particularly exemplifies iterative optimization—a fundamental concept appearing throughout computer science and engineering. Learners should internalize the general pattern: define an objective function, compute gradients indicating improvement direction, update parameters incrementally, and iterate until convergence.</p>\n<p><strong>Implementation Skills and Best Practices</strong> ensure learners can translate mathematical understanding into working code. This involves more than syntax—learners should develop intuition for numerical computation challenges like floating-point precision, matrix operations efficiency, and algorithm convergence detection. The progression from scalar operations (single-variable regression) to vectorized matrix operations (multiple regression) teaches computational thinking patterns essential for scientific computing and data analysis.</p>\n<blockquote>\n<p><strong>Key Insight</strong>: The goal is not to build the fastest or most feature-complete regression implementation, but to construct understanding. Every design decision prioritizes clarity and educational value over computational efficiency or production readiness.</p>\n</blockquote>\n<h3 id=\"implementation-goals\">Implementation Goals</h3>\n<p>Our system should demonstrate core machine learning capabilities while maintaining transparency in its operations. The implementation goals balance functionality with educational accessibility, ensuring learners can trace every computation from input data to final predictions.</p>\n<p><strong>Functional Capability Requirements</strong> define what our system must accomplish to provide a complete learning experience. The system should handle the full machine learning workflow: data loading and preprocessing, model training with parameter optimization, prediction generation for new inputs, and model evaluation with interpretable metrics. This end-to-end functionality helps learners understand machine learning as an integrated process rather than disconnected steps.</p>\n<table>\n<thead>\n<tr>\n<th>System Capability</th>\n<th>Implementation Requirement</th>\n<th>Educational Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Loading</td>\n<td>Read CSV files, handle missing values, validate input formats</td>\n<td>Realistic data handling experience, error management</td>\n</tr>\n<tr>\n<td>Single Variable Regression</td>\n<td>Implement closed-form solution using normal equation</td>\n<td>Mathematical foundation without optimization complexity</td>\n</tr>\n<tr>\n<td>Gradient Descent Optimization</td>\n<td>Iterative parameter updates with configurable learning rate</td>\n<td>Algorithmic thinking, convergence understanding</td>\n</tr>\n<tr>\n<td>Multiple Feature Regression</td>\n<td>Matrix operations, vectorized computations</td>\n<td>Computational scaling, linear algebra application</td>\n</tr>\n<tr>\n<td>Feature Normalization</td>\n<td>Z-score standardization, feature scaling</td>\n<td>Data preprocessing importance, numerical stability</td>\n</tr>\n<tr>\n<td>Model Evaluation</td>\n<td>R-squared calculation, prediction accuracy assessment</td>\n<td>Model validation, statistical interpretation</td>\n</tr>\n<tr>\n<td>L2 Regularization</td>\n<td>Ridge regression penalty term</td>\n<td>Overfitting prevention, bias-variance management</td>\n</tr>\n</tbody></table>\n<p><strong>Algorithm Implementation Standards</strong> ensure our code serves as an effective teaching tool. Each algorithm should be implemented clearly and directly, avoiding clever optimizations that obscure the underlying logic. Function names should reflect mathematical concepts (<code>fit</code>, <code>predict</code>, <code>score</code>), and internal calculations should mirror textbook formulations where possible. The gradient descent implementation particularly should expose intermediate values like cost history and parameter evolution, enabling learners to visualize the optimization process.</p>\n<p><strong>Extensibility and Experimentation Support</strong> allows learners to explore beyond the basic implementation. The system architecture should accommodate natural extensions like polynomial features, different regularization penalties, or alternative optimization algorithms. This extensibility reinforces understanding by encouraging learners to modify and experiment with the core algorithms.</p>\n<blockquote>\n<p><strong>Design Principle</strong>: Every function should be simple enough that a learner can implement it from scratch with mathematical understanding and basic programming skills. Complex optimizations that require advanced algorithmic knowledge violate this principle.</p>\n</blockquote>\n<h3 id=\"explicit-non-goals\">Explicit Non-Goals</h3>\n<p>Clearly defining what we will not implement prevents scope creep and maintains focus on fundamental concepts. These non-goals are not value judgments—many represent important machine learning topics—but rather boundary setting to ensure depth over breadth in our educational approach.</p>\n<p><strong>Advanced Machine Learning Algorithms</strong> remain outside our scope despite their practical importance. Logistic regression, although conceptually similar to linear regression, introduces classification concepts and sigmoid functions that complicate the mathematical foundation we&#39;re establishing. Neural networks, support vector machines, and ensemble methods involve substantially different optimization landscapes and would dilute focus from understanding gradient-based parameter learning in its simplest form.</p>\n<table>\n<thead>\n<tr>\n<th>Excluded Topic</th>\n<th>Rationale for Exclusion</th>\n<th>Suggested Learning Sequence</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Logistic Regression</td>\n<td>Introduces classification, sigmoid functions, maximum likelihood</td>\n<td>Natural next step after mastering linear regression</td>\n</tr>\n<tr>\n<td>Neural Networks</td>\n<td>Multi-layer optimization, backpropagation complexity</td>\n<td>Requires solid gradient descent foundation first</td>\n</tr>\n<tr>\n<td>Ensemble Methods</td>\n<td>Meta-learning concepts, model combination strategies</td>\n<td>Advanced topic after individual algorithm mastery</td>\n</tr>\n<tr>\n<td>Support Vector Machines</td>\n<td>Convex optimization, kernel methods, different mathematical framework</td>\n<td>Alternative approach after regression understanding</td>\n</tr>\n<tr>\n<td>Time Series Analysis</td>\n<td>Temporal dependencies, autoregression, stationarity concepts</td>\n<td>Specialized application requiring regression foundation</td>\n</tr>\n</tbody></table>\n<p><strong>Production Optimization and Scalability</strong> explicitly fall outside our educational mission. Real-world machine learning systems require sophisticated optimizations like stochastic gradient descent, mini-batch processing, distributed computation, and specialized linear algebra libraries. These optimizations obscure the fundamental algorithms we&#39;re teaching and introduce engineering complexity unrelated to machine learning understanding.</p>\n<p><strong>Advanced Statistical Concepts</strong> would enhance our models but distract from core algorithmic learning. Hypothesis testing for parameter significance, confidence intervals for predictions, and advanced model selection criteria represent important statistical machine learning topics. However, these concepts require substantial statistical background and shift focus from computational algorithm understanding to statistical inference theory.</p>\n<p><strong>Data Engineering and Pipeline Complexity</strong> exceed our scope despite their practical necessity. Real machine learning projects involve data cleaning, feature engineering, missing value imputation strategies, outlier detection, and data validation pipelines. While we include basic data loading and normalization, comprehensive data preprocessing would overwhelm learners with engineering concerns before they understand the core algorithms.</p>\n<blockquote>\n<p><strong>Scope Boundary Principle</strong>: If a feature requires more than introductory mathematics (calculus and linear algebra) or more than basic programming concepts (loops, functions, arrays), it likely belongs in a follow-up project rather than this foundational implementation.</p>\n</blockquote>\n<p><strong>Performance Optimization and Numerical Libraries</strong> represent another explicit non-goal. While NumPy provides essential array operations, we avoid advanced numerical optimization libraries like SciPy&#39;s optimization modules or specialized linear algebra routines. Our implementations should remain comprehensible and traceable, even if they sacrifice computational efficiency. This pedagogical approach helps learners understand what optimized libraries actually accomplish when they eventually use them in production contexts.</p>\n<p>The distinction between learning goals and non-goals creates a focused educational experience. Learners completing this project should possess solid intuition for linear regression mathematics, practical experience with gradient-based optimization, and implementation skills transferable to more advanced machine learning algorithms. They should understand exactly what happens inside the &quot;black box&quot; of linear regression, providing a foundation for both using machine learning libraries effectively and implementing more sophisticated algorithms from scratch.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This implementation guidance provides concrete technology recommendations and starter code to help learners focus on the core machine learning concepts rather than getting stuck on infrastructure details.</p>\n<p><strong>A. Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Handling</td>\n<td>Pure NumPy arrays + CSV module</td>\n<td>Pandas DataFrames with advanced preprocessing</td>\n</tr>\n<tr>\n<td>Linear Algebra</td>\n<td>NumPy basic operations (dot, transpose)</td>\n<td>SciPy sparse matrices and optimized BLAS</td>\n</tr>\n<tr>\n<td>Visualization</td>\n<td>Matplotlib basic plotting</td>\n<td>Seaborn statistical plots + interactive widgets</td>\n</tr>\n<tr>\n<td>Testing</td>\n<td>Built-in assert statements + manual verification</td>\n<td>pytest with parameterized tests and fixtures</td>\n</tr>\n<tr>\n<td>Documentation</td>\n<td>Inline comments + docstrings</td>\n<td>Sphinx documentation with mathematical notation</td>\n</tr>\n</tbody></table>\n<p>For this educational project, choose the simple options. The goal is understanding the algorithms, not mastering data science toolchains.</p>\n<p><strong>B. Recommended File Structure</strong></p>\n<p>Organize your project to mirror the learning progression and separate concerns clearly:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>linear-regression/\n├── data/\n│   ├── synthetic_data.csv          ← Generated datasets for testing\n│   └── real_data.csv               ← Optional real-world dataset\n├── src/\n│   ├── __init__.py\n│   ├── data_handler.py             ← Data loading, validation, normalization\n│   ├── simple_regression.py        ← Milestone 1: Single variable, closed-form\n│   ├── gradient_descent.py         ← Milestone 2: Optimization algorithm\n│   ├── multiple_regression.py      ← Milestone 3: Multi-variable extension\n│   └── utils.py                    ← Helper functions, evaluation metrics\n├── tests/\n│   ├── test_data_handler.py\n│   ├── test_simple_regression.py\n│   ├── test_gradient_descent.py\n│   └── test_multiple_regression.py\n├── notebooks/\n│   ├── milestone1_exploration.ipynb ← Interactive testing and visualization\n│   ├── milestone2_optimization.ipynb\n│   └── milestone3_comparison.ipynb\n└── main.py                         ← Demo script showing complete workflow</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code</strong></p>\n<p>Here&#39;s complete, working infrastructure code that handles the non-learning aspects:</p>\n<p><strong>utils.py</strong> (Complete implementation):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> csv</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tuple, Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> generate_linear_data</span><span style=\"color:#E1E4E8\">(n_samples: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, slope: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, intercept: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        noise_std: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, x_range: Tuple[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">)) -> Tuple[np.ndarray, np.ndarray]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Generate synthetic linear data with controlled noise for testing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    np.random.seed(</span><span style=\"color:#79B8FF\">42</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Reproducible results</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    x </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.random.uniform(x_range[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">], x_range[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">], n_samples)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> slope </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> x </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> intercept </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> np.random.normal(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, noise_std, n_samples)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> x.reshape(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">), y</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> load_csv_data</span><span style=\"color:#E1E4E8\">(filename: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, feature_columns: </span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">, target_column: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Tuple[np.ndarray, np.ndarray]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Load feature and target data from CSV file.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    features </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    targets </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(filename, </span><span style=\"color:#9ECBFF\">'r'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#FFAB70\"> file</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        reader </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> csv.DictReader(</span><span style=\"color:#FFAB70\">file</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> row </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> reader:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            feature_values </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">(row[col]) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> col </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> feature_columns]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            target_value </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> float</span><span style=\"color:#E1E4E8\">(row[target_column])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            features.append(feature_values)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            targets.append(target_value)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> np.array(features), np.array(targets)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_data</span><span style=\"color:#E1E4E8\">(features: np.ndarray, targets: np.ndarray) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validate that features and targets are compatible for regression.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(features.shape) </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Features must be 2D array, got shape </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">features.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(targets.shape) </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Targets must be 1D array, got shape </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">targets.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> features.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> targets.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Feature samples (</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">features.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">) != target samples (</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">targets.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> features.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Need at least 2 samples for regression, got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">features.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> plot_regression_results</span><span style=\"color:#E1E4E8\">(x: np.ndarray, y: np.ndarray, predictions: np.ndarray, title: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Visualize regression fit (requires matplotlib).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        import</span><span style=\"color:#E1E4E8\"> matplotlib.pyplot </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> plt</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        plt.figure(</span><span style=\"color:#FFAB70\">figsize</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">6</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        plt.scatter(x.flatten(), y, </span><span style=\"color:#FFAB70\">alpha</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.6</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">label</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'Actual Data'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        plt.plot(x.flatten(), predictions, </span><span style=\"color:#9ECBFF\">'r-'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">linewidth</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">label</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'Fitted Line'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        plt.xlabel(</span><span style=\"color:#9ECBFF\">'Feature Value'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        plt.ylabel(</span><span style=\"color:#9ECBFF\">'Target Value'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        plt.title(title)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        plt.legend()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        plt.grid(</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">alpha</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        plt.show()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    except</span><span style=\"color:#79B8FF\"> ImportError</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Matplotlib not available - skipping visualization\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code</strong></p>\n<p>Here are the skeleton implementations for the main learning components:</p>\n<p><strong>simple_regression.py</strong> (Skeleton for Milestone 1):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SimpleLinearRegression</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Single-variable linear regression using closed-form solution.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.slope_: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.intercept_: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.is_fitted_: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> fit</span><span style=\"color:#E1E4E8\">(self, x: np.ndarray, y: np.ndarray) -> </span><span style=\"color:#9ECBFF\">'SimpleLinearRegression'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Fit linear regression using ordinary least squares closed-form solution.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        The normal equation gives us: slope = cov(x,y) / var(x), intercept = mean(y) - slope * mean(x)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate input data shapes and types</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Extract x and y as 1D arrays (handle single feature case)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate sample means of x and y</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Calculate covariance of x and y: sum((x - x_mean) * (y - y_mean)) / (n - 1)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Calculate variance of x: sum((x - x_mean)^2) / (n - 1)  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Compute slope as covariance / variance (handle zero variance case)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Compute intercept as y_mean - slope * x_mean</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Set is_fitted_ = True and store parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Return self for method chaining</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> predict</span><span style=\"color:#E1E4E8\">(self, x: np.ndarray) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate predictions using fitted parameters: y = slope * x + intercept.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if model is fitted (raise ValueError if not)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate input shape matches training data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Apply linear equation: predictions = slope_ * x + intercept_</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return predictions as numpy array</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> score</span><span style=\"color:#E1E4E8\">(self, x: np.ndarray, y: np.ndarray) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate R-squared coefficient of determination.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate predictions for input x</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate sum of squared residuals: sum((y - predictions)^2)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate total sum of squares: sum((y - mean(y))^2)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Compute R-squared: 1 - (residual_ss / total_ss)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Handle edge case where total_ss = 0 (constant target)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>gradient_descent.py</strong> (Skeleton for Milestone 2):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Tuple</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> GradientDescentRegression</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Linear regression using gradient descent optimization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, learning_rate: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.01</span><span style=\"color:#E1E4E8\">, max_iterations: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">, tolerance: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-6</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.learning_rate </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> learning_rate</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_iterations </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_iterations</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tolerance </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tolerance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Parameters learned during training</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.slope_: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.intercept_: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.is_fitted_: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Training history for analysis</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cost_history_: List[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.parameter_history_: List[Tuple[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _compute_cost</span><span style=\"color:#E1E4E8\">(self, x: np.ndarray, y: np.ndarray, slope: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, intercept: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate mean squared error cost function.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate predictions using current parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate squared errors: (predictions - y)^2</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return mean of squared errors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _compute_gradients</span><span style=\"color:#E1E4E8\">(self, x: np.ndarray, y: np.ndarray, slope: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, intercept: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> Tuple[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate partial derivatives of cost function with respect to parameters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate predictions using current parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate prediction errors: predictions - y</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate gradient for slope: (2/n) * sum(errors * x)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Calculate gradient for intercept: (2/n) * sum(errors)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return both gradients as tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> fit</span><span style=\"color:#E1E4E8\">(self, x: np.ndarray, y: np.ndarray) -> </span><span style=\"color:#9ECBFF\">'GradientDescentRegression'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Fit linear regression using gradient descent optimization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate and prepare input data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize parameters (slope=0, intercept=mean(y) works well)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Clear history lists</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Main optimization loop:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         a. Compute current cost and store in history</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         b. Compute gradients for current parameters  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         c. Update parameters: param = param - learning_rate * gradient</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         d. Store parameters in history</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         e. Check convergence: if cost improvement &#x3C; tolerance, break</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         f. Check max_iterations limit</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Set is_fitted_ = True</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return self</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Python Hints</strong></p>\n<ul>\n<li>Use <code>np.mean()</code>, <code>np.sum()</code>, and <code>np.dot()</code> for vectorized operations instead of Python loops</li>\n<li>Handle division by zero with <code>np.isclose(variance, 0)</code> checks before dividing</li>\n<li>Use <code>x.reshape(-1, 1)</code> to ensure features are always 2D arrays (n_samples, n_features)</li>\n<li>Store training history in lists, convert to NumPy arrays only when needed for analysis</li>\n<li>Use type hints (<code>-&gt; np.ndarray</code>) to make interfaces clear and catch errors early</li>\n<li>Implement <code>__repr__</code> methods to make debugging easier: <code>f&quot;SimpleLinearRegression(slope={self.slope_}, intercept={self.intercept_})&quot;</code></li>\n</ul>\n<p><strong>F. Milestone Checkpoints</strong></p>\n<p><strong>Milestone 1 Checkpoint:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Create test data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">x, y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> generate_linear_data(</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">slope</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2.5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">intercept</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">noise_std</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.5</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test your implementation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SimpleLinearRegression()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">model.fit(x, y)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">predictions </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model.predict(x)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">r_squared </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model.score(x, y)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"True slope: 2.5, Fitted slope: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">model.slope_</span><span style=\"color:#F97583\">:.2f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"True intercept: 1.0, Fitted intercept: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">model.intercept_</span><span style=\"color:#F97583\">:.2f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"R-squared: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">r_squared</span><span style=\"color:#F97583\">:.3f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected output:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># True slope: 2.5, Fitted slope: 2.48</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># True intercept: 1.0, Fitted intercept: 1.02  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># R-squared: 0.962</span></span></code></pre></div>\n\n<p><strong>Milestone 2 Checkpoint:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Compare gradient descent with closed-form solution</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">gd_model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> GradientDescentRegression(</span><span style=\"color:#FFAB70\">learning_rate</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.01</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">max_iterations</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">gd_model.fit(x, y)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Gradient descent converged in </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(gd_model.cost_history_)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> iterations\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Final cost: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">gd_model.cost_history_[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">:.6f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Parameters match closed-form: slope diff = </span><span style=\"color:#79B8FF\">{abs</span><span style=\"color:#E1E4E8\">(model.slope_ </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> gd_model.slope_)</span><span style=\"color:#F97583\">:.4f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Plot cost history to verify convergence</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> matplotlib.pyplot </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> plt</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">plt.plot(gd_model.cost_history_)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">plt.xlabel(</span><span style=\"color:#9ECBFF\">'Iteration'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">plt.ylabel(</span><span style=\"color:#9ECBFF\">'Cost (MSE)'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">plt.title(</span><span style=\"color:#9ECBFF\">'Gradient Descent Convergence'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">plt.show()</span></span></code></pre></div>\n\n<p><strong>G. Common Implementation Pitfalls</strong></p>\n<table>\n<thead>\n<tr>\n<th>Problem Symptom</th>\n<th>Likely Cause</th>\n<th>Debugging Steps</th>\n<th>Solution</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>slope_</code> is NaN</td>\n<td>Division by zero in variance calculation</td>\n<td>Check if all x values are identical</td>\n<td>Add variance check: <code>if np.isclose(x_var, 0): raise ValueError(&quot;Cannot fit line to constant x values&quot;)</code></td>\n</tr>\n<tr>\n<td>R-squared &gt; 1.0 or negative</td>\n<td>Wrong residual or total sum calculation</td>\n<td>Print intermediate values: residuals, total variance</td>\n<td>Ensure using sample variance, not population variance in denominator</td>\n</tr>\n<tr>\n<td>Gradient descent diverges</td>\n<td>Learning rate too high</td>\n<td>Plot cost history - should decrease monotonically</td>\n<td>Reduce learning rate by factor of 10, try 0.001 or 0.0001</td>\n</tr>\n<tr>\n<td>Convergence very slow</td>\n<td>Learning rate too low or features not normalized</td>\n<td>Check iteration count and parameter changes</td>\n<td>Increase learning rate or implement feature scaling</td>\n</tr>\n<tr>\n<td>Matrix dimension errors</td>\n<td>Wrong array shapes for multiple features</td>\n<td>Print shapes at each step: <code>print(f&quot;x shape: {x.shape}, y shape: {y.shape}&quot;)</code></td>\n<td>Use <code>x.reshape(-1, 1)</code> for single features, ensure y is 1D</td>\n</tr>\n</tbody></table>\n<h2 id=\"high-level-architecture\">High-Level Architecture</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (provides the foundational architecture that evolves across M1: Simple Linear Regression, M2: Gradient Descent, M3: Multiple Linear Regression)</p>\n</blockquote>\n<p>This section presents the system&#39;s architectural blueprint, designed to support a progressive learning journey from basic mathematical concepts to sophisticated machine learning algorithms. The architecture emphasizes educational clarity while maintaining the flexibility to evolve from simple closed-form solutions to complex iterative optimization techniques.</p>\n<h3 id=\"component-overview-the-four-main-components\">Component Overview: The Four Main Components</h3>\n<p>The linear regression system is built around four core components that mirror the conceptual phases of machine learning: data preparation, model representation, optimization, and evaluation. This architectural division serves both pedagogical and practical purposes, allowing learners to understand each phase independently while appreciating their interconnected nature.</p>\n<p><strong>Mental Model: The Machine Learning Assembly Line</strong></p>\n<p>Think of our system as a modern manufacturing assembly line, but instead of building cars, we&#39;re building mathematical models that can make predictions. Each station (component) has a specific job: the first station prepares raw materials (data), the second station shapes the basic structure (model), the third station fine-tunes the assembly (optimization), and the fourth station tests quality (evaluation). Just like in manufacturing, the output of one station becomes the input to the next, and problems at any station affect the entire production line.</p>\n<p>The <strong>DataHandler</strong> component serves as the system&#39;s intake and preparation facility. This component is responsible for ingesting raw data from various sources, validating its structure and content, and transforming it into the standardized format required by the mathematical algorithms. The DataHandler abstracts away the complexities of different data formats, missing value handling, and feature scaling, presenting a clean, consistent interface to downstream components.</p>\n<table>\n<thead>\n<tr>\n<th>Responsibility</th>\n<th>Description</th>\n<th>Key Operations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Loading</td>\n<td>Read data from CSV files, arrays, or synthetic generators</td>\n<td><code>load_csv_data()</code>, <code>generate_linear_data()</code></td>\n</tr>\n<tr>\n<td>Data Validation</td>\n<td>Ensure data compatibility and detect common issues</td>\n<td><code>validate_data()</code></td>\n</tr>\n<tr>\n<td>Feature Engineering</td>\n<td>Transform raw features into model-ready format</td>\n<td><code>normalize_features()</code></td>\n</tr>\n<tr>\n<td>Error Handling</td>\n<td>Gracefully handle malformed or incompatible data</td>\n<td>Input sanitization, type checking</td>\n</tr>\n</tbody></table>\n<p>The <strong>SimpleLinearRegression</strong> component implements the foundational single-variable linear regression using the closed-form solution. This component embodies the mathematical elegance of the ordinary least squares method, providing learners with immediate feedback and perfect convergence without the complexities of iterative optimization. It serves as both a complete solution for simple problems and a reference implementation for validating more complex approaches.</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>fit(x, y)</code></td>\n<td>x: np.ndarray, y: np.ndarray</td>\n<td>self</td>\n<td>Computes optimal slope and intercept using normal equation</td>\n</tr>\n<tr>\n<td><code>predict(x)</code></td>\n<td>x: np.ndarray</td>\n<td>np.ndarray</td>\n<td>Generates predictions using fitted parameters</td>\n</tr>\n<tr>\n<td><code>score(x, y)</code></td>\n<td>x: np.ndarray, y: np.ndarray</td>\n<td>float</td>\n<td>Calculates R-squared coefficient of determination</td>\n</tr>\n</tbody></table>\n<p>The <strong>GradientDescentOptimizer</strong> component introduces the iterative optimization paradigm that underpins modern machine learning. Unlike the closed-form approach, this component finds optimal parameters through repeated refinement, mimicking how humans learn through practice and feedback. This component is essential for understanding how complex models with millions of parameters can be trained when closed-form solutions become computationally infeasible.</p>\n<table>\n<thead>\n<tr>\n<th>Core Function</th>\n<th>Purpose</th>\n<th>Mathematical Foundation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Cost Computation</td>\n<td>Measure prediction error</td>\n<td>Mean squared error: <code>(1/2n) * Σ(y_pred - y_actual)²</code></td>\n</tr>\n<tr>\n<td>Gradient Calculation</td>\n<td>Determine parameter update direction</td>\n<td>Partial derivatives of cost function</td>\n</tr>\n<tr>\n<td>Parameter Updates</td>\n<td>Iteratively improve parameter estimates</td>\n<td><code>parameter = parameter - learning_rate * gradient</code></td>\n</tr>\n<tr>\n<td>Convergence Detection</td>\n<td>Determine when optimization is complete</td>\n<td>Cost change falls below tolerance threshold</td>\n</tr>\n</tbody></table>\n<p>The <strong>MultipleLinearRegression</strong> component extends the system to handle multiple input features, transforming the problem from fitting lines to fitting hyperplanes in multi-dimensional space. This component introduces vectorization and matrix operations, demonstrating how mathematical elegance scales to handle real-world complexity. It also incorporates regularization techniques to prevent overfitting when dealing with many features.</p>\n<blockquote>\n<p><strong>Key Architectural Insight</strong>: The progression from SimpleLinearRegression → GradientDescentOptimizer → MultipleLinearRegression mirrors the typical machine learning learning curve: start with intuitive concepts, understand optimization principles, then scale to realistic complexity. Each component builds conceptual understanding that makes the next component accessible.</p>\n</blockquote>\n<h3 id=\"recommended-project-structure\">Recommended Project Structure</h3>\n<p>The project structure is designed to mirror the learning progression, with each directory corresponding to a conceptual milestone. This organization helps learners understand dependencies and provides clear boundaries between different algorithmic approaches.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>linear_regression_project/\n├── README.md                           # Project overview and learning objectives\n├── requirements.txt                    # Python dependencies (numpy, matplotlib, pandas)\n├── setup.py                            # Package installation script\n│\n├── data/                               # Sample datasets and data generation utilities\n│   ├── synthetic/                      # Generated datasets for testing and validation\n│   │   ├── simple_linear.csv          # Single-variable synthetic data\n│   │   ├── multiple_features.csv      # Multi-variable synthetic data\n│   │   └── noisy_data.csv             # High-noise dataset for testing robustness\n│   ├── real_world/                    # Actual datasets for final validation\n│   │   ├── housing_prices.csv        # Real estate prediction dataset\n│   │   └── student_scores.csv        # Educational performance dataset\n│   └── data_generator.py              # Utilities for creating synthetic datasets\n│\n├── src/                               # Main source code organized by learning milestone\n│   ├── __init__.py\n│   ├── data_handler/                  # M1: Data loading and preprocessing\n│   │   ├── __init__.py\n│   │   ├── loader.py                  # CSV loading and validation\n│   │   ├── validator.py               # Data quality checks and error detection\n│   │   ├── preprocessor.py            # Feature scaling and normalization\n│   │   └── synthetic_generator.py     # Synthetic data creation for testing\n│   │\n│   ├── simple_regression/             # M1: Closed-form linear regression\n│   │   ├── __init__.py\n│   │   ├── model.py                   # SimpleLinearRegression class implementation\n│   │   ├── math_utils.py              # Mathematical utilities (mean, variance, etc.)\n│   │   └── evaluation.py              # R-squared and other metrics\n│   │\n│   ├── gradient_descent/              # M2: Iterative optimization\n│   │   ├── __init__.py\n│   │   ├── optimizer.py               # GradientDescentRegression class\n│   │   ├── cost_functions.py          # Mean squared error and derivatives\n│   │   └── convergence.py             # Stopping criteria and progress tracking\n│   │\n│   ├── multiple_regression/           # M3: Multi-variable and advanced features\n│   │   ├── __init__.py\n│   │   ├── model.py                   # MultipleLinearRegression class\n│   │   ├── matrix_operations.py       # Vectorized computations\n│   │   ├── regularization.py          # L2 regularization (Ridge regression)\n│   │   └── feature_scaling.py         # Advanced normalization techniques\n│   │\n│   └── utils/                         # Shared utilities across all components\n│       ├── __init__.py\n│       ├── plotting.py                # Visualization helpers for debugging\n│       ├── metrics.py                 # Common evaluation metrics\n│       └── debugging.py               # Logging and diagnostic tools\n│\n├── tests/                             # Comprehensive test suite organized by milestone\n│   ├── __init__.py\n│   ├── test_data_handler/             # Unit tests for data handling components\n│   │   ├── test_loader.py\n│   │   ├── test_validator.py\n│   │   └── test_preprocessor.py\n│   ├── test_simple_regression/        # Tests for closed-form implementation\n│   │   ├── test_model.py\n│   │   └── test_evaluation.py\n│   ├── test_gradient_descent/         # Tests for optimization components\n│   │   ├── test_optimizer.py\n│   │   └── test_convergence.py\n│   ├── test_multiple_regression/      # Tests for multi-variable regression\n│   │   ├── test_model.py\n│   │   └── test_regularization.py\n│   └── integration/                   # End-to-end testing\n│       ├── test_complete_pipeline.py\n│       └── test_cross_validation.py\n│\n├── examples/                          # Educational examples and tutorials\n│   ├── milestone_1_simple.py         # Complete M1 example with explanations\n│   ├── milestone_2_gradient.py       # M2 example showing optimization process\n│   ├── milestone_3_multiple.py       # M3 example with real datasets\n│   ├── debugging_example.py          # Common debugging scenarios\n│   └── comparison_study.py           # Compare all three approaches\n│\n├── notebooks/                        # Jupyter notebooks for interactive learning\n│   ├── 01_data_exploration.ipynb     # Understanding data and visualization\n│   ├── 02_closed_form_solution.ipynb # Mathematical derivation and implementation\n│   ├── 03_gradient_descent_viz.ipynb # Visualizing the optimization process\n│   └── 04_advanced_topics.ipynb     # Regularization and feature engineering\n│\n└── docs/                             # Additional documentation\n    ├── mathematical_background.md    # Derivations and theory\n    ├── troubleshooting.md            # Common issues and solutions\n    └── extensions.md                 # Ideas for further development</code></pre></div>\n\n<p>This structure provides several key benefits for learners:</p>\n<p><strong>Progressive Complexity</strong>: Each directory builds on previous concepts, allowing learners to master one milestone before proceeding to the next. The separation prevents cognitive overload while maintaining clear conceptual boundaries.</p>\n<p><strong>Clear Dependencies</strong>: The structure makes component dependencies explicit. Data handling is foundational and used by all regression components. Simple regression provides a reference for gradient descent validation. Multiple regression builds on both previous approaches.</p>\n<p><strong>Practical Organization</strong>: Tests, examples, and documentation are co-located with relevant code, encouraging good development practices from the beginning of the learning journey.</p>\n<blockquote>\n<p><strong>Architecture Decision: Milestone-Based Directory Structure vs. Feature-Based Structure</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to organize code in a way that supports progressive learning while maintaining good software engineering practices</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Feature-based (models/, optimizers/, data/ directories)</li>\n<li>Milestone-based (simple_regression/, gradient_descent/, multiple_regression/)</li>\n<li>Single flat directory with all components mixed</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Milestone-based directory structure with shared utilities</li>\n<li><strong>Rationale</strong>: Learning progression is the primary concern for this educational project. Clear milestone boundaries help learners understand conceptual dependencies and provide natural stopping points for consolidation. Feature-based organization would mix complexity levels and obscure the learning path.</li>\n<li><strong>Consequences</strong>: Some code duplication between milestones, but clearer learning progression and easier incremental development</li>\n</ul>\n</blockquote>\n<h3 id=\"component-interactions\">Component Interactions</h3>\n<p>The component interactions follow a clear data flow pattern that mirrors the machine learning workflow: data preparation, model fitting, prediction generation, and performance evaluation. Understanding these interactions is crucial for debugging and extending the system.</p>\n<p><img src=\"/api/project/linear-regression/architecture-doc/asset?path=diagrams%2Fsystem-components.svg\" alt=\"System Component Architecture\"></p>\n<p><strong>Training Data Flow Sequence</strong></p>\n<p>The training process follows a standardized sequence regardless of which regression approach is used. This consistency allows learners to focus on algorithmic differences rather than integration complexity.</p>\n<ol>\n<li><p><strong>Data Acquisition</strong>: The client code invokes <code>load_csv_data()</code> on the DataHandler component, providing file path and column specifications. The DataHandler reads the raw data, performs initial type checking, and returns feature and target arrays.</p>\n</li>\n<li><p><strong>Data Validation</strong>: The <code>validate_data()</code> function checks for common issues: mismatched array lengths, missing values, infinite values, and data types incompatible with numerical computation. This step prevents downstream failures and provides clear error messages.</p>\n</li>\n<li><p><strong>Feature Preprocessing</strong>: For gradient descent and multiple regression, <code>normalize_features()</code> applies z-score standardization to ensure all features contribute equally to the optimization process. Simple linear regression can skip this step for single variables.</p>\n</li>\n<li><p><strong>Model Initialization</strong>: The chosen regression class (<code>SimpleLinearRegression</code>, <code>GradientDescentRegression</code>, or <code>MultipleLinearRegression</code>) is instantiated with appropriate hyperparameters. Initial parameter values are set to defaults or small random values.</p>\n</li>\n<li><p><strong>Parameter Fitting</strong>: The <code>fit()</code> method is called with training data. The internal behavior varies by component:</p>\n<ul>\n<li>SimpleLinearRegression computes parameters directly using the normal equation</li>\n<li>GradientDescentRegression iteratively optimizes parameters using the cost function gradient</li>\n<li>MultipleLinearRegression uses vectorized gradient descent with optional regularization</li>\n</ul>\n</li>\n<li><p><strong>Training Completion</strong>: The model sets its <code>is_fitted_</code> flag to True and stores learned parameters. Gradient descent models also preserve <code>cost_history_</code> and <code>parameter_history_</code> for analysis and debugging.</p>\n</li>\n</ol>\n<p><strong>Prediction Data Flow Sequence</strong></p>\n<p>Prediction follows a simpler, stateless pattern once models are trained:</p>\n<ol>\n<li><p><strong>Input Validation</strong>: New input data undergoes the same validation as training data to ensure compatibility with the fitted model&#39;s expectations.</p>\n</li>\n<li><p><strong>Feature Preprocessing</strong>: Input features are normalized using the same scaling parameters computed during training. This ensures consistency between training and prediction phases.</p>\n</li>\n<li><p><strong>Prediction Computation</strong>: The <code>predict()</code> method applies the learned parameters to compute output values:</p>\n<ul>\n<li>Simple regression: <code>y_pred = slope_ * x + intercept_</code></li>\n<li>Multiple regression: <code>y_pred = X @ weights_</code> (matrix-vector multiplication)</li>\n</ul>\n</li>\n<li><p><strong>Output Formatting</strong>: Predictions are returned as NumPy arrays with appropriate shapes and data types for downstream consumption.</p>\n</li>\n</ol>\n<p><strong>Component Communication Protocols</strong></p>\n<p>The components communicate through well-defined interfaces that abstract implementation details while providing rich error information:</p>\n<table>\n<thead>\n<tr>\n<th>Interface</th>\n<th>Data Format</th>\n<th>Error Handling</th>\n<th>State Dependencies</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>DataHandler → Models</td>\n<td><code>Tuple[np.ndarray, np.ndarray]</code></td>\n<td>Raises <code>ValueError</code> with descriptive messages</td>\n<td>Stateless operations</td>\n</tr>\n<tr>\n<td>Models.fit()</td>\n<td>Input arrays, returns self</td>\n<td>Raises <code>ValueError</code> for invalid data</td>\n<td>Changes model from unfitted to fitted</td>\n</tr>\n<tr>\n<td>Models.predict()</td>\n<td>Input array, returns predictions</td>\n<td>Raises <code>RuntimeError</code> if not fitted</td>\n<td>Requires fitted model state</td>\n</tr>\n<tr>\n<td>Models.score()</td>\n<td>Input arrays, returns float</td>\n<td>Combines fit checking and R-squared computation</td>\n<td>Requires fitted model state</td>\n</tr>\n</tbody></table>\n<p><strong>Inter-Component Dependencies</strong></p>\n<p>The components exhibit a layered dependency structure that supports the educational progression:</p>\n<ul>\n<li><strong>DataHandler</strong> has no dependencies on other components, making it the foundation layer</li>\n<li><strong>SimpleLinearRegression</strong> depends only on DataHandler for preprocessing, representing the first complete machine learning pipeline</li>\n<li><strong>GradientDescentOptimizer</strong> extends SimpleLinearRegression concepts but can be implemented independently, allowing comparison between approaches</li>\n<li><strong>MultipleLinearRegression</strong> builds on gradient descent principles and may reuse optimization code, representing the culmination of learned concepts</li>\n</ul>\n<blockquote>\n<p><strong>Design Insight: Loose Coupling with Rich Interfaces</strong></p>\n<p>The architecture prioritizes loose coupling between components while providing rich error information and debugging support. Each component can be tested independently, but when integrated, they provide detailed diagnostic information about failures. This design supports both learning (clear error messages aid understanding) and development (components can be built and tested incrementally).</p>\n</blockquote>\n<p><strong>State Management and Persistence</strong></p>\n<p>Component state management follows machine learning best practices while remaining simple enough for educational use:</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Persistent State</th>\n<th>Transient State</th>\n<th>State Transitions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>DataHandler</td>\n<td>None (stateless)</td>\n<td>Loaded data arrays</td>\n<td>Load → Validate → Transform</td>\n</tr>\n<tr>\n<td>SimpleLinearRegression</td>\n<td><code>slope_</code>, <code>intercept_</code>, <code>is_fitted_</code></td>\n<td>Computation intermediates</td>\n<td>Unfitted → Fitted</td>\n</tr>\n<tr>\n<td>GradientDescentRegression</td>\n<td>Parameters + <code>cost_history_</code></td>\n<td>Current gradients, iteration count</td>\n<td>Unfitted → Training → Fitted/Failed</td>\n</tr>\n<tr>\n<td>MultipleLinearRegression</td>\n<td>Weight vector + scaling parameters</td>\n<td>Matrix intermediates</td>\n<td>Unfitted → Training → Fitted/Failed</td>\n</tr>\n</tbody></table>\n<p><strong>Error Propagation and Recovery</strong></p>\n<p>The component interactions include comprehensive error handling that provides educational value:</p>\n<ul>\n<li><strong>Data Errors</strong>: Invalid file formats, missing columns, or incompatible data types are caught at the DataHandler level with specific error messages indicating the problem location and suggested fixes</li>\n<li><strong>Mathematical Errors</strong>: Division by zero, matrix singularity, or numerical overflow are detected during computation with error messages explaining the mathematical issue and potential solutions</li>\n<li><strong>Convergence Errors</strong>: Gradient descent failure (divergence or slow convergence) is detected and reported with diagnostic information about learning rates, cost function behavior, and suggested parameter adjustments</li>\n<li><strong>State Errors</strong>: Attempting to predict with unfitted models or refit already-fitted models results in clear error messages explaining the expected component lifecycle</li>\n</ul>\n<p><img src=\"/api/project/linear-regression/architecture-doc/asset?path=diagrams%2Fmilestone-progression.svg\" alt=\"Learning Milestone Progression\"></p>\n<p>This error handling strategy serves dual educational purposes: it prevents silent failures that would confuse learners, and it teaches proper error handling practices that are essential for production machine learning systems.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The implementation approach balances educational clarity with practical software development skills. This guidance provides the scaffolding needed to transform the architectural design into working code while preserving the learning objectives.</p>\n<p><strong>Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Implementation</th>\n<th>Advanced Implementation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Handling</td>\n<td>Pure NumPy arrays with CSV reading</td>\n<td>Pandas DataFrames with comprehensive data validation</td>\n</tr>\n<tr>\n<td>Mathematical Operations</td>\n<td>NumPy basic operations</td>\n<td>NumPy with BLAS optimization and numerical stability checks</td>\n</tr>\n<tr>\n<td>Visualization</td>\n<td>Matplotlib with basic plotting</td>\n<td>Seaborn with statistical plots and interactive widgets</td>\n</tr>\n<tr>\n<td>Testing</td>\n<td>Python unittest with synthetic data</td>\n<td>Pytest with property-based testing and real datasets</td>\n</tr>\n<tr>\n<td>Documentation</td>\n<td>Inline comments and docstrings</td>\n<td>Sphinx with mathematical notation and tutorials</td>\n</tr>\n</tbody></table>\n<p><strong>Core Infrastructure Starter Code</strong></p>\n<p>The following complete utilities provide the foundation for learner implementations:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/utils/plotting.py - Complete visualization utilities</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> matplotlib.pyplot </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> plt</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tuple, Optional, List</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> plot_regression_results</span><span style=\"color:#E1E4E8\">(x_train: np.ndarray, y_train: np.ndarray, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                           x_test: np.ndarray, y_test: np.ndarray,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                           predictions: np.ndarray, model_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"Regression\"</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Create comprehensive regression visualization with training data, test data, and predictions.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Essential for debugging model behavior and understanding fit quality.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fig, (ax1, ax2) </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> plt.subplots(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">figsize</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">12</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Left plot: Training data with fitted line</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ax1.scatter(x_train, y_train, </span><span style=\"color:#FFAB70\">alpha</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.6</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">color</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'blue'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">label</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'Training Data'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(x_test) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sorted_indices </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.argsort(x_test.flatten())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ax1.plot(x_test[sorted_indices], predictions[sorted_indices], </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                color</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'red'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">linewidth</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">label</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'Fitted Line'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ax1.set_xlabel(</span><span style=\"color:#9ECBFF\">'Feature Value'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ax1.set_ylabel(</span><span style=\"color:#9ECBFF\">'Target Value'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ax1.set_title(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">model_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> - Fitted Model'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ax1.legend()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ax1.grid(</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">alpha</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Right plot: Residuals analysis</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    residuals </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> y_test </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> predictions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ax2.scatter(predictions, residuals, </span><span style=\"color:#FFAB70\">alpha</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.6</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">color</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'green'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ax2.axhline(</span><span style=\"color:#FFAB70\">y</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">color</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'red'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">linestyle</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'--'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ax2.set_xlabel(</span><span style=\"color:#9ECBFF\">'Predicted Values'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ax2.set_ylabel(</span><span style=\"color:#9ECBFF\">'Residuals'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ax2.set_title(</span><span style=\"color:#9ECBFF\">'Residuals Plot (should be random)'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ax2.grid(</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">alpha</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.tight_layout()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.show()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> plot_cost_history</span><span style=\"color:#E1E4E8\">(cost_history: List[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">], title: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"Training Progress\"</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Visualize gradient descent convergence for debugging optimization issues.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.figure(</span><span style=\"color:#FFAB70\">figsize</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">6</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.plot(cost_history, </span><span style=\"color:#FFAB70\">linewidth</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.xlabel(</span><span style=\"color:#9ECBFF\">'Iteration'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.ylabel(</span><span style=\"color:#9ECBFF\">'Cost (Mean Squared Error)'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.title(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">title</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> - Cost Function Over Time'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.grid(</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">alpha</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.yscale(</span><span style=\"color:#9ECBFF\">'log'</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Log scale often reveals convergence patterns better</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.show()</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/utils/metrics.py - Complete evaluation utilities</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tuple</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> calculate_r_squared</span><span style=\"color:#E1E4E8\">(y_true: np.ndarray, y_pred: np.ndarray) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Calculate coefficient of determination (R-squared) with proper error handling.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    R-squared measures the proportion of variance in the target explained by the model.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(y_true) </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(y_pred):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Array length mismatch: y_true(</span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(y_true)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">) != y_pred(</span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(y_pred)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Total sum of squares (variance in actual values)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ss_tot </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.sum((y_true </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> np.mean(y_true)) </span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Residual sum of squares (unexplained variance)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ss_res </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.sum((y_true </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> y_pred) </span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Handle edge case where all actual values are identical</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> ss_tot </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> ss_res </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> else</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    r_squared </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> -</span><span style=\"color:#E1E4E8\"> (ss_res </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> ss_tot)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> float</span><span style=\"color:#E1E4E8\">(r_squared)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> calculate_mse</span><span style=\"color:#E1E4E8\">(y_true: np.ndarray, y_pred: np.ndarray) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Calculate Mean Squared Error with input validation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(y_true) </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(y_pred):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Array length mismatch: y_true(</span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(y_true)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">) != y_pred(</span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(y_pred)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mse </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.mean((y_true </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> y_pred) </span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> float</span><span style=\"color:#E1E4E8\">(mse)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> calculate_rmse</span><span style=\"color:#E1E4E8\">(y_true: np.ndarray, y_pred: np.ndarray) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Calculate Root Mean Squared Error (same units as target variable).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> np.sqrt(calculate_mse(y_true, y_pred))</span></span></code></pre></div>\n\n<p><strong>Component Implementation Skeletons</strong></p>\n<p>For core learning components, provide detailed skeletons with comprehensive TODOs:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/simple_regression/model.py - Core learning component skeleton</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SimpleLinearRegression</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Single-variable linear regression using closed-form ordinary least squares solution.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This implementation demonstrates the mathematical elegance of the normal equation:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    slope = Σ((x - x_mean) * (y - y_mean)) / Σ((x - x_mean)²)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    intercept = y_mean - slope * x_mean</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.slope_: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.intercept_: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.is_fitted_: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> fit</span><span style=\"color:#E1E4E8\">(self, x: np.ndarray, y: np.ndarray) -> </span><span style=\"color:#9ECBFF\">'SimpleLinearRegression'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Fit linear regression using the closed-form solution.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            x: Feature values, shape (n_samples,)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            y: Target values, shape (n_samples,)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            self: Fitted model instance</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate input arrays (check shapes, types, lengths match)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use np.asarray() to ensure NumPy arrays, check x.shape == y.shape</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check for edge cases that would cause mathematical errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: What happens if all x values are identical? (Zero denominator)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate means of x and y for centered computation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use np.mean() - this is x_bar and y_bar in the equations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Compute slope using the normal equation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: numerator = sum((x - x_mean) * (y - y_mean))</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #       denominator = sum((x - x_mean) ** 2)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #       slope = numerator / denominator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Compute intercept using the relationship intercept = y_mean - slope * x_mean</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: This ensures the line passes through the point (x_mean, y_mean)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Store computed parameters and mark model as fitted</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Set self.slope_, self.intercept_, and self.is_fitted_</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> predict</span><span style=\"color:#E1E4E8\">(self, x: np.ndarray) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Generate predictions using the fitted linear model.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            x: Feature values for prediction, shape (n_samples,)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Predicted target values, shape (n_samples,)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if model has been fitted</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Raise RuntimeError with clear message if not self.is_fitted_</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate input array format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Convert to NumPy array, check for reasonable shape and no NaN/inf values</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Apply linear equation y = mx + b</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use vectorized NumPy operations: self.slope_ * x + self.intercept_</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span><span style=\"color:#6A737D\">  # Remove this line when implementing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> score</span><span style=\"color:#E1E4E8\">(self, x: np.ndarray, y: np.ndarray) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Calculate R-squared coefficient of determination.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            x: Feature values, shape (n_samples,)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            y: True target values, shape (n_samples,)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            R-squared value between 0 and 1 (higher is better)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate predictions for the provided x values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use self.predict(x)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate R-squared using the coefficient of determination formula</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Import and use calculate_r_squared from utils.metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span><span style=\"color:#6A737D\">  # Remove this line when implementing</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint Verification</strong></p>\n<p>After implementing each milestone, learners should verify their progress with these concrete checkpoints:</p>\n<p><strong>Milestone 1 Checkpoint - Simple Linear Regression:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># examples/milestone_1_checkpoint.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> src.data_handler.synthetic_generator </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> generate_linear_data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> src.simple_regression.model </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> SimpleLinearRegression</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Generate test data with known parameters</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">x_test, y_test </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> generate_linear_data(</span><span style=\"color:#FFAB70\">n_samples</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">slope</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2.5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">intercept</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                                     noise_std</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">x_range</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Fit model and check parameter recovery</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SimpleLinearRegression()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">model.fit(x_test, y_test)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"True parameters: slope=2.5, intercept=1.0\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Fitted parameters: slope=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">model.slope_</span><span style=\"color:#F97583\">:.3f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, intercept=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">model.intercept_</span><span style=\"color:#F97583\">:.3f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Parameter error: slope_error=</span><span style=\"color:#79B8FF\">{abs</span><span style=\"color:#E1E4E8\">(model.slope_ </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 2.5</span><span style=\"color:#E1E4E8\">)</span><span style=\"color:#F97583\">:.3f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, intercept_error=</span><span style=\"color:#79B8FF\">{abs</span><span style=\"color:#E1E4E8\">(model.intercept_ </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">)</span><span style=\"color:#F97583\">:.3f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Both errors should be &#x3C; 0.2 for this low-noise dataset</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> abs</span><span style=\"color:#E1E4E8\">(model.slope_ </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 2.5</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 0.2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Slope estimation error too high\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> abs</span><span style=\"color:#E1E4E8\">(model.intercept_ </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 0.2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Intercept estimation error too high\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test R-squared calculation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">r_squared </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model.score(x_test, y_test)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"R-squared: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">r_squared</span><span style=\"color:#F97583\">:.3f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> r_squared </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0.95</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"R-squared should be very high for low-noise synthetic data\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"✅ Milestone 1 checkpoint passed!\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Language-Specific Implementation Hints</strong></p>\n<p><strong>NumPy Best Practices:</strong></p>\n<ul>\n<li>Use <code>np.asarray()</code> instead of requiring exact array types - this accepts lists, tuples, and other array-like inputs</li>\n<li>Check for <code>np.isfinite()</code> to detect NaN and infinite values that will break computations</li>\n<li>Use <code>np.allclose()</code> for floating-point comparisons instead of exact equality</li>\n<li>Leverage broadcasting: <code>x.reshape(-1, 1)</code> converts 1D arrays to column vectors for matrix operations</li>\n</ul>\n<p><strong>Common Python Pitfalls:</strong></p>\n<ul>\n<li>Division by zero: Always check denominators before division, especially <code>np.sum((x - x.mean()) ** 2)</code></li>\n<li>Array shape mismatches: Use <code>.flatten()</code> or <code>.ravel()</code> to ensure 1D arrays when needed</li>\n<li>Integer division: Use <code>float()</code> conversion or ensure arrays are float type to avoid integer truncation</li>\n<li>Mutable default arguments: Never use <code>def func(param=[]):</code> - use <code>def func(param=None):</code> instead</li>\n</ul>\n<p><strong>Performance Optimization:</strong></p>\n<ul>\n<li>Use vectorized NumPy operations instead of Python loops</li>\n<li>Pre-allocate arrays when size is known: <code>np.zeros(n_samples)</code> instead of appending to lists</li>\n<li>Use <code>np.dot()</code> or <code>@</code> operator for matrix multiplication instead of manual loops</li>\n<li>Consider memory layout: use <code>np.ascontiguousarray()</code> for better cache performance with large arrays</li>\n</ul>\n<p><strong>Debugging Strategy for Each Component:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Common Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnostic Steps</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>DataHandler</td>\n<td>&quot;ValueError: could not convert string to float&quot;</td>\n<td>Non-numeric data in CSV</td>\n<td>Print first few rows, check column types</td>\n<td>Clean data or specify correct columns</td>\n</tr>\n<tr>\n<td>SimpleLinearRegression</td>\n<td>Parameters are NaN or infinite</td>\n<td>Division by zero (constant x values)</td>\n<td>Check <code>np.var(x)</code> - should be &gt; 0</td>\n<td>Use different dataset or add noise</td>\n</tr>\n<tr>\n<td>GradientDescent</td>\n<td>Cost increases instead of decreasing</td>\n<td>Learning rate too high</td>\n<td>Plot cost history, try learning_rate/10</td>\n<td>Reduce learning rate by factors of 10</td>\n</tr>\n<tr>\n<td>MultipleRegression</td>\n<td>Very poor R-squared despite good single features</td>\n<td>Features on different scales</td>\n<td>Check feature means and standard deviations</td>\n<td>Apply feature normalization</td>\n</tr>\n</tbody></table>\n<p>This implementation guidance provides learners with the scaffolding needed to build working implementations while preserving the educational value of discovering algorithmic details through hands-on coding.</p>\n<h2 id=\"data-model-and-core-types\">Data Model and Core Types</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (M1: Simple Linear Regression, M2: Gradient Descent, M3: Multiple Linear Regression)</p>\n</blockquote>\n<p>This section establishes the foundational data structures that flow through our linear regression system. Think of these types as the vocabulary our components use to communicate with each other - they define not just what data we store, but how it transforms as it moves from raw input through training to final predictions.</p>\n<h3 id=\"mental-model-the-learning-journal\">Mental Model: The Learning Journal</h3>\n<p>Imagine our linear regression system as a student keeping a detailed learning journal. The <strong>Dataset</strong> is like the practice problems and exercises - raw information that needs to be organized and understood. The <strong>ModelParameters</strong> represent the student&#39;s current understanding - their &quot;best guess&quot; about how the world works based on what they&#39;ve learned so far. The <strong>TrainingHistory</strong> is the journal itself - a record of every attempt, every mistake, and every improvement made along the way. Finally, the <strong>PredictionResult</strong> is like the student&#39;s confident answer to a new question, backed by everything they&#39;ve learned.</p>\n<p>Just as a student&#39;s understanding evolves from confusion to clarity through practice, our data structures capture this transformation from raw numbers to trained intelligence.</p>\n<h3 id=\"core-data-types\">Core Data Types</h3>\n<p>Our system revolves around four fundamental data structures, each serving a specific role in the machine learning pipeline. These types are designed to be simple enough for educational purposes while robust enough to handle real regression tasks.</p>\n<p><img src=\"/api/project/linear-regression/architecture-doc/asset?path=diagrams%2Fdata-model.svg\" alt=\"Data Model and Type Relationships\"></p>\n<h4 id=\"dataset-structure\">Dataset Structure</h4>\n<p>The <code>Dataset</code> type encapsulates all the information needed to train and evaluate our regression models. It serves as the primary container for features, targets, and associated metadata.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>features</code></td>\n<td><code>np.ndarray</code></td>\n<td>Input feature matrix with shape (n_samples, n_features) where each row is an observation</td>\n</tr>\n<tr>\n<td><code>targets</code></td>\n<td><code>np.ndarray</code></td>\n<td>Target values vector with shape (n_samples,) containing the dependent variable values</td>\n</tr>\n<tr>\n<td><code>feature_names</code></td>\n<td><code>List[str]</code></td>\n<td>Human-readable names for each feature column, used for debugging and visualization</td>\n</tr>\n<tr>\n<td><code>n_samples</code></td>\n<td><code>int</code></td>\n<td>Number of training examples in the dataset, derived from features.shape[0]</td>\n</tr>\n<tr>\n<td><code>n_features</code></td>\n<td><code>int</code></td>\n<td>Number of input features per example, derived from features.shape[1]</td>\n</tr>\n<tr>\n<td><code>is_normalized</code></td>\n<td><code>bool</code></td>\n<td>Flag indicating whether features have been normalized using z-score standardization</td>\n</tr>\n<tr>\n<td><code>normalization_stats</code></td>\n<td><code>Dict[str, np.ndarray]</code></td>\n<td>Stored means and standard deviations for each feature, used to normalize new data</td>\n</tr>\n</tbody></table>\n<p>The <code>Dataset</code> type abstracts away the complexity of data management while ensuring consistency across all components. The normalization statistics are particularly important - they allow us to apply the same transformation to new prediction data that was applied during training, preventing distribution shift issues.</p>\n<blockquote>\n<p><strong>Design Insight:</strong> We store normalization statistics within the dataset rather than in the model because normalization is a property of the data, not the learning algorithm. This allows the same normalized dataset to be used with different models while maintaining consistency.</p>\n</blockquote>\n<h4 id=\"modelparameters-structure\">ModelParameters Structure</h4>\n<p>The <code>ModelParameters</code> type represents the learned weights that define our regression model. This structure evolves across milestones, starting simple and growing more sophisticated as we add features.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>slope_</code></td>\n<td><code>float</code></td>\n<td>Linear coefficient for single-variable regression (M1, M2)</td>\n</tr>\n<tr>\n<td><code>intercept_</code></td>\n<td><code>float</code></td>\n<td>Bias term representing the y-intercept of the regression line</td>\n</tr>\n<tr>\n<td><code>weights_</code></td>\n<td><code>np.ndarray</code></td>\n<td>Weight vector for multiple features (M3), with shape (n_features,)</td>\n</tr>\n<tr>\n<td><code>is_fitted_</code></td>\n<td><code>bool</code></td>\n<td>Flag indicating whether parameters have been estimated from training data</td>\n</tr>\n<tr>\n<td><code>fitting_method_</code></td>\n<td><code>str</code></td>\n<td>Method used for parameter estimation: &quot;closed_form&quot; or &quot;gradient_descent&quot;</td>\n</tr>\n<tr>\n<td><code>regularization_strength_</code></td>\n<td><code>float</code></td>\n<td>L2 regularization coefficient (lambda) for Ridge regression in M3</td>\n</tr>\n<tr>\n<td><code>feature_count_</code></td>\n<td><code>int</code></td>\n<td>Number of features the model was trained on, used for prediction validation</td>\n</tr>\n</tbody></table>\n<p>The underscore suffix on most fields follows the scikit-learn convention, indicating these are attributes set during the fitting process rather than user-provided parameters. This naming helps distinguish between configuration (no underscore) and learned parameters (underscore suffix).</p>\n<blockquote>\n<p><strong>Design Insight:</strong> We maintain both <code>slope_</code> and <code>weights_</code> fields even though they&#39;re mathematically equivalent for single-variable regression. This design choice prioritizes educational clarity over memory efficiency, allowing learners to see the conceptual progression from simple to multiple regression.</p>\n</blockquote>\n<h4 id=\"traininghistory-structure\">TrainingHistory Structure</h4>\n<p>The <code>TrainingHistory</code> type captures the entire optimization journey, providing insight into how gradient descent converges to the optimal solution. This is crucial for debugging and understanding the learning process.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>cost_history_</code></td>\n<td><code>List[float]</code></td>\n<td>Mean squared error at each iteration, should decrease monotonically</td>\n</tr>\n<tr>\n<td><code>parameter_history_</code></td>\n<td><code>List[Tuple[float, float]]</code></td>\n<td>Slope and intercept values at each iteration for M1/M2</td>\n</tr>\n<tr>\n<td><code>weight_history_</code></td>\n<td><code>List[np.ndarray]</code></td>\n<td>Weight vector snapshots for multiple regression (M3)</td>\n</tr>\n<tr>\n<td><code>gradient_norms_</code></td>\n<td><code>List[float]</code></td>\n<td>Magnitude of gradient vector at each iteration, indicates convergence</td>\n</tr>\n<tr>\n<td><code>learning_rate</code></td>\n<td><code>float</code></td>\n<td>Step size used during optimization, affects convergence speed and stability</td>\n</tr>\n<tr>\n<td><code>max_iterations</code></td>\n<td><code>int</code></td>\n<td>Maximum allowed training iterations, prevents infinite loops</td>\n</tr>\n<tr>\n<td><code>tolerance</code></td>\n<td><code>float</code></td>\n<td>Convergence threshold for cost improvement between iterations</td>\n</tr>\n<tr>\n<td><code>converged_</code></td>\n<td><code>bool</code></td>\n<td>Flag indicating whether optimization reached convergence criteria</td>\n</tr>\n<tr>\n<td><code>final_iteration_</code></td>\n<td><code>int</code></td>\n<td>Actual number of iterations performed before stopping</td>\n</tr>\n<tr>\n<td><code>convergence_reason_</code></td>\n<td><code>str</code></td>\n<td>Why training stopped: &quot;converged&quot;, &quot;max_iterations&quot;, or &quot;diverged&quot;</td>\n</tr>\n</tbody></table>\n<p>The training history serves multiple purposes beyond mere record-keeping. It enables sophisticated debugging techniques like plotting cost curves, detecting oscillation in parameter updates, and analyzing the relationship between learning rate and convergence behavior.</p>\n<blockquote>\n<p><strong>Critical Insight:</strong> The <code>gradient_norms_</code> field is often overlooked but extremely valuable. When the gradient magnitude approaches zero, we know we&#39;re near an optimum. Sudden spikes in gradient norm can indicate numerical instability or learning rate issues.</p>\n</blockquote>\n<h4 id=\"predictionresult-structure\">PredictionResult Structure</h4>\n<p>The <code>PredictionResult</code> type encapsulates not just the predicted values but also confidence metrics and diagnostic information that help users understand model performance.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>predictions</code></td>\n<td><code>np.ndarray</code></td>\n<td>Predicted target values with shape (n_samples,)</td>\n</tr>\n<tr>\n<td><code>residuals</code></td>\n<td><code>np.ndarray</code></td>\n<td>Prediction errors (actual - predicted) when true targets are available</td>\n</tr>\n<tr>\n<td><code>confidence_intervals</code></td>\n<td><code>np.ndarray</code></td>\n<td>95% confidence bounds for predictions (M3 extension)</td>\n</tr>\n<tr>\n<td><code>r_squared</code></td>\n<td><code>float</code></td>\n<td>Coefficient of determination when true targets are provided for evaluation</td>\n</tr>\n<tr>\n<td><code>mean_squared_error</code></td>\n<td><code>float</code></td>\n<td>Average squared prediction error when true targets are available</td>\n</tr>\n<tr>\n<td><code>mean_absolute_error</code></td>\n<td><code>float</code></td>\n<td>Average absolute prediction error for robust error assessment</td>\n</tr>\n<tr>\n<td><code>input_shape</code></td>\n<td><code>Tuple[int, int]</code></td>\n<td>Shape of input data used for predictions, for validation and debugging</td>\n</tr>\n<tr>\n<td><code>model_type</code></td>\n<td><code>str</code></td>\n<td>Type of model used: &quot;simple_linear&quot;, &quot;gradient_descent&quot;, or &quot;multiple_linear&quot;</td>\n</tr>\n</tbody></table>\n<p>The inclusion of multiple error metrics provides a comprehensive view of model performance. Mean squared error heavily penalizes large errors, while mean absolute error gives equal weight to all errors. This combination helps identify whether prediction errors are consistent or dominated by outliers.</p>\n<h3 id=\"architecture-decision-records\">Architecture Decision Records</h3>\n<blockquote>\n<p><strong>Decision: NumPy Arrays vs Pure Python Lists for Numerical Data</strong></p>\n<ul>\n<li><strong>Context</strong>: We need efficient storage and computation for potentially large datasets with thousands of samples and features. Python lists are simple but inefficient for numerical operations.</li>\n<li><strong>Options Considered</strong>: Pure Python lists, NumPy arrays, Pandas DataFrames</li>\n<li><strong>Decision</strong>: NumPy arrays for all numerical data (features, targets, predictions)</li>\n<li><strong>Rationale</strong>: NumPy provides vectorized operations essential for efficient matrix math, consistent with scientific Python ecosystem, and offers better memory layout for numerical computing. Type safety prevents common bugs like accidentally mixing scalars with arrays.</li>\n<li><strong>Consequences</strong>: Requires NumPy dependency, slight learning curve for array indexing, but enables performant vectorized operations and prepares learners for advanced ML libraries.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Separate History Tracking vs Embedded in Model</strong>  </p>\n<ul>\n<li><strong>Context</strong>: Gradient descent generates valuable diagnostic information during training that learners need for debugging and understanding convergence behavior.</li>\n<li><strong>Options Considered</strong>: Store history in model parameters, separate TrainingHistory class, or no history tracking</li>\n<li><strong>Decision</strong>: Separate TrainingHistory type that models can optionally populate</li>\n<li><strong>Rationale</strong>: Separation of concerns - model parameters represent learned knowledge while history represents the learning process. Allows models to be lightweight when history isn&#39;t needed, but provides rich debugging when it is.</li>\n<li><strong>Consequences</strong>: More complex data flow but cleaner architecture. Enables sophisticated debugging techniques and educational insights into optimization behavior.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Pure Python Lists</td>\n<td>Simple, no dependencies, familiar syntax</td>\n<td>Slow for large data, no vectorization, memory inefficient</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>NumPy Arrays</td>\n<td>Fast vectorized ops, memory efficient, scientific Python standard</td>\n<td>Learning curve, dependency, less familiar indexing</td>\n<td>✅</td>\n</tr>\n<tr>\n<td>Pandas DataFrames</td>\n<td>Rich data manipulation, handles mixed types, great for exploration</td>\n<td>Heavy dependency, overkill for numerical arrays, slower than NumPy</td>\n<td>❌</td>\n</tr>\n</tbody></table>\n<h3 id=\"type-relationships-and-lifecycle\">Type Relationships and Lifecycle</h3>\n<p>Understanding how these data types interact and transform throughout the machine learning pipeline is crucial for system design. The lifecycle follows a clear progression from raw data through trained models to actionable predictions.</p>\n<h4 id=\"data-transformation-pipeline\">Data Transformation Pipeline</h4>\n<p>The journey begins with raw numerical data and culminates in trained models capable of making predictions. Each stage involves specific transformations that prepare the data for the next phase.</p>\n<p><strong>Stage 1: Data Ingestion and Validation</strong>\nRaw CSV files or array data enters the system and gets transformed into the structured <code>Dataset</code> format. This involves parsing numerical values, handling missing data, validating dimensions, and organizing features and targets into consistent arrays. The validation process ensures that feature and target arrays have compatible shapes and contain valid numerical values.</p>\n<p><strong>Stage 2: Preprocessing and Normalization</strong><br>The <code>Dataset</code> undergoes feature scaling to ensure all input variables have comparable ranges. Z-score normalization transforms each feature to have zero mean and unit variance, preventing features with large scales from dominating the learning process. The normalization statistics get stored in the dataset for later use with new prediction data.</p>\n<p><strong>Stage 3: Model Parameter Initialization</strong>\nA fresh <code>ModelParameters</code> structure is created with initial values. For closed-form solutions, parameters remain uninitialized until the normal equation provides the optimal values. For gradient descent, parameters start with small random values or zeros, and the <code>is_fitted_</code> flag remains False.</p>\n<p><strong>Stage 4: Training and Optimization</strong>\nDuring training, the model iteratively updates parameters while populating the <code>TrainingHistory</code> with diagnostic information. Each iteration generates new parameter values, cost measurements, and gradient calculations. The history tracks this evolution, providing insight into convergence behavior.</p>\n<p><strong>Stage 5: Convergence and Finalization</strong><br>Training terminates when convergence criteria are met or maximum iterations are reached. Final parameters are stored in the <code>ModelParameters</code> structure with <code>is_fitted_</code> set to True. The training history captures the convergence reason and final optimization state.</p>\n<p><strong>Stage 6: Prediction and Evaluation</strong>\nNew input data flows through the trained model to generate <code>PredictionResult</code> objects. If true target values are available, the result includes evaluation metrics. The prediction process validates input dimensions against the trained model&#39;s expected feature count.</p>\n<h4 id=\"state-transitions-and-invariants\">State Transitions and Invariants</h4>\n<p>Several important invariants must be maintained throughout the data lifecycle to ensure system correctness and prevent subtle bugs.</p>\n<p><strong>Dataset Invariants:</strong></p>\n<ul>\n<li><code>features.shape[0]</code> must always equal <code>targets.shape[0]</code> (same number of samples)</li>\n<li><code>n_samples</code> and <code>n_features</code> must match the actual array dimensions</li>\n<li>When <code>is_normalized</code> is True, <code>normalization_stats</code> must contain valid mean/std for each feature</li>\n<li>Feature names list length must match <code>n_features</code> when provided</li>\n</ul>\n<p><strong>ModelParameters Invariants:</strong></p>\n<ul>\n<li><code>is_fitted_</code> can only be True if either <code>slope_/intercept_</code> or <code>weights_</code> contain valid learned values</li>\n<li><code>feature_count_</code> must match the number of features the model was trained on</li>\n<li>For single regression: <code>weights_</code> is None, <code>slope_/intercept_</code> are valid floats</li>\n<li>For multiple regression: <code>weights_.shape[0]</code> equals <code>feature_count_</code>, <code>slope_</code> may be unused</li>\n</ul>\n<p><strong>TrainingHistory Invariants:</strong></p>\n<ul>\n<li>All history lists must have the same length (each iteration adds one entry to each list)  </li>\n<li><code>cost_history_</code> values should generally decrease (monotonic convergence)</li>\n<li><code>final_iteration_</code> must be less than or equal to <code>max_iterations</code></li>\n<li><code>converged_</code> can only be True if cost improvement fell below <code>tolerance</code></li>\n</ul>\n<p><strong>PredictionResult Invariants:</strong></p>\n<ul>\n<li><code>predictions.shape[0]</code> must equal the number of input samples</li>\n<li>When true targets are provided, <code>residuals</code> must equal <code>targets - predictions</code>  </li>\n<li>Error metrics are only valid when computed from true target values</li>\n<li><code>input_shape</code> must match the dimensions of the data used for prediction</li>\n</ul>\n<h4 id=\"data-flow-protocols\">Data Flow Protocols</h4>\n<p>The interaction between data types follows specific protocols that ensure consistency and prevent common integration errors.</p>\n<p><strong>Training Protocol:</strong></p>\n<ol>\n<li><code>Dataset</code> provides features and targets to the model</li>\n<li>Model initializes fresh <code>ModelParameters</code> and <code>TrainingHistory</code>  </li>\n<li>For each iteration, model updates parameters and appends to history</li>\n<li>Model sets convergence flags and final parameter values</li>\n<li>Trained model can generate predictions using learned parameters</li>\n</ol>\n<p><strong>Prediction Protocol:</strong></p>\n<ol>\n<li>Input features are validated against trained model&#39;s <code>feature_count_</code></li>\n<li>If dataset was normalized, prediction data undergoes same normalization using stored statistics</li>\n<li>Model applies learned parameters to generate predictions</li>\n<li>Results are packaged into <code>PredictionResult</code> with appropriate metadata</li>\n<li>If true targets are provided, evaluation metrics are computed and included</li>\n</ol>\n<p><strong>Error Recovery Protocol:</strong>\nWhen invariants are violated or data inconsistencies are detected, the system follows a fail-fast approach. Rather than attempting to fix corrupted data, components raise descriptive exceptions that help learners identify and correct the underlying issues.</p>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>Understanding these data types seems straightforward, but several subtle issues frequently trip up learners working with machine learning systems.</p>\n<p>⚠️ <strong>Pitfall: Array Shape Mismatches</strong>\nLearners often create feature arrays with incorrect dimensions, such as 1D arrays with shape <code>(n_samples,)</code> instead of 2D arrays with shape <code>(n_samples, 1)</code> for single-feature regression. This causes matrix multiplication errors in multiple regression components. Always ensure features are 2D arrays, even for single variables, using <code>reshape(-1, 1)</code> when necessary.</p>\n<p>⚠️ <strong>Pitfall: Forgetting Normalization Statistics</strong><br>When normalizing training data, learners frequently forget to store the mean and standard deviation values needed to normalize prediction data. This causes distribution shift where the model sees differently scaled data at prediction time compared to training time. Always save normalization statistics in the <code>Dataset</code> structure and apply the same transformation to new data.</p>\n<p>⚠️ <strong>Pitfall: Parameter History Memory Growth</strong>\nFor long training runs, storing parameter values at every iteration can consume significant memory. Learners sometimes create memory pressure by storing large weight vectors for thousands of iterations. Consider storing history every N iterations for long training runs, or implement a circular buffer for recent history.</p>\n<p>⚠️ <strong>Pitfall: Mixing Data Types</strong><br>Python&#39;s dynamic typing allows mixing integers, floats, and NumPy arrays in ways that cause subtle bugs. For example, accidentally using a Python list where a NumPy array is expected can cause shape and broadcasting errors. Always validate data types when crossing component boundaries.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Fitted State</strong>\nLearners sometimes forget to update the <code>is_fitted_</code> flag after training, or fail to reset it when retraining with new data. This can cause the model to make predictions with stale parameters or reject valid prediction requests. Always maintain consistent fitted state across training operations.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides concrete implementation recommendations for the data types, bridging the design concepts with working code.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Recommended For</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Array Operations</td>\n<td>NumPy basic indexing</td>\n<td>NumPy advanced indexing + broadcasting</td>\n<td>All milestones</td>\n</tr>\n<tr>\n<td>Data Validation</td>\n<td>Manual type/shape checks</td>\n<td>Custom validation decorators</td>\n<td>M1: Manual, M2+: Decorators</td>\n</tr>\n<tr>\n<td>Serialization</td>\n<td>JSON + lists</td>\n<td>Pickle + NumPy arrays</td>\n<td>Development: JSON, Production: Pickle</td>\n</tr>\n<tr>\n<td>Error Handling</td>\n<td>Basic assertions</td>\n<td>Custom exception hierarchy</td>\n<td>M1: Assertions, M2+: Custom exceptions</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<p>The data types should be organized to reflect their role as foundational system components:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>linear_regression/\n  data_types/\n    __init__.py              ← exports all public types\n    dataset.py               ← Dataset class and utilities  \n    model_parameters.py      ← ModelParameters class\n    training_history.py      ← TrainingHistory class\n    prediction_result.py     ← PredictionResult class\n    validation.py            ← data validation utilities\n  tests/\n    test_data_types.py       ← comprehensive type tests\n    test_validation.py       ← validation logic tests</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p>Complete implementation of data validation utilities that learners can use immediately:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># validation.py - Complete validation utilities</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, List, Tuple, Any</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_array_dimensions</span><span style=\"color:#E1E4E8\">(features: np.ndarray, targets: np.ndarray) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validate that feature and target arrays have compatible dimensions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(features, np.ndarray) </span><span style=\"color:#F97583\">or</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(targets, np.ndarray):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> TypeError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Features and targets must be NumPy arrays\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> features.ndim </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Features must be 2D array, got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">features.ndim</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">D\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> targets.ndim </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Targets must be 1D array, got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">targets.ndim</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">D\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> features.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> targets.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Feature samples (</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">features.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">) != target samples (</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">targets.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> features.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Cannot work with empty dataset\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_numerical_data</span><span style=\"color:#E1E4E8\">(data: np.ndarray, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validate that array contains only finite numerical values.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> np.isfinite(data).all():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        nan_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.isnan(data).sum()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        inf_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.isinf(data).sum()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> contains </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">nan_count</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> NaN and </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">inf_count</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> infinite values\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ValidationError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Custom exception for data validation failures.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p>Dataset class skeleton with detailed implementation guidance:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># dataset.py - Core Dataset implementation skeleton</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Optional, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .validation </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> validate_array_dimensions, validate_numerical_data</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Dataset</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Container for regression training and testing data.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, features: np.ndarray, targets: np.ndarray, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 feature_names: Optional[List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize dataset with feature and target arrays.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            features: 2D array of shape (n_samples, n_features)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            targets: 1D array of shape (n_samples,)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            feature_names: Optional names for each feature column</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate input arrays using validation utilities</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Store features and targets as instance variables</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compute and store n_samples and n_features from array shapes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Initialize feature_names (generate default names if None provided)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Initialize normalization flags and statistics to default values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Validate numerical data contains no NaN or infinite values</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> normalize_features</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#9ECBFF\">'Dataset'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Apply z-score normalization to features.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if already normalized, raise error if so</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Compute mean and standard deviation for each feature column</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle zero standard deviation (constant features) appropriately  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Apply z-score transformation: (x - mean) / std</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Store normalization statistics for later use with new data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Set is_normalized flag to True</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return self for method chaining</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> apply_normalization</span><span style=\"color:#E1E4E8\">(self, features: np.ndarray) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Apply stored normalization to new feature data.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check that dataset has been normalized (has stored statistics)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate that input features have correct number of columns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Apply same mean/std transformation used on training data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle edge case where stored std is zero (constant features)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return normalized feature array</span></span></code></pre></div>\n\n<p>ModelParameters class with milestone-specific evolution:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># model_parameters.py - Model parameter storage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ModelParameters</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Storage for learned regression parameters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize empty parameter structure.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize single regression parameters (slope_, intercept_) </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize multiple regression parameters (weights_)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Set is_fitted_ flag to False</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Initialize metadata fields (fitting_method_, feature_count_, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Set regularization_strength_ to default value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> set_single_regression_params</span><span style=\"color:#E1E4E8\">(self, slope: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, intercept: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                   method: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"closed_form\"</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Set parameters for single-variable regression (M1, M2).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate that slope and intercept are finite numbers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Store slope_ and intercept_ values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Set feature_count_ to 1 for single regression</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Record fitting_method_ used</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Set is_fitted_ flag to True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> set_multiple_regression_params</span><span style=\"color:#E1E4E8\">(self, weights: np.ndarray, method: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"gradient_descent\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                     regularization: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Set parameters for multiple regression (M3).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate weights array is 1D and contains finite values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Store weights_ array and extract intercept if included</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Set feature_count_ based on weights array length</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Record fitting method and regularization strength</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Set is_fitted_ flag to True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_for_prediction</span><span style=\"color:#E1E4E8\">(self, n_features: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate parameters are ready for prediction with given feature count.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check that model has been fitted (is_fitted_ is True)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate that input feature count matches trained feature count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Ensure appropriate parameters are set (slope/intercept or weights)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Raise descriptive errors for any validation failures</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Milestone 1 Checkpoint:</strong>\nAfter implementing the basic data types, learners should be able to:</p>\n<ul>\n<li>Create a <code>Dataset</code> from NumPy arrays with automatic dimension validation</li>\n<li>Initialize <code>ModelParameters</code> and set single regression coefficients  </li>\n<li>Generate synthetic data and verify dataset properties match expectations</li>\n<li>Run: <code>python -m pytest tests/test_data_types.py::test_dataset_creation</code></li>\n<li>Expected: All dataset creation tests pass, including edge cases for empty data and dimension mismatches</li>\n</ul>\n<p><strong>Milestone 2 Checkpoint:</strong><br>With training history support added:</p>\n<ul>\n<li>Create <code>TrainingHistory</code> and populate it during mock gradient descent iterations</li>\n<li>Verify cost history shows decreasing trend for well-behaved optimization</li>\n<li>Test convergence detection logic with various termination scenarios</li>\n<li>Run: <code>python -c &quot;from data_types import TrainingHistory; h = TrainingHistory(); print(&#39;History initialized&#39;)&quot;</code></li>\n<li>Expected: No import errors, history object created successfully</li>\n</ul>\n<p><strong>Milestone 3 Checkpoint:</strong>\nWith multiple regression support:</p>\n<ul>\n<li>Create datasets with multiple features and verify normalization works correctly</li>\n<li>Set multiple regression parameters and validate prediction readiness  </li>\n<li>Test prediction result generation with confidence intervals</li>\n<li>Run: Multi-feature synthetic dataset creation and parameter fitting test</li>\n<li>Expected: All dimension validations pass, normalization preserves relationships</li>\n</ul>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>&quot;shapes not aligned&quot; error</td>\n<td>Feature array wrong dimensions</td>\n<td>Check <code>features.shape</code>, should be (n_samples, n_features)</td>\n<td>Use <code>reshape(-1, 1)</code> for single features</td>\n</tr>\n<tr>\n<td>Predictions wildly incorrect</td>\n<td>Used unnormalized test data with normalized model</td>\n<td>Check <code>dataset.is_normalized</code> flag</td>\n<td>Apply same normalization to prediction data</td>\n</tr>\n<tr>\n<td>Memory usage grows during training</td>\n<td>Storing full parameter history</td>\n<td>Monitor <code>parameter_history_</code> size</td>\n<td>Store history every N iterations, not every iteration</td>\n</tr>\n<tr>\n<td>&quot;model not fitted&quot; error</td>\n<td>Forgot to set <code>is_fitted_</code> flag</td>\n<td>Check <code>model.is_fitted_</code> after training</td>\n<td>Set flag to True after successful parameter estimation</td>\n</tr>\n<tr>\n<td>NaN in results</td>\n<td>Division by zero in normalization</td>\n<td>Check for constant features (std = 0)</td>\n<td>Add small epsilon to standard deviation or remove constant features</td>\n</tr>\n</tbody></table>\n<h2 id=\"data-handler-component\">Data Handler Component</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (M1: Simple Linear Regression - provides CSV loading and basic validation, M2: Gradient Descent - adds feature normalization, M3: Multiple Linear Regression - extends to multi-feature datasets and matrix construction)</p>\n</blockquote>\n<h3 id=\"mental-model-the-data-preparation-kitchen\">Mental Model: The Data Preparation Kitchen</h3>\n<p>Think of the Data Handler as the prep kitchen in a high-end restaurant. Before any cooking begins, skilled prep cooks transform raw ingredients into perfectly measured, cleaned, and organized components that the chefs can use immediately. Raw vegetables arrive dirty and irregularly sized, but they leave the prep kitchen washed, chopped to uniform pieces, and organized in labeled containers.</p>\n<p>Similarly, your machine learning data arrives messy and inconsistent. Features might be measured in completely different scales (age in years, income in dollars, temperature in Celsius), contain missing values, or be stored in awkward formats. The Data Handler is your prep kitchen - it takes this raw, chaotic data and transforms it into clean, standardized, numerically stable inputs that your regression algorithms can consume without choking.</p>\n<p>Just as a prep cook knows that onions must be diced uniformly so they cook evenly, the Data Handler knows that features must be scaled uniformly so gradient descent converges reliably. And just as the prep kitchen validates that ingredients are fresh and safe before using them, the Data Handler validates that your data is numerically sound and properly formatted before feeding it to your model.</p>\n<p>The critical insight is that <strong>data preprocessing is not optional cleanup work - it&#39;s the foundation that determines whether your machine learning algorithm will succeed or fail</strong>. A model trained on poorly prepared data is like a meal cooked with rotten ingredients: no amount of culinary skill can save it.</p>\n<p><img src=\"/api/project/linear-regression/architecture-doc/asset?path=diagrams%2Fdata-preprocessing-flow.svg\" alt=\"Data Preprocessing Pipeline\"></p>\n<h3 id=\"responsibilities-and-interface\">Responsibilities and Interface</h3>\n<p>The Data Handler Component serves as the gateway between raw data sources and the machine learning pipeline. It owns three fundamental responsibilities: <strong>data ingestion</strong>, <strong>data validation</strong>, and <strong>data transformation</strong>. Each responsibility represents a critical checkpoint that prevents downstream failures and ensures numerical stability throughout the training process.</p>\n<h4 id=\"data-ingestion-responsibilities\">Data Ingestion Responsibilities</h4>\n<p>The data ingestion layer handles the mechanical aspects of loading data from various sources. For this educational implementation, we focus primarily on CSV file loading, but the design allows for extension to other formats. The ingestion process must be robust enough to handle real-world messiness: inconsistent column ordering, varying data types, missing headers, and encoding issues.</p>\n<p>The <code>load_csv_data</code> function serves as the primary entry point for data ingestion. It must intelligently handle column selection, automatically infer data types where possible, and provide clear error messages when data cannot be loaded. The function signature emphasizes explicit feature and target column specification to avoid ambiguity about what the model should learn.</p>\n<h4 id=\"data-validation-responsibilities\">Data Validation Responsibilities</h4>\n<p>Data validation represents the quality control checkpoint where we detect problems before they cause cryptic failures during training. The validation layer checks for numerical soundness, dimensional compatibility, and statistical properties that could cause optimization difficulties.</p>\n<p>The <code>validate_data</code> function performs comprehensive checks on both features and targets. It verifies that arrays have compatible shapes, contain only finite numerical values, and possess sufficient variation for meaningful model fitting. This validation prevents common failure modes like attempting to fit a line to constant data or passing NaN values to gradient descent algorithms.</p>\n<h4 id=\"data-transformation-responsibilities\">Data Transformation Responsibilities</h4>\n<p>The transformation layer prepares validated data for optimal algorithm performance. This includes feature scaling, matrix construction for multiple regression, and maintaining the metadata necessary to apply identical transformations to new prediction data.</p>\n<p>The <code>normalize_features</code> function implements z-score normalization, transforming each feature to zero mean and unit variance. This transformation is crucial for gradient descent convergence when features have vastly different scales. The function must store normalization statistics to ensure prediction data receives identical scaling.</p>\n<h4 id=\"component-interface-specification\">Component Interface Specification</h4>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>load_csv_data</code></td>\n<td><code>filename: str, feature_columns: List[str], target_column: str</code></td>\n<td><code>Tuple[np.ndarray, np.ndarray]</code></td>\n<td>Load and parse CSV data into feature and target arrays</td>\n</tr>\n<tr>\n<td><code>validate_data</code></td>\n<td><code>features: np.ndarray, targets: np.ndarray</code></td>\n<td><code>None</code></td>\n<td>Validate data compatibility and raise exceptions for invalid data</td>\n</tr>\n<tr>\n<td><code>normalize_features</code></td>\n<td><code>features: np.ndarray</code></td>\n<td><code>np.ndarray</code></td>\n<td>Apply z-score normalization and return scaled features</td>\n</tr>\n<tr>\n<td><code>generate_linear_data</code></td>\n<td><code>n_samples: int, slope: float, intercept: float, noise_std: float, x_range: Tuple[float, float]</code></td>\n<td><code>Tuple[np.ndarray, np.ndarray]</code></td>\n<td>Create synthetic linear dataset for testing</td>\n</tr>\n<tr>\n<td><code>validate_array_dimensions</code></td>\n<td><code>features: np.ndarray, targets: np.ndarray</code></td>\n<td><code>None</code></td>\n<td>Check array shape compatibility for regression</td>\n</tr>\n<tr>\n<td><code>validate_numerical_data</code></td>\n<td><code>data: np.ndarray, name: str</code></td>\n<td><code>None</code></td>\n<td>Check for NaN, infinite, or non-numeric values</td>\n</tr>\n</tbody></table>\n<p>The interface design emphasizes <strong>explicit parameter specification</strong> over implicit behavior. Rather than guessing which columns contain features, the caller must explicitly specify feature and target columns. This design choice prevents silent errors where the wrong data gets used for training.</p>\n<p>Each validation function follows the <strong>fail-fast principle</strong>: they either succeed silently or raise descriptive exceptions immediately. This approach helps learners understand exactly what went wrong and where, rather than encountering mysterious failures deep in the training loop.</p>\n<h3 id=\"data-validation-and-normalization\">Data Validation and Normalization</h3>\n<h4 id=\"comprehensive-data-validation-strategy\">Comprehensive Data Validation Strategy</h4>\n<p>Data validation forms the first line of defense against numerical instabilities and training failures. The validation process operates in multiple phases, each designed to catch different categories of problems before they can cause downstream issues.</p>\n<p><strong>Phase 1: Structure Validation</strong> checks that the basic array structures are compatible with linear regression requirements. This includes verifying that feature arrays are two-dimensional (samples × features), target arrays are one-dimensional, and both arrays have the same number of samples. Structure validation catches shape mismatches that would cause matrix multiplication failures during training.</p>\n<p><strong>Phase 2: Numerical Validation</strong> examines the actual numerical content of the arrays. It detects NaN values, infinite values, and non-numeric content that could break optimization algorithms. The validator also checks for degenerate cases like constant features (zero variance) that would cause division-by-zero errors in normalization.</p>\n<p><strong>Phase 3: Statistical Validation</strong> analyzes the statistical properties of the data to identify conditions that could cause optimization difficulties. This includes checking for extremely small variances that could cause numerical precision issues, highly correlated features that might cause instability, and target value ranges that could cause gradient explosion.</p>\n<table>\n<thead>\n<tr>\n<th>Validation Check</th>\n<th>Detection Method</th>\n<th>Failure Condition</th>\n<th>Error Message</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Array Shapes</td>\n<td><code>features.shape[0] == targets.shape[0]</code></td>\n<td>Sample count mismatch</td>\n<td>&quot;Feature array has {n} samples but target array has {m} samples&quot;</td>\n</tr>\n<tr>\n<td>Dimensionality</td>\n<td><code>features.ndim == 2, targets.ndim == 1</code></td>\n<td>Wrong array dimensions</td>\n<td>&quot;Features must be 2D array (samples, features), targets must be 1D&quot;</td>\n</tr>\n<tr>\n<td>Numeric Values</td>\n<td><code>np.isfinite(data).all()</code></td>\n<td>NaN or infinite values</td>\n<td>&quot;Found {count} non-finite values in {array_name}&quot;</td>\n</tr>\n<tr>\n<td>Feature Variance</td>\n<td><code>np.var(features, axis=0) &gt; 1e-10</code></td>\n<td>Constant features</td>\n<td>&quot;Feature &#39;{name}&#39; has zero variance (constant values)&quot;</td>\n</tr>\n<tr>\n<td>Sample Count</td>\n<td><code>len(targets) &gt;= 2</code></td>\n<td>Insufficient data</td>\n<td>&quot;Need at least 2 samples for regression, got {n}&quot;</td>\n</tr>\n<tr>\n<td>Target Range</td>\n<td><code>np.ptp(targets) &gt; 1e-10</code></td>\n<td>Constant targets</td>\n<td>&quot;Target values are constant, cannot fit regression model&quot;</td>\n</tr>\n</tbody></table>\n<h4 id=\"z-score-normalization-implementation\">Z-Score Normalization Implementation</h4>\n<p>Feature normalization addresses one of the most common causes of gradient descent failure: features with vastly different scales. When one feature ranges from 0-1 (like a percentage) and another ranges from 0-100000 (like income), the optimization landscape becomes elongated and difficult to navigate. Gradient descent may oscillate wildly or converge extremely slowly.</p>\n<p>Z-score normalization transforms each feature to have zero mean and unit variance using the formula: <code>(x - μ) / σ</code>. This transformation preserves the relative relationships between data points while ensuring all features contribute equally to the optimization process.</p>\n<p>The normalization process must be <strong>stateful</strong> - it stores the mean and standard deviation computed from training data and applies these exact same statistics to new prediction data. This ensures that prediction inputs undergo identical transformations to training inputs.</p>\n<table>\n<thead>\n<tr>\n<th>Normalization Step</th>\n<th>Formula</th>\n<th>Purpose</th>\n<th>Implementation Note</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Compute Mean</td>\n<td><code>μ = np.mean(features, axis=0)</code></td>\n<td>Center distribution at zero</td>\n<td>Computed per feature column</td>\n</tr>\n<tr>\n<td>Compute Std Dev</td>\n<td><code>σ = np.std(features, axis=0)</code></td>\n<td>Scale to unit variance</td>\n<td>Add epsilon to prevent division by zero</td>\n</tr>\n<tr>\n<td>Apply Transform</td>\n<td><code>(features - μ) / σ</code></td>\n<td>Normalize each feature</td>\n<td>Broadcast operations for efficiency</td>\n</tr>\n<tr>\n<td>Store Statistics</td>\n<td>Save μ, σ in Dataset</td>\n<td>Enable prediction normalization</td>\n<td>Must persist with trained model</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Critical Design Insight</strong>: Normalization statistics must be computed from training data only, never from the combination of training and test data. Computing statistics from test data would constitute <strong>data leakage</strong> - using future information to influence model training.</p>\n</blockquote>\n<h4 id=\"missing-value-handling-strategy\">Missing Value Handling Strategy</h4>\n<p>While this educational implementation focuses on complete datasets, real-world data often contains missing values. The Data Handler includes basic missing value detection and provides clear guidance on handling strategies.</p>\n<p><strong>Detection Phase</strong>: The system scans for common missing value indicators including NaN values, None entries, empty strings, and placeholder values like -999 or &quot;NULL&quot;. Detection operates before any numerical processing to prevent missing indicators from being interpreted as valid data.</p>\n<p><strong>Handling Strategies</strong>: For educational purposes, the system requires complete data and raises informative errors when missing values are detected. The error messages guide learners toward appropriate handling strategies: removing incomplete samples (listwise deletion), filling with statistical measures (mean imputation), or using more sophisticated techniques like multiple imputation.</p>\n<h3 id=\"architecture-decision-records\">Architecture Decision Records</h3>\n<h4 id=\"decision-numpy-vs-pure-python-for-numerical-operations\">Decision: NumPy vs Pure Python for Numerical Operations</h4>\n<p><strong>Context</strong>: The implementation must handle numerical computations for data loading, validation, and transformation. Pure Python lists and loops would be simpler for beginners to understand, while NumPy arrays provide vectorized operations and better numerical stability.</p>\n<p><strong>Options Considered</strong>:</p>\n<ol>\n<li><strong>Pure Python</strong>: Use standard Python lists, loops, and basic math operations</li>\n<li><strong>NumPy Arrays</strong>: Use NumPy for all numerical data and operations</li>\n<li><strong>Hybrid Approach</strong>: Use Python lists for small operations, NumPy for large computations</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Pure Python</td>\n<td>Familiar syntax, no dependencies, easier debugging</td>\n<td>Slow performance, numerical instability, verbose matrix operations</td>\n</tr>\n<tr>\n<td>NumPy Arrays</td>\n<td>Vectorized performance, numerical stability, broadcasting</td>\n<td>Additional dependency, learning curve for beginners</td>\n</tr>\n<tr>\n<td>Hybrid Approach</td>\n<td>Gradual complexity increase</td>\n<td>Inconsistent interfaces, type conversion overhead</td>\n</tr>\n</tbody></table>\n<p><strong>Decision</strong>: Use NumPy arrays for all numerical operations throughout the Data Handler.</p>\n<p><strong>Rationale</strong>: While NumPy has a learning curve, the benefits far outweigh the costs for a machine learning implementation. Numerical stability is crucial for regression algorithms - Python&#39;s floating-point arithmetic can accumulate errors that cause training failures. NumPy&#39;s vectorized operations also teach learners the importance of efficient computation in ML. The educational value of learning NumPy operations outweighs the initial complexity.</p>\n<p><strong>Consequences</strong>: Learners must understand basic NumPy operations (array creation, indexing, broadcasting) before implementing the Data Handler. However, this knowledge directly transfers to other ML libraries and production systems. The implementation is more robust and performs better than pure Python alternatives.</p>\n<h4 id=\"decision-normalization-strategy-selection\">Decision: Normalization Strategy Selection</h4>\n<p><strong>Context</strong>: Multiple feature scaling approaches exist, each with different properties and use cases. The choice affects gradient descent convergence and model interpretability.</p>\n<p><strong>Options Considered</strong>:</p>\n<ol>\n<li><strong>Z-Score Normalization</strong>: Transform to zero mean, unit variance</li>\n<li><strong>Min-Max Scaling</strong>: Transform to fixed range like [0,1]  </li>\n<li><strong>Robust Scaling</strong>: Use median and IQR instead of mean and standard deviation</li>\n<li><strong>No Scaling</strong>: Leave features in original scales</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Z-Score</td>\n<td>Handles outliers reasonably, standard approach</td>\n<td>Assumes roughly normal distribution</td>\n</tr>\n<tr>\n<td>Min-Max</td>\n<td>Bounded output range, preserves zero values</td>\n<td>Sensitive to outliers, can compress most data</td>\n</tr>\n<tr>\n<td>Robust Scaling</td>\n<td>Very resilient to outliers</td>\n<td>Less standard, may not fully address scale differences</td>\n</tr>\n<tr>\n<td>No Scaling</td>\n<td>Preserves original meaning</td>\n<td>Causes gradient descent convergence problems</td>\n</tr>\n</tbody></table>\n<p><strong>Decision</strong>: Implement z-score normalization as the primary scaling method.</p>\n<p><strong>Rationale</strong>: Z-score normalization provides the best balance of numerical stability, theoretical soundness, and educational value. It directly addresses the gradient descent convergence issues that learners will encounter, and the mathematical concept (standardization) appears throughout statistics and machine learning. The transformation is reversible and preserves the relative relationships between data points.</p>\n<p><strong>Consequences</strong>: The implementation must handle edge cases like zero-variance features. Learners will understand why feature scaling matters and gain experience with a fundamental preprocessing technique. The approach works well for most datasets learners will encounter.</p>\n<h4 id=\"decision-error-handling-philosophy\">Decision: Error Handling Philosophy</h4>\n<p><strong>Context</strong>: Data validation can fail in numerous ways, and the system must decide how to communicate failures to learners effectively.</p>\n<p><strong>Options Considered</strong>:</p>\n<ol>\n<li><strong>Silent Fixing</strong>: Automatically handle problems without notification</li>\n<li><strong>Warning-Based</strong>: Issue warnings but continue processing</li>\n<li><strong>Strict Validation</strong>: Raise exceptions for any data quality issues</li>\n<li><strong>Configurable Strictness</strong>: Allow users to choose validation level</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Silent Fixing</td>\n<td>Convenient, never blocks training</td>\n<td>Hides important data quality issues</td>\n</tr>\n<tr>\n<td>Warning-Based</td>\n<td>Alerts user but allows progress</td>\n<td>Warnings often ignored, problems persist</td>\n</tr>\n<tr>\n<td>Strict Validation</td>\n<td>Forces awareness of data quality</td>\n<td>Can be frustrating for beginners</td>\n</tr>\n<tr>\n<td>Configurable</td>\n<td>Flexible for different experience levels</td>\n<td>Adds complexity, may enable bad practices</td>\n</tr>\n</tbody></table>\n<p><strong>Decision</strong>: Implement strict validation with descriptive error messages.</p>\n<p><strong>Rationale</strong>: For an educational implementation, understanding data quality is as important as understanding algorithms. Silent fixes prevent learning, and warnings are easily ignored. Strict validation forces learners to confront real-world data issues and develop good practices. Descriptive error messages turn failures into learning opportunities.</p>\n<p><strong>Consequences</strong>: Learners may initially find the strict validation frustrating, but they will develop better data handling practices. The implementation requires comprehensive error message design to be educational rather than obstructive. This approach builds habits that transfer to production systems.</p>\n<h4 id=\"decision-dataset-container-design\">Decision: Dataset Container Design</h4>\n<p><strong>Context</strong>: The system needs to package processed data with metadata for use by regression components. This could be done with simple tuples, dictionary structures, or custom classes.</p>\n<p><strong>Options Considered</strong>:</p>\n<ol>\n<li><strong>Simple Tuples</strong>: Return <code>(features, targets)</code> tuples from processing functions</li>\n<li><strong>Dictionary Bundles</strong>: Use dictionaries with standardized keys for data and metadata  </li>\n<li><strong>Dataset Class</strong>: Create a structured <code>Dataset</code> class with methods and properties</li>\n<li><strong>NamedTuple</strong>: Use typing.NamedTuple for structure without full class complexity</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Simple Tuples</td>\n<td>Minimal complexity, familiar pattern</td>\n<td>No metadata storage, error-prone unpacking</td>\n</tr>\n<tr>\n<td>Dictionary Bundles</td>\n<td>Flexible, can store arbitrary metadata</td>\n<td>No type checking, key name inconsistencies</td>\n</tr>\n<tr>\n<td>Dataset Class</td>\n<td>Type safety, clear interface, extensible</td>\n<td>More complex for beginners to understand</td>\n</tr>\n<tr>\n<td>NamedTuple</td>\n<td>Structured but simple, immutable</td>\n<td>Limited extensibility, no methods</td>\n</tr>\n</tbody></table>\n<p><strong>Decision</strong>: Implement a structured <code>Dataset</code> class with clear properties and methods.</p>\n<p><strong>Rationale</strong>: The educational value of understanding structured data containers outweighs the added complexity. A well-designed class teaches object-oriented design principles while providing type safety and clear interfaces. The metadata storage capabilities are essential for features like normalization statistics persistence.</p>\n<p><strong>Consequences</strong>: Learners must understand basic class design and property access. The implementation provides a template for structured data handling that applies to larger ML systems. The class design enables cleaner interfaces between components.</p>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<h4 id=\"-pitfall-using-python-lists-instead-of-numpy-arrays\">⚠️ <strong>Pitfall: Using Python Lists Instead of NumPy Arrays</strong></h4>\n<p>Many beginners attempt to use standard Python lists for numerical data, leading to performance problems and type inconsistencies throughout the system. Python lists are designed for general-purpose data storage, not numerical computation.</p>\n<p><strong>Why This Fails</strong>: Python lists store objects with type checking overhead, making numerical operations slow. Mathematical operations like element-wise multiplication require explicit loops instead of vectorized operations. Type checking becomes inconsistent when mixing lists and NumPy arrays from other components.</p>\n<p><strong>How to Avoid</strong>: Always convert input data to NumPy arrays immediately after loading. Use <code>np.array()</code> for small datasets or <code>np.loadtxt()</code>/<code>np.genfromtxt()</code> for CSV loading. Verify array types with <code>isinstance(data, np.ndarray)</code> in validation functions.</p>\n<p><strong>Detection</strong>: Watch for error messages about unsupported operations between lists and arrays, or surprisingly slow performance on medium-sized datasets (&gt;1000 samples).</p>\n<h4 id=\"-pitfall-forgetting-to-handle-division-by-zero-in-normalization\">⚠️ <strong>Pitfall: Forgetting to Handle Division by Zero in Normalization</strong></h4>\n<p>Z-score normalization divides by standard deviation, which can be zero for constant features. This causes NaN values that propagate through the entire training process, leading to mysterious convergence failures.</p>\n<p><strong>Why This Fails</strong>: Constant features have zero variance, making the standard deviation zero. The normalization formula <code>(x - μ) / σ</code> becomes <code>(x - μ) / 0</code>, producing NaN values. These NaN values break gradient calculations and cause optimization algorithms to fail silently or produce nonsensical results.</p>\n<p><strong>How to Avoid</strong>: Check for zero variance before normalization using <code>np.var(features, axis=0) &lt; epsilon</code>. Either remove constant features, add small epsilon values to prevent division by zero, or raise informative errors explaining the problem.</p>\n<p><strong>Detection</strong>: Look for NaN values appearing after normalization, or gradient descent producing NaN costs after the first few iterations.</p>\n<h4 id=\"-pitfall-applying-different-normalization-to-training-and-prediction-data\">⚠️ <strong>Pitfall: Applying Different Normalization to Training and Prediction Data</strong></h4>\n<p>A critical error occurs when normalization statistics are recomputed for prediction data instead of using the statistics from training data. This breaks the fundamental assumption that training and prediction inputs occupy the same feature space.</p>\n<p><strong>Why This Fails</strong>: If prediction data has different mean/variance than training data (which is normal), recomputing normalization statistics creates a different transformation. The model was trained on data normalized with training statistics, so applying different normalization makes prediction inputs incompatible with the learned parameters.</p>\n<p><strong>How to Avoid</strong>: Store normalization statistics (mean and standard deviation) during training and apply these exact values to prediction data. Never call <code>normalize_features()</code> on prediction data - instead use <code>apply_normalization()</code> with stored statistics.</p>\n<p><strong>Detection</strong>: Predictions that seem completely wrong despite good training performance, or prediction errors that scale with how different the prediction data distribution is from training data.</p>\n<h4 id=\"-pitfall-inadequate-data-type-validation\">⚠️ <strong>Pitfall: Inadequate Data Type Validation</strong></h4>\n<p>Loading CSV data often produces string arrays or mixed-type arrays instead of numerical arrays, causing subtle errors during mathematical operations. Pandas and basic CSV readers may interpret numerical data as strings, especially with missing values or formatting inconsistencies.</p>\n<p><strong>Why This Fails</strong>: String arrays support some mathematical operations without immediate errors, but produce incorrect results. For example, <code>&quot;1&quot; + &quot;2&quot;</code> equals <code>&quot;12&quot;</code> instead of <code>3</code>. These errors can be difficult to debug because they don&#39;t always cause immediate exceptions.</p>\n<p><strong>How to Avoid</strong>: Explicitly validate data types after loading using <code>np.issubdtype(data.dtype, np.number)</code>. Use <code>dtype=float</code> parameters in loading functions. Implement comprehensive type checking in validation functions before any mathematical operations.</p>\n<p><strong>Detection</strong>: Mathematical operations producing string concatenation results, or arrays with <code>object</code> or <code>&lt;U</code> (Unicode string) dtypes when numerical data was expected.</p>\n<h4 id=\"-pitfall-ignoring-feature-scale-differences-during-debugging\">⚠️ <strong>Pitfall: Ignoring Feature Scale Differences During Debugging</strong></h4>\n<p>When gradient descent fails to converge, beginners often adjust learning rates or iteration counts without examining feature scales. Unscaled features can create optimization landscapes where reasonable learning rates cause oscillation or extremely slow convergence.</p>\n<p><strong>Why This Fails</strong>: Features with vastly different scales (age: 0-100, income: 0-100000) create cost function contours that are elongated ellipses instead of circles. Gradient descent must take tiny steps to avoid overshooting in the high-scale dimension, making convergence extremely slow in the low-scale dimension.</p>\n<p><strong>How to Avoid</strong>: Always examine feature scales using <code>np.mean()</code> and <code>np.std()</code> for each feature before training. If scales differ by more than one order of magnitude, normalization is essential. Implement scale analysis as part of the data validation process.</p>\n<p><strong>Detection</strong>: Gradient descent requiring thousands of iterations to converge, cost oscillating wildly with moderate learning rates, or some parameters changing much faster than others during training.</p>\n<h4 id=\"-pitfall-insufficient-sample-size-validation\">⚠️ <strong>Pitfall: Insufficient Sample Size Validation</strong></h4>\n<p>Linear regression requires more samples than features to avoid overfitting, but beginners often attempt to fit models on datasets with very few samples or more features than samples, leading to perfect training performance that doesn&#39;t generalize.</p>\n<p><strong>Why This Fails</strong>: With fewer samples than parameters, the system of equations is underdetermined and has infinite solutions. The model can achieve perfect fit on training data by essentially memorizing it, but performs terribly on new data. Matrix operations may also become numerically unstable.</p>\n<p><strong>How to Avoid</strong>: Validate that sample count significantly exceeds feature count (recommended minimum: 10 samples per feature). Implement explicit checks in data validation and provide clear error messages explaining the statistical requirements.</p>\n<p><strong>Detection</strong>: Training accuracy that seems too good to be true (R-squared = 1.0), or models that perform perfectly on training data but terribly on any new data.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Loading</td>\n<td><code>csv.reader</code> + manual parsing</td>\n<td><code>pandas.read_csv()</code> with full feature support</td>\n</tr>\n<tr>\n<td>Numerical Arrays</td>\n<td><code>numpy</code> with basic operations</td>\n<td><code>numpy</code> with advanced broadcasting and vectorization</td>\n</tr>\n<tr>\n<td>Data Validation</td>\n<td>Manual checks with if/else logic</td>\n<td><code>jsonschema</code> or custom validation framework</td>\n</tr>\n<tr>\n<td>File Handling</td>\n<td>Basic <code>open()</code> with exception handling</td>\n<td><code>pathlib.Path</code> for robust path operations</td>\n</tr>\n<tr>\n<td>Error Reporting</td>\n<td>Simple exception raising</td>\n<td>Structured logging with detailed error context</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>linear_regression_project/\n├── data/\n│   ├── __init__.py\n│   ├── handler.py              ← Main DataHandler class (core learning goal)\n│   ├── validation.py           ← Data validation utilities (infrastructure)\n│   ├── synthetic.py            ← Synthetic data generation (infrastructure)\n│   └── examples/\n│       ├── housing.csv         ← Sample dataset\n│       └── synthetic_linear.csv\n├── tests/\n│   ├── test_data_handler.py    ← Comprehensive unit tests\n│   └── fixtures/\n│       └── test_data.csv\n└── utils/\n    ├── __init__.py\n    └── constants.py            ← Shared constants and tolerances</code></pre></div>\n\n<p>This structure separates the core learning component (<code>handler.py</code>) from supporting infrastructure, allowing learners to focus on the essential data processing logic while having robust utilities available.</p>\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>File: <code>data/validation.py</code></strong> (Complete implementation)</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, List, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> warnings</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_array_dimensions</span><span style=\"color:#E1E4E8\">(features: np.ndarray, targets: np.ndarray) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validate that feature and target arrays have compatible dimensions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> features.ndim </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Features must be 2D array (samples, features), got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">features.ndim</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">D\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> targets.ndim </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Targets must be 1D array, got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">targets.ndim</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">D\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> features.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> targets.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"Feature array has </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">features.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> samples \"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"but target array has </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">targets.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> samples\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_numerical_data</span><span style=\"color:#E1E4E8\">(data: np.ndarray, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Check for NaN, infinite, or non-numeric values in array.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> np.issubdtype(data.dtype, np.number):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> array contains non-numeric data (dtype: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">data.dtype</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> np.isfinite(data).all():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        nan_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.isnan(data).sum()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        inf_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.isinf(data).sum()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"Found </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">nan_count</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> NaN and </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">inf_count</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> infinite values in </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> array\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> check_feature_variance</span><span style=\"color:#E1E4E8\">(features: np.ndarray, min_variance: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-10</span><span style=\"color:#E1E4E8\">) -> List[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Check for constant features and return indices of problematic features.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    variances </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.var(features, </span><span style=\"color:#FFAB70\">axis</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    constant_features </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.where(variances </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> min_variance)[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> constant_features.tolist()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_sample_size</span><span style=\"color:#E1E4E8\">(n_samples: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, n_features: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, min_ratio: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Ensure sufficient samples relative to feature count.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> n_samples </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Need at least 2 samples for regression, got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">n_samples</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> n_samples </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> n_features </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> min_ratio:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        warnings.warn(</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"Only </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">n_samples</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> samples for </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">n_features</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> features. \"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"Recommend at least </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">n_features </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> min_ratio</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> samples.\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span></code></pre></div>\n\n<p><strong>File: <code>data/synthetic.py</code></strong> (Complete implementation)</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tuple</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> generate_linear_data</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    n_samples: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    slope: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    intercept: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    noise_std: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    x_range: Tuple[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    random_seed: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">) -> Tuple[np.ndarray, np.ndarray]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Generate synthetic linear dataset for testing and learning.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> random_seed </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        np.random.seed(random_seed)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Generate evenly spaced x values with some random jitter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    x_base </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.linspace(x_range[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">], x_range[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">], n_samples)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    x_jitter </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.random.normal(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, (x_range[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> x_range[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]) </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 0.02</span><span style=\"color:#E1E4E8\">, n_samples)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    x </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> x_base </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> x_jitter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Generate y values with linear relationship plus noise</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    y_perfect </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> slope </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> x </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> intercept</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    noise </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.random.normal(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, noise_std, n_samples)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> y_perfect </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> noise</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Reshape x to be 2D (samples, 1)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    X </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> x.reshape(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> X, y</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> generate_multiple_linear_data</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    n_samples: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    weights: np.ndarray,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    intercept: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    noise_std: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    feature_ranges: Optional[List[Tuple[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    random_seed: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">) -> Tuple[np.ndarray, np.ndarray]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Generate synthetic multiple linear regression dataset.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> random_seed </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        np.random.seed(random_seed)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    n_features </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(weights)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> feature_ranges </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        feature_ranges </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">)] </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> n_features</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Generate features</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    X </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.zeros((n_samples, n_features))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i, (low, high) </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(feature_ranges):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        X[:, i] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.random.uniform(low, high, n_samples)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Generate targets</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    y_perfect </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> X </span><span style=\"color:#F97583\">@</span><span style=\"color:#E1E4E8\"> weights </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> intercept</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    noise </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.random.normal(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, noise_std, n_samples)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> y_perfect </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> noise</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> X, y</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>File: <code>data/handler.py</code></strong> (Learning implementation)</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> csv</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Tuple, Dict, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .validation </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> validate_array_dimensions, validate_numerical_data, check_feature_variance</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Dataset</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Container for processed dataset with metadata.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    features: np.ndarray</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    targets: np.ndarray</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    feature_names: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    n_samples: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    n_features: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    is_normalized: </span><span style=\"color:#79B8FF\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    normalization_stats: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, np.ndarray]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> normalize_features</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#9ECBFF\">'Dataset'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Apply z-score normalization to features and return new Dataset.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if already normalized, raise error if so</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Compute mean and std for each feature using np.mean, np.std</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle zero-variance features (add epsilon or raise error)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Apply transformation: (features - mean) / std</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Store normalization statistics for later use</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return new Dataset with normalized features and updated metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use axis=0 for column-wise operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> apply_normalization</span><span style=\"color:#E1E4E8\">(self, features: np.ndarray) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Apply stored normalization statistics to new feature data.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check that normalization statistics exist</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate that input features have correct number of features</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Apply stored mean and std: (features - stored_mean) / stored_std</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return normalized features</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use broadcasting for efficient computation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DataHandler</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Handles data loading, validation, and preprocessing for linear regression.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tolerance </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1e-10</span><span style=\"color:#6A737D\">  # For numerical comparisons</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> load_csv_data</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        self, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        filename: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        feature_columns: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        target_column: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ) -> Tuple[np.ndarray, np.ndarray]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load data from CSV file and return feature and target arrays.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Open CSV file and read header row</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Find indices of specified feature and target columns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Read data rows and extract specified columns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Convert string data to float arrays using np.array(dtype=float)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate that all specified columns exist</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Handle file not found and parsing errors with informative messages</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use csv.DictReader for easier column access</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_data</span><span style=\"color:#E1E4E8\">(self, features: np.ndarray, targets: np.ndarray) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Comprehensive data validation for regression compatibility.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check array dimensions using validate_array_dimensions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate numerical data using validate_numerical_data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check for sufficient samples (at least 2)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Identify constant features using check_feature_variance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Check target variance (constant targets can't be modeled)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Raise informative errors for each validation failure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Call validation utilities from validation.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_dataset</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        self,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        features: np.ndarray,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        targets: np.ndarray,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        feature_names: Optional[List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ) -> Dataset:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create a Dataset object with validation and metadata.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate input data using validate_data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Generate default feature names if not provided</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Extract dataset dimensions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Initialize empty normalization statistics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Create and return Dataset object</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use f\"feature_{i}\" for default feature names</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints\">Language-Specific Hints</h4>\n<p><strong>NumPy Broadcasting</strong>: When applying normalization, NumPy&#39;s broadcasting allows you to subtract a 1D mean array from a 2D feature array automatically. <code>features - mean</code> works even though shapes are (n_samples, n_features) and (n_features,).</p>\n<p><strong>CSV Handling</strong>: Use <code>csv.DictReader</code> instead of basic <code>csv.reader</code> for easier column access by name. Handle encoding issues by specifying <code>encoding=&#39;utf-8&#39;</code> in the file open call.</p>\n<p><strong>Error Context</strong>: When raising exceptions, include the actual values that caused the problem: <code>f&quot;Expected {expected} but got {actual}&quot;</code>. This makes debugging much easier for learners.</p>\n<p><strong>Memory Efficiency</strong>: For large datasets, consider using <code>np.loadtxt()</code> or <code>np.genfromtxt()</code> instead of manual CSV parsing. These functions handle type conversion and missing values more efficiently.</p>\n<p><strong>Type Hints</strong>: Use <code>Optional[List[str]]</code> for parameters that can be None, and <code>Tuple[np.ndarray, np.ndarray]</code> for functions returning multiple arrays.</p>\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p><strong>After implementing data loading (M1 focus):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from data.handler import DataHandler</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from data.synthetic import generate_linear_data</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Test synthetic data generation</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">X, y = generate_linear_data(100, slope=2.0, intercept=1.0)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print(f'Generated data: X shape {X.shape}, y shape {y.shape}')</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Test data handler creation</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">handler = DataHandler()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">dataset = handler.create_dataset(X, y, ['feature_1'])</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print(f'Dataset: {dataset.n_samples} samples, {dataset.n_features} features')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n\n<p><strong>Expected Output</strong>:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Generated data: X shape (100, 1), y shape (100,)\nDataset: 100 samples, 1 features</code></pre></div>\n\n<p><strong>After implementing normalization (M2 focus):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">import numpy as np</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from data.handler import DataHandler, Dataset</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Create test data with different scales</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">features = np.array([[1, 1000], [2, 2000], [3, 3000]], dtype=float)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">targets = np.array([10, 20, 30], dtype=float)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">handler = DataHandler()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">dataset = handler.create_dataset(features, targets, ['small_scale', 'large_scale'])</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">normalized_dataset = dataset.normalize_features()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print('Original feature means:', np.mean(dataset.features, axis=0))</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print('Normalized feature means:', np.mean(normalized_dataset.features, axis=0))</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print('Normalized feature stds:', np.std(normalized_dataset.features, axis=0))</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n\n<p><strong>Expected Output</strong>:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Original feature means: [   2. 2000.]\nNormalized feature means: [ 2.22044605e-17 -1.48029737e-17]  # Approximately zero\nNormalized feature stds: [1. 1.]  # Exactly 1.0</code></pre></div>\n\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>&quot;unsupported operand type&quot; errors</td>\n<td>Using Python lists instead of NumPy arrays</td>\n<td>Check <code>type(data)</code> before operations</td>\n<td>Convert with <code>np.array(data, dtype=float)</code></td>\n</tr>\n<tr>\n<td>NaN values in normalized data</td>\n<td>Division by zero in std calculation</td>\n<td>Check <code>np.var(features, axis=0)</code> for zeros</td>\n<td>Add epsilon or remove constant features</td>\n</tr>\n<tr>\n<td>&quot;shapes not aligned&quot; in matrix operations</td>\n<td>Wrong array dimensions</td>\n<td>Print <code>array.shape</code> for all arrays</td>\n<td>Reshape with <code>array.reshape(-1, 1)</code> or transpose</td>\n</tr>\n<tr>\n<td>Very slow convergence in later milestones</td>\n<td>Features not normalized</td>\n<td>Check <code>np.mean()</code> and <code>np.std()</code> of each feature</td>\n<td>Apply z-score normalization</td>\n</tr>\n<tr>\n<td>Perfect training accuracy (R² = 1.0)</td>\n<td>More features than samples</td>\n<td>Compare <code>n_samples</code> vs <code>n_features</code></td>\n<td>Get more data or reduce features</td>\n</tr>\n<tr>\n<td>String concatenation instead of addition</td>\n<td>CSV data loaded as strings</td>\n<td>Check <code>array.dtype</code> after loading</td>\n<td>Use <code>dtype=float</code> in loading functions</td>\n</tr>\n</tbody></table>\n<h2 id=\"simple-linear-regression-component\">Simple Linear Regression Component</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> M1: Simple Linear Regression</p>\n</blockquote>\n<p>This component implements single-variable linear regression using the closed-form ordinary least squares solution. The <code>SimpleLinearRegression</code> class serves as the foundational implementation that demonstrates the mathematical principles of linear modeling before introducing the complexities of iterative optimization. This component establishes the core concepts of model fitting, parameter estimation, and prediction that will be extended in subsequent milestones.</p>\n<p>The simple linear regression component represents the most direct approach to understanding the relationship between a single input variable and a continuous target variable. Unlike iterative methods, the closed-form solution provides an exact mathematical answer that can be computed in a single step, making it ideal for educational purposes and establishing baseline understanding.</p>\n<h3 id=\"mental-model-the-best-fit-line\">Mental Model: The Best-Fit Line</h3>\n<p>Think of simple linear regression as finding the &quot;best-fitting ruler&quot; through a scattered set of points on graph paper. Imagine you have a collection of data points scattered across a coordinate plane, and your task is to place a straight ruler (representing the regression line) in such a way that it comes as close as possible to all the points simultaneously.</p>\n<p>The &quot;best&quot; position for this ruler is not subjective—it&#39;s mathematically defined as the line that minimizes the total squared distance from each point to the line. Picture drawing vertical lines from each data point down to your ruler; these vertical distances represent the <strong>residuals</strong> or prediction errors. The optimal ruler position minimizes the sum of the squares of these vertical distances, which is why we call this the &quot;least squares&quot; solution.</p>\n<p>This mental model reveals several key insights. First, the ruler (regression line) doesn&#39;t need to pass through any specific point—it finds the position that balances all points optimally. Second, points further from the line contribute more to the error because we square the distances, meaning outliers have disproportionate influence. Third, there&#39;s only one optimal ruler position for any given set of points, which corresponds to the unique closed-form solution.</p>\n<p>The mathematical beauty of simple linear regression lies in the fact that this &quot;best ruler position&quot; can be calculated directly using a formula, without any trial-and-error or iterative searching. The slope and intercept of the optimal line emerge naturally from the statistical properties of the data itself.</p>\n<h3 id=\"closed-form-solution-algorithm\">Closed-Form Solution Algorithm</h3>\n<p>The ordinary least squares solution provides a direct mathematical formula for computing the optimal slope and intercept parameters. This approach contrasts with iterative methods by solving the optimization problem analytically, leveraging calculus to find the exact minimum of the cost function in a single computation.</p>\n<p>The mathematical foundation rests on the principle that the optimal parameters minimize the sum of squared residuals. For a linear model <code>y = mx + b</code>, where <code>m</code> is the slope and <code>b</code> is the intercept, the residual for each data point <code>(xi, yi)</code> is <code>yi - (mxi + b)</code>. The cost function becomes the sum of all squared residuals: <code>∑(yi - mxi - b)²</code>.</p>\n<p>To find the minimum, we take partial derivatives of this cost function with respect to both <code>m</code> and <code>b</code>, set them equal to zero, and solve the resulting system of linear equations. This yields the normal equations, which provide direct formulas for the optimal parameters.</p>\n<p>The algorithm proceeds through the following steps:</p>\n<ol>\n<li><p><strong>Data Validation and Preparation</strong>: Verify that input arrays <code>x</code> and <code>y</code> have the same length and contain only finite numerical values. Convert inputs to NumPy arrays if necessary and ensure they have compatible shapes for mathematical operations.</p>\n</li>\n<li><p><strong>Compute Sample Statistics</strong>: Calculate the mean of the input features (<code>x_mean</code>) and the mean of the target values (<code>y_mean</code>). These sample means appear in the closed-form formulas and represent the centroid of the data distribution.</p>\n</li>\n<li><p><strong>Calculate Slope Using Covariance Formula</strong>: Compute the slope parameter using the formula <code>m = Σ((xi - x_mean) * (yi - y_mean)) / Σ((xi - x_mean)²)</code>. The numerator represents the sample covariance between x and y, while the denominator represents the sample variance of x. This ratio quantifies how much y changes, on average, for each unit change in x.</p>\n</li>\n<li><p><strong>Calculate Intercept Using Means</strong>: Compute the intercept parameter using the formula <code>b = y_mean - m * x_mean</code>. This ensures that the regression line passes through the point <code>(x_mean, y_mean)</code>, which is the centroid of the data. This property guarantees that the sum of residuals equals zero.</p>\n</li>\n<li><p><strong>Numerical Stability Check</strong>: Verify that the denominator in the slope calculation is not zero or extremely small (less than tolerance <code>1e-10</code>). A zero denominator indicates that all x values are identical, making it impossible to determine a meaningful slope. In such cases, the component should raise a descriptive error message.</p>\n</li>\n<li><p><strong>Parameter Storage</strong>: Store the computed slope and intercept in the model&#39;s state variables <code>slope_</code> and <code>intercept_</code>, and set the <code>is_fitted_</code> flag to <code>True</code> to indicate successful training.</p>\n</li>\n</ol>\n<p>The mathematical elegance of this approach lies in its directness—no iterations, no convergence criteria, no learning rate tuning. The solution is computed exactly in one pass through the data, making it both computationally efficient and numerically stable for well-conditioned problems.</p>\n<table>\n<thead>\n<tr>\n<th>Parameter</th>\n<th>Formula</th>\n<th>Interpretation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Slope (m)</td>\n<td><code>Σ((xi - x_mean) * (yi - y_mean)) / Σ((xi - x_mean)²)</code></td>\n<td>Rate of change of y with respect to x</td>\n</tr>\n<tr>\n<td>Intercept (b)</td>\n<td><code>y_mean - m * x_mean</code></td>\n<td>Expected y value when x equals zero</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p>The critical insight is that the closed-form solution leverages the geometric property that the optimal regression line always passes through the centroid of the data. This constraint, combined with the least squares criterion, uniquely determines both parameters.</p>\n</blockquote>\n<h3 id=\"prediction-and-evaluation\">Prediction and Evaluation</h3>\n<p>Once the model parameters are fitted, the <code>SimpleLinearRegression</code> component can generate predictions for new input values and evaluate model performance using standard regression metrics. The prediction process applies the learned linear relationship to transform input features into target estimates.</p>\n<p><strong>Prediction Generation</strong>: The <code>predict(x)</code> method applies the fitted linear model to compute predicted values using the equation <code>y_pred = slope_ * x + intercept_</code>. This method accepts either scalar values or NumPy arrays, automatically broadcasting the computation across all input elements. Before prediction, the method validates that the model has been fitted by checking the <code>is_fitted_</code> flag, raising an informative error if <code>fit()</code> has not been called.</p>\n<p>The prediction process involves several steps:</p>\n<ol>\n<li><p><strong>Model State Validation</strong>: Confirm that <code>is_fitted_</code> is <code>True</code>, indicating that valid parameters have been computed. This fail-fast approach prevents silent errors from uninitialized parameters.</p>\n</li>\n<li><p><strong>Input Preparation</strong>: Convert the input to a NumPy array if necessary and verify that it contains only finite numerical values. Handle both scalar inputs and array inputs uniformly through NumPy&#39;s broadcasting mechanism.</p>\n</li>\n<li><p><strong>Linear Transformation</strong>: Apply the fitted equation <code>y_pred = slope_ * x + intercept_</code> to generate predictions. NumPy automatically handles broadcasting for array inputs, computing predictions for all elements simultaneously.</p>\n</li>\n<li><p><strong>Output Formatting</strong>: Return predictions as a NumPy array with the same shape as the input, ensuring consistent interface behavior regardless of input format.</p>\n</li>\n</ol>\n<p><strong>Model Evaluation</strong>: The <code>score(x, y)</code> method computes the R-squared coefficient of determination, which measures the proportion of variance in the target variable that is predictable from the input features. R-squared serves as the primary evaluation metric for regression models, providing an interpretable measure of model performance.</p>\n<p>The R-squared calculation follows this procedure:</p>\n<ol>\n<li><p><strong>Generate Predictions</strong>: Use the fitted model to predict target values for the provided input features, creating the prediction array <code>y_pred</code>.</p>\n</li>\n<li><p><strong>Compute Total Sum of Squares (TSS)</strong>: Calculate the total variance in the target variable using <code>TSS = Σ(yi - y_mean)²</code>, where <code>y_mean</code> is the mean of the actual target values. This represents the total variation that a model could potentially explain.</p>\n</li>\n<li><p><strong>Compute Residual Sum of Squares (RSS)</strong>: Calculate the unexplained variance using <code>RSS = Σ(yi - y_pred_i)²</code>, representing the variation that remains after applying the model.</p>\n</li>\n<li><p><strong>Calculate R-squared</strong>: Compute the coefficient of determination using <code>R² = 1 - RSS/TSS</code>. This formula gives the fraction of total variance explained by the model.</p>\n</li>\n</ol>\n<p>The R-squared metric has several important properties. Perfect predictions yield R² = 1, indicating that the model explains all variance in the target variable. A model that performs no better than predicting the mean yields R² = 0. Negative R-squared values are possible and indicate that the model performs worse than simply predicting the mean for all inputs.</p>\n<table>\n<thead>\n<tr>\n<th>Evaluation Metric</th>\n<th>Formula</th>\n<th>Interpretation</th>\n<th>Range</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>R-squared</td>\n<td><code>1 - Σ(yi - y_pred_i)² / Σ(yi - y_mean)²</code></td>\n<td>Proportion of variance explained</td>\n<td>[−∞, 1]</td>\n</tr>\n<tr>\n<td>Mean Squared Error</td>\n<td><code>Σ(yi - y_pred_i)² / n</code></td>\n<td>Average squared prediction error</td>\n<td>[0, ∞)</td>\n</tr>\n<tr>\n<td>Residual</td>\n<td><code>yi - y_pred_i</code></td>\n<td>Individual prediction error</td>\n<td>(−∞, ∞)</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p>Understanding R-squared interpretation is crucial: R² = 0.8 means the model explains 80% of the variance in the target variable, while the remaining 20% is either noise or systematic patterns not captured by the linear relationship.</p>\n</blockquote>\n<h3 id=\"architecture-decision-records\">Architecture Decision Records</h3>\n<p>This section documents the key design decisions made in implementing the simple linear regression component, providing rationale for architectural choices that impact both educational value and technical implementation.</p>\n<blockquote>\n<p><strong>Decision: Closed-Form Solution as Primary Implementation</strong></p>\n<ul>\n<li><strong>Context</strong>: Linear regression can be solved either through direct analytical computation (closed-form solution using normal equations) or through iterative optimization (gradient descent). For the foundational milestone, we need to choose the primary approach that best serves educational objectives while providing a solid foundation for later extensions.</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Closed-form solution only</li>\n<li>Gradient descent only </li>\n<li>Both approaches with equal emphasis</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement closed-form solution as the primary method for M1, with gradient descent introduced separately in M2</li>\n<li><strong>Rationale</strong>: The closed-form approach provides immediate mathematical insight into the optimization problem without the complexities of iterative algorithms. Students can see the direct relationship between data statistics and optimal parameters. This builds intuition for the underlying mathematical principles before introducing algorithmic considerations like learning rates and convergence criteria.</li>\n<li><strong>Consequences</strong>: Enables rapid prototyping and validation of the linear regression concept. Provides exact solutions for comparison when implementing gradient descent. Limitations include inability to handle very large datasets efficiently and no extension to non-linear optimization problems.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>Pros</th>\n<th>Cons</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Closed-form only</td>\n<td>Exact solution, fast computation, mathematical insight</td>\n<td>Limited to linear problems, memory intensive for large datasets</td>\n</tr>\n<tr>\n<td>Gradient descent only</td>\n<td>Scales to large data, extends to non-linear problems</td>\n<td>Requires hyperparameter tuning, convergence complexity</td>\n</tr>\n<tr>\n<td>Both approaches</td>\n<td>Comprehensive understanding, comparison opportunities</td>\n<td>Increased complexity, potential confusion for beginners</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: NumPy Arrays as Primary Data Structure</strong></p>\n<ul>\n<li><strong>Context</strong>: The component needs to handle numerical data efficiently while maintaining compatibility with common Python data science workflows. Options include pure Python lists, NumPy arrays, or supporting multiple input formats with internal conversion.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Pure Python lists for simplicity</li>\n<li>NumPy arrays exclusively</li>\n<li>Flexible input with automatic conversion</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use NumPy arrays as the primary internal representation with automatic conversion from other formats</li>\n<li><strong>Rationale</strong>: NumPy provides vectorized operations essential for efficient mathematical computation. Broadcasting capabilities handle scalar and array inputs uniformly. The scientific Python ecosystem expects NumPy compatibility. Automatic conversion reduces friction for users while maintaining performance benefits internally.</li>\n<li><strong>Consequences</strong>: Enables efficient computation and broadcasting. Requires NumPy dependency. Provides natural integration with plotting libraries and data analysis workflows. Potential memory overhead for very small datasets, but this is negligible for educational purposes.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Fail-Fast Validation Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: The component can encounter various error conditions including incompatible data shapes, non-numeric values, constant input features, and attempts to predict before fitting. The validation strategy determines when and how these errors are detected and reported.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Minimal validation with silent failures</li>\n<li>Comprehensive validation with immediate errors</li>\n<li>Lazy validation during computation</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement comprehensive fail-fast validation with descriptive error messages</li>\n<li><strong>Rationale</strong>: Educational implementations benefit from clear, immediate feedback when mistakes occur. Fail-fast prevents silent failures that could lead to incorrect results and confusion. Descriptive error messages guide learners toward correct usage patterns and help diagnose data quality issues.</li>\n<li><strong>Consequences</strong>: Improves debugging experience and reduces frustration for learners. Adds computational overhead for validation checks. Requires careful design of error messages to be educational rather than cryptic. Encourages good practices in data handling and model usage.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Single Responsibility Component Design</strong></p>\n<ul>\n<li><strong>Context</strong>: The <code>SimpleLinearRegression</code> component could potentially include data loading, preprocessing, visualization, and advanced evaluation metrics. The scope decision affects both code organization and learning progression.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Monolithic component handling all regression-related tasks</li>\n<li>Focused component handling only model fitting and prediction</li>\n<li>Minimal component with external dependencies for all auxiliary functions</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Design focused component responsible only for model fitting, prediction, and basic evaluation, with clear interfaces to other components</li>\n<li><strong>Rationale</strong>: Single responsibility principle improves code organization and testing. Allows independent development and understanding of different system aspects. Facilitates composition of components for different use cases. Matches the modular architecture established in the high-level design.</li>\n<li><strong>Consequences</strong>: Requires clear interface design for component communication. Students learn proper separation of concerns. Code becomes more modular and testable. May require more initial setup to achieve complete workflows, but this reinforces architectural understanding.</li>\n</ul>\n</blockquote>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>Understanding common mistakes helps learners avoid frustrating debugging sessions and develops good practices for numerical computing and machine learning implementations.</p>\n<p>⚠️ <strong>Pitfall: Division by Zero in Slope Calculation</strong></p>\n<p>The most critical numerical issue occurs when all input features have identical values, causing the denominator in the slope formula <code>Σ((xi - x_mean)²)</code> to equal zero. This happens when the dataset contains no variation in the input variable, such as when all data points have the same x-coordinate.</p>\n<p>When this occurs, the mathematical concept of slope becomes undefined—there&#39;s no meaningful way to determine how y changes with respect to x if x never changes. Attempting to compute the slope results in division by zero, which NumPy handles by producing infinite or NaN values that propagate through subsequent calculations.</p>\n<p><strong>Detection Strategy</strong>: Check if the variance of the input features is below the numerical tolerance threshold (<code>1e-10</code>) before performing the division. This catches both exact zeros and values so small they represent numerical noise.</p>\n<p><strong>Proper Handling</strong>: Raise a descriptive <code>ValueError</code> with a message like &quot;Cannot fit linear regression: input features have zero variance (all values are identical).&quot; This immediately alerts the user to the data quality issue rather than producing meaningless results.</p>\n<p><strong>Prevention</strong>: Include data validation that checks for feature variance during the initial data loading and preparation phases. Consider providing guidance about minimum data requirements for meaningful regression analysis.</p>\n<p>⚠️ <strong>Pitfall: Incorrect Array Shape Handling</strong></p>\n<p>NumPy&#39;s broadcasting rules can mask shape incompatibilities that lead to unexpected behavior. Common issues include mixing 1D arrays with 2D column vectors, scalar inputs that should be arrays, and mismatched dimensions between features and targets.</p>\n<p>The <code>fit(x, y)</code> method expects x and y to have compatible shapes for element-wise operations. However, NumPy&#39;s automatic broadcasting can sometimes succeed when it shouldn&#39;t, leading to incorrect parameter calculations. For example, a scalar x value might broadcast against an array y, producing mathematically valid but conceptually wrong results.</p>\n<p><strong>Detection Strategy</strong>: Explicitly validate that input arrays have the same length using <code>len(x) == len(y)</code> before relying on NumPy operations. Check that both inputs are at least 1D arrays and handle scalar inputs by converting them to arrays.</p>\n<p><strong>Proper Handling</strong>: Implement comprehensive shape validation in the <code>fit()</code> method that checks array compatibility and converts inputs to consistent formats. Provide clear error messages that specify the expected shapes and actual shapes received.</p>\n<p><strong>Prevention</strong>: Design the interface to accept common input formats (lists, scalars, 1D arrays, 2D column vectors) and standardize them internally. Document the expected input formats clearly in method docstrings.</p>\n<p>⚠️ <strong>Pitfall: Misinterpretation of R-squared Values</strong></p>\n<p>R-squared is frequently misunderstood, leading to incorrect conclusions about model quality. Common misconceptions include assuming that higher R-squared always means better predictions, that R-squared measures the correctness of the linear relationship, or that specific R-squared thresholds indicate good or bad models.</p>\n<p>R-squared measures the proportion of variance explained by the model relative to the total variance in the target variable. However, this doesn&#39;t directly translate to prediction accuracy for individual points, and the acceptable range depends heavily on the domain and data characteristics. In some fields, R-squared values of 0.3 represent strong relationships, while other applications might expect values above 0.9.</p>\n<p><strong>Common Misinterpretations</strong>:</p>\n<ul>\n<li>Assuming R² = 0.7 means &quot;70% accuracy&quot; in terms of individual predictions</li>\n<li>Believing that negative R-squared values indicate a bug rather than poor model performance</li>\n<li>Using R-squared to compare models fitted on different datasets or with different target variable scales</li>\n<li>Focusing solely on R-squared without examining residual patterns or other diagnostic metrics</li>\n</ul>\n<p><strong>Proper Understanding</strong>: R-squared indicates how much of the target variable&#39;s variation is captured by the linear relationship with the input features. High R-squared suggests strong linear correlation, but doesn&#39;t guarantee accurate individual predictions or validate the appropriateness of the linear model assumption.</p>\n<p><strong>Best Practices</strong>: Always examine R-squared in conjunction with residual plots, prediction scatter plots, and domain knowledge. Document typical R-squared ranges for the specific application domain. Provide educational examples showing how R-squared relates to actual prediction quality.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Numerical Precision Handling</strong></p>\n<p>Floating-point arithmetic introduces small errors that can accumulate during computation, particularly when working with data that has vastly different scales or when performing operations on very large or very small numbers. These numerical precision issues can affect the stability and accuracy of parameter estimates.</p>\n<p>Common scenarios include datasets with features spanning many orders of magnitude, very large sample sizes that affect sum computations, or input values close to the machine epsilon that cause precision loss in subtraction operations.</p>\n<p><strong>Detection Strategy</strong>: Implement checks for numerical stability by comparing computed values against reasonable bounds and detecting NaN or infinite results. Monitor the condition number of computations when possible.</p>\n<p><strong>Proper Handling</strong>: Use appropriate tolerance thresholds (like <code>1e-10</code>) for comparisons involving floating-point results. Consider feature scaling recommendations when numerical issues are detected. Provide warnings when computations may be affected by precision limitations.</p>\n<p><strong>Prevention</strong>: Design the interface to accept and recommend normalized data. Include numerical stability considerations in the documentation and provide guidance on data preprocessing steps that improve computational stability.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This subsection provides concrete implementation guidance for building the <code>SimpleLinearRegression</code> component, including complete starter code, file organization recommendations, and practical development checkpoints.</p>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Core Computation</td>\n<td>Pure NumPy arrays</td>\n<td>NumPy + SciPy for numerical stability</td>\n</tr>\n<tr>\n<td>Input Validation</td>\n<td>Manual type checking</td>\n<td>Schema validation with Pydantic</td>\n</tr>\n<tr>\n<td>Testing Framework</td>\n<td>Built-in unittest</td>\n<td>pytest with parametrized tests</td>\n</tr>\n<tr>\n<td>Documentation</td>\n<td>Inline docstrings</td>\n<td>Sphinx with mathematical notation</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure:</strong></p>\n<p>The simple linear regression component should be organized to support the learning progression while maintaining clean separation of concerns:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  src/\n    linear_regression/\n      __init__.py                    ← package initialization\n      simple_regression.py           ← SimpleLinearRegression class\n      data_handler.py               ← DataHandler class (from previous milestone)\n      utils.py                      ← shared utilities and constants\n  tests/\n    test_simple_regression.py       ← comprehensive unit tests\n    test_integration.py             ← end-to-end workflow tests\n    fixtures/\n      sample_data.csv              ← test datasets\n  examples/\n    basic_usage.py                 ← simple usage demonstration\n    synthetic_data_demo.py         ← complete workflow example\n  README.md                        ← getting started guide</code></pre></div>\n\n<p><strong>Infrastructure Starter Code:</strong></p>\n<p>Here&#39;s the complete utilities module that provides shared constants and helper functions:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Utility functions and constants for linear regression implementation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Provides shared numerical constants, validation helpers, and common calculations.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Union, Tuple</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Numerical constants</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">TOLERANCE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-10</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">DEFAULT_LEARNING_RATE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.01</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MAX_ITERATIONS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1000</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_regression_inputs</span><span style=\"color:#E1E4E8\">(x: np.ndarray, y: np.ndarray) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Comprehensive validation for regression input data.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        x: Input features array</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        y: Target values array</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        ValueError: If data is incompatible for regression</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        TypeError: If inputs are not numeric arrays</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Convert to numpy arrays if needed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    x </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.asarray(x, </span><span style=\"color:#FFAB70\">dtype</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.asarray(y, </span><span style=\"color:#FFAB70\">dtype</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Check for compatible shapes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> x.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> y.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Feature and target arrays must have same length. \"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                        f</span><span style=\"color:#9ECBFF\">\"Got x: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">x.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, y: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">y.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Check for finite values</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> np.all(np.isfinite(x)):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Input features contain non-finite values (NaN or infinity)\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> np.all(np.isfinite(y)):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Target values contain non-finite values (NaN or infinity)\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Check for minimum sample size</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(x) </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Need at least 2 samples for regression. Got </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(x)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> compute_r_squared</span><span style=\"color:#E1E4E8\">(y_true: np.ndarray, y_pred: np.ndarray) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Calculate R-squared coefficient of determination.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        y_true: Actual target values</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        y_pred: Predicted target values</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        R-squared value (can be negative)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    y_true </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.asarray(y_true)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    y_pred </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.asarray(y_pred)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Total sum of squares (variance in y_true)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ss_tot </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.sum((y_true </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> np.mean(y_true)) </span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Residual sum of squares (unexplained variance)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ss_res </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.sum((y_true </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> y_pred) </span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Handle edge case where all y_true values are identical</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> ss_tot </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> TOLERANCE</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> ss_res </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> TOLERANCE</span><span style=\"color:#F97583\"> else</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#F97583\"> -</span><span style=\"color:#E1E4E8\"> (ss_res </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> ss_tot)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> generate_synthetic_data</span><span style=\"color:#E1E4E8\">(n_samples: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, true_slope: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, true_intercept: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                          noise_std: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">, x_range: Tuple[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">)) -> Tuple[np.ndarray, np.ndarray]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Generate synthetic linear data for testing and demonstration.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        n_samples: Number of data points to generate</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        true_slope: True slope parameter</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        true_intercept: True intercept parameter</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        noise_std: Standard deviation of noise to add</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        x_range: Range for x values as (min, max) tuple</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Tuple of (x_values, y_values) as numpy arrays</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    np.random.seed(</span><span style=\"color:#79B8FF\">42</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># For reproducible examples</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Generate x values uniformly in the specified range</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    x </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.random.uniform(x_range[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">], x_range[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">], n_samples)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Generate y values with linear relationship plus noise</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> true_slope </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> x </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> true_intercept </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> np.random.normal(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, noise_std, n_samples)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> x, y</span></span></code></pre></div>\n\n<p><strong>Core Logic Skeleton:</strong></p>\n<p>Here&#39;s the <code>SimpleLinearRegression</code> class with complete method signatures and detailed TODO comments that map directly to the algorithm steps described in the design section:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Simple Linear Regression implementation using closed-form ordinary least squares.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Union</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .utils </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> validate_regression_inputs, compute_r_squared, </span><span style=\"color:#79B8FF\">TOLERANCE</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SimpleLinearRegression</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Single-variable linear regression using the closed-form ordinary least squares solution.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This implementation demonstrates the mathematical foundation of linear regression</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    by computing optimal parameters directly through the normal equations rather</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    than iterative optimization.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Attributes:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        slope_ (float): Fitted slope parameter (set after calling fit)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        intercept_ (float): Fitted intercept parameter (set after calling fit)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        is_fitted_ (bool): Whether the model has been trained on data</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize unfitted model with default parameter values.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.slope_ </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.intercept_ </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.is_fitted_ </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> fit</span><span style=\"color:#E1E4E8\">(self, x: Union[</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">, np.ndarray], y: Union[</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">, np.ndarray]) -> </span><span style=\"color:#9ECBFF\">'SimpleLinearRegression'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Fit linear regression model using closed-form ordinary least squares.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Computes optimal slope and intercept parameters that minimize the sum of</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        squared residuals using the analytical normal equation solution.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            x: Input features (1D array-like)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            y: Target values (1D array-like)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            self: Fitted model instance for method chaining</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            ValueError: If input data is incompatible or has zero variance</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Convert inputs to numpy arrays and validate compatibility</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use validate_regression_inputs() from utils module</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Store as self._x_train and self._y_train for potential future use</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate sample means for both x and y</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # These will be used in both slope and intercept calculations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Store as x_mean and y_mean local variables</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compute slope using covariance formula</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Formula: slope = Σ((xi - x_mean) * (yi - y_mean)) / Σ((xi - x_mean)²)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Calculate numerator (covariance) and denominator (x variance) separately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check denominator against TOLERANCE to avoid division by zero</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle zero variance case</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # If denominator &#x3C; TOLERANCE, raise ValueError with descriptive message</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # \"Cannot fit linear regression: input features have zero variance\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Calculate intercept using the means</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Formula: intercept = y_mean - slope * x_mean</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # This ensures the line passes through the data centroid</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Store fitted parameters and set fitted flag</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Set self.slope_, self.intercept_, and self.is_fitted_ = True</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Return self to enable method chaining</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span><span style=\"color:#6A737D\">  # Remove this line when implementing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> predict</span><span style=\"color:#E1E4E8\">(self, x: Union[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">, np.ndarray]) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Generate predictions using the fitted linear model.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            x: Input features for prediction</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Predicted target values as numpy array</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            ValueError: If model has not been fitted</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if model has been fitted</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Raise ValueError(\"Model must be fitted before making predictions\") if not</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Convert input to numpy array and validate</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Handle both scalar and array inputs uniformly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check for finite values using np.isfinite()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Apply linear model equation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Formula: y_pred = slope_ * x + intercept_</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # NumPy broadcasting will handle array operations automatically</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return predictions as numpy array</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Ensure return type is consistent regardless of input type</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span><span style=\"color:#6A737D\">  # Remove this line when implementing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> score</span><span style=\"color:#E1E4E8\">(self, x: Union[</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">, np.ndarray], y: Union[</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">, np.ndarray]) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Calculate R-squared coefficient of determination for the model.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            x: Input features for evaluation</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            y: True target values for comparison</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            R-squared score (proportion of variance explained)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            ValueError: If model has not been fitted</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate that model is fitted</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check self.is_fitted_ flag and raise error if False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Generate predictions for the input data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use self.predict(x) to get predicted values</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate R-squared using utility function</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use compute_r_squared(y, y_pred) from utils module</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # This handles edge cases like zero variance in targets</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return R-squared value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Value will be between -infinity and 1.0</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Negative values indicate worse-than-mean performance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span><span style=\"color:#6A737D\">  # Remove this line when implementing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_params</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Get fitted model parameters.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Dictionary with slope, intercept, and fitting status</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'slope'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.slope_,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'intercept'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.intercept_,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'is_fitted'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.is_fitted_</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __repr__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"String representation of the model.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_fitted_:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"SimpleLinearRegression(slope=</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.slope_</span><span style=\"color:#F97583\">:.4f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, intercept=</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.intercept_</span><span style=\"color:#F97583\">:.4f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#9ECBFF\"> \"SimpleLinearRegression(unfitted)\"</span></span></code></pre></div>\n\n<p><strong>Language-Specific Hints:</strong></p>\n<ul>\n<li>Use <code>np.asarray(data, dtype=float)</code> to safely convert input data while preserving NumPy arrays and converting lists/scalars appropriately</li>\n<li>The <code>@</code> operator in Python 3.5+ provides matrix multiplication, but for simple linear regression, element-wise operations with <code>*</code> and <code>+</code> are more transparent</li>\n<li>NumPy&#39;s <code>np.mean()</code>, <code>np.sum()</code>, and <code>np.var()</code> functions automatically handle array operations efficiently</li>\n<li>Use <code>np.isfinite()</code> to check for both NaN and infinity values in a single operation</li>\n<li>The <code>typing</code> module&#39;s <code>Union</code> type helps document flexible input acceptance while maintaining type safety</li>\n</ul>\n<p><strong>Milestone Checkpoint:</strong></p>\n<p>After implementing the <code>SimpleLinearRegression</code> class, verify your implementation with these concrete tests:</p>\n<p><strong>Test 1: Perfect Linear Data</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Create perfect linear relationship</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">x </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.array([</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.array([</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">6</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">])  </span><span style=\"color:#6A737D\"># y = 2x + 0</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SimpleLinearRegression()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">model.fit(x, y)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> abs</span><span style=\"color:#E1E4E8\">(model.slope_ </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 2.0</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 1e-10</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> abs</span><span style=\"color:#E1E4E8\">(model.intercept_ </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 1e-10</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> abs</span><span style=\"color:#E1E4E8\">(model.score(x, y) </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 1e-10</span></span></code></pre></div>\n\n<p><strong>Test 2: Real-World Data</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Generate noisy data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">x, y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> generate_synthetic_data(</span><span style=\"color:#FFAB70\">n_samples</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">true_slope</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">3.5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">true_intercept</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2.1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">noise_std</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.5</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SimpleLinearRegression()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">model.fit(x, y)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Verify reasonable parameter recovery</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> abs</span><span style=\"color:#E1E4E8\">(model.slope_ </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 3.5</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 0.2</span><span style=\"color:#6A737D\">  # Allow for noise</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> abs</span><span style=\"color:#E1E4E8\">(model.intercept_ </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 2.1</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 0.5</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> model.score(x, y) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0.8</span><span style=\"color:#6A737D\">  # High R-squared expected with low noise</span></span></code></pre></div>\n\n<p><strong>Test 3: Error Handling</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test zero variance detection</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">x_constant </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.array([</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">y_varied </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.array([</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    model.fit(x_constant, y_varied)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Should raise ValueError for zero variance\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">except</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#9ECBFF\"> \"zero variance\"</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(e).lower()</span></span></code></pre></div>\n\n<p><strong>Signs of Success</strong>: Your implementation should handle all test cases above, produce reasonable parameter estimates for noisy data, and provide clear error messages for invalid inputs. The R-squared calculation should return 1.0 for perfect linear relationships and values between 0-1 for typical noisy datasets.</p>\n<p><strong>Signs of Problems</strong>: If parameters are wildly incorrect, check the covariance and variance calculations. If you get division by zero errors, verify the variance checking logic. If R-squared is negative for reasonable data, examine the prediction calculation for bugs.</p>\n<p><img src=\"/api/project/linear-regression/architecture-doc/asset?path=diagrams%2Fmodel-state-machine.svg\" alt=\"Model State Machine\"></p>\n<h2 id=\"gradient-descent-optimizer-component\">Gradient Descent Optimizer Component</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> M2: Gradient Descent</p>\n</blockquote>\n<p>This component implements iterative parameter optimization using gradient descent, providing an alternative to the closed-form solutions developed in Milestone 1. While the <code>SimpleLinearRegression</code> component finds the optimal parameters directly through mathematical calculation, the <code>GradientDescentRegression</code> component discovers them through an iterative process of gradual improvement. This approach becomes essential when closed-form solutions are computationally expensive or mathematically intractable, and it provides the foundation for understanding how most modern machine learning algorithms optimize their parameters.</p>\n<p>The gradient descent optimizer serves as both a practical alternative to closed-form solutions and an educational bridge to more advanced optimization techniques used in neural networks and other complex models. By implementing gradient descent from scratch, learners develop intuition for how machines &quot;learn&quot; through iterative improvement rather than direct calculation.</p>\n<h3 id=\"mental-model-hill-climbing-in-reverse\">Mental Model: Hill Climbing in Reverse</h3>\n<p>Think of gradient descent as <strong>finding the bottom of a valley while blindfolded</strong>. Imagine you&#39;re standing somewhere on a hilly landscape in dense fog, and your goal is to reach the lowest point in the valley. You can&#39;t see where you&#39;re going, but you can feel the slope of the ground under your feet.</p>\n<p>Your strategy is simple but effective: at each step, you feel around with your foot to determine which direction slopes downward most steeply, then take a step in that direction. You repeat this process—feel the slope, step downward—until you can&#39;t find any direction that goes further down. At that point, you&#39;ve likely reached the bottom of the valley (or at least a local depression).</p>\n<p>In our linear regression context:</p>\n<ul>\n<li>The <strong>landscape</strong> is our cost function surface, where each point represents a different combination of slope and intercept parameters</li>\n<li>The <strong>height at any point</strong> represents how badly our model performs with those parameters (measured by mean squared error)</li>\n<li>The <strong>valley bottom</strong> represents the parameter values that minimize our prediction errors</li>\n<li><strong>Feeling the slope</strong> corresponds to calculating gradients—the mathematical measure of how steeply the cost function changes as we adjust each parameter</li>\n<li><strong>Taking a step downward</strong> means updating our parameters by moving them in the direction that reduces cost most quickly</li>\n<li>The <strong>step size</strong> is our learning rate—how big a step we take in the downward direction</li>\n</ul>\n<p>This analogy reveals why gradient descent works: by consistently moving in the direction of steepest descent (negative gradient direction), we&#39;re guaranteed to reach a local minimum of our cost function. For linear regression, this local minimum is also the global minimum—there&#39;s only one valley bottom, and gradient descent will find it regardless of where we start.</p>\n<p>The blindfolded hiker analogy also explains common challenges: if you take steps that are too large (high learning rate), you might overshoot the valley bottom and end up bouncing back and forth across it, or even climbing out of the valley entirely. If your steps are too small (low learning rate), you&#39;ll eventually reach the bottom but it might take an impractically long time.</p>\n<h3 id=\"cost-function-implementation\">Cost Function Implementation</h3>\n<p>The <strong>cost function</strong> serves as our objective measure of how well our current parameters fit the data. In the hill-climbing analogy, the cost function defines the landscape we&#39;re navigating—it assigns a &quot;height&quot; (cost value) to every possible combination of parameters. Our goal is to find the parameter values that produce the lowest cost.</p>\n<p>For linear regression, we use <strong>mean squared error</strong> as our cost function because it has mathematical properties that make gradient descent both efficient and reliable. The mean squared error measures the average squared difference between our model&#39;s predictions and the actual target values in our training data.</p>\n<p>The mathematical formulation computes cost as the sum of squared residuals divided by the number of samples. For each data point, we calculate the prediction using our current slope and intercept parameters, subtract the actual target value to get the residual, square that residual to eliminate negative values and emphasize larger errors, then average across all data points to get a single cost value.</p>\n<table>\n<thead>\n<tr>\n<th>Cost Function Property</th>\n<th>Explanation</th>\n<th>Why Important</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Always Non-negative</strong></td>\n<td>Squared errors ensure cost ≥ 0</td>\n<td>Provides clear minimum at cost = 0</td>\n</tr>\n<tr>\n<td><strong>Convex Shape</strong></td>\n<td>Single global minimum, no local minima</td>\n<td>Guarantees gradient descent finds optimal solution</td>\n</tr>\n<tr>\n<td><strong>Smooth and Differentiable</strong></td>\n<td>Has well-defined gradients everywhere</td>\n<td>Enables reliable gradient calculation</td>\n</tr>\n<tr>\n<td><strong>Emphasizes Large Errors</strong></td>\n<td>Squaring makes big mistakes costlier than small ones</td>\n<td>Prioritizes fixing worst predictions first</td>\n</tr>\n<tr>\n<td><strong>Scale Invariant</strong></td>\n<td>Mean normalization makes cost comparable across datasets</td>\n<td>Enables consistent learning rate tuning</td>\n</tr>\n</tbody></table>\n<p>The cost function implementation must handle several practical considerations. First, it needs to be numerically stable when dealing with large prediction errors or extreme parameter values. Second, it should be computationally efficient since it will be called hundreds or thousands of times during training. Third, it must return consistent results across different data scales and sizes.</p>\n<blockquote>\n<p><strong>Key Design Insight</strong>: The choice of mean squared error over alternatives like mean absolute error or other loss functions isn&#39;t arbitrary. MSE produces gradients that are proportional to the error magnitude, which creates a natural &quot;acceleration&quot; effect—the algorithm takes larger steps when far from the optimum and smaller steps when close, leading to efficient convergence.</p>\n</blockquote>\n<p><strong>Cost Function Computational Flow:</strong></p>\n<ol>\n<li><strong>Parameter Input Validation</strong>: Verify that slope and intercept parameters are finite numbers, not NaN or infinite values that could destabilize computation</li>\n<li><strong>Prediction Generation</strong>: Apply the linear model equation y_pred = slope * x + intercept to generate predictions for all input features simultaneously using vectorized operations</li>\n<li><strong>Residual Calculation</strong>: Compute residuals by subtracting actual targets from predictions, creating a vector of prediction errors</li>\n<li><strong>Error Squaring</strong>: Square each residual to eliminate sign differences and emphasize larger errors quadratically</li>\n<li><strong>Mean Computation</strong>: Sum all squared errors and divide by the number of samples to get the mean squared error</li>\n<li><strong>Numerical Stability Check</strong>: Verify the result is finite and return an error flag if numerical issues are detected</li>\n</ol>\n<p>The cost function serves dual purposes in our system: it provides the objective value that we&#39;re trying to minimize, and it serves as a convergence indicator—when the cost stops decreasing significantly between iterations, we know we&#39;ve reached the optimal parameters.</p>\n<blockquote>\n<p><strong>Architecture Decision: Vectorized vs Loop-Based Cost Calculation</strong></p>\n<ul>\n<li><strong>Context</strong>: Cost function will be called thousands of times during training, making performance critical</li>\n<li><strong>Options Considered</strong>: Element-wise loops, list comprehension, vectorized NumPy operations</li>\n<li><strong>Decision</strong>: Use fully vectorized NumPy operations for all computations</li>\n<li><strong>Rationale</strong>: Vectorized operations are 10-100x faster than Python loops and more readable than manual optimization</li>\n<li><strong>Consequences</strong>: Requires NumPy dependency but provides significant performance gains and numerical stability</li>\n</ul>\n</blockquote>\n<h3 id=\"gradient-calculation\">Gradient Calculation</h3>\n<p><strong>Gradient calculation</strong> is the mathematical heart of the optimization process. While the cost function tells us how well our current parameters perform, the gradients tell us how to improve them. Gradients represent the partial derivatives of the cost function with respect to each parameter—they indicate both the direction and magnitude of change needed to reduce cost most effectively.</p>\n<p>Think of gradients as <strong>vector compass readings</strong> in our parameter space. For simple linear regression with two parameters (slope and intercept), the gradient is a two-dimensional vector where the first component indicates how much the cost function changes as we adjust the slope, and the second component indicates how much it changes as we adjust the intercept. The direction of this vector points toward the steepest uphill direction, so we move in the opposite direction to go downhill.</p>\n<p>The mathematical foundation relies on calculus and the chain rule. We derive the partial derivative of the mean squared error with respect to each parameter, producing formulas that can be computed directly from our current parameter values and training data. For linear regression, these derivatives have closed-form expressions that are computationally efficient to evaluate.</p>\n<table>\n<thead>\n<tr>\n<th>Parameter</th>\n<th>Gradient Formula</th>\n<th>Intuitive Meaning</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Slope</strong></td>\n<td><code>2 * mean(residuals * x_values)</code></td>\n<td>How much cost changes per unit change in slope</td>\n</tr>\n<tr>\n<td><strong>Intercept</strong></td>\n<td><code>2 * mean(residuals)</code></td>\n<td>How much cost changes per unit change in intercept</td>\n</tr>\n<tr>\n<td><strong>Magnitude</strong></td>\n<td><code>sqrt(slope_grad² + intercept_grad²)</code></td>\n<td>Overall strength of improvement signal</td>\n</tr>\n<tr>\n<td><strong>Direction</strong></td>\n<td><code>arctan(intercept_grad / slope_grad)</code></td>\n<td>Direction of steepest descent in parameter space</td>\n</tr>\n</tbody></table>\n<p>The gradient calculation process transforms our current prediction errors into specific parameter adjustment recommendations. When residuals are predominantly positive (predictions too low), the gradients will recommend increasing the slope and intercept to raise predictions. When residuals are predominantly negative (predictions too high), the gradients will recommend decreasing parameters to lower predictions.</p>\n<p><strong>Gradient Computation Algorithm:</strong></p>\n<ol>\n<li><strong>Current Predictions</strong>: Use existing slope and intercept parameters to generate predictions for all training examples</li>\n<li><strong>Residual Vector</strong>: Compute prediction errors by subtracting actual targets from predictions</li>\n<li><strong>Slope Gradient</strong>: Calculate the correlation between residuals and input features, scaled by the mean—this measures how much the slope should change</li>\n<li><strong>Intercept Gradient</strong>: Calculate the mean of residuals—this measures how much the intercept should change</li>\n<li><strong>Gradient Scaling</strong>: Apply the 2/n scaling factor (where n is the number of samples) to match the cost function derivative</li>\n<li><strong>Numerical Validation</strong>: Check that gradients are finite and within reasonable bounds to prevent numerical instability</li>\n</ol>\n<p>The mathematical elegance of linear regression gradients lies in their interpretability. The slope gradient directly measures the linear correlation between prediction errors and input features—if larger input values tend to have larger errors, the slope needs adjustment. The intercept gradient measures the overall bias in predictions—if predictions are consistently too high or too low, the intercept needs adjustment.</p>\n<blockquote>\n<p><strong>Critical Implementation Detail</strong>: Gradient computation must use the same prediction method as the cost function to ensure mathematical consistency. Any discrepancy between how predictions are calculated in the two functions will cause the gradients to point in incorrect directions, preventing convergence.</p>\n</blockquote>\n<p><strong>Gradient Numerical Properties:</strong></p>\n<p>The gradient calculation must handle several numerical considerations that can affect training stability. First, gradients can become very large when parameters are far from optimal, potentially causing parameter updates to overshoot. Second, gradients approach zero as we near the optimum, which can cause training to slow down prematurely if we don&#39;t choose convergence criteria carefully. Third, the relative magnitudes of slope and intercept gradients depend on the scale of input features, which affects learning rate selection.</p>\n<table>\n<thead>\n<tr>\n<th>Gradient Magnitude Range</th>\n<th>Interpretation</th>\n<th>Recommended Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Very Large (&gt;1000)</strong></td>\n<td>Parameters far from optimum</td>\n<td>Reduce learning rate or check for numerical issues</td>\n</tr>\n<tr>\n<td><strong>Moderate (1-1000)</strong></td>\n<td>Normal optimization progress</td>\n<td>Continue with current learning rate</td>\n</tr>\n<tr>\n<td><strong>Small (0.01-1)</strong></td>\n<td>Approaching convergence</td>\n<td>Monitor for convergence criteria</td>\n</tr>\n<tr>\n<td><strong>Very Small (&lt;0.01)</strong></td>\n<td>Near optimal or stuck</td>\n<td>Check convergence or consider stopping</td>\n</tr>\n<tr>\n<td><strong>Zero</strong></td>\n<td>At optimum or numerical underflow</td>\n<td>Stop training—convergence achieved</td>\n</tr>\n</tbody></table>\n<h3 id=\"parameter-update-rules\">Parameter Update Rules</h3>\n<p>The <strong>parameter update rule</strong> is where gradient descent takes its &quot;step downhill&quot; in the parameter landscape. This rule combines the gradient information with a learning rate to determine how much to adjust each parameter in each iteration. The update rule represents the core learning mechanism—how the algorithm uses feedback from prediction errors to improve its parameter estimates.</p>\n<p>The fundamental update equation follows the pattern: new_parameter = old_parameter - learning_rate * gradient. The negative sign is crucial because gradients point in the direction of steepest ascent (uphill), but we want to move toward the minimum (downhill). The learning rate scales the gradient to control step size—it determines how aggressively we move in the gradient direction.</p>\n<p><strong>Parameter Update Process:</strong></p>\n<ol>\n<li><strong>Gradient Computation</strong>: Calculate current gradients for slope and intercept using the gradient calculation process described above</li>\n<li><strong>Learning Rate Application</strong>: Multiply each gradient by the learning rate to determine the magnitude of parameter adjustment</li>\n<li><strong>Parameter Adjustment</strong>: Subtract the scaled gradients from current parameters to move in the direction of decreasing cost</li>\n<li><strong>Parameter Validation</strong>: Check that updated parameters remain within reasonable bounds and haven&#39;t become infinite or NaN</li>\n<li><strong>History Recording</strong>: Store the new parameters and their associated cost for convergence monitoring and debugging</li>\n<li><strong>Iteration Advancement</strong>: Increment the iteration counter and prepare for the next optimization step</li>\n</ol>\n<p>The learning rate serves as the most critical hyperparameter in gradient descent optimization. It controls the trade-off between convergence speed and stability. A learning rate that&#39;s too high causes the algorithm to take steps that are too large, potentially overshooting the minimum and causing the cost to oscillate or even increase. A learning rate that&#39;s too low causes the algorithm to take tiny steps, requiring many iterations to reach the optimum and potentially getting stuck in flat regions of the cost surface.</p>\n<table>\n<thead>\n<tr>\n<th>Learning Rate Range</th>\n<th>Behavior</th>\n<th>Pros</th>\n<th>Cons</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Very High (&gt;1.0)</strong></td>\n<td>Large steps, often overshoots</td>\n<td>Fast when far from optimum</td>\n<td>Unstable, may diverge</td>\n</tr>\n<tr>\n<td><strong>High (0.1-1.0)</strong></td>\n<td>Moderate steps, some oscillation</td>\n<td>Good initial progress</td>\n<td>May overshoot near optimum</td>\n</tr>\n<tr>\n<td><strong>Medium (0.01-0.1)</strong></td>\n<td>Steady progress with minor oscillation</td>\n<td>Reliable convergence</td>\n<td>Moderate convergence speed</td>\n</tr>\n<tr>\n<td><strong>Low (0.001-0.01)</strong></td>\n<td>Small steps, smooth progress</td>\n<td>Stable, precise convergence</td>\n<td>Slow convergence</td>\n</tr>\n<tr>\n<td><strong>Very Low (&lt;0.001)</strong></td>\n<td>Tiny steps, very slow progress</td>\n<td>Maximum stability</td>\n<td>Impractically slow</td>\n</tr>\n</tbody></table>\n<p><strong>Adaptive Learning Rate Strategies:</strong></p>\n<p>While our implementation uses a fixed learning rate for simplicity, understanding adaptive strategies provides insight into more advanced optimization. The key insight is that optimal learning rates change during training—we often want larger steps when far from the optimum and smaller steps when close to it.</p>\n<blockquote>\n<p><strong>Architecture Decision: Fixed vs Adaptive Learning Rate</strong></p>\n<ul>\n<li><strong>Context</strong>: Learning rate significantly affects convergence speed and stability, but adaptive methods add complexity</li>\n<li><strong>Options Considered</strong>: Fixed rate, linear decay, exponential decay, adaptive gradient methods</li>\n<li><strong>Decision</strong>: Use fixed learning rate with manual tuning guidance</li>\n<li><strong>Rationale</strong>: Provides clear understanding of learning rate effects without obscuring core gradient descent concepts</li>\n<li><strong>Consequences</strong>: Requires manual tuning but offers transparent, predictable behavior ideal for learning</li>\n</ul>\n</blockquote>\n<p>The parameter update rule also incorporates momentum concepts implicitly through the iterative process. Each parameter update builds on previous updates, creating a form of &quot;momentum&quot; that helps the algorithm maintain direction and speed through flat regions of the cost surface. This momentum effect becomes more pronounced when the gradient consistently points in similar directions across iterations.</p>\n<p><strong>Parameter Update Numerical Considerations:</strong></p>\n<p>The update rule must handle numerical precision issues that can arise during optimization. Parameters can grow very large if gradients consistently point in one direction, potentially causing overflow. Conversely, very small gradients combined with low learning rates can cause underflow, where parameter changes become too small to register in floating-point arithmetic.</p>\n<table>\n<thead>\n<tr>\n<th>Numerical Issue</th>\n<th>Symptoms</th>\n<th>Detection Method</th>\n<th>Prevention Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Parameter Overflow</strong></td>\n<td>Parameters become infinite</td>\n<td>Check for inf/NaN after update</td>\n<td>Gradient clipping, learning rate reduction</td>\n</tr>\n<tr>\n<td><strong>Parameter Underflow</strong></td>\n<td>No parameter change despite non-zero gradients</td>\n<td>Monitor parameter change magnitude</td>\n<td>Increase learning rate, check precision</td>\n</tr>\n<tr>\n<td><strong>Oscillating Parameters</strong></td>\n<td>Parameters alternate between values</td>\n<td>Track parameter history variance</td>\n<td>Reduce learning rate, check for numerical instability</td>\n</tr>\n<tr>\n<td><strong>Stuck Parameters</strong></td>\n<td>Parameters stop changing prematurely</td>\n<td>Monitor gradient magnitude and parameter changes</td>\n<td>Adjust convergence criteria, increase learning rate</td>\n</tr>\n</tbody></table>\n<h3 id=\"convergence-detection\">Convergence Detection</h3>\n<p><strong>Convergence detection</strong> determines when gradient descent has found the optimal parameters and training should stop. This mechanism prevents unnecessary computation while ensuring we&#39;ve reached a satisfactory solution. Effective convergence detection balances between stopping too early (suboptimal results) and running too long (wasted computation).</p>\n<p>The primary convergence criterion monitors the <strong>cost improvement</strong> between consecutive iterations. When the reduction in cost falls below a specified tolerance threshold for several consecutive iterations, we consider the algorithm to have converged. This approach recognizes that near the optimum, cost improvements become progressively smaller until they&#39;re negligible for practical purposes.</p>\n<p><strong>Multi-Criteria Convergence Strategy:</strong></p>\n<p>Robust convergence detection uses multiple criteria to handle different convergence scenarios. A single criterion can fail in edge cases—for example, cost-based detection might fail when cost oscillates slightly, while gradient-based detection might fail due to numerical precision issues.</p>\n<table>\n<thead>\n<tr>\n<th>Convergence Criterion</th>\n<th>Formula</th>\n<th>Purpose</th>\n<th>Failure Modes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Cost Improvement</strong></td>\n<td><code>abs(cost[i] - cost[i-1]) &lt; tolerance</code></td>\n<td>Detects when cost stops decreasing</td>\n<td>Fails with oscillating costs</td>\n</tr>\n<tr>\n<td><strong>Relative Cost Change</strong></td>\n<td><code>abs(cost[i] - cost[i-1]) / cost[i-1] &lt; tolerance</code></td>\n<td>Handles different cost scales</td>\n<td>Fails when cost approaches zero</td>\n</tr>\n<tr>\n<td><strong>Gradient Magnitude</strong></td>\n<td><code>sqrt(slope_grad² + intercept_grad²) &lt; tolerance</code></td>\n<td>Detects when gradients approach zero</td>\n<td>Sensitive to numerical precision</td>\n</tr>\n<tr>\n<td><strong>Parameter Stability</strong></td>\n<td><code>max(abs(param_change)) &lt; tolerance</code></td>\n<td>Detects when parameters stop changing</td>\n<td>Can miss slow convergence</td>\n</tr>\n<tr>\n<td><strong>Maximum Iterations</strong></td>\n<td><code>iteration_count &gt;= max_iterations</code></td>\n<td>Prevents infinite loops</td>\n<td>May stop before convergence</td>\n</tr>\n</tbody></table>\n<p>The convergence detection algorithm combines these criteria using a logical OR relationship—convergence is declared when any criterion is satisfied. This approach provides robustness against individual criterion failures while maintaining clear stopping conditions.</p>\n<p><strong>Convergence Detection Algorithm:</strong></p>\n<ol>\n<li><strong>Cost History Analysis</strong>: Compare current cost with previous iteration&#39;s cost to measure improvement magnitude</li>\n<li><strong>Gradient Magnitude Check</strong>: Calculate the L2 norm of current gradients to detect when they approach zero</li>\n<li><strong>Parameter Change Analysis</strong>: Measure how much parameters changed in the current iteration</li>\n<li><strong>Tolerance Comparison</strong>: Compare each metric against its respective tolerance threshold</li>\n<li><strong>Consecutive Iteration Tracking</strong>: Require convergence criteria to be satisfied for multiple consecutive iterations to avoid false positives</li>\n<li><strong>Iteration Limit Check</strong>: Enforce maximum iteration limit to prevent infinite loops in pathological cases</li>\n<li><strong>Convergence Declaration</strong>: Set convergence flag and record convergence reason for diagnostic purposes</li>\n</ol>\n<blockquote>\n<p><strong>Key Design Insight</strong>: Convergence detection must account for the fact that gradient descent can exhibit different behaviors near the optimum. Some problems converge smoothly with monotonically decreasing costs, while others oscillate slightly around the minimum due to discrete parameter updates. Requiring consecutive iterations to meet criteria helps distinguish true convergence from temporary fluctuations.</p>\n</blockquote>\n<p><strong>Premature vs Delayed Convergence:</strong></p>\n<p>Convergence detection involves balancing two types of errors. Premature convergence stops training before reaching the true optimum, resulting in suboptimal model performance. Delayed convergence continues training long after reaching practical optimality, wasting computational resources without meaningful improvement.</p>\n<table>\n<thead>\n<tr>\n<th>Convergence Type</th>\n<th>Symptoms</th>\n<th>Causes</th>\n<th>Solutions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Premature</strong></td>\n<td>Training stops early, cost could decrease further</td>\n<td>Tolerance too loose, insufficient consecutive checks</td>\n<td>Tighten tolerance, require more consecutive iterations</td>\n</tr>\n<tr>\n<td><strong>Delayed</strong></td>\n<td>Training continues with minimal improvement</td>\n<td>Tolerance too strict, numerical precision limits</td>\n<td>Relax tolerance, add maximum iteration limits</td>\n</tr>\n<tr>\n<td><strong>False Positive</strong></td>\n<td>Declares convergence then cost decreases again</td>\n<td>Temporary flat regions, oscillating costs</td>\n<td>Require consecutive iteration criteria</td>\n</tr>\n<tr>\n<td><strong>Never Converges</strong></td>\n<td>Training runs indefinitely</td>\n<td>Learning rate too high, numerical instability</td>\n<td>Implement maximum iterations, check gradients</td>\n</tr>\n</tbody></table>\n<p>The convergence detection system records detailed information about why training stopped, which becomes crucial for debugging and hyperparameter tuning. This metadata helps users understand whether their model achieved genuine convergence or stopped due to limits and constraints.</p>\n<blockquote>\n<p><strong>Architecture Decision: Single vs Multiple Convergence Criteria</strong></p>\n<ul>\n<li><strong>Context</strong>: Different optimization scenarios may satisfy different convergence conditions first</li>\n<li><strong>Options Considered</strong>: Cost-only, gradient-only, parameter-change-only, combined criteria</li>\n<li><strong>Decision</strong>: Implement multiple criteria with logical OR combination</li>\n<li><strong>Rationale</strong>: Provides robustness against individual criterion failures and handles various convergence patterns</li>\n<li><strong>Consequences</strong>: More complex logic but significantly more reliable convergence detection across different problem types</li>\n</ul>\n</blockquote>\n<h3 id=\"architecture-decision-records\">Architecture Decision Records</h3>\n<p>The gradient descent optimizer component involves several critical design decisions that affect both educational value and practical performance. Each decision represents a trade-off between simplicity, performance, numerical stability, and pedagogical clarity.</p>\n<blockquote>\n<p><strong>Decision: Batch vs Stochastic vs Mini-Batch Gradient Descent</strong></p>\n<ul>\n<li><strong>Context</strong>: Different gradient computation strategies offer different convergence properties and computational characteristics</li>\n<li><strong>Options Considered</strong>: Batch gradient descent (full dataset), stochastic gradient descent (single sample), mini-batch gradient descent (subset of samples)</li>\n<li><strong>Decision</strong>: Implement batch gradient descent using the full dataset for each gradient computation</li>\n<li><strong>Rationale</strong>: Provides deterministic, smooth convergence that&#39;s easier to understand and debug; eliminates random variation that could confuse learning process; ensures gradients point in true direction of steepest descent</li>\n<li><strong>Consequences</strong>: Higher memory usage and computational cost per iteration but more predictable convergence behavior ideal for educational purposes</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Gradient Descent Variant</th>\n<th>Memory Usage</th>\n<th>Computational Cost Per Iteration</th>\n<th>Convergence Behavior</th>\n<th>Educational Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Batch (Chosen)</strong></td>\n<td>High (full dataset)</td>\n<td>High</td>\n<td>Smooth, deterministic</td>\n<td>Excellent for learning</td>\n</tr>\n<tr>\n<td><strong>Stochastic</strong></td>\n<td>Low (single sample)</td>\n<td>Low</td>\n<td>Noisy, faster initial progress</td>\n<td>Confusing for beginners</td>\n</tr>\n<tr>\n<td><strong>Mini-batch</strong></td>\n<td>Medium (subset)</td>\n<td>Medium</td>\n<td>Balanced noise/smoothness</td>\n<td>Good but more complex</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Manual vs Automatic Learning Rate Selection</strong></p>\n<ul>\n<li><strong>Context</strong>: Learning rate significantly affects convergence but optimal values depend on data characteristics</li>\n<li><strong>Options Considered</strong>: Fixed manual rate, automatic line search, adaptive rate methods, learning rate schedules</li>\n<li><strong>Decision</strong>: Use fixed manual learning rate with clear guidance for selection</li>\n<li><strong>Rationale</strong>: Forces learners to understand learning rate effects through experimentation; provides transparent, predictable behavior; avoids obscuring core concepts with complex adaptive algorithms</li>\n<li><strong>Consequences</strong>: Requires manual tuning and may need adjustment for different datasets, but provides clear cause-and-effect understanding</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Numerical Precision Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Floating-point arithmetic can cause numerical instability in gradient computations and parameter updates</li>\n<li><strong>Options Considered</strong>: Single precision (float32), double precision (float64), arbitrary precision arithmetic</li>\n<li><strong>Decision</strong>: Use double precision (float64) throughout with explicit overflow/underflow checking</li>\n<li><strong>Rationale</strong>: Provides sufficient precision for educational datasets while remaining computationally efficient; reduces likelihood of numerical issues that could confuse learning process</li>\n<li><strong>Consequences</strong>: Higher memory usage but significantly more stable numerical behavior and clearer debugging</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Gradient Computation Approach</strong></p>\n<ul>\n<li><strong>Context</strong>: Gradients can be computed analytically, numerically, or through automatic differentiation</li>\n<li><strong>Options Considered</strong>: Analytical derivatives (manual derivation), numerical differentiation (finite differences), automatic differentiation frameworks</li>\n<li><strong>Decision</strong>: Implement analytical derivatives with explicit mathematical formulas</li>\n<li><strong>Rationale</strong>: Provides exact gradients without approximation error; forces understanding of underlying mathematics; offers maximum computational efficiency; enables clear connection between calculus and implementation</li>\n<li><strong>Consequences</strong>: Requires manual derivation and implementation of derivative formulas but provides deepest mathematical understanding</li>\n</ul>\n</blockquote>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>Gradient descent implementation involves several subtle issues that frequently cause problems for learners. Understanding these pitfalls and their solutions is crucial for successful implementation and debugging.</p>\n<p>⚠️ <strong>Pitfall: Learning Rate Too High Causing Divergence</strong></p>\n<p><strong>Description</strong>: The most common failure mode occurs when the learning rate is set too high, causing parameter updates to overshoot the minimum. Instead of converging to optimal parameters, the cost function value increases with each iteration, often growing exponentially until parameters become infinite.</p>\n<p><strong>Why It&#39;s Wrong</strong>: Large learning rates cause the algorithm to take steps that are too big, jumping over the minimum point in the cost function. Each step moves further from the optimum rather than closer to it, creating a divergent spiral where parameters grow without bound.</p>\n<p><strong>How to Detect</strong>: Monitor cost function values during training—if costs increase consistently or oscillate wildly between very large values, the learning rate is too high. Parameter values themselves may become extremely large (&gt;1e6) or infinite.</p>\n<p><strong>How to Fix</strong>: Reduce learning rate by factors of 10 until convergence behavior appears. Start with learning rates around 0.001 and increase gradually if convergence is too slow. Implement gradient clipping to prevent parameter explosions.</p>\n<p>⚠️ <strong>Pitfall: Learning Rate Too Low Causing Premature Stopping</strong></p>\n<p><strong>Description</strong>: Excessively low learning rates cause gradient descent to make tiny progress in each iteration. The algorithm may appear to stop improving while still far from the true optimum, leading to premature convergence declarations and suboptimal model performance.</p>\n<p><strong>Why It&#39;s Wrong</strong>: Tiny parameter updates mean the algorithm requires thousands or tens of thousands of iterations to reach the optimum. Combined with loose convergence criteria, this often triggers early stopping when improvement per iteration falls below the threshold despite significant potential improvement remaining.</p>\n<p><strong>How to Detect</strong>: Training stops after relatively few iterations with large gradient magnitudes still present. Cost improvements are extremely small but consistent, suggesting more progress is possible. Final cost values are significantly higher than expected.</p>\n<p><strong>How to Fix</strong>: Increase learning rate gradually until reasonable convergence speed appears. Tighten convergence criteria to require smaller improvements before stopping. Monitor both cost improvement and gradient magnitude to distinguish true convergence from slow progress.</p>\n<p>⚠️ <strong>Pitfall: Forgetting to Normalize Features</strong></p>\n<p><strong>Description</strong>: When input features have very different scales (e.g., age in years vs income in dollars), the cost function surface becomes elongated and narrow. Gradient descent struggles with such surfaces because optimal learning rates for different parameters differ dramatically.</p>\n<p><strong>Why It&#39;s Wrong</strong>: Features with large scales dominate gradient calculations, causing the algorithm to focus on adjusting parameters for high-scale features while largely ignoring low-scale features. This creates inefficient zigzag paths toward the minimum instead of direct descent.</p>\n<p><strong>How to Detect</strong>: Some parameters change rapidly while others barely change at all during training. Convergence is much slower than expected, with cost decreasing in a zigzag pattern rather than smooth descent. Different features have vastly different ranges in the dataset.</p>\n<p><strong>How to Fix</strong>: Apply z-score normalization to all features before training, scaling each to zero mean and unit variance. Store normalization parameters to apply the same transformation to prediction data. Consider the impact of normalization on parameter interpretation.</p>\n<p>⚠️ <strong>Pitfall: Incorrect Gradient Implementation</strong></p>\n<p><strong>Description</strong>: Mathematical errors in gradient calculation formulas cause the algorithm to move in wrong directions, preventing convergence or causing divergence. Common errors include sign mistakes, missing scaling factors, or inconsistent prediction calculations between cost and gradient functions.</p>\n<p><strong>Why It&#39;s Wrong</strong>: Incorrect gradients point in suboptimal directions, causing the algorithm to move away from the minimum instead of toward it. Even small errors can prevent convergence or cause extremely slow progress, making the optimization process unreliable.</p>\n<p><strong>How to Detect</strong>: Cost function fails to decrease consistently despite reasonable learning rates. Parameters move in unexpected directions (e.g., slope increases when it should decrease). Gradient magnitudes don&#39;t correlate with expected improvement directions.</p>\n<p><strong>How to Fix</strong>: Verify gradient formulas through numerical differentiation—compute gradients using finite differences and compare with analytical formulas. Test gradient calculations on simple datasets where optimal parameters are known. Ensure prediction calculations are identical in cost and gradient functions.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Convergence Criteria</strong></p>\n<p><strong>Description</strong>: Poorly chosen convergence thresholds either stop training prematurely or allow training to continue indefinitely without meaningful improvement. Single-criterion convergence detection fails in edge cases like oscillating costs or numerical precision limits.</p>\n<p><strong>Why It&#39;s Wrong</strong>: Premature stopping produces suboptimal models, while delayed stopping wastes computation. Inadequate criteria fail to distinguish between true convergence, temporary plateaus, and numerical precision limits, leading to unreliable training outcomes.</p>\n<p><strong>How to Detect</strong>: Training stops while significant improvement potential remains, or training continues indefinitely with minimal progress. Convergence behavior varies dramatically across similar datasets or repeated runs.</p>\n<p><strong>How to Fix</strong>: Implement multiple convergence criteria with appropriate thresholds for cost improvement, gradient magnitude, and parameter stability. Require criteria to be satisfied for consecutive iterations to avoid false positives. Always enforce maximum iteration limits as safety nets.</p>\n<p>⚠️ <strong>Pitfall: Numerical Instability from Poor Parameter Initialization</strong></p>\n<p><strong>Description</strong>: Starting parameters that are too large can cause initial cost calculations to overflow, while parameters that are too small may cause underflow in gradient calculations. Poor initialization can also place the algorithm in flat regions of the cost surface.</p>\n<p><strong>Why It&#39;s Wrong</strong>: Extreme initial parameter values can cause numerical overflow or underflow in the first few iterations, preventing the algorithm from getting started. Even if numerically stable, poor initialization can significantly slow convergence or cause the algorithm to get stuck.</p>\n<p><strong>How to Detect</strong>: Initial cost calculations produce infinite or NaN values. Training fails to start or makes no progress in early iterations despite reasonable learning rates. Parameters remain close to initial values throughout training.</p>\n<p><strong>How to Fix</strong>: Initialize parameters to small random values (typically between -0.1 and 0.1) or use domain knowledge for reasonable starting points. For linear regression, initializing slope to zero and intercept to the mean of target values often works well. Implement bounds checking for parameter initialization.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The gradient descent optimizer bridges mathematical optimization theory with practical machine learning implementation. This section provides complete, working infrastructure code and detailed implementation skeletons that help learners focus on core gradient descent concepts while handling necessary but peripheral concerns like data validation and numerical stability.</p>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Numerical Operations</strong></td>\n<td><code>numpy</code> arrays with basic operations</td>\n<td><code>scipy.optimize</code> for comparison and validation</td>\n</tr>\n<tr>\n<td><strong>Gradient Calculation</strong></td>\n<td>Manual analytical derivatives</td>\n<td><code>autograd</code> or <code>jax</code> for automatic differentiation</td>\n</tr>\n<tr>\n<td><strong>Plotting and Visualization</strong></td>\n<td><code>matplotlib</code> for cost curves and parameter traces</td>\n<td><code>plotly</code> for interactive optimization visualization</td>\n</tr>\n<tr>\n<td><strong>Data Storage</strong></td>\n<td>Python lists for training history</td>\n<td><code>pandas.DataFrame</code> for structured history tracking</td>\n</tr>\n<tr>\n<td><strong>Numerical Stability</strong></td>\n<td>Manual overflow/underflow checks</td>\n<td><code>numpy.errstate</code> context managers</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  src/\n    models/\n      gradient_descent_regressor.py    ← Core implementation (learner implements)\n      base_regressor.py               ← Base class with shared functionality\n    optimizers/\n      gradient_descent.py             ← Generic optimizer (starter code provided)\n      convergence.py                  ← Convergence detection (starter code provided)\n    utils/\n      numerical_stability.py          ← Overflow/underflow handling (complete)\n      visualization.py                ← Training progress plots (complete)\n      metrics.py                      ← Cost function and evaluation (starter code)\n  tests/\n    test_gradient_descent.py          ← Unit tests for core functionality\n    test_convergence.py               ← Convergence detection tests\n  examples/\n    gradient_descent_demo.py          ← Complete working example</code></pre></div>\n\n<p><strong>Complete Infrastructure: Numerical Stability Utilities</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Numerical stability utilities for gradient descent optimization.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Handles overflow, underflow, and precision issues.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tuple, Optional, Union</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> warnings</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Global constants for numerical stability</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">TOLERANCE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-10</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MAX_PARAMETER_VALUE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e8</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MIN_GRADIENT_NORM</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-12</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MAX_GRADIENT_NORM</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e8</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> check_numerical_stability</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parameters: np.ndarray,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gradients: np.ndarray,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cost: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parameter_names: Optional[</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">) -> Tuple[</span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Comprehensive numerical stability check for gradient descent.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        (is_stable, error_message): Stability status and diagnostic message</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    param_names </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parameter_names </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"param_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(parameters))]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Check for NaN or infinite parameters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> np.any(</span><span style=\"color:#F97583\">~</span><span style=\"color:#E1E4E8\">np.isfinite(parameters)):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        invalid_params </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [param_names[i] </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(parameters)) </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                         if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> np.isfinite(parameters[i])]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Non-finite parameters detected: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">invalid_params</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Check for parameter overflow</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> np.any(np.abs(parameters) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> MAX_PARAMETER_VALUE</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        large_params </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [param_names[i] </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(parameters))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                       if</span><span style=\"color:#E1E4E8\"> np.abs(parameters[i]) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> MAX_PARAMETER_VALUE</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Parameter overflow detected: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">large_params</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Check for NaN or infinite gradients</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> np.any(</span><span style=\"color:#F97583\">~</span><span style=\"color:#E1E4E8\">np.isfinite(gradients)):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        invalid_grads </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">param_names[i]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">_grad\"</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(gradients))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> np.isfinite(gradients[i])]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Non-finite gradients detected: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">invalid_grads</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Check for gradient explosion</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gradient_norm </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.linalg.norm(gradients)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> gradient_norm </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> MAX_GRADIENT_NORM</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Gradient explosion: norm=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">gradient_norm</span><span style=\"color:#F97583\">:.2e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Check for cost function issues</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> np.isfinite(cost) </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> cost </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Invalid cost value: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">cost</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Numerically stable\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> safe_parameter_update</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parameters: np.ndarray,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gradients: np.ndarray,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    learning_rate: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">) -> Tuple[np.ndarray, </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Safely update parameters with overflow protection.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        (new_parameters, success, message)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Calculate proposed parameter changes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parameter_changes </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> learning_rate </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> gradients</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Check if changes are too large</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> np.any(np.abs(parameter_changes) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> MAX_PARAMETER_VALUE</span><span style=\"color:#F97583\"> /</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> parameters, </span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Parameter change too large - reduce learning rate\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Apply updates</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    new_parameters </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parameters </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> parameter_changes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Verify results are finite and reasonable</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> np.any(</span><span style=\"color:#F97583\">~</span><span style=\"color:#E1E4E8\">np.isfinite(new_parameters)):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> parameters, </span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Parameter update produced non-finite values\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> np.any(np.abs(new_parameters) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> MAX_PARAMETER_VALUE</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> parameters, </span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Parameter update caused overflow\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> new_parameters, </span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Update successful\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> initialize_parameters_safely</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    n_parameters: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    initialization_scale: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.01</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    random_seed: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Initialize parameters with safe random values.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> random_seed </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        np.random.seed(random_seed)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> np.random.normal(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, initialization_scale, n_parameters)</span></span></code></pre></div>\n\n<p><strong>Complete Infrastructure: Convergence Detection</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Convergence detection for gradient descent optimization.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Implements multiple criteria with robust checking.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Tuple, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ConvergenceConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for convergence detection criteria.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cost_tolerance: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-6</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gradient_tolerance: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-6</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parameter_tolerance: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-8</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    relative_tolerance: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-6</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    consecutive_iterations: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_iterations: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1000</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ConvergenceDetector</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Multi-criteria convergence detection for gradient descent.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: ConvergenceConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cost_history: List[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.parameter_history: List[np.ndarray] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.gradient_history: List[np.ndarray] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.consecutive_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.converged </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.convergence_reason </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> check_convergence</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        self,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cost: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        parameters: np.ndarray,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        gradients: np.ndarray,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        iteration: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ) -> Tuple[</span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Check all convergence criteria and return decision.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            (converged, reason): Convergence status and reason</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Store current state</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cost_history.append(cost)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.parameter_history.append(parameters.copy())</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.gradient_history.append(gradients.copy())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check maximum iterations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> iteration </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.max_iterations:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.converged </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.convergence_reason </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Maximum iterations (</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.config.max_iterations</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">) reached\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.convergence_reason</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Need at least 2 points for improvement-based criteria</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.cost_history) </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Insufficient history for convergence check\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check each convergence criterion</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        criteria_met </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Cost improvement criterion</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cost_improvement </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> abs</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.cost_history[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.cost_history[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> cost_improvement </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.cost_tolerance:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            criteria_met.append(</span><span style=\"color:#9ECBFF\">\"cost_improvement\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Relative cost improvement criterion</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.cost_history[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            relative_improvement </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> cost_improvement </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.cost_history[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> relative_improvement </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.relative_tolerance:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                criteria_met.append(</span><span style=\"color:#9ECBFF\">\"relative_cost_improvement\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Gradient magnitude criterion</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        gradient_norm </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.linalg.norm(gradients)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> gradient_norm </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.gradient_tolerance:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            criteria_met.append(</span><span style=\"color:#9ECBFF\">\"gradient_magnitude\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Parameter change criterion</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.parameter_history) </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            parameter_change </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.linalg.norm(</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.parameter_history[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.parameter_history[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> parameter_change </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.parameter_tolerance:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                criteria_met.append(</span><span style=\"color:#9ECBFF\">\"parameter_stability\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check if any criteria are met</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> criteria_met:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.consecutive_count </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.consecutive_count </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.consecutive_iterations:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.converged </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.convergence_reason </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Criteria satisfied for </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.consecutive_count</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> consecutive iterations: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">criteria_met</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.convergence_reason</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Convergence criteria met (</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">criteria_met</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">) but need </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.config.consecutive_iterations </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.consecutive_count</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> more consecutive iterations\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.consecutive_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"No convergence criteria satisfied. Cost improvement: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">cost_improvement</span><span style=\"color:#F97583\">:.2e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, Gradient norm: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">gradient_norm</span><span style=\"color:#F97583\">:.2e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_convergence_summary</span><span style=\"color:#E1E4E8\">(self) -> Dict:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return detailed convergence information for debugging.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"converged\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.converged,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"reason\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.convergence_reason,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"total_iterations\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.cost_history),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"final_cost\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.cost_history[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.cost_history </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"final_gradient_norm\"</span><span style=\"color:#E1E4E8\">: np.linalg.norm(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.gradient_history[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]) </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.gradient_history </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"cost_history\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.cost_history.copy(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"consecutive_count\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.consecutive_count</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span></code></pre></div>\n\n<p><strong>Core Implementation Skeleton: GradientDescentRegression Class</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Gradient descent linear regression implementation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Learners implement the core optimization logic.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tuple, List, Optional, Dict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .base_regressor </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> BaseRegressor  </span><span style=\"color:#6A737D\"># Provided base class</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .convergence </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ConvergenceDetector, ConvergenceConfig</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .numerical_stability </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> check_numerical_stability, safe_parameter_update</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> GradientDescentRegression</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">BaseRegressor</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Linear regression using gradient descent optimization.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This class learns the relationship y = slope * x + intercept by iteratively</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    minimizing mean squared error using gradient descent.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        self,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        learning_rate: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.01</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        max_iterations: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tolerance: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-6</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        random_seed: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Initialize gradient descent regressor.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            learning_rate: Step size for parameter updates</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            max_iterations: Maximum number of optimization iterations</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            tolerance: Convergence tolerance for cost improvement</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            random_seed: Random seed for parameter initialization</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Initialize regression parameters</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.slope_ </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.intercept_ </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.is_fitted_ </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Store hyperparameters</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.learning_rate </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> learning_rate</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_iterations </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_iterations</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tolerance </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tolerance</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.random_seed </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> random_seed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Training history for analysis and debugging</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cost_history_: List[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.parameter_history_: List[Tuple[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Set up convergence detection</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        conv_config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ConvergenceConfig(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            cost_tolerance</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">tolerance,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            gradient_tolerance</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">tolerance,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            max_iterations</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">max_iterations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.convergence_detector </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ConvergenceDetector(conv_config)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> fit</span><span style=\"color:#E1E4E8\">(self, x: np.ndarray, y: np.ndarray) -> </span><span style=\"color:#9ECBFF\">'GradientDescentRegression'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Train the linear regression model using gradient descent.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            x: Input features, shape (n_samples,)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            y: Target values, shape (n_samples,)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            self: Fitted regressor instance</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate input data shapes and types</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Check that x and y have same length, are 1D arrays, contain finite values</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Set slope to small random value, intercept to mean of y values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use self.random_seed for reproducibility</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Main optimization loop</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # for iteration in range(self.max_iterations):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3a: Compute current cost using _compute_cost method</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3b: Compute gradients using _compute_gradients method  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3c: Check numerical stability of parameters and gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3d: Update parameters using gradient descent rule</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3e: Record current state in history</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3f: Check convergence criteria</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3g: Break if converged</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Set fitted flag and return self</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # self.is_fitted_ = True</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # return self</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> NotImplementedError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Implement gradient descent training loop\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _compute_cost</span><span style=\"color:#E1E4E8\">(self, x: np.ndarray, y: np.ndarray, slope: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, intercept: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Compute mean squared error cost function.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            x: Input features</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            y: Target values  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            slope: Current slope parameter</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            intercept: Current intercept parameter</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Mean squared error cost</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate predictions using current parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: predictions = slope * x + intercept</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Compute residuals (prediction errors)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: residuals = predictions - y</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compute mean squared error</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: cost = mean(residuals^2)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Validate cost is finite and non-negative</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use np.isfinite and check cost >= 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> NotImplementedError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Implement mean squared error calculation\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _compute_gradients</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        self, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        x: np.ndarray, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        y: np.ndarray, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        slope: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        intercept: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ) -> Tuple[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Compute partial derivatives of cost function.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            x: Input features</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            y: Target values</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            slope: Current slope parameter</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            intercept: Current intercept parameter</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            (slope_gradient, intercept_gradient): Partial derivatives</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate predictions using current parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use same prediction formula as in _compute_cost</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Compute residuals</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: residuals = predictions - y</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compute slope gradient</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: slope_grad = 2 * mean(residuals * x)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # This measures correlation between errors and input values</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Compute intercept gradient  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: intercept_grad = 2 * mean(residuals)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # This measures average bias in predictions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate gradients are finite</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Check both gradients with np.isfinite</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> NotImplementedError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Implement gradient calculation\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> predict</span><span style=\"color:#E1E4E8\">(self, x: np.ndarray) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Make predictions using fitted model.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            x: Input features for prediction</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Predicted values</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check that model has been fitted</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: if not self.is_fitted_: raise ValueError(\"Model not fitted\")</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate input data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Check x is 1D array with finite values</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Generate predictions using learned parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: return self.slope_ * x + self.intercept_</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> NotImplementedError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Implement prediction method\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> score</span><span style=\"color:#E1E4E8\">(self, x: np.ndarray, y: np.ndarray) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Calculate R-squared coefficient of determination.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            x: Input features</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            y: True target values</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            R-squared score between 0 and 1</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate predictions for input data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Compute total sum of squares (TSS)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: TSS = sum((y - mean(y))^2)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compute residual sum of squares (RSS)  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: RSS = sum((y - predictions)^2)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Compute R-squared</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: R² = 1 - RSS/TSS</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Handle edge case where TSS = 0 (constant targets)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> NotImplementedError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Implement R-squared calculation\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_training_history</span><span style=\"color:#E1E4E8\">(self) -> Dict:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return complete training history for analysis.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"cost_history\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.cost_history_.copy(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"parameter_history\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.parameter_history_.copy(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"convergence_info\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.convergence_detector.get_convergence_summary(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"hyperparameters\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"learning_rate\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.learning_rate,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"max_iterations\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.max_iterations,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"tolerance\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.tolerance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint:</strong></p>\n<p>After implementing the core gradient descent functionality, learners should be able to run this validation:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_gradient_descent.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> examples/gradient_descent_demo.py</span></span></code></pre></div>\n\n<p>Expected behavior:</p>\n<ul>\n<li>Training cost should decrease monotonically (or with minor oscillations)</li>\n<li>Algorithm should converge within 100-1000 iterations for simple datasets</li>\n<li>Final R-squared scores should match closed-form solutions within 1e-4 tolerance</li>\n<li>Parameter values should be reasonable (typically between -100 and 100 for normalized data)</li>\n<li>Cost history plots should show smooth descent curves</li>\n</ul>\n<p><strong>Signs of successful implementation:</strong></p>\n<ul>\n<li>Cost decreases from initial value and eventually plateaus</li>\n<li>Parameters converge to stable values</li>\n<li>Predictions closely match those from <code>SimpleLinearRegression</code> component</li>\n<li>Algorithm handles different learning rates gracefully (slower/faster convergence)</li>\n<li>Convergence detection stops training at appropriate points</li>\n</ul>\n<p><strong>Common debugging steps:</strong></p>\n<ol>\n<li><strong>Diverging cost</strong>: Reduce learning rate by factor of 10</li>\n<li><strong>Slow convergence</strong>: Increase learning rate or check feature normalization</li>\n<li><strong>Oscillating parameters</strong>: Implement consecutive iteration requirements for convergence</li>\n<li><strong>Wrong final parameters</strong>: Verify gradient calculation formulas and signs</li>\n<li><strong>Infinite values</strong>: Add numerical stability checks and parameter bounds</li>\n</ol>\n<p><img src=\"/api/project/linear-regression/architecture-doc/asset?path=diagrams%2Fgradient-descent-flow.svg\" alt=\"Gradient Descent Algorithm Flow\"></p>\n<p><img src=\"/api/project/linear-regression/architecture-doc/asset?path=diagrams%2Ftraining-sequence.svg\" alt=\"Training Process Sequence\"></p>\n<p><img src=\"/api/project/linear-regression/architecture-doc/asset?path=diagrams%2Fmodel-state-machine.svg\" alt=\"Model Training State Machine\"></p>\n<h2 id=\"multiple-linear-regression-component\">Multiple Linear Regression Component</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> M3: Multiple Linear Regression</p>\n</blockquote>\n<p>This component represents the culmination of our linear regression implementation, extending from single-variable regression to handle multiple input features simultaneously. The transition from simple to multiple linear regression transforms our system from fitting lines in two-dimensional space to fitting hyperplanes in multi-dimensional space, requiring fundamental changes in data representation, mathematical formulation, and computational approach.</p>\n<h3 id=\"mental-model-multi-dimensional-plane-fitting\">Mental Model: Multi-Dimensional Plane Fitting</h3>\n<p><strong>Extending from fitting lines to fitting hyperplanes through multi-dimensional data</strong></p>\n<p>Think of simple linear regression as being a carpenter trying to position a straight plank (representing y = mx + b) so it lies as close as possible to a scattered collection of nails on a wall. You can visualize this easily: the wall is two-dimensional (x and y axes), the nails are data points, and you&#39;re finding the best angle and position for your plank.</p>\n<p>Multiple linear regression is like being an architect designing a table surface that must come as close as possible to a collection of points floating in a room. Now you&#39;re working in three dimensions if you have two features (x₁, x₂, and y), or even higher dimensions with more features. Instead of a straight line, you&#39;re positioning a flat plane (or hyperplane in higher dimensions) through multi-dimensional space.</p>\n<p>The mathematical challenge transforms from finding two parameters (slope and intercept) to finding as many parameters as you have features, plus one intercept term. Instead of the equation y = mx + b, you&#39;re solving y = w₁x₁ + w₂x₂ + w₃x₃ + ... + wₙxₙ + b, where each wᵢ represents how much that particular feature contributes to the prediction.</p>\n<p>The intuition remains the same: you&#39;re still minimizing the sum of squared distances between your predictions and the actual data points. However, these &quot;distances&quot; now exist in multi-dimensional space, and the &quot;surface&quot; you&#39;re fitting can orient itself along multiple dimensions simultaneously. A feature with a large positive weight means that increases in that feature strongly push predictions upward, while a feature with a negative weight pushes predictions downward.</p>\n<p>This mental model helps explain why feature scaling becomes critical: if one feature ranges from 0-1 while another ranges from 0-10000, the hyperplane will be dramatically tilted toward accommodating the large-scale feature, potentially ignoring the subtle but important patterns in the smaller-scale feature.</p>\n<h3 id=\"matrix-formulation\">Matrix Formulation</h3>\n<p><strong>Converting to matrix form and understanding the design matrix structure</strong></p>\n<p>The transition from scalar equations to matrix operations represents the fundamental architectural shift that makes multiple linear regression computationally feasible. Rather than handling each feature and parameter individually, we reorganize all our data and parameters into matrices that allow us to perform all computations simultaneously through vectorized operations.</p>\n<p>The design matrix X becomes the cornerstone of our matrix formulation. For a dataset with n samples and p features, X is an n × (p+1) matrix where each row represents one data sample and each column represents one feature, plus an additional column of ones for the intercept term. The first column contains all ones (the intercept column), and subsequent columns contain the actual feature values.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Dimensions</th>\n<th>Content</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Design Matrix X</td>\n<td>n × (p+1)</td>\n<td>[1, x₁, x₂, ..., xₚ] for each sample</td>\n<td>Input features with intercept column</td>\n</tr>\n<tr>\n<td>Weight Vector w</td>\n<td>(p+1) × 1</td>\n<td>[b, w₁, w₂, ..., wₚ]</td>\n<td>Model parameters including intercept</td>\n</tr>\n<tr>\n<td>Target Vector y</td>\n<td>n × 1</td>\n<td>[y₁, y₂, ..., yₙ]</td>\n<td>Ground truth values</td>\n</tr>\n<tr>\n<td>Prediction Vector ŷ</td>\n<td>n × 1</td>\n<td>[ŷ₁, ŷ₂, ..., ŷₙ]</td>\n<td>Model predictions</td>\n</tr>\n</tbody></table>\n<p>The matrix equation y = Xw elegantly captures the entire linear relationship. When we multiply the design matrix X by the weight vector w, we automatically compute predictions for all samples simultaneously. The first element of w (the intercept) gets multiplied by the column of ones, adding the bias term to every prediction. Each subsequent element of w gets multiplied by its corresponding feature column, contributing its weighted influence to the final predictions.</p>\n<p>The cost function transforms from a scalar computation to a matrix operation: Cost = (1/2n)||Xw - y||². This represents the mean squared error across all samples, computed as the squared Euclidean norm of the residual vector (Xw - y). The matrix formulation automatically handles the summation across all samples and features that we would otherwise need to implement with nested loops.</p>\n<p>Gradient computation becomes a single matrix multiplication: ∇w = (1/n)X^T(Xw - y). The transpose operation X^T effectively rotates our perspective from &quot;samples × features&quot; to &quot;features × samples,&quot; allowing us to compute how each parameter should change based on all the prediction errors simultaneously. This single matrix operation replaces what would otherwise require separate partial derivative calculations for each parameter.</p>\n<p>The architectural decision to use matrix formulation over scalar loops has profound implications for both computational efficiency and code maintainability. Matrix operations leverage highly optimized linear algebra libraries (like NumPy&#39;s underlying BLAS implementations), often achieving order-of-magnitude performance improvements over naive Python loops. Additionally, the matrix approach makes the code more declarative and less prone to indexing errors.</p>\n<blockquote>\n<p><strong>The critical insight is that matrix formulation doesn&#39;t just make multiple linear regression faster—it makes it conceptually simpler by treating all parameters uniformly and eliminating special-case handling for individual features.</strong></p>\n</blockquote>\n<h3 id=\"vectorized-gradient-descent\">Vectorized Gradient Descent</h3>\n<p><strong>Implementing batch updates for all parameters simultaneously</strong></p>\n<p>Vectorized gradient descent represents the computational heart of multiple linear regression, extending the iterative optimization approach from single-parameter updates to simultaneous updates of entire parameter vectors. This transformation requires careful consideration of numerical stability, convergence criteria, and computational efficiency when dealing with higher-dimensional parameter spaces.</p>\n<p>The parameter update rule generalizes from scalar arithmetic to vector operations: w(t+1) = w(t) - α∇w, where α represents the learning rate and ∇w is the gradient vector computed for all parameters simultaneously. Unlike simple linear regression where we updated slope and intercept separately, vectorized gradient descent updates all components of the weight vector in a single atomic operation.</p>\n<table>\n<thead>\n<tr>\n<th>Operation</th>\n<th>Mathematical Form</th>\n<th>Computational Implementation</th>\n<th>Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Cost Computation</td>\n<td>(1/2n)\\</td>\n<td>Xw - y\\</td>\n<td>²</td>\n</tr>\n<tr>\n<td>Gradient Computation</td>\n<td>(1/n)X^T(Xw - y)</td>\n<td><code>X.T @ (X @ weights - y) / n</code></td>\n<td>O(np²)</td>\n</tr>\n<tr>\n<td>Parameter Update</td>\n<td>w = w - α∇w</td>\n<td><code>weights -= learning_rate * gradients</code></td>\n<td>O(p)</td>\n</tr>\n<tr>\n<td>Convergence Check</td>\n<td>\\</td>\n<td>∇w\\</td>\n<td>₂ &lt; ε</td>\n</tr>\n</tbody></table>\n<p>The batch gradient descent algorithm follows these detailed steps:</p>\n<ol>\n<li>Initialize the weight vector w with small random values or zeros, ensuring the vector includes space for the intercept term as the first component</li>\n<li>Construct the design matrix X by prepending a column of ones to the feature matrix, creating the (n × p+1) structure required for matrix multiplication</li>\n<li>For each iteration until convergence or maximum iterations reached:</li>\n<li>Compute predictions for all samples using matrix multiplication: ŷ = Xw, leveraging NumPy&#39;s optimized linear algebra routines</li>\n<li>Calculate the residual vector by subtracting actual targets from predictions: residuals = ŷ - y</li>\n<li>Compute the cost as the mean of squared residuals: cost = (1/2n)||residuals||²</li>\n<li>Calculate gradients for all parameters simultaneously: ∇w = (1/n)X^T × residuals</li>\n<li>Update all parameters using the vectorized update rule: w = w - α∇w</li>\n<li>Check convergence by evaluating multiple criteria: cost improvement, gradient magnitude, and parameter change magnitude</li>\n<li>Store training history including cost values, parameter vectors, and gradient norms for debugging and analysis</li>\n</ol>\n<p>The convergence detection becomes more sophisticated with multiple parameters because we must consider the behavior of the entire parameter vector rather than individual scalars. We implement multiple convergence criteria that must be satisfied simultaneously:</p>\n<table>\n<thead>\n<tr>\n<th>Convergence Criterion</th>\n<th>Formula</th>\n<th>Interpretation</th>\n<th>Threshold</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Cost Improvement</td>\n<td>|cost(t) - cost(t-1)| &lt; ε₁</td>\n<td>Optimization progress stalled</td>\n<td>1e-8</td>\n</tr>\n<tr>\n<td>Gradient Magnitude</td>\n<td>\\</td>\n<td>∇w\\</td>\n<td>₂ &lt; ε₂</td>\n</tr>\n<tr>\n<td>Parameter Change</td>\n<td>\\</td>\n<td>w(t) - w(t-1)\\</td>\n<td>₂ &lt; ε₃</td>\n</tr>\n<tr>\n<td>Relative Improvement</td>\n<td>|cost(t) - cost(t-1)|/cost(t-1) &lt; ε₄</td>\n<td>Relative progress minimal</td>\n<td>1e-10</td>\n</tr>\n</tbody></table>\n<p>The learning rate selection becomes more critical with multiple features because the parameter space has more dimensions along which the optimization can become unstable. Features with vastly different scales can create ill-conditioned optimization surfaces where the optimal learning rate for one parameter is too large or too small for others. This motivates the crucial importance of feature scaling.</p>\n<p>Numerical stability monitoring must track the behavior of all parameters simultaneously. We implement safeguards that detect parameter explosion (any parameter exceeding MAX_PARAMETER_VALUE), gradient explosion (gradient norm exceeding MAX_GRADIENT_NORM), and numerical underflow (gradient norm below MIN_GRADIENT_NORM). When instability is detected, the algorithm can automatically reduce the learning rate or terminate with a diagnostic message.</p>\n<blockquote>\n<p><strong>Decision: Batch Gradient Descent vs Stochastic Variants</strong></p>\n<ul>\n<li><strong>Context</strong>: Multiple approaches exist for computing gradients in multi-dimensional parameter spaces</li>\n<li><strong>Options Considered</strong>: <ul>\n<li>Batch gradient descent (full dataset per iteration)</li>\n<li>Stochastic gradient descent (single sample per iteration)</li>\n<li>Mini-batch gradient descent (subset of samples per iteration)</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Implement batch gradient descent for the educational implementation</li>\n<li><strong>Rationale</strong>: Batch gradient descent provides the most stable and predictable convergence behavior for learning purposes, with deterministic results that aid debugging and understanding. The computational overhead is acceptable for the dataset sizes typical in educational scenarios</li>\n<li><strong>Consequences</strong>: Enables consistent convergence behavior and reproducible results, but requires more memory and computational resources per iteration compared to stochastic variants</li>\n</ul>\n</blockquote>\n<h3 id=\"feature-scaling-and-engineering\">Feature Scaling and Engineering</h3>\n<p><strong>Normalization techniques and handling the intercept term</strong></p>\n<p>Feature scaling represents one of the most critical preprocessing steps in multiple linear regression, transforming raw features with potentially vastly different scales into a normalized space where gradient descent can converge efficiently and stably. The architectural challenge lies in implementing scaling that is both mathematically sound and practically robust while maintaining the ability to transform new data consistently.</p>\n<p>The fundamental problem arises when features exist on dramatically different scales. Consider predicting house prices using features like &quot;number of bedrooms&quot; (typically 1-5) and &quot;lot size in square feet&quot; (typically 1000-10000). Without scaling, the gradient descent algorithm will be dominated by the large-magnitude feature, potentially ignoring the subtle but important patterns in the smaller-scale feature. The optimization surface becomes elongated and ill-conditioned, requiring impractically small learning rates to avoid divergence.</p>\n<p>Z-score normalization (standardization) transforms each feature to have zero mean and unit variance: x_scaled = (x - μ)/σ, where μ is the feature mean and σ is the feature standard deviation. This transformation preserves the relative relationships between data points while ensuring all features contribute equally to the optimization dynamics.</p>\n<table>\n<thead>\n<tr>\n<th>Scaling Method</th>\n<th>Formula</th>\n<th>When to Use</th>\n<th>Properties</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Z-score (Standard)</td>\n<td>(x - μ)/σ</td>\n<td>Default choice, features normally distributed</td>\n<td>Mean=0, Std=1, preserves outliers</td>\n</tr>\n<tr>\n<td>Min-Max Scaling</td>\n<td>(x - min)/(max - min)</td>\n<td>Features need specific range [0,1]</td>\n<td>Bounded output, sensitive to outliers</td>\n</tr>\n<tr>\n<td>Robust Scaling</td>\n<td>(x - median)/IQR</td>\n<td>Features contain outliers</td>\n<td>Uses percentiles, less sensitive to outliers</td>\n</tr>\n<tr>\n<td>Unit Vector Scaling</td>\n<td>x/\\</td>\n<td>x\\</td>\n<td>₂</td>\n</tr>\n</tbody></table>\n<p>The implementation must carefully handle the storage and application of scaling parameters to ensure consistency between training and prediction phases. During training, we compute and store the mean and standard deviation for each feature. During prediction, we must apply exactly the same scaling transformation to new data, which requires persistent storage of these scaling parameters.</p>\n<p>The intercept term requires special architectural consideration because it should never be scaled. The intercept represents the baseline prediction when all features equal zero (in the scaled space), and scaling the intercept would fundamentally alter this interpretation. Our design matrix construction must ensure the first column remains as ones, unaffected by any scaling operations applied to the feature columns.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Implementation approach for feature scaling architecture:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> FeatureScaler</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, method</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'standard'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.method </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> method</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.feature_means_ </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.feature_stds_ </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.is_fitted_ </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> fit</span><span style=\"color:#E1E4E8\">(self, features):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Compute and store scaling parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Never include intercept column in scaling calculations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> transform</span><span style=\"color:#E1E4E8\">(self, features):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Apply stored scaling parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Preserve intercept column unchanged</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> fit_transform</span><span style=\"color:#E1E4E8\">(self, features):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Convenience method combining fit and transform</span></span></code></pre></div>\n\n<p>The scaling operation integrates seamlessly with our Dataset architecture, updating the normalization_stats dictionary with the computed parameters and setting the is_normalized flag to track the scaling state. This design ensures that predictions on new data automatically apply consistent scaling without requiring manual parameter management.</p>\n<p>Feature engineering considerations extend beyond simple scaling to include handling of categorical variables, polynomial features, and interaction terms. However, for the educational scope of this implementation, we focus on continuous numerical features with standard scaling as the primary transformation.</p>\n<p>The architectural decision to implement scaling within the DataHandler component rather than the regression model itself reflects the principle that data preprocessing should be separated from model training. This separation enables the scaling logic to be reused across different model types and ensures that scaling parameters are managed consistently throughout the data pipeline.</p>\n<blockquote>\n<p><strong>Decision: When to Apply Feature Scaling</strong></p>\n<ul>\n<li><strong>Context</strong>: Feature scaling can be applied at data loading time or during model fitting</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Scale during data loading (eager scaling)</li>\n<li>Scale during model fitting (lazy scaling)</li>\n<li>Scale on-demand during prediction (dynamic scaling)</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Implement scaling as an explicit method call during data preprocessing, before model fitting</li>\n<li><strong>Rationale</strong>: Explicit scaling provides learners with clear visibility into the preprocessing pipeline and enables experimentation with different scaling strategies. It also prevents accidental data leakage by ensuring scaling parameters are computed only from training data</li>\n<li><strong>Consequences</strong>: Requires explicit scaling calls but provides maximum flexibility and educational transparency</li>\n</ul>\n</blockquote>\n<h3 id=\"l2-regularization-ridge\">L2 Regularization (Ridge)</h3>\n<p><strong>Adding penalty terms to prevent overfitting with many features</strong></p>\n<p>L2 regularization, commonly known as Ridge regression, addresses the fundamental challenge of overfitting that emerges when working with multiple features, particularly when the number of features approaches or exceeds the number of training samples. The regularization mechanism adds a penalty term to the cost function that discourages large parameter values, effectively constraining the model&#39;s complexity and improving its generalization to unseen data.</p>\n<p>The mathematical foundation of Ridge regression modifies our standard cost function by adding a penalty term proportional to the sum of squared parameter values: Cost = (1/2n)||Xw - y||² + λ(1/2)||w||², where λ (lambda) represents the regularization strength hyperparameter. The penalty term (1/2)||w||² computes the squared L2 norm of the weight vector, excluding the intercept term which should not be regularized.</p>\n<p>The architectural motivation for including regularization stems from the curse of dimensionality and the bias-variance tradeoff. As we add more features to our model, the parameter space becomes increasingly high-dimensional, creating more opportunities for the model to memorize training data rather than learning generalizable patterns. Regularization acts as a constraint that forces the model to find simpler solutions by preferring smaller parameter values.</p>\n<table>\n<thead>\n<tr>\n<th>Regularization Aspect</th>\n<th>Standard Regression</th>\n<th>Ridge Regression</th>\n<th>Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Cost Function</td>\n<td>(1/2n)\\</td>\n<td>Xw - y\\</td>\n<td>²</td>\n</tr>\n<tr>\n<td>Gradient Computation</td>\n<td>(1/n)X^T(Xw - y)</td>\n<td>(1/n)X^T(Xw - y) + λw</td>\n<td>Shrinks weights toward zero</td>\n</tr>\n<tr>\n<td>Parameter Updates</td>\n<td>w = w - α∇w</td>\n<td>w = w - α(∇w + λw)</td>\n<td>Implicit weight decay</td>\n</tr>\n<tr>\n<td>Solution Behavior</td>\n<td>Can overfit with many features</td>\n<td>Controlled complexity</td>\n<td>Better generalization</td>\n</tr>\n</tbody></table>\n<p>The gradient computation incorporates the regularization term by adding λw to the standard gradient: ∇w_regularized = (1/n)X^T(Xw - y) + λw. This addition has the effect of pulling all parameters toward zero during each update step, with the strength of this &quot;weight decay&quot; controlled by the regularization parameter λ. Importantly, the intercept term (first element of w) should be excluded from regularization because it represents the baseline prediction and should not be penalized for being large.</p>\n<p>The regularization strength λ represents a crucial hyperparameter that controls the tradeoff between fitting the training data and keeping the model simple. Small λ values (approaching 0) provide minimal regularization and may allow overfitting, while large λ values heavily penalize parameter magnitude and may lead to underfitting. The optimal λ value typically requires experimentation or cross-validation to determine.</p>\n<table>\n<thead>\n<tr>\n<th>Lambda Value</th>\n<th>Regularization Effect</th>\n<th>Model Behavior</th>\n<th>When to Use</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>λ = 0</td>\n<td>No regularization</td>\n<td>Standard linear regression</td>\n<td>More samples than features</td>\n</tr>\n<tr>\n<td>λ ≈ 0.01</td>\n<td>Light regularization</td>\n<td>Slight bias toward simplicity</td>\n<td>Mild overfitting signs</td>\n</tr>\n<tr>\n<td>λ ≈ 0.1</td>\n<td>Moderate regularization</td>\n<td>Balanced complexity control</td>\n<td>Moderate overfitting risk</td>\n</tr>\n<tr>\n<td>λ ≥ 1.0</td>\n<td>Strong regularization</td>\n<td>Heavy bias toward zero weights</td>\n<td>High overfitting risk</td>\n</tr>\n</tbody></table>\n<p>The implementation architecture extends our existing gradient descent framework by modifying the gradient computation and cost calculation to include regularization terms. The regularization strength becomes a configurable parameter of the model, stored in the ModelParameters structure and used consistently throughout the training process.</p>\n<p>The convergence criteria require slight modification under regularization because the additional penalty term affects both the cost function trajectory and the gradient magnitudes. Regularized gradients may not approach zero as closely as unregularized gradients, requiring adjusted convergence thresholds that account for the persistent regularization pressure.</p>\n<p>Ridge regression provides several architectural advantages beyond overfitting prevention. The regularization term improves the numerical conditioning of the optimization problem, often enabling more stable convergence even with correlated features. Additionally, Ridge regression handles multicollinearity more gracefully than unregularized regression by distributing parameter values across correlated features rather than arbitrarily assigning large weights to individual features.</p>\n<blockquote>\n<p><strong>Decision: L2 vs L1 Regularization</strong></p>\n<ul>\n<li><strong>Context</strong>: Multiple regularization methods exist for controlling model complexity</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>L2 regularization (Ridge): penalty = λ||w||²</li>\n<li>L1 regularization (Lasso): penalty = λ||w||₁  </li>\n<li>Elastic Net: penalty = λ₁||w||₁ + λ₂||w||²</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Implement L2 regularization (Ridge) for the educational version</li>\n<li><strong>Rationale</strong>: L2 regularization provides smooth, differentiable gradients that work seamlessly with gradient descent optimization. It shrinks parameters toward zero without creating sparsity, making it easier to understand and debug. The mathematical foundation aligns well with the least squares framework already established</li>\n<li><strong>Consequences</strong>: Provides effective overfitting control and improved numerical stability, but does not perform feature selection like L1 regularization would</li>\n</ul>\n</blockquote>\n<h3 id=\"architecture-decision-records\">Architecture Decision Records</h3>\n<p><strong>Decisions about matrix operations, regularization strength, and feature scaling methods</strong></p>\n<p>The architectural decisions for multiple linear regression fundamentally shape both the educational value and practical implementation of the system. Each decision represents a deliberate tradeoff between simplicity, performance, educational clarity, and extensibility.</p>\n<blockquote>\n<p><strong>Decision: NumPy Matrix Operations vs Pure Python Implementation</strong></p>\n<ul>\n<li><strong>Context</strong>: Multiple approaches exist for implementing matrix operations required for vectorized computations</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Pure Python with nested loops for all matrix operations</li>\n<li>NumPy arrays with explicit loop implementations for educational clarity</li>\n<li>Full NumPy vectorization using optimized linear algebra routines</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Use full NumPy vectorization for all matrix operations</li>\n<li><strong>Rationale</strong>: NumPy provides orders of magnitude performance improvement over pure Python, essential for reasonable training times on realistic datasets. The vectorized operations more accurately represent production machine learning implementations. Educational value comes from understanding the mathematical concepts rather than implementing low-level matrix arithmetic</li>\n<li><strong>Consequences</strong>: Requires NumPy dependency but enables practical performance and industry-standard implementation patterns. Students learn to think in terms of matrix operations rather than scalar loops</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Default Regularization Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Regularization strength significantly impacts model behavior and requires sensible defaults</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>No regularization by default (λ = 0)</li>\n<li>Light regularization by default (λ = 0.01) </li>\n<li>Adaptive regularization based on dataset characteristics</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: No regularization by default, with easy configuration for Ridge regularization</li>\n<li><strong>Rationale</strong>: Zero regularization maintains consistency with standard linear regression behavior and allows students to observe overfitting effects directly. Regularization should be an explicit choice that students make when they understand the need for it. This approach supports progressive learning where students first master basic concepts before adding complexity</li>\n<li><strong>Consequences</strong>: Students may initially encounter overfitting in high-dimensional scenarios, creating learning opportunities to understand when and why regularization is needed</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Feature Scaling Integration Architecture</strong></p>\n<ul>\n<li><strong>Context</strong>: Feature scaling can be tightly coupled with the model or implemented as separate preprocessing</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Automatic scaling within the model&#39;s fit method</li>\n<li>Manual scaling as a separate preprocessing step</li>\n<li>Optional scaling with automatic detection of scale differences</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Manual scaling as an explicit preprocessing step with clear API methods</li>\n<li><strong>Rationale</strong>: Explicit preprocessing teaches the importance of feature scaling and prevents hidden transformations that obscure the learning process. Manual control enables experimentation with different scaling strategies and makes the data pipeline transparent. This approach aligns with production machine learning practices where preprocessing is explicitly managed</li>\n<li><strong>Consequences</strong>: Requires students to explicitly consider and implement feature scaling, which increases initial complexity but provides deeper understanding of preprocessing importance</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Parameter Initialization Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Initial parameter values significantly affect gradient descent convergence in high-dimensional spaces</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Zero initialization for all parameters</li>\n<li>Small random initialization from normal distribution  </li>\n<li>Xavier/He initialization adapted for linear regression</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Small random initialization from normal distribution with scale 0.01</li>\n<li><strong>Rationale</strong>: Random initialization prevents symmetry breaking issues that can occur with zero initialization when features are scaled. Small scale (0.01) ensures parameters start near zero without being exactly zero, providing gentle initial gradients. This approach works well across different dataset scales and feature counts</li>\n<li><strong>Consequences</strong>: Provides robust initialization across various scenarios while remaining simple to understand and implement</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Decision Area</th>\n<th>Chosen Approach</th>\n<th>Alternative Considered</th>\n<th>Trade-off Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Memory Management</td>\n<td>Store full design matrix</td>\n<td>Compute features on-demand</td>\n<td>Favors simplicity over memory efficiency for educational datasets</td>\n</tr>\n<tr>\n<td>Convergence Detection</td>\n<td>Multiple criteria (cost, gradient, parameters)</td>\n<td>Single cost-based criterion</td>\n<td>More robust but requires tuning multiple thresholds</td>\n</tr>\n<tr>\n<td>Learning Rate Strategy</td>\n<td>Fixed learning rate</td>\n<td>Adaptive/decaying learning rate</td>\n<td>Simpler to understand and debug for learning purposes</td>\n</tr>\n<tr>\n<td>Numerical Precision</td>\n<td>Standard float64</td>\n<td>Mixed precision computation</td>\n<td>Prioritizes numerical stability over computational efficiency</td>\n</tr>\n</tbody></table>\n<p>The parameter update architecture implements safeguards against numerical instability that becomes more critical with multiple parameters. We implement gradient clipping to prevent explosion, parameter bounds to prevent overflow, and automatic learning rate reduction when instability is detected.</p>\n<p>The error handling architecture extends to include matrix dimension validation, ensuring that all matrix operations receive compatible dimensions. The system validates that the feature matrix dimensions align with the parameter vector dimensions and that all samples have consistent feature counts.</p>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p><strong>Matrix dimension errors, feature scaling issues, and regularization parameter tuning</strong></p>\n<p>Multiple linear regression introduces several categories of errors that are less common or entirely absent in simple linear regression. These pitfalls often stem from the increased complexity of matrix operations, feature interactions, and the additional hyperparameters introduced by regularization and scaling.</p>\n<p>⚠️ <strong>Pitfall: Matrix Dimension Mismatches</strong>\nThe most frequent error in multiple linear regression implementations involves incompatible matrix dimensions during mathematical operations. The design matrix X must have dimensions (n_samples × n_features+1), the weight vector w must have dimensions (n_features+1 × 1), and the target vector y must have dimensions (n_samples × 1). Common mistakes include forgetting to add the intercept column to X, creating w with the wrong number of parameters, or attempting to multiply matrices with incompatible inner dimensions. The error typically manifests as NumPy broadcasting errors or explicit dimension mismatch exceptions. To avoid this, always validate matrix shapes before mathematical operations and implement helper functions that construct matrices with correct dimensions. Use assertions to check dimensions at critical points: <code>assert X.shape[1] == len(weights), f&quot;Feature count mismatch: X has {X.shape[1]} columns but weights has {len(weights)} elements&quot;</code>.</p>\n<p>⚠️ <strong>Pitfall: Scaling the Intercept Term</strong>\nA subtle but critical error involves including the intercept column (column of ones) in feature scaling operations. When the intercept column gets scaled, it no longer represents a constant bias term, fundamentally altering the model&#39;s mathematical properties and typically causing convergence problems or nonsensical results. This error often occurs when applying scaling transformations to the entire design matrix instead of only the feature columns. The intercept column should always remain as pure ones, unaffected by any scaling operations. Implement scaling operations that explicitly exclude the first column: <code>scaled_features = scaler.fit_transform(X[:, 1:])</code>, then reconstruct the design matrix by concatenating the intercept column with the scaled features.</p>\n<p>⚠️ <strong>Pitfall: Data Leakage in Feature Scaling</strong>\nFeature scaling parameters (mean and standard deviation) must be computed exclusively from training data, never from the combination of training and test data. Computing scaling parameters from the entire dataset creates data leakage, where information from test samples influences the model training process. This leakage often produces overly optimistic performance estimates that don&#39;t reflect real-world generalization. The correct approach computes scaling parameters during training (<code>scaler.fit(X_train)</code>), then applies these same parameters to test data (<code>X_test_scaled = scaler.transform(X_test)</code>). Never call <code>fit_transform</code> on test data; always use the <code>transform</code> method with parameters learned from training data.</p>\n<p>⚠️ <strong>Pitfall: Regularizing the Intercept Parameter</strong>\nRidge regularization should exclude the intercept term from the penalty calculation because the intercept represents the baseline prediction and should not be constrained toward zero. Including the intercept in regularization can bias the model toward predicting zero regardless of the actual target distribution. This error typically occurs when applying the regularization penalty to the entire weight vector instead of excluding the first element (intercept). Implement regularization that explicitly excludes the intercept: <code>l2_penalty = lambda_reg * np.sum(weights[1:] ** 2)</code> and modify gradients accordingly: <code>regularized_gradients[1:] += lambda_reg * weights[1:]</code>.</p>\n<p>⚠️ <strong>Pitfall: Inappropriate Regularization Strength</strong>\nRegularization strength (λ) requires careful tuning because inappropriate values can lead to severe underfitting or provide no overfitting protection. Very large λ values (≥1.0) can shrink all parameters toward zero, effectively ignoring the training data and producing poor predictions. Very small λ values (≤1e-6) provide minimal regularization benefit. The optimal λ typically falls between 0.001 and 0.1 for most datasets, but requires experimentation or cross-validation. Start with λ = 0.01 as a reasonable default and adjust based on training vs validation performance. Monitor the magnitude of parameter values; if they approach zero, reduce λ; if they grow very large, increase λ.</p>\n<p>⚠️ <strong>Pitfall: Learning Rate Too Large for High-Dimensional Spaces</strong>\nLearning rates that work well for simple linear regression often cause divergence in multiple linear regression due to the increased complexity of the parameter space. With more parameters, the optimization surface becomes more complex, and the same learning rate may be too aggressive for stable convergence. Symptoms include oscillating or increasing cost values, parameter values that grow extremely large, or NaN values appearing in gradients or parameters. Start with smaller learning rates (0.001-0.01) for multiple features and implement adaptive learning rate reduction when instability is detected. Monitor parameter magnitudes and cost function behavior; stable training should show monotonically decreasing cost and bounded parameter values.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Convergence Criteria</strong>\nSingle convergence criteria (like cost-based stopping) may be insufficient for multiple linear regression because the high-dimensional parameter space can exhibit complex convergence behavior. The cost function may plateau while individual parameters continue changing significantly, or gradients may remain non-zero due to regularization effects. Implement multiple convergence criteria that all must be satisfied: cost improvement below threshold, gradient magnitude below threshold, and parameter change magnitude below threshold. Use relative thresholds in addition to absolute thresholds to handle different problem scales: <code>abs(cost_change) / cost &lt; relative_tolerance</code>.</p>\n<table>\n<thead>\n<tr>\n<th>Error Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnostic Approach</th>\n<th>Fix Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>&quot;Cannot multiply matrices&quot;</td>\n<td>Dimension mismatch</td>\n<td>Print shapes before operations</td>\n<td>Validate and fix matrix construction</td>\n</tr>\n<tr>\n<td>Cost increases during training</td>\n<td>Learning rate too high or intercept scaled</td>\n<td>Monitor parameter magnitudes</td>\n<td>Reduce learning rate, check scaling</td>\n</tr>\n<tr>\n<td>All parameters near zero</td>\n<td>Regularization too strong</td>\n<td>Check λ value and parameter history</td>\n<td>Reduce regularization strength</td>\n</tr>\n<tr>\n<td>Model ignores some features</td>\n<td>Poor feature scaling</td>\n<td>Check feature value ranges</td>\n<td>Apply proper normalization</td>\n</tr>\n<tr>\n<td>NaN values in gradients</td>\n<td>Numerical overflow</td>\n<td>Check parameter bounds</td>\n<td>Add gradient clipping, reduce learning rate</td>\n</tr>\n<tr>\n<td>Perfect training accuracy, poor test</td>\n<td>Data leakage in scaling</td>\n<td>Review preprocessing pipeline</td>\n<td>Recompute scaling from training only</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides concrete implementation guidance for building the multiple linear regression component, focusing on the matrix operations, vectorized computations, and architectural patterns needed to extend from simple linear regression to multi-dimensional feature spaces.</p>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Matrix Operations</td>\n<td>NumPy with basic linear algebra</td>\n<td>NumPy with BLAS-optimized routines</td>\n</tr>\n<tr>\n<td>Feature Scaling</td>\n<td>Manual standardization</td>\n<td>Scikit-learn preprocessing (for reference)</td>\n</tr>\n<tr>\n<td>Regularization</td>\n<td>Basic L2 penalty implementation</td>\n<td>Configurable regularization factory</td>\n</tr>\n<tr>\n<td>Validation</td>\n<td>Simple assertion checks</td>\n<td>Comprehensive input validation suite</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n├── src/\n│   ├── data_handler.py           ← Enhanced with scaling capabilities\n│   ├── simple_regression.py      ← From previous milestone\n│   ├── gradient_descent.py       ← From previous milestone\n│   ├── multiple_regression.py    ← New: Main multiple regression class\n│   └── feature_scaler.py         ← New: Feature scaling utilities\n├── tests/\n│   ├── test_multiple_regression.py\n│   └── test_feature_scaling.py\n├── data/\n│   ├── housing.csv               ← Multi-feature dataset\n│   └── synthetic_multi.csv       ← Generated multi-dimensional data\n└── examples/\n    ├── multiple_regression_demo.py\n    └── regularization_comparison.py</code></pre></div>\n\n<p><strong>Complete Feature Scaler Implementation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Dict, Any</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StandardScaler</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Z-score normalization for multiple features.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Transforms features to have zero mean and unit variance.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Preserves intercept column (first column) unchanged.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.mean_: Optional[np.ndarray] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.std_: Optional[np.ndarray] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.is_fitted_: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.n_features_: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> fit</span><span style=\"color:#E1E4E8\">(self, X: np.ndarray) -> </span><span style=\"color:#9ECBFF\">'StandardScaler'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute scaling parameters from training data.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            X: Feature matrix with shape (n_samples, n_features+1)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">               First column should be intercept (all ones)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            self for method chaining</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> X.ndim </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Expected 2D array, got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">X.ndim</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">D\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Extract feature columns (exclude intercept)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        features </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> X[:, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">:] </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> X.shape[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> else</span><span style=\"color:#E1E4E8\"> X</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.mean_ </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.mean(features, </span><span style=\"color:#FFAB70\">axis</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.std_ </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.std(features, </span><span style=\"color:#FFAB70\">axis</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Handle constant features (std = 0)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.std_[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.std_ </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.n_features_ </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> features.shape[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.is_fitted_ </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> transform</span><span style=\"color:#E1E4E8\">(self, X: np.ndarray) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Apply scaling to new data using stored parameters.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            X: Feature matrix with shape (n_samples, n_features+1)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Scaled feature matrix with same shape as input</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_fitted_:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Scaler not fitted. Call fit() first.\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        X_scaled </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> X.copy()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Scale only feature columns, preserve intercept</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> X.shape[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            features </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> X[:, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">:]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            scaled_features </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (features </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.mean_) </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.std_</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            X_scaled[:, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">:] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scaled_features</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> X_scaled</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> fit_transform</span><span style=\"color:#E1E4E8\">(self, X: np.ndarray) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Fit scaler and transform data in one step.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.fit(X).transform(X)</span></span></code></pre></div>\n\n<p><strong>Multiple Linear Regression Core Class Structure:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Tuple, List, Dict, Any</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MultipleLinearRegression</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Multiple linear regression with gradient descent and optional Ridge regularization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 learning_rate: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.01</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 max_iterations: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 tolerance: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-6</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 regularization_strength: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.learning_rate </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> learning_rate</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_iterations </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_iterations</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tolerance </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tolerance</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.regularization_strength </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> regularization_strength</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Model parameters</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.weights_: Optional[np.ndarray] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.is_fitted_: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.n_features_: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Training history</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cost_history_: List[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.weight_history_: List[np.ndarray] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.gradient_norms_: List[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Convergence info</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.converged_: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.final_iteration_: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.convergence_reason_: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> fit</span><span style=\"color:#E1E4E8\">(self, X: np.ndarray, y: np.ndarray) -> </span><span style=\"color:#9ECBFF\">'MultipleLinearRegression'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Train the multiple regression model using gradient descent.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            X: Feature matrix with shape (n_samples, n_features+1)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">               First column should be intercept (all ones)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            y: Target vector with shape (n_samples,)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            self for method chaining</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate input dimensions and data types</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize weights vector with small random values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Implement gradient descent loop with vectorized operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Compute cost using matrix operations: (1/2n)||Xw - y||²</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Add L2 regularization to cost: + (λ/2)||w[1:]||²</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Compute gradients: (1/n)X^T(Xw - y) + λ[0, w1, w2, ...]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Update weights: w = w - α * gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Check multiple convergence criteria</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Store training history for debugging</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Handle numerical instability and divergence</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> predict</span><span style=\"color:#E1E4E8\">(self, X: np.ndarray) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate predictions using fitted model.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            X: Feature matrix with shape (n_samples, n_features+1)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Predictions with shape (n_samples,)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate model is fitted</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate input dimensions match training data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compute predictions using matrix multiplication: Xw</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return predictions as 1D array</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> score</span><span style=\"color:#E1E4E8\">(self, X: np.ndarray, y: np.ndarray) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate R-squared coefficient of determination.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            X: Feature matrix with shape (n_samples, n_features+1)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            y: Target vector with shape (n_samples,)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            R-squared score between 0 and 1</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate predictions for input data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Compute total sum of squares: Σ(yi - ȳ)²</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compute residual sum of squares: Σ(yi - ŷi)²</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Calculate R² = 1 - RSS/TSS</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Handle edge cases (perfect fit, no variance)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _compute_cost</span><span style=\"color:#E1E4E8\">(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute regularized mean squared error cost.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            X: Feature matrix</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            y: Target vector  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            weights: Current parameter values</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Regularized cost value</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Compute predictions: Xw</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Compute residuals: predictions - y</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compute MSE: (1/2n) * ||residuals||²</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Add L2 penalty: (λ/2) * ||weights[1:]||² (exclude intercept)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return total cost</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _compute_gradients</span><span style=\"color:#E1E4E8\">(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute regularized gradients for all parameters.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            X: Feature matrix</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            y: Target vector</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            weights: Current parameter values</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Gradient vector with same shape as weights</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Compute predictions: Xw</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Compute residuals: predictions - y</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compute base gradients: (1/n) * X^T * residuals</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Add L2 regularization: gradients[1:] += λ * weights[1:]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Keep intercept gradient unregularized</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return complete gradient vector</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Design Matrix Construction Helper:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> create_design_matrix</span><span style=\"color:#E1E4E8\">(features: np.ndarray) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Construct design matrix by adding intercept column.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        features: Raw feature matrix (n_samples, n_features)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Design matrix (n_samples, n_features+1) with intercept column</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> features.ndim </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        features </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> features.reshape(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    n_samples </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> features.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    intercept_column </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.ones((n_samples, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> np.hstack([intercept_column, features])</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_regression_data</span><span style=\"color:#E1E4E8\">(X: np.ndarray, y: np.ndarray) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Comprehensive validation for regression inputs.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> X.ndim </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"X must be 2D, got shape </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">X.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> y.ndim </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"y must be 1D, got shape </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">y.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> X.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(y):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Sample count mismatch: X has </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">X.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, y has </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(y)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> X.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> X.shape[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Insufficient samples: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">X.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> samples for </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">X.shape[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> features\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Check for NaN or infinite values</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> np.isfinite(X).all():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"X contains NaN or infinite values\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> np.isfinite(y).all():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"y contains NaN or infinite values\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Synthetic Data Generation for Testing:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> generate_multiple_regression_data</span><span style=\"color:#E1E4E8\">(n_samples: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                    n_features: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                    noise_std: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                    random_state: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> Tuple[np.ndarray, np.ndarray]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Generate synthetic data for multiple linear regression.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Creates data following y = X @ true_weights + noise</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Tuple of (features, targets) arrays</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> random_state </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        np.random.seed(random_state)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Generate random features</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    X </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.random.randn(n_samples, n_features)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Generate true weights</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    true_weights </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.random.randn(n_features) </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 2.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    true_intercept </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.random.randn() </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 0.5</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Generate targets with noise</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> X </span><span style=\"color:#F97583\">@</span><span style=\"color:#E1E4E8\"> true_weights </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> true_intercept </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> np.random.normal(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, noise_std, n_samples)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> X, y</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint:</strong>\nAfter implementing the multiple linear regression component, verify the following behavior:</p>\n<ol>\n<li><strong>Matrix Operations Test</strong>: Create a simple 2-feature dataset and verify that matrix multiplication produces correct predictions:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">   X, y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> generate_multiple_regression_data(</span><span style=\"color:#FFAB70\">n_samples</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">50</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">n_features</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">random_state</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">42</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   X_design </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> create_design_matrix(X)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MultipleLinearRegression(</span><span style=\"color:#FFAB70\">learning_rate</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.01</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">max_iterations</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   model.fit(X_design, y)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   predictions </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model.predict(X_design)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   r_squared </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model.score(X_design, y)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">   print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"R-squared: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">r_squared</span><span style=\"color:#F97583\">:.4f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Should be > 0.8 for synthetic data</span></span></code></pre></div>\n\n<ol start=\"2\">\n<li><strong>Feature Scaling Test</strong>: Compare convergence with and without feature scaling on data with different feature scales:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">   # Test with unscaled features of different magnitudes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   X_unscaled </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.column_stack([np.random.randn(</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">), np.random.randn(</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Training should converge much faster after scaling</span></span></code></pre></div>\n\n<ol start=\"3\">\n<li><strong>Regularization Test</strong>: Verify that increasing regularization strength shrinks parameter magnitudes:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">   # Compare parameter values with λ = 0.0 vs λ = 1.0</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Higher regularization should produce smaller parameter values</span></span></code></pre></div>\n\n<ol start=\"4\">\n<li><strong>Convergence Monitoring</strong>: Check that training history shows decreasing cost and stabilizing parameters:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">   plt.plot(model.cost_history_)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   plt.title(</span><span style=\"color:#9ECBFF\">\"Cost Function Over Training\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Should show monotonic decrease to convergence</span></span></code></pre></div>\n\n<p>The implementation should handle matrices with up to 10-20 features efficiently, converge within 1000 iterations for well-conditioned problems, and provide stable predictions with properly scaled features.</p>\n<h2 id=\"interactions-and-data-flow\">Interactions and Data Flow</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (M1: Simple Linear Regression - basic training and prediction flow, M2: Gradient Descent - iterative optimization flow, M3: Multiple Linear Regression - vectorized operations flow)</p>\n</blockquote>\n<p>This section describes the orchestration of all components working together to transform raw data into trained models and generate predictions. Understanding these interactions is crucial because machine learning systems are inherently pipeline-oriented, where data flows through multiple transformation stages, each component depends on specific input formats and contracts, and the sequence of operations determines both correctness and performance.</p>\n<h3 id=\"mental-model-the-assembly-line-for-intelligence\">Mental Model: The Assembly Line for Intelligence</h3>\n<p>Think of our linear regression system as an <strong>intelligent assembly line</strong> where raw data enters at one end and emerges as a trained model capable of making predictions. Just like a car assembly line has specific stations (body assembly, engine installation, painting, quality control), our system has specialized components that must work in precise sequence.</p>\n<p>The <strong>DataHandler</strong> acts like the <strong>receiving dock</strong> - it accepts raw materials (CSV files, arrays) and prepares them for processing by checking quality, standardizing formats, and organizing everything properly. The <strong>model components</strong> (SimpleLinearRegression, GradientDescentOptimizer, MultipleLinearRegression) are like <strong>specialized workstations</strong> - each has specific tools and capabilities, and they transform the data in predictable ways. The <strong>prediction pipeline</strong> is like the <strong>shipping department</strong> - it takes the finished product (trained model) and uses it to fulfill new orders (generate predictions for new data).</p>\n<p>Critical to this analogy is understanding that unlike a physical assembly line, our data flows through <strong>state transformations</strong> rather than physical modifications. The raw features don&#39;t get &quot;consumed&quot; - instead, they get <strong>analyzed</strong> to extract mathematical relationships (parameters) that capture the underlying patterns. Once we have these parameters, we can apply them to entirely new data to make predictions.</p>\n<h3 id=\"training-data-flow\">Training Data Flow</h3>\n<p>The training process represents the core learning pipeline where raw data transforms into a mathematical model capable of making predictions. This flow varies slightly between our three milestone approaches, but follows a consistent pattern of data preparation, parameter learning, and model validation.</p>\n<h4 id=\"step-by-step-training-process\">Step-by-Step Training Process</h4>\n<p>The complete training data flow follows these sequential stages:</p>\n<p><strong>Stage 1: Data Ingestion and Initial Validation</strong></p>\n<ol>\n<li>Raw data enters through <code>DataHandler.load_csv_data()</code> or direct array input</li>\n<li>The system validates basic data properties using <code>validate_data()</code> - checking for proper numeric types, compatible array shapes, and absence of pathological values like infinite or NaN entries</li>\n<li>Data gets packaged into a <code>Dataset</code> structure that includes feature matrix, target vector, metadata about dimensionality, and normalization status</li>\n<li>The handler performs regression-specific validation using <code>validate_regression_inputs()</code> to ensure we have sufficient samples relative to features (minimum 10:1 ratio) and that the target variable has adequate variance for meaningful fitting</li>\n</ol>\n<p><strong>Stage 2: Feature Preprocessing and Normalization</strong> \n5. For gradient descent and multiple regression (M2/M3), the system applies z-score normalization via <code>normalize_features()</code> to prevent features with large scales from dominating the optimization\n6. The <code>StandardScaler</code> computes and stores mean and standard deviation statistics for each feature, enabling consistent transformation of future prediction data<br>7. For multiple regression, the system constructs the design matrix using <code>create_design_matrix()</code>, adding an intercept column of ones to handle the bias term mathematically\n8. Normalized features and scaling parameters get stored in the updated <code>Dataset</code> structure for later use during prediction</p>\n<p><strong>Stage 3: Model Initialization and Parameter Setup</strong>\n9. The chosen model component (SimpleLinearRegression, GradientDescentRegression, or MultipleLinearRegression) initializes its internal parameter storage structures\n10. For closed-form solutions (M1), no initial parameter values are needed since the normal equation computes them directly\n11. For gradient descent approaches (M2/M3), parameters initialize to zeros or small random values, and the <code>TrainingHistory</code> structure gets prepared to track the optimization process\n12. The system validates that learning rate, convergence tolerance, and maximum iteration settings are within reasonable bounds</p>\n<p><strong>Stage 4: Parameter Learning Process</strong>\n13. <strong>Closed-form path (M1)</strong>: The SimpleLinearRegression component applies the ordinary least squares normal equation directly, computing optimal slope and intercept in a single mathematical operation\n14. <strong>Iterative optimization path (M2/M3)</strong>: The gradient descent loop begins, alternating between cost computation using <code>_compute_cost()</code>, gradient calculation using <code>_compute_gradients()</code>, and parameter updates following the gradient descent rule\n15. Each iteration, the system checks multiple convergence criteria through <code>ConvergenceDetector</code> - cost improvement below threshold, gradient magnitude below tolerance, or parameter changes below minimum significance\n16. The optimization process records comprehensive training history including cost values, parameter evolution, and gradient norms for debugging and analysis purposes</p>\n<p><strong>Stage 5: Model Finalization and Validation</strong>\n17. Upon convergence or reaching maximum iterations, the system packages final parameters into <code>ModelParameters</code> structure with metadata about the fitting method, regularization strength, and feature scaling applied\n18. The model&#39;s <code>is_fitted_</code> flag gets set to True, enabling prediction capabilities while preventing accidental reuse of untrained models\n19. Training history gets finalized with convergence status, final iteration count, and the specific reason for termination (tolerance reached, max iterations, numerical instability detected)\n20. The system validates the learned parameters for numerical stability - checking for overflow, underflow, or parameters that suggest overfitting or underfitting</p>\n<h4 id=\"data-format-transformations-during-training\">Data Format Transformations During Training</h4>\n<table>\n<thead>\n<tr>\n<th>Stage</th>\n<th>Input Format</th>\n<th>Output Format</th>\n<th>Key Transformations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Loading</td>\n<td>CSV files or raw arrays</td>\n<td><code>Dataset</code> structure</td>\n<td>Type conversion, shape validation, metadata extraction</td>\n</tr>\n<tr>\n<td>Preprocessing</td>\n<td>Raw numeric features</td>\n<td>Normalized features + scaling stats</td>\n<td>Z-score normalization, design matrix construction</td>\n</tr>\n<tr>\n<td>Parameter Learning</td>\n<td>Prepared feature matrix + targets</td>\n<td><code>ModelParameters</code> + <code>TrainingHistory</code></td>\n<td>Optimization algorithm application</td>\n</tr>\n<tr>\n<td>Model Storage</td>\n<td>Optimization results</td>\n<td>Fitted model with prediction capability</td>\n<td>Parameter packaging, validation flags</td>\n</tr>\n</tbody></table>\n<h4 id=\"training-flow-variations-by-milestone\">Training Flow Variations by Milestone</h4>\n<p><strong>Milestone 1 (Simple Linear Regression)</strong>: The training flow emphasizes the closed-form solution path. Data flows from loading through basic validation directly to the normal equation computation. No iterative optimization occurs, making this the fastest training path but limited to single-variable problems without regularization.</p>\n<p><strong>Milestone 2 (Gradient Descent)</strong>: Introduces the iterative optimization flow with comprehensive convergence detection. Data flows through the same preprocessing stages, but then enters the gradient descent loop with extensive history tracking. This milestone emphasizes understanding the optimization dynamics and debugging convergence issues.</p>\n<p><strong>Milestone 3 (Multiple Linear Regression)</strong>: Extends to vectorized operations with feature scaling and regularization. The training flow handles arbitrary feature counts through matrix operations, includes Ridge regularization in the cost function, and demonstrates how proper preprocessing enables stable optimization of high-dimensional problems.</p>\n<h4 id=\"critical-synchronization-points\">Critical Synchronization Points</h4>\n<p>The training pipeline has several critical synchronization points where components must coordinate:</p>\n<p><strong>Data-Model Interface</strong>: The model component must validate that incoming data matches its expected dimensionality and format. For single regression, this means ensuring exactly one feature column. For multiple regression, this means validating that the feature count matches what the model was configured to handle.</p>\n<p><strong>Preprocessing-Prediction Consistency</strong>: The normalization statistics computed during training must be preserved and applied identically during prediction. This requires careful coordination between <code>StandardScaler</code> state and the model&#39;s prediction pipeline.</p>\n<p><strong>Convergence-History Synchronization</strong>: The optimization loop must maintain perfect synchronization between parameter updates and history recording. Any mismatch between <code>parameter_history_</code> and <code>cost_history_</code> indices would break debugging and analysis capabilities.</p>\n<h3 id=\"prediction-data-flow\">Prediction Data Flow</h3>\n<p>The prediction process transforms new input data through the same preprocessing pipeline used during training, then applies the learned model parameters to generate predictions along with confidence metrics and error estimates.</p>\n<h4 id=\"mental-model-the-factory-production-line\">Mental Model: The Factory Production Line</h4>\n<p>Think of prediction as a <strong>factory production line</strong> that&#39;s been calibrated during training. The training process was like <strong>setting up and calibrating</strong> all the machinery - determining the exact specifications, adjusting the equipment, and validating the quality control procedures. Now prediction is the <strong>actual production run</strong> - raw materials (new data) flow through the same stations, but instead of learning the settings, we apply the previously learned settings to transform inputs into outputs.</p>\n<p>The key insight is that prediction must <strong>exactly replicate</strong> the training preprocessing pipeline. If training data was normalized, prediction data must be normalized using the same statistics. If training used a design matrix with intercept column, prediction must construct an identical design matrix structure. Any deviation from this preprocessing consistency will produce incorrect predictions.</p>\n<h4 id=\"step-by-step-prediction-process\">Step-by-Step Prediction Process</h4>\n<p><strong>Stage 1: Input Validation and Format Checking</strong></p>\n<ol>\n<li>New data enters through the model&#39;s <code>predict()</code> method as raw feature arrays</li>\n<li>The system validates input data format, checking for correct data types, absence of NaN/infinite values, and appropriate array dimensions</li>\n<li>For single regression, validation ensures exactly one feature per sample; for multiple regression, validation confirms feature count matches training dimensionality</li>\n<li>The model checks its <code>is_fitted_</code> status to prevent prediction attempts on untrained models</li>\n</ol>\n<p><strong>Stage 2: Preprocessing Pipeline Replication</strong>\n5. If the model was trained with feature normalization, prediction data gets transformed using the stored <code>StandardScaler</code> statistics via <code>apply_normalization()</code>\n6. The same mean and standard deviation values from training get applied to normalize new features, ensuring identical scaling\n7. For multiple regression, the system constructs the design matrix using <code>create_design_matrix()</code>, adding the intercept column in the same position as during training\n8. All preprocessing steps must exactly match the training pipeline to maintain mathematical consistency</p>\n<p><strong>Stage 3: Model Parameter Application</strong>\n9. The system retrieves stored model parameters (<code>slope_</code>, <code>intercept_</code>, or <code>weights_</code>) from the fitted model\n10. <strong>Single regression path</strong>: Predictions compute as <code>y_pred = slope_ * x + intercept_</code> using scalar arithmetic\n11. <strong>Multiple regression path</strong>: Predictions compute as <code>y_pred = X @ weights_</code> using vectorized matrix multiplication where <code>X</code> is the design matrix and <code>weights_</code> includes the intercept term\n12. The mathematical operations produce raw prediction values for each input sample</p>\n<p><strong>Stage 4: Prediction Packaging and Metadata Generation</strong>\n13. Raw predictions get packaged into a <code>PredictionResult</code> structure along with comprehensive metadata\n14. The system computes residuals if ground truth values are provided, calculating <code>residuals = y_true - y_pred</code>\n15. Confidence intervals get estimated using prediction variance calculations (simplified bootstrap or analytical approximations)\n16. Performance metrics like mean squared error and mean absolute error get computed when ground truth is available for validation</p>\n<p><strong>Stage 5: Output Validation and Error Checking</strong>\n17. The system validates prediction outputs for numerical stability, checking for overflow, underflow, or impossible values\n18. Predictions get range-checked against reasonable bounds based on training data statistics\n19. The <code>PredictionResult</code> structure gets populated with input metadata like sample count, feature dimensionality, and model type for traceability\n20. Final output includes both predictions and comprehensive diagnostic information for debugging and analysis</p>\n<h4 id=\"prediction-data-format-requirements\">Prediction Data Format Requirements</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Input Requirements</th>\n<th>Output Format</th>\n<th>Validation Checks</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Single Regression</td>\n<td>1D array or 2D array with 1 column</td>\n<td>1D prediction array</td>\n<td>Shape, data type, finite values</td>\n</tr>\n<tr>\n<td>Multiple Regression</td>\n<td>2D array with n_features columns</td>\n<td>1D prediction array + metadata</td>\n<td>Dimension match, normalization compatibility</td>\n</tr>\n<tr>\n<td>All Models</td>\n<td>Same preprocessing as training</td>\n<td><code>PredictionResult</code> structure</td>\n<td>Model fitted status, numerical stability</td>\n</tr>\n</tbody></table>\n<h4 id=\"preprocessing-consistency-requirements\">Preprocessing Consistency Requirements</h4>\n<p>The prediction pipeline must maintain perfect consistency with the training preprocessing:</p>\n<p><strong>Normalization Consistency</strong>: The <code>StandardScaler</code> fitted during training stores <code>mean_</code> and <code>std_</code> arrays that must be applied identically to prediction data. Using different normalization statistics would shift and scale features incorrectly, producing meaningless predictions.</p>\n<p><strong>Design Matrix Consistency</strong>: For multiple regression, the design matrix construction during prediction must match training exactly - same intercept column position, same feature ordering, same data types. Matrix shape mismatches would cause dimension errors in the prediction computation.</p>\n<p><strong>Data Type Consistency</strong>: Prediction data must use the same numeric precision (float32 vs float64) as training data to prevent subtle numerical differences that accumulate through matrix operations.</p>\n<h4 id=\"prediction-flow-variations-by-milestone\">Prediction Flow Variations by Milestone</h4>\n<p><strong>Milestone 1 Prediction Flow</strong>: Simple scalar arithmetic with minimal preprocessing. The flow emphasizes understanding the basic prediction equation and handling edge cases like division by zero or extreme input values.</p>\n<p><strong>Milestone 2 Prediction Flow</strong>: Identical to M1 mathematically, but includes more comprehensive error checking and history tracking for debugging optimization issues. The flow demonstrates how gradient descent parameters produce identical predictions to closed-form solutions.</p>\n<p><strong>Milestone 3 Prediction Flow</strong>: Full vectorized pipeline with matrix operations, feature scaling, and regularization effects. The flow handles arbitrary dimensionality and demonstrates computational efficiency through proper vectorization.</p>\n<h3 id=\"component-communication-protocols\">Component Communication Protocols</h3>\n<p>The communication between system components follows strict interface contracts that ensure data consistency, error propagation, and state management throughout the training and prediction pipelines.</p>\n<h4 id=\"mental-model-the-diplomatic-embassy-system\">Mental Model: The Diplomatic Embassy System</h4>\n<p>Think of component communication as a <strong>diplomatic embassy system</strong> where each component is a sovereign nation with its own internal rules, but they must communicate through <strong>formal diplomatic protocols</strong> to accomplish shared goals. Each component has <strong>ambassadors</strong> (public methods) that handle incoming requests and <strong>treaties</strong> (interface contracts) that specify exactly what information gets exchanged in what format.</p>\n<p>The <code>DataHandler</code> acts like the <strong>United Nations</strong> - it speaks everyone&#39;s language and provides neutral ground for data exchange. The model components are like <strong>specialized technical agencies</strong> - each has deep expertise in its domain but needs standardized protocols to share information. The prediction system is like an <strong>international trade network</strong> - it must ensure that goods (data) produced in one country (component) can be consumed safely in another.</p>\n<p>Critical to this analogy is understanding that <strong>protocol violations are serious diplomatic incidents</strong> - if one component sends data in the wrong format or violates interface contracts, the entire system can fail in unpredictable ways. Therefore, our protocols include extensive <strong>verification</strong> (input validation), <strong>translation services</strong> (data format conversion), and <strong>error reporting mechanisms</strong> (exception handling with context).</p>\n<h4 id=\"interface-contracts-and-data-exchange-formats\">Interface Contracts and Data Exchange Formats</h4>\n<p>Each component exposes well-defined interfaces with strict contracts about input requirements, output guarantees, and error conditions:</p>\n<p><strong>DataHandler Interface Contract</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Input Requirements</th>\n<th>Output Guarantees</th>\n<th>Error Conditions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>load_csv_data()</code></td>\n<td>Valid file path, existing columns</td>\n<td><code>Dataset</code> with validated arrays</td>\n<td>File not found, column missing, invalid data types</td>\n</tr>\n<tr>\n<td><code>validate_data()</code></td>\n<td>Feature and target arrays</td>\n<td>Silent success or exception</td>\n<td>Shape mismatch, NaN values, insufficient samples</td>\n</tr>\n<tr>\n<td><code>normalize_features()</code></td>\n<td>Numeric feature matrix</td>\n<td><code>Dataset</code> with normalized features + stats</td>\n<td>Constant features, numerical overflow</td>\n</tr>\n<tr>\n<td><code>apply_normalization()</code></td>\n<td>Features + fitted scaler</td>\n<td>Consistently scaled feature matrix</td>\n<td>Dimension mismatch, unfitted scaler</td>\n</tr>\n</tbody></table>\n<p>The DataHandler acts as the <strong>data embassy</strong>, translating between external formats (CSV files, raw arrays) and internal formats (<code>Dataset</code> structures). It guarantees that any <code>Dataset</code> it produces will be mathematically valid for regression - proper shapes, numeric types, no pathological values.</p>\n<p><strong>Model Component Interface Contracts</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Input Requirements</th>\n<th>Output Guarantees</th>\n<th>State Changes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>fit(x, y)</code></td>\n<td>Validated feature/target arrays</td>\n<td>Fitted model with <code>is_fitted_=True</code></td>\n<td>Parameters learned, history recorded</td>\n</tr>\n<tr>\n<td><code>predict(x)</code></td>\n<td>Same format as training features</td>\n<td>Prediction array matching input samples</td>\n<td>No state changes</td>\n</tr>\n<tr>\n<td><code>score(x, y)</code></td>\n<td>Features + ground truth targets</td>\n<td>R-squared coefficient in [0, 1]</td>\n<td>No state changes</td>\n</tr>\n</tbody></table>\n<p>Each model component acts as a <strong>specialized technical agency</strong> with deep expertise in its optimization approach. The interface contracts ensure that regardless of internal implementation differences (closed-form vs gradient descent vs regularization), all models expose identical external interfaces.</p>\n<p><strong>Gradient Descent Optimizer Specific Contracts</strong></p>\n<table>\n<thead>\n<tr>\n<th>Internal Method</th>\n<th>Input Requirements</th>\n<th>Output Guarantees</th>\n<th>Side Effects</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>_compute_cost()</code></td>\n<td>Features, targets, current parameters</td>\n<td>Non-negative cost value</td>\n<td>None</td>\n</tr>\n<tr>\n<td><code>_compute_gradients()</code></td>\n<td>Features, targets, current parameters</td>\n<td>Gradient vector/tuple</td>\n<td>None</td>\n</tr>\n<tr>\n<td><code>check_convergence()</code></td>\n<td>Cost history, parameters, gradients</td>\n<td>Boolean convergence status + reason</td>\n<td>History updates</td>\n</tr>\n</tbody></table>\n<p>The gradient descent optimizer maintains additional internal contracts for the optimization loop. These methods are prefixed with underscore to indicate internal use, but they follow strict mathematical contracts that enable testing and debugging.</p>\n<h4 id=\"message-formats-and-data-structures\">Message Formats and Data Structures</h4>\n<p>The communication between components uses standardized data structures that act as <strong>diplomatic messages</strong> - each has a specific format that all parties understand:</p>\n<p><strong>Dataset Message Format</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Dataset Structure:\n- features: np.ndarray (n_samples, n_features) - Core feature matrix\n- targets: np.ndarray (n_samples,) - Target values for supervised learning  \n- feature_names: List[str] - Human-readable feature identifiers\n- n_samples: int - Number of training examples\n- n_features: int - Dimensionality of feature space\n- is_normalized: bool - Whether features have been scaled\n- normalization_stats: Dict[str, np.ndarray] - Mean/std for consistent scaling</code></pre></div>\n\n<p>The <code>Dataset</code> structure acts as the <strong>universal data passport</strong> that can cross component boundaries safely. Every component that receives a <code>Dataset</code> can trust its format and mathematical properties.</p>\n<p><strong>ModelParameters Message Format</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>ModelParameters Structure:\n- slope_: float - Single regression slope coefficient\n- intercept_: float - Bias term for all regression types  \n- weights_: np.ndarray - Multiple regression weight vector\n- is_fitted_: bool - Training completion status\n- fitting_method_: str - Algorithm used (&quot;closed_form&quot;, &quot;gradient_descent&quot;, &quot;ridge&quot;)\n- regularization_strength_: float - L2 penalty coefficient\n- feature_count_: int - Expected input dimensionality</code></pre></div>\n\n<p>The <code>ModelParameters</code> structure serves as the <strong>credentials</strong> that prove a model has been properly trained and specify exactly what kind of predictions it can make.</p>\n<p><strong>TrainingHistory Message Format</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>TrainingHistory Structure:\n- cost_history_: List[float] - Optimization objective over time\n- parameter_history_: List[Tuple[float, float]] - Single regression parameter evolution\n- weight_history_: List[np.ndarray] - Multiple regression parameter evolution  \n- gradient_norms_: List[float] - Optimization progress indicators\n- converged_: bool - Whether optimization completed successfully\n- final_iteration_: int - Stopping iteration number\n- convergence_reason_: str - Specific termination condition met</code></pre></div>\n\n<p>The <code>TrainingHistory</code> structure acts as the <strong>optimization logbook</strong> that records the complete learning process for debugging and analysis.</p>\n<h4 id=\"component-interaction-patterns\">Component Interaction Patterns</h4>\n<p><strong>Producer-Consumer Pattern</strong>: The DataHandler produces <code>Dataset</code> structures that model components consume during training. This pattern ensures data flows unidirectionally and prevents circular dependencies.</p>\n<p><strong>Command Pattern</strong>: The <code>fit()</code> method represents a command that changes model state from unfitted to fitted. The command pattern ensures that training is an atomic operation - either it completes successfully with a valid model, or it fails completely without partial state.</p>\n<p><strong>Factory Pattern</strong>: Different model classes (SimpleLinearRegression, GradientDescentRegression, MultipleLinearRegression) implement the same interface contract but create different internal implementations. This allows the system to be extended with new optimization algorithms without changing client code.</p>\n<p><strong>Observer Pattern</strong>: The gradient descent optimization loop notifies the <code>TrainingHistory</code> observer of each iteration&#39;s results. This pattern enables comprehensive logging and debugging without cluttering the core optimization logic.</p>\n<h4 id=\"error-propagation-and-recovery-protocols\">Error Propagation and Recovery Protocols</h4>\n<p>Component communication includes structured error handling that preserves diagnostic information across component boundaries:</p>\n<p><strong>Data Validation Errors</strong>: When DataHandler detects invalid input data, it raises specific exceptions with detailed context about what validation failed and suggested fixes. These errors should <strong>fail fast</strong> rather than allowing invalid data to corrupt downstream processing.</p>\n<p><strong>Numerical Stability Errors</strong>: When optimization components detect numerical instability (overflow, underflow, divergence), they package diagnostic information about parameter values, gradient magnitudes, and convergence history before propagating the error upward.</p>\n<p><strong>Interface Contract Violations</strong>: When a component receives input that violates its interface contract (wrong dimensions, unfitted model, etc.), it raises immediately with specific information about what was expected versus what was received.</p>\n<p><strong>Graceful Degradation</strong>: In some cases, components can detect problems and recover gracefully - for example, if gradient descent fails to converge, it can still return the best parameters found so far along with a warning about convergence failure.</p>\n<h4 id=\"architecture-decision-records-for-communication-design\">Architecture Decision Records for Communication Design</h4>\n<blockquote>\n<p><strong>Decision: Immutable Data Structures</strong></p>\n<ul>\n<li><strong>Context</strong>: Components need to share data safely without accidental modification</li>\n<li><strong>Options Considered</strong>: Mutable shared state, deep copying, immutable structures</li>\n<li><strong>Decision</strong>: Use immutable data structures with explicit copy operations when mutation needed</li>\n<li><strong>Rationale</strong>: Prevents bugs from accidental state modification, makes data flow more predictable, enables safe parallel processing</li>\n<li><strong>Consequences</strong>: Slight memory overhead from copying, but eliminates entire class of state-mutation bugs</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Explicit Interface Contracts</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to ensure components can interoperate reliably across different implementations</li>\n<li><strong>Options Considered</strong>: Duck typing, formal interfaces, documentation-only contracts</li>\n<li><strong>Decision</strong>: Formal interface definitions with runtime validation</li>\n<li><strong>Rationale</strong>: Catches integration errors early, enables confident refactoring, provides clear contracts for testing</li>\n<li><strong>Consequences</strong>: More verbose code, but much more reliable component integration</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Structured Error Information</strong></p>\n<ul>\n<li><strong>Context</strong>: Machine learning systems can fail in complex ways that require detailed debugging</li>\n<li><strong>Options Considered</strong>: Simple exception messages, error codes, structured diagnostic objects</li>\n<li><strong>Decision</strong>: Structured exception objects with diagnostic context and suggested fixes</li>\n<li><strong>Rationale</strong>: ML debugging often requires understanding numerical details, parameter states, and convergence history</li>\n<li><strong>Consequences</strong>: More complex error handling code, but much better debugging experience</li>\n</ul>\n</blockquote>\n<p><img src=\"/api/project/linear-regression/architecture-doc/asset?path=diagrams%2Fsystem-components.svg\" alt=\"System Component Architecture\"></p>\n<p><img src=\"/api/project/linear-regression/architecture-doc/asset?path=diagrams%2Ftraining-sequence.svg\" alt=\"Training Process Sequence\"></p>\n<h4 id=\"common-pitfalls-in-component-communication\">Common Pitfalls in Component Communication</h4>\n<p>⚠️ <strong>Pitfall: Preprocessing Inconsistency Between Training and Prediction</strong>\nThe most common communication failure occurs when prediction data gets preprocessed differently than training data. For example, if training data was normalized but prediction data is not, or if different normalization statistics are used. This breaks the mathematical assumptions of the learned model parameters.</p>\n<p><strong>Detection</strong>: Predictions will have wrong magnitude/scale, or you&#39;ll get numerical errors from matrix operations with mismatched dimensions.</p>\n<p><strong>Fix</strong>: Always use the same <code>StandardScaler</code> instance for both training and prediction, and validate that preprocessing steps exactly match between training and prediction pipelines.</p>\n<p>⚠️ <strong>Pitfall: State Mutation During Prediction</strong>\nSome implementations accidentally modify model state during prediction (e.g., updating history logs, changing parameters). This breaks the prediction contract and can cause race conditions in concurrent environments.</p>\n<p><strong>Detection</strong>: Model behavior changes between prediction calls, or predictions give different results for identical inputs.</p>\n<p><strong>Fix</strong>: Ensure prediction methods are truly read-only - no modifications to any model attributes, no appending to history lists, no parameter updates.</p>\n<p>⚠️ <strong>Pitfall: Interface Contract Violations Under Edge Cases</strong>\nComponents often work correctly for normal inputs but violate interface contracts for edge cases like single-sample datasets, constant features, or extreme parameter values.</p>\n<p><strong>Detection</strong>: Intermittent failures that are hard to reproduce, or failures that only occur with specific datasets.</p>\n<p><strong>Fix</strong>: Implement comprehensive input validation in every public method, with specific checks for mathematical edge cases relevant to regression.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component Layer</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Flow Orchestration</td>\n<td>Simple function calls with explicit sequencing</td>\n<td>Pipeline classes with step validation and rollback</td>\n</tr>\n<tr>\n<td>Inter-Component Communication</td>\n<td>Direct method calls with exception handling</td>\n<td>Message passing with async queues and retry logic</td>\n</tr>\n<tr>\n<td>State Management</td>\n<td>Object attributes with manual validation</td>\n<td>State machines with transition guards and logging</td>\n</tr>\n<tr>\n<td>Error Propagation</td>\n<td>Standard Python exceptions with context</td>\n<td>Structured error objects with diagnostic payloads</td>\n</tr>\n<tr>\n<td>Data Serialization</td>\n<td>JSON for simple cases, pickle for arrays</td>\n<td>Protocol buffers or Apache Arrow for performance</td>\n</tr>\n<tr>\n<td>Logging and Monitoring</td>\n<td>Python logging module with structured messages</td>\n<td>Dedicated ML monitoring with metrics collection</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure-for-communication-management\">Recommended File Structure for Communication Management</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>linear_regression/\n├── core/\n│   ├── __init__.py\n│   ├── interfaces.py          ← Abstract base classes defining contracts\n│   ├── data_types.py          ← All data structures (Dataset, ModelParameters, etc.)\n│   └── exceptions.py          ← Structured exception classes\n├── pipeline/\n│   ├── __init__.py  \n│   ├── training_pipeline.py   ← Orchestrates training data flow\n│   ├── prediction_pipeline.py ← Orchestrates prediction data flow  \n│   └── validation.py          ← Cross-component validation logic\n├── components/\n│   ├── data_handler.py        ← Data loading and preprocessing\n│   ├── simple_regression.py   ← M1 implementation\n│   ├── gradient_descent.py    ← M2 implementation  \n│   └── multiple_regression.py ← M3 implementation\n└── utils/\n    ├── communication.py       ← Helper functions for component interaction\n    └── debugging.py           ← Diagnostic tools for data flow analysis</code></pre></div>\n\n<h4 id=\"core-data-structures-implementation\">Core Data Structures Implementation</h4>\n<p>Complete, ready-to-use data structures that define the communication contracts:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Core data types for component communication.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Tuple, Optional, Union</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Dataset</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Universal data structure for component communication.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    features: np.ndarray</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    targets: np.ndarray  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    feature_names: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    n_samples: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    n_features: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    is_normalized: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    normalization_stats: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, np.ndarray] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate data consistency after initialization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.features.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.targets.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Sample count mismatch: features </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.features.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, targets </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.targets.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.features.shape[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.n_features:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Feature count mismatch: array </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.features.shape[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, metadata </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.n_features</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ModelParameters</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Trained model parameters with metadata.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    slope_: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    intercept_: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    weights_: Optional[np.ndarray] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    is_fitted_: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fitting_method_: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    regularization_strength_: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    feature_count_: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_for_prediction</span><span style=\"color:#E1E4E8\">(self, n_features: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Ensure model is ready for prediction with given feature count.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_fitted_:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Model must be fitted before making predictions\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.feature_count_ </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> n_features:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Feature count mismatch: trained </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.feature_count_</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, input </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">n_features</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TrainingHistory</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Complete optimization history for analysis and debugging.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cost_history_: List[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parameter_history_: List[Tuple[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">) </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    weight_history_: List[np.ndarray] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gradient_norms_: List[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    learning_rate: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.01</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_iterations: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1000</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tolerance: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-6</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    converged_: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    final_iteration_: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    convergence_reason_: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PredictionResult</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Complete prediction output with diagnostics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    predictions: np.ndarray</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    residuals: Optional[np.ndarray] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    confidence_intervals: Optional[np.ndarray] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    r_squared: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mean_squared_error: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mean_absolute_error: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    input_shape: Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    model_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span></code></pre></div>\n\n<h4 id=\"pipeline-orchestration-implementation\">Pipeline Orchestration Implementation</h4>\n<p>Complete training and prediction pipeline orchestrators:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Pipeline orchestration for training and prediction workflows.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Union, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .core.data_types </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dataset, ModelParameters, TrainingHistory, PredictionResult</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .core.exceptions </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ValidationError, NumericalInstabilityError</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TrainingPipeline</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Orchestrates the complete training data flow.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, data_handler, model_component):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.data_handler </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> data_handler</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model_component</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.history </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TrainingHistory()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> execute_training_flow</span><span style=\"color:#E1E4E8\">(self, data_source: Union[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, np.ndarray], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                            feature_columns: Optional[List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                            target_column: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> ModelParameters:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Execute complete training pipeline from data source to fitted model.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns fitted ModelParameters with complete training history.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises structured exceptions with diagnostic information.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load and validate raw data using data_handler.load_csv_data() or direct array input</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Apply preprocessing pipeline (normalization, design matrix construction) if needed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Initialize model component with training configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Execute model.fit() with preprocessed data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate training results and extract final parameters </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Package results with complete training history</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Run post-training validation checks for numerical stability</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Package diagnostic information with the exception</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._add_diagnostic_context(e)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PredictionPipeline</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Orchestrates the complete prediction data flow.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, model_parameters: ModelParameters, data_handler):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.parameters </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model_parameters</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.data_handler </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> data_handler</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> execute_prediction_flow</span><span style=\"color:#E1E4E8\">(self, new_data: np.ndarray, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                              ground_truth: Optional[np.ndarray] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> PredictionResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Execute complete prediction pipeline maintaining preprocessing consistency.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns PredictionResult with predictions and diagnostic metrics.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate input data format and dimensions against model requirements</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Apply identical preprocessing pipeline used during training</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Generate predictions using stored model parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Compute confidence intervals and error estimates if possible</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Calculate performance metrics if ground truth provided</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Package results with comprehensive metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Validate output for numerical stability and reasonable ranges</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._add_diagnostic_context(e, new_data)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span></span></code></pre></div>\n\n<h4 id=\"component-interface-definitions\">Component Interface Definitions</h4>\n<p>Abstract base classes that define the communication contracts:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Abstract interfaces defining component communication contracts.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .data_types </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dataset, ModelParameters, PredictionResult</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DataHandlerInterface</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Contract for data loading and preprocessing components.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> load_csv_data</span><span style=\"color:#E1E4E8\">(self, filename: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, feature_columns: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                     target_column: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Tuple[np.ndarray, np.ndarray]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load data from CSV with validation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_data</span><span style=\"color:#E1E4E8\">(self, features: np.ndarray, targets: np.ndarray) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate data meets regression requirements.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> normalize_features</span><span style=\"color:#E1E4E8\">(self, features: np.ndarray) -> Dataset:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Apply z-score normalization and return Dataset.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RegressionModelInterface</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Contract for all regression model implementations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> fit</span><span style=\"color:#E1E4E8\">(self, x: np.ndarray, y: np.ndarray) -> </span><span style=\"color:#9ECBFF\">'RegressionModelInterface'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Train model on data, return self for chaining.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> predict</span><span style=\"color:#E1E4E8\">(self, x: np.ndarray) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate predictions for input data.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> score</span><span style=\"color:#E1E4E8\">(self, x: np.ndarray, y: np.ndarray) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate R-squared coefficient of determination.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"error-handling-and-diagnostic-classes\">Error Handling and Diagnostic Classes</h4>\n<p>Structured exception classes with diagnostic information:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Structured exceptions with ML-specific diagnostic information.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MLException</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class for ML-specific exceptions with diagnostic context.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, context: </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, suggested_fix: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(message)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.context </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> context </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.suggested_fix </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> suggested_fix</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> add_context</span><span style=\"color:#E1E4E8\">(self, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Add diagnostic information to the exception.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.context.update(kwargs)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __str__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        base_msg </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__str__</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.context:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            context_str </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \", \"</span><span style=\"color:#E1E4E8\">.join(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">k</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">v</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> k, v </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.context.items())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            base_msg </span><span style=\"color:#F97583\">+=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\" (Context: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">context_str</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.suggested_fix:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            base_msg </span><span style=\"color:#F97583\">+=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\" (Suggested fix: </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.suggested_fix</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> base_msg</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ValidationError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">MLException</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Data validation failed with specific diagnostic information.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> NumericalInstabilityError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">MLException</span><span style=\"color:#E1E4E8\">):  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Numerical computation became unstable.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ConvergenceError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">MLException</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Optimization failed to converge within specified criteria.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> InterfaceContractViolation</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">MLException</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Component interface contract was violated.\"\"\"</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints-for-data-flow\">Milestone Checkpoints for Data Flow</h4>\n<p><strong>Milestone 1 Checkpoint</strong>: After implementing basic training and prediction flow:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test the complete pipeline with synthetic data</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_training_pipeline.py::test_simple_regression_flow</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected behavior: </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Data loads successfully from CSV or array input</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Model trains using closed-form solution  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Predictions match expected values within tolerance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - R-squared score is reasonable (> 0.8 for synthetic data with low noise)</span></span></code></pre></div>\n\n<p><strong>Milestone 2 Checkpoint</strong>: After implementing gradient descent optimization flow:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test iterative optimization pipeline</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_gradient_descent_pipeline.py::test_convergence_flow</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected behavior:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Cost decreases monotonically during training</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Gradient descent parameters match closed-form solution within tolerance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Training history captures complete optimization trajectory</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Convergence detection works correctly for different tolerance settings</span></span></code></pre></div>\n\n<p><strong>Milestone 3 Checkpoint</strong>: After implementing multiple regression flow:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test vectorized operations pipeline  </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_multiple_regression_pipeline.py::test_feature_scaling_flow</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected behavior:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Multiple features handled correctly through matrix operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Feature normalization applied consistently between training/prediction  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Regularization affects parameter values as expected</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Performance scales reasonably with feature count</span></span></code></pre></div>\n\n<h4 id=\"debugging-data-flow-issues\">Debugging Data Flow Issues</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnostic Steps</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Predictions have wrong scale/magnitude</td>\n<td>Preprocessing inconsistency between train/predict</td>\n<td>Check normalization stats, compare preprocessing steps</td>\n<td>Use identical StandardScaler for both pipelines</td>\n</tr>\n<tr>\n<td>Matrix dimension errors during prediction</td>\n<td>Design matrix construction differs from training</td>\n<td>Verify intercept column handling, feature count</td>\n<td>Ensure identical design matrix creation</td>\n</tr>\n<tr>\n<td>Model state changes during prediction</td>\n<td>Prediction method modifies model attributes</td>\n<td>Add assertions for immutability, check history updates</td>\n<td>Make prediction methods read-only</td>\n</tr>\n<tr>\n<td>Intermittent failures with edge cases</td>\n<td>Interface validation missing for boundary conditions</td>\n<td>Test with single samples, constant features, extreme values</td>\n<td>Add comprehensive input validation</td>\n</tr>\n<tr>\n<td>Performance degrades with more features</td>\n<td>Inefficient data copying or non-vectorized operations</td>\n<td>Profile memory usage and computational complexity</td>\n<td>Use in-place operations and proper vectorization</td>\n</tr>\n</tbody></table>\n<h2 id=\"error-handling-and-edge-cases\">Error Handling and Edge Cases</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (M1: Simple Linear Regression - handles basic data validation and division by zero, M2: Gradient Descent - adds optimization failure detection and learning rate issues, M3: Multiple Linear Regression - comprehensive matrix dimension validation and regularization edge cases)</p>\n</blockquote>\n<p>This section establishes a comprehensive error handling strategy that transforms potential silent failures into informative feedback, enabling learners to debug their implementations effectively while understanding the numerical challenges inherent in machine learning algorithms.</p>\n<h3 id=\"mental-model-the-safety-net-system\">Mental Model: The Safety Net System</h3>\n<p>Think of error handling in linear regression like a multi-layered safety net system at a construction site. Just as construction workers need protection at different heights and stages of building, our machine learning system needs protection at different phases of data processing and model training.</p>\n<p>The <strong>first safety net</strong> catches you early - this is input validation that prevents bad data from entering the system, like checking that workers have proper safety equipment before they even start climbing. The <strong>second safety net</strong> operates during computation - this is numerical stability monitoring that detects when mathematical operations are becoming dangerous, like sensors that warn when wind speeds make crane operation unsafe. The <strong>third safety net</strong> handles optimization failures - this is convergence monitoring that recognizes when the learning process itself is going wrong, like supervisors who notice when construction is proceeding in an unsafe direction and call for a strategy change.</p>\n<p>Each layer serves a specific purpose: fail early with clear messages when possible, gracefully handle recoverable issues during processing, and provide meaningful diagnostics when fundamental assumptions break down. This layered approach ensures that learners receive actionable feedback rather than mysterious crashes or silent incorrect results.</p>\n<h3 id=\"numerical-stability-issues\">Numerical Stability Issues</h3>\n<p>Numerical stability represents one of the most subtle but critical aspects of machine learning implementation. Unlike logical bugs that produce obvious errors, numerical instability often manifests as gradually degrading results, convergence to incorrect solutions, or sudden catastrophic failures after many successful iterations.</p>\n<p>The <strong>core challenge</strong> stems from the inherent limitations of floating-point arithmetic when combined with the mathematical operations required for linear regression. Small rounding errors can accumulate over many gradient descent iterations, leading to parameter drift even when the algorithm should have converged. Division operations in gradient calculations can produce infinite or undefined results when denominators approach zero. Matrix operations with poorly conditioned data can amplify numerical errors exponentially.</p>\n<p>Our approach implements <strong>proactive monitoring</strong> rather than reactive error handling. Instead of waiting for NaN values to propagate through the system, we establish numerical bounds and stability checks at each critical computation point. This allows us to detect emerging instability early and either correct the issue automatically or fail gracefully with diagnostic information.</p>\n<h4 id=\"division-by-zero-protection\">Division by Zero Protection</h4>\n<p>Division by zero scenarios occur primarily in three contexts within linear regression: computing the slope in simple linear regression when all x-values are identical, calculating gradients when the feature variance is zero, and computing R-squared when the target variable has no variance.</p>\n<p>The <strong>slope calculation</strong> in simple linear regression requires computing the covariance between features and targets divided by the variance of features. When all feature values are identical (zero variance), this division becomes undefined. Our protection strategy detects this condition by checking whether the feature variance falls below a numerical tolerance threshold, typically <code>TOLERANCE = 1e-10</code>. Rather than allowing the division to proceed and produce infinity or NaN, we immediately raise a descriptive error explaining that the feature lacks sufficient variation for linear regression.</p>\n<p><strong>Gradient calculations</strong> face similar challenges when feature columns have zero or near-zero variance after normalization. During gradient descent, we compute partial derivatives that involve dividing by feature statistics. Our stability checker validates that all feature variances exceed the minimum threshold before beginning optimization. If any features are detected as constant, we provide specific guidance about removing these features or adding regularization.</p>\n<p><strong>R-squared computation</strong> requires dividing the explained variance by the total variance of target values. When target values are constant (zero variance), this calculation becomes undefined. We handle this edge case by detecting constant targets during data validation and returning an R-squared value of NaN with an explanatory warning, since the concept of explained variance is meaningless when there is no variance to explain.</p>\n<table>\n<thead>\n<tr>\n<th>Protection Type</th>\n<th>Detection Method</th>\n<th>Threshold</th>\n<th>Recovery Action</th>\n<th>Error Message</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Feature Variance</td>\n<td><code>np.var(features) &lt; TOLERANCE</code></td>\n<td>1e-10</td>\n<td>Immediate failure</td>\n<td>&quot;Feature column {i} has zero variance. Remove constant features or add noise.&quot;</td>\n</tr>\n<tr>\n<td>Target Variance</td>\n<td><code>np.var(targets) &lt; TOLERANCE</code></td>\n<td>1e-10</td>\n<td>Set R² = NaN</td>\n<td>&quot;Target variable is constant. R-squared is undefined.&quot;</td>\n</tr>\n<tr>\n<td>Gradient Division</td>\n<td><code>denominator &lt; MIN_GRADIENT_NORM</code></td>\n<td>1e-12</td>\n<td>Skip update</td>\n<td>&quot;Gradient calculation unstable. Consider feature scaling.&quot;</td>\n</tr>\n<tr>\n<td>Matrix Condition</td>\n<td><code>np.linalg.cond(X) &gt; 1e12</code></td>\n<td>1e12</td>\n<td>Add regularization</td>\n<td>&quot;Design matrix is ill-conditioned. Enable regularization.&quot;</td>\n</tr>\n</tbody></table>\n<h4 id=\"overflow-and-underflow-detection\">Overflow and Underflow Detection</h4>\n<p><strong>Overflow conditions</strong> occur when parameter values or intermediate calculations exceed the representable range of floating-point numbers. In gradient descent, this typically happens when the learning rate is too large, causing parameter updates that grow exponentially until they reach infinity. We establish parameter bounds of <code>MAX_PARAMETER_VALUE = 1e8</code> as a reasonable upper limit for most regression problems. When any parameter exceeds this threshold, we detect potential overflow and reduce the learning rate automatically or terminate with guidance about hyperparameter adjustment.</p>\n<p><strong>Underflow problems</strong> manifest when gradients become so small that they fall below machine precision, effectively stalling the optimization process. We monitor gradient norms using <code>MIN_GRADIENT_NORM = 1e-12</code> as the lower threshold. When gradient norms consistently fall below this value but the cost function has not converged, we detect potential underflow and suggest increasing the learning rate or checking for numerical precision issues.</p>\n<p><strong>Intermediate calculation protection</strong> monitors the results of matrix operations, especially during multiple linear regression where matrix multiplication can amplify existing numerical errors. We validate that cost function values remain finite and that matrix operations produce well-defined results. When we detect NaN or infinite values in intermediate calculations, we immediately halt processing and provide diagnostic information about which operation failed.</p>\n<h4 id=\"precision-loss-prevention\">Precision Loss Prevention</h4>\n<p><strong>Accumulated rounding errors</strong> represent a subtle but persistent threat to gradient descent convergence. Small floating-point errors in each parameter update can accumulate over hundreds of iterations, eventually causing the optimization to drift away from the true minimum. Our prevention strategy includes monitoring the relative change in parameters between iterations and detecting when updates become smaller than machine precision.</p>\n<p><strong>Feature scaling impact</strong> on numerical stability cannot be overstated. When features have vastly different scales (e.g., age in years vs. income in dollars), the condition number of the design matrix degrades dramatically, leading to numerical instability in both closed-form and iterative solutions. We enforce feature normalization for multiple linear regression and provide warnings when feature scales differ by more than several orders of magnitude.</p>\n<p><strong>Matrix conditioning</strong> becomes critical in multiple linear regression, where we must solve systems involving the design matrix. We compute the condition number using <code>np.linalg.cond()</code> and warn when it exceeds reasonable thresholds. Poorly conditioned matrices indicate that small changes in input data can produce large changes in the solution, suggesting the need for regularization or feature reduction.</p>\n<blockquote>\n<p><strong>Architecture Decision: Fail-Fast vs. Graceful Degradation</strong></p>\n<ul>\n<li><strong>Context</strong>: When numerical instability is detected, we must decide whether to immediately terminate with an error or attempt to continue with degraded precision</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Always fail immediately when any numerical issue is detected</li>\n<li>Attempt automatic recovery (reduce learning rate, add regularization) </li>\n<li>Continue with warnings and hope for eventual recovery</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Hybrid approach - fail fast for fundamental issues (zero variance), attempt recovery for optimization issues (learning rate adjustment)</li>\n<li><strong>Rationale</strong>: Educational value comes from understanding when problems are fundamental vs. solvable, while preventing silent failures that teach incorrect lessons</li>\n<li><strong>Consequences</strong>: More complex error handling logic, but learners develop intuition about which numerical issues are recoverable vs. fundamental design problems</li>\n</ul>\n</blockquote>\n<h3 id=\"input-validation-and-sanitization\">Input Validation and Sanitization</h3>\n<p>Input validation serves as the first line of defense against data-related errors, transforming potentially cryptic runtime failures into immediate, actionable feedback. Rather than allowing invalid data to propagate through the system and cause mysterious failures during matrix operations or optimization, we implement comprehensive validation that catches problems at the point of data ingestion.</p>\n<h4 id=\"array-shape-and-type-validation\">Array Shape and Type Validation</h4>\n<p><strong>Array dimensionality checking</strong> ensures that feature and target arrays have compatible shapes before any mathematical operations begin. The fundamental requirement for regression is that the number of samples in features matches the number of samples in targets. We validate this using <code>features.shape[0] == targets.shape[0]</code> and provide specific error messages when mismatches occur, including the actual dimensions detected to help learners diagnose the source of the problem.</p>\n<p><strong>Feature matrix structure</strong> validation becomes particularly important in multiple linear regression, where we expect a 2D array with samples as rows and features as columns. We check that feature arrays have exactly two dimensions and that the number of features is reasonable relative to the number of samples. A common guideline is maintaining at least 10 samples per feature to avoid overfitting, which we enforce through <code>MIN_RATIO = 10</code> samples per feature minimum.</p>\n<p><strong>Data type consistency</strong> checking ensures that all input arrays contain numeric data compatible with floating-point operations. We validate that arrays contain numeric dtypes (int, float) rather than strings or objects, and we automatically promote integer arrays to float64 to prevent precision loss during calculations. This prevents runtime errors during mathematical operations and ensures consistent precision across all computations.</p>\n<table>\n<thead>\n<tr>\n<th>Validation Type</th>\n<th>Check Performed</th>\n<th>Error Condition</th>\n<th>Recovery Action</th>\n<th>Error Message</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Shape Compatibility</td>\n<td><code>features.shape[0] == targets.shape[0]</code></td>\n<td>Mismatch</td>\n<td>Immediate failure</td>\n<td>&quot;Feature samples ({f}) != target samples ({t})&quot;</td>\n</tr>\n<tr>\n<td>Feature Dimensions</td>\n<td><code>len(features.shape) == 2</code></td>\n<td>Wrong dimensions</td>\n<td>Reshape or fail</td>\n<td>&quot;Features must be 2D array, got {d}D&quot;</td>\n</tr>\n<tr>\n<td>Sample Sufficiency</td>\n<td><code>n_samples &gt;= MIN_RATIO * n_features</code></td>\n<td>Too few samples</td>\n<td>Warning + continue</td>\n<td>&quot;Only {r:.1f} samples per feature. Consider reducing features.&quot;</td>\n</tr>\n<tr>\n<td>Data Types</td>\n<td><code>np.issubdtype(arr.dtype, np.number)</code></td>\n<td>Non-numeric</td>\n<td>Attempt conversion</td>\n<td>&quot;Array contains non-numeric data: {dtype}&quot;</td>\n</tr>\n</tbody></table>\n<h4 id=\"missing-value-and-outlier-detection\">Missing Value and Outlier Detection</h4>\n<p><strong>Missing value identification</strong> scans input arrays for NaN, None, or other indicators of missing data. Linear regression algorithms cannot handle missing values directly, so we must detect and address them before training begins. We use <code>np.isnan()</code> and <code>np.isinf()</code> to identify problematic values and provide specific counts and locations of missing data to help learners understand the scope of the data quality issue.</p>\n<p><strong>Outlier detection</strong> helps identify data points that may disproportionately influence the regression line, especially important for educational purposes where learners need to understand how linear regression responds to extreme values. We implement simple outlier detection using the interquartile range (IQR) method, flagging data points that fall more than 1.5 * IQR beyond the quartiles. While we don&#39;t automatically remove outliers, we provide warnings and statistics about their potential impact.</p>\n<p><strong>Range validation</strong> checks whether feature and target values fall within reasonable ranges for the problem domain. Extremely large or small values can cause numerical instability even if they&#39;re technically valid. We validate that all values fall within reasonable bounds (e.g., between -1e6 and 1e6) and warn when values appear to be on scales that might cause numerical issues.</p>\n<h4 id=\"data-quality-metrics\">Data Quality Metrics</h4>\n<p><strong>Statistical validation</strong> computes basic descriptive statistics for features and targets to identify potential data quality issues before they cause training problems. We calculate means, standard deviations, minimum and maximum values, and check for degenerate cases where these statistics indicate problematic data distributions.</p>\n<p><strong>Correlation analysis</strong> for multiple regression identifies perfect or near-perfect correlations between features, which can cause numerical instability in matrix operations. We compute the correlation matrix and warn when any feature pairs have correlations above 0.95, suggesting multicollinearity issues that may require feature selection or regularization.</p>\n<p><strong>Distribution checks</strong> identify severely skewed or abnormal distributions that may benefit from transformation before regression analysis. While linear regression doesn&#39;t assume normality of features, severely skewed data can impact numerical stability and interpretation of results. We provide warnings when feature distributions are extremely skewed (skewness &gt; 3) or have unusual characteristics.</p>\n<blockquote>\n<p><strong>Architecture Decision: Validation Strictness Level</strong></p>\n<ul>\n<li><strong>Context</strong>: Determining how strict input validation should be - whether to reject data with minor issues or proceed with warnings</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Strict validation that rejects any data quality issues</li>\n<li>Permissive validation that proceeds with warnings</li>\n<li>Configurable validation levels based on user preference</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Moderate strictness with clear warnings - fail for fundamental incompatibilities, warn for quality issues that may affect results</li>\n<li><strong>Rationale</strong>: Educational goal is to teach learners to recognize data quality issues while still allowing experimentation with imperfect data</li>\n<li><strong>Consequences</strong>: More complex validation logic with multiple severity levels, but learners develop better intuition about real-world data challenges</li>\n</ul>\n</blockquote>\n<h3 id=\"optimization-failure-recovery\">Optimization Failure Recovery</h3>\n<p>Optimization failure recovery addresses the fundamental challenge that gradient descent, unlike closed-form solutions, can fail to converge or converge to incorrect solutions. These failures often occur gradually over many iterations, making them difficult to detect without systematic monitoring. Our recovery strategy focuses on early detection of optimization problems and providing specific guidance for resolution.</p>\n<h4 id=\"divergence-detection\">Divergence Detection</h4>\n<p><strong>Parameter explosion</strong> represents the most dramatic form of optimization failure, where parameters grow without bound due to excessive learning rates or numerical instability. We monitor parameter values at each iteration and trigger divergence detection when any parameter exceeds <code>MAX_PARAMETER_VALUE = 1e8</code>. When divergence is detected, we immediately halt optimization and provide specific guidance about reducing the learning rate or checking for data scaling issues.</p>\n<p><strong>Cost function monitoring</strong> tracks the mean squared error at each iteration to detect both divergence (increasing cost) and stagnation (unchanging cost despite continued updates). We maintain a history of recent cost values and compute the trend over the last several iterations. Divergence is indicated by consistently increasing cost over multiple consecutive iterations, while stagnation is indicated by cost changes below the convergence tolerance without meeting other convergence criteria.</p>\n<p><strong>Gradient explosion detection</strong> identifies cases where gradient magnitudes become extremely large, indicating numerical instability in the optimization process. We compute the L2 norm of the gradient vector at each iteration and trigger alerts when it exceeds <code>MAX_GRADIENT_NORM = 1e8</code>. Gradient explosion often precedes parameter explosion and can be addressed by reducing the learning rate or improving feature scaling.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Detection Criteria</th>\n<th>Immediate Action</th>\n<th>Recovery Strategy</th>\n<th>Diagnostic Message</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Parameter Explosion</td>\n<td><code>max(abs(parameters)) &gt; MAX_PARAMETER_VALUE</code></td>\n<td>Halt optimization</td>\n<td>Reduce learning rate by 10x</td>\n<td>&quot;Parameters diverging. Try learning_rate = {new_rate}&quot;</td>\n</tr>\n<tr>\n<td>Cost Divergence</td>\n<td><code>cost[t] &gt; 1.1 * cost[t-5]</code> for 3+ iterations</td>\n<td>Halt optimization</td>\n<td>Reset and reduce learning rate</td>\n<td>&quot;Cost increasing. Check data scaling and learning rate.&quot;</td>\n</tr>\n<tr>\n<td>Gradient Explosion</td>\n<td><code>np.linalg.norm(gradient) &gt; MAX_GRADIENT_NORM</code></td>\n<td>Halt optimization</td>\n<td>Feature scaling + rate reduction</td>\n<td>&quot;Gradient explosion detected. Normalize features and reduce learning rate.&quot;</td>\n</tr>\n<tr>\n<td>Stagnation</td>\n<td><code>abs(cost[t] - cost[t-10]) &lt; TOLERANCE</code> without convergence</td>\n<td>Continue with warning</td>\n<td>Increase learning rate or check tolerance</td>\n<td>&quot;Cost plateaued without convergence. Consider different learning rate.&quot;</td>\n</tr>\n</tbody></table>\n<h4 id=\"learning-rate-adaptation\">Learning Rate Adaptation</h4>\n<p><strong>Automatic learning rate adjustment</strong> provides a safety mechanism when the initial learning rate proves inappropriate for the optimization landscape. Rather than requiring manual hyperparameter tuning, we implement simple adaptive strategies that can rescue failing optimizations. When divergence is detected, we automatically reduce the learning rate by a factor of 10 and restart optimization from the initial parameters.</p>\n<p><strong>Learning rate bounds</strong> establish reasonable limits for automatic adjustment to prevent the system from choosing learning rates that are either too large (causing continued divergence) or too small (causing extremely slow convergence). We maintain bounds between <code>1e-6</code> (minimum practical learning rate) and <code>1.0</code> (maximum stable learning rate for most problems).</p>\n<p><strong>Convergence rate monitoring</strong> tracks the speed at which the optimization approaches convergence to detect learning rates that are too conservative. When the cost function decreases very slowly over many iterations (less than 1% improvement over 100 iterations), we suggest increasing the learning rate to accelerate convergence while maintaining stability.</p>\n<h4 id=\"convergence-criteria-refinement\">Convergence Criteria Refinement</h4>\n<p><strong>Multi-criteria convergence detection</strong> improves upon simple cost-based stopping conditions by incorporating multiple indicators of optimization completion. We monitor cost improvement, parameter stability, and gradient magnitude simultaneously, requiring satisfaction of convergence criteria across multiple metrics before declaring success. This prevents premature stopping when one metric appears stable while others indicate continued optimization potential.</p>\n<p><strong>Adaptive tolerance adjustment</strong> recognizes that appropriate convergence tolerances depend on the scale and characteristics of the specific dataset. Rather than using fixed tolerances, we compute relative tolerances based on the initial cost function value and parameter magnitudes. This ensures that convergence detection works appropriately across datasets with different scales and characteristics.</p>\n<p><strong>Convergence confidence assessment</strong> provides probabilistic estimates of convergence quality rather than binary success/failure indicators. We track the consistency of convergence signals over multiple recent iterations and provide confidence scores that help learners understand whether optimization terminated due to strong convergence evidence or marginal satisfaction of stopping criteria.</p>\n<p>The <code>ConvergenceDetector</code> maintains comprehensive state about the optimization process:</p>\n<table>\n<thead>\n<tr>\n<th>State Field</th>\n<th>Type</th>\n<th>Purpose</th>\n<th>Update Frequency</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>cost_history</code></td>\n<td>List[float]</td>\n<td>Tracks cost function values for trend analysis</td>\n<td>Every iteration</td>\n</tr>\n<tr>\n<td><code>parameter_history</code></td>\n<td>List[np.ndarray]</td>\n<td>Stores recent parameter vectors for stability checking</td>\n<td>Every iteration</td>\n</tr>\n<tr>\n<td><code>gradient_history</code></td>\n<td>List[np.ndarray]</td>\n<td>Maintains gradient magnitudes for explosion detection</td>\n<td>Every iteration</td>\n</tr>\n<tr>\n<td><code>consecutive_count</code></td>\n<td>int</td>\n<td>Counts consecutive iterations meeting convergence criteria</td>\n<td>Every iteration</td>\n</tr>\n<tr>\n<td><code>converged</code></td>\n<td>bool</td>\n<td>Indicates whether convergence has been achieved</td>\n<td>When criteria met</td>\n</tr>\n<tr>\n<td><code>convergence_reason</code></td>\n<td>str</td>\n<td>Explains which criteria triggered convergence</td>\n<td>When convergence detected</td>\n</tr>\n</tbody></table>\n<h4 id=\"recovery-strategy-implementation\">Recovery Strategy Implementation</h4>\n<p><strong>Automatic restart mechanisms</strong> allow the optimization to recover from temporary numerical issues without requiring manual intervention. When certain types of failures are detected (particularly learning rate issues), we automatically reset parameters to their initial values and restart optimization with adjusted hyperparameters. This provides a seamless experience for learners while demonstrating the iterative nature of hyperparameter tuning.</p>\n<p><strong>Progressive fallback strategies</strong> implement a hierarchy of recovery attempts when initial optimization fails. We first attempt learning rate reduction, then feature scaling adjustments, and finally recommend regularization if numerical instability persists. Each fallback level provides more aggressive intervention while maintaining the educational goal of understanding why different approaches are necessary.</p>\n<p><strong>Diagnostic information collection</strong> ensures that when optimization ultimately fails despite recovery attempts, learners receive comprehensive information about what was attempted and why it failed. We maintain detailed logs of all recovery actions taken, parameter adjustments made, and the specific failure modes encountered, enabling learners to understand both the automatic recovery process and the underlying mathematical challenges.</p>\n<blockquote>\n<p><strong>Architecture Decision: Automatic vs. Manual Recovery</strong></p>\n<ul>\n<li><strong>Context</strong>: When optimization failures are detected, we must decide whether to attempt automatic recovery or require manual intervention</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Fully automatic recovery with minimal user visibility</li>\n<li>Manual recovery requiring explicit user action for each failure</li>\n<li>Automatic recovery with detailed logging and user override options</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Automatic recovery for common issues (learning rate adjustment) with detailed logging, manual intervention required for fundamental problems (data quality)</li>\n<li><strong>Rationale</strong>: Educational value comes from understanding what recovery strategies work and why, while avoiding tedious manual tuning for well-understood issues</li>\n<li><strong>Consequences</strong>: More complex error handling logic, but learners see both automatic recovery in action and receive guidance for manual intervention when necessary</li>\n</ul>\n</blockquote>\n<p>⚠️ <strong>Pitfall: Silent Numerical Failures</strong>\nMany numerical issues in machine learning manifest as gradually degrading performance rather than obvious crashes. For example, when the learning rate is slightly too high, the optimization may appear to converge normally but settle on a suboptimal solution. Our monitoring systems are designed to catch these subtle failures by tracking multiple convergence indicators simultaneously. Always check that your final cost is reasonable relative to the data scale, not just that the optimization &quot;finished.&quot;</p>\n<p>⚠️ <strong>Pitfall: Ignoring Data Scale Effects</strong>\nA common mistake is implementing numerical stability checks with fixed thresholds that don&#39;t account for the natural scale of the data. For instance, checking for parameter values above 1e8 may be appropriate for normalized data but too restrictive for raw financial data where values naturally reach millions. Our validation systems compute relative thresholds based on data characteristics rather than using absolute fixed values.</p>\n<p>⚠️ <strong>Pitfall: Over-Aggressive Error Handling</strong>\nWhile comprehensive error checking is important, overly strict validation can prevent learners from experimenting with edge cases that might actually work. For example, automatically rejecting datasets with fewer than 10 samples per feature prevents exploration of regularization techniques that can handle such scenarios. Our approach balances protection against clear numerical issues with allowance for educational experimentation.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This subsection provides concrete implementation strategies for the error handling and validation systems described above, organized to support learners at each milestone of the project.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Validation Framework</td>\n<td>Manual checks with numpy functions</td>\n<td>Custom validation classes with inheritance</td>\n</tr>\n<tr>\n<td>Error Messages</td>\n<td>String formatting with problem description</td>\n<td>Structured error objects with codes and context</td>\n</tr>\n<tr>\n<td>Numerical Monitoring</td>\n<td>Basic threshold checking in training loops</td>\n<td>Comprehensive monitoring with state machines</td>\n</tr>\n<tr>\n<td>Recovery Strategies</td>\n<td>Fixed learning rate reduction</td>\n<td>Adaptive hyperparameter adjustment algorithms</td>\n</tr>\n<tr>\n<td>Logging</td>\n<td>Print statements with error details</td>\n<td>Structured logging with multiple severity levels</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<p>The error handling system integrates throughout the project structure rather than being isolated in a single module:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  src/\n    validation/\n      __init__.py              ← validation module exports\n      data_validator.py        ← DataValidator class implementation\n      numerical_validator.py   ← NumericalStabilityChecker class\n      convergence_detector.py  ← ConvergenceDetector class implementation\n    errors/\n      __init__.py              ← custom exception definitions\n      regression_errors.py     ← RegressionError hierarchy\n    models/\n      simple_regression.py     ← integrates validation in fit() method\n      gradient_descent.py      ← integrates numerical monitoring\n      multiple_regression.py   ← comprehensive validation integration\n    utils/\n      monitoring.py            ← TrainingMonitor helper class\n      recovery.py              ← OptimizationRecovery strategies\n  tests/\n    test_validation.py         ← validation system tests\n    test_error_handling.py     ← error scenario tests\n    test_edge_cases.py         ← boundary condition tests</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Custom Exception Hierarchy:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Custom exception classes for linear regression implementation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Provides structured error handling with specific error types and recovery guidance.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RegressionError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base exception for all linear regression errors.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message, error_code</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">, suggested_fix</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">, context</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(message)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.error_code </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> error_code</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.suggested_fix </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> suggested_fix</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.context </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> context </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __str__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        base_msg </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__str__</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.suggested_fix:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">base_msg</span><span style=\"color:#79B8FF\">}\\n</span><span style=\"color:#9ECBFF\">Suggested fix: </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.suggested_fix</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> base_msg</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DataValidationError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">RegressionError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Raised when input data fails validation checks.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> NumericalInstabilityError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">RegressionError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Raised when numerical computations become unstable.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ConvergenceError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">RegressionError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Raised when optimization fails to converge properly.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MatrixConditionError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">RegressionError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Raised when matrix operations encounter conditioning problems.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Usage example for learners:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># try:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#     model.fit(X, y)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># except DataValidationError as e:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#     print(f\"Data problem: {e}\")</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#     print(f\"Try this: {e.suggested_fix}\")</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#     print(f\"Context: {e.context}\")</span></span></code></pre></div>\n\n<p><strong>Numerical Constants and Thresholds:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Numerical constants for stability checking and validation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">These values are chosen based on typical floating-point precision limits</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">and practical experience with regression problems.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Fundamental numerical limits</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">TOLERANCE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-10</span><span style=\"color:#6A737D\">                    # General numerical comparison threshold</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MACHINE_EPSILON</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> np.finfo(</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">).eps  </span><span style=\"color:#6A737D\"># Machine precision for floating-point</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Parameter bounds for stability</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MAX_PARAMETER_VALUE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e8</span><span style=\"color:#6A737D\">           # Overflow detection threshold</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MIN_PARAMETER_CHANGE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-12</span><span style=\"color:#6A737D\">        # Underflow detection for parameter updates</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Gradient monitoring thresholds  </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MIN_GRADIENT_NORM</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-12</span><span style=\"color:#6A737D\">           # Gradient underflow threshold</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MAX_GRADIENT_NORM</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e8</span><span style=\"color:#6A737D\">             # Gradient explosion threshold</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Matrix conditioning limits</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MAX_CONDITION_NUMBER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e12</span><span style=\"color:#6A737D\">         # Matrix conditioning threshold</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MIN_VARIANCE_THRESHOLD</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-10</span><span style=\"color:#6A737D\">      # Zero variance detection</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Optimization parameters</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">DEFAULT_LEARNING_RATE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.01</span><span style=\"color:#6A737D\">        # Safe starting learning rate</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MAX_LEARNING_RATE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#6A737D\">             # Upper bound for automatic adjustment</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MIN_LEARNING_RATE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-6</span><span style=\"color:#6A737D\">            # Lower bound for automatic adjustment</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MAX_ITERATIONS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#6A737D\">               # Default iteration limit</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Data quality thresholds</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MIN_SAMPLES_PER_FEATURE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#6A737D\">        # Minimum sample to feature ratio</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MAX_OUTLIER_ZSCORE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3.0</span><span style=\"color:#6A737D\">            # Outlier detection threshold</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MAX_CORRELATION_THRESHOLD</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.95</span><span style=\"color:#6A737D\">     # Multicollinearity detection</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>Comprehensive Data Validation Class:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DataValidator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Comprehensive validation for regression input data.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, tolerance</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">TOLERANCE</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tolerance </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tolerance</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.validation_results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_regression_inputs</span><span style=\"color:#E1E4E8\">(self, features, targets):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Perform comprehensive validation of features and targets for regression.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            features: Feature array (n_samples, n_features) or (n_samples,) for simple regression</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            targets: Target array (n_samples,)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            DataValidationError: When validation fails with specific guidance</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate array types and convert to numpy arrays if needed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check array dimensions and reshape if necessary for compatibility</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate sample count compatibility between features and targets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check for missing values (NaN, inf) and provide locations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate data types are numeric and convertible to float64</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Check for sufficient samples relative to features (avoid overfitting)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Detect zero-variance features that will cause division by zero</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Check for extreme outliers that may cause numerical instability</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Store validation statistics for later use in training</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Return validation summary with warnings and recommendations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> check_matrix_conditioning</span><span style=\"color:#E1E4E8\">(self, X):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Check design matrix conditioning for numerical stability.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            X: Design matrix with intercept column</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Tuple[bool, str]: (is_stable, diagnostic_message)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Compute condition number using np.linalg.cond()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Compare against MAX_CONDITION_NUMBER threshold</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check for perfect correlations between features</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Identify specific problematic feature combinations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Generate specific recommendations (regularization, feature removal)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Numerical Stability Monitoring:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> NumericalStabilityChecker</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Real-time monitoring of numerical stability during optimization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.parameter_history </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.gradient_history </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cost_history </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.stability_warnings </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> check_numerical_stability</span><span style=\"color:#E1E4E8\">(self, parameters, gradients, cost, iteration):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Comprehensive stability check for current optimization state.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            parameters: Current parameter values (slope, intercept) or weights vector</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            gradients: Current gradient values  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            cost: Current cost function value</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            iteration: Current iteration number</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Tuple[bool, str, str]: (is_stable, warning_message, suggested_action)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check for parameter overflow (values exceeding MAX_PARAMETER_VALUE)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Detect gradient explosion (norm exceeding MAX_GRADIENT_NORM)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check for gradient underflow (norm below MIN_GRADIENT_NORM)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Validate cost function value is finite and reasonable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Detect cost function divergence (increasing trend over recent iterations)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Check for parameter stagnation (no meaningful updates)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Analyze parameter update magnitudes relative to current values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Update internal history for trend analysis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Generate specific diagnostic messages for detected issues</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Recommend specific recovery actions (learning rate, scaling, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> suggest_learning_rate_adjustment</span><span style=\"color:#E1E4E8\">(self, current_lr, problem_type):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Suggest learning rate adjustments based on detected stability issues.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            current_lr: Current learning rate</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            problem_type: Type of stability issue detected</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Tuple[float, str]: (suggested_lr, explanation)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Analyze problem type (divergence, stagnation, explosion)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Compute appropriate adjustment factor based on problem severity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Ensure suggested rate stays within MIN_LEARNING_RATE to MAX_LEARNING_RATE bounds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Generate explanation of why this adjustment should help</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Include guidance about monitoring the adjustment effectiveness</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Advanced Convergence Detection:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ConvergenceDetector</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Multi-criteria convergence detection with adaptive thresholds.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> ConvergenceConfig()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cost_history </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.parameter_history </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.gradient_history </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.consecutive_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.converged </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.convergence_reason </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> check_convergence</span><span style=\"color:#E1E4E8\">(self, cost, parameters, gradients, iteration):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Multi-criteria convergence detection with detailed analysis.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            cost: Current cost function value</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            parameters: Current parameter vector</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            gradients: Current gradient vector  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            iteration: Current iteration number</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Tuple[bool, str]: (has_converged, detailed_reason)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Update internal histories with current values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check cost improvement criterion (absolute and relative)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check parameter stability criterion (small parameter changes)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check gradient magnitude criterion (approaching zero)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Require multiple consecutive iterations meeting criteria</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Detect false convergence (premature stopping)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Analyze convergence quality and confidence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Generate detailed explanation of convergence decision</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Update convergence state and reason</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Return convergence status with full diagnostic information</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> analyze_convergence_quality</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Assess the quality and confidence of detected convergence.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Dict: Convergence quality metrics and confidence assessment</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Compute convergence confidence score based on multiple criteria</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Analyze final gradient norm relative to problem scale</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check cost function stability over recent iterations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Assess parameter stability and reasonable final values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Generate convergence quality report with recommendations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Milestone 1 Checkpoint - Basic Validation:</strong>\nAfter implementing basic data validation for simple linear regression:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test basic validation with synthetic data</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from src.validation.data_validator import DataValidator</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from src.models.simple_regression import SimpleLinearRegression</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">import numpy as np</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Test normal case</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">X = np.array([1, 2, 3, 4, 5])</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">y = np.array([2, 4, 6, 8, 10])</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">model = SimpleLinearRegression()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">model.fit(X, y)  # Should work without errors</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print('✓ Normal case passes')</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Test zero variance detection  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">X_const = np.array([3, 3, 3, 3, 3])</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">try:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    model.fit(X_const, y)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    print('✗ Should have detected zero variance')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">except Exception as e:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    print(f'✓ Zero variance detected: {e}')</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Test dimension mismatch</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">X_wrong = np.array([1, 2, 3])</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">try:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    model.fit(X_wrong, y)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    print('✗ Should have detected dimension mismatch')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">except Exception as e:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    print(f'✓ Dimension mismatch detected: {e}')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n\n<p>Expected output shows successful validation of normal data and appropriate error detection for problematic cases.</p>\n<p><strong>Milestone 2 Checkpoint - Gradient Descent Stability:</strong>\nAfter implementing numerical stability monitoring for gradient descent:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test stability monitoring with different learning rates</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from src.models.gradient_descent import GradientDescentRegression</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">import numpy as np</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Generate test data</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">np.random.seed(42)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">X = np.random.randn(100)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">y = 2 * X + 1 + 0.1 * np.random.randn(100)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Test stable learning rate</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">model_stable = GradientDescentRegression(learning_rate=0.01)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">model_stable.fit(X, y)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print(f'✓ Stable training converged: {model_stable.converged_}')</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Test unstable learning rate (should detect divergence)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">model_unstable = GradientDescentRegression(learning_rate=10.0)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">try:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    model_unstable.fit(X, y)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    if not model_unstable.converged_:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        print('✓ Divergence detected correctly')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    else:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        print('✗ Should have detected divergence')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">except Exception as e:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    print(f'✓ Divergence caught: {type(e).__name__}')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n\n<p>This checkpoint verifies that stability monitoring correctly identifies problematic learning rates and provides appropriate error handling.</p>\n<p><strong>Milestone 3 Checkpoint - Multiple Regression Validation:</strong>\nAfter implementing comprehensive validation for multiple linear regression:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test matrix conditioning and feature scaling validation</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from src.models.multiple_regression import MultipleLinearRegression</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">import numpy as np</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Generate well-conditioned data</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">np.random.seed(42)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">X = np.random.randn(100, 3)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">y = X @ [2, -1, 0.5] + 0.1 * np.random.randn(100)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">model = MultipleLinearRegression()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">model.fit(X, y)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print(f'✓ Well-conditioned case: R² = {model.score(X, y):.3f}')</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Test poorly conditioned data (perfect correlation)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">X_bad = np.random.randn(100, 3)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">X_bad[:, 2] = X_bad[:, 1]  # Perfect correlation</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">y_bad = X_bad[:, :2] @ [2, -1] + 0.1 * np.random.randn(100)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">try:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    model_bad = MultipleLinearRegression()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    model_bad.fit(X_bad, y_bad)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    print('⚠ Should warn about conditioning but may still work')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">except Exception as e:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    print(f'✓ Conditioning problem detected: {type(e).__name__}')</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Test insufficient samples</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">X_few = np.random.randn(5, 10)  # 5 samples, 10 features</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">y_few = np.random.randn(5)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">try:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    model_few = MultipleLinearRegression()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    model_few.fit(X_few, y_few)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    print('⚠ Should warn about overfitting risk')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">except Exception as e:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    print(f'✓ Insufficient data detected: {type(e).__name__}')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n\n<p>This checkpoint tests the full validation system including matrix conditioning, multicollinearity detection, and sample sufficiency checking.</p>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>np.linalg.LinAlgError: SVD did not converge</code></td>\n<td>Poorly conditioned design matrix</td>\n<td>Check <code>np.linalg.cond(X)</code> and feature correlations</td>\n<td>Add regularization or remove correlated features</td>\n</tr>\n<tr>\n<td>Parameters become NaN after few iterations</td>\n<td>Learning rate too high or numerical overflow</td>\n<td>Print parameter values each iteration, check gradients</td>\n<td>Reduce learning rate by 10x, ensure feature scaling</td>\n</tr>\n<tr>\n<td>Cost function oscillates wildly</td>\n<td>Learning rate too high for optimization landscape</td>\n<td>Plot cost history, check gradient magnitudes</td>\n<td>Reduce learning rate, verify feature normalization</td>\n</tr>\n<tr>\n<td>Optimization never converges (hits max iterations)</td>\n<td>Learning rate too small or poor convergence criteria</td>\n<td>Check cost improvement rate, examine gradient norms</td>\n<td>Increase learning rate or relax convergence tolerance</td>\n</tr>\n<tr>\n<td>&quot;Singular matrix&quot; error in closed-form solution</td>\n<td>Perfect correlation between features or zero variance</td>\n<td>Check feature correlation matrix and variances</td>\n<td>Remove constant features, check for identical columns</td>\n</tr>\n<tr>\n<td>Silent incorrect results (converges to wrong answer)</td>\n<td>Numerical precision loss or inappropriate scaling</td>\n<td>Compare with closed-form solution, check condition number</td>\n<td>Implement feature scaling, use higher precision</td>\n</tr>\n<tr>\n<td>Memory usage grows unexpectedly during training</td>\n<td>Storing too much history in convergence detection</td>\n<td>Monitor history list sizes, check memory profiling</td>\n<td>Limit history length, use fixed-size circular buffers</td>\n</tr>\n<tr>\n<td>Training extremely slow despite simple data</td>\n<td>Inefficient matrix operations or excessive validation</td>\n<td>Profile code execution, time individual operations</td>\n<td>Vectorize operations, reduce validation frequency</td>\n</tr>\n</tbody></table>\n<h2 id=\"testing-strategy-and-milestone-checkpoints\">Testing Strategy and Milestone Checkpoints</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (M1: Simple Linear Regression - validates closed-form solutions and basic prediction, M2: Gradient Descent - tests iterative optimization and convergence, M3: Multiple Linear Regression - verifies matrix operations and regularization)</p>\n</blockquote>\n<p>This section defines a comprehensive testing strategy that serves both educational and verification purposes. Unlike production testing that focuses primarily on correctness and performance, our testing approach emphasizes learning validation—ensuring that each implementation milestone demonstrates understanding of the underlying mathematical concepts while building confidence through measurable success criteria.</p>\n<h3 id=\"mental-model-the-mathematical-proof-checker\">Mental Model: The Mathematical Proof Checker</h3>\n<p>Think of our testing strategy as a mathematical proof checker that validates not just the final answer, but every step of reasoning along the way. Just as a mathematics professor doesn&#39;t simply look at whether a student got the right answer to a calculus problem, but examines the work to ensure they understand derivatives, limits, and algebraic manipulation, our testing framework validates both computational correctness and conceptual understanding.</p>\n<p>Each test serves as a checkpoint that answers the question: &quot;Does this implementation demonstrate mastery of the underlying mathematical concept?&quot; For the closed-form solution in Milestone 1, we verify that the learner understands the normal equation. For gradient descent in Milestone 2, we confirm they grasp optimization principles and convergence detection. For multiple regression in Milestone 3, we ensure they comprehend matrix operations and regularization effects.</p>\n<p>This approach builds confidence progressively—each passing test provides concrete evidence that the learner has mastered a specific concept and is ready to tackle the next level of complexity.</p>\n<h2 id=\"unit-testing-approach\">Unit Testing Approach</h2>\n<p>The unit testing strategy treats each component as an isolated mathematical function with precisely defined input-output relationships. We test individual components using carefully constructed synthetic datasets where we know the exact expected outcomes, allowing us to validate both numerical accuracy and algorithmic correctness.</p>\n<h3 id=\"synthetic-data-generation-strategy\">Synthetic Data Generation Strategy</h3>\n<p>Our synthetic data generation forms the foundation of predictable, repeatable unit tests. Rather than using random data that might mask subtle bugs through statistical averaging, we construct datasets with known mathematical properties that expose specific edge cases and validate precise algorithmic behavior.</p>\n<table>\n<thead>\n<tr>\n<th>Test Data Type</th>\n<th>Purpose</th>\n<th>Construction Method</th>\n<th>Expected Properties</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Perfect Linear Data</td>\n<td>Validate noise-free fitting</td>\n<td><code>y = 2.5 * x + 1.7</code> exactly</td>\n<td>R² = 1.0, residuals = 0</td>\n</tr>\n<tr>\n<td>Controlled Noise Data</td>\n<td>Test robustness to realistic conditions</td>\n<td>Add Gaussian noise σ=0.1</td>\n<td>R² &gt; 0.95, predictable MSE</td>\n</tr>\n<tr>\n<td>Edge Case Data</td>\n<td>Expose numerical stability issues</td>\n<td>Single point, constant x, large values</td>\n<td>Graceful error handling</td>\n</tr>\n<tr>\n<td>Multi-collinear Data</td>\n<td>Test multiple regression challenges</td>\n<td>Features with exact linear dependence</td>\n<td>Proper error detection</td>\n</tr>\n<tr>\n<td>Standardization Test Data</td>\n<td>Validate feature scaling</td>\n<td>Mean=100, std=50 known values</td>\n<td>Normalized mean=0, std=1</td>\n</tr>\n</tbody></table>\n<p>The synthetic data generator provides complete control over every aspect of the dataset, enabling us to test specific mathematical properties in isolation:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> generate_perfect_linear_data</span><span style=\"color:#E1E4E8\">(n_samples</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">, slope</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2.5</span><span style=\"color:#E1E4E8\">, intercept</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1.7</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Creates data that perfectly follows y = slope * x + intercept\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    x </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.linspace(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">, n_samples)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> slope </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> x </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> intercept  </span><span style=\"color:#6A737D\"># No noise - perfect relationship</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> x.reshape(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">), y</span></span></code></pre></div>\n\n<h3 id=\"component-level-test-structure\">Component-Level Test Structure</h3>\n<p>Each component receives a dedicated test suite that validates both its mathematical correctness and its adherence to the defined interface contracts. The test structure mirrors the component architecture, ensuring complete coverage of all public methods and critical internal calculations.</p>\n<p><strong>SimpleLinearRegression Unit Tests:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Test Method</th>\n<th>Validates</th>\n<th>Input Data</th>\n<th>Expected Output</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>test_perfect_fit_closed_form</code></td>\n<td>Normal equation accuracy</td>\n<td>Perfect linear data</td>\n<td>slope_=2.5, intercept_=1.7, R²=1.0</td>\n</tr>\n<tr>\n<td><code>test_prediction_accuracy</code></td>\n<td>Parameter application</td>\n<td>New x values</td>\n<td>Exact y = mx + b calculation</td>\n</tr>\n<tr>\n<td><code>test_score_calculation</code></td>\n<td>R-squared computation</td>\n<td>Known residuals</td>\n<td>Precise coefficient of determination</td>\n</tr>\n<tr>\n<td><code>test_empty_data_handling</code></td>\n<td>Input validation</td>\n<td>Empty arrays</td>\n<td><code>DataValidationError</code> with clear message</td>\n</tr>\n<tr>\n<td><code>test_single_point_edge_case</code></td>\n<td>Mathematical edge case</td>\n<td>One data point</td>\n<td>Undefined slope error</td>\n</tr>\n<tr>\n<td><code>test_constant_x_values</code></td>\n<td>Division by zero protection</td>\n<td>All x identical</td>\n<td><code>NumericalInstabilityError</code></td>\n</tr>\n</tbody></table>\n<p><strong>GradientDescentRegression Unit Tests:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Test Method</th>\n<th>Validates</th>\n<th>Input Data</th>\n<th>Expected Outcome</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>test_convergence_perfect_data</code></td>\n<td>Optimization accuracy</td>\n<td>Perfect linear relationship</td>\n<td>Converged parameters ≈ true values</td>\n</tr>\n<tr>\n<td><code>test_cost_function_calculation</code></td>\n<td>MSE computation</td>\n<td>Known residuals</td>\n<td>Exact mean squared error</td>\n</tr>\n<tr>\n<td><code>test_gradient_computation</code></td>\n<td>Partial derivatives</td>\n<td>Analytical comparison</td>\n<td>Gradients match calculus</td>\n</tr>\n<tr>\n<td><code>test_learning_rate_sensitivity</code></td>\n<td>Parameter update behavior</td>\n<td>Various learning rates</td>\n<td>Stable vs. divergent trajectories</td>\n</tr>\n<tr>\n<td><code>test_convergence_detection</code></td>\n<td>Stopping criteria</td>\n<td>Controlled cost sequence</td>\n<td>Proper termination timing</td>\n</tr>\n<tr>\n<td><code>test_cost_history_tracking</code></td>\n<td>Training monitoring</td>\n<td>Multi-iteration fit</td>\n<td>Complete cost trajectory</td>\n</tr>\n</tbody></table>\n<p><strong>MultipleLinearRegression Unit Tests:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Test Method</th>\n<th>Validates</th>\n<th>Input Data</th>\n<th>Expected Result</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>test_design_matrix_construction</code></td>\n<td>Matrix formulation</td>\n<td>Multiple features</td>\n<td>Proper intercept column addition</td>\n</tr>\n<tr>\n<td><code>test_vectorized_gradient_computation</code></td>\n<td>Batch operations</td>\n<td>Multi-dimensional data</td>\n<td>Correct gradient vector</td>\n</tr>\n<tr>\n<td><code>test_regularization_effect</code></td>\n<td>Ridge penalty</td>\n<td>High regularization</td>\n<td>Reduced parameter magnitudes</td>\n</tr>\n<tr>\n<td><code>test_feature_scaling_integration</code></td>\n<td>Normalization compatibility</td>\n<td>Different feature scales</td>\n<td>Consistent convergence</td>\n</tr>\n<tr>\n<td><code>test_matrix_dimension_validation</code></td>\n<td>Shape compatibility</td>\n<td>Mismatched dimensions</td>\n<td>Clear error messages</td>\n</tr>\n</tbody></table>\n<h3 id=\"mathematical-verification-approach\">Mathematical Verification Approach</h3>\n<p>Beyond functional correctness, our unit tests verify mathematical properties that demonstrate conceptual understanding. Each test validates specific mathematical principles rather than just computational outputs.</p>\n<p><strong>Closed-Form Solution Verification:</strong>\nWe validate that the implemented normal equation produces results identical to manual matrix calculations. For the simple case, we verify:</p>\n<ul>\n<li>The slope calculation: <code>slope = Σ[(xi - x̄)(yi - ȳ)] / Σ[(xi - x̄)²]</code></li>\n<li>The intercept calculation: <code>intercept = ȳ - slope * x̄</code></li>\n<li>The R-squared computation: <code>R² = 1 - SSres/SStot</code></li>\n</ul>\n<p><strong>Gradient Descent Mathematical Consistency:</strong>\nWe test that gradient descent converges to the same solution as the closed-form approach on identical data, within numerical tolerance. This validates both the gradient calculation and the parameter update rule.</p>\n<p><strong>Regularization Effect Verification:</strong>\nFor Ridge regression, we confirm that increasing <code>regularization_strength</code> progressively shrinks parameter magnitudes while maintaining reasonable fit quality, demonstrating understanding of the bias-variance tradeoff.</p>\n<blockquote>\n<p><strong>Key Insight</strong>: Mathematical verification tests serve dual purposes—they catch implementation bugs while confirming that the learner understands the underlying mathematical relationships. A test that verifies gradient computation teaches the learner about partial derivatives as much as it validates code correctness.</p>\n</blockquote>\n<h3 id=\"error-handling-validation\">Error Handling Validation</h3>\n<p>Unit tests extensively validate error handling behavior, ensuring that the system fails gracefully with informative messages rather than producing silent incorrect results or cryptic crashes.</p>\n<table>\n<thead>\n<tr>\n<th>Error Condition</th>\n<th>Test Validation</th>\n<th>Expected Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Null/empty input arrays</td>\n<td>Pass None or empty <code>np.ndarray</code></td>\n<td><code>DataValidationError</code> with descriptive message</td>\n</tr>\n<tr>\n<td>Mismatched array lengths</td>\n<td>Different feature/target sizes</td>\n<td><code>DataValidationError</code> specifying length mismatch</td>\n</tr>\n<tr>\n<td>Non-numeric data</td>\n<td>String or boolean values</td>\n<td><code>DataValidationError</code> indicating data type issue</td>\n</tr>\n<tr>\n<td>Infinite/NaN values</td>\n<td>Inject <code>np.inf</code> or <code>np.nan</code></td>\n<td><code>NumericalInstabilityError</code> with location info</td>\n</tr>\n<tr>\n<td>Singular matrix conditions</td>\n<td>Perfectly correlated features</td>\n<td><code>NumericalInstabilityError</code> suggesting solutions</td>\n</tr>\n<tr>\n<td>Gradient descent divergence</td>\n<td>Excessive learning rate</td>\n<td><code>ConvergenceError</code> with learning rate adjustment advice</td>\n</tr>\n</tbody></table>\n<h3 id=\"common-unit-testing-pitfalls\">Common Unit Testing Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Testing with Random Data Only</strong>\nMany learners write tests that use <code>np.random.rand()</code> to generate test data, making tests non-deterministic and hiding edge cases. Random data can mask bugs through statistical averaging—a slightly incorrect gradient calculation might still produce reasonable results on most random datasets.</p>\n<p><em>Fix</em>: Use deterministic synthetic data with known mathematical properties. Set <code>np.random.seed()</code> when randomness is necessary, and test specific edge cases with carefully constructed data.</p>\n<p>⚠️ <strong>Pitfall: Insufficient Numerical Precision Testing</strong>\nTesting for exact equality (<code>assert result == 2.5</code>) fails with floating-point arithmetic. Conversely, using overly loose tolerances (<code>assert abs(result - 2.5) &lt; 0.1</code>) can miss significant numerical errors.</p>\n<p><em>Fix</em>: Use appropriate tolerances based on the mathematical operation. For closed-form solutions on perfect data, use tight tolerances (<code>1e-10</code>). For iterative algorithms, use tolerances related to convergence criteria (<code>1e-6</code>).</p>\n<p>⚠️ <strong>Pitfall: Not Testing the Learning Process</strong>\nFocusing only on final results misses important learning validation. Testing that gradient descent eventually reaches the right answer doesn&#39;t confirm understanding of the optimization process.</p>\n<p><em>Fix</em>: Test intermediate states—verify that the cost function decreases monotonically, that gradients shrink as convergence approaches, and that parameter updates follow expected patterns.</p>\n<h2 id=\"integration-testing\">Integration Testing</h2>\n<p>Integration testing validates the complete end-to-end system behavior using realistic datasets and workflows. Unlike unit tests that isolate individual components, integration tests verify that components collaborate correctly and that the entire pipeline produces mathematically sound results on real-world data.</p>\n<h3 id=\"real-dataset-integration\">Real Dataset Integration</h3>\n<p>We use carefully selected real datasets that exhibit different characteristics and challenges, allowing learners to validate their implementation against data with genuine complexity while maintaining interpretable results.</p>\n<table>\n<thead>\n<tr>\n<th>Dataset</th>\n<th>Source</th>\n<th>Characteristics</th>\n<th>Integration Test Focus</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Boston Housing (subset)</td>\n<td>Scikit-learn</td>\n<td>506 samples, 13 features</td>\n<td>Multiple regression with mixed feature types</td>\n</tr>\n<tr>\n<td>California Housing (sample)</td>\n<td>Scikit-learn</td>\n<td>Geographic data, large feature ranges</td>\n<td>Feature normalization necessity</td>\n</tr>\n<tr>\n<td>Synthetic Polynomial</td>\n<td>Generated</td>\n<td>Known polynomial relationship</td>\n<td>Basis function extension testing</td>\n</tr>\n<tr>\n<td>Small Business Revenue</td>\n<td>Custom CSV</td>\n<td>Simple business metrics</td>\n<td>CSV loading and preprocessing validation</td>\n</tr>\n</tbody></table>\n<p><strong>Boston Housing Integration Test:</strong>\nThis test validates the complete multiple regression pipeline on a well-understood dataset where published results provide benchmarks for comparison. The test loads the data using <code>DataHandler</code>, applies feature normalization, trains both closed-form and gradient descent models, and compares their convergence behavior.</p>\n<p><strong>California Housing Feature Scaling Test:</strong>\nThis integration test deliberately uses the California housing dataset without preprocessing to demonstrate the necessity of feature normalization. The test shows how gradient descent fails to converge on raw data (where longitude ranges from -124 to -114 while median income ranges from 0.5 to 15) but succeeds after applying <code>StandardScaler</code>.</p>\n<p><strong>Custom CSV Integration Test:</strong>\nUsing a small, interpretable business dataset (e.g., advertising spend vs. revenue), this test validates the complete data loading pipeline. Learners can manually verify results and understand the business interpretation of coefficients.</p>\n<h3 id=\"cross-validation-integration\">Cross-Validation Integration</h3>\n<p>Integration testing includes cross-validation workflows that demonstrate model generalization and help learners understand overfitting concepts. We implement simple train-test splits rather than full k-fold cross-validation to maintain educational focus.</p>\n<table>\n<thead>\n<tr>\n<th>Validation Approach</th>\n<th>Dataset Split</th>\n<th>Metrics Computed</th>\n<th>Learning Objective</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Simple Train-Test Split</td>\n<td>80% train, 20% test</td>\n<td>Train R², Test R², Gap Analysis</td>\n<td>Overfitting detection</td>\n</tr>\n<tr>\n<td>Temporal Split</td>\n<td>First 70% chronological</td>\n<td>Time-based prediction validation</td>\n<td>Real-world relevance</td>\n</tr>\n<tr>\n<td>Feature Subset Validation</td>\n<td>Incremental feature addition</td>\n<td>Performance vs. complexity</td>\n<td>Feature selection intuition</td>\n</tr>\n</tbody></table>\n<p>The cross-validation integration tests reveal important machine learning principles:</p>\n<ul>\n<li><strong>Generalization Gap</strong>: Comparing training and testing R-squared scores helps learners understand when models memorize rather than learn patterns</li>\n<li><strong>Feature Selection Impact</strong>: Testing subsets of features demonstrates how additional predictors can improve or hurt generalization</li>\n<li><strong>Regularization Benefits</strong>: Ridge regression typically shows smaller generalization gaps than unregularized models</li>\n</ul>\n<h3 id=\"algorithm-comparison-integration\">Algorithm Comparison Integration</h3>\n<p>A critical integration test compares the closed-form solution with gradient descent on identical datasets, validating that both approaches converge to the same mathematical solution while demonstrating their different computational characteristics.</p>\n<table>\n<thead>\n<tr>\n<th>Comparison Aspect</th>\n<th>Closed-Form Result</th>\n<th>Gradient Descent Result</th>\n<th>Expected Relationship</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Final parameters</td>\n<td>Direct calculation</td>\n<td>Converged parameters</td>\n<td>Within numerical tolerance</td>\n</tr>\n<tr>\n<td>Computational time</td>\n<td>Instant (matrix ops)</td>\n<td>Multiple iterations</td>\n<td>Gradient descent slower</td>\n</tr>\n<tr>\n<td>Memory usage</td>\n<td>Full matrix inversion</td>\n<td>Incremental updates</td>\n<td>Depends on dataset size</td>\n</tr>\n<tr>\n<td>Numerical stability</td>\n<td>Matrix conditioning dependent</td>\n<td>Learning rate dependent</td>\n<td>Different failure modes</td>\n</tr>\n</tbody></table>\n<p>This comparison teaches learners when to choose each approach:</p>\n<ul>\n<li><strong>Small datasets</strong>: Closed-form solution preferred for speed and exactness</li>\n<li><strong>Large datasets</strong>: Gradient descent scales better with massive feature counts</li>\n<li><strong>Online learning</strong>: Only gradient descent supports incremental updates</li>\n<li><strong>Numerical challenges</strong>: Each approach has different stability characteristics</li>\n</ul>\n<h3 id=\"pipeline-robustness-testing\">Pipeline Robustness Testing</h3>\n<p>Integration tests validate system behavior under realistic adverse conditions that commonly occur in real-world data science workflows.</p>\n<table>\n<thead>\n<tr>\n<th>Robustness Test</th>\n<th>Adverse Condition</th>\n<th>Expected System Response</th>\n<th>Learning Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Missing value handling</td>\n<td>CSV with empty cells</td>\n<td>Graceful error with row identification</td>\n<td>Data quality awareness</td>\n</tr>\n<tr>\n<td>Mixed data types</td>\n<td>Numeric and string columns</td>\n<td>Clear type conversion guidance</td>\n<td>Data preprocessing necessity</td>\n</tr>\n<tr>\n<td>Extreme feature scales</td>\n<td>Features differing by 10^6 magnitude</td>\n<td>Convergence failure without normalization</td>\n<td>Feature scaling importance</td>\n</tr>\n<tr>\n<td>Near-singular matrices</td>\n<td>Highly correlated features</td>\n<td>Numerical instability warning</td>\n<td>Multicollinearity understanding</td>\n</tr>\n<tr>\n<td>Large datasets</td>\n<td>10,000+ samples</td>\n<td>Memory and performance monitoring</td>\n<td>Computational complexity awareness</td>\n</tr>\n</tbody></table>\n<h3 id=\"end-to-end-workflow-validation\">End-to-End Workflow Validation</h3>\n<p>The most comprehensive integration test validates the complete workflow that a learner would follow in practice: data loading, preprocessing, model training, evaluation, and prediction on new data.</p>\n<p><strong>Complete Workflow Test Sequence:</strong></p>\n<ol>\n<li>Load dataset from CSV using <code>load_csv_data()</code></li>\n<li>Validate data using <code>validate_regression_inputs()</code></li>\n<li>Split into training and testing sets</li>\n<li>Initialize and fit <code>SimpleLinearRegression</code> model</li>\n<li>Compare with <code>GradientDescentRegression</code> on same data</li>\n<li>Scale features and apply <code>MultipleLinearRegression</code></li>\n<li>Generate predictions on test set</li>\n<li>Compute comprehensive evaluation metrics</li>\n<li>Visualize results and residuals</li>\n</ol>\n<p>This workflow integration test serves as the final validation that all components work together harmoniously and that the learner can successfully apply their implementation to solve real regression problems.</p>\n<h3 id=\"common-integration-testing-pitfalls\">Common Integration Testing Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Using Only &quot;Clean&quot; Academic Datasets</strong>\nMany learners test only on preprocessed datasets like those from scikit-learn, missing the reality of messy real-world data. This creates false confidence when their implementation encounters actual CSV files with missing values, mixed data types, or formatting issues.</p>\n<p><em>Fix</em>: Include integration tests with deliberately messy data—CSV files with missing values, inconsistent formatting, mixed data types, and extra columns. Test the complete data cleaning pipeline.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Feature Scale Effects</strong>\nTesting primarily on normalized datasets masks the critical importance of feature scaling. Learners may not realize their gradient descent implementation fails on real data with natural feature scales.</p>\n<p><em>Fix</em>: Include explicit tests that demonstrate gradient descent failure on unnormalized data followed by success after feature scaling. Make the failure and recovery visible and educational.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Cross-Validation Understanding</strong>\nSimply splitting data and comparing metrics doesn&#39;t teach the deeper lesson about overfitting and generalization. Many learners miss the connection between model complexity and generalization gap.</p>\n<p><em>Fix</em>: Design cross-validation tests that explicitly demonstrate overfitting scenarios. Show how increasing polynomial degree initially improves training performance but eventually hurts test performance.</p>\n<h2 id=\"milestone-success-checkpoints\">Milestone Success Checkpoints</h2>\n<p>Each milestone defines specific, measurable success criteria that serve both as learning validation and implementation checkpoints. These criteria ensure progressive mastery while building confidence through demonstrable achievements.</p>\n<h3 id=\"milestone-1-simple-linear-regression-success-criteria\">Milestone 1: Simple Linear Regression Success Criteria</h3>\n<p>Milestone 1 establishes the foundation of linear regression understanding through closed-form solution implementation. Success validation focuses on mathematical correctness and basic prediction capability.</p>\n<p><strong>Mathematical Accuracy Checkpoints:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Checkpoint</th>\n<th>Test Data</th>\n<th>Success Criteria</th>\n<th>Validates Understanding</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Perfect Fit Validation</td>\n<td><code>y = 3x + 2</code> (no noise)</td>\n<td>slope_ = 3.0 ± 1e-10, intercept_ = 2.0 ± 1e-10</td>\n<td>Normal equation implementation</td>\n</tr>\n<tr>\n<td>R-squared Calculation</td>\n<td>Known residual pattern</td>\n<td>R² = 0.9876 (specific value)</td>\n<td>Coefficient of determination concept</td>\n</tr>\n<tr>\n<td>Prediction Accuracy</td>\n<td>New x values [1, 2, 3]</td>\n<td>Predictions [5, 8, 11] exactly</td>\n<td>Parameter application</td>\n</tr>\n<tr>\n<td>Boston Housing Subset</td>\n<td>First 50 samples, single feature</td>\n<td>R² &gt; 0.4, reasonable coefficients</td>\n<td>Real data applicability</td>\n</tr>\n</tbody></table>\n<p><strong>Interface Contract Validation:</strong></p>\n<p>The <code>SimpleLinearRegression</code> class must satisfy all interface requirements consistently:</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Pre-fit Behavior</th>\n<th>Post-fit Behavior</th>\n<th>Error Conditions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>fit(x, y)</code></td>\n<td>Sets is_fitted_=False</td>\n<td>Sets is_fitted_=True, returns self</td>\n<td>Validates input arrays</td>\n</tr>\n<tr>\n<td><code>predict(x)</code></td>\n<td>Raises <code>RegressionError</code></td>\n<td>Returns predictions array</td>\n<td>Validates x dimensionality</td>\n</tr>\n<tr>\n<td><code>score(x, y)</code></td>\n<td>Raises <code>RegressionError</code></td>\n<td>Returns R-squared float</td>\n<td>Validates fitted state</td>\n</tr>\n<tr>\n<td>Attribute access</td>\n<td>slope_=None, intercept_=None</td>\n<td>Computed values available</td>\n<td>Consistent state management</td>\n</tr>\n</tbody></table>\n<p><strong>Verification Commands:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run Milestone 1 test suite</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_simple_linear_regression.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Manual verification script</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> scripts/milestone1_verification.py</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected output:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ Perfect linear fit: R² = 1.000</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ Slope accuracy: 2.500 (expected: 2.500)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ Intercept accuracy: 1.700 (expected: 1.700)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ Prediction validation passed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ Boston housing test: R² = 0.423</span></span></code></pre></div>\n\n<p><strong>Milestone 1 Behavioral Checkpoints:</strong></p>\n<p>Beyond mathematical accuracy, learners must demonstrate understanding of the underlying concepts:</p>\n<ol>\n<li><strong>Normal Equation Understanding</strong>: Can explain why the closed-form solution works and its mathematical derivation</li>\n<li><strong>Least Squares Principle</strong>: Understands that linear regression minimizes sum of squared residuals</li>\n<li><strong>R-squared Interpretation</strong>: Can explain what R² = 0.7 means in practical terms</li>\n<li><strong>Prediction Process</strong>: Understands that prediction applies fitted parameters to new data</li>\n<li><strong>Limitation Recognition</strong>: Recognizes when closed-form solution breaks (singular matrices, perfect collinearity)</li>\n</ol>\n<h3 id=\"milestone-2-gradient-descent-success-criteria\">Milestone 2: Gradient Descent Success Criteria</h3>\n<p>Milestone 2 introduces iterative optimization, requiring demonstration of convergence understanding and algorithmic implementation skills.</p>\n<p><strong>Convergence Validation Checkpoints:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Checkpoint</th>\n<th>Test Configuration</th>\n<th>Success Criteria</th>\n<th>Demonstrates</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Basic Convergence</td>\n<td>Perfect linear data, lr=0.01</td>\n<td>Converged within 100 iterations</td>\n<td>Algorithm correctness</td>\n</tr>\n<tr>\n<td>Cost Reduction</td>\n<td>Monitor cost_history_</td>\n<td>Monotonic decrease to &lt; 1e-6</td>\n<td>Optimization understanding</td>\n</tr>\n<tr>\n<td>Parameter Accuracy</td>\n<td>Compare to closed-form solution</td>\n<td>Within 1e-4 of analytical solution</td>\n<td>Mathematical consistency</td>\n</tr>\n<tr>\n<td>Learning Rate Sensitivity</td>\n<td>Test lr=[0.001, 0.01, 0.1, 1.0]</td>\n<td>Stable convergence for appropriate rates</td>\n<td>Hyperparameter awareness</td>\n</tr>\n<tr>\n<td>Convergence Detection</td>\n<td>Controlled cost plateau</td>\n<td>Stops within tolerance window</td>\n<td>Termination criteria</td>\n</tr>\n</tbody></table>\n<p><strong>Gradient Descent Algorithm Validation:</strong></p>\n<p>The core optimization loop must demonstrate correct implementation of gradient descent principles:</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm Component</th>\n<th>Test Method</th>\n<th>Expected Behavior</th>\n<th>Mathematical Validation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Cost function</td>\n<td>Known residuals</td>\n<td>Exact MSE calculation</td>\n<td><code>cost = Σ(yi - ŷi)² / (2n)</code></td>\n</tr>\n<tr>\n<td>Gradient calculation</td>\n<td>Analytical comparison</td>\n<td>Match partial derivatives</td>\n<td><code>∂J/∂m = Σ(ŷi - yi)xi / n</code></td>\n</tr>\n<tr>\n<td>Parameter updates</td>\n<td>Single iteration</td>\n<td>Correct update formula</td>\n<td><code>θ := θ - α∇J(θ)</code></td>\n</tr>\n<tr>\n<td>Convergence check</td>\n<td>Multiple criteria</td>\n<td>Combined tolerance evaluation</td>\n<td>Cost, gradient, parameter changes</td>\n</tr>\n</tbody></table>\n<p><strong>Training History Validation:</strong></p>\n<p>The <code>TrainingHistory</code> object must capture complete optimization trajectory:</p>\n<table>\n<thead>\n<tr>\n<th>History Component</th>\n<th>Required Content</th>\n<th>Validation Method</th>\n<th>Educational Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>cost_history_</td>\n<td>Cost at each iteration</td>\n<td>Monotonic decrease check</td>\n<td>Optimization visualization</td>\n</tr>\n<tr>\n<td>parameter_history_</td>\n<td>Parameters at each step</td>\n<td>Convergence trajectory</td>\n<td>Parameter evolution understanding</td>\n</tr>\n<tr>\n<td>converged_</td>\n<td>Boolean convergence flag</td>\n<td>Matches stopping criteria</td>\n<td>Termination understanding</td>\n</tr>\n<tr>\n<td>final_iteration_</td>\n<td>Iteration count</td>\n<td>Reasonable convergence speed</td>\n<td>Algorithm efficiency</td>\n</tr>\n<tr>\n<td>convergence_reason_</td>\n<td>Descriptive string</td>\n<td>Matches actual stopping condition</td>\n<td>Debugging capability</td>\n</tr>\n</tbody></table>\n<p><strong>Verification Commands:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run Milestone 2 test suite  </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_gradient_descent.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Convergence visualization</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> scripts/milestone2_convergence_analysis.py</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected output:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ Convergence achieved in 87 iterations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ Final cost: 0.000003 (tolerance: 0.000001)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ Parameters match closed-form solution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ Cost history shows monotonic decrease</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ All learning rates 0.001-0.1 converged</span></span></code></pre></div>\n\n<h3 id=\"milestone-3-multiple-linear-regression-success-criteria\">Milestone 3: Multiple Linear Regression Success Criteria</h3>\n<p>Milestone 3 represents the culmination of linear regression mastery, requiring matrix operation competency and regularization understanding.</p>\n<p><strong>Matrix Operation Validation:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Matrix Component</th>\n<th>Test Data</th>\n<th>Success Criteria</th>\n<th>Validates</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Design matrix construction</td>\n<td>3 features + intercept</td>\n<td>Shape (n, 4), first column all 1s</td>\n<td>Matrix formulation understanding</td>\n</tr>\n<tr>\n<td>Vectorized gradient computation</td>\n<td>Multiple features</td>\n<td>Gradient shape (n_features + 1,)</td>\n<td>Batch operation implementation</td>\n</tr>\n<tr>\n<td>Parameter vector</td>\n<td>Multiple regression fit</td>\n<td>weights_ shape matches features</td>\n<td>Weight vector concept</td>\n</tr>\n<tr>\n<td>Prediction matrix multiplication</td>\n<td>Test feature matrix</td>\n<td>Correct matrix-vector product</td>\n<td>Linear algebra application</td>\n</tr>\n</tbody></table>\n<p><strong>Multiple Regression Mathematical Validation:</strong></p>\n<p>The implementation must demonstrate correct handling of multi-dimensional optimization:</p>\n<table>\n<thead>\n<tr>\n<th>Mathematical Concept</th>\n<th>Test Approach</th>\n<th>Expected Result</th>\n<th>Conceptual Understanding</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Hyperplane fitting</td>\n<td>3D visualization data</td>\n<td>Reasonable plane coefficients</td>\n<td>Multi-dimensional relationships</td>\n</tr>\n<tr>\n<td>Feature interaction</td>\n<td>Orthogonal vs. correlated features</td>\n<td>Different convergence behavior</td>\n<td>Feature independence effects</td>\n</tr>\n<tr>\n<td>Regularization impact</td>\n<td>Ridge with λ=[0, 0.1, 1.0, 10.0]</td>\n<td>Progressive coefficient shrinkage</td>\n<td>Bias-variance tradeoff</td>\n</tr>\n<tr>\n<td>Feature scaling necessity</td>\n<td>Mixed feature scales</td>\n<td>Convergence failure without scaling</td>\n<td>Preprocessing importance</td>\n</tr>\n</tbody></table>\n<p><strong>Regularization Effectiveness Validation:</strong></p>\n<p>Ridge regression must demonstrate proper L2 penalty implementation:</p>\n<table>\n<thead>\n<tr>\n<th>Regularization Strength</th>\n<th>Expected Behavior</th>\n<th>Test Method</th>\n<th>Learning Outcome</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>λ = 0.0</td>\n<td>Identical to unregularized</td>\n<td>Parameter comparison</td>\n<td>Regularization baseline</td>\n</tr>\n<tr>\n<td>λ = 0.1</td>\n<td>Slight coefficient shrinkage</td>\n<td>Magnitude reduction check</td>\n<td>Gentle penalty effect</td>\n</tr>\n<tr>\n<td>λ = 1.0</td>\n<td>Moderate shrinkage</td>\n<td>Reduced overfitting evidence</td>\n<td>Practical regularization</td>\n</tr>\n<tr>\n<td>λ = 10.0</td>\n<td>Strong shrinkage</td>\n<td>Near-zero coefficients</td>\n<td>Over-regularization recognition</td>\n</tr>\n</tbody></table>\n<p><strong>Feature Engineering Validation:</strong></p>\n<p>The system must properly handle feature preprocessing and transformation:</p>\n<table>\n<thead>\n<tr>\n<th>Feature Processing</th>\n<th>Test Scenario</th>\n<th>Success Criteria</th>\n<th>Teaches</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Standardization</td>\n<td>Different feature scales</td>\n<td>Mean ≈ 0, std ≈ 1 for all features</td>\n<td>Z-score normalization</td>\n</tr>\n<tr>\n<td>Design matrix</td>\n<td>Manual verification</td>\n<td>Correct intercept column placement</td>\n<td>Matrix structure</td>\n</tr>\n<tr>\n<td>New data scaling</td>\n<td>Apply fitted scaler</td>\n<td>Consistent transformation</td>\n<td>Preprocessing consistency</td>\n</tr>\n<tr>\n<td>Missing value detection</td>\n<td>Incomplete feature matrix</td>\n<td>Clear error with location info</td>\n<td>Data quality validation</td>\n</tr>\n</tbody></table>\n<p><strong>Verification Commands:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run comprehensive Milestone 3 test suite</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_multiple_regression.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Full system integration test</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> scripts/milestone3_full_integration.py</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected output:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ Design matrix construction: Shape (506, 14)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ Vectorized gradients: Shape (14,) </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ Ridge regularization: λ=0.1 reduces overfitting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ Feature scaling: All features normalized</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ Cross-validation: Test R² = 0.64, Train R² = 0.67</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ Algorithm comparison: Closed-form vs. GD within tolerance</span></span></code></pre></div>\n\n<h3 id=\"progressive-learning-validation\">Progressive Learning Validation</h3>\n<p>The milestone checkpoints build understanding progressively, with each milestone requiring mastery of previous concepts while introducing new complexity layers.</p>\n<p><strong>Concept Mastery Progression:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Milestone</th>\n<th>Core Concepts</th>\n<th>Required Understanding</th>\n<th>Builds Foundation For</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>M1: Simple Regression</td>\n<td>Linear relationships, least squares</td>\n<td>Mathematical optimization principles</td>\n<td>Iterative optimization</td>\n</tr>\n<tr>\n<td>M2: Gradient Descent</td>\n<td>Iterative optimization, convergence</td>\n<td>Algorithm design and tuning</td>\n<td>Multi-dimensional optimization</td>\n</tr>\n<tr>\n<td>M3: Multiple Regression</td>\n<td>Matrix operations, regularization</td>\n<td>Production ML considerations</td>\n<td>Advanced ML algorithms</td>\n</tr>\n</tbody></table>\n<p><strong>Implementation Skill Development:</strong></p>\n<p>Each milestone develops specific technical competencies that prepare learners for advanced machine learning topics:</p>\n<ol>\n<li><strong>Milestone 1</strong>: NumPy array manipulation, basic linear algebra, interface design</li>\n<li><strong>Milestone 2</strong>: Algorithm implementation, convergence monitoring, hyperparameter sensitivity  </li>\n<li><strong>Milestone 3</strong>: Matrix operations, feature engineering, performance optimization</li>\n</ol>\n<h3 id=\"common-milestone-checkpoint-pitfalls\">Common Milestone Checkpoint Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Focusing Only on Final Metrics</strong>\nMany learners consider a milestone complete when final R-squared scores look reasonable, missing the deeper learning validation. Getting the right answer without understanding the process defeats the educational purpose.</p>\n<p><em>Fix</em>: Include process validation in checkpoints. Test intermediate states, verify gradient calculations manually, and require explanation of why each step works.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Edge Case Testing</strong><br>Milestone checkpoints often test only the &quot;happy path&quot; with clean data and good convergence, missing the robust error handling that characterizes professional implementations.</p>\n<p><em>Fix</em>: Include edge case validation in every milestone checkpoint. Test division by zero, convergence failure, and malformed input data. Require graceful degradation.</p>\n<p>⚠️ <strong>Pitfall: Insufficient Cross-Milestone Integration</strong>\nTesting each milestone in isolation misses the important integration between concepts. Learners may understand gradient descent separately from feature scaling but fail to combine them effectively.</p>\n<p><em>Fix</em>: Include integration checkpoints that require combining concepts from multiple milestones. Test the complete workflow end-to-end at each stage.</p>\n<p><img src=\"/api/project/linear-regression/architecture-doc/asset?path=diagrams%2Fmilestone-progression.svg\" alt=\"Learning Milestone Progression\"></p>\n<blockquote>\n<p><strong>Key Insight</strong>: Milestone checkpoints serve dual purposes—they validate implementation correctness while confirming conceptual understanding. A learner who passes all checkpoints has demonstrated not just coding competency, but mastery of linear regression fundamentals that will transfer to advanced machine learning concepts.</p>\n</blockquote>\n<p>The testing strategy and milestone checkpoints transform abstract learning goals into concrete, measurable achievements. Each checkpoint provides clear evidence of progress while building the foundation for subsequent learning. This structured approach ensures that learners develop both theoretical understanding and practical implementation skills necessary for advanced machine learning study.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides the practical implementation support needed to execute the testing strategy effectively, bridging the gap between testing theory and actual code validation.</p>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Testing Framework</td>\n<td>Python <code>unittest</code> (built-in)</td>\n<td><code>pytest</code> with fixtures</td>\n</tr>\n<tr>\n<td>Assertion Library</td>\n<td>Basic <code>assert</code> statements</td>\n<td>NumPy testing (<code>np.testing.assert_allclose</code>)</td>\n</tr>\n<tr>\n<td>Test Data Generation</td>\n<td>Manual array creation</td>\n<td>Property-based testing with <code>hypothesis</code></td>\n</tr>\n<tr>\n<td>Visualization</td>\n<td><code>matplotlib</code> for simple plots</td>\n<td><code>seaborn</code> + <code>plotly</code> for interactive analysis</td>\n</tr>\n<tr>\n<td>Performance Monitoring</td>\n<td>Basic timing with <code>time.time()</code></td>\n<td><code>pytest-benchmark</code> for statistical timing</td>\n</tr>\n<tr>\n<td>Coverage Analysis</td>\n<td>Visual inspection</td>\n<td><code>pytest-cov</code> for coverage reports</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended Testing File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  src/\n    data_handler.py\n    simple_regression.py  \n    gradient_descent.py\n    multiple_regression.py\n  tests/\n    conftest.py                    ← pytest fixtures and test data\n    test_data_handler.py          ← unit tests for data loading/preprocessing\n    test_simple_regression.py     ← milestone 1 validation tests\n    test_gradient_descent.py      ← milestone 2 convergence tests\n    test_multiple_regression.py   ← milestone 3 matrix operation tests\n    test_integration.py           ← end-to-end integration tests\n  test_data/\n    synthetic_linear.csv          ← known linear relationship\n    boston_housing_subset.csv     ← real data sample\n    messy_data.csv               ← missing values, mixed types\n  scripts/\n    milestone1_verification.py    ← M1 checkpoint validation\n    milestone2_convergence.py     ← M2 optimization analysis\n    milestone3_integration.py     ← M3 full system validation</code></pre></div>\n\n<p><strong>Complete Test Infrastructure (Starter Code):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/conftest.py - Shared test fixtures and utilities</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pytest</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pandas </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> pd</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@pytest.fixture</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> perfect_linear_data</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Perfect linear relationship for exact testing\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    np.random.seed(</span><span style=\"color:#79B8FF\">42</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    x </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.linspace(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">50</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    y </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 2.5</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> x </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1.7</span><span style=\"color:#6A737D\">  # Known slope and intercept</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> x.reshape(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">), y</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@pytest.fixture</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> noisy_linear_data</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Linear relationship with controlled noise\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    np.random.seed(</span><span style=\"color:#79B8FF\">42</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    x </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.linspace(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    y </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 2.5</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> x </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1.7</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> np.random.normal(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0.1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> x.reshape(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">), y</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@pytest.fixture</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> multiple_features_data</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Multiple regression test data\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    np.random.seed(</span><span style=\"color:#79B8FF\">42</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    n_samples, n_features </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 200</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    X </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.random.randn(n_samples, n_features)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    true_weights </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.array([</span><span style=\"color:#79B8FF\">1.5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">2.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0.8</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> X </span><span style=\"color:#F97583\">@</span><span style=\"color:#E1E4E8\"> true_weights </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 0.5</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> np.random.normal(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0.1</span><span style=\"color:#E1E4E8\">, n_samples)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> X, y, true_weights</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@pytest.fixture</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> edge_case_data</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Edge cases that should trigger errors\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'empty_arrays'</span><span style=\"color:#E1E4E8\">: (np.array([]), np.array([])),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'mismatched_lengths'</span><span style=\"color:#E1E4E8\">: (np.array([</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">]), np.array([</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">])),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'single_point'</span><span style=\"color:#E1E4E8\">: (np.array([</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]), np.array([</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">])),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'constant_x'</span><span style=\"color:#E1E4E8\">: (np.array([</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">]), np.array([</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">])),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'nan_values'</span><span style=\"color:#E1E4E8\">: (np.array([</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, np.nan]), np.array([</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">])),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'infinite_values'</span><span style=\"color:#E1E4E8\">: (np.array([</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, np.inf]), np.array([</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">]))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> assert_regression_parameters_close</span><span style=\"color:#E1E4E8\">(actual_slope, actual_intercept, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                     expected_slope, expected_intercept, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                     tolerance</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1e-6</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Helper for parameter validation\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    np.testing.assert_allclose(actual_slope, expected_slope, </span><span style=\"color:#FFAB70\">atol</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">tolerance,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                              err_msg</span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Slope mismatch: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">actual_slope</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> vs </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">expected_slope</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    np.testing.assert_allclose(actual_intercept, expected_intercept, </span><span style=\"color:#FFAB70\">atol</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">tolerance,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                              err_msg</span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Intercept mismatch: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">actual_intercept</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> vs </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">expected_intercept</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_convergence_history</span><span style=\"color:#E1E4E8\">(cost_history, tolerance</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1e-6</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validate gradient descent convergence properties\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(cost_history) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Cost history should not be empty\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Check monotonic decrease (allowing for numerical noise)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(cost_history)):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> cost_history[i] </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#E1E4E8\"> cost_history[i</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1e-12</span><span style=\"color:#E1E4E8\">, \\</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"Cost increased at iteration </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">cost_history[i]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> > </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">cost_history[i</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Check final convergence</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    final_cost </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> cost_history[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> final_cost </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> tolerance, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Final cost </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">final_cost</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> exceeds tolerance </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">tolerance</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> True</span></span></code></pre></div>\n\n<p><strong>Core Testing Skeletons (TODOs for learner implementation):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/test_simple_regression.py - Milestone 1 validation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pytest</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> src.simple_regression </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> SimpleLinearRegression</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestSimpleLinearRegression</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_perfect_fit_accuracy</span><span style=\"color:#E1E4E8\">(self, perfect_linear_data):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate closed-form solution on perfect data\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        X, y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> perfect_linear_data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SimpleLinearRegression()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Call model.fit(X, y) and verify it returns self</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check that model.is_fitted_ is True after fitting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Verify model.slope_ equals 2.5 within tolerance 1e-10</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify model.intercept_ equals 1.7 within tolerance 1e-10</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Check model.score(X, y) returns 1.0 (perfect fit)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use assert_regression_parameters_close from conftest.py</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_prediction_accuracy</span><span style=\"color:#E1E4E8\">(self, perfect_linear_data):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate prediction calculation\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        X, y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> perfect_linear_data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SimpleLinearRegression()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        model.fit(X, y)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create test points X_new = [[0], [1], [2]]  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Call model.predict(X_new) to get predictions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate expected predictions: y = 2.5 * x + 1.7</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify predictions match expected within tolerance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: For x=0, expect y=1.7; for x=1, expect y=4.2; etc.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_error_handling</span><span style=\"color:#E1E4E8\">(self, edge_case_data):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate proper error handling\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SimpleLinearRegression()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Test prediction before fitting raises RegressionError</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Test empty arrays raise DataValidationError  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Test mismatched array lengths raise DataValidationError</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Test constant x values raise NumericalInstabilityError</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Verify error messages are descriptive and helpful</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use pytest.raises(ExceptionType) context manager</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/test_gradient_descent.py - Milestone 2 validation  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pytest</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> src.gradient_descent </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> GradientDescentRegression</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestGradientDescentOptimization</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_convergence_perfect_data</span><span style=\"color:#E1E4E8\">(self, perfect_linear_data):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate gradient descent convergence\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        X, y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> perfect_linear_data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> GradientDescentRegression(</span><span style=\"color:#FFAB70\">learning_rate</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.01</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">max_iterations</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">tolerance</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1e-6</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Fit model and verify convergence (model.converged_ is True)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check final parameters match true values (2.5, 1.7) within 1e-4</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Verify cost_history_ shows monotonic decrease</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check final cost is below tolerance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Verify convergence_reason_ is descriptive</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use validate_convergence_history from conftest.py</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_learning_rate_sensitivity</span><span style=\"color:#E1E4E8\">(self, noisy_linear_data):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test different learning rates\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        X, y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> noisy_linear_data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        learning_rates </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#79B8FF\">0.001</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0.01</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0.1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Test each learning rate in a loop</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Verify rates [0.001, 0.01, 0.1] converge successfully  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Verify rate 1.0 either converges slowly or diverges</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check that smaller learning rates take more iterations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate that all converged models reach similar parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Track convergence success and iteration counts</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_gradient_calculation_accuracy</span><span style=\"color:#E1E4E8\">(self, perfect_linear_data):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate gradient computation against analytical solution\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        X, y </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> perfect_linear_data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> GradientDescentRegression()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize parameters to arbitrary values (slope=0, intercept=0)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Compute gradients using model._compute_gradients()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate analytical gradients manually using calculus formulas</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify computed gradients match analytical within tolerance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Test gradient calculation at multiple parameter values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Analytical gradients are ∂J/∂m = Σ(ŷ-y)x/n, ∂J/∂b = Σ(ŷ-y)/n</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint Scripts:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># scripts/milestone1_verification.py - Automated M1 checkpoint</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> sys</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Add src to path for imports</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">sys.path.append(</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(Path(</span><span style=\"color:#79B8FF\">__file__</span><span style=\"color:#E1E4E8\">).parent.parent </span><span style=\"color:#F97583\">/</span><span style=\"color:#9ECBFF\"> 'src'</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> simple_regression </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> SimpleLinearRegression</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> data_handler </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> generate_synthetic_data</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> run_milestone1_checkpoint</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Complete Milestone 1 validation checkpoint\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"🔍 Running Milestone 1 Verification...\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate perfect linear data using generate_synthetic_data()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create and fit SimpleLinearRegression model</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Verify parameters match expected values (slope, intercept)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Test prediction accuracy on new data points</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Load and test Boston housing subset data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Print comprehensive success report</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"✅ Milestone 1: Simple Linear Regression - COMPLETED\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> __name__</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#9ECBFF\"> \"__main__\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    success </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> run_milestone1_checkpoint()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sys.exit(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> success </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Language-Specific Implementation Hints:</strong></p>\n<p><strong>NumPy Testing Best Practices:</strong></p>\n<ul>\n<li>Use <code>np.testing.assert_allclose()</code> instead of exact equality for floating-point comparisons</li>\n<li>Set appropriate <code>atol</code> (absolute tolerance) and <code>rtol</code> (relative tolerance) based on expected numerical precision</li>\n<li>Use <code>np.testing.assert_array_equal()</code> for integer arrays and shape validation</li>\n</ul>\n<p><strong>Error Testing Patterns:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test specific exception types</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">with</span><span style=\"color:#E1E4E8\"> pytest.raises(DataValidationError, </span><span style=\"color:#FFAB70\">match</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Array lengths must match\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    model.fit(X_wrong_shape, y)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test exception attributes  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">with</span><span style=\"color:#E1E4E8\"> pytest.raises(ConvergenceError) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> exc_info:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    model.fit(X, y)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> exc_info.value.error_code </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"DIVERGENCE\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#9ECBFF\"> \"learning_rate\"</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> exc_info.value.suggested_fix</span></span></code></pre></div>\n\n<p><strong>Gradient Descent Testing Strategies:</strong></p>\n<ul>\n<li>Test convergence with multiple random seeds to ensure robustness</li>\n<li>Verify that cost function value matches manual MSE calculation</li>\n<li>Test parameter update formula by checking single iteration changes</li>\n<li>Validate that learning rate scaling affects convergence speed proportionally</li>\n</ul>\n<p><strong>Cross-Validation Implementation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> simple_train_test_split</span><span style=\"color:#E1E4E8\">(X, y, test_size</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.2</span><span style=\"color:#E1E4E8\">, random_state</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">42</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Simple train-test split for integration testing\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement random shuffling and splitting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return X_train, X_test, y_train, y_test</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Ensure reproducible splits with random_state</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<p><strong>Debugging Tips for Testing:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>All gradient descent tests timeout</td>\n<td>Learning rate too high causing divergence</td>\n<td>Check if cost_history_ shows increasing values</td>\n<td>Reduce learning rate, add divergence detection</td>\n</tr>\n<tr>\n<td>Perfect fit tests fail with small errors</td>\n<td>Numerical precision limitations</td>\n<td>Compare expected vs actual tolerances</td>\n<td>Adjust tolerance to 1e-10 for closed-form, 1e-6 for iterative</td>\n</tr>\n<tr>\n<td>Integration tests pass but unit tests fail</td>\n<td>Test data inconsistency</td>\n<td>Verify same datasets used in both test types</td>\n<td>Use shared fixtures from conftest.py</td>\n</tr>\n<tr>\n<td>Regularization tests show no effect</td>\n<td>Lambda parameter not being applied</td>\n<td>Check cost function includes penalty term</td>\n<td>Verify _compute_cost() adds L2 penalty</td>\n</tr>\n</tbody></table>\n<h2 id=\"debugging-guide\">Debugging Guide</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (M1: Simple Linear Regression - debugging data issues and basic fitting, M2: Gradient Descent - debugging convergence and optimization problems, M3: Multiple Linear Regression - debugging matrix operations and scaling issues)</p>\n</blockquote>\n<p>Debugging machine learning implementations presents unique challenges that extend beyond traditional software debugging. Unlike typical software bugs where incorrect output is immediately obvious, machine learning bugs often manifest as subtle performance degradation, slow convergence, or mathematically correct but practically useless results. This section provides a systematic approach to identifying, diagnosing, and resolving the most common issues encountered when implementing linear regression from scratch.</p>\n<h3 id=\"mental-model-the-medical-diagnostic-process\">Mental Model: The Medical Diagnostic Process</h3>\n<p>Think of debugging machine learning code like a doctor diagnosing a patient with mysterious symptoms. Just as a doctor follows a systematic process—gathering symptoms, forming hypotheses, running targeted tests, and prescribing treatment—debugging ML code requires careful observation of symptoms (poor predictions, slow training), hypothesis formation (data issues vs algorithmic problems), targeted investigation (plotting cost curves, inspecting gradients), and systematic fixes. The key insight is that ML bugs rarely have single causes; they often result from combinations of data quality issues, numerical instabilities, and algorithmic choices that interact in complex ways.</p>\n<p>The diagnostic process becomes more complex because machine learning algorithms can appear to work correctly while producing suboptimal results. A gradient descent implementation might converge to some solution, but whether it&#39;s the optimal solution requires deeper investigation. This is why systematic debugging approaches and comprehensive validation techniques are essential for ML implementations.</p>\n<h3 id=\"common-bug-patterns\">Common Bug Patterns</h3>\n<p>Understanding common bug patterns helps developers quickly identify and resolve issues without extensive debugging sessions. Each pattern includes the typical symptoms learners observe, the underlying causes, diagnostic techniques for confirming the hypothesis, and step-by-step fixes.</p>\n<h4 id=\"data-related-bug-patterns\">Data-Related Bug Patterns</h4>\n<table>\n<thead>\n<tr>\n<th>Bug Pattern</th>\n<th>Symptoms</th>\n<th>Underlying Cause</th>\n<th>Diagnostic Approach</th>\n<th>Fix Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Feature-Target Dimension Mismatch</strong></td>\n<td>Array shape errors during fitting, predictions return wrong shapes</td>\n<td>Inconsistent array dimensions between features and targets</td>\n<td>Check <code>features.shape</code> and <code>targets.shape</code>, verify they match expected dimensions</td>\n<td>Reshape arrays using <code>.reshape()</code> or fix data loading logic</td>\n</tr>\n<tr>\n<td><strong>Unnormalized Features</strong></td>\n<td>Gradient descent fails to converge, parameters grow extremely large</td>\n<td>Features on vastly different scales cause gradient imbalance</td>\n<td>Plot feature distributions, check standard deviations across features</td>\n<td>Apply z-score normalization before training</td>\n</tr>\n<tr>\n<td><strong>Missing Data Propagation</strong></td>\n<td>NaN values in predictions, cost function returns NaN</td>\n<td>NaN values in input data propagate through calculations</td>\n<td>Use <code>np.isnan()</code> to check for missing values in datasets</td>\n<td>Remove or impute missing values before training</td>\n</tr>\n<tr>\n<td><strong>Data Leakage in Normalization</strong></td>\n<td>Unrealistically high R-squared scores on test data</td>\n<td>Normalization statistics computed on entire dataset including test data</td>\n<td>Verify normalization computed only on training data</td>\n<td>Recompute normalization using only training features</td>\n</tr>\n<tr>\n<td><strong>Constant Feature Columns</strong></td>\n<td>Division by zero errors during normalization</td>\n<td>Features with zero variance cause division by zero in z-score calculation</td>\n<td>Check <code>np.std(features, axis=0)</code> for zero values</td>\n<td>Remove constant features or add small epsilon to standard deviation</td>\n</tr>\n</tbody></table>\n<h4 id=\"algorithmic-bug-patterns\">Algorithmic Bug Patterns</h4>\n<table>\n<thead>\n<tr>\n<th>Bug Pattern</th>\n<th>Symptoms</th>\n<th>Underlying Cause</th>\n<th>Diagnostic Approach</th>\n<th>Fix Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Gradient Descent Divergence</strong></td>\n<td>Cost increases instead of decreasing, parameters explode to large values</td>\n<td>Learning rate too high causes overshooting</td>\n<td>Plot cost history, check if cost oscillates or increases</td>\n<td>Reduce learning rate by factor of 10, add gradient clipping</td>\n</tr>\n<tr>\n<td><strong>Premature Convergence</strong></td>\n<td>Training stops too early, suboptimal final parameters</td>\n<td>Convergence tolerance too loose or inappropriate stopping criteria</td>\n<td>Compare final parameters to analytical solution, check convergence reason</td>\n<td>Tighten tolerance, use multiple convergence criteria</td>\n</tr>\n<tr>\n<td><strong>Gradient Computation Errors</strong></td>\n<td>Slow convergence, parameters update in wrong direction</td>\n<td>Incorrect partial derivative implementation</td>\n<td>Compare computed gradients to numerical gradients using finite differences</td>\n<td>Fix gradient calculation, verify against mathematical derivation</td>\n</tr>\n<tr>\n<td><strong>Learning Rate Too Small</strong></td>\n<td>Extremely slow convergence, minimal parameter updates</td>\n<td>Conservative learning rate causes tiny steps</td>\n<td>Monitor parameter changes per iteration, check gradient magnitudes</td>\n<td>Increase learning rate gradually, use adaptive learning rate</td>\n</tr>\n<tr>\n<td><strong>Matrix Dimension Errors</strong></td>\n<td>Broadcasting errors, unexpected array shapes in multiple regression</td>\n<td>Incorrect design matrix construction or weight vector dimensions</td>\n<td>Print array shapes at each operation, verify matrix multiplication compatibility</td>\n<td>Fix design matrix construction, ensure weight vector matches feature count plus one</td>\n</tr>\n</tbody></table>\n<h4 id=\"numerical-stability-patterns\">Numerical Stability Patterns</h4>\n<table>\n<thead>\n<tr>\n<th>Bug Pattern</th>\n<th>Symptoms</th>\n<th>Underlying Cause</th>\n<th>Diagnostic Approach</th>\n<th>Fix Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Parameter Overflow</strong></td>\n<td>Parameters become inf or extremely large values</td>\n<td>Accumulated numerical errors cause parameter explosion</td>\n<td>Check parameter magnitudes during training, monitor for inf values</td>\n<td>Add parameter clipping, reduce learning rate, improve numerical precision</td>\n</tr>\n<tr>\n<td><strong>Gradient Underflow</strong></td>\n<td>Parameters stop updating despite not converging</td>\n<td>Gradients become too small to affect parameter updates</td>\n<td>Monitor gradient magnitudes, check for values near machine epsilon</td>\n<td>Increase learning rate, check for numerical precision loss</td>\n</tr>\n<tr>\n<td><strong>Matrix Conditioning Issues</strong></td>\n<td>Unstable solutions in multiple regression, high sensitivity to input changes</td>\n<td>Design matrix becomes nearly singular or poorly conditioned</td>\n<td>Compute condition number of design matrix using <code>np.linalg.cond()</code></td>\n<td>Add regularization, remove redundant features, improve data quality</td>\n</tr>\n<tr>\n<td><strong>Precision Loss in Cost Calculation</strong></td>\n<td>Cost function plateaus prematurely, no further improvement possible</td>\n<td>Floating point precision insufficient for small cost differences</td>\n<td>Monitor relative cost changes, check if improvements below machine precision</td>\n<td>Use double precision, implement relative convergence criteria</td>\n</tr>\n<tr>\n<td><strong>Accumulated Rounding Errors</strong></td>\n<td>Gradual parameter drift, unstable convergence behavior</td>\n<td>Small rounding errors accumulate over many iterations</td>\n<td>Compare single-precision vs double-precision results, monitor error accumulation</td>\n<td>Use higher precision arithmetic, implement periodic parameter normalization</td>\n</tr>\n</tbody></table>\n<h3 id=\"ml-specific-debugging-techniques\">ML-Specific Debugging Techniques</h3>\n<p>Machine learning debugging requires specialized techniques that go beyond traditional software debugging approaches. These techniques focus on understanding algorithm behavior, validating mathematical correctness, and diagnosing performance issues specific to optimization and statistical modeling.</p>\n<h4 id=\"visualization-based-debugging\">Visualization-Based Debugging</h4>\n<p>Visualization provides immediate insights into algorithm behavior and data quality issues that are difficult to detect through numerical inspection alone. The key is knowing what to plot and how to interpret the results.</p>\n<p><strong>Cost Function Monitoring:</strong> The cost history plot serves as the primary diagnostic tool for gradient descent debugging. A healthy cost curve should decrease monotonically with occasional plateaus. Oscillating costs indicate learning rate issues, while flat costs suggest convergence or numerical precision problems. Plotting both raw cost values and cost differences helps identify convergence patterns and numerical precision limits.</p>\n<p><strong>Parameter Evolution Tracking:</strong> Plotting parameter values over training iterations reveals optimization dynamics. Parameters should generally move toward stable values with decreasing update magnitudes. Oscillating parameters indicate learning rate problems, while parameters that grow without bound suggest gradient computation errors or numerical instabilities.</p>\n<p><strong>Gradient Magnitude Analysis:</strong> Monitoring gradient magnitudes throughout training helps diagnose optimization problems. Gradients should generally decrease as the algorithm approaches optimal parameters. Gradient explosion (rapidly increasing magnitudes) indicates numerical instability, while gradient vanishing (magnitudes near zero) suggests either convergence or underflow issues.</p>\n<p><strong>Residual Analysis:</strong> Plotting prediction residuals against fitted values reveals model assumptions violations and data quality issues. Residuals should appear randomly scattered around zero with constant variance. Patterns in residuals indicate missing nonlinear relationships, heteroscedasticity, or outliers affecting model performance.</p>\n<h4 id=\"mathematical-verification-techniques\">Mathematical Verification Techniques</h4>\n<p>Mathematical verification ensures implementation correctness by comparing results against known analytical solutions or established numerical methods.</p>\n<p><strong>Gradient Checking with Finite Differences:</strong> The most reliable method for validating gradient computations involves comparing analytical gradients to numerical approximations using finite differences. For each parameter, compute <code>(cost(θ + ε) - cost(θ - ε)) / (2ε)</code> with small ε (typically 1e-5) and compare to the analytical gradient. Differences larger than 1e-7 indicate gradient computation errors.</p>\n<p><strong>Closed-Form Solution Validation:</strong> For simple linear regression, compare gradient descent results to the analytical solution computed using normal equations. Parameters should match within numerical precision (typically 1e-10). Significant differences indicate either gradient descent implementation errors or convergence issues.</p>\n<p><strong>Synthetic Data Testing:</strong> Generate synthetic datasets with known parameters, train the model, and verify that recovered parameters match the ground truth. This approach isolates algorithmic issues from data quality problems and provides definitive validation of implementation correctness.</p>\n<p><strong>Invariant Checking:</strong> Verify mathematical invariants during training. For example, the cost function should never increase in properly implemented gradient descent with appropriate learning rates. R-squared values should remain between 0 and 1 for well-specified models. Violations of these invariants indicate fundamental implementation errors.</p>\n<h4 id=\"logging-and-inspection-strategies\">Logging and Inspection Strategies</h4>\n<p>Comprehensive logging enables post-mortem analysis of training failures and provides insights into algorithm behavior during successful runs.</p>\n<p><strong>Multi-Level Logging Architecture:</strong> Implement logging at multiple granularities: summary statistics (final parameters, convergence metrics), iteration-level metrics (cost, gradient norms, parameter updates), and detailed debugging information (intermediate calculations, array shapes, numerical stability checks). This hierarchical approach provides overview information for normal operation and detailed diagnostics for debugging.</p>\n<p><strong>Checkpoint-Based Debugging:</strong> Save complete model state at regular intervals during training, including parameters, gradients, cost values, and intermediate calculations. This enables analysis of training progression and identification of the exact iteration where problems begin. Checkpoints also support training resumption after debugging fixes.</p>\n<p><strong>Interactive Debugging Sessions:</strong> Design code to support step-by-step execution during training iterations. Implement debugging hooks that allow inspection of variables, plotting of intermediate results, and modification of parameters without restarting training. This approach enables real-time diagnosis of convergence problems.</p>\n<p><strong>Performance Profiling Integration:</strong> Combine algorithmic debugging with performance profiling to identify computational bottlenecks. Slow convergence might result from inefficient implementations rather than algorithmic issues. Profile matrix operations, gradient computations, and convergence checking to ensure optimal performance.</p>\n<h3 id=\"performance-and-convergence-debugging\">Performance and Convergence Debugging</h3>\n<p>Performance and convergence issues represent the most challenging aspects of machine learning debugging because they often result from subtle interactions between data characteristics, algorithmic parameters, and numerical precision considerations.</p>\n<h4 id=\"convergence-diagnosis-framework\">Convergence Diagnosis Framework</h4>\n<p><strong>Multi-Criteria Convergence Analysis:</strong> Implement comprehensive convergence detection using multiple criteria simultaneously. Cost-based convergence (relative cost change below threshold) catches optimization completion. Gradient-based convergence (gradient magnitude below threshold) detects mathematical optimality. Parameter-based convergence (parameter change below threshold) identifies stability. Analyzing which criteria trigger convergence provides insights into optimization dynamics.</p>\n<p><strong>Convergence Quality Assessment:</strong> Not all convergence represents successful optimization. Implement convergence quality metrics that evaluate whether the achieved solution represents a global optimum, local optimum, or saddle point. Compare final cost to theoretical minimum (when available), analyze gradient characteristics at convergence, and verify solution stability through parameter perturbation testing.</p>\n<p><strong>Learning Rate Adaptation Strategies:</strong> Implement adaptive learning rate mechanisms that respond to convergence characteristics. If cost oscillates, reduce learning rate. If convergence is extremely slow, increase learning rate. If gradients become too small, adjust precision or modify convergence criteria. Systematic learning rate adaptation prevents many convergence problems while maintaining training efficiency.</p>\n<p><strong>Convergence Failure Recovery:</strong> Design recovery mechanisms for common convergence failures. Gradient explosion triggers learning rate reduction and parameter reinitialization. Premature convergence triggers tolerance tightening and training continuation. Numerical instability triggers precision increase and parameter clipping. These automatic recovery mechanisms enable robust training without manual intervention.</p>\n<h4 id=\"numerical-stability-monitoring\">Numerical Stability Monitoring</h4>\n<p><strong>Real-Time Stability Assessment:</strong> Implement continuous monitoring of numerical stability indicators during training. Track parameter magnitudes, gradient norms, condition numbers, and numerical precision metrics at each iteration. Early detection of stability problems enables preventive action before catastrophic failures occur.</p>\n<p><strong>Overflow and Underflow Protection:</strong> Design comprehensive protection against numerical overflow and underflow. Implement parameter clipping that constrains values to representable ranges. Use numerically stable formulations for mathematical operations. Detect and handle special values (inf, NaN) before they propagate through calculations. These protections ensure training can continue even under adverse numerical conditions.</p>\n<p><strong>Matrix Conditioning Analysis:</strong> For multiple linear regression, monitor design matrix conditioning throughout training. Compute condition numbers and detect near-singularity conditions that cause numerical instability. Implement automatic regularization or feature removal when conditioning becomes problematic. Poor matrix conditioning is a common cause of unstable solutions in multiple regression.</p>\n<p><strong>Precision Management Strategies:</strong> Implement dynamic precision management that adapts to problem requirements. Use double precision when numerical stability is critical. Monitor precision loss during calculations and upgrade precision when necessary. Balance computational efficiency with numerical accuracy based on convergence requirements and stability analysis.</p>\n<h4 id=\"performance-optimization-debugging\">Performance Optimization Debugging</h4>\n<p><strong>Computational Bottleneck Identification:</strong> Profile training performance to identify computational bottlenecks that cause slow training. Matrix operations, gradient computations, and convergence checking each have different optimization requirements. Use profiling results to guide optimization efforts and ensure training efficiency scales appropriately with problem size.</p>\n<p><strong>Memory Usage Analysis:</strong> Monitor memory consumption during training, especially for large datasets or multiple regression with many features. Identify memory leaks, excessive array copying, and inefficient data structures. Memory pressure can cause performance degradation and numerical instability, making memory analysis essential for robust implementations.</p>\n<p><strong>Algorithm Complexity Verification:</strong> Verify that implementation complexity matches theoretical expectations. Simple linear regression should scale linearly with sample count. Multiple regression should scale with the cube of feature count due to matrix operations. Suboptimal scaling indicates algorithmic inefficiencies that affect training performance.</p>\n<p><strong>Vectorization Optimization:</strong> Ensure maximum utilization of vectorized operations using NumPy. Replace loops with matrix operations wherever possible. Profile vector operations to verify they achieve expected performance gains. Poor vectorization is a common cause of slow training in educational implementations.</p>\n<p><img src=\"/api/project/linear-regression/architecture-doc/asset?path=diagrams%2Ferror-handling-flow.svg\" alt=\"Error Detection and Recovery Flow\"></p>\n<h4 id=\"common-debugging-scenarios-and-solutions\">Common Debugging Scenarios and Solutions</h4>\n<p><strong>Scenario 1: Model Fits Training Data Perfectly but Fails on New Data</strong></p>\n<p>This scenario typically indicates overfitting or data leakage rather than implementation bugs. Diagnostic steps include verifying proper train-test splits, checking for feature normalization leakage, and analyzing model complexity relative to dataset size. Solutions involve implementing proper cross-validation, fixing normalization procedures, and adding regularization if appropriate.</p>\n<p><strong>Scenario 2: Gradient Descent Converges to Different Solutions on Repeated Runs</strong></p>\n<p>This scenario suggests numerical instability or poor conditioning rather than algorithm randomness. Linear regression has a unique global optimum, so different solutions indicate implementation problems. Diagnostic approaches include checking matrix conditioning, verifying gradient computations, and analyzing learning rate appropriateness. Solutions involve adding regularization, improving numerical precision, or using closed-form solutions for comparison.</p>\n<p><strong>Scenario 3: Training Appears Successful but Predictions Are Nonsensical</strong></p>\n<p>This scenario often results from unit mismatches, scaling issues, or incorrect prediction implementation. Diagnostic steps include verifying feature scaling consistency between training and prediction, checking prediction function implementation, and analyzing prediction magnitudes relative to training targets. Solutions involve fixing feature scaling, correcting prediction logic, or debugging data preprocessing pipelines.</p>\n<p><strong>Scenario 4: Algorithm Runs Without Errors but Produces Poor Results</strong></p>\n<p>This scenario requires systematic validation of each component. Start with data quality analysis, verify mathematical implementations against analytical solutions, check parameter initialization, and analyze convergence quality. The problem often lies in subtle implementation details rather than obvious bugs.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides concrete tools and techniques for implementing robust debugging capabilities in your linear regression system. The focus is on practical debugging infrastructure that can be integrated throughout development and used for ongoing diagnosis.</p>\n<h4 id=\"technology-recommendations-for-debugging\">Technology Recommendations for Debugging</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Visualization</td>\n<td><code>matplotlib</code> with basic plotting functions</td>\n<td><code>matplotlib</code> + <code>seaborn</code> with custom debugging dashboards</td>\n</tr>\n<tr>\n<td>Logging</td>\n<td>Python <code>logging</code> module with file output</td>\n<td><code>logging</code> + <code>wandb</code> or <code>tensorboard</code> for experiment tracking</td>\n</tr>\n<tr>\n<td>Numerical Validation</td>\n<td>Manual assertion checking</td>\n<td><code>numpy.testing</code> module with comprehensive test suites</td>\n</tr>\n<tr>\n<td>Profiling</td>\n<td>Built-in <code>time.time()</code> measurements</td>\n<td><code>cProfile</code> + <code>line_profiler</code> for detailed performance analysis</td>\n</tr>\n<tr>\n<td>Interactive Debugging</td>\n<td>Print statements and manual inspection</td>\n<td><code>ipdb</code> or <code>pdb</code> with breakpoints and variable inspection</td>\n</tr>\n</tbody></table>\n<h4 id=\"debugging-infrastructure-starter-code\">Debugging Infrastructure Starter Code</h4>\n<p><strong>Debugging Logger Configuration:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> matplotlib.pyplot </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> plt</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Any, Optional, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> os</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DebugConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for debugging and logging behavior.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    log_level: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"INFO\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    save_plots: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plot_dir: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"debug_plots\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    log_file: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"training_debug.log\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    checkpoint_interval: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    enable_convergence_plots: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    enable_gradient_checking: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    numerical_precision_threshold: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-10</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MLDebugger</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Comprehensive debugging utilities for machine learning implementations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: DebugConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.setup_logging()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.training_history </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.gradient_history </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.parameter_history </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> setup_logging</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Configure logging system for debugging output.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logging.basicConfig(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            level</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">getattr</span><span style=\"color:#E1E4E8\">(logging, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.config.log_level),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            format</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">%(asctime)s</span><span style=\"color:#9ECBFF\"> - </span><span style=\"color:#79B8FF\">%(levelname)s</span><span style=\"color:#9ECBFF\"> - </span><span style=\"color:#79B8FF\">%(funcName)s</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">%(lineno)d</span><span style=\"color:#9ECBFF\"> - </span><span style=\"color:#79B8FF\">%(message)s</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            handlers</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                logging.FileHandler(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.config.log_file),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                logging.StreamHandler()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.save_plots:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            os.makedirs(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.config.plot_dir, </span><span style=\"color:#FFAB70\">exist_ok</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> log_training_iteration</span><span style=\"color:#E1E4E8\">(self, iteration: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, cost: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                             parameters: np.ndarray, gradients: np.ndarray):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Log detailed information for each training iteration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store iteration data in training_history</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check for numerical instabilities in parameters and gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Log warnings if parameters or gradients exceed safe ranges</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store gradient norms and parameter norms for analysis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> check_gradient_correctness</span><span style=\"color:#E1E4E8\">(self, compute_cost_func, compute_gradient_func,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                 parameters: np.ndarray, X: np.ndarray, y: np.ndarray,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                 epsilon: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-5</span><span style=\"color:#E1E4E8\">) -> Tuple[</span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate gradient computation using finite differences.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement finite difference gradient checking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Compare analytical gradients to numerical approximations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return boolean indicating correctness and detailed error metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Log detailed comparison results for debugging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> plot_convergence_diagnostics</span><span style=\"color:#E1E4E8\">(self, cost_history: List[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                   parameter_history: List[np.ndarray],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                   save_path: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate comprehensive convergence diagnostic plots.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create subplots for cost history, parameter evolution, and gradient norms</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Plot cost reduction rate and detect convergence patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Show parameter trajectories and stability analysis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Save plots to debug directory if enabled</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> diagnose_convergence_issues</span><span style=\"color:#E1E4E8\">(self, cost_history: List[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                  gradient_history: List[np.ndarray]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Analyze convergence behavior and identify common problems.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Detect oscillating costs indicating learning rate issues</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Identify premature convergence or stagnation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check for gradient explosion or vanishing gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return diagnostic report with specific recommendations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_data_quality</span><span style=\"color:#E1E4E8\">(self, X: np.ndarray, y: np.ndarray) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Comprehensive data quality validation and reporting.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check for missing values, infinite values, and data type issues</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Analyze feature scales and distributions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Detect constant features and highly correlated features</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate array shapes and compatibility</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return detailed data quality report</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"core-debugging-function-skeletons\">Core Debugging Function Skeletons</h4>\n<p><strong>Gradient Validation Implementation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_gradients_with_finite_differences</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    model,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    X: np.ndarray,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    y: np.ndarray,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    epsilon: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-5</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tolerance: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-7</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">) -> Tuple[</span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Validate gradient computation using finite difference approximation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This function computes numerical gradients using finite differences and</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    compares them to analytical gradients from the model implementation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get current model parameters (slope, intercept or weight vector)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Compute analytical gradients using model's gradient function</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: For each parameter, compute numerical gradient using finite differences:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #         grad_numerical[i] = (cost(params + epsilon*e_i) - cost(params - epsilon*e_i)) / (2*epsilon)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #         where e_i is unit vector in direction i</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Compare analytical and numerical gradients element-wise</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Compute relative and absolute errors for each parameter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return validation result and detailed error metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Use np.abs(analytical - numerical) / (np.abs(analytical) + 1e-8) for relative error</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> diagnose_learning_rate_issues</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cost_history: List[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parameter_history: List[np.ndarray],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    current_learning_rate: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">) -> Tuple[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Analyze training behavior to diagnose learning rate problems.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns diagnosis type, suggested learning rate, and explanation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if cost is increasing - indicates learning rate too high</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check for oscillating costs - indicates learning rate slightly too high</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check for extremely slow convergence - indicates learning rate too low</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Analyze parameter update magnitudes relative to parameter values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return diagnosis type ('too_high', 'too_low', 'oscillating', 'appropriate')</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Suggest new learning rate based on diagnosis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Provide detailed explanation of the diagnosis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Cost increasing for multiple iterations definitely means learning rate too high</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> detect_numerical_instabilities</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parameters: np.ndarray,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gradients: np.ndarray,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cost: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    iteration: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">) -> Tuple[List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Real-time detection of numerical stability problems during training.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns lists of detected issues and suggested fixes.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check for inf or NaN values in parameters, gradients, and cost</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Detect parameter overflow (values exceeding MAX_PARAMETER_VALUE)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Detect gradient explosion (gradient norms exceeding MAX_GRADIENT_NORM)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Detect gradient underflow (gradient norms below MIN_GRADIENT_NORM)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Check for loss of numerical precision in cost calculation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Compile list of detected issues with descriptive names</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Generate specific fix recommendations for each detected issue</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Use np.linalg.norm() to compute gradient magnitudes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"comprehensive-testing-and-validation-framework\">Comprehensive Testing and Validation Framework</h4>\n<p><strong>Milestone Checkpoint Validation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_milestone_1_implementation</span><span style=\"color:#E1E4E8\">(model_class):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Comprehensive validation for Milestone 1: Simple Linear Regression.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Tests closed-form solution correctness and basic prediction capability.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"=== Milestone 1 Validation: Simple Linear Regression ===\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create synthetic linear data with known parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Fit model using closed-form solution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Verify recovered parameters match ground truth within tolerance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Test prediction accuracy on new data points</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate R-squared calculation correctness</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Test edge cases (constant features, single data point, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Report detailed validation results and success/failure status</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Expected output: \"✓ All Milestone 1 tests passed\" or specific failure details</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_milestone_2_implementation</span><span style=\"color:#E1E4E8\">(model_class):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Comprehensive validation for Milestone 2: Gradient Descent.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Tests iterative optimization convergence and gradient computation correctness.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"=== Milestone 2 Validation: Gradient Descent ===\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Test gradient computation using finite difference validation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Verify convergence to same solution as closed-form method</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Test different learning rates and validate convergence behavior</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify cost function decreases monotonically</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Test convergence detection triggers appropriately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Validate training history tracking and parameter evolution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Report convergence quality and optimization effectiveness</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Expected behavior: Should converge to within 1e-6 of analytical solution</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_milestone_3_implementation</span><span style=\"color:#E1E4E8\">(model_class):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Comprehensive validation for Milestone 3: Multiple Linear Regression.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Tests matrix operations, feature scaling, and regularization.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"=== Milestone 3 Validation: Multiple Linear Regression ===\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Test design matrix construction with multiple features</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate feature normalization preserves relationships</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Test vectorized gradient descent on multi-dimensional problems</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify regularization affects overfitting appropriately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Test matrix conditioning and numerical stability</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Compare results to sklearn LinearRegression for validation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Test scalability with increasing feature counts</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Expected behavior: Should match sklearn results within numerical precision</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"debugging-command-line-interface\">Debugging Command-Line Interface</h4>\n<p><strong>Interactive Debugging Session:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> debug_training_session</span><span style=\"color:#E1E4E8\">(model, X, y, debug_config: DebugConfig):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Interactive debugging session for training problems.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Provides step-by-step diagnosis and fixes for common training issues.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    debugger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MLDebugger(debug_config)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Starting interactive debugging session...\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Available commands: 'data', 'gradients', 'convergence', 'parameters', 'quit'\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    while</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        command </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> input</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Debug> \"</span><span style=\"color:#E1E4E8\">).strip().lower()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> command </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'data'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Run comprehensive data quality analysis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Display feature distributions, correlation matrix, missing values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Recommend preprocessing steps if issues detected</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> command </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'gradients'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Perform gradient checking with finite differences</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Display gradient correctness results and error analysis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Suggest fixes if gradient computation errors detected</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> command </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'convergence'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Analyze current training progress and convergence quality</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate convergence diagnostic plots</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Recommend learning rate or tolerance adjustments</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> command </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'parameters'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Display current parameter values and evolution history</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check for parameter instabilities or unexpected behavior</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Compare to expected parameter ranges or theoretical values</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> command </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'quit'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            break</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Unknown command. Available: 'data', 'gradients', 'convergence', 'parameters', 'quit'\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<h4 id=\"language-specific-debugging-hints\">Language-Specific Debugging Hints</h4>\n<p><strong>Python/NumPy Specific Techniques:</strong></p>\n<ul>\n<li>Use <code>np.seterr(all=&#39;raise&#39;)</code> during development to catch numerical issues immediately rather than allowing NaN propagation</li>\n<li>Leverage <code>np.testing.assert_array_almost_equal()</code> for robust numerical comparisons that account for floating-point precision</li>\n<li>Use <code>warnings.filterwarnings(&#39;error&#39;)</code> to convert NumPy warnings into exceptions for debugging</li>\n<li>Implement custom <code>__repr__</code> methods for model classes to provide detailed state information during debugging</li>\n<li>Use <code>np.savez()</code> to save complete debugging sessions including arrays, parameters, and metadata for later analysis</li>\n</ul>\n<p><strong>Performance Debugging:</strong></p>\n<ul>\n<li>Use <code>%timeit</code> in Jupyter notebooks to profile individual operations and identify bottlenecks</li>\n<li>Implement timing decorators that automatically log execution times for critical functions</li>\n<li>Use <code>memory_profiler</code> to track memory usage and detect memory leaks during long training sessions</li>\n<li>Profile matrix operations separately to ensure they achieve expected O(n³) scaling for multiple regression</li>\n</ul>\n<h4 id=\"milestone-debugging-checkpoints\">Milestone Debugging Checkpoints</h4>\n<p><strong>After Milestone 1 (Simple Linear Regression):</strong></p>\n<ul>\n<li>Run: <code>python -m pytest tests/test_simple_regression.py -v</code></li>\n<li>Expected: All tests pass, recovered slope/intercept match analytical solution within 1e-10</li>\n<li>Manual verification: Plot fitted line against data points, visually verify good fit</li>\n<li>Debug signs: If parameters don&#39;t match analytical solution, check closed-form implementation</li>\n</ul>\n<p><strong>After Milestone 2 (Gradient Descent):</strong></p>\n<ul>\n<li>Run: <code>python debug_gradient_descent.py --validate-gradients</code></li>\n<li>Expected: Gradient checking passes, convergence achieved within 1000 iterations</li>\n<li>Manual verification: Cost should decrease monotonically, final parameters match Milestone 1 results</li>\n<li>Debug signs: If cost oscillates, reduce learning rate; if convergence is slow, increase learning rate</li>\n</ul>\n<p><strong>After Milestone 3 (Multiple Linear Regression):</strong></p>\n<ul>\n<li>Run: <code>python validate_multiple_regression.py --compare-sklearn</code></li>\n<li>Expected: Results match sklearn LinearRegression within 1e-8, proper scaling handling</li>\n<li>Manual verification: Feature normalization should result in zero mean, unit variance</li>\n<li>Debug signs: If results differ from sklearn, check design matrix construction and feature scaling</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Debugging in Isolation</strong>\nMany learners debug components in isolation without considering system-wide interactions. Always test the complete pipeline from data loading through prediction, as bugs often emerge from component interfaces and data flow between modules.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Numerical Precision</strong>\nFloating-point precision issues cause many subtle bugs in machine learning implementations. Always use appropriate tolerances for comparisons and monitor for precision loss during iterative algorithms.</p>\n<p>⚠️ <strong>Pitfall: Over-Relying on Print Statements</strong>\nWhile print statements help with initial debugging, implement comprehensive logging and visualization capabilities for complex debugging scenarios. Print statements become insufficient for optimization problems that require understanding algorithm behavior over many iterations.</p>\n<h2 id=\"future-extensions-and-learning-path\">Future Extensions and Learning Path</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (foundation for continued learning beyond M3: Multiple Linear Regression)</p>\n</blockquote>\n<p>This section outlines the natural progression from our foundational linear regression implementation toward more advanced machine learning concepts. The architecture and design patterns established in our three milestones create a solid foundation for extending into polynomial regression, cross-validation, regularization variants, and eventually more complex algorithms like logistic regression and neural networks.</p>\n<h3 id=\"mental-model-the-learning-journey-map\">Mental Model: The Learning Journey Map</h3>\n<p>Think of our linear regression implementation as the <strong>foundational base camp</strong> in a mountain climbing expedition toward machine learning mastery. Just as mountaineers establish a well-equipped base camp with proper supplies, communication systems, and safety protocols before attempting higher peaks, our implementation provides the essential tools and patterns needed for more advanced algorithms.</p>\n<p>The base camp (our current implementation) has established critical infrastructure: data handling pipelines, numerical optimization routines, parameter management systems, and debugging tools. From this foundation, we can launch expeditions to nearby peaks (polynomial features, regularization variants) or prepare for more challenging ascents (logistic regression, neural networks). Each extension builds upon the same core principles but adapts the techniques to handle new types of mathematical relationships and optimization challenges.</p>\n<p>The key insight is that <strong>machine learning algorithms share common patterns</strong>: they all involve data preprocessing, parameter optimization, prediction generation, and model evaluation. By mastering these patterns in the linear regression context, learners develop transferable skills that accelerate learning of more complex algorithms.</p>\n<h3 id=\"immediate-extensions\">Immediate Extensions</h3>\n<p>These extensions represent natural next steps that build directly upon our existing architecture without requiring fundamental changes to the core design patterns. Each extension introduces one or two new concepts while leveraging the solid foundation we&#39;ve established.</p>\n<h4 id=\"polynomial-features-and-non-linear-relationships\">Polynomial Features and Non-Linear Relationships</h4>\n<p>Our current implementation handles linear relationships through the equation <code>y = w₁x₁ + w₂x₂ + ... + wₙxₙ + b</code>. The most natural extension involves <strong>polynomial feature engineering</strong>, which transforms linear models to capture non-linear patterns by creating new features based on powers and interactions of existing features.</p>\n<p>The mathematical foundation involves expanding our feature space through polynomial basis functions. For a single input feature <code>x</code>, we can create polynomial features <code>[1, x, x², x³, ...]</code> up to some degree <code>d</code>. For multiple input features, we add interaction terms like <code>x₁x₂</code>, <code>x₁²x₂</code>, etc. This transforms a non-linear problem in the original feature space into a linear problem in the expanded polynomial feature space.</p>\n<p><strong>Implementation Architecture for Polynomial Features:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Current Responsibility</th>\n<th>Extended Responsibility</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>DataHandler</code></td>\n<td>Load and normalize features</td>\n<td>Add polynomial feature generation and interaction terms</td>\n</tr>\n<tr>\n<td><code>MultipleLinearRegression</code></td>\n<td>Fit linear combinations of features</td>\n<td>Unchanged - still fits linear model, but on expanded features</td>\n</tr>\n<tr>\n<td><code>StandardScaler</code></td>\n<td>Normalize individual features</td>\n<td>Handle scaling of polynomial features with different magnitude ranges</td>\n</tr>\n</tbody></table>\n<p>The architectural beauty lies in <strong>reusing our existing linear regression engine unchanged</strong>. The <code>MultipleLinearRegression</code> component continues to solve the same linear optimization problem, but now operates on an expanded feature matrix where polynomial relationships appear as linear relationships.</p>\n<blockquote>\n<p><strong>Decision: Polynomial Feature Generation Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to handle non-linear relationships while preserving our linear regression architecture</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Modify core regression algorithm to handle non-linear terms directly</li>\n<li>Create polynomial features as preprocessing step before existing linear regression</li>\n<li>Build separate polynomial regression class from scratch</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Preprocess data to create polynomial features, then use existing linear regression</li>\n<li><strong>Rationale</strong>: Preserves existing architecture, demonstrates feature engineering concepts, maintains mathematical simplicity</li>\n<li><strong>Consequences</strong>: Enables non-linear modeling without code changes to core algorithm, but increases feature dimensionality and risk of overfitting</li>\n</ul>\n</blockquote>\n<p><strong>Polynomial Feature Generation Process:</strong></p>\n<ol>\n<li><strong>Degree Selection</strong>: Choose maximum polynomial degree <code>d</code> based on problem complexity and available data</li>\n<li><strong>Feature Expansion</strong>: For each original feature, generate powers up to degree <code>d</code></li>\n<li><strong>Interaction Terms</strong>: Create cross-products between different original features up to total degree <code>d</code></li>\n<li><strong>Feature Naming</strong>: Maintain interpretability by naming generated features like <code>x1^2</code>, <code>x1*x2</code>, etc.</li>\n<li><strong>Scaling Considerations</strong>: Apply normalization after polynomial generation since <code>x²</code> has very different scale than <code>x</code></li>\n</ol>\n<p>The implementation extends our <code>DataHandler</code> with a <code>PolynomialFeatureGenerator</code> that transforms the original feature matrix into an expanded polynomial feature matrix. This generator needs to handle the combinatorial explosion of features carefully - for <code>n</code> original features and degree <code>d</code>, we can generate up to <code>C(n+d,d)</code> polynomial features.</p>\n<p><strong>Cross-Validation for Model Selection</strong></p>\n<p>Cross-validation represents the next critical concept for preventing overfitting, especially important when working with polynomial features that can easily memorize training data. Our implementation establishes the foundation for cross-validation through our existing train-test evaluation patterns.</p>\n<p>The <strong>k-fold cross-validation</strong> algorithm involves partitioning the dataset into <code>k</code> equal-sized subsets, then training <code>k</code> different models where each model uses <code>k-1</code> subsets for training and the remaining subset for validation. This provides a more robust estimate of model performance than a single train-test split.</p>\n<p><strong>Cross-Validation Architecture Integration:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>New Responsibility</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>DataHandler</code></td>\n<td>Add <code>create_cv_splits(k)</code> method to generate k-fold partitions</td>\n</tr>\n<tr>\n<td><code>CrossValidator</code></td>\n<td>New component to orchestrate training multiple models and aggregate results</td>\n</tr>\n<tr>\n<td><code>ModelSelection</code></td>\n<td>New component to compare different configurations (polynomial degrees, regularization strengths)</td>\n</tr>\n</tbody></table>\n<p>The cross-validation process integrates naturally with our existing training pipeline. Each fold reuses our established data loading, preprocessing, model fitting, and evaluation components. The <code>CrossValidator</code> component acts as an orchestrator that manages multiple training sessions and aggregates performance metrics.</p>\n<blockquote>\n<p><strong>Key Design Insight</strong>: Cross-validation doesn&#39;t require changes to our core regression algorithms. Instead, it&#39;s a <strong>meta-algorithm</strong> that uses our existing training and evaluation infrastructure multiple times with different data partitions.</p>\n</blockquote>\n<p><strong>Implementation Steps for Cross-Validation:</strong></p>\n<ol>\n<li><strong>Data Partitioning</strong>: Split dataset into <code>k</code> approximately equal-sized folds, preserving target variable distribution</li>\n<li><strong>Fold Training Loop</strong>: For each fold <code>i</code>, train model on folds <code>{1...k} - {i}</code> and evaluate on fold <code>i</code></li>\n<li><strong>Metric Aggregation</strong>: Collect performance metrics (R-squared, MSE) from each fold and compute mean and standard deviation</li>\n<li><strong>Model Selection</strong>: Compare aggregated metrics across different model configurations</li>\n<li><strong>Final Training</strong>: Train final model on entire dataset using best configuration found via cross-validation</li>\n</ol>\n<h4 id=\"regularization-variants-and-robust-linear-models\">Regularization Variants and Robust Linear Models</h4>\n<p>Our current implementation includes basic L2 regularization (Ridge regression) in the <code>MultipleLinearRegression</code> component. The natural extensions involve <strong>L1 regularization (Lasso)</strong> and <strong>Elastic Net</strong> (combined L1 and L2), each addressing different aspects of model complexity and feature selection.</p>\n<p><strong>Mathematical Foundation of Regularization Extensions:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Regularization Type</th>\n<th>Cost Function</th>\n<th>Key Property</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Ridge (L2)</td>\n<td><code>MSE + α∑w²</code></td>\n<td>Shrinks coefficients toward zero, keeps all features</td>\n</tr>\n<tr>\n<td>Lasso (L1)</td>\n<td>`MSE + α∑</td>\n<td>w</td>\n</tr>\n<tr>\n<td>Elastic Net</td>\n<td>`MSE + α₁∑</td>\n<td>w</td>\n</tr>\n</tbody></table>\n<p>The architectural challenge lies in adapting our gradient descent optimizer to handle the non-differentiable L1 penalty. While L2 regularization has a smooth derivative <code>2αw</code>, L1 regularization has a non-smooth derivative that requires <strong>subgradient methods</strong> or <strong>proximal gradient descent</strong>.</p>\n<p><strong>Regularization Architecture Extensions:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Current Implementation</th>\n<th>L1/Elastic Net Extension</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>MultipleLinearRegression</code></td>\n<td>Handles L2 penalty in cost and gradient</td>\n<td>Add regularization type parameter and different penalty calculations</td>\n</tr>\n<tr>\n<td><code>_compute_gradients()</code></td>\n<td>Simple gradient with L2 penalty</td>\n<td>Conditional logic for L1 subgradients or proximal operators</td>\n</tr>\n<tr>\n<td><code>TrainingHistory</code></td>\n<td>Tracks cost evolution</td>\n<td>Add regularization path tracking to see feature selection process</td>\n</tr>\n</tbody></table>\n<p>The implementation requires careful handling of the L1 penalty&#39;s non-differentiability at zero. Common approaches include <strong>soft thresholding</strong> in proximal gradient methods or <strong>coordinate descent</strong> algorithms that optimize one parameter at a time while holding others fixed.</p>\n<blockquote>\n<p><strong>Decision: Regularization Implementation Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Need L1 regularization for feature selection while maintaining gradient descent framework</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Implement proximal gradient descent with soft thresholding operators</li>\n<li>Switch to coordinate descent algorithm specifically for Lasso</li>\n<li>Use subgradient methods with step size scheduling</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement proximal gradient descent with soft thresholding for L1, keep existing gradient descent for L2</li>\n<li><strong>Rationale</strong>: Proximal methods generalize our existing gradient descent while handling non-smooth penalties elegantly</li>\n<li><strong>Consequences</strong>: Enables feature selection capabilities while preserving familiar optimization framework, but adds complexity to parameter update logic</li>\n</ul>\n</blockquote>\n<h3 id=\"advanced-topics-path\">Advanced Topics Path</h3>\n<p>The foundation established through our linear regression implementation creates natural pathways toward more sophisticated machine learning algorithms. Each advanced topic builds upon core concepts we&#39;ve mastered while introducing new mathematical frameworks and computational challenges.</p>\n<h4 id=\"logistic-regression-and-classification\">Logistic Regression and Classification</h4>\n<p><strong>Logistic regression</strong> represents the most direct extension from linear regression, transitioning from predicting continuous values to predicting class probabilities. The mathematical connection involves replacing the linear prediction <code>y = Xw + b</code> with a probability prediction <code>p = sigmoid(Xw + b)</code>, where the sigmoid function <code>σ(z) = 1/(1 + e^(-z))</code> maps any real number to the interval [0,1].</p>\n<p><strong>Conceptual Bridge from Linear to Logistic Regression:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>Linear Regression</th>\n<th>Logistic Regression</th>\n<th>Architectural Similarity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Prediction Function</strong></td>\n<td><code>y = Xw + b</code></td>\n<td><code>p = sigmoid(Xw + b)</code></td>\n<td>Same linear combination, different output transformation</td>\n</tr>\n<tr>\n<td><strong>Cost Function</strong></td>\n<td>Mean Squared Error</td>\n<td>Cross-Entropy Loss</td>\n<td>Different loss function, same optimization framework</td>\n</tr>\n<tr>\n<td><strong>Optimization</strong></td>\n<td>Gradient Descent</td>\n<td>Gradient Descent</td>\n<td>Identical optimization algorithm and convergence detection</td>\n</tr>\n<tr>\n<td><strong>Parameter Updates</strong></td>\n<td><code>w -= lr * ∇MSE</code></td>\n<td><code>w -= lr * ∇CrossEntropy</code></td>\n<td>Same update rule, different gradient computation</td>\n</tr>\n</tbody></table>\n<p>The architectural transition involves <strong>minimal changes to our existing components</strong>. The <code>DataHandler</code> requires binary target encoding, the cost function changes from MSE to cross-entropy, and gradient computation adapts to the new loss function. However, the core optimization loop, convergence detection, parameter management, and evaluation infrastructure remain largely unchanged.</p>\n<p><strong>Implementation Pathway for Logistic Regression:</strong></p>\n<ol>\n<li><strong>Target Encoding</strong>: Extend <code>DataHandler</code> to handle binary classification targets (0/1 encoding)</li>\n<li><strong>Sigmoid Activation</strong>: Add activation functions that transform linear outputs to probabilities</li>\n<li><strong>Cross-Entropy Cost</strong>: Replace MSE cost function with log-likelihood maximization</li>\n<li><strong>Gradient Adaptation</strong>: Modify gradient computation for logistic cost function</li>\n<li><strong>Classification Metrics</strong>: Extend evaluation beyond R-squared to accuracy, precision, recall</li>\n</ol>\n<p>The mathematical foundation becomes <code>Cost = -∑[y*log(p) + (1-y)*log(1-p)]</code>, where <code>p = sigmoid(Xw + b)</code>. The gradient computation yields <code>∇w = (1/m) * X^T * (p - y)</code>, which has the same computational pattern as linear regression gradients but derives from a different loss function.</p>\n<blockquote>\n<p><strong>Learning Progression Insight</strong>: Logistic regression demonstrates that many machine learning algorithms share the same <strong>optimization backbone</strong> (gradient descent) but differ in their <strong>mathematical modeling assumptions</strong> (linear vs. probabilistic relationships).</p>\n</blockquote>\n<h4 id=\"neural-networks-and-deep-learning-foundations\">Neural Networks and Deep Learning Foundations</h4>\n<p>Our linear regression implementation establishes several foundational concepts that transfer directly to neural networks: <strong>matrix operations</strong>, <strong>gradient computation</strong>, <strong>iterative optimization</strong>, and <strong>parameter management</strong>. A neural network can be understood as a composition of multiple linear transformations with non-linear activation functions.</p>\n<p><strong>Architectural Concepts Bridge:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Linear Regression Component</th>\n<th>Neural Network Equivalent</th>\n<th>Conceptual Connection</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>weights_</code> parameter vector</td>\n<td>Weight matrices for each layer</td>\n<td>Parameters to be optimized</td>\n</tr>\n<tr>\n<td><code>_compute_gradients()</code> method</td>\n<td>Backpropagation algorithm</td>\n<td>Gradient computation for optimization</td>\n</tr>\n<tr>\n<td><code>fit()</code> training loop</td>\n<td>Training epoch iterations</td>\n<td>Iterative parameter updates</td>\n</tr>\n<tr>\n<td><code>predict()</code> method</td>\n<td>Forward propagation</td>\n<td>Input-to-output transformation</td>\n</tr>\n<tr>\n<td><code>StandardScaler</code> normalization</td>\n<td>Input normalization layers</td>\n<td>Data preprocessing for stable training</td>\n</tr>\n</tbody></table>\n<p>A <strong>single-layer neural network</strong> (perceptron) is mathematically identical to our linear regression: <code>output = activation(Xw + b)</code>. Multi-layer networks compose multiple such transformations: <code>output = f₃(f₂(f₁(X)))</code>, where each <code>fᵢ</code> represents a linear transformation followed by a non-linear activation.</p>\n<p>The <strong>backpropagation algorithm</strong> extends our gradient computation using the chain rule. Instead of computing gradients for a single set of parameters, backpropagation computes gradients for parameters in each layer by propagating error derivatives backward through the network.</p>\n<p><strong>Neural Network Implementation Pathway:</strong></p>\n<ol>\n<li><strong>Layer Abstraction</strong>: Extend our linear transformation to a <code>Layer</code> class with forward and backward methods</li>\n<li><strong>Activation Functions</strong>: Add non-linear transformations (ReLU, sigmoid, tanh) after linear layers</li>\n<li><strong>Network Composition</strong>: Chain multiple layers with automatic gradient flow between them</li>\n<li><strong>Backpropagation</strong>: Generalize our gradient computation to handle multi-layer parameter updates</li>\n<li><strong>Advanced Optimizers</strong>: Extend beyond basic gradient descent to momentum, Adam, etc.</li>\n</ol>\n<p>The implementation builds upon our established patterns. Each layer maintains its own <code>weights_</code> and provides <code>forward()</code> and <code>backward()</code> methods. The <code>Network</code> class orchestrates the forward pass (prediction) and backward pass (gradient computation) across all layers, similar to how our current <code>fit()</code> method orchestrates training steps.</p>\n<h4 id=\"advanced-optimization-algorithms\">Advanced Optimization Algorithms</h4>\n<p>Our basic gradient descent implementation with fixed learning rates provides the foundation for more sophisticated optimization algorithms that adapt learning rates, incorporate momentum, or use second-order information.</p>\n<p><strong>Optimization Algorithm Progression:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th>Key Innovation</th>\n<th>Implementation Extension</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>SGD with Momentum</strong></td>\n<td>Accumulates velocity term to accelerate convergence</td>\n<td>Add velocity vector to parameter updates</td>\n</tr>\n<tr>\n<td><strong>AdaGrad</strong></td>\n<td>Adapts learning rate per parameter based on historical gradients</td>\n<td>Maintain per-parameter gradient accumulation</td>\n</tr>\n<tr>\n<td><strong>Adam</strong></td>\n<td>Combines momentum with adaptive learning rates</td>\n<td>Track both gradient momentum and second moment estimates</td>\n</tr>\n<tr>\n<td><strong>L-BFGS</strong></td>\n<td>Uses quasi-Newton methods with limited memory</td>\n<td>Maintain history of recent gradients for second-order approximation</td>\n</tr>\n</tbody></table>\n<p>Each algorithm extends our basic <code>parameter_update_rule</code> with additional state variables and more sophisticated update computations. The architectural pattern remains consistent: compute gradients, update parameters based on gradient and algorithm-specific state, check convergence.</p>\n<p><strong>Adam Optimizer Extension Example:</strong></p>\n<p>The Adam optimizer maintains exponentially decaying averages of gradients (<code>m_t</code>) and squared gradients (<code>v_t</code>). The update rule becomes:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>m_t = β₁ * m_(t-1) + (1-β₁) * g_t\nv_t = β₂ * v_(t-1) + (1-β₂) * g_t²\nparameters -= lr * m̂_t / (√v̂_t + ε)</code></pre></div>\n\n<p>This requires extending our <code>TrainingHistory</code> to track momentum terms and adapting our convergence detection to handle adaptive learning rates.</p>\n<h3 id=\"production-readiness\">Production Readiness</h3>\n<p>Transforming our educational implementation into a production-ready system requires addressing scalability, reliability, maintainability, and performance concerns that were deliberately simplified for learning purposes.</p>\n<h4 id=\"computational-efficiency-and-scalability\">Computational Efficiency and Scalability</h4>\n<p>Our current implementation prioritizes clarity and educational value over computational efficiency. Production deployment requires optimizing for large datasets, distributed computing, and real-time prediction scenarios.</p>\n<p><strong>Performance Optimization Requirements:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Current Implementation</th>\n<th>Production Requirement</th>\n<th>Optimization Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Full-batch gradient descent</strong></td>\n<td>Handle datasets larger than memory</td>\n<td>Implement mini-batch and online learning algorithms</td>\n</tr>\n<tr>\n<td><strong>Pure NumPy operations</strong></td>\n<td>Leverage hardware acceleration</td>\n<td>Integrate with optimized BLAS libraries, GPU computation</td>\n</tr>\n<tr>\n<td><strong>Single-threaded training</strong></td>\n<td>Utilize multi-core processors</td>\n<td>Parallelize gradient computation and parameter updates</td>\n</tr>\n<tr>\n<td><strong>In-memory data handling</strong></td>\n<td>Handle terabyte-scale datasets</td>\n<td>Implement data streaming and out-of-core processing</td>\n</tr>\n<tr>\n<td><strong>Python implementation</strong></td>\n<td>High-performance inference</td>\n<td>Compile critical paths to C/C++ or use specialized ML frameworks</td>\n</tr>\n</tbody></table>\n<p>The <strong>mini-batch gradient descent</strong> extension involves modifying our training loop to process data in small batches rather than the entire dataset. This requires changes to our <code>DataHandler</code> for batch generation and our optimization components for accumulating gradients across batches.</p>\n<p><strong>Scalability Architecture Patterns:</strong></p>\n<ol>\n<li><strong>Data Pipeline Streaming</strong>: Replace in-memory arrays with data generators that yield batches from disk or network</li>\n<li><strong>Distributed Parameter Updates</strong>: Implement parameter server architectures for multi-machine training</li>\n<li><strong>Model Checkpointing</strong>: Add serialization for model state to enable training resumption and deployment</li>\n<li><strong>Prediction Serving</strong>: Separate training and inference with optimized prediction pipelines</li>\n<li><strong>Feature Store Integration</strong>: Connect with production feature management systems</li>\n</ol>\n<h4 id=\"robust-production-data-handling\">Robust Production Data Handling</h4>\n<p>Our current <code>DataHandler</code> provides basic CSV loading and validation suitable for controlled educational scenarios. Production systems must handle diverse data sources, streaming updates, missing values, data quality monitoring, and schema evolution.</p>\n<p><strong>Production Data Management Requirements:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Current Scope</th>\n<th>Production Extension</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Data Loading</strong></td>\n<td>CSV files with clean data</td>\n<td>Multiple formats (JSON, Parquet, databases), streaming ingestion</td>\n</tr>\n<tr>\n<td><strong>Data Validation</strong></td>\n<td>Basic shape and type checking</td>\n<td>Comprehensive data quality monitoring, drift detection</td>\n</tr>\n<tr>\n<td><strong>Missing Values</strong></td>\n<td>Assumes complete data</td>\n<td>Robust imputation strategies and missing data handling</td>\n</tr>\n<tr>\n<td><strong>Feature Engineering</strong></td>\n<td>Manual polynomial features</td>\n<td>Automated feature generation and selection pipelines</td>\n</tr>\n<tr>\n<td><strong>Data Versioning</strong></td>\n<td>No versioning</td>\n<td>Track data lineage and enable reproducible training</td>\n</tr>\n</tbody></table>\n<p>The production <code>DataHandler</code> requires integration with <strong>data pipeline orchestration</strong> systems, <strong>feature stores</strong> for consistent feature definitions, and <strong>monitoring systems</strong> for data quality and model performance tracking.</p>\n<p><strong>Enterprise Integration Patterns:</strong></p>\n<ol>\n<li><strong>Feature Store Integration</strong>: Connect with centralized feature management systems for consistent train/serve features</li>\n<li><strong>Data Quality Monitoring</strong>: Implement statistical tests for data drift, schema validation, and anomaly detection</li>\n<li><strong>Model Registry</strong>: Version control for trained models with metadata tracking and deployment management</li>\n<li><strong>A/B Testing Framework</strong>: Infrastructure for controlled model rollouts and performance comparison</li>\n<li><strong>Monitoring and Alerting</strong>: Real-time tracking of prediction accuracy, latency, and system health</li>\n</ol>\n<h4 id=\"model-lifecycle-management\">Model Lifecycle Management</h4>\n<p>Our educational implementation focuses on training and prediction but lacks the comprehensive model lifecycle management required for production deployment.</p>\n<p><strong>Model Lifecycle Components:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Lifecycle Stage</th>\n<th>Current Implementation</th>\n<th>Production Requirements</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Training</strong></td>\n<td>One-time fit() call</td>\n<td>Automated retraining pipelines with data freshness triggers</td>\n</tr>\n<tr>\n<td><strong>Validation</strong></td>\n<td>Basic R-squared metric</td>\n<td>Comprehensive model validation including fairness and bias testing</td>\n</tr>\n<tr>\n<td><strong>Deployment</strong></td>\n<td>Direct Python object usage</td>\n<td>Containerized deployment with version management and rollback capability</td>\n</tr>\n<tr>\n<td><strong>Monitoring</strong></td>\n<td>No post-deployment tracking</td>\n<td>Real-time performance monitoring with automated alerting</td>\n</tr>\n<tr>\n<td><strong>Maintenance</strong></td>\n<td>Manual code updates</td>\n<td>Automated model updates based on performance degradation detection</td>\n</tr>\n</tbody></table>\n<p>Production systems require <strong>MLOps infrastructure</strong> that automates model training, validation, deployment, and monitoring. This includes CI/CD pipelines for model code, automated testing of model performance, and infrastructure for safe model rollouts.</p>\n<p><strong>Production Deployment Architecture:</strong></p>\n<ol>\n<li><strong>Model Serving Infrastructure</strong>: REST/gRPC APIs with load balancing and auto-scaling for prediction requests</li>\n<li><strong>Feature Pipeline Management</strong>: Real-time feature computation and caching for low-latency predictions</li>\n<li><strong>Model Performance Monitoring</strong>: Track prediction accuracy, inference latency, and model drift over time</li>\n<li><strong>Automated Retraining</strong>: Trigger model updates based on performance degradation or new data availability</li>\n<li><strong>Safety and Compliance</strong>: Implement model explainability, audit trails, and bias monitoring for regulated industries</li>\n</ol>\n<blockquote>\n<p><strong>Production Readiness Principle</strong>: The transition from educational to production code requires shifting focus from <strong>algorithmic understanding</strong> to <strong>operational reliability</strong>. The core mathematical concepts remain the same, but the supporting infrastructure expands dramatically.</p>\n</blockquote>\n<h4 id=\"security-and-privacy-considerations\">Security and Privacy Considerations</h4>\n<p>Production machine learning systems must address data privacy, model security, and regulatory compliance requirements that are outside the scope of our educational implementation.</p>\n<p><strong>Security and Privacy Requirements:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>Educational Scope</th>\n<th>Production Requirements</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Data Privacy</strong></td>\n<td>Public datasets</td>\n<td>PII protection, encryption at rest and in transit</td>\n</tr>\n<tr>\n<td><strong>Model Security</strong></td>\n<td>Open model parameters</td>\n<td>Secure model deployment, protection against adversarial attacks</td>\n</tr>\n<tr>\n<td><strong>Access Control</strong></td>\n<td>No authentication</td>\n<td>Role-based access control for training data and models</td>\n</tr>\n<tr>\n<td><strong>Audit Logging</strong></td>\n<td>Basic training history</td>\n<td>Comprehensive audit trails for regulatory compliance</td>\n</tr>\n<tr>\n<td><strong>Data Governance</strong></td>\n<td>No retention policies</td>\n<td>Data retention, right-to-be-forgotten, consent management</td>\n</tr>\n</tbody></table>\n<p>Production systems may require <strong>differential privacy</strong> techniques during training, <strong>federated learning</strong> approaches for distributed sensitive data, and <strong>secure multi-party computation</strong> for collaborative model training without data sharing.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This implementation guidance provides concrete next steps for extending your linear regression foundation toward more advanced machine learning concepts. The progression follows natural complexity gradients while building upon your established architecture patterns.</p>\n<h4 id=\"technology-stack-evolution\">Technology Stack Evolution</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Current Foundation</th>\n<th>Immediate Extensions</th>\n<th>Advanced Production</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Core Computation</strong></td>\n<td>NumPy</td>\n<td>SciPy (optimization), Pandas (data)</td>\n<td>CuPy (GPU), Dask (distributed)</td>\n</tr>\n<tr>\n<td><strong>Visualization</strong></td>\n<td>Matplotlib</td>\n<td>Seaborn (statistical plots)</td>\n<td>Plotly (interactive), TensorBoard (training)</td>\n</tr>\n<tr>\n<td><strong>Data Handling</strong></td>\n<td>CSV files</td>\n<td>Pandas (multiple formats)</td>\n<td>Apache Arrow, Parquet</td>\n</tr>\n<tr>\n<td><strong>Model Persistence</strong></td>\n<td>Pickle</td>\n<td>Joblib (NumPy optimization)</td>\n<td>MLflow, ONNX</td>\n</tr>\n<tr>\n<td><strong>Web APIs</strong></td>\n<td>None</td>\n<td>Flask/FastAPI</td>\n<td>Kubernetes, Docker</td>\n</tr>\n</tbody></table>\n<h4 id=\"project-structure-evolution\">Project Structure Evolution</h4>\n<p>Your current project structure provides an excellent foundation for extensions. Here&#39;s how to organize advanced features:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>linear_regression_project/\n├── core/                          # Your existing core implementation\n│   ├── data_handler.py           # Current DataHandler\n│   ├── simple_regression.py      # SimpleLinearRegression\n│   ├── gradient_descent.py       # GradientDescentRegression\n│   └── multiple_regression.py    # MultipleLinearRegression\n├── extensions/                    # New extensions directory\n│   ├── __init__.py\n│   ├── polynomial_features.py    # Polynomial feature generation\n│   ├── regularization.py         # L1, Elastic Net extensions\n│   ├── cross_validation.py       # K-fold validation framework\n│   └── model_selection.py        # Hyperparameter tuning\n├── advanced/                      # Advanced algorithm implementations\n│   ├── __init__.py\n│   ├── logistic_regression.py    # Classification extension\n│   ├── neural_network.py         # Basic neural network\n│   └── optimizers.py             # Advanced optimization algorithms\n├── production/                    # Production-ready components\n│   ├── __init__.py\n│   ├── model_serving.py          # REST API for predictions\n│   ├── pipeline.py               # End-to-end ML pipeline\n│   └── monitoring.py             # Model performance tracking\n├── examples/                      # Comprehensive examples\n│   ├── polynomial_regression_demo.py\n│   ├── logistic_regression_demo.py\n│   └── production_pipeline_demo.py\n└── tests/                         # Extended testing\n    ├── test_extensions.py\n    ├── test_advanced.py\n    └── test_production.py</code></pre></div>\n\n<h4 id=\"polynomial-features-implementation-starter\">Polynomial Features Implementation Starter</h4>\n<p>This complete implementation extends your <code>DataHandler</code> with polynomial feature generation:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># extensions/polynomial_features.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> itertools </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> combinations_with_replacement</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> warnings</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PolynomialFeatureGenerator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Generates polynomial features for non-linear regression.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, degree: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">, include_bias: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">, interaction_only: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize polynomial feature generator.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            degree: Maximum polynomial degree</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            include_bias: Whether to include constant (bias) term</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            interaction_only: If True, only include interaction terms (no powers)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.degree </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> degree</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.include_bias </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> include_bias</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.interaction_only </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> interaction_only</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.feature_names_: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.n_input_features_: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.n_output_features_: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> fit</span><span style=\"color:#E1E4E8\">(self, X: np.ndarray, feature_names: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'PolynomialFeatureGenerator'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Learn the polynomial feature combinations from input data.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate input array X for proper shape and data type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Store number of input features for later validation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Generate feature names if not provided (use \"x0\", \"x1\", etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Calculate all polynomial feature combinations up to specified degree</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Store feature names for interpretability</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Calculate total number of output features for memory planning</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use itertools.combinations_with_replacement for feature combinations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> transform</span><span style=\"color:#E1E4E8\">(self, X: np.ndarray) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Transform input features to polynomial features.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate that fit() has been called first</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check input feature count matches training data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Initialize output array with correct shape</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Add bias column if include_bias is True</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Generate polynomial features for each combination</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Handle potential overflow in polynomial computation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Consider using np.column_stack for efficient array building</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> fit_transform</span><span style=\"color:#E1E4E8\">(self, X: np.ndarray, feature_names: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Fit and transform in one step.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.fit(X, feature_names).transform(X)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_feature_names_out</span><span style=\"color:#E1E4E8\">(self, input_features: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get descriptive names for output features.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return stored feature names for interpretability</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Extension to your existing DataHandler</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PolynomialDataHandler</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Extends DataHandler with polynomial feature generation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, base_handler, degree: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.base_handler </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> base_handler</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.poly_generator </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> PolynomialFeatureGenerator(</span><span style=\"color:#FFAB70\">degree</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">degree)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.is_fitted_ </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> prepare_polynomial_features</span><span style=\"color:#E1E4E8\">(self, features: np.ndarray, fit: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Prepare polynomial features for training or prediction.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Apply base data handling (normalization, validation)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Generate polynomial features using fit_transform or transform</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Apply feature scaling to polynomial features</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Store transformation state for consistent prediction preprocessing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Polynomial features often have very different scales than original features</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"cross-validation-framework-starter\">Cross-Validation Framework Starter</h4>\n<p>This implementation provides a complete cross-validation framework that integrates with your existing models:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># extensions/cross_validation.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Tuple, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> sklearn.model_selection </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> KFold</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> warnings</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CrossValidator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"K-fold cross-validation for regression models.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, k: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#E1E4E8\">, shuffle: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">, random_state: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.k </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> k</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.shuffle </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> shuffle</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.random_state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> random_state</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cv_results_: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, List[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_model</span><span style=\"color:#E1E4E8\">(self, model_class, X: np.ndarray, y: np.ndarray, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                      model_params: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Perform k-fold cross-validation on a model.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize KFold splitter with specified parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize result storage for each fold</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Iterate through each fold split</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Train model on training fold using model_class</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Evaluate model on validation fold</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Store metrics (R-squared, MSE, etc.) for each fold</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Compute aggregate statistics (mean, std) across folds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Return comprehensive results dictionary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Handle potential fitting failures gracefully with try-catch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> hyperparameter_search</span><span style=\"color:#E1E4E8\">(self, model_class, X: np.ndarray, y: np.ndarray,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                             param_grid: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, List[Any]]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Grid search with cross-validation for hyperparameter tuning.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate all combinations of parameters from grid</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For each parameter combination, run cross-validation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Track best performance and corresponding parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return best parameters and detailed results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use itertools.product for parameter combination generation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> simple_train_test_split</span><span style=\"color:#E1E4E8\">(X: np.ndarray, y: np.ndarray, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                          test_size: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.2</span><span style=\"color:#E1E4E8\">, random_state: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> Tuple:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Simple implementation of train-test split.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Set random seed if provided for reproducibility</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate split indices based on test_size</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Randomly shuffle indices if desired</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Split features and targets based on calculated indices</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return train and test sets</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"logistic-regression-implementation-skeleton\">Logistic Regression Implementation Skeleton</h4>\n<p>This skeleton demonstrates the architectural similarity between linear and logistic regression:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># advanced/logistic_regression.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> warnings</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> LogisticRegression</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Binary logistic regression using gradient descent.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, learning_rate: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.01</span><span style=\"color:#E1E4E8\">, max_iterations: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 tolerance: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-6</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize parameters identical to your LinearRegression class</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Same optimization parameters, different cost function</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _sigmoid</span><span style=\"color:#E1E4E8\">(self, z: np.ndarray) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute sigmoid activation function.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Implement sigmoid function: 1 / (1 + exp(-z))</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Handle numerical overflow for very large negative z values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Clip extreme values to prevent numerical instability</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use np.clip to prevent overflow in exponential</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _compute_cost</span><span style=\"color:#E1E4E8\">(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute logistic regression cost (cross-entropy loss).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Compute linear combination: z = X @ weights</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Apply sigmoid to get probabilities: p = sigmoid(z)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compute cross-entropy: -[y*log(p) + (1-y)*log(1-p)]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return average cost across all samples</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Add small epsilon to prevent log(0) errors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _compute_gradients</span><span style=\"color:#E1E4E8\">(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute gradients of logistic cost function.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Compute predictions using current weights</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate prediction errors (predictions - targets)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compute gradient: (1/m) * X.T @ errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return gradient vector</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Gradient computation is very similar to linear regression!</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> fit</span><span style=\"color:#E1E4E8\">(self, X: np.ndarray, y: np.ndarray) -> </span><span style=\"color:#9ECBFF\">'LogisticRegression'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Train logistic regression model using gradient descent.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate binary classification targets (0 and 1 only)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Add intercept column to feature matrix</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Initialize weights (same as linear regression)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Implement gradient descent loop (reuse your existing pattern!)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Track cost history and check convergence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Store final weights and set fitted flag</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Training loop structure identical to LinearRegression</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> predict_proba</span><span style=\"color:#E1E4E8\">(self, X: np.ndarray) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Predict class probabilities.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate model is fitted</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Add intercept column to input features</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compute linear combination with learned weights</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Apply sigmoid to get probabilities</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: This is your prediction pipeline</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> predict</span><span style=\"color:#E1E4E8\">(self, X: np.ndarray) -> np.ndarray:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Predict binary class labels (0 or 1).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get probability predictions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Apply threshold (0.5) to convert probabilities to classes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return binary predictions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"production-api-serving-starter\">Production API Serving Starter</h4>\n<p>This complete implementation shows how to serve your trained models via REST API:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># production/model_serving.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> flask </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Flask, request, jsonify</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pickle</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> traceback</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">app </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Flask(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">logging.basicConfig(</span><span style=\"color:#FFAB70\">level</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">logging.</span><span style=\"color:#79B8FF\">INFO</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ModelServer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Production model serving with REST API.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.models </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.prediction_history </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> load_model</span><span style=\"color:#E1E4E8\">(self, model_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, model_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load trained model from disk.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(model_path, </span><span style=\"color:#9ECBFF\">'rb'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pickle.load(f)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.models[model_name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'model'</span><span style=\"color:#E1E4E8\">: model,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'loaded_at'</span><span style=\"color:#E1E4E8\">: datetime.now(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'prediction_count'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logging.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Loaded model </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">model_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> from </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">model_path</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logging.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Failed to load model </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">model_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{str</span><span style=\"color:#E1E4E8\">(e)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> predict</span><span style=\"color:#E1E4E8\">(self, model_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, features: np.ndarray) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Make prediction with error handling and logging.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate model exists and is loaded</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate input features shape and data types</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Make prediction using loaded model</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Log prediction request and response</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Update model usage statistics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return prediction with metadata</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Global model server instance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">model_server </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ModelServer()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@app.route</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'/health'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">methods</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">'GET'</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> health_check</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Health check endpoint for load balancers.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> jsonify({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'status'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'healthy'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'models_loaded'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(model_server.models),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'timestamp'</span><span style=\"color:#E1E4E8\">: datetime.now().isoformat()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@app.route</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'/predict/&#x3C;model_name>'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">methods</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">'POST'</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> predict</span><span style=\"color:#E1E4E8\">(model_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Main prediction endpoint.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Parse JSON request body to get features</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate required fields are present</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Convert features to NumPy array</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Call model_server.predict() with features</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return prediction results as JSON</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Handle errors gracefully with appropriate HTTP status codes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logging.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Prediction error: </span><span style=\"color:#79B8FF\">{str</span><span style=\"color:#E1E4E8\">(e)</span><span style=\"color:#79B8FF\">}\\n{</span><span style=\"color:#E1E4E8\">traceback.format_exc()</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> jsonify({</span><span style=\"color:#9ECBFF\">'error'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(e)}), </span><span style=\"color:#79B8FF\">500</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@app.route</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'/models'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">methods</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">'GET'</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> list_models</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"List all loaded models with metadata.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return information about all loaded models</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> __name__</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#9ECBFF\"> '__main__'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Load your trained models</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # model_server.load_model('path/to/your/trained_model.pkl', 'linear_regression_v1')</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    app.run(</span><span style=\"color:#FFAB70\">host</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'0.0.0.0'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">port</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">5000</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">debug</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints-for-extensions\">Milestone Checkpoints for Extensions</h4>\n<p><strong>Polynomial Features Checkpoint:</strong>\nAfter implementing polynomial feature generation, verify:</p>\n<ol>\n<li><strong>Synthetic Data Test</strong>: Generate quadratic relationship <code>y = x² + noise</code>, fit polynomial model with degree=2, should achieve R² &gt; 0.95</li>\n<li><strong>Feature Count Verification</strong>: Input with 3 features and degree=2 should generate 9 polynomial features</li>\n<li><strong>Overfitting Detection</strong>: High-degree polynomials on small datasets should show training R² &gt;&gt; test R²</li>\n</ol>\n<p><strong>Cross-Validation Checkpoint:</strong>\nAfter implementing k-fold validation:</p>\n<ol>\n<li><strong>Stability Test</strong>: 5-fold CV on same data should give similar mean R² across multiple runs</li>\n<li><strong>Hyperparameter Selection</strong>: CV should identify optimal polynomial degree that balances training and validation performance</li>\n<li><strong>Statistical Significance</strong>: Standard deviation across folds should be reasonable (typically &lt; 0.1 for R²)</li>\n</ol>\n<p><strong>Logistic Regression Checkpoint:</strong>\nAfter implementing classification:</p>\n<ol>\n<li><strong>Binary Classification</strong>: Fit model on binary classification data, should achieve &gt; 85% accuracy on linearly separable data</li>\n<li><strong>Probability Calibration</strong>: Predicted probabilities should be well-calibrated (predictions near 0.7 should be correct ~70% of time)</li>\n<li><strong>Decision Boundary</strong>: Visualization should show clear linear decision boundary for 2D data</li>\n</ol>\n<p><strong>Production API Checkpoint:</strong>\nAfter deploying model serving:</p>\n<ol>\n<li><strong>Load Test</strong>: API should handle 100 concurrent prediction requests without errors</li>\n<li><strong>Latency Test</strong>: Single prediction should complete in &lt; 100ms</li>\n<li><strong>Error Handling</strong>: Invalid inputs should return appropriate HTTP error codes with helpful messages</li>\n</ol>\n<h2 id=\"glossary\">Glossary</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (foundational terminology that applies across M1: Simple Linear Regression, M2: Gradient Descent, M3: Multiple Linear Regression)</p>\n</blockquote>\n<p>This glossary provides comprehensive definitions for all technical terms, mathematical concepts, and domain-specific vocabulary used throughout the linear regression implementation. Understanding these terms is essential for grasping both the theoretical foundations and practical implementation details of machine learning algorithms.</p>\n<h3 id=\"mental-model-the-language-of-machine-learning\">Mental Model: The Language of Machine Learning</h3>\n<p>Think of this glossary as a translation dictionary for a foreign country you&#39;re visiting for the first time. Machine learning has its own specialized vocabulary that can seem intimidating initially, but each term represents a concrete concept with practical implications. Just as learning key phrases helps you navigate a new city, mastering these terms will help you navigate the world of machine learning with confidence. Each definition bridges the gap between mathematical theory and computational implementation.</p>\n<h3 id=\"mathematical-and-statistical-terms\">Mathematical and Statistical Terms</h3>\n<p>The mathematical foundation of linear regression relies on several key statistical and optimization concepts that form the theoretical backbone of the implementation.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context in Project</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>ordinary least squares</strong></td>\n<td>Standard linear regression optimization method that finds the line minimizing the sum of squared residuals between actual and predicted values</td>\n<td>Primary method used in M1 for closed-form solution</td>\n</tr>\n<tr>\n<td><strong>normal equations</strong></td>\n<td>Analytical solution to the least squares problem that directly computes optimal parameters without iteration: slope = Σ((x-x̄)(y-ȳ))/Σ((x-x̄)²)</td>\n<td>Implemented in <code>SimpleLinearRegression.fit()</code> for direct parameter calculation</td>\n</tr>\n<tr>\n<td><strong>least squares</strong></td>\n<td>Optimization principle that minimizes the sum of squared differences between observed and predicted values</td>\n<td>Fundamental principle underlying both closed-form and iterative solutions</td>\n</tr>\n<tr>\n<td><strong>coefficient of determination</strong></td>\n<td>Statistical measure (R-squared) representing the proportion of variance in the dependent variable explained by the independent variables, ranging from 0 to 1</td>\n<td>Computed by <code>score()</code> method to evaluate model quality</td>\n</tr>\n<tr>\n<td><strong>residuals</strong></td>\n<td>Difference between actual target values and predicted values: residual = y_actual - y_predicted</td>\n<td>Used in cost function computation and model evaluation throughout all milestones</td>\n</tr>\n<tr>\n<td><strong>gradient</strong></td>\n<td>Vector of partial derivatives of the cost function with respect to each parameter, indicating the direction of steepest increase</td>\n<td>Core of gradient descent algorithm in M2, computed by <code>_compute_gradients()</code></td>\n</tr>\n<tr>\n<td><strong>cost function</strong></td>\n<td>Objective function that measures model performance, specifically mean squared error: (1/2m)Σ(y_pred - y_actual)²</td>\n<td>Minimized during training in M2 and M3, implemented in <code>_compute_cost()</code></td>\n</tr>\n<tr>\n<td><strong>learning rate</strong></td>\n<td>Step size parameter controlling how much parameters change during each gradient descent iteration</td>\n<td>Critical hyperparameter in M2 and M3, affects convergence speed and stability</td>\n</tr>\n<tr>\n<td><strong>convergence</strong></td>\n<td>State reached when optimization algorithm finds a stable solution where further iterations produce minimal improvement</td>\n<td>Detected by multiple criteria in M2 and M3 to determine when to stop training</td>\n</tr>\n</tbody></table>\n<h3 id=\"optimization-and-algorithm-terms\">Optimization and Algorithm Terms</h3>\n<p>The iterative optimization aspects of gradient descent introduce specialized terminology for understanding how algorithms learn from data.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context in Project</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>gradient descent</strong></td>\n<td>Iterative optimization algorithm that updates parameters by moving in the direction opposite to the gradient</td>\n<td>Primary training method in M2 and M3, implemented in <code>GradientDescentRegression.fit()</code></td>\n</tr>\n<tr>\n<td><strong>batch gradient descent</strong></td>\n<td>Variant of gradient descent that computes gradients using the entire training dataset in each iteration</td>\n<td>Standard approach used throughout M2 and M3 for stable convergence</td>\n</tr>\n<tr>\n<td><strong>parameter update rule</strong></td>\n<td>Core gradient descent equation: θ_new = θ_old - α * ∇J(θ), where α is learning rate and ∇J is gradient</td>\n<td>Fundamental step implemented in all gradient descent components</td>\n</tr>\n<tr>\n<td><strong>convergence detection</strong></td>\n<td>Process of determining when optimization has reached a satisfactory solution using multiple criteria</td>\n<td>Implemented with cost tolerance, gradient magnitude, and parameter change thresholds</td>\n</tr>\n<tr>\n<td><strong>gradient magnitude</strong></td>\n<td>L2 norm (Euclidean length) of the gradient vector, indicating how steep the cost function is at current parameters</td>\n<td>Used in convergence detection to identify when gradients become sufficiently small</td>\n</tr>\n<tr>\n<td><strong>cost improvement</strong></td>\n<td>Decrease in cost function value between consecutive iterations, indicating training progress</td>\n<td>Monitored to ensure algorithm is making progress and to detect convergence</td>\n</tr>\n<tr>\n<td><strong>gradient explosion</strong></td>\n<td>Condition where gradients become extremely large, often indicating numerical instability or inappropriate learning rate</td>\n<td>Detected by <code>NumericalStabilityChecker</code> to prevent parameter divergence</td>\n</tr>\n<tr>\n<td><strong>parameter divergence</strong></td>\n<td>Situation where parameters grow without bound due to optimization problems, typically from excessive learning rate</td>\n<td>Monitored continuously during training to trigger error handling</td>\n</tr>\n</tbody></table>\n<h3 id=\"data-handling-and-preprocessing-terms\">Data Handling and Preprocessing Terms</h3>\n<p>Data preparation and validation introduce terminology related to ensuring data quality and numerical stability.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context in Project</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>z-score normalization</strong></td>\n<td>Standardization technique that transforms features to have zero mean and unit variance: z = (x - μ)/σ</td>\n<td>Essential preprocessing step in M3, implemented in <code>StandardScaler</code></td>\n</tr>\n<tr>\n<td><strong>feature scaling</strong></td>\n<td>Process of transforming input features to comparable ranges to improve optimization performance</td>\n<td>Critical for multiple linear regression to ensure fair parameter updates</td>\n</tr>\n<tr>\n<td><strong>design matrix</strong></td>\n<td>Feature matrix augmented with a column of ones to represent the intercept term in matrix form</td>\n<td>Constructed in M3 to enable vectorized operations: X_design = [1, x1, x2, ...]</td>\n</tr>\n<tr>\n<td><strong>data validation</strong></td>\n<td>Comprehensive process of checking input data for compatibility with regression requirements</td>\n<td>Implemented in <code>DataValidator</code> to catch issues before training begins</td>\n</tr>\n<tr>\n<td><strong>broadcasting</strong></td>\n<td>NumPy mechanism that automatically aligns arrays of different shapes during arithmetic operations</td>\n<td>Enables efficient vectorized operations without explicit loops in M3</td>\n</tr>\n<tr>\n<td><strong>vectorization</strong></td>\n<td>Technique of using matrix operations instead of explicit loops to improve computational efficiency</td>\n<td>Central to M3 implementation for handling multiple features simultaneously</td>\n</tr>\n<tr>\n<td><strong>data leakage</strong></td>\n<td>Problematic situation where information from the future or target variable inappropriately influences training</td>\n<td>Prevented through careful data splitting and preprocessing isolation</td>\n</tr>\n<tr>\n<td><strong>fail-fast principle</strong></td>\n<td>Design philosophy of generating immediate, informative errors rather than allowing silent failures</td>\n<td>Implemented throughout data validation to help learners identify issues quickly</td>\n</tr>\n</tbody></table>\n<h3 id=\"model-architecture-and-component-terms\">Model Architecture and Component Terms</h3>\n<p>The system architecture introduces terminology for understanding how components interact and manage state.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context in Project</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>closed-form solution</strong></td>\n<td>Direct analytical calculation that produces exact results without iteration, using mathematical formulas</td>\n<td>Implemented in M1 <code>SimpleLinearRegression</code> for educational comparison with iterative methods</td>\n</tr>\n<tr>\n<td><strong>hyperplane</strong></td>\n<td>Multi-dimensional generalization of a plane that separates or fits through data points in feature space</td>\n<td>What multiple linear regression fits: a hyperplane through n-dimensional feature space</td>\n</tr>\n<tr>\n<td><strong>regularization</strong></td>\n<td>Technique that adds penalty terms to the cost function to prevent overfitting by constraining parameter complexity</td>\n<td>Implemented as L2 regularization in M3 <code>MultipleLinearRegression</code></td>\n</tr>\n<tr>\n<td><strong>Ridge regression</strong></td>\n<td>Linear regression variant that includes L2 regularization: cost = MSE + λΣ(weights²)</td>\n<td>Available option in M3 for handling overfitting with many features</td>\n</tr>\n<tr>\n<td><strong>matrix conditioning</strong></td>\n<td>Numerical measure of how sensitive matrix operations are to small changes in input values</td>\n<td>Monitored to detect ill-conditioned design matrices that cause numerical instability</td>\n</tr>\n<tr>\n<td><strong>numerical stability</strong></td>\n<td>Property of algorithms that remain accurate despite floating-point precision limitations and computational errors</td>\n<td>Critical concern addressed throughout M2 and M3 with stability checking</td>\n</tr>\n<tr>\n<td><strong>overflow protection</strong></td>\n<td>Safeguards that prevent computational values from exceeding representable floating-point ranges</td>\n<td>Implemented in parameter update rules to maintain numerical validity</td>\n</tr>\n<tr>\n<td><strong>underflow detection</strong></td>\n<td>Mechanisms that identify when values become too small to represent accurately in floating-point arithmetic</td>\n<td>Used in gradient magnitude checking to detect vanishing gradients</td>\n</tr>\n</tbody></table>\n<h3 id=\"testing-and-validation-terms\">Testing and Validation Terms</h3>\n<p>The testing strategy introduces terminology for ensuring implementation correctness and educational effectiveness.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context in Project</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>unit testing</strong></td>\n<td>Testing approach that validates individual components in isolation using synthetic data with known outcomes</td>\n<td>Applied to each component separately: data handling, model fitting, optimization</td>\n</tr>\n<tr>\n<td><strong>integration testing</strong></td>\n<td>End-to-end testing that validates the complete system using real datasets and cross-validation</td>\n<td>Ensures components work together correctly across all milestones</td>\n</tr>\n<tr>\n<td><strong>milestone checkpoints</strong></td>\n<td>Specific success criteria and behavioral expectations that must be met after completing each learning stage</td>\n<td>Defined for M1, M2, and M3 to provide clear progress indicators</td>\n</tr>\n<tr>\n<td><strong>synthetic data</strong></td>\n<td>Artificially generated datasets with known mathematical properties, ideal for validating implementations</td>\n<td>Created by <code>generate_linear_data()</code> with controlled noise and known parameters</td>\n</tr>\n<tr>\n<td><strong>cross-validation</strong></td>\n<td>Technique for assessing model generalization by training and testing on different data subsets</td>\n<td>Used in integration testing to ensure models generalize beyond training data</td>\n</tr>\n<tr>\n<td><strong>mathematical verification</strong></td>\n<td>Process of validating implementation correctness by comparing results against analytical solutions</td>\n<td>Critical for ensuring gradient calculations match finite difference approximations</td>\n</tr>\n<tr>\n<td><strong>edge case testing</strong></td>\n<td>Systematic testing of system behavior under adverse conditions like missing data or extreme values</td>\n<td>Ensures robustness and appropriate error handling throughout the system</td>\n</tr>\n<tr>\n<td><strong>finite differences</strong></td>\n<td>Numerical approximation technique for computing derivatives: f&#39;(x) ≈ (f(x+h) - f(x))/h</td>\n<td>Used in gradient checking to validate analytical gradient computations</td>\n</tr>\n<tr>\n<td><strong>gradient checking</strong></td>\n<td>Validation technique that compares analytical gradients to numerical approximations for correctness</td>\n<td>Essential debugging tool implemented to catch gradient computation errors</td>\n</tr>\n</tbody></table>\n<h3 id=\"debugging-and-error-handling-terms\">Debugging and Error Handling Terms</h3>\n<p>The debugging infrastructure introduces specialized terminology for diagnosing and resolving machine learning problems.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context in Project</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>convergence validation</strong></td>\n<td>Process of verifying that gradient descent successfully reaches a stable, optimal solution</td>\n<td>Implemented with multiple criteria to ensure training quality</td>\n</tr>\n<tr>\n<td><strong>numerical precision</strong></td>\n<td>Accuracy of floating-point computations, limited by machine representation and algorithmic stability</td>\n<td>Monitored throughout optimization to prevent precision loss</td>\n</tr>\n<tr>\n<td><strong>parameter clipping</strong></td>\n<td>Technique of constraining parameter values to safe ranges to prevent numerical overflow</td>\n<td>Applied when parameters exceed <code>MAX_PARAMETER_VALUE</code> threshold</td>\n</tr>\n<tr>\n<td><strong>learning rate adaptation</strong></td>\n<td>Process of automatically adjusting the step size during optimization based on training behavior</td>\n<td>Suggested by diagnostic tools when convergence problems are detected</td>\n</tr>\n<tr>\n<td><strong>validation strictness</strong></td>\n<td>Design balance between preventing errors and allowing experimentation for educational purposes</td>\n<td>Tuned to provide helpful guidance without being overly restrictive</td>\n</tr>\n</tbody></table>\n<h3 id=\"advanced-concepts-and-extensions\">Advanced Concepts and Extensions</h3>\n<p>Terms related to future extensions and advanced machine learning concepts that build on this foundation.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context in Project</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>polynomial features</strong></td>\n<td>Transformed features that include powers and interactions of original features to capture non-linear relationships</td>\n<td>Introduced in future extensions as natural progression from linear models</td>\n</tr>\n<tr>\n<td><strong>k-fold</strong></td>\n<td>Cross-validation method that partitions data into k equal subsets for robust model evaluation</td>\n<td>Implemented in advanced testing for model validation</td>\n</tr>\n<tr>\n<td><strong>hyperparameter tuning</strong></td>\n<td>Systematic process of selecting optimal configuration parameters like learning rate and regularization strength</td>\n<td>Essential skill for practical machine learning applications</td>\n</tr>\n<tr>\n<td><strong>feature engineering</strong></td>\n<td>Process of creating new features from existing ones to improve model performance and capture domain knowledge</td>\n<td>Natural extension of preprocessing concepts introduced in data handling</td>\n</tr>\n<tr>\n<td><strong>sigmoid function</strong></td>\n<td>Logistic function σ(z) = 1/(1 + e^(-z)) that maps real numbers to probabilities between 0 and 1</td>\n<td>Foundation for logistic regression extension</td>\n</tr>\n<tr>\n<td><strong>cross-entropy loss</strong></td>\n<td>Cost function for classification problems that measures the quality of probability predictions</td>\n<td>Used in logistic regression as alternative to mean squared error</td>\n</tr>\n<tr>\n<td><strong>overfitting</strong></td>\n<td>Phenomenon where models memorize training data rather than learning generalizable patterns</td>\n<td>Addressed by regularization and proper validation techniques</td>\n</tr>\n<tr>\n<td><strong>model serving</strong></td>\n<td>Production deployment of trained models via APIs for real-time predictions</td>\n<td>Represents the practical application of the educational implementation</td>\n</tr>\n<tr>\n<td><strong>production readiness</strong></td>\n<td>Set of capabilities required for reliable deployment in real-world environments</td>\n<td>Encompasses robustness, scalability, and monitoring requirements</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This implementation guidance provides practical direction for understanding and applying the terminology throughout the linear regression project.</p>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Concept Category</th>\n<th>Basic Understanding</th>\n<th>Advanced Application</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Mathematical Terms</td>\n<td>Focus on intuitive understanding through examples</td>\n<td>Study formal mathematical proofs and derivations</td>\n</tr>\n<tr>\n<td>Algorithm Terms</td>\n<td>Implement step-by-step with detailed logging</td>\n<td>Optimize for computational efficiency and numerical stability</td>\n</tr>\n<tr>\n<td>Data Terms</td>\n<td>Practice with small, well-understood datasets</td>\n<td>Handle real-world messy data with missing values and outliers</td>\n</tr>\n<tr>\n<td>Architecture Terms</td>\n<td>Build simple, clear implementations</td>\n<td>Design for extensibility and maintainability</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-study-approach\">B. Recommended Study Approach</h4>\n<p>The terminology in this glossary follows a specific learning progression that mirrors the project milestones:</p>\n<p><strong>Milestone 1 Focus Terms:</strong></p>\n<ul>\n<li>ordinary least squares, normal equations, least squares</li>\n<li>coefficient of determination, residuals</li>\n<li>closed-form solution</li>\n<li>unit testing, synthetic data</li>\n</ul>\n<p><strong>Milestone 2 Addition Terms:</strong></p>\n<ul>\n<li>gradient descent, batch gradient descent, parameter update rule</li>\n<li>cost function, learning rate, convergence</li>\n<li>gradient magnitude, convergence detection</li>\n<li>numerical stability, gradient checking</li>\n</ul>\n<p><strong>Milestone 3 Extension Terms:</strong></p>\n<ul>\n<li>z-score normalization, feature scaling, design matrix</li>\n<li>vectorization, broadcasting, hyperplane</li>\n<li>regularization, Ridge regression, matrix conditioning</li>\n<li>cross-validation, hyperparameter tuning</li>\n</ul>\n<h4 id=\"c-terminology-application-in-code\">C. Terminology Application in Code</h4>\n<p>Each term in this glossary corresponds to specific implementation patterns:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Example of term application in method documentation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _compute_gradients</span><span style=\"color:#E1E4E8\">(self, x: np.ndarray, y: np.ndarray, slope: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, intercept: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> Tuple[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Compute partial derivatives of cost function with respect to parameters.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This implements the gradient calculation for batch gradient descent,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    computing the gradient magnitude for convergence detection.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Uses vectorization to avoid explicit loops and improve numerical stability.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Implementation follows terminology definitions</span></span></code></pre></div>\n\n<h4 id=\"d-common-terminology-mistakes\">D. Common Terminology Mistakes</h4>\n<table>\n<thead>\n<tr>\n<th>Mistake</th>\n<th>Correct Understanding</th>\n<th>Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Confusing &quot;gradient&quot; with &quot;slope&quot;</td>\n<td>Gradient is vector of partial derivatives; slope is single parameter</td>\n<td>Misunderstanding multivariable optimization</td>\n</tr>\n<tr>\n<td>Using &quot;accuracy&quot; for regression</td>\n<td>Use R-squared or mean squared error for regression evaluation</td>\n<td>Inappropriate evaluation metrics</td>\n</tr>\n<tr>\n<td>Calling feature scaling &quot;normalization&quot;</td>\n<td>Distinguish z-score normalization from min-max scaling</td>\n<td>Different preprocessing effects</td>\n</tr>\n<tr>\n<td>Confusing &quot;convergence&quot; with &quot;completion&quot;</td>\n<td>Convergence means reaching stable solution, not just finishing iterations</td>\n<td>Misunderstanding optimization quality</td>\n</tr>\n</tbody></table>\n<h4 id=\"e-debugging-with-terminology\">E. Debugging with Terminology</h4>\n<p>When encountering issues, use precise terminology to diagnose problems:</p>\n<ul>\n<li>&quot;Parameter divergence&quot; indicates learning rate too high</li>\n<li>&quot;Gradient explosion&quot; suggests numerical instability</li>\n<li>&quot;Poor convergence&quot; may indicate insufficient iterations or inappropriate tolerance</li>\n<li>&quot;Low R-squared&quot; suggests model doesn&#39;t fit data well</li>\n</ul>\n<h4 id=\"f-milestone-terminology-checkpoints\">F. Milestone Terminology Checkpoints</h4>\n<p><strong>After Milestone 1:</strong> Learners should comfortably use terms related to least squares, closed-form solutions, and basic model evaluation.</p>\n<p><strong>After Milestone 2:</strong> Vocabulary should expand to include gradient descent terminology, convergence concepts, and optimization language.</p>\n<p><strong>After Milestone 3:</strong> Complete terminology mastery including matrix operations, regularization, and advanced preprocessing concepts.</p>\n<p>Each checkpoint represents not just vocabulary acquisition but deep conceptual understanding that enables effective communication about machine learning concepts and debugging of implementation issues.</p>\n","toc":[{"level":1,"text":"Linear Regression from Scratch: Design Document","id":"linear-regression-from-scratch-design-document"},{"level":2,"text":"Overview","id":"overview"},{"level":2,"text":"Context and Problem Statement","id":"context-and-problem-statement"},{"level":3,"text":"Mental Model: The Line-Fitting Detective","id":"mental-model-the-line-fitting-detective"},{"level":3,"text":"Mathematical Foundation","id":"mathematical-foundation"},{"level":3,"text":"Existing Approaches Comparison","id":"existing-approaches-comparison"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Project Structure","id":"recommended-project-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Algorithm Skeletons","id":"core-algorithm-skeletons"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Goals and Non-Goals","id":"goals-and-non-goals"},{"level":3,"text":"Learning Goals","id":"learning-goals"},{"level":3,"text":"Implementation Goals","id":"implementation-goals"},{"level":3,"text":"Explicit Non-Goals","id":"explicit-non-goals"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"High-Level Architecture","id":"high-level-architecture"},{"level":3,"text":"Component Overview: The Four Main Components","id":"component-overview-the-four-main-components"},{"level":3,"text":"Recommended Project Structure","id":"recommended-project-structure"},{"level":3,"text":"Component Interactions","id":"component-interactions"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Data Model and Core Types","id":"data-model-and-core-types"},{"level":3,"text":"Mental Model: The Learning Journal","id":"mental-model-the-learning-journal"},{"level":3,"text":"Core Data Types","id":"core-data-types"},{"level":4,"text":"Dataset Structure","id":"dataset-structure"},{"level":4,"text":"ModelParameters Structure","id":"modelparameters-structure"},{"level":4,"text":"TrainingHistory Structure","id":"traininghistory-structure"},{"level":4,"text":"PredictionResult Structure","id":"predictionresult-structure"},{"level":3,"text":"Architecture Decision Records","id":"architecture-decision-records"},{"level":3,"text":"Type Relationships and Lifecycle","id":"type-relationships-and-lifecycle"},{"level":4,"text":"Data Transformation Pipeline","id":"data-transformation-pipeline"},{"level":4,"text":"State Transitions and Invariants","id":"state-transitions-and-invariants"},{"level":4,"text":"Data Flow Protocols","id":"data-flow-protocols"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Data Handler Component","id":"data-handler-component"},{"level":3,"text":"Mental Model: The Data Preparation Kitchen","id":"mental-model-the-data-preparation-kitchen"},{"level":3,"text":"Responsibilities and Interface","id":"responsibilities-and-interface"},{"level":4,"text":"Data Ingestion Responsibilities","id":"data-ingestion-responsibilities"},{"level":4,"text":"Data Validation Responsibilities","id":"data-validation-responsibilities"},{"level":4,"text":"Data Transformation Responsibilities","id":"data-transformation-responsibilities"},{"level":4,"text":"Component Interface Specification","id":"component-interface-specification"},{"level":3,"text":"Data Validation and Normalization","id":"data-validation-and-normalization"},{"level":4,"text":"Comprehensive Data Validation Strategy","id":"comprehensive-data-validation-strategy"},{"level":4,"text":"Z-Score Normalization Implementation","id":"z-score-normalization-implementation"},{"level":4,"text":"Missing Value Handling Strategy","id":"missing-value-handling-strategy"},{"level":3,"text":"Architecture Decision Records","id":"architecture-decision-records"},{"level":4,"text":"Decision: NumPy vs Pure Python for Numerical Operations","id":"decision-numpy-vs-pure-python-for-numerical-operations"},{"level":4,"text":"Decision: Normalization Strategy Selection","id":"decision-normalization-strategy-selection"},{"level":4,"text":"Decision: Error Handling Philosophy","id":"decision-error-handling-philosophy"},{"level":4,"text":"Decision: Dataset Container Design","id":"decision-dataset-container-design"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":4,"text":"⚠️ Pitfall: Using Python Lists Instead of NumPy Arrays","id":"-pitfall-using-python-lists-instead-of-numpy-arrays"},{"level":4,"text":"⚠️ Pitfall: Forgetting to Handle Division by Zero in Normalization","id":"-pitfall-forgetting-to-handle-division-by-zero-in-normalization"},{"level":4,"text":"⚠️ Pitfall: Applying Different Normalization to Training and Prediction Data","id":"-pitfall-applying-different-normalization-to-training-and-prediction-data"},{"level":4,"text":"⚠️ Pitfall: Inadequate Data Type Validation","id":"-pitfall-inadequate-data-type-validation"},{"level":4,"text":"⚠️ Pitfall: Ignoring Feature Scale Differences During Debugging","id":"-pitfall-ignoring-feature-scale-differences-during-debugging"},{"level":4,"text":"⚠️ Pitfall: Insufficient Sample Size Validation","id":"-pitfall-insufficient-sample-size-validation"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Language-Specific Hints","id":"language-specific-hints"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Simple Linear Regression Component","id":"simple-linear-regression-component"},{"level":3,"text":"Mental Model: The Best-Fit Line","id":"mental-model-the-best-fit-line"},{"level":3,"text":"Closed-Form Solution Algorithm","id":"closed-form-solution-algorithm"},{"level":3,"text":"Prediction and Evaluation","id":"prediction-and-evaluation"},{"level":3,"text":"Architecture Decision Records","id":"architecture-decision-records"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Gradient Descent Optimizer Component","id":"gradient-descent-optimizer-component"},{"level":3,"text":"Mental Model: Hill Climbing in Reverse","id":"mental-model-hill-climbing-in-reverse"},{"level":3,"text":"Cost Function Implementation","id":"cost-function-implementation"},{"level":3,"text":"Gradient Calculation","id":"gradient-calculation"},{"level":3,"text":"Parameter Update Rules","id":"parameter-update-rules"},{"level":3,"text":"Convergence Detection","id":"convergence-detection"},{"level":3,"text":"Architecture Decision Records","id":"architecture-decision-records"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Multiple Linear Regression Component","id":"multiple-linear-regression-component"},{"level":3,"text":"Mental Model: Multi-Dimensional Plane Fitting","id":"mental-model-multi-dimensional-plane-fitting"},{"level":3,"text":"Matrix Formulation","id":"matrix-formulation"},{"level":3,"text":"Vectorized Gradient Descent","id":"vectorized-gradient-descent"},{"level":3,"text":"Feature Scaling and Engineering","id":"feature-scaling-and-engineering"},{"level":3,"text":"L2 Regularization (Ridge)","id":"l2-regularization-ridge"},{"level":3,"text":"Architecture Decision Records","id":"architecture-decision-records"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Interactions and Data Flow","id":"interactions-and-data-flow"},{"level":3,"text":"Mental Model: The Assembly Line for Intelligence","id":"mental-model-the-assembly-line-for-intelligence"},{"level":3,"text":"Training Data Flow","id":"training-data-flow"},{"level":4,"text":"Step-by-Step Training Process","id":"step-by-step-training-process"},{"level":4,"text":"Data Format Transformations During Training","id":"data-format-transformations-during-training"},{"level":4,"text":"Training Flow Variations by Milestone","id":"training-flow-variations-by-milestone"},{"level":4,"text":"Critical Synchronization Points","id":"critical-synchronization-points"},{"level":3,"text":"Prediction Data Flow","id":"prediction-data-flow"},{"level":4,"text":"Mental Model: The Factory Production Line","id":"mental-model-the-factory-production-line"},{"level":4,"text":"Step-by-Step Prediction Process","id":"step-by-step-prediction-process"},{"level":4,"text":"Prediction Data Format Requirements","id":"prediction-data-format-requirements"},{"level":4,"text":"Preprocessing Consistency Requirements","id":"preprocessing-consistency-requirements"},{"level":4,"text":"Prediction Flow Variations by Milestone","id":"prediction-flow-variations-by-milestone"},{"level":3,"text":"Component Communication Protocols","id":"component-communication-protocols"},{"level":4,"text":"Mental Model: The Diplomatic Embassy System","id":"mental-model-the-diplomatic-embassy-system"},{"level":4,"text":"Interface Contracts and Data Exchange Formats","id":"interface-contracts-and-data-exchange-formats"},{"level":4,"text":"Message Formats and Data Structures","id":"message-formats-and-data-structures"},{"level":4,"text":"Component Interaction Patterns","id":"component-interaction-patterns"},{"level":4,"text":"Error Propagation and Recovery Protocols","id":"error-propagation-and-recovery-protocols"},{"level":4,"text":"Architecture Decision Records for Communication Design","id":"architecture-decision-records-for-communication-design"},{"level":4,"text":"Common Pitfalls in Component Communication","id":"common-pitfalls-in-component-communication"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure for Communication Management","id":"recommended-file-structure-for-communication-management"},{"level":4,"text":"Core Data Structures Implementation","id":"core-data-structures-implementation"},{"level":4,"text":"Pipeline Orchestration Implementation","id":"pipeline-orchestration-implementation"},{"level":4,"text":"Component Interface Definitions","id":"component-interface-definitions"},{"level":4,"text":"Error Handling and Diagnostic Classes","id":"error-handling-and-diagnostic-classes"},{"level":4,"text":"Milestone Checkpoints for Data Flow","id":"milestone-checkpoints-for-data-flow"},{"level":4,"text":"Debugging Data Flow Issues","id":"debugging-data-flow-issues"},{"level":2,"text":"Error Handling and Edge Cases","id":"error-handling-and-edge-cases"},{"level":3,"text":"Mental Model: The Safety Net System","id":"mental-model-the-safety-net-system"},{"level":3,"text":"Numerical Stability Issues","id":"numerical-stability-issues"},{"level":4,"text":"Division by Zero Protection","id":"division-by-zero-protection"},{"level":4,"text":"Overflow and Underflow Detection","id":"overflow-and-underflow-detection"},{"level":4,"text":"Precision Loss Prevention","id":"precision-loss-prevention"},{"level":3,"text":"Input Validation and Sanitization","id":"input-validation-and-sanitization"},{"level":4,"text":"Array Shape and Type Validation","id":"array-shape-and-type-validation"},{"level":4,"text":"Missing Value and Outlier Detection","id":"missing-value-and-outlier-detection"},{"level":4,"text":"Data Quality Metrics","id":"data-quality-metrics"},{"level":3,"text":"Optimization Failure Recovery","id":"optimization-failure-recovery"},{"level":4,"text":"Divergence Detection","id":"divergence-detection"},{"level":4,"text":"Learning Rate Adaptation","id":"learning-rate-adaptation"},{"level":4,"text":"Convergence Criteria Refinement","id":"convergence-criteria-refinement"},{"level":4,"text":"Recovery Strategy Implementation","id":"recovery-strategy-implementation"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Testing Strategy and Milestone Checkpoints","id":"testing-strategy-and-milestone-checkpoints"},{"level":3,"text":"Mental Model: The Mathematical Proof Checker","id":"mental-model-the-mathematical-proof-checker"},{"level":2,"text":"Unit Testing Approach","id":"unit-testing-approach"},{"level":3,"text":"Synthetic Data Generation Strategy","id":"synthetic-data-generation-strategy"},{"level":3,"text":"Component-Level Test Structure","id":"component-level-test-structure"},{"level":3,"text":"Mathematical Verification Approach","id":"mathematical-verification-approach"},{"level":3,"text":"Error Handling Validation","id":"error-handling-validation"},{"level":3,"text":"Common Unit Testing Pitfalls","id":"common-unit-testing-pitfalls"},{"level":2,"text":"Integration Testing","id":"integration-testing"},{"level":3,"text":"Real Dataset Integration","id":"real-dataset-integration"},{"level":3,"text":"Cross-Validation Integration","id":"cross-validation-integration"},{"level":3,"text":"Algorithm Comparison Integration","id":"algorithm-comparison-integration"},{"level":3,"text":"Pipeline Robustness Testing","id":"pipeline-robustness-testing"},{"level":3,"text":"End-to-End Workflow Validation","id":"end-to-end-workflow-validation"},{"level":3,"text":"Common Integration Testing Pitfalls","id":"common-integration-testing-pitfalls"},{"level":2,"text":"Milestone Success Checkpoints","id":"milestone-success-checkpoints"},{"level":3,"text":"Milestone 1: Simple Linear Regression Success Criteria","id":"milestone-1-simple-linear-regression-success-criteria"},{"level":3,"text":"Milestone 2: Gradient Descent Success Criteria","id":"milestone-2-gradient-descent-success-criteria"},{"level":3,"text":"Milestone 3: Multiple Linear Regression Success Criteria","id":"milestone-3-multiple-linear-regression-success-criteria"},{"level":3,"text":"Progressive Learning Validation","id":"progressive-learning-validation"},{"level":3,"text":"Common Milestone Checkpoint Pitfalls","id":"common-milestone-checkpoint-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Debugging Guide","id":"debugging-guide"},{"level":3,"text":"Mental Model: The Medical Diagnostic Process","id":"mental-model-the-medical-diagnostic-process"},{"level":3,"text":"Common Bug Patterns","id":"common-bug-patterns"},{"level":4,"text":"Data-Related Bug Patterns","id":"data-related-bug-patterns"},{"level":4,"text":"Algorithmic Bug Patterns","id":"algorithmic-bug-patterns"},{"level":4,"text":"Numerical Stability Patterns","id":"numerical-stability-patterns"},{"level":3,"text":"ML-Specific Debugging Techniques","id":"ml-specific-debugging-techniques"},{"level":4,"text":"Visualization-Based Debugging","id":"visualization-based-debugging"},{"level":4,"text":"Mathematical Verification Techniques","id":"mathematical-verification-techniques"},{"level":4,"text":"Logging and Inspection Strategies","id":"logging-and-inspection-strategies"},{"level":3,"text":"Performance and Convergence Debugging","id":"performance-and-convergence-debugging"},{"level":4,"text":"Convergence Diagnosis Framework","id":"convergence-diagnosis-framework"},{"level":4,"text":"Numerical Stability Monitoring","id":"numerical-stability-monitoring"},{"level":4,"text":"Performance Optimization Debugging","id":"performance-optimization-debugging"},{"level":4,"text":"Common Debugging Scenarios and Solutions","id":"common-debugging-scenarios-and-solutions"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations for Debugging","id":"technology-recommendations-for-debugging"},{"level":4,"text":"Debugging Infrastructure Starter Code","id":"debugging-infrastructure-starter-code"},{"level":4,"text":"Core Debugging Function Skeletons","id":"core-debugging-function-skeletons"},{"level":4,"text":"Comprehensive Testing and Validation Framework","id":"comprehensive-testing-and-validation-framework"},{"level":4,"text":"Debugging Command-Line Interface","id":"debugging-command-line-interface"},{"level":4,"text":"Language-Specific Debugging Hints","id":"language-specific-debugging-hints"},{"level":4,"text":"Milestone Debugging Checkpoints","id":"milestone-debugging-checkpoints"},{"level":2,"text":"Future Extensions and Learning Path","id":"future-extensions-and-learning-path"},{"level":3,"text":"Mental Model: The Learning Journey Map","id":"mental-model-the-learning-journey-map"},{"level":3,"text":"Immediate Extensions","id":"immediate-extensions"},{"level":4,"text":"Polynomial Features and Non-Linear Relationships","id":"polynomial-features-and-non-linear-relationships"},{"level":4,"text":"Regularization Variants and Robust Linear Models","id":"regularization-variants-and-robust-linear-models"},{"level":3,"text":"Advanced Topics Path","id":"advanced-topics-path"},{"level":4,"text":"Logistic Regression and Classification","id":"logistic-regression-and-classification"},{"level":4,"text":"Neural Networks and Deep Learning Foundations","id":"neural-networks-and-deep-learning-foundations"},{"level":4,"text":"Advanced Optimization Algorithms","id":"advanced-optimization-algorithms"},{"level":3,"text":"Production Readiness","id":"production-readiness"},{"level":4,"text":"Computational Efficiency and Scalability","id":"computational-efficiency-and-scalability"},{"level":4,"text":"Robust Production Data Handling","id":"robust-production-data-handling"},{"level":4,"text":"Model Lifecycle Management","id":"model-lifecycle-management"},{"level":4,"text":"Security and Privacy Considerations","id":"security-and-privacy-considerations"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Stack Evolution","id":"technology-stack-evolution"},{"level":4,"text":"Project Structure Evolution","id":"project-structure-evolution"},{"level":4,"text":"Polynomial Features Implementation Starter","id":"polynomial-features-implementation-starter"},{"level":4,"text":"Cross-Validation Framework Starter","id":"cross-validation-framework-starter"},{"level":4,"text":"Logistic Regression Implementation Skeleton","id":"logistic-regression-implementation-skeleton"},{"level":4,"text":"Production API Serving Starter","id":"production-api-serving-starter"},{"level":4,"text":"Milestone Checkpoints for Extensions","id":"milestone-checkpoints-for-extensions"},{"level":2,"text":"Glossary","id":"glossary"},{"level":3,"text":"Mental Model: The Language of Machine Learning","id":"mental-model-the-language-of-machine-learning"},{"level":3,"text":"Mathematical and Statistical Terms","id":"mathematical-and-statistical-terms"},{"level":3,"text":"Optimization and Algorithm Terms","id":"optimization-and-algorithm-terms"},{"level":3,"text":"Data Handling and Preprocessing Terms","id":"data-handling-and-preprocessing-terms"},{"level":3,"text":"Model Architecture and Component Terms","id":"model-architecture-and-component-terms"},{"level":3,"text":"Testing and Validation Terms","id":"testing-and-validation-terms"},{"level":3,"text":"Debugging and Error Handling Terms","id":"debugging-and-error-handling-terms"},{"level":3,"text":"Advanced Concepts and Extensions","id":"advanced-concepts-and-extensions"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended Study Approach","id":"b-recommended-study-approach"},{"level":4,"text":"C. Terminology Application in Code","id":"c-terminology-application-in-code"},{"level":4,"text":"D. Common Terminology Mistakes","id":"d-common-terminology-mistakes"},{"level":4,"text":"E. Debugging with Terminology","id":"e-debugging-with-terminology"},{"level":4,"text":"F. Milestone Terminology Checkpoints","id":"f-milestone-terminology-checkpoints"}],"title":"Linear Regression from Scratch: Design Document","markdown":"# Linear Regression from Scratch: Design Document\n\n\n## Overview\n\nThis system implements linear regression with gradient descent optimization from first principles, teaching the fundamental concepts of machine learning through hands-on implementation. The key architectural challenge is designing a flexible, educational framework that demonstrates both closed-form solutions and iterative optimization while handling single and multiple variable scenarios.\n\n\n> This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.\n\n\n## Context and Problem Statement\n\n> **Milestone(s):** All milestones (foundational understanding)\n\n### Mental Model: The Line-Fitting Detective\n\nImagine you're a detective investigating a pattern in scattered clues. You have a collection of evidence points—each piece has two measurements: the time it was found (X-coordinate) and its importance level (Y-coordinate). Your job is to find the underlying relationship that connects these clues.\n\nAs you plot these evidence points on a board, you notice they seem to follow a rough line. Some points are exactly on the line, others are scattered nearby. Your detective instincts tell you there's a pattern here, but how do you find the **best possible line** that captures this relationship? And once you find it, how can you use this line to predict the importance of future evidence based on when you find it?\n\nThis is exactly what **linear regression** does—it's a mathematical detective that finds the best-fitting line through scattered data points. But unlike our human detective who might eyeball the pattern, linear regression uses precise mathematical methods to find the optimal line that minimizes the total distance between the line and all the evidence points.\n\nThe detective has two main approaches available: the **instant analysis method** (closed-form solution) where they can immediately calculate the perfect line using a mathematical formula, and the **iterative refinement method** (gradient descent) where they start with a guess and gradually improve it until they find the best line. Both approaches lead to the same destination, but they teach us different fundamental concepts about how machines learn from data.\n\nJust as our detective needs to handle different types of cases—sometimes with just one type of evidence (simple linear regression), sometimes with multiple types of clues (multiple linear regression)—our implementation must be flexible enough to grow from basic line-fitting to complex pattern recognition while maintaining educational clarity about the underlying mathematical principles.\n\n### Mathematical Foundation\n\nThe mathematical foundation of linear regression rests on two fundamental concepts: **least squares optimization** and **gradient-based parameter estimation**. Understanding both approaches is crucial because they represent different philosophies in machine learning—analytical solutions versus iterative optimization—and each teaches distinct concepts that apply broadly across the field.\n\nThe **least squares principle** provides the theoretical foundation for what we mean by \"best fit.\" When we have data points scattered around a potential line, we need a precise definition of \"best.\" Least squares defines this as the line that minimizes the sum of squared vertical distances between each point and the line. Mathematically, if we have points (x₁, y₁), (x₂, y₂), ..., (xₙ, yₙ), and our line is ŷ = mx + b, then we want to minimize the cost function:\n\n**Cost(m, b) = (1/2n) × Σ(yᵢ - (mxᵢ + b))²**\n\nThe factor of 1/2n is included for mathematical convenience—it doesn't change the location of the minimum, but it simplifies derivative calculations. This cost function is **convex**, meaning it has a single global minimum with no local minima, which guarantees that any optimization method will find the same unique solution.\n\nFor simple linear regression, this optimization problem has a **closed-form analytical solution** derived using calculus. By setting the partial derivatives of the cost function equal to zero and solving the resulting system of equations, we obtain the **normal equations**:\n\n- Slope: **m = Σ((xᵢ - x̄)(yᵢ - ȳ)) / Σ((xᵢ - x̄)²)**\n- Intercept: **b = ȳ - m × x̄**\n\nwhere x̄ and ȳ are the means of the input and output variables respectively. This closed-form solution is computationally efficient and provides exact results, making it ideal for educational purposes because learners can immediately see the direct relationship between the data and the fitted parameters.\n\nHowever, the closed-form approach becomes computationally intractable for multiple linear regression with many features, and it doesn't generalize to more complex machine learning models. This is where **gradient descent optimization** becomes essential. Gradient descent is an iterative algorithm that starts with initial parameter guesses and repeatedly updates them in the direction that most rapidly decreases the cost function.\n\nThe gradient descent update rules are:\n- **m_new = m_old - α × ∂Cost/∂m**\n- **b_new = b_old - α × ∂Cost/∂b**\n\nwhere α is the **learning rate** that controls the step size. The partial derivatives (gradients) for linear regression are:\n- **∂Cost/∂m = (1/n) × Σ((mx_i + b - y_i) × x_i)**\n- **∂Cost/∂b = (1/n) × Σ(mx_i + b - y_i)**\n\nThe learning rate α is a critical hyperparameter that balances convergence speed against stability. Too small, and the algorithm converges slowly; too large, and it may overshoot the minimum and diverge. Understanding this trade-off is fundamental to all iterative optimization in machine learning.\n\n**Convergence detection** is another crucial concept. We stop iterating when the improvement in cost between consecutive iterations falls below a threshold ε, indicating we've reached the minimum: **|Cost_new - Cost_old| < ε**. This teaches learners about numerical precision, stopping criteria, and the practical aspects of optimization algorithms.\n\nThe **matrix formulation** becomes essential for multiple linear regression. Instead of separate slope and intercept parameters, we work with a weight vector **w** and express predictions as **ŷ = Xw**, where X is the design matrix that includes a column of ones for the bias term. The gradient descent update becomes: **w_new = w_old - α × X^T(Xw_old - y) / n**, demonstrating vectorization and efficient computation.\n\nBoth approaches—closed-form and iterative—solve the same mathematical problem but teach complementary skills. The closed-form solution teaches the mathematical foundations and provides insight into the statistical properties of linear regression. Gradient descent teaches optimization principles, numerical methods, and computational approaches that scale to complex machine learning models where closed-form solutions don't exist.\n\n### Existing Approaches Comparison\n\nUnderstanding the landscape of existing approaches to linear regression helps learners appreciate why building from scratch provides educational value and how our implementation relates to production tools they'll encounter in their careers.\n\n**Closed-Form Analytical Solutions** represent the classical statistical approach to linear regression. Libraries like NumPy's `numpy.linalg.lstsq` and the normal equation implementation in basic statistics packages fall into this category. The mathematical approach directly computes optimal parameters using matrix operations: **w = (X^T X)^(-1) X^T y** for the general case.\n\n| Aspect | Advantages | Disadvantages |\n|--------|------------|---------------|\n| **Computational Complexity** | O(n) for simple regression, exact solution in one step | O(n³) for matrix inversion in multiple regression, becomes intractable with many features |\n| **Numerical Stability** | Deterministic, no hyperparameters to tune | Susceptible to numerical issues when X^T X is near-singular |\n| **Educational Value** | Clear mathematical foundation, direct connection between data and parameters | Doesn't teach optimization concepts crucial for advanced ML |\n| **Scalability** | Works well for small to medium datasets | Memory requirements grow quadratically with feature count |\n| **Generalization** | Limited to linear models with analytical solutions | Doesn't extend to neural networks, logistic regression, or other iterative models |\n\n**Iterative Optimization Methods** encompass gradient descent and its variants, which form the backbone of modern machine learning. This includes basic gradient descent, stochastic gradient descent (SGD), and advanced optimizers like Adam and RMSprop used in deep learning frameworks.\n\n| Aspect | Advantages | Disadvantages |\n|--------|------------|---------------|\n| **Computational Complexity** | O(n) per iteration, scales linearly with data size | Requires multiple iterations to converge, total complexity depends on convergence rate |\n| **Memory Efficiency** | Constant memory usage regardless of feature count | Requires storage for gradients and potentially momentum terms |\n| **Educational Value** | Teaches fundamental optimization concepts used throughout ML | More complex to implement and debug correctly |\n| **Extensibility** | Generalizes to all differentiable models | Requires understanding of calculus and numerical optimization |\n| **Hyperparameter Sensitivity** | Flexible, can be tuned for different problems | Requires careful tuning of learning rate and convergence criteria |\n\n**Black-Box Production Libraries** like scikit-learn, TensorFlow, and PyTorch provide highly optimized implementations with extensive features, automatic hyperparameter tuning, and production-ready performance optimizations.\n\n| Aspect | Advantages | Disadvantages |\n|--------|------------|---------------|\n| **Development Speed** | Single function call, extensive documentation | No understanding of underlying mechanisms |\n| **Performance** | Highly optimized, handles edge cases automatically | Difficult to debug when things go wrong |\n| **Feature Completeness** | Cross-validation, regularization, multiple solvers | Overwhelming options for beginners |\n| **Production Readiness** | Battle-tested, handles real-world data issues | Hides important implementation details |\n| **Learning Curve** | Easy to use for simple cases | Steep learning curve for advanced features, black-box behavior |\n\n> **Key Insight**: The fundamental trade-off is between **educational transparency** and **production efficiency**. Our from-scratch implementation prioritizes understanding over performance, making explicit the concepts that production libraries abstract away.\n\nOur implementation strategy deliberately chooses educational value over performance optimization. We implement both closed-form and iterative approaches to demonstrate the conceptual bridge between classical statistics and modern machine learning optimization. This dual approach ensures learners understand when to apply each method and why both exist in the broader ecosystem.\n\nThe **progression from simple to complex** mirrors how the field of machine learning evolved historically. Simple linear regression with closed-form solutions represents the foundation of statistical learning theory. Gradient descent represents the computational revolution that enabled complex models. Multiple linear regression with vectorization demonstrates the mathematical sophistication needed for high-dimensional problems. This progression prepares learners for advanced topics like neural networks, where gradient descent is the only viable optimization approach.\n\n**Architectural Philosophy**: Rather than choosing one approach over another, our implementation demonstrates how different mathematical and computational strategies solve the same underlying problem. This comparative approach helps learners develop intuition about when to apply different techniques—use closed-form solutions when they exist and are computationally feasible, fall back to iterative optimization for complex models or large datasets.\n\nThe implementation serves as a **bridge between theory and practice**. Unlike production libraries that hide complexity, and unlike pure mathematical treatments that avoid implementation details, our approach makes both the mathematics and the computational considerations explicit and understandable. This prepares learners to both understand the tools they'll use professionally and to implement novel approaches when existing tools don't meet their needs.\n\n### Implementation Guidance\n\nThe implementation of linear regression from scratch serves as a foundation for understanding both classical statistics and modern machine learning. This section provides concrete guidance for building an educational system that demonstrates mathematical concepts through working code.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| **Core Computation** | Pure Python with lists/loops | NumPy arrays with vectorized operations |\n| **Data Handling** | CSV reading with built-in `csv` module | Pandas DataFrames with robust type handling |\n| **Visualization** | Matplotlib with basic plotting | Seaborn for statistical visualizations |\n| **Testing** | Built-in `unittest` framework | Pytest with parametrized tests |\n| **Numerical Precision** | Python `float` (sufficient for learning) | NumPy `float64` for numerical stability |\n\n**Recommendation**: Start with NumPy as the core computational engine. While pure Python lists would be more educational for understanding loops and basic operations, NumPy's vectorized operations are essential for multiple linear regression and provide better numerical stability. The performance benefits also make experimentation more enjoyable.\n\n#### Recommended Project Structure\n\n```\nlinear-regression-from-scratch/\n│\n├── src/\n│   ├── __init__.py\n│   ├── data_handler.py           ← Data loading, validation, preprocessing\n│   ├── simple_regression.py      ← Milestone 1: Simple linear regression\n│   ├── gradient_descent.py       ← Milestone 2: Optimization engine\n│   ├── multiple_regression.py    ← Milestone 3: Multiple features\n│   └── evaluation.py             ← R-squared, visualization utilities\n│\n├── tests/\n│   ├── __init__.py\n│   ├── test_data_handler.py      ← Data loading and preprocessing tests\n│   ├── test_simple_regression.py ← Simple regression validation\n│   ├── test_gradient_descent.py  ← Optimization algorithm tests\n│   └── test_integration.py       ← End-to-end workflow tests\n│\n├── examples/\n│   ├── synthetic_data.py         ← Generate test datasets\n│   ├── milestone_1_demo.py       ← Simple regression example\n│   ├── milestone_2_demo.py       ← Gradient descent example\n│   └── milestone_3_demo.py       ← Multiple regression example\n│\n├── data/\n│   ├── housing.csv               ← Real dataset for testing\n│   └── synthetic.csv             ← Generated data with known parameters\n│\n└── requirements.txt              ← numpy, matplotlib, pytest\n```\n\nThis structure separates concerns clearly and mirrors the learning progression. Each milestone builds on previous components without modifying them, demonstrating good software engineering practices.\n\n#### Infrastructure Starter Code\n\n**Data Loading and Validation Utilities** (`src/data_handler.py`):\n\n```python\nimport numpy as np\nimport csv\nfrom typing import Tuple, List, Optional, Union\n\ndef load_csv_data(filename: str, feature_columns: List[str], \n                  target_column: str) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Load feature and target data from CSV file.\n    \n    Args:\n        filename: Path to CSV file\n        feature_columns: List of column names to use as features\n        target_column: Name of target variable column\n    \n    Returns:\n        Tuple of (features, targets) as numpy arrays\n    \n    Raises:\n        FileNotFoundError: If CSV file doesn't exist\n        ValueError: If columns are missing or data is invalid\n    \"\"\"\n    features = []\n    targets = []\n    \n    with open(filename, 'r', newline='') as csvfile:\n        reader = csv.DictReader(csvfile)\n        \n        # Validate columns exist\n        if not all(col in reader.fieldnames for col in feature_columns):\n            missing = [col for col in feature_columns if col not in reader.fieldnames]\n            raise ValueError(f\"Missing feature columns: {missing}\")\n        \n        if target_column not in reader.fieldnames:\n            raise ValueError(f\"Missing target column: {target_column}\")\n        \n        # Read data rows\n        for row_num, row in enumerate(reader, start=2):  # Start at 2 for header\n            try:\n                feature_values = [float(row[col]) for col in feature_columns]\n                target_value = float(row[target_column])\n                \n                features.append(feature_values)\n                targets.append(target_value)\n                \n            except ValueError as e:\n                raise ValueError(f\"Invalid numeric data in row {row_num}: {e}\")\n    \n    if len(features) == 0:\n        raise ValueError(\"No valid data rows found\")\n    \n    return np.array(features), np.array(targets)\n\ndef validate_data(features: np.ndarray, targets: np.ndarray) -> None:\n    \"\"\"\n    Validate that feature and target arrays are compatible for regression.\n    \n    Args:\n        features: Feature matrix (n_samples, n_features)\n        targets: Target vector (n_samples,)\n    \n    Raises:\n        ValueError: If data has incompatible shapes or contains invalid values\n    \"\"\"\n    if features.ndim != 2:\n        raise ValueError(f\"Features must be 2D array, got {features.ndim}D\")\n    \n    if targets.ndim != 1:\n        raise ValueError(f\"Targets must be 1D array, got {targets.ndim}D\")\n    \n    if features.shape[0] != targets.shape[0]:\n        raise ValueError(f\"Feature count {features.shape[0]} != target count {targets.shape[0]}\")\n    \n    if features.shape[0] < 2:\n        raise ValueError(\"Need at least 2 data points for regression\")\n    \n    if np.any(np.isnan(features)) or np.any(np.isnan(targets)):\n        raise ValueError(\"Data contains NaN values\")\n    \n    if np.any(np.isinf(features)) or np.any(np.isinf(targets)):\n        raise ValueError(\"Data contains infinite values\")\n\ndef normalize_features(features: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Normalize features to zero mean and unit variance (z-score normalization).\n    \n    Args:\n        features: Raw feature matrix (n_samples, n_features)\n    \n    Returns:\n        Tuple of (normalized_features, means, stds) where:\n        - normalized_features: Features scaled to mean=0, std=1\n        - means: Original mean of each feature (for inverse transform)\n        - stds: Original std of each feature (for inverse transform)\n    \"\"\"\n    means = np.mean(features, axis=0)\n    stds = np.std(features, axis=0)\n    \n    # Handle constant features (std = 0)\n    stds = np.where(stds == 0, 1.0, stds)\n    \n    normalized = (features - means) / stds\n    return normalized, means, stds\n```\n\n**Synthetic Data Generation** (`examples/synthetic_data.py`):\n\n```python\nimport numpy as np\nfrom typing import Tuple\n\ndef generate_linear_data(n_samples: int, slope: float, intercept: float, \n                        noise_std: float = 0.1, x_range: Tuple[float, float] = (0, 10)) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Generate synthetic data following y = slope * x + intercept + noise.\n    \n    Args:\n        n_samples: Number of data points to generate\n        slope: True slope parameter\n        intercept: True intercept parameter  \n        noise_std: Standard deviation of Gaussian noise\n        x_range: (min, max) range for x values\n    \n    Returns:\n        Tuple of (x_values, y_values) as 1D numpy arrays\n    \"\"\"\n    np.random.seed(42)  # Reproducible results for testing\n    \n    x_min, x_max = x_range\n    x = np.random.uniform(x_min, x_max, n_samples)\n    \n    # Perfect linear relationship\n    y_perfect = slope * x + intercept\n    \n    # Add Gaussian noise\n    noise = np.random.normal(0, noise_std, n_samples)\n    y = y_perfect + noise\n    \n    return x, y\n\ndef generate_multiple_linear_data(n_samples: int, coefficients: np.ndarray, \n                                intercept: float, noise_std: float = 0.1) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Generate synthetic data for multiple linear regression.\n    \n    Args:\n        n_samples: Number of data points\n        coefficients: True coefficient vector (one per feature)\n        intercept: True intercept parameter\n        noise_std: Standard deviation of Gaussian noise\n    \n    Returns:\n        Tuple of (feature_matrix, targets) where feature_matrix is (n_samples, n_features)\n    \"\"\"\n    np.random.seed(42)\n    \n    n_features = len(coefficients)\n    \n    # Generate random features (standardized)\n    features = np.random.randn(n_samples, n_features)\n    \n    # Perfect linear relationship\n    y_perfect = np.dot(features, coefficients) + intercept\n    \n    # Add noise\n    noise = np.random.normal(0, noise_std, n_samples)\n    y = y_perfect + noise\n    \n    return features, y\n```\n\n#### Core Algorithm Skeletons\n\n**Simple Linear Regression** (`src/simple_regression.py`):\n\n```python\nimport numpy as np\nfrom typing import Tuple\n\nclass SimpleLinearRegression:\n    \"\"\"Simple linear regression using closed-form ordinary least squares solution.\"\"\"\n    \n    def __init__(self):\n        self.slope_ = None\n        self.intercept_ = None\n        self.is_fitted_ = False\n    \n    def fit(self, x: np.ndarray, y: np.ndarray) -> 'SimpleLinearRegression':\n        \"\"\"\n        Fit simple linear regression model using closed-form solution.\n        \n        Args:\n            x: Input features (1D array of shape n_samples)\n            y: Target values (1D array of shape n_samples)\n        \n        Returns:\n            self (fitted model)\n        \"\"\"\n        # TODO 1: Validate input arrays have same length and are 1D\n        # TODO 2: Check for minimum number of samples (at least 2)\n        # TODO 3: Calculate mean of x and y using np.mean()\n        # TODO 4: Calculate slope using formula: sum((x - x_mean) * (y - y_mean)) / sum((x - x_mean)^2)\n        # TODO 5: Calculate intercept using formula: y_mean - slope * x_mean\n        # TODO 6: Set is_fitted_ to True and store parameters\n        # Hint: Use np.sum() for summations, be careful about division by zero\n        \n        return self\n    \n    def predict(self, x: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Make predictions using fitted linear model.\n        \n        Args:\n            x: Input values to predict on (1D array)\n        \n        Returns:\n            Predicted values (1D array same length as x)\n        \n        Raises:\n            RuntimeError: If model hasn't been fitted yet\n        \"\"\"\n        # TODO 1: Check if model is fitted, raise RuntimeError if not\n        # TODO 2: Apply linear equation: y = slope * x + intercept\n        # TODO 3: Return predictions as numpy array\n        \n        pass\n    \n    def score(self, x: np.ndarray, y: np.ndarray) -> float:\n        \"\"\"\n        Calculate R-squared coefficient of determination.\n        \n        Args:\n            x: Input features\n            y: True target values\n        \n        Returns:\n            R-squared score (between 0 and 1 for reasonable models)\n        \"\"\"\n        # TODO 1: Generate predictions using self.predict(x)\n        # TODO 2: Calculate total sum of squares: sum((y - y_mean)^2)\n        # TODO 3: Calculate residual sum of squares: sum((y - predictions)^2)  \n        # TODO 4: Calculate R-squared: 1 - (residual_ss / total_ss)\n        # TODO 5: Handle edge case where total_ss = 0 (constant y values)\n        \n        pass\n```\n\n**Gradient Descent Optimizer** (`src/gradient_descent.py`):\n\n```python\nimport numpy as np\nfrom typing import List, Tuple, Optional\n\nclass GradientDescentRegression:\n    \"\"\"Linear regression using gradient descent optimization.\"\"\"\n    \n    def __init__(self, learning_rate: float = 0.01, max_iterations: int = 1000, \n                 tolerance: float = 1e-6):\n        self.learning_rate = learning_rate\n        self.max_iterations = max_iterations\n        self.tolerance = tolerance\n        \n        # Parameters (set during fitting)\n        self.slope_ = None\n        self.intercept_ = None\n        self.is_fitted_ = False\n        \n        # Training history (for debugging and visualization)\n        self.cost_history_: List[float] = []\n        self.parameter_history_: List[Tuple[float, float]] = []\n    \n    def _compute_cost(self, x: np.ndarray, y: np.ndarray, slope: float, intercept: float) -> float:\n        \"\"\"\n        Compute mean squared error cost function.\n        \n        Args:\n            x: Input features\n            y: Target values  \n            slope: Current slope parameter\n            intercept: Current intercept parameter\n        \n        Returns:\n            Mean squared error cost\n        \"\"\"\n        # TODO 1: Calculate predictions using current parameters: y_pred = slope * x + intercept\n        # TODO 2: Calculate residuals: residuals = y - y_pred\n        # TODO 3: Calculate mean squared error: (1/2n) * sum(residuals^2)\n        # TODO 4: Return cost as float\n        # Hint: Factor of 1/2 simplifies gradients, n is len(x)\n        \n        pass\n    \n    def _compute_gradients(self, x: np.ndarray, y: np.ndarray, slope: float, intercept: float) -> Tuple[float, float]:\n        \"\"\"\n        Compute gradients of cost function with respect to parameters.\n        \n        Args:\n            x: Input features\n            y: Target values\n            slope: Current slope parameter  \n            intercept: Current intercept parameter\n        \n        Returns:\n            Tuple of (gradient_slope, gradient_intercept)\n        \"\"\"\n        # TODO 1: Calculate predictions: y_pred = slope * x + intercept\n        # TODO 2: Calculate prediction errors: errors = y_pred - y\n        # TODO 3: Calculate gradient w.r.t slope: (1/n) * sum(errors * x)\n        # TODO 4: Calculate gradient w.r.t intercept: (1/n) * sum(errors)  \n        # TODO 5: Return both gradients as tuple\n        # Hint: These are partial derivatives of MSE cost function\n        \n        pass\n    \n    def fit(self, x: np.ndarray, y: np.ndarray) -> 'GradientDescentRegression':\n        \"\"\"\n        Fit linear regression using gradient descent optimization.\n        \n        Args:\n            x: Input features (1D array)\n            y: Target values (1D array)\n        \n        Returns:\n            self (fitted model)\n        \"\"\"\n        # TODO 1: Validate inputs (same as simple regression)\n        # TODO 2: Initialize parameters - slope=0, intercept=0 (or random small values)\n        # TODO 3: Clear history lists for this training run\n        # TODO 4: Main gradient descent loop:\n        #   a. Compute current cost and store in history\n        #   b. Compute gradients for both parameters  \n        #   c. Update parameters: param = param - learning_rate * gradient\n        #   d. Store current parameters in history\n        #   e. Check convergence: if cost improvement < tolerance, break\n        #   f. Check max iterations to prevent infinite loops\n        # TODO 5: Set is_fitted_ = True\n        # TODO 6: Return self\n        \n        return self\n    \n    def predict(self, x: np.ndarray) -> np.ndarray:\n        \"\"\"Make predictions (same interface as SimpleLinearRegression).\"\"\"\n        # TODO: Same implementation as SimpleLinearRegression.predict()\n        pass\n    \n    def score(self, x: np.ndarray, y: np.ndarray) -> float:\n        \"\"\"Calculate R-squared (same interface as SimpleLinearRegression).\"\"\"\n        # TODO: Same implementation as SimpleLinearRegression.score()\n        pass\n```\n\n#### Milestone Checkpoints\n\n**Milestone 1 Checkpoint**: After implementing SimpleLinearRegression, run this validation:\n\n```python\n# Create synthetic data with known parameters\nx, y = generate_linear_data(100, slope=2.5, intercept=1.0, noise_std=0.1)\n\n# Fit model and check parameters are close to true values\nmodel = SimpleLinearRegression()\nmodel.fit(x, y)\n\nprint(f\"Fitted slope: {model.slope_:.3f} (true: 2.5)\")\nprint(f\"Fitted intercept: {model.intercept_:.3f} (true: 1.0)\")\nprint(f\"R-squared: {model.score(x, y):.3f}\")\n\n# Expected output:\n# Fitted slope: ~2.5 (within 0.1)\n# Fitted intercept: ~1.0 (within 0.1)  \n# R-squared: >0.95\n```\n\n**Milestone 2 Checkpoint**: After implementing GradientDescentRegression:\n\n```python\n# Same synthetic data\nmodel_gd = GradientDescentRegression(learning_rate=0.01, max_iterations=1000)\nmodel_gd.fit(x, y)\n\n# Should match closed-form solution\nmodel_closed = SimpleLinearRegression()\nmodel_closed.fit(x, y)\n\nprint(f\"Gradient descent slope: {model_gd.slope_:.3f}\")\nprint(f\"Closed-form slope: {model_closed.slope_:.3f}\")\nprint(f\"Cost decreased: {model_gd.cost_history_[0]:.3f} -> {model_gd.cost_history_[-1]:.3f}\")\n\n# Expected: Parameters match within 0.01, cost decreases monotonically\n```\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|-------------|-----------------|-----|\n| Division by zero error | All x values are the same | Print `np.var(x)` - should be > 0 | Check data loading, ensure x has variation |\n| Gradient descent diverges | Learning rate too high | Plot cost history - should decrease | Reduce learning rate by factor of 10 |\n| R-squared is negative | Predictions worse than mean | Compare predictions vs actual values | Check model fitting, data quality |\n| Parameters don't match expected | Data scaling issues | Print data statistics (min, max, std) | Normalize features before fitting |\n| Slow convergence | Learning rate too small | Check number of iterations to converge | Increase learning rate gradually |\n\n\n## Goals and Non-Goals\n\n> **Milestone(s):** All milestones (defines the scope and learning objectives for the entire project)\n\nThis section establishes the educational foundation and scope boundaries for our linear regression implementation. Unlike production machine learning systems that optimize for performance and feature completeness, this project prioritizes understanding and pedagogical clarity. We explicitly define what concepts learners should master, what capabilities the system should demonstrate, and which advanced topics remain outside our scope to maintain focus on fundamental principles.\n\n### Learning Goals\n\nThe primary educational objective is to demystify machine learning by implementing linear regression from mathematical first principles. Rather than treating machine learning as a black box where data goes in and predictions come out, learners will understand every step of the process, from the underlying mathematical relationships to the computational algorithms that optimize model parameters.\n\n**Mathematical Foundation Mastery** represents the cornerstone of our learning objectives. Learners should develop an intuitive understanding of what it means to \"fit a line to data\" beyond simply calling a library function. This includes grasping the least squares principle—that we seek parameters minimizing the sum of squared prediction errors—and understanding why this particular loss function makes mathematical and practical sense. The relationship between the closed-form normal equation solution and gradient descent optimization should become clear, with learners appreciating that both approaches solve the same underlying mathematical problem through different computational strategies.\n\n| Learning Concept | Concrete Understanding | Assessment Criteria |\n|------------------|----------------------|-------------------|\n| Least Squares Principle | Can explain why we minimize squared errors rather than absolute errors | Describes mathematical properties: differentiability, unique solutions, statistical interpretation |\n| Gradient Descent Intuition | Visualizes parameter space as a landscape with cost function as elevation | Can predict behavior: learning rate too high causes oscillation, too low causes slow convergence |\n| Feature Scaling Impact | Understands why features need similar scales for gradient descent | Recognizes when normalization is needed and can implement z-score standardization |\n| Overfitting Recognition | Identifies when models memorize training data vs learning patterns | Explains bias-variance tradeoff and role of regularization |\n| R-squared Interpretation | Interprets coefficient of determination as explained variance proportion | Distinguishes between correlation and causation, recognizes R-squared limitations |\n\n**Algorithmic Thinking Development** forms the second pillar of our educational approach. Machine learning involves systematic problem-solving procedures, and learners should understand these algorithms as step-by-step processes rather than magical transformations. The gradient descent algorithm particularly exemplifies iterative optimization—a fundamental concept appearing throughout computer science and engineering. Learners should internalize the general pattern: define an objective function, compute gradients indicating improvement direction, update parameters incrementally, and iterate until convergence.\n\n**Implementation Skills and Best Practices** ensure learners can translate mathematical understanding into working code. This involves more than syntax—learners should develop intuition for numerical computation challenges like floating-point precision, matrix operations efficiency, and algorithm convergence detection. The progression from scalar operations (single-variable regression) to vectorized matrix operations (multiple regression) teaches computational thinking patterns essential for scientific computing and data analysis.\n\n> **Key Insight**: The goal is not to build the fastest or most feature-complete regression implementation, but to construct understanding. Every design decision prioritizes clarity and educational value over computational efficiency or production readiness.\n\n### Implementation Goals\n\nOur system should demonstrate core machine learning capabilities while maintaining transparency in its operations. The implementation goals balance functionality with educational accessibility, ensuring learners can trace every computation from input data to final predictions.\n\n**Functional Capability Requirements** define what our system must accomplish to provide a complete learning experience. The system should handle the full machine learning workflow: data loading and preprocessing, model training with parameter optimization, prediction generation for new inputs, and model evaluation with interpretable metrics. This end-to-end functionality helps learners understand machine learning as an integrated process rather than disconnected steps.\n\n| System Capability | Implementation Requirement | Educational Purpose |\n|-------------------|---------------------------|-------------------|\n| Data Loading | Read CSV files, handle missing values, validate input formats | Realistic data handling experience, error management |\n| Single Variable Regression | Implement closed-form solution using normal equation | Mathematical foundation without optimization complexity |\n| Gradient Descent Optimization | Iterative parameter updates with configurable learning rate | Algorithmic thinking, convergence understanding |\n| Multiple Feature Regression | Matrix operations, vectorized computations | Computational scaling, linear algebra application |\n| Feature Normalization | Z-score standardization, feature scaling | Data preprocessing importance, numerical stability |\n| Model Evaluation | R-squared calculation, prediction accuracy assessment | Model validation, statistical interpretation |\n| L2 Regularization | Ridge regression penalty term | Overfitting prevention, bias-variance management |\n\n**Algorithm Implementation Standards** ensure our code serves as an effective teaching tool. Each algorithm should be implemented clearly and directly, avoiding clever optimizations that obscure the underlying logic. Function names should reflect mathematical concepts (`fit`, `predict`, `score`), and internal calculations should mirror textbook formulations where possible. The gradient descent implementation particularly should expose intermediate values like cost history and parameter evolution, enabling learners to visualize the optimization process.\n\n**Extensibility and Experimentation Support** allows learners to explore beyond the basic implementation. The system architecture should accommodate natural extensions like polynomial features, different regularization penalties, or alternative optimization algorithms. This extensibility reinforces understanding by encouraging learners to modify and experiment with the core algorithms.\n\n> **Design Principle**: Every function should be simple enough that a learner can implement it from scratch with mathematical understanding and basic programming skills. Complex optimizations that require advanced algorithmic knowledge violate this principle.\n\n### Explicit Non-Goals\n\nClearly defining what we will not implement prevents scope creep and maintains focus on fundamental concepts. These non-goals are not value judgments—many represent important machine learning topics—but rather boundary setting to ensure depth over breadth in our educational approach.\n\n**Advanced Machine Learning Algorithms** remain outside our scope despite their practical importance. Logistic regression, although conceptually similar to linear regression, introduces classification concepts and sigmoid functions that complicate the mathematical foundation we're establishing. Neural networks, support vector machines, and ensemble methods involve substantially different optimization landscapes and would dilute focus from understanding gradient-based parameter learning in its simplest form.\n\n| Excluded Topic | Rationale for Exclusion | Suggested Learning Sequence |\n|----------------|------------------------|----------------------------|\n| Logistic Regression | Introduces classification, sigmoid functions, maximum likelihood | Natural next step after mastering linear regression |\n| Neural Networks | Multi-layer optimization, backpropagation complexity | Requires solid gradient descent foundation first |\n| Ensemble Methods | Meta-learning concepts, model combination strategies | Advanced topic after individual algorithm mastery |\n| Support Vector Machines | Convex optimization, kernel methods, different mathematical framework | Alternative approach after regression understanding |\n| Time Series Analysis | Temporal dependencies, autoregression, stationarity concepts | Specialized application requiring regression foundation |\n\n**Production Optimization and Scalability** explicitly fall outside our educational mission. Real-world machine learning systems require sophisticated optimizations like stochastic gradient descent, mini-batch processing, distributed computation, and specialized linear algebra libraries. These optimizations obscure the fundamental algorithms we're teaching and introduce engineering complexity unrelated to machine learning understanding.\n\n**Advanced Statistical Concepts** would enhance our models but distract from core algorithmic learning. Hypothesis testing for parameter significance, confidence intervals for predictions, and advanced model selection criteria represent important statistical machine learning topics. However, these concepts require substantial statistical background and shift focus from computational algorithm understanding to statistical inference theory.\n\n**Data Engineering and Pipeline Complexity** exceed our scope despite their practical necessity. Real machine learning projects involve data cleaning, feature engineering, missing value imputation strategies, outlier detection, and data validation pipelines. While we include basic data loading and normalization, comprehensive data preprocessing would overwhelm learners with engineering concerns before they understand the core algorithms.\n\n> **Scope Boundary Principle**: If a feature requires more than introductory mathematics (calculus and linear algebra) or more than basic programming concepts (loops, functions, arrays), it likely belongs in a follow-up project rather than this foundational implementation.\n\n**Performance Optimization and Numerical Libraries** represent another explicit non-goal. While NumPy provides essential array operations, we avoid advanced numerical optimization libraries like SciPy's optimization modules or specialized linear algebra routines. Our implementations should remain comprehensible and traceable, even if they sacrifice computational efficiency. This pedagogical approach helps learners understand what optimized libraries actually accomplish when they eventually use them in production contexts.\n\nThe distinction between learning goals and non-goals creates a focused educational experience. Learners completing this project should possess solid intuition for linear regression mathematics, practical experience with gradient-based optimization, and implementation skills transferable to more advanced machine learning algorithms. They should understand exactly what happens inside the \"black box\" of linear regression, providing a foundation for both using machine learning libraries effectively and implementing more sophisticated algorithms from scratch.\n\n### Implementation Guidance\n\nThis implementation guidance provides concrete technology recommendations and starter code to help learners focus on the core machine learning concepts rather than getting stuck on infrastructure details.\n\n**A. Technology Recommendations**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Data Handling | Pure NumPy arrays + CSV module | Pandas DataFrames with advanced preprocessing |\n| Linear Algebra | NumPy basic operations (dot, transpose) | SciPy sparse matrices and optimized BLAS |\n| Visualization | Matplotlib basic plotting | Seaborn statistical plots + interactive widgets |\n| Testing | Built-in assert statements + manual verification | pytest with parameterized tests and fixtures |\n| Documentation | Inline comments + docstrings | Sphinx documentation with mathematical notation |\n\nFor this educational project, choose the simple options. The goal is understanding the algorithms, not mastering data science toolchains.\n\n**B. Recommended File Structure**\n\nOrganize your project to mirror the learning progression and separate concerns clearly:\n\n```\nlinear-regression/\n├── data/\n│   ├── synthetic_data.csv          ← Generated datasets for testing\n│   └── real_data.csv               ← Optional real-world dataset\n├── src/\n│   ├── __init__.py\n│   ├── data_handler.py             ← Data loading, validation, normalization\n│   ├── simple_regression.py        ← Milestone 1: Single variable, closed-form\n│   ├── gradient_descent.py         ← Milestone 2: Optimization algorithm\n│   ├── multiple_regression.py      ← Milestone 3: Multi-variable extension\n│   └── utils.py                    ← Helper functions, evaluation metrics\n├── tests/\n│   ├── test_data_handler.py\n│   ├── test_simple_regression.py\n│   ├── test_gradient_descent.py\n│   └── test_multiple_regression.py\n├── notebooks/\n│   ├── milestone1_exploration.ipynb ← Interactive testing and visualization\n│   ├── milestone2_optimization.ipynb\n│   └── milestone3_comparison.ipynb\n└── main.py                         ← Demo script showing complete workflow\n```\n\n**C. Infrastructure Starter Code**\n\nHere's complete, working infrastructure code that handles the non-learning aspects:\n\n**utils.py** (Complete implementation):\n```python\nimport numpy as np\nimport csv\nfrom typing import Tuple, Optional\n\ndef generate_linear_data(n_samples: int, slope: float, intercept: float, \n                        noise_std: float, x_range: Tuple[float, float] = (-10, 10)) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Generate synthetic linear data with controlled noise for testing.\"\"\"\n    np.random.seed(42)  # Reproducible results\n    x = np.random.uniform(x_range[0], x_range[1], n_samples)\n    y = slope * x + intercept + np.random.normal(0, noise_std, n_samples)\n    return x.reshape(-1, 1), y\n\ndef load_csv_data(filename: str, feature_columns: list, target_column: str) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Load feature and target data from CSV file.\"\"\"\n    features = []\n    targets = []\n    \n    with open(filename, 'r') as file:\n        reader = csv.DictReader(file)\n        for row in reader:\n            feature_values = [float(row[col]) for col in feature_columns]\n            target_value = float(row[target_column])\n            features.append(feature_values)\n            targets.append(target_value)\n    \n    return np.array(features), np.array(targets)\n\ndef validate_data(features: np.ndarray, targets: np.ndarray) -> None:\n    \"\"\"Validate that features and targets are compatible for regression.\"\"\"\n    if len(features.shape) != 2:\n        raise ValueError(f\"Features must be 2D array, got shape {features.shape}\")\n    if len(targets.shape) != 1:\n        raise ValueError(f\"Targets must be 1D array, got shape {targets.shape}\")\n    if features.shape[0] != targets.shape[0]:\n        raise ValueError(f\"Feature samples ({features.shape[0]}) != target samples ({targets.shape[0]})\")\n    if features.shape[0] < 2:\n        raise ValueError(f\"Need at least 2 samples for regression, got {features.shape[0]}\")\n\ndef plot_regression_results(x: np.ndarray, y: np.ndarray, predictions: np.ndarray, title: str) -> None:\n    \"\"\"Visualize regression fit (requires matplotlib).\"\"\"\n    try:\n        import matplotlib.pyplot as plt\n        plt.figure(figsize=(10, 6))\n        plt.scatter(x.flatten(), y, alpha=0.6, label='Actual Data')\n        plt.plot(x.flatten(), predictions, 'r-', linewidth=2, label='Fitted Line')\n        plt.xlabel('Feature Value')\n        plt.ylabel('Target Value')\n        plt.title(title)\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        plt.show()\n    except ImportError:\n        print(\"Matplotlib not available - skipping visualization\")\n```\n\n**D. Core Logic Skeleton Code**\n\nHere are the skeleton implementations for the main learning components:\n\n**simple_regression.py** (Skeleton for Milestone 1):\n```python\nimport numpy as np\nfrom typing import Optional\n\nclass SimpleLinearRegression:\n    \"\"\"Single-variable linear regression using closed-form solution.\"\"\"\n    \n    def __init__(self):\n        self.slope_: Optional[float] = None\n        self.intercept_: Optional[float] = None\n        self.is_fitted_: bool = False\n    \n    def fit(self, x: np.ndarray, y: np.ndarray) -> 'SimpleLinearRegression':\n        \"\"\"Fit linear regression using ordinary least squares closed-form solution.\n        \n        The normal equation gives us: slope = cov(x,y) / var(x), intercept = mean(y) - slope * mean(x)\n        \"\"\"\n        # TODO 1: Validate input data shapes and types\n        # TODO 2: Extract x and y as 1D arrays (handle single feature case)\n        # TODO 3: Calculate sample means of x and y\n        # TODO 4: Calculate covariance of x and y: sum((x - x_mean) * (y - y_mean)) / (n - 1)\n        # TODO 5: Calculate variance of x: sum((x - x_mean)^2) / (n - 1)  \n        # TODO 6: Compute slope as covariance / variance (handle zero variance case)\n        # TODO 7: Compute intercept as y_mean - slope * x_mean\n        # TODO 8: Set is_fitted_ = True and store parameters\n        # TODO 9: Return self for method chaining\n        pass\n    \n    def predict(self, x: np.ndarray) -> np.ndarray:\n        \"\"\"Generate predictions using fitted parameters: y = slope * x + intercept.\"\"\"\n        # TODO 1: Check if model is fitted (raise ValueError if not)\n        # TODO 2: Validate input shape matches training data\n        # TODO 3: Apply linear equation: predictions = slope_ * x + intercept_\n        # TODO 4: Return predictions as numpy array\n        pass\n    \n    def score(self, x: np.ndarray, y: np.ndarray) -> float:\n        \"\"\"Calculate R-squared coefficient of determination.\"\"\"\n        # TODO 1: Generate predictions for input x\n        # TODO 2: Calculate sum of squared residuals: sum((y - predictions)^2)\n        # TODO 3: Calculate total sum of squares: sum((y - mean(y))^2)\n        # TODO 4: Compute R-squared: 1 - (residual_ss / total_ss)\n        # TODO 5: Handle edge case where total_ss = 0 (constant target)\n        pass\n```\n\n**gradient_descent.py** (Skeleton for Milestone 2):\n```python\nimport numpy as np\nfrom typing import List, Tuple\n\nclass GradientDescentRegression:\n    \"\"\"Linear regression using gradient descent optimization.\"\"\"\n    \n    def __init__(self, learning_rate: float = 0.01, max_iterations: int = 1000, tolerance: float = 1e-6):\n        self.learning_rate = learning_rate\n        self.max_iterations = max_iterations\n        self.tolerance = tolerance\n        \n        # Parameters learned during training\n        self.slope_: Optional[float] = None\n        self.intercept_: Optional[float] = None\n        self.is_fitted_: bool = False\n        \n        # Training history for analysis\n        self.cost_history_: List[float] = []\n        self.parameter_history_: List[Tuple[float, float]] = []\n    \n    def _compute_cost(self, x: np.ndarray, y: np.ndarray, slope: float, intercept: float) -> float:\n        \"\"\"Calculate mean squared error cost function.\"\"\"\n        # TODO 1: Generate predictions using current parameters\n        # TODO 2: Calculate squared errors: (predictions - y)^2\n        # TODO 3: Return mean of squared errors\n        pass\n    \n    def _compute_gradients(self, x: np.ndarray, y: np.ndarray, slope: float, intercept: float) -> Tuple[float, float]:\n        \"\"\"Calculate partial derivatives of cost function with respect to parameters.\"\"\"\n        # TODO 1: Generate predictions using current parameters\n        # TODO 2: Calculate prediction errors: predictions - y\n        # TODO 3: Calculate gradient for slope: (2/n) * sum(errors * x)\n        # TODO 4: Calculate gradient for intercept: (2/n) * sum(errors)\n        # TODO 5: Return both gradients as tuple\n        pass\n    \n    def fit(self, x: np.ndarray, y: np.ndarray) -> 'GradientDescentRegression':\n        \"\"\"Fit linear regression using gradient descent optimization.\"\"\"\n        # TODO 1: Validate and prepare input data\n        # TODO 2: Initialize parameters (slope=0, intercept=mean(y) works well)\n        # TODO 3: Clear history lists\n        # TODO 4: Main optimization loop:\n        #         a. Compute current cost and store in history\n        #         b. Compute gradients for current parameters  \n        #         c. Update parameters: param = param - learning_rate * gradient\n        #         d. Store parameters in history\n        #         e. Check convergence: if cost improvement < tolerance, break\n        #         f. Check max_iterations limit\n        # TODO 5: Set is_fitted_ = True\n        # TODO 6: Return self\n        pass\n```\n\n**E. Language-Specific Python Hints**\n\n- Use `np.mean()`, `np.sum()`, and `np.dot()` for vectorized operations instead of Python loops\n- Handle division by zero with `np.isclose(variance, 0)` checks before dividing\n- Use `x.reshape(-1, 1)` to ensure features are always 2D arrays (n_samples, n_features)\n- Store training history in lists, convert to NumPy arrays only when needed for analysis\n- Use type hints (`-> np.ndarray`) to make interfaces clear and catch errors early\n- Implement `__repr__` methods to make debugging easier: `f\"SimpleLinearRegression(slope={self.slope_}, intercept={self.intercept_})\"`\n\n**F. Milestone Checkpoints**\n\n**Milestone 1 Checkpoint:**\n```python\n# Create test data\nx, y = generate_linear_data(100, slope=2.5, intercept=1.0, noise_std=0.5)\n\n# Test your implementation\nmodel = SimpleLinearRegression()\nmodel.fit(x, y)\npredictions = model.predict(x)\nr_squared = model.score(x, y)\n\nprint(f\"True slope: 2.5, Fitted slope: {model.slope_:.2f}\")\nprint(f\"True intercept: 1.0, Fitted intercept: {model.intercept_:.2f}\")\nprint(f\"R-squared: {r_squared:.3f}\")\n\n# Expected output:\n# True slope: 2.5, Fitted slope: 2.48\n# True intercept: 1.0, Fitted intercept: 1.02  \n# R-squared: 0.962\n```\n\n**Milestone 2 Checkpoint:**\n```python\n# Compare gradient descent with closed-form solution\ngd_model = GradientDescentRegression(learning_rate=0.01, max_iterations=1000)\ngd_model.fit(x, y)\n\nprint(f\"Gradient descent converged in {len(gd_model.cost_history_)} iterations\")\nprint(f\"Final cost: {gd_model.cost_history_[-1]:.6f}\")\nprint(f\"Parameters match closed-form: slope diff = {abs(model.slope_ - gd_model.slope_):.4f}\")\n\n# Plot cost history to verify convergence\nimport matplotlib.pyplot as plt\nplt.plot(gd_model.cost_history_)\nplt.xlabel('Iteration')\nplt.ylabel('Cost (MSE)')\nplt.title('Gradient Descent Convergence')\nplt.show()\n```\n\n**G. Common Implementation Pitfalls**\n\n| Problem Symptom | Likely Cause | Debugging Steps | Solution |\n|-----------------|--------------|-----------------|----------|\n| `slope_` is NaN | Division by zero in variance calculation | Check if all x values are identical | Add variance check: `if np.isclose(x_var, 0): raise ValueError(\"Cannot fit line to constant x values\")` |\n| R-squared > 1.0 or negative | Wrong residual or total sum calculation | Print intermediate values: residuals, total variance | Ensure using sample variance, not population variance in denominator |\n| Gradient descent diverges | Learning rate too high | Plot cost history - should decrease monotonically | Reduce learning rate by factor of 10, try 0.001 or 0.0001 |\n| Convergence very slow | Learning rate too low or features not normalized | Check iteration count and parameter changes | Increase learning rate or implement feature scaling |\n| Matrix dimension errors | Wrong array shapes for multiple features | Print shapes at each step: `print(f\"x shape: {x.shape}, y shape: {y.shape}\")` | Use `x.reshape(-1, 1)` for single features, ensure y is 1D |\n\n\n## High-Level Architecture\n\n> **Milestone(s):** All milestones (provides the foundational architecture that evolves across M1: Simple Linear Regression, M2: Gradient Descent, M3: Multiple Linear Regression)\n\nThis section presents the system's architectural blueprint, designed to support a progressive learning journey from basic mathematical concepts to sophisticated machine learning algorithms. The architecture emphasizes educational clarity while maintaining the flexibility to evolve from simple closed-form solutions to complex iterative optimization techniques.\n\n### Component Overview: The Four Main Components\n\nThe linear regression system is built around four core components that mirror the conceptual phases of machine learning: data preparation, model representation, optimization, and evaluation. This architectural division serves both pedagogical and practical purposes, allowing learners to understand each phase independently while appreciating their interconnected nature.\n\n**Mental Model: The Machine Learning Assembly Line**\n\nThink of our system as a modern manufacturing assembly line, but instead of building cars, we're building mathematical models that can make predictions. Each station (component) has a specific job: the first station prepares raw materials (data), the second station shapes the basic structure (model), the third station fine-tunes the assembly (optimization), and the fourth station tests quality (evaluation). Just like in manufacturing, the output of one station becomes the input to the next, and problems at any station affect the entire production line.\n\nThe **DataHandler** component serves as the system's intake and preparation facility. This component is responsible for ingesting raw data from various sources, validating its structure and content, and transforming it into the standardized format required by the mathematical algorithms. The DataHandler abstracts away the complexities of different data formats, missing value handling, and feature scaling, presenting a clean, consistent interface to downstream components.\n\n| Responsibility | Description | Key Operations |\n|---|---|---|\n| Data Loading | Read data from CSV files, arrays, or synthetic generators | `load_csv_data()`, `generate_linear_data()` |\n| Data Validation | Ensure data compatibility and detect common issues | `validate_data()` |\n| Feature Engineering | Transform raw features into model-ready format | `normalize_features()` |\n| Error Handling | Gracefully handle malformed or incompatible data | Input sanitization, type checking |\n\nThe **SimpleLinearRegression** component implements the foundational single-variable linear regression using the closed-form solution. This component embodies the mathematical elegance of the ordinary least squares method, providing learners with immediate feedback and perfect convergence without the complexities of iterative optimization. It serves as both a complete solution for simple problems and a reference implementation for validating more complex approaches.\n\n| Method | Parameters | Returns | Description |\n|---|---|---|---|\n| `fit(x, y)` | x: np.ndarray, y: np.ndarray | self | Computes optimal slope and intercept using normal equation |\n| `predict(x)` | x: np.ndarray | np.ndarray | Generates predictions using fitted parameters |\n| `score(x, y)` | x: np.ndarray, y: np.ndarray | float | Calculates R-squared coefficient of determination |\n\nThe **GradientDescentOptimizer** component introduces the iterative optimization paradigm that underpins modern machine learning. Unlike the closed-form approach, this component finds optimal parameters through repeated refinement, mimicking how humans learn through practice and feedback. This component is essential for understanding how complex models with millions of parameters can be trained when closed-form solutions become computationally infeasible.\n\n| Core Function | Purpose | Mathematical Foundation |\n|---|---|---|\n| Cost Computation | Measure prediction error | Mean squared error: `(1/2n) * Σ(y_pred - y_actual)²` |\n| Gradient Calculation | Determine parameter update direction | Partial derivatives of cost function |\n| Parameter Updates | Iteratively improve parameter estimates | `parameter = parameter - learning_rate * gradient` |\n| Convergence Detection | Determine when optimization is complete | Cost change falls below tolerance threshold |\n\nThe **MultipleLinearRegression** component extends the system to handle multiple input features, transforming the problem from fitting lines to fitting hyperplanes in multi-dimensional space. This component introduces vectorization and matrix operations, demonstrating how mathematical elegance scales to handle real-world complexity. It also incorporates regularization techniques to prevent overfitting when dealing with many features.\n\n> **Key Architectural Insight**: The progression from SimpleLinearRegression → GradientDescentOptimizer → MultipleLinearRegression mirrors the typical machine learning learning curve: start with intuitive concepts, understand optimization principles, then scale to realistic complexity. Each component builds conceptual understanding that makes the next component accessible.\n\n### Recommended Project Structure\n\nThe project structure is designed to mirror the learning progression, with each directory corresponding to a conceptual milestone. This organization helps learners understand dependencies and provides clear boundaries between different algorithmic approaches.\n\n```\nlinear_regression_project/\n├── README.md                           # Project overview and learning objectives\n├── requirements.txt                    # Python dependencies (numpy, matplotlib, pandas)\n├── setup.py                            # Package installation script\n│\n├── data/                               # Sample datasets and data generation utilities\n│   ├── synthetic/                      # Generated datasets for testing and validation\n│   │   ├── simple_linear.csv          # Single-variable synthetic data\n│   │   ├── multiple_features.csv      # Multi-variable synthetic data\n│   │   └── noisy_data.csv             # High-noise dataset for testing robustness\n│   ├── real_world/                    # Actual datasets for final validation\n│   │   ├── housing_prices.csv        # Real estate prediction dataset\n│   │   └── student_scores.csv        # Educational performance dataset\n│   └── data_generator.py              # Utilities for creating synthetic datasets\n│\n├── src/                               # Main source code organized by learning milestone\n│   ├── __init__.py\n│   ├── data_handler/                  # M1: Data loading and preprocessing\n│   │   ├── __init__.py\n│   │   ├── loader.py                  # CSV loading and validation\n│   │   ├── validator.py               # Data quality checks and error detection\n│   │   ├── preprocessor.py            # Feature scaling and normalization\n│   │   └── synthetic_generator.py     # Synthetic data creation for testing\n│   │\n│   ├── simple_regression/             # M1: Closed-form linear regression\n│   │   ├── __init__.py\n│   │   ├── model.py                   # SimpleLinearRegression class implementation\n│   │   ├── math_utils.py              # Mathematical utilities (mean, variance, etc.)\n│   │   └── evaluation.py              # R-squared and other metrics\n│   │\n│   ├── gradient_descent/              # M2: Iterative optimization\n│   │   ├── __init__.py\n│   │   ├── optimizer.py               # GradientDescentRegression class\n│   │   ├── cost_functions.py          # Mean squared error and derivatives\n│   │   └── convergence.py             # Stopping criteria and progress tracking\n│   │\n│   ├── multiple_regression/           # M3: Multi-variable and advanced features\n│   │   ├── __init__.py\n│   │   ├── model.py                   # MultipleLinearRegression class\n│   │   ├── matrix_operations.py       # Vectorized computations\n│   │   ├── regularization.py          # L2 regularization (Ridge regression)\n│   │   └── feature_scaling.py         # Advanced normalization techniques\n│   │\n│   └── utils/                         # Shared utilities across all components\n│       ├── __init__.py\n│       ├── plotting.py                # Visualization helpers for debugging\n│       ├── metrics.py                 # Common evaluation metrics\n│       └── debugging.py               # Logging and diagnostic tools\n│\n├── tests/                             # Comprehensive test suite organized by milestone\n│   ├── __init__.py\n│   ├── test_data_handler/             # Unit tests for data handling components\n│   │   ├── test_loader.py\n│   │   ├── test_validator.py\n│   │   └── test_preprocessor.py\n│   ├── test_simple_regression/        # Tests for closed-form implementation\n│   │   ├── test_model.py\n│   │   └── test_evaluation.py\n│   ├── test_gradient_descent/         # Tests for optimization components\n│   │   ├── test_optimizer.py\n│   │   └── test_convergence.py\n│   ├── test_multiple_regression/      # Tests for multi-variable regression\n│   │   ├── test_model.py\n│   │   └── test_regularization.py\n│   └── integration/                   # End-to-end testing\n│       ├── test_complete_pipeline.py\n│       └── test_cross_validation.py\n│\n├── examples/                          # Educational examples and tutorials\n│   ├── milestone_1_simple.py         # Complete M1 example with explanations\n│   ├── milestone_2_gradient.py       # M2 example showing optimization process\n│   ├── milestone_3_multiple.py       # M3 example with real datasets\n│   ├── debugging_example.py          # Common debugging scenarios\n│   └── comparison_study.py           # Compare all three approaches\n│\n├── notebooks/                        # Jupyter notebooks for interactive learning\n│   ├── 01_data_exploration.ipynb     # Understanding data and visualization\n│   ├── 02_closed_form_solution.ipynb # Mathematical derivation and implementation\n│   ├── 03_gradient_descent_viz.ipynb # Visualizing the optimization process\n│   └── 04_advanced_topics.ipynb     # Regularization and feature engineering\n│\n└── docs/                             # Additional documentation\n    ├── mathematical_background.md    # Derivations and theory\n    ├── troubleshooting.md            # Common issues and solutions\n    └── extensions.md                 # Ideas for further development\n```\n\nThis structure provides several key benefits for learners:\n\n**Progressive Complexity**: Each directory builds on previous concepts, allowing learners to master one milestone before proceeding to the next. The separation prevents cognitive overload while maintaining clear conceptual boundaries.\n\n**Clear Dependencies**: The structure makes component dependencies explicit. Data handling is foundational and used by all regression components. Simple regression provides a reference for gradient descent validation. Multiple regression builds on both previous approaches.\n\n**Practical Organization**: Tests, examples, and documentation are co-located with relevant code, encouraging good development practices from the beginning of the learning journey.\n\n> **Architecture Decision: Milestone-Based Directory Structure vs. Feature-Based Structure**\n> - **Context**: Need to organize code in a way that supports progressive learning while maintaining good software engineering practices\n> - **Options Considered**: \n>   1. Feature-based (models/, optimizers/, data/ directories)\n>   2. Milestone-based (simple_regression/, gradient_descent/, multiple_regression/)\n>   3. Single flat directory with all components mixed\n> - **Decision**: Milestone-based directory structure with shared utilities\n> - **Rationale**: Learning progression is the primary concern for this educational project. Clear milestone boundaries help learners understand conceptual dependencies and provide natural stopping points for consolidation. Feature-based organization would mix complexity levels and obscure the learning path.\n> - **Consequences**: Some code duplication between milestones, but clearer learning progression and easier incremental development\n\n### Component Interactions\n\nThe component interactions follow a clear data flow pattern that mirrors the machine learning workflow: data preparation, model fitting, prediction generation, and performance evaluation. Understanding these interactions is crucial for debugging and extending the system.\n\n![System Component Architecture](./diagrams/system-components.svg)\n\n**Training Data Flow Sequence**\n\nThe training process follows a standardized sequence regardless of which regression approach is used. This consistency allows learners to focus on algorithmic differences rather than integration complexity.\n\n1. **Data Acquisition**: The client code invokes `load_csv_data()` on the DataHandler component, providing file path and column specifications. The DataHandler reads the raw data, performs initial type checking, and returns feature and target arrays.\n\n2. **Data Validation**: The `validate_data()` function checks for common issues: mismatched array lengths, missing values, infinite values, and data types incompatible with numerical computation. This step prevents downstream failures and provides clear error messages.\n\n3. **Feature Preprocessing**: For gradient descent and multiple regression, `normalize_features()` applies z-score standardization to ensure all features contribute equally to the optimization process. Simple linear regression can skip this step for single variables.\n\n4. **Model Initialization**: The chosen regression class (`SimpleLinearRegression`, `GradientDescentRegression`, or `MultipleLinearRegression`) is instantiated with appropriate hyperparameters. Initial parameter values are set to defaults or small random values.\n\n5. **Parameter Fitting**: The `fit()` method is called with training data. The internal behavior varies by component:\n   - SimpleLinearRegression computes parameters directly using the normal equation\n   - GradientDescentRegression iteratively optimizes parameters using the cost function gradient\n   - MultipleLinearRegression uses vectorized gradient descent with optional regularization\n\n6. **Training Completion**: The model sets its `is_fitted_` flag to True and stores learned parameters. Gradient descent models also preserve `cost_history_` and `parameter_history_` for analysis and debugging.\n\n**Prediction Data Flow Sequence**\n\nPrediction follows a simpler, stateless pattern once models are trained:\n\n1. **Input Validation**: New input data undergoes the same validation as training data to ensure compatibility with the fitted model's expectations.\n\n2. **Feature Preprocessing**: Input features are normalized using the same scaling parameters computed during training. This ensures consistency between training and prediction phases.\n\n3. **Prediction Computation**: The `predict()` method applies the learned parameters to compute output values:\n   - Simple regression: `y_pred = slope_ * x + intercept_`\n   - Multiple regression: `y_pred = X @ weights_` (matrix-vector multiplication)\n\n4. **Output Formatting**: Predictions are returned as NumPy arrays with appropriate shapes and data types for downstream consumption.\n\n**Component Communication Protocols**\n\nThe components communicate through well-defined interfaces that abstract implementation details while providing rich error information:\n\n| Interface | Data Format | Error Handling | State Dependencies |\n|---|---|---|---|\n| DataHandler → Models | `Tuple[np.ndarray, np.ndarray]` | Raises `ValueError` with descriptive messages | Stateless operations |\n| Models.fit() | Input arrays, returns self | Raises `ValueError` for invalid data | Changes model from unfitted to fitted |\n| Models.predict() | Input array, returns predictions | Raises `RuntimeError` if not fitted | Requires fitted model state |\n| Models.score() | Input arrays, returns float | Combines fit checking and R-squared computation | Requires fitted model state |\n\n**Inter-Component Dependencies**\n\nThe components exhibit a layered dependency structure that supports the educational progression:\n\n- **DataHandler** has no dependencies on other components, making it the foundation layer\n- **SimpleLinearRegression** depends only on DataHandler for preprocessing, representing the first complete machine learning pipeline\n- **GradientDescentOptimizer** extends SimpleLinearRegression concepts but can be implemented independently, allowing comparison between approaches\n- **MultipleLinearRegression** builds on gradient descent principles and may reuse optimization code, representing the culmination of learned concepts\n\n> **Design Insight: Loose Coupling with Rich Interfaces**\n> \n> The architecture prioritizes loose coupling between components while providing rich error information and debugging support. Each component can be tested independently, but when integrated, they provide detailed diagnostic information about failures. This design supports both learning (clear error messages aid understanding) and development (components can be built and tested incrementally).\n\n**State Management and Persistence**\n\nComponent state management follows machine learning best practices while remaining simple enough for educational use:\n\n| Component | Persistent State | Transient State | State Transitions |\n|---|---|---|---|\n| DataHandler | None (stateless) | Loaded data arrays | Load → Validate → Transform |\n| SimpleLinearRegression | `slope_`, `intercept_`, `is_fitted_` | Computation intermediates | Unfitted → Fitted |\n| GradientDescentRegression | Parameters + `cost_history_` | Current gradients, iteration count | Unfitted → Training → Fitted/Failed |\n| MultipleLinearRegression | Weight vector + scaling parameters | Matrix intermediates | Unfitted → Training → Fitted/Failed |\n\n**Error Propagation and Recovery**\n\nThe component interactions include comprehensive error handling that provides educational value:\n\n- **Data Errors**: Invalid file formats, missing columns, or incompatible data types are caught at the DataHandler level with specific error messages indicating the problem location and suggested fixes\n- **Mathematical Errors**: Division by zero, matrix singularity, or numerical overflow are detected during computation with error messages explaining the mathematical issue and potential solutions\n- **Convergence Errors**: Gradient descent failure (divergence or slow convergence) is detected and reported with diagnostic information about learning rates, cost function behavior, and suggested parameter adjustments\n- **State Errors**: Attempting to predict with unfitted models or refit already-fitted models results in clear error messages explaining the expected component lifecycle\n\n![Learning Milestone Progression](./diagrams/milestone-progression.svg)\n\nThis error handling strategy serves dual educational purposes: it prevents silent failures that would confuse learners, and it teaches proper error handling practices that are essential for production machine learning systems.\n\n### Implementation Guidance\n\nThe implementation approach balances educational clarity with practical software development skills. This guidance provides the scaffolding needed to transform the architectural design into working code while preserving the learning objectives.\n\n**Technology Recommendations**\n\n| Component | Simple Implementation | Advanced Implementation |\n|---|---|---|\n| Data Handling | Pure NumPy arrays with CSV reading | Pandas DataFrames with comprehensive data validation |\n| Mathematical Operations | NumPy basic operations | NumPy with BLAS optimization and numerical stability checks |\n| Visualization | Matplotlib with basic plotting | Seaborn with statistical plots and interactive widgets |\n| Testing | Python unittest with synthetic data | Pytest with property-based testing and real datasets |\n| Documentation | Inline comments and docstrings | Sphinx with mathematical notation and tutorials |\n\n**Core Infrastructure Starter Code**\n\nThe following complete utilities provide the foundation for learner implementations:\n\n```python\n# src/utils/plotting.py - Complete visualization utilities\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom typing import Tuple, Optional, List\n\ndef plot_regression_results(x_train: np.ndarray, y_train: np.ndarray, \n                           x_test: np.ndarray, y_test: np.ndarray,\n                           predictions: np.ndarray, model_name: str = \"Regression\") -> None:\n    \"\"\"\n    Create comprehensive regression visualization with training data, test data, and predictions.\n    Essential for debugging model behavior and understanding fit quality.\n    \"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n    \n    # Left plot: Training data with fitted line\n    ax1.scatter(x_train, y_train, alpha=0.6, color='blue', label='Training Data')\n    if len(x_test) > 0:\n        sorted_indices = np.argsort(x_test.flatten())\n        ax1.plot(x_test[sorted_indices], predictions[sorted_indices], \n                color='red', linewidth=2, label='Fitted Line')\n    ax1.set_xlabel('Feature Value')\n    ax1.set_ylabel('Target Value')\n    ax1.set_title(f'{model_name} - Fitted Model')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # Right plot: Residuals analysis\n    residuals = y_test - predictions\n    ax2.scatter(predictions, residuals, alpha=0.6, color='green')\n    ax2.axhline(y=0, color='red', linestyle='--')\n    ax2.set_xlabel('Predicted Values')\n    ax2.set_ylabel('Residuals')\n    ax2.set_title('Residuals Plot (should be random)')\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\ndef plot_cost_history(cost_history: List[float], title: str = \"Training Progress\") -> None:\n    \"\"\"\n    Visualize gradient descent convergence for debugging optimization issues.\n    \"\"\"\n    plt.figure(figsize=(10, 6))\n    plt.plot(cost_history, linewidth=2)\n    plt.xlabel('Iteration')\n    plt.ylabel('Cost (Mean Squared Error)')\n    plt.title(f'{title} - Cost Function Over Time')\n    plt.grid(True, alpha=0.3)\n    plt.yscale('log')  # Log scale often reveals convergence patterns better\n    plt.show()\n```\n\n```python\n# src/utils/metrics.py - Complete evaluation utilities\nimport numpy as np\nfrom typing import Tuple\n\ndef calculate_r_squared(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"\n    Calculate coefficient of determination (R-squared) with proper error handling.\n    R-squared measures the proportion of variance in the target explained by the model.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError(f\"Array length mismatch: y_true({len(y_true)}) != y_pred({len(y_pred)})\")\n    \n    # Total sum of squares (variance in actual values)\n    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n    \n    # Residual sum of squares (unexplained variance)\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    \n    # Handle edge case where all actual values are identical\n    if ss_tot == 0:\n        return 1.0 if ss_res == 0 else 0.0\n    \n    r_squared = 1 - (ss_res / ss_tot)\n    return float(r_squared)\n\ndef calculate_mse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Calculate Mean Squared Error with input validation.\"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError(f\"Array length mismatch: y_true({len(y_true)}) != y_pred({len(y_pred)})\")\n    \n    mse = np.mean((y_true - y_pred) ** 2)\n    return float(mse)\n\ndef calculate_rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Calculate Root Mean Squared Error (same units as target variable).\"\"\"\n    return np.sqrt(calculate_mse(y_true, y_pred))\n```\n\n**Component Implementation Skeletons**\n\nFor core learning components, provide detailed skeletons with comprehensive TODOs:\n\n```python\n# src/simple_regression/model.py - Core learning component skeleton\nimport numpy as np\nfrom typing import Optional\n\nclass SimpleLinearRegression:\n    \"\"\"\n    Single-variable linear regression using closed-form ordinary least squares solution.\n    \n    This implementation demonstrates the mathematical elegance of the normal equation:\n    slope = Σ((x - x_mean) * (y - y_mean)) / Σ((x - x_mean)²)\n    intercept = y_mean - slope * x_mean\n    \"\"\"\n    \n    def __init__(self):\n        self.slope_: Optional[float] = None\n        self.intercept_: Optional[float] = None\n        self.is_fitted_: bool = False\n    \n    def fit(self, x: np.ndarray, y: np.ndarray) -> 'SimpleLinearRegression':\n        \"\"\"\n        Fit linear regression using the closed-form solution.\n        \n        Args:\n            x: Feature values, shape (n_samples,)\n            y: Target values, shape (n_samples,)\n            \n        Returns:\n            self: Fitted model instance\n        \"\"\"\n        # TODO 1: Validate input arrays (check shapes, types, lengths match)\n        # Hint: Use np.asarray() to ensure NumPy arrays, check x.shape == y.shape\n        \n        # TODO 2: Check for edge cases that would cause mathematical errors\n        # Hint: What happens if all x values are identical? (Zero denominator)\n        \n        # TODO 3: Calculate means of x and y for centered computation\n        # Hint: Use np.mean() - this is x_bar and y_bar in the equations\n        \n        # TODO 4: Compute slope using the normal equation\n        # Hint: numerator = sum((x - x_mean) * (y - y_mean))\n        #       denominator = sum((x - x_mean) ** 2)\n        #       slope = numerator / denominator\n        \n        # TODO 5: Compute intercept using the relationship intercept = y_mean - slope * x_mean\n        # Hint: This ensures the line passes through the point (x_mean, y_mean)\n        \n        # TODO 6: Store computed parameters and mark model as fitted\n        # Hint: Set self.slope_, self.intercept_, and self.is_fitted_\n        \n        return self\n    \n    def predict(self, x: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Generate predictions using the fitted linear model.\n        \n        Args:\n            x: Feature values for prediction, shape (n_samples,)\n            \n        Returns:\n            Predicted target values, shape (n_samples,)\n        \"\"\"\n        # TODO 1: Check if model has been fitted\n        # Hint: Raise RuntimeError with clear message if not self.is_fitted_\n        \n        # TODO 2: Validate input array format\n        # Hint: Convert to NumPy array, check for reasonable shape and no NaN/inf values\n        \n        # TODO 3: Apply linear equation y = mx + b\n        # Hint: Use vectorized NumPy operations: self.slope_ * x + self.intercept_\n        \n        pass  # Remove this line when implementing\n    \n    def score(self, x: np.ndarray, y: np.ndarray) -> float:\n        \"\"\"\n        Calculate R-squared coefficient of determination.\n        \n        Args:\n            x: Feature values, shape (n_samples,)\n            y: True target values, shape (n_samples,)\n            \n        Returns:\n            R-squared value between 0 and 1 (higher is better)\n        \"\"\"\n        # TODO 1: Generate predictions for the provided x values\n        # Hint: Use self.predict(x)\n        \n        # TODO 2: Calculate R-squared using the coefficient of determination formula\n        # Hint: Import and use calculate_r_squared from utils.metrics\n        \n        pass  # Remove this line when implementing\n```\n\n**Milestone Checkpoint Verification**\n\nAfter implementing each milestone, learners should verify their progress with these concrete checkpoints:\n\n**Milestone 1 Checkpoint - Simple Linear Regression:**\n```python\n# examples/milestone_1_checkpoint.py\nimport numpy as np\nfrom src.data_handler.synthetic_generator import generate_linear_data\nfrom src.simple_regression.model import SimpleLinearRegression\n\n# Generate test data with known parameters\nx_test, y_test = generate_linear_data(n_samples=100, slope=2.5, intercept=1.0, \n                                     noise_std=0.1, x_range=(0, 10))\n\n# Fit model and check parameter recovery\nmodel = SimpleLinearRegression()\nmodel.fit(x_test, y_test)\n\nprint(f\"True parameters: slope=2.5, intercept=1.0\")\nprint(f\"Fitted parameters: slope={model.slope_:.3f}, intercept={model.intercept_:.3f}\")\nprint(f\"Parameter error: slope_error={abs(model.slope_ - 2.5):.3f}, intercept_error={abs(model.intercept_ - 1.0):.3f}\")\n\n# Both errors should be < 0.2 for this low-noise dataset\nassert abs(model.slope_ - 2.5) < 0.2, \"Slope estimation error too high\"\nassert abs(model.intercept_ - 1.0) < 0.2, \"Intercept estimation error too high\"\n\n# Test R-squared calculation\nr_squared = model.score(x_test, y_test)\nprint(f\"R-squared: {r_squared:.3f}\")\nassert r_squared > 0.95, \"R-squared should be very high for low-noise synthetic data\"\n\nprint(\"✅ Milestone 1 checkpoint passed!\")\n```\n\n**Language-Specific Implementation Hints**\n\n**NumPy Best Practices:**\n- Use `np.asarray()` instead of requiring exact array types - this accepts lists, tuples, and other array-like inputs\n- Check for `np.isfinite()` to detect NaN and infinite values that will break computations\n- Use `np.allclose()` for floating-point comparisons instead of exact equality\n- Leverage broadcasting: `x.reshape(-1, 1)` converts 1D arrays to column vectors for matrix operations\n\n**Common Python Pitfalls:**\n- Division by zero: Always check denominators before division, especially `np.sum((x - x.mean()) ** 2)`\n- Array shape mismatches: Use `.flatten()` or `.ravel()` to ensure 1D arrays when needed\n- Integer division: Use `float()` conversion or ensure arrays are float type to avoid integer truncation\n- Mutable default arguments: Never use `def func(param=[]):` - use `def func(param=None):` instead\n\n**Performance Optimization:**\n- Use vectorized NumPy operations instead of Python loops\n- Pre-allocate arrays when size is known: `np.zeros(n_samples)` instead of appending to lists\n- Use `np.dot()` or `@` operator for matrix multiplication instead of manual loops\n- Consider memory layout: use `np.ascontiguousarray()` for better cache performance with large arrays\n\n**Debugging Strategy for Each Component:**\n\n| Component | Common Symptom | Likely Cause | Diagnostic Steps | Fix |\n|---|---|---|---|---|\n| DataHandler | \"ValueError: could not convert string to float\" | Non-numeric data in CSV | Print first few rows, check column types | Clean data or specify correct columns |\n| SimpleLinearRegression | Parameters are NaN or infinite | Division by zero (constant x values) | Check `np.var(x)` - should be > 0 | Use different dataset or add noise |\n| GradientDescent | Cost increases instead of decreasing | Learning rate too high | Plot cost history, try learning_rate/10 | Reduce learning rate by factors of 10 |\n| MultipleRegression | Very poor R-squared despite good single features | Features on different scales | Check feature means and standard deviations | Apply feature normalization |\n\nThis implementation guidance provides learners with the scaffolding needed to build working implementations while preserving the educational value of discovering algorithmic details through hands-on coding.\n\n\n## Data Model and Core Types\n\n> **Milestone(s):** All milestones (M1: Simple Linear Regression, M2: Gradient Descent, M3: Multiple Linear Regression)\n\nThis section establishes the foundational data structures that flow through our linear regression system. Think of these types as the vocabulary our components use to communicate with each other - they define not just what data we store, but how it transforms as it moves from raw input through training to final predictions.\n\n### Mental Model: The Learning Journal\n\nImagine our linear regression system as a student keeping a detailed learning journal. The **Dataset** is like the practice problems and exercises - raw information that needs to be organized and understood. The **ModelParameters** represent the student's current understanding - their \"best guess\" about how the world works based on what they've learned so far. The **TrainingHistory** is the journal itself - a record of every attempt, every mistake, and every improvement made along the way. Finally, the **PredictionResult** is like the student's confident answer to a new question, backed by everything they've learned.\n\nJust as a student's understanding evolves from confusion to clarity through practice, our data structures capture this transformation from raw numbers to trained intelligence.\n\n### Core Data Types\n\nOur system revolves around four fundamental data structures, each serving a specific role in the machine learning pipeline. These types are designed to be simple enough for educational purposes while robust enough to handle real regression tasks.\n\n![Data Model and Type Relationships](./diagrams/data-model.svg)\n\n#### Dataset Structure\n\nThe `Dataset` type encapsulates all the information needed to train and evaluate our regression models. It serves as the primary container for features, targets, and associated metadata.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `features` | `np.ndarray` | Input feature matrix with shape (n_samples, n_features) where each row is an observation |\n| `targets` | `np.ndarray` | Target values vector with shape (n_samples,) containing the dependent variable values |\n| `feature_names` | `List[str]` | Human-readable names for each feature column, used for debugging and visualization |\n| `n_samples` | `int` | Number of training examples in the dataset, derived from features.shape[0] |\n| `n_features` | `int` | Number of input features per example, derived from features.shape[1] |\n| `is_normalized` | `bool` | Flag indicating whether features have been normalized using z-score standardization |\n| `normalization_stats` | `Dict[str, np.ndarray]` | Stored means and standard deviations for each feature, used to normalize new data |\n\nThe `Dataset` type abstracts away the complexity of data management while ensuring consistency across all components. The normalization statistics are particularly important - they allow us to apply the same transformation to new prediction data that was applied during training, preventing distribution shift issues.\n\n> **Design Insight:** We store normalization statistics within the dataset rather than in the model because normalization is a property of the data, not the learning algorithm. This allows the same normalized dataset to be used with different models while maintaining consistency.\n\n#### ModelParameters Structure\n\nThe `ModelParameters` type represents the learned weights that define our regression model. This structure evolves across milestones, starting simple and growing more sophisticated as we add features.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `slope_` | `float` | Linear coefficient for single-variable regression (M1, M2) |\n| `intercept_` | `float` | Bias term representing the y-intercept of the regression line |\n| `weights_` | `np.ndarray` | Weight vector for multiple features (M3), with shape (n_features,) |\n| `is_fitted_` | `bool` | Flag indicating whether parameters have been estimated from training data |\n| `fitting_method_` | `str` | Method used for parameter estimation: \"closed_form\" or \"gradient_descent\" |\n| `regularization_strength_` | `float` | L2 regularization coefficient (lambda) for Ridge regression in M3 |\n| `feature_count_` | `int` | Number of features the model was trained on, used for prediction validation |\n\nThe underscore suffix on most fields follows the scikit-learn convention, indicating these are attributes set during the fitting process rather than user-provided parameters. This naming helps distinguish between configuration (no underscore) and learned parameters (underscore suffix).\n\n> **Design Insight:** We maintain both `slope_` and `weights_` fields even though they're mathematically equivalent for single-variable regression. This design choice prioritizes educational clarity over memory efficiency, allowing learners to see the conceptual progression from simple to multiple regression.\n\n#### TrainingHistory Structure\n\nThe `TrainingHistory` type captures the entire optimization journey, providing insight into how gradient descent converges to the optimal solution. This is crucial for debugging and understanding the learning process.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `cost_history_` | `List[float]` | Mean squared error at each iteration, should decrease monotonically |\n| `parameter_history_` | `List[Tuple[float, float]]` | Slope and intercept values at each iteration for M1/M2 |\n| `weight_history_` | `List[np.ndarray]` | Weight vector snapshots for multiple regression (M3) |\n| `gradient_norms_` | `List[float]` | Magnitude of gradient vector at each iteration, indicates convergence |\n| `learning_rate` | `float` | Step size used during optimization, affects convergence speed and stability |\n| `max_iterations` | `int` | Maximum allowed training iterations, prevents infinite loops |\n| `tolerance` | `float` | Convergence threshold for cost improvement between iterations |\n| `converged_` | `bool` | Flag indicating whether optimization reached convergence criteria |\n| `final_iteration_` | `int` | Actual number of iterations performed before stopping |\n| `convergence_reason_` | `str` | Why training stopped: \"converged\", \"max_iterations\", or \"diverged\" |\n\nThe training history serves multiple purposes beyond mere record-keeping. It enables sophisticated debugging techniques like plotting cost curves, detecting oscillation in parameter updates, and analyzing the relationship between learning rate and convergence behavior.\n\n> **Critical Insight:** The `gradient_norms_` field is often overlooked but extremely valuable. When the gradient magnitude approaches zero, we know we're near an optimum. Sudden spikes in gradient norm can indicate numerical instability or learning rate issues.\n\n#### PredictionResult Structure\n\nThe `PredictionResult` type encapsulates not just the predicted values but also confidence metrics and diagnostic information that help users understand model performance.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `predictions` | `np.ndarray` | Predicted target values with shape (n_samples,) |\n| `residuals` | `np.ndarray` | Prediction errors (actual - predicted) when true targets are available |\n| `confidence_intervals` | `np.ndarray` | 95% confidence bounds for predictions (M3 extension) |\n| `r_squared` | `float` | Coefficient of determination when true targets are provided for evaluation |\n| `mean_squared_error` | `float` | Average squared prediction error when true targets are available |\n| `mean_absolute_error` | `float` | Average absolute prediction error for robust error assessment |\n| `input_shape` | `Tuple[int, int]` | Shape of input data used for predictions, for validation and debugging |\n| `model_type` | `str` | Type of model used: \"simple_linear\", \"gradient_descent\", or \"multiple_linear\" |\n\nThe inclusion of multiple error metrics provides a comprehensive view of model performance. Mean squared error heavily penalizes large errors, while mean absolute error gives equal weight to all errors. This combination helps identify whether prediction errors are consistent or dominated by outliers.\n\n### Architecture Decision Records\n\n> **Decision: NumPy Arrays vs Pure Python Lists for Numerical Data**\n> - **Context**: We need efficient storage and computation for potentially large datasets with thousands of samples and features. Python lists are simple but inefficient for numerical operations.\n> - **Options Considered**: Pure Python lists, NumPy arrays, Pandas DataFrames\n> - **Decision**: NumPy arrays for all numerical data (features, targets, predictions)\n> - **Rationale**: NumPy provides vectorized operations essential for efficient matrix math, consistent with scientific Python ecosystem, and offers better memory layout for numerical computing. Type safety prevents common bugs like accidentally mixing scalars with arrays.\n> - **Consequences**: Requires NumPy dependency, slight learning curve for array indexing, but enables performant vectorized operations and prepares learners for advanced ML libraries.\n\n> **Decision: Separate History Tracking vs Embedded in Model**  \n> - **Context**: Gradient descent generates valuable diagnostic information during training that learners need for debugging and understanding convergence behavior.\n> - **Options Considered**: Store history in model parameters, separate TrainingHistory class, or no history tracking\n> - **Decision**: Separate TrainingHistory type that models can optionally populate\n> - **Rationale**: Separation of concerns - model parameters represent learned knowledge while history represents the learning process. Allows models to be lightweight when history isn't needed, but provides rich debugging when it is.\n> - **Consequences**: More complex data flow but cleaner architecture. Enables sophisticated debugging techniques and educational insights into optimization behavior.\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| Pure Python Lists | Simple, no dependencies, familiar syntax | Slow for large data, no vectorization, memory inefficient | ❌ |\n| NumPy Arrays | Fast vectorized ops, memory efficient, scientific Python standard | Learning curve, dependency, less familiar indexing | ✅ |\n| Pandas DataFrames | Rich data manipulation, handles mixed types, great for exploration | Heavy dependency, overkill for numerical arrays, slower than NumPy | ❌ |\n\n### Type Relationships and Lifecycle\n\nUnderstanding how these data types interact and transform throughout the machine learning pipeline is crucial for system design. The lifecycle follows a clear progression from raw data through trained models to actionable predictions.\n\n#### Data Transformation Pipeline\n\nThe journey begins with raw numerical data and culminates in trained models capable of making predictions. Each stage involves specific transformations that prepare the data for the next phase.\n\n**Stage 1: Data Ingestion and Validation**\nRaw CSV files or array data enters the system and gets transformed into the structured `Dataset` format. This involves parsing numerical values, handling missing data, validating dimensions, and organizing features and targets into consistent arrays. The validation process ensures that feature and target arrays have compatible shapes and contain valid numerical values.\n\n**Stage 2: Preprocessing and Normalization**  \nThe `Dataset` undergoes feature scaling to ensure all input variables have comparable ranges. Z-score normalization transforms each feature to have zero mean and unit variance, preventing features with large scales from dominating the learning process. The normalization statistics get stored in the dataset for later use with new prediction data.\n\n**Stage 3: Model Parameter Initialization**\nA fresh `ModelParameters` structure is created with initial values. For closed-form solutions, parameters remain uninitialized until the normal equation provides the optimal values. For gradient descent, parameters start with small random values or zeros, and the `is_fitted_` flag remains False.\n\n**Stage 4: Training and Optimization**\nDuring training, the model iteratively updates parameters while populating the `TrainingHistory` with diagnostic information. Each iteration generates new parameter values, cost measurements, and gradient calculations. The history tracks this evolution, providing insight into convergence behavior.\n\n**Stage 5: Convergence and Finalization**  \nTraining terminates when convergence criteria are met or maximum iterations are reached. Final parameters are stored in the `ModelParameters` structure with `is_fitted_` set to True. The training history captures the convergence reason and final optimization state.\n\n**Stage 6: Prediction and Evaluation**\nNew input data flows through the trained model to generate `PredictionResult` objects. If true target values are available, the result includes evaluation metrics. The prediction process validates input dimensions against the trained model's expected feature count.\n\n#### State Transitions and Invariants\n\nSeveral important invariants must be maintained throughout the data lifecycle to ensure system correctness and prevent subtle bugs.\n\n**Dataset Invariants:**\n- `features.shape[0]` must always equal `targets.shape[0]` (same number of samples)\n- `n_samples` and `n_features` must match the actual array dimensions\n- When `is_normalized` is True, `normalization_stats` must contain valid mean/std for each feature\n- Feature names list length must match `n_features` when provided\n\n**ModelParameters Invariants:**\n- `is_fitted_` can only be True if either `slope_/intercept_` or `weights_` contain valid learned values\n- `feature_count_` must match the number of features the model was trained on\n- For single regression: `weights_` is None, `slope_/intercept_` are valid floats\n- For multiple regression: `weights_.shape[0]` equals `feature_count_`, `slope_` may be unused\n\n**TrainingHistory Invariants:**\n- All history lists must have the same length (each iteration adds one entry to each list)  \n- `cost_history_` values should generally decrease (monotonic convergence)\n- `final_iteration_` must be less than or equal to `max_iterations`\n- `converged_` can only be True if cost improvement fell below `tolerance`\n\n**PredictionResult Invariants:**\n- `predictions.shape[0]` must equal the number of input samples\n- When true targets are provided, `residuals` must equal `targets - predictions`  \n- Error metrics are only valid when computed from true target values\n- `input_shape` must match the dimensions of the data used for prediction\n\n#### Data Flow Protocols\n\nThe interaction between data types follows specific protocols that ensure consistency and prevent common integration errors.\n\n**Training Protocol:**\n1. `Dataset` provides features and targets to the model\n2. Model initializes fresh `ModelParameters` and `TrainingHistory`  \n3. For each iteration, model updates parameters and appends to history\n4. Model sets convergence flags and final parameter values\n5. Trained model can generate predictions using learned parameters\n\n**Prediction Protocol:**\n1. Input features are validated against trained model's `feature_count_`\n2. If dataset was normalized, prediction data undergoes same normalization using stored statistics\n3. Model applies learned parameters to generate predictions\n4. Results are packaged into `PredictionResult` with appropriate metadata\n5. If true targets are provided, evaluation metrics are computed and included\n\n**Error Recovery Protocol:**\nWhen invariants are violated or data inconsistencies are detected, the system follows a fail-fast approach. Rather than attempting to fix corrupted data, components raise descriptive exceptions that help learners identify and correct the underlying issues.\n\n### Common Pitfalls\n\nUnderstanding these data types seems straightforward, but several subtle issues frequently trip up learners working with machine learning systems.\n\n⚠️ **Pitfall: Array Shape Mismatches**\nLearners often create feature arrays with incorrect dimensions, such as 1D arrays with shape `(n_samples,)` instead of 2D arrays with shape `(n_samples, 1)` for single-feature regression. This causes matrix multiplication errors in multiple regression components. Always ensure features are 2D arrays, even for single variables, using `reshape(-1, 1)` when necessary.\n\n⚠️ **Pitfall: Forgetting Normalization Statistics**  \nWhen normalizing training data, learners frequently forget to store the mean and standard deviation values needed to normalize prediction data. This causes distribution shift where the model sees differently scaled data at prediction time compared to training time. Always save normalization statistics in the `Dataset` structure and apply the same transformation to new data.\n\n⚠️ **Pitfall: Parameter History Memory Growth**\nFor long training runs, storing parameter values at every iteration can consume significant memory. Learners sometimes create memory pressure by storing large weight vectors for thousands of iterations. Consider storing history every N iterations for long training runs, or implement a circular buffer for recent history.\n\n⚠️ **Pitfall: Mixing Data Types**  \nPython's dynamic typing allows mixing integers, floats, and NumPy arrays in ways that cause subtle bugs. For example, accidentally using a Python list where a NumPy array is expected can cause shape and broadcasting errors. Always validate data types when crossing component boundaries.\n\n⚠️ **Pitfall: Inconsistent Fitted State**\nLearners sometimes forget to update the `is_fitted_` flag after training, or fail to reset it when retraining with new data. This can cause the model to make predictions with stale parameters or reject valid prediction requests. Always maintain consistent fitted state across training operations.\n\n### Implementation Guidance\n\nThis section provides concrete implementation recommendations for the data types, bridging the design concepts with working code.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option | Recommended For |\n|-----------|---------------|-----------------|-----------------|\n| Array Operations | NumPy basic indexing | NumPy advanced indexing + broadcasting | All milestones |\n| Data Validation | Manual type/shape checks | Custom validation decorators | M1: Manual, M2+: Decorators |  \n| Serialization | JSON + lists | Pickle + NumPy arrays | Development: JSON, Production: Pickle |\n| Error Handling | Basic assertions | Custom exception hierarchy | M1: Assertions, M2+: Custom exceptions |\n\n#### Recommended File Structure\n\nThe data types should be organized to reflect their role as foundational system components:\n\n```\nlinear_regression/\n  data_types/\n    __init__.py              ← exports all public types\n    dataset.py               ← Dataset class and utilities  \n    model_parameters.py      ← ModelParameters class\n    training_history.py      ← TrainingHistory class\n    prediction_result.py     ← PredictionResult class\n    validation.py            ← data validation utilities\n  tests/\n    test_data_types.py       ← comprehensive type tests\n    test_validation.py       ← validation logic tests\n```\n\n#### Infrastructure Starter Code\n\nComplete implementation of data validation utilities that learners can use immediately:\n\n```python\n# validation.py - Complete validation utilities\nimport numpy as np\nfrom typing import Optional, List, Tuple, Any\n\ndef validate_array_dimensions(features: np.ndarray, targets: np.ndarray) -> None:\n    \"\"\"Validate that feature and target arrays have compatible dimensions.\"\"\"\n    if not isinstance(features, np.ndarray) or not isinstance(targets, np.ndarray):\n        raise TypeError(\"Features and targets must be NumPy arrays\")\n    \n    if features.ndim != 2:\n        raise ValueError(f\"Features must be 2D array, got {features.ndim}D\")\n    \n    if targets.ndim != 1:\n        raise ValueError(f\"Targets must be 1D array, got {targets.ndim}D\")\n    \n    if features.shape[0] != targets.shape[0]:\n        raise ValueError(f\"Feature samples ({features.shape[0]}) != target samples ({targets.shape[0]})\")\n    \n    if features.shape[0] == 0:\n        raise ValueError(\"Cannot work with empty dataset\")\n\ndef validate_numerical_data(data: np.ndarray, name: str) -> None:\n    \"\"\"Validate that array contains only finite numerical values.\"\"\"\n    if not np.isfinite(data).all():\n        nan_count = np.isnan(data).sum()\n        inf_count = np.isinf(data).sum()\n        raise ValueError(f\"{name} contains {nan_count} NaN and {inf_count} infinite values\")\n\nclass ValidationError(Exception):\n    \"\"\"Custom exception for data validation failures.\"\"\"\n    pass\n```\n\n#### Core Logic Skeleton Code\n\nDataset class skeleton with detailed implementation guidance:\n\n```python\n# dataset.py - Core Dataset implementation skeleton\nimport numpy as np\nfrom typing import List, Dict, Optional, Tuple\nfrom .validation import validate_array_dimensions, validate_numerical_data\n\nclass Dataset:\n    \"\"\"Container for regression training and testing data.\"\"\"\n    \n    def __init__(self, features: np.ndarray, targets: np.ndarray, \n                 feature_names: Optional[List[str]] = None):\n        \"\"\"Initialize dataset with feature and target arrays.\n        \n        Args:\n            features: 2D array of shape (n_samples, n_features)\n            targets: 1D array of shape (n_samples,)\n            feature_names: Optional names for each feature column\n        \"\"\"\n        # TODO 1: Validate input arrays using validation utilities\n        # TODO 2: Store features and targets as instance variables\n        # TODO 3: Compute and store n_samples and n_features from array shapes\n        # TODO 4: Initialize feature_names (generate default names if None provided)\n        # TODO 5: Initialize normalization flags and statistics to default values\n        # TODO 6: Validate numerical data contains no NaN or infinite values\n        \n    def normalize_features(self) -> 'Dataset':\n        \"\"\"Apply z-score normalization to features.\"\"\"\n        # TODO 1: Check if already normalized, raise error if so\n        # TODO 2: Compute mean and standard deviation for each feature column\n        # TODO 3: Handle zero standard deviation (constant features) appropriately  \n        # TODO 4: Apply z-score transformation: (x - mean) / std\n        # TODO 5: Store normalization statistics for later use with new data\n        # TODO 6: Set is_normalized flag to True\n        # TODO 7: Return self for method chaining\n        \n    def apply_normalization(self, features: np.ndarray) -> np.ndarray:\n        \"\"\"Apply stored normalization to new feature data.\"\"\"\n        # TODO 1: Check that dataset has been normalized (has stored statistics)\n        # TODO 2: Validate that input features have correct number of columns\n        # TODO 3: Apply same mean/std transformation used on training data\n        # TODO 4: Handle edge case where stored std is zero (constant features)\n        # TODO 5: Return normalized feature array\n```\n\nModelParameters class with milestone-specific evolution:\n\n```python\n# model_parameters.py - Model parameter storage\nimport numpy as np\nfrom typing import Optional\n\nclass ModelParameters:\n    \"\"\"Storage for learned regression parameters.\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize empty parameter structure.\"\"\"\n        # TODO 1: Initialize single regression parameters (slope_, intercept_) \n        # TODO 2: Initialize multiple regression parameters (weights_)\n        # TODO 3: Set is_fitted_ flag to False\n        # TODO 4: Initialize metadata fields (fitting_method_, feature_count_, etc.)\n        # TODO 5: Set regularization_strength_ to default value\n        \n    def set_single_regression_params(self, slope: float, intercept: float, \n                                   method: str = \"closed_form\") -> None:\n        \"\"\"Set parameters for single-variable regression (M1, M2).\"\"\"\n        # TODO 1: Validate that slope and intercept are finite numbers\n        # TODO 2: Store slope_ and intercept_ values\n        # TODO 3: Set feature_count_ to 1 for single regression\n        # TODO 4: Record fitting_method_ used\n        # TODO 5: Set is_fitted_ flag to True\n        \n    def set_multiple_regression_params(self, weights: np.ndarray, method: str = \"gradient_descent\",\n                                     regularization: float = 0.0) -> None:\n        \"\"\"Set parameters for multiple regression (M3).\"\"\"\n        # TODO 1: Validate weights array is 1D and contains finite values\n        # TODO 2: Store weights_ array and extract intercept if included\n        # TODO 3: Set feature_count_ based on weights array length\n        # TODO 4: Record fitting method and regularization strength\n        # TODO 5: Set is_fitted_ flag to True\n        \n    def validate_for_prediction(self, n_features: int) -> None:\n        \"\"\"Validate parameters are ready for prediction with given feature count.\"\"\"\n        # TODO 1: Check that model has been fitted (is_fitted_ is True)\n        # TODO 2: Validate that input feature count matches trained feature count\n        # TODO 3: Ensure appropriate parameters are set (slope/intercept or weights)\n        # TODO 4: Raise descriptive errors for any validation failures\n```\n\n#### Milestone Checkpoints\n\n**Milestone 1 Checkpoint:**\nAfter implementing the basic data types, learners should be able to:\n- Create a `Dataset` from NumPy arrays with automatic dimension validation\n- Initialize `ModelParameters` and set single regression coefficients  \n- Generate synthetic data and verify dataset properties match expectations\n- Run: `python -m pytest tests/test_data_types.py::test_dataset_creation`\n- Expected: All dataset creation tests pass, including edge cases for empty data and dimension mismatches\n\n**Milestone 2 Checkpoint:**  \nWith training history support added:\n- Create `TrainingHistory` and populate it during mock gradient descent iterations\n- Verify cost history shows decreasing trend for well-behaved optimization\n- Test convergence detection logic with various termination scenarios\n- Run: `python -c \"from data_types import TrainingHistory; h = TrainingHistory(); print('History initialized')\"`\n- Expected: No import errors, history object created successfully\n\n**Milestone 3 Checkpoint:**\nWith multiple regression support:\n- Create datasets with multiple features and verify normalization works correctly\n- Set multiple regression parameters and validate prediction readiness  \n- Test prediction result generation with confidence intervals\n- Run: Multi-feature synthetic dataset creation and parameter fitting test\n- Expected: All dimension validations pass, normalization preserves relationships\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| \"shapes not aligned\" error | Feature array wrong dimensions | Check `features.shape`, should be (n_samples, n_features) | Use `reshape(-1, 1)` for single features |\n| Predictions wildly incorrect | Used unnormalized test data with normalized model | Check `dataset.is_normalized` flag | Apply same normalization to prediction data |\n| Memory usage grows during training | Storing full parameter history | Monitor `parameter_history_` size | Store history every N iterations, not every iteration |\n| \"model not fitted\" error | Forgot to set `is_fitted_` flag | Check `model.is_fitted_` after training | Set flag to True after successful parameter estimation |\n| NaN in results | Division by zero in normalization | Check for constant features (std = 0) | Add small epsilon to standard deviation or remove constant features |\n\n\n## Data Handler Component\n\n> **Milestone(s):** All milestones (M1: Simple Linear Regression - provides CSV loading and basic validation, M2: Gradient Descent - adds feature normalization, M3: Multiple Linear Regression - extends to multi-feature datasets and matrix construction)\n\n### Mental Model: The Data Preparation Kitchen\n\nThink of the Data Handler as the prep kitchen in a high-end restaurant. Before any cooking begins, skilled prep cooks transform raw ingredients into perfectly measured, cleaned, and organized components that the chefs can use immediately. Raw vegetables arrive dirty and irregularly sized, but they leave the prep kitchen washed, chopped to uniform pieces, and organized in labeled containers.\n\nSimilarly, your machine learning data arrives messy and inconsistent. Features might be measured in completely different scales (age in years, income in dollars, temperature in Celsius), contain missing values, or be stored in awkward formats. The Data Handler is your prep kitchen - it takes this raw, chaotic data and transforms it into clean, standardized, numerically stable inputs that your regression algorithms can consume without choking.\n\nJust as a prep cook knows that onions must be diced uniformly so they cook evenly, the Data Handler knows that features must be scaled uniformly so gradient descent converges reliably. And just as the prep kitchen validates that ingredients are fresh and safe before using them, the Data Handler validates that your data is numerically sound and properly formatted before feeding it to your model.\n\nThe critical insight is that **data preprocessing is not optional cleanup work - it's the foundation that determines whether your machine learning algorithm will succeed or fail**. A model trained on poorly prepared data is like a meal cooked with rotten ingredients: no amount of culinary skill can save it.\n\n![Data Preprocessing Pipeline](./diagrams/data-preprocessing-flow.svg)\n\n### Responsibilities and Interface\n\nThe Data Handler Component serves as the gateway between raw data sources and the machine learning pipeline. It owns three fundamental responsibilities: **data ingestion**, **data validation**, and **data transformation**. Each responsibility represents a critical checkpoint that prevents downstream failures and ensures numerical stability throughout the training process.\n\n#### Data Ingestion Responsibilities\n\nThe data ingestion layer handles the mechanical aspects of loading data from various sources. For this educational implementation, we focus primarily on CSV file loading, but the design allows for extension to other formats. The ingestion process must be robust enough to handle real-world messiness: inconsistent column ordering, varying data types, missing headers, and encoding issues.\n\nThe `load_csv_data` function serves as the primary entry point for data ingestion. It must intelligently handle column selection, automatically infer data types where possible, and provide clear error messages when data cannot be loaded. The function signature emphasizes explicit feature and target column specification to avoid ambiguity about what the model should learn.\n\n#### Data Validation Responsibilities\n\nData validation represents the quality control checkpoint where we detect problems before they cause cryptic failures during training. The validation layer checks for numerical soundness, dimensional compatibility, and statistical properties that could cause optimization difficulties.\n\nThe `validate_data` function performs comprehensive checks on both features and targets. It verifies that arrays have compatible shapes, contain only finite numerical values, and possess sufficient variation for meaningful model fitting. This validation prevents common failure modes like attempting to fit a line to constant data or passing NaN values to gradient descent algorithms.\n\n#### Data Transformation Responsibilities\n\nThe transformation layer prepares validated data for optimal algorithm performance. This includes feature scaling, matrix construction for multiple regression, and maintaining the metadata necessary to apply identical transformations to new prediction data.\n\nThe `normalize_features` function implements z-score normalization, transforming each feature to zero mean and unit variance. This transformation is crucial for gradient descent convergence when features have vastly different scales. The function must store normalization statistics to ensure prediction data receives identical scaling.\n\n#### Component Interface Specification\n\n| Method Name | Parameters | Returns | Description |\n|-------------|------------|---------|-------------|\n| `load_csv_data` | `filename: str, feature_columns: List[str], target_column: str` | `Tuple[np.ndarray, np.ndarray]` | Load and parse CSV data into feature and target arrays |\n| `validate_data` | `features: np.ndarray, targets: np.ndarray` | `None` | Validate data compatibility and raise exceptions for invalid data |\n| `normalize_features` | `features: np.ndarray` | `np.ndarray` | Apply z-score normalization and return scaled features |\n| `generate_linear_data` | `n_samples: int, slope: float, intercept: float, noise_std: float, x_range: Tuple[float, float]` | `Tuple[np.ndarray, np.ndarray]` | Create synthetic linear dataset for testing |\n| `validate_array_dimensions` | `features: np.ndarray, targets: np.ndarray` | `None` | Check array shape compatibility for regression |\n| `validate_numerical_data` | `data: np.ndarray, name: str` | `None` | Check for NaN, infinite, or non-numeric values |\n\nThe interface design emphasizes **explicit parameter specification** over implicit behavior. Rather than guessing which columns contain features, the caller must explicitly specify feature and target columns. This design choice prevents silent errors where the wrong data gets used for training.\n\nEach validation function follows the **fail-fast principle**: they either succeed silently or raise descriptive exceptions immediately. This approach helps learners understand exactly what went wrong and where, rather than encountering mysterious failures deep in the training loop.\n\n### Data Validation and Normalization\n\n#### Comprehensive Data Validation Strategy\n\nData validation forms the first line of defense against numerical instabilities and training failures. The validation process operates in multiple phases, each designed to catch different categories of problems before they can cause downstream issues.\n\n**Phase 1: Structure Validation** checks that the basic array structures are compatible with linear regression requirements. This includes verifying that feature arrays are two-dimensional (samples × features), target arrays are one-dimensional, and both arrays have the same number of samples. Structure validation catches shape mismatches that would cause matrix multiplication failures during training.\n\n**Phase 2: Numerical Validation** examines the actual numerical content of the arrays. It detects NaN values, infinite values, and non-numeric content that could break optimization algorithms. The validator also checks for degenerate cases like constant features (zero variance) that would cause division-by-zero errors in normalization.\n\n**Phase 3: Statistical Validation** analyzes the statistical properties of the data to identify conditions that could cause optimization difficulties. This includes checking for extremely small variances that could cause numerical precision issues, highly correlated features that might cause instability, and target value ranges that could cause gradient explosion.\n\n| Validation Check | Detection Method | Failure Condition | Error Message |\n|------------------|------------------|-------------------|---------------|\n| Array Shapes | `features.shape[0] == targets.shape[0]` | Sample count mismatch | \"Feature array has {n} samples but target array has {m} samples\" |\n| Dimensionality | `features.ndim == 2, targets.ndim == 1` | Wrong array dimensions | \"Features must be 2D array (samples, features), targets must be 1D\" |\n| Numeric Values | `np.isfinite(data).all()` | NaN or infinite values | \"Found {count} non-finite values in {array_name}\" |\n| Feature Variance | `np.var(features, axis=0) > 1e-10` | Constant features | \"Feature '{name}' has zero variance (constant values)\" |\n| Sample Count | `len(targets) >= 2` | Insufficient data | \"Need at least 2 samples for regression, got {n}\" |\n| Target Range | `np.ptp(targets) > 1e-10` | Constant targets | \"Target values are constant, cannot fit regression model\" |\n\n#### Z-Score Normalization Implementation\n\nFeature normalization addresses one of the most common causes of gradient descent failure: features with vastly different scales. When one feature ranges from 0-1 (like a percentage) and another ranges from 0-100000 (like income), the optimization landscape becomes elongated and difficult to navigate. Gradient descent may oscillate wildly or converge extremely slowly.\n\nZ-score normalization transforms each feature to have zero mean and unit variance using the formula: `(x - μ) / σ`. This transformation preserves the relative relationships between data points while ensuring all features contribute equally to the optimization process.\n\nThe normalization process must be **stateful** - it stores the mean and standard deviation computed from training data and applies these exact same statistics to new prediction data. This ensures that prediction inputs undergo identical transformations to training inputs.\n\n| Normalization Step | Formula | Purpose | Implementation Note |\n|-------------------|---------|---------|-------------------|\n| Compute Mean | `μ = np.mean(features, axis=0)` | Center distribution at zero | Computed per feature column |\n| Compute Std Dev | `σ = np.std(features, axis=0)` | Scale to unit variance | Add epsilon to prevent division by zero |\n| Apply Transform | `(features - μ) / σ` | Normalize each feature | Broadcast operations for efficiency |\n| Store Statistics | Save μ, σ in Dataset | Enable prediction normalization | Must persist with trained model |\n\n> **Critical Design Insight**: Normalization statistics must be computed from training data only, never from the combination of training and test data. Computing statistics from test data would constitute **data leakage** - using future information to influence model training.\n\n#### Missing Value Handling Strategy\n\nWhile this educational implementation focuses on complete datasets, real-world data often contains missing values. The Data Handler includes basic missing value detection and provides clear guidance on handling strategies.\n\n**Detection Phase**: The system scans for common missing value indicators including NaN values, None entries, empty strings, and placeholder values like -999 or \"NULL\". Detection operates before any numerical processing to prevent missing indicators from being interpreted as valid data.\n\n**Handling Strategies**: For educational purposes, the system requires complete data and raises informative errors when missing values are detected. The error messages guide learners toward appropriate handling strategies: removing incomplete samples (listwise deletion), filling with statistical measures (mean imputation), or using more sophisticated techniques like multiple imputation.\n\n### Architecture Decision Records\n\n#### Decision: NumPy vs Pure Python for Numerical Operations\n\n**Context**: The implementation must handle numerical computations for data loading, validation, and transformation. Pure Python lists and loops would be simpler for beginners to understand, while NumPy arrays provide vectorized operations and better numerical stability.\n\n**Options Considered**:\n1. **Pure Python**: Use standard Python lists, loops, and basic math operations\n2. **NumPy Arrays**: Use NumPy for all numerical data and operations\n3. **Hybrid Approach**: Use Python lists for small operations, NumPy for large computations\n\n| Option | Pros | Cons |\n|--------|------|------|\n| Pure Python | Familiar syntax, no dependencies, easier debugging | Slow performance, numerical instability, verbose matrix operations |\n| NumPy Arrays | Vectorized performance, numerical stability, broadcasting | Additional dependency, learning curve for beginners |\n| Hybrid Approach | Gradual complexity increase | Inconsistent interfaces, type conversion overhead |\n\n**Decision**: Use NumPy arrays for all numerical operations throughout the Data Handler.\n\n**Rationale**: While NumPy has a learning curve, the benefits far outweigh the costs for a machine learning implementation. Numerical stability is crucial for regression algorithms - Python's floating-point arithmetic can accumulate errors that cause training failures. NumPy's vectorized operations also teach learners the importance of efficient computation in ML. The educational value of learning NumPy operations outweighs the initial complexity.\n\n**Consequences**: Learners must understand basic NumPy operations (array creation, indexing, broadcasting) before implementing the Data Handler. However, this knowledge directly transfers to other ML libraries and production systems. The implementation is more robust and performs better than pure Python alternatives.\n\n#### Decision: Normalization Strategy Selection\n\n**Context**: Multiple feature scaling approaches exist, each with different properties and use cases. The choice affects gradient descent convergence and model interpretability.\n\n**Options Considered**:\n1. **Z-Score Normalization**: Transform to zero mean, unit variance\n2. **Min-Max Scaling**: Transform to fixed range like [0,1]  \n3. **Robust Scaling**: Use median and IQR instead of mean and standard deviation\n4. **No Scaling**: Leave features in original scales\n\n| Option | Pros | Cons |\n|--------|------|------|\n| Z-Score | Handles outliers reasonably, standard approach | Assumes roughly normal distribution |\n| Min-Max | Bounded output range, preserves zero values | Sensitive to outliers, can compress most data |\n| Robust Scaling | Very resilient to outliers | Less standard, may not fully address scale differences |\n| No Scaling | Preserves original meaning | Causes gradient descent convergence problems |\n\n**Decision**: Implement z-score normalization as the primary scaling method.\n\n**Rationale**: Z-score normalization provides the best balance of numerical stability, theoretical soundness, and educational value. It directly addresses the gradient descent convergence issues that learners will encounter, and the mathematical concept (standardization) appears throughout statistics and machine learning. The transformation is reversible and preserves the relative relationships between data points.\n\n**Consequences**: The implementation must handle edge cases like zero-variance features. Learners will understand why feature scaling matters and gain experience with a fundamental preprocessing technique. The approach works well for most datasets learners will encounter.\n\n#### Decision: Error Handling Philosophy\n\n**Context**: Data validation can fail in numerous ways, and the system must decide how to communicate failures to learners effectively.\n\n**Options Considered**:\n1. **Silent Fixing**: Automatically handle problems without notification\n2. **Warning-Based**: Issue warnings but continue processing\n3. **Strict Validation**: Raise exceptions for any data quality issues\n4. **Configurable Strictness**: Allow users to choose validation level\n\n| Option | Pros | Cons |\n|--------|------|------|\n| Silent Fixing | Convenient, never blocks training | Hides important data quality issues |\n| Warning-Based | Alerts user but allows progress | Warnings often ignored, problems persist |\n| Strict Validation | Forces awareness of data quality | Can be frustrating for beginners |\n| Configurable | Flexible for different experience levels | Adds complexity, may enable bad practices |\n\n**Decision**: Implement strict validation with descriptive error messages.\n\n**Rationale**: For an educational implementation, understanding data quality is as important as understanding algorithms. Silent fixes prevent learning, and warnings are easily ignored. Strict validation forces learners to confront real-world data issues and develop good practices. Descriptive error messages turn failures into learning opportunities.\n\n**Consequences**: Learners may initially find the strict validation frustrating, but they will develop better data handling practices. The implementation requires comprehensive error message design to be educational rather than obstructive. This approach builds habits that transfer to production systems.\n\n#### Decision: Dataset Container Design\n\n**Context**: The system needs to package processed data with metadata for use by regression components. This could be done with simple tuples, dictionary structures, or custom classes.\n\n**Options Considered**:\n1. **Simple Tuples**: Return `(features, targets)` tuples from processing functions\n2. **Dictionary Bundles**: Use dictionaries with standardized keys for data and metadata  \n3. **Dataset Class**: Create a structured `Dataset` class with methods and properties\n4. **NamedTuple**: Use typing.NamedTuple for structure without full class complexity\n\n| Option | Pros | Cons |\n|--------|------|------|\n| Simple Tuples | Minimal complexity, familiar pattern | No metadata storage, error-prone unpacking |\n| Dictionary Bundles | Flexible, can store arbitrary metadata | No type checking, key name inconsistencies |\n| Dataset Class | Type safety, clear interface, extensible | More complex for beginners to understand |\n| NamedTuple | Structured but simple, immutable | Limited extensibility, no methods |\n\n**Decision**: Implement a structured `Dataset` class with clear properties and methods.\n\n**Rationale**: The educational value of understanding structured data containers outweighs the added complexity. A well-designed class teaches object-oriented design principles while providing type safety and clear interfaces. The metadata storage capabilities are essential for features like normalization statistics persistence.\n\n**Consequences**: Learners must understand basic class design and property access. The implementation provides a template for structured data handling that applies to larger ML systems. The class design enables cleaner interfaces between components.\n\n### Common Pitfalls\n\n#### ⚠️ **Pitfall: Using Python Lists Instead of NumPy Arrays**\n\nMany beginners attempt to use standard Python lists for numerical data, leading to performance problems and type inconsistencies throughout the system. Python lists are designed for general-purpose data storage, not numerical computation.\n\n**Why This Fails**: Python lists store objects with type checking overhead, making numerical operations slow. Mathematical operations like element-wise multiplication require explicit loops instead of vectorized operations. Type checking becomes inconsistent when mixing lists and NumPy arrays from other components.\n\n**How to Avoid**: Always convert input data to NumPy arrays immediately after loading. Use `np.array()` for small datasets or `np.loadtxt()`/`np.genfromtxt()` for CSV loading. Verify array types with `isinstance(data, np.ndarray)` in validation functions.\n\n**Detection**: Watch for error messages about unsupported operations between lists and arrays, or surprisingly slow performance on medium-sized datasets (>1000 samples).\n\n#### ⚠️ **Pitfall: Forgetting to Handle Division by Zero in Normalization**\n\nZ-score normalization divides by standard deviation, which can be zero for constant features. This causes NaN values that propagate through the entire training process, leading to mysterious convergence failures.\n\n**Why This Fails**: Constant features have zero variance, making the standard deviation zero. The normalization formula `(x - μ) / σ` becomes `(x - μ) / 0`, producing NaN values. These NaN values break gradient calculations and cause optimization algorithms to fail silently or produce nonsensical results.\n\n**How to Avoid**: Check for zero variance before normalization using `np.var(features, axis=0) < epsilon`. Either remove constant features, add small epsilon values to prevent division by zero, or raise informative errors explaining the problem.\n\n**Detection**: Look for NaN values appearing after normalization, or gradient descent producing NaN costs after the first few iterations.\n\n#### ⚠️ **Pitfall: Applying Different Normalization to Training and Prediction Data**\n\nA critical error occurs when normalization statistics are recomputed for prediction data instead of using the statistics from training data. This breaks the fundamental assumption that training and prediction inputs occupy the same feature space.\n\n**Why This Fails**: If prediction data has different mean/variance than training data (which is normal), recomputing normalization statistics creates a different transformation. The model was trained on data normalized with training statistics, so applying different normalization makes prediction inputs incompatible with the learned parameters.\n\n**How to Avoid**: Store normalization statistics (mean and standard deviation) during training and apply these exact values to prediction data. Never call `normalize_features()` on prediction data - instead use `apply_normalization()` with stored statistics.\n\n**Detection**: Predictions that seem completely wrong despite good training performance, or prediction errors that scale with how different the prediction data distribution is from training data.\n\n#### ⚠️ **Pitfall: Inadequate Data Type Validation**\n\nLoading CSV data often produces string arrays or mixed-type arrays instead of numerical arrays, causing subtle errors during mathematical operations. Pandas and basic CSV readers may interpret numerical data as strings, especially with missing values or formatting inconsistencies.\n\n**Why This Fails**: String arrays support some mathematical operations without immediate errors, but produce incorrect results. For example, `\"1\" + \"2\"` equals `\"12\"` instead of `3`. These errors can be difficult to debug because they don't always cause immediate exceptions.\n\n**How to Avoid**: Explicitly validate data types after loading using `np.issubdtype(data.dtype, np.number)`. Use `dtype=float` parameters in loading functions. Implement comprehensive type checking in validation functions before any mathematical operations.\n\n**Detection**: Mathematical operations producing string concatenation results, or arrays with `object` or `<U` (Unicode string) dtypes when numerical data was expected.\n\n#### ⚠️ **Pitfall: Ignoring Feature Scale Differences During Debugging**\n\nWhen gradient descent fails to converge, beginners often adjust learning rates or iteration counts without examining feature scales. Unscaled features can create optimization landscapes where reasonable learning rates cause oscillation or extremely slow convergence.\n\n**Why This Fails**: Features with vastly different scales (age: 0-100, income: 0-100000) create cost function contours that are elongated ellipses instead of circles. Gradient descent must take tiny steps to avoid overshooting in the high-scale dimension, making convergence extremely slow in the low-scale dimension.\n\n**How to Avoid**: Always examine feature scales using `np.mean()` and `np.std()` for each feature before training. If scales differ by more than one order of magnitude, normalization is essential. Implement scale analysis as part of the data validation process.\n\n**Detection**: Gradient descent requiring thousands of iterations to converge, cost oscillating wildly with moderate learning rates, or some parameters changing much faster than others during training.\n\n#### ⚠️ **Pitfall: Insufficient Sample Size Validation**\n\nLinear regression requires more samples than features to avoid overfitting, but beginners often attempt to fit models on datasets with very few samples or more features than samples, leading to perfect training performance that doesn't generalize.\n\n**Why This Fails**: With fewer samples than parameters, the system of equations is underdetermined and has infinite solutions. The model can achieve perfect fit on training data by essentially memorizing it, but performs terribly on new data. Matrix operations may also become numerically unstable.\n\n**How to Avoid**: Validate that sample count significantly exceeds feature count (recommended minimum: 10 samples per feature). Implement explicit checks in data validation and provide clear error messages explaining the statistical requirements.\n\n**Detection**: Training accuracy that seems too good to be true (R-squared = 1.0), or models that perform perfectly on training data but terribly on any new data.\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Data Loading | `csv.reader` + manual parsing | `pandas.read_csv()` with full feature support |\n| Numerical Arrays | `numpy` with basic operations | `numpy` with advanced broadcasting and vectorization |\n| Data Validation | Manual checks with if/else logic | `jsonschema` or custom validation framework |\n| File Handling | Basic `open()` with exception handling | `pathlib.Path` for robust path operations |\n| Error Reporting | Simple exception raising | Structured logging with detailed error context |\n\n#### Recommended File Structure\n\n```\nlinear_regression_project/\n├── data/\n│   ├── __init__.py\n│   ├── handler.py              ← Main DataHandler class (core learning goal)\n│   ├── validation.py           ← Data validation utilities (infrastructure)\n│   ├── synthetic.py            ← Synthetic data generation (infrastructure)\n│   └── examples/\n│       ├── housing.csv         ← Sample dataset\n│       └── synthetic_linear.csv\n├── tests/\n│   ├── test_data_handler.py    ← Comprehensive unit tests\n│   └── fixtures/\n│       └── test_data.csv\n└── utils/\n    ├── __init__.py\n    └── constants.py            ← Shared constants and tolerances\n```\n\nThis structure separates the core learning component (`handler.py`) from supporting infrastructure, allowing learners to focus on the essential data processing logic while having robust utilities available.\n\n#### Infrastructure Starter Code\n\n**File: `data/validation.py`** (Complete implementation)\n```python\nimport numpy as np\nfrom typing import Optional, List, Tuple\nimport warnings\n\ndef validate_array_dimensions(features: np.ndarray, targets: np.ndarray) -> None:\n    \"\"\"Validate that feature and target arrays have compatible dimensions.\"\"\"\n    if features.ndim != 2:\n        raise ValueError(f\"Features must be 2D array (samples, features), got {features.ndim}D\")\n    \n    if targets.ndim != 1:\n        raise ValueError(f\"Targets must be 1D array, got {targets.ndim}D\")\n    \n    if features.shape[0] != targets.shape[0]:\n        raise ValueError(\n            f\"Feature array has {features.shape[0]} samples \"\n            f\"but target array has {targets.shape[0]} samples\"\n        )\n\ndef validate_numerical_data(data: np.ndarray, name: str) -> None:\n    \"\"\"Check for NaN, infinite, or non-numeric values in array.\"\"\"\n    if not np.issubdtype(data.dtype, np.number):\n        raise ValueError(f\"{name} array contains non-numeric data (dtype: {data.dtype})\")\n    \n    if not np.isfinite(data).all():\n        nan_count = np.isnan(data).sum()\n        inf_count = np.isinf(data).sum()\n        raise ValueError(\n            f\"Found {nan_count} NaN and {inf_count} infinite values in {name} array\"\n        )\n\ndef check_feature_variance(features: np.ndarray, min_variance: float = 1e-10) -> List[int]:\n    \"\"\"Check for constant features and return indices of problematic features.\"\"\"\n    variances = np.var(features, axis=0)\n    constant_features = np.where(variances < min_variance)[0]\n    return constant_features.tolist()\n\ndef validate_sample_size(n_samples: int, n_features: int, min_ratio: int = 10) -> None:\n    \"\"\"Ensure sufficient samples relative to feature count.\"\"\"\n    if n_samples < 2:\n        raise ValueError(f\"Need at least 2 samples for regression, got {n_samples}\")\n    \n    if n_samples < n_features * min_ratio:\n        warnings.warn(\n            f\"Only {n_samples} samples for {n_features} features. \"\n            f\"Recommend at least {n_features * min_ratio} samples.\"\n        )\n```\n\n**File: `data/synthetic.py`** (Complete implementation)\n```python\nimport numpy as np\nfrom typing import Tuple\n\ndef generate_linear_data(\n    n_samples: int, \n    slope: float, \n    intercept: float, \n    noise_std: float = 0.1,\n    x_range: Tuple[float, float] = (0, 10),\n    random_seed: Optional[int] = None\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Generate synthetic linear dataset for testing and learning.\"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    \n    # Generate evenly spaced x values with some random jitter\n    x_base = np.linspace(x_range[0], x_range[1], n_samples)\n    x_jitter = np.random.normal(0, (x_range[1] - x_range[0]) * 0.02, n_samples)\n    x = x_base + x_jitter\n    \n    # Generate y values with linear relationship plus noise\n    y_perfect = slope * x + intercept\n    noise = np.random.normal(0, noise_std, n_samples)\n    y = y_perfect + noise\n    \n    # Reshape x to be 2D (samples, 1)\n    X = x.reshape(-1, 1)\n    \n    return X, y\n\ndef generate_multiple_linear_data(\n    n_samples: int,\n    weights: np.ndarray,\n    intercept: float,\n    noise_std: float = 0.1,\n    feature_ranges: Optional[List[Tuple[float, float]]] = None,\n    random_seed: Optional[int] = None\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Generate synthetic multiple linear regression dataset.\"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    \n    n_features = len(weights)\n    \n    if feature_ranges is None:\n        feature_ranges = [(0, 10)] * n_features\n    \n    # Generate features\n    X = np.zeros((n_samples, n_features))\n    for i, (low, high) in enumerate(feature_ranges):\n        X[:, i] = np.random.uniform(low, high, n_samples)\n    \n    # Generate targets\n    y_perfect = X @ weights + intercept\n    noise = np.random.normal(0, noise_std, n_samples)\n    y = y_perfect + noise\n    \n    return X, y\n```\n\n#### Core Logic Skeleton Code\n\n**File: `data/handler.py`** (Learning implementation)\n```python\nimport numpy as np\nimport csv\nfrom typing import List, Tuple, Dict, Optional\nfrom dataclasses import dataclass\nfrom .validation import validate_array_dimensions, validate_numerical_data, check_feature_variance\n\n@dataclass\nclass Dataset:\n    \"\"\"Container for processed dataset with metadata.\"\"\"\n    features: np.ndarray\n    targets: np.ndarray\n    feature_names: List[str]\n    n_samples: int\n    n_features: int\n    is_normalized: bool\n    normalization_stats: Dict[str, np.ndarray]\n    \n    def normalize_features(self) -> 'Dataset':\n        \"\"\"Apply z-score normalization to features and return new Dataset.\"\"\"\n        # TODO 1: Check if already normalized, raise error if so\n        # TODO 2: Compute mean and std for each feature using np.mean, np.std\n        # TODO 3: Handle zero-variance features (add epsilon or raise error)\n        # TODO 4: Apply transformation: (features - mean) / std\n        # TODO 5: Store normalization statistics for later use\n        # TODO 6: Return new Dataset with normalized features and updated metadata\n        # Hint: Use axis=0 for column-wise operations\n        pass\n    \n    def apply_normalization(self, features: np.ndarray) -> np.ndarray:\n        \"\"\"Apply stored normalization statistics to new feature data.\"\"\"\n        # TODO 1: Check that normalization statistics exist\n        # TODO 2: Validate that input features have correct number of features\n        # TODO 3: Apply stored mean and std: (features - stored_mean) / stored_std\n        # TODO 4: Return normalized features\n        # Hint: Use broadcasting for efficient computation\n        pass\n\nclass DataHandler:\n    \"\"\"Handles data loading, validation, and preprocessing for linear regression.\"\"\"\n    \n    def __init__(self):\n        self.tolerance = 1e-10  # For numerical comparisons\n    \n    def load_csv_data(\n        self, \n        filename: str, \n        feature_columns: List[str], \n        target_column: str\n    ) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Load data from CSV file and return feature and target arrays.\"\"\"\n        # TODO 1: Open CSV file and read header row\n        # TODO 2: Find indices of specified feature and target columns\n        # TODO 3: Read data rows and extract specified columns\n        # TODO 4: Convert string data to float arrays using np.array(dtype=float)\n        # TODO 5: Validate that all specified columns exist\n        # TODO 6: Handle file not found and parsing errors with informative messages\n        # Hint: Use csv.DictReader for easier column access\n        pass\n    \n    def validate_data(self, features: np.ndarray, targets: np.ndarray) -> None:\n        \"\"\"Comprehensive data validation for regression compatibility.\"\"\"\n        # TODO 1: Check array dimensions using validate_array_dimensions\n        # TODO 2: Validate numerical data using validate_numerical_data\n        # TODO 3: Check for sufficient samples (at least 2)\n        # TODO 4: Identify constant features using check_feature_variance\n        # TODO 5: Check target variance (constant targets can't be modeled)\n        # TODO 6: Raise informative errors for each validation failure\n        # Hint: Call validation utilities from validation.py\n        pass\n    \n    def create_dataset(\n        self,\n        features: np.ndarray,\n        targets: np.ndarray,\n        feature_names: Optional[List[str]] = None\n    ) -> Dataset:\n        \"\"\"Create a Dataset object with validation and metadata.\"\"\"\n        # TODO 1: Validate input data using validate_data\n        # TODO 2: Generate default feature names if not provided\n        # TODO 3: Extract dataset dimensions\n        # TODO 4: Initialize empty normalization statistics\n        # TODO 5: Create and return Dataset object\n        # Hint: Use f\"feature_{i}\" for default feature names\n        pass\n```\n\n#### Language-Specific Hints\n\n**NumPy Broadcasting**: When applying normalization, NumPy's broadcasting allows you to subtract a 1D mean array from a 2D feature array automatically. `features - mean` works even though shapes are (n_samples, n_features) and (n_features,).\n\n**CSV Handling**: Use `csv.DictReader` instead of basic `csv.reader` for easier column access by name. Handle encoding issues by specifying `encoding='utf-8'` in the file open call.\n\n**Error Context**: When raising exceptions, include the actual values that caused the problem: `f\"Expected {expected} but got {actual}\"`. This makes debugging much easier for learners.\n\n**Memory Efficiency**: For large datasets, consider using `np.loadtxt()` or `np.genfromtxt()` instead of manual CSV parsing. These functions handle type conversion and missing values more efficiently.\n\n**Type Hints**: Use `Optional[List[str]]` for parameters that can be None, and `Tuple[np.ndarray, np.ndarray]` for functions returning multiple arrays.\n\n#### Milestone Checkpoint\n\n**After implementing data loading (M1 focus):**\n```bash\npython -c \"\nfrom data.handler import DataHandler\nfrom data.synthetic import generate_linear_data\n\n# Test synthetic data generation\nX, y = generate_linear_data(100, slope=2.0, intercept=1.0)\nprint(f'Generated data: X shape {X.shape}, y shape {y.shape}')\n\n# Test data handler creation\nhandler = DataHandler()\ndataset = handler.create_dataset(X, y, ['feature_1'])\nprint(f'Dataset: {dataset.n_samples} samples, {dataset.n_features} features')\n\"\n```\n\n**Expected Output**:\n```\nGenerated data: X shape (100, 1), y shape (100,)\nDataset: 100 samples, 1 features\n```\n\n**After implementing normalization (M2 focus):**\n```bash\npython -c \"\nimport numpy as np\nfrom data.handler import DataHandler, Dataset\n\n# Create test data with different scales\nfeatures = np.array([[1, 1000], [2, 2000], [3, 3000]], dtype=float)\ntargets = np.array([10, 20, 30], dtype=float)\n\nhandler = DataHandler()\ndataset = handler.create_dataset(features, targets, ['small_scale', 'large_scale'])\nnormalized_dataset = dataset.normalize_features()\n\nprint('Original feature means:', np.mean(dataset.features, axis=0))\nprint('Normalized feature means:', np.mean(normalized_dataset.features, axis=0))\nprint('Normalized feature stds:', np.std(normalized_dataset.features, axis=0))\n\"\n```\n\n**Expected Output**:\n```\nOriginal feature means: [   2. 2000.]\nNormalized feature means: [ 2.22044605e-17 -1.48029737e-17]  # Approximately zero\nNormalized feature stds: [1. 1.]  # Exactly 1.0\n```\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| \"unsupported operand type\" errors | Using Python lists instead of NumPy arrays | Check `type(data)` before operations | Convert with `np.array(data, dtype=float)` |\n| NaN values in normalized data | Division by zero in std calculation | Check `np.var(features, axis=0)` for zeros | Add epsilon or remove constant features |\n| \"shapes not aligned\" in matrix operations | Wrong array dimensions | Print `array.shape` for all arrays | Reshape with `array.reshape(-1, 1)` or transpose |\n| Very slow convergence in later milestones | Features not normalized | Check `np.mean()` and `np.std()` of each feature | Apply z-score normalization |\n| Perfect training accuracy (R² = 1.0) | More features than samples | Compare `n_samples` vs `n_features` | Get more data or reduce features |\n| String concatenation instead of addition | CSV data loaded as strings | Check `array.dtype` after loading | Use `dtype=float` in loading functions |\n\n\n## Simple Linear Regression Component\n\n> **Milestone(s):** M1: Simple Linear Regression\n\nThis component implements single-variable linear regression using the closed-form ordinary least squares solution. The `SimpleLinearRegression` class serves as the foundational implementation that demonstrates the mathematical principles of linear modeling before introducing the complexities of iterative optimization. This component establishes the core concepts of model fitting, parameter estimation, and prediction that will be extended in subsequent milestones.\n\nThe simple linear regression component represents the most direct approach to understanding the relationship between a single input variable and a continuous target variable. Unlike iterative methods, the closed-form solution provides an exact mathematical answer that can be computed in a single step, making it ideal for educational purposes and establishing baseline understanding.\n\n### Mental Model: The Best-Fit Line\n\nThink of simple linear regression as finding the \"best-fitting ruler\" through a scattered set of points on graph paper. Imagine you have a collection of data points scattered across a coordinate plane, and your task is to place a straight ruler (representing the regression line) in such a way that it comes as close as possible to all the points simultaneously.\n\nThe \"best\" position for this ruler is not subjective—it's mathematically defined as the line that minimizes the total squared distance from each point to the line. Picture drawing vertical lines from each data point down to your ruler; these vertical distances represent the **residuals** or prediction errors. The optimal ruler position minimizes the sum of the squares of these vertical distances, which is why we call this the \"least squares\" solution.\n\nThis mental model reveals several key insights. First, the ruler (regression line) doesn't need to pass through any specific point—it finds the position that balances all points optimally. Second, points further from the line contribute more to the error because we square the distances, meaning outliers have disproportionate influence. Third, there's only one optimal ruler position for any given set of points, which corresponds to the unique closed-form solution.\n\nThe mathematical beauty of simple linear regression lies in the fact that this \"best ruler position\" can be calculated directly using a formula, without any trial-and-error or iterative searching. The slope and intercept of the optimal line emerge naturally from the statistical properties of the data itself.\n\n### Closed-Form Solution Algorithm\n\nThe ordinary least squares solution provides a direct mathematical formula for computing the optimal slope and intercept parameters. This approach contrasts with iterative methods by solving the optimization problem analytically, leveraging calculus to find the exact minimum of the cost function in a single computation.\n\nThe mathematical foundation rests on the principle that the optimal parameters minimize the sum of squared residuals. For a linear model `y = mx + b`, where `m` is the slope and `b` is the intercept, the residual for each data point `(xi, yi)` is `yi - (mxi + b)`. The cost function becomes the sum of all squared residuals: `∑(yi - mxi - b)²`.\n\nTo find the minimum, we take partial derivatives of this cost function with respect to both `m` and `b`, set them equal to zero, and solve the resulting system of linear equations. This yields the normal equations, which provide direct formulas for the optimal parameters.\n\nThe algorithm proceeds through the following steps:\n\n1. **Data Validation and Preparation**: Verify that input arrays `x` and `y` have the same length and contain only finite numerical values. Convert inputs to NumPy arrays if necessary and ensure they have compatible shapes for mathematical operations.\n\n2. **Compute Sample Statistics**: Calculate the mean of the input features (`x_mean`) and the mean of the target values (`y_mean`). These sample means appear in the closed-form formulas and represent the centroid of the data distribution.\n\n3. **Calculate Slope Using Covariance Formula**: Compute the slope parameter using the formula `m = Σ((xi - x_mean) * (yi - y_mean)) / Σ((xi - x_mean)²)`. The numerator represents the sample covariance between x and y, while the denominator represents the sample variance of x. This ratio quantifies how much y changes, on average, for each unit change in x.\n\n4. **Calculate Intercept Using Means**: Compute the intercept parameter using the formula `b = y_mean - m * x_mean`. This ensures that the regression line passes through the point `(x_mean, y_mean)`, which is the centroid of the data. This property guarantees that the sum of residuals equals zero.\n\n5. **Numerical Stability Check**: Verify that the denominator in the slope calculation is not zero or extremely small (less than tolerance `1e-10`). A zero denominator indicates that all x values are identical, making it impossible to determine a meaningful slope. In such cases, the component should raise a descriptive error message.\n\n6. **Parameter Storage**: Store the computed slope and intercept in the model's state variables `slope_` and `intercept_`, and set the `is_fitted_` flag to `True` to indicate successful training.\n\nThe mathematical elegance of this approach lies in its directness—no iterations, no convergence criteria, no learning rate tuning. The solution is computed exactly in one pass through the data, making it both computationally efficient and numerically stable for well-conditioned problems.\n\n| Parameter | Formula | Interpretation |\n|-----------|---------|----------------|\n| Slope (m) | `Σ((xi - x_mean) * (yi - y_mean)) / Σ((xi - x_mean)²)` | Rate of change of y with respect to x |\n| Intercept (b) | `y_mean - m * x_mean` | Expected y value when x equals zero |\n\n> The critical insight is that the closed-form solution leverages the geometric property that the optimal regression line always passes through the centroid of the data. This constraint, combined with the least squares criterion, uniquely determines both parameters.\n\n### Prediction and Evaluation\n\nOnce the model parameters are fitted, the `SimpleLinearRegression` component can generate predictions for new input values and evaluate model performance using standard regression metrics. The prediction process applies the learned linear relationship to transform input features into target estimates.\n\n**Prediction Generation**: The `predict(x)` method applies the fitted linear model to compute predicted values using the equation `y_pred = slope_ * x + intercept_`. This method accepts either scalar values or NumPy arrays, automatically broadcasting the computation across all input elements. Before prediction, the method validates that the model has been fitted by checking the `is_fitted_` flag, raising an informative error if `fit()` has not been called.\n\nThe prediction process involves several steps:\n\n1. **Model State Validation**: Confirm that `is_fitted_` is `True`, indicating that valid parameters have been computed. This fail-fast approach prevents silent errors from uninitialized parameters.\n\n2. **Input Preparation**: Convert the input to a NumPy array if necessary and verify that it contains only finite numerical values. Handle both scalar inputs and array inputs uniformly through NumPy's broadcasting mechanism.\n\n3. **Linear Transformation**: Apply the fitted equation `y_pred = slope_ * x + intercept_` to generate predictions. NumPy automatically handles broadcasting for array inputs, computing predictions for all elements simultaneously.\n\n4. **Output Formatting**: Return predictions as a NumPy array with the same shape as the input, ensuring consistent interface behavior regardless of input format.\n\n**Model Evaluation**: The `score(x, y)` method computes the R-squared coefficient of determination, which measures the proportion of variance in the target variable that is predictable from the input features. R-squared serves as the primary evaluation metric for regression models, providing an interpretable measure of model performance.\n\nThe R-squared calculation follows this procedure:\n\n1. **Generate Predictions**: Use the fitted model to predict target values for the provided input features, creating the prediction array `y_pred`.\n\n2. **Compute Total Sum of Squares (TSS)**: Calculate the total variance in the target variable using `TSS = Σ(yi - y_mean)²`, where `y_mean` is the mean of the actual target values. This represents the total variation that a model could potentially explain.\n\n3. **Compute Residual Sum of Squares (RSS)**: Calculate the unexplained variance using `RSS = Σ(yi - y_pred_i)²`, representing the variation that remains after applying the model.\n\n4. **Calculate R-squared**: Compute the coefficient of determination using `R² = 1 - RSS/TSS`. This formula gives the fraction of total variance explained by the model.\n\nThe R-squared metric has several important properties. Perfect predictions yield R² = 1, indicating that the model explains all variance in the target variable. A model that performs no better than predicting the mean yields R² = 0. Negative R-squared values are possible and indicate that the model performs worse than simply predicting the mean for all inputs.\n\n| Evaluation Metric | Formula | Interpretation | Range |\n|------------------|---------|----------------|--------|\n| R-squared | `1 - Σ(yi - y_pred_i)² / Σ(yi - y_mean)²` | Proportion of variance explained | [−∞, 1] |\n| Mean Squared Error | `Σ(yi - y_pred_i)² / n` | Average squared prediction error | [0, ∞) |\n| Residual | `yi - y_pred_i` | Individual prediction error | (−∞, ∞) |\n\n> Understanding R-squared interpretation is crucial: R² = 0.8 means the model explains 80% of the variance in the target variable, while the remaining 20% is either noise or systematic patterns not captured by the linear relationship.\n\n### Architecture Decision Records\n\nThis section documents the key design decisions made in implementing the simple linear regression component, providing rationale for architectural choices that impact both educational value and technical implementation.\n\n> **Decision: Closed-Form Solution as Primary Implementation**\n> - **Context**: Linear regression can be solved either through direct analytical computation (closed-form solution using normal equations) or through iterative optimization (gradient descent). For the foundational milestone, we need to choose the primary approach that best serves educational objectives while providing a solid foundation for later extensions.\n> - **Options Considered**: \n>   1. Closed-form solution only\n>   2. Gradient descent only \n>   3. Both approaches with equal emphasis\n> - **Decision**: Implement closed-form solution as the primary method for M1, with gradient descent introduced separately in M2\n> - **Rationale**: The closed-form approach provides immediate mathematical insight into the optimization problem without the complexities of iterative algorithms. Students can see the direct relationship between data statistics and optimal parameters. This builds intuition for the underlying mathematical principles before introducing algorithmic considerations like learning rates and convergence criteria.\n> - **Consequences**: Enables rapid prototyping and validation of the linear regression concept. Provides exact solutions for comparison when implementing gradient descent. Limitations include inability to handle very large datasets efficiently and no extension to non-linear optimization problems.\n\n| Approach | Pros | Cons |\n|----------|------|------|\n| Closed-form only | Exact solution, fast computation, mathematical insight | Limited to linear problems, memory intensive for large datasets |\n| Gradient descent only | Scales to large data, extends to non-linear problems | Requires hyperparameter tuning, convergence complexity |\n| Both approaches | Comprehensive understanding, comparison opportunities | Increased complexity, potential confusion for beginners |\n\n> **Decision: NumPy Arrays as Primary Data Structure**\n> - **Context**: The component needs to handle numerical data efficiently while maintaining compatibility with common Python data science workflows. Options include pure Python lists, NumPy arrays, or supporting multiple input formats with internal conversion.\n> - **Options Considered**:\n>   1. Pure Python lists for simplicity\n>   2. NumPy arrays exclusively\n>   3. Flexible input with automatic conversion\n> - **Decision**: Use NumPy arrays as the primary internal representation with automatic conversion from other formats\n> - **Rationale**: NumPy provides vectorized operations essential for efficient mathematical computation. Broadcasting capabilities handle scalar and array inputs uniformly. The scientific Python ecosystem expects NumPy compatibility. Automatic conversion reduces friction for users while maintaining performance benefits internally.\n> - **Consequences**: Enables efficient computation and broadcasting. Requires NumPy dependency. Provides natural integration with plotting libraries and data analysis workflows. Potential memory overhead for very small datasets, but this is negligible for educational purposes.\n\n> **Decision: Fail-Fast Validation Strategy**\n> - **Context**: The component can encounter various error conditions including incompatible data shapes, non-numeric values, constant input features, and attempts to predict before fitting. The validation strategy determines when and how these errors are detected and reported.\n> - **Options Considered**:\n>   1. Minimal validation with silent failures\n>   2. Comprehensive validation with immediate errors\n>   3. Lazy validation during computation\n> - **Decision**: Implement comprehensive fail-fast validation with descriptive error messages\n> - **Rationale**: Educational implementations benefit from clear, immediate feedback when mistakes occur. Fail-fast prevents silent failures that could lead to incorrect results and confusion. Descriptive error messages guide learners toward correct usage patterns and help diagnose data quality issues.\n> - **Consequences**: Improves debugging experience and reduces frustration for learners. Adds computational overhead for validation checks. Requires careful design of error messages to be educational rather than cryptic. Encourages good practices in data handling and model usage.\n\n> **Decision: Single Responsibility Component Design**\n> - **Context**: The `SimpleLinearRegression` component could potentially include data loading, preprocessing, visualization, and advanced evaluation metrics. The scope decision affects both code organization and learning progression.\n> - **Options Considered**:\n>   1. Monolithic component handling all regression-related tasks\n>   2. Focused component handling only model fitting and prediction\n>   3. Minimal component with external dependencies for all auxiliary functions\n> - **Decision**: Design focused component responsible only for model fitting, prediction, and basic evaluation, with clear interfaces to other components\n> - **Rationale**: Single responsibility principle improves code organization and testing. Allows independent development and understanding of different system aspects. Facilitates composition of components for different use cases. Matches the modular architecture established in the high-level design.\n> - **Consequences**: Requires clear interface design for component communication. Students learn proper separation of concerns. Code becomes more modular and testable. May require more initial setup to achieve complete workflows, but this reinforces architectural understanding.\n\n### Common Pitfalls\n\nUnderstanding common mistakes helps learners avoid frustrating debugging sessions and develops good practices for numerical computing and machine learning implementations.\n\n⚠️ **Pitfall: Division by Zero in Slope Calculation**\n\nThe most critical numerical issue occurs when all input features have identical values, causing the denominator in the slope formula `Σ((xi - x_mean)²)` to equal zero. This happens when the dataset contains no variation in the input variable, such as when all data points have the same x-coordinate.\n\nWhen this occurs, the mathematical concept of slope becomes undefined—there's no meaningful way to determine how y changes with respect to x if x never changes. Attempting to compute the slope results in division by zero, which NumPy handles by producing infinite or NaN values that propagate through subsequent calculations.\n\n**Detection Strategy**: Check if the variance of the input features is below the numerical tolerance threshold (`1e-10`) before performing the division. This catches both exact zeros and values so small they represent numerical noise.\n\n**Proper Handling**: Raise a descriptive `ValueError` with a message like \"Cannot fit linear regression: input features have zero variance (all values are identical).\" This immediately alerts the user to the data quality issue rather than producing meaningless results.\n\n**Prevention**: Include data validation that checks for feature variance during the initial data loading and preparation phases. Consider providing guidance about minimum data requirements for meaningful regression analysis.\n\n⚠️ **Pitfall: Incorrect Array Shape Handling**\n\nNumPy's broadcasting rules can mask shape incompatibilities that lead to unexpected behavior. Common issues include mixing 1D arrays with 2D column vectors, scalar inputs that should be arrays, and mismatched dimensions between features and targets.\n\nThe `fit(x, y)` method expects x and y to have compatible shapes for element-wise operations. However, NumPy's automatic broadcasting can sometimes succeed when it shouldn't, leading to incorrect parameter calculations. For example, a scalar x value might broadcast against an array y, producing mathematically valid but conceptually wrong results.\n\n**Detection Strategy**: Explicitly validate that input arrays have the same length using `len(x) == len(y)` before relying on NumPy operations. Check that both inputs are at least 1D arrays and handle scalar inputs by converting them to arrays.\n\n**Proper Handling**: Implement comprehensive shape validation in the `fit()` method that checks array compatibility and converts inputs to consistent formats. Provide clear error messages that specify the expected shapes and actual shapes received.\n\n**Prevention**: Design the interface to accept common input formats (lists, scalars, 1D arrays, 2D column vectors) and standardize them internally. Document the expected input formats clearly in method docstrings.\n\n⚠️ **Pitfall: Misinterpretation of R-squared Values**\n\nR-squared is frequently misunderstood, leading to incorrect conclusions about model quality. Common misconceptions include assuming that higher R-squared always means better predictions, that R-squared measures the correctness of the linear relationship, or that specific R-squared thresholds indicate good or bad models.\n\nR-squared measures the proportion of variance explained by the model relative to the total variance in the target variable. However, this doesn't directly translate to prediction accuracy for individual points, and the acceptable range depends heavily on the domain and data characteristics. In some fields, R-squared values of 0.3 represent strong relationships, while other applications might expect values above 0.9.\n\n**Common Misinterpretations**:\n- Assuming R² = 0.7 means \"70% accuracy\" in terms of individual predictions\n- Believing that negative R-squared values indicate a bug rather than poor model performance\n- Using R-squared to compare models fitted on different datasets or with different target variable scales\n- Focusing solely on R-squared without examining residual patterns or other diagnostic metrics\n\n**Proper Understanding**: R-squared indicates how much of the target variable's variation is captured by the linear relationship with the input features. High R-squared suggests strong linear correlation, but doesn't guarantee accurate individual predictions or validate the appropriateness of the linear model assumption.\n\n**Best Practices**: Always examine R-squared in conjunction with residual plots, prediction scatter plots, and domain knowledge. Document typical R-squared ranges for the specific application domain. Provide educational examples showing how R-squared relates to actual prediction quality.\n\n⚠️ **Pitfall: Inadequate Numerical Precision Handling**\n\nFloating-point arithmetic introduces small errors that can accumulate during computation, particularly when working with data that has vastly different scales or when performing operations on very large or very small numbers. These numerical precision issues can affect the stability and accuracy of parameter estimates.\n\nCommon scenarios include datasets with features spanning many orders of magnitude, very large sample sizes that affect sum computations, or input values close to the machine epsilon that cause precision loss in subtraction operations.\n\n**Detection Strategy**: Implement checks for numerical stability by comparing computed values against reasonable bounds and detecting NaN or infinite results. Monitor the condition number of computations when possible.\n\n**Proper Handling**: Use appropriate tolerance thresholds (like `1e-10`) for comparisons involving floating-point results. Consider feature scaling recommendations when numerical issues are detected. Provide warnings when computations may be affected by precision limitations.\n\n**Prevention**: Design the interface to accept and recommend normalized data. Include numerical stability considerations in the documentation and provide guidance on data preprocessing steps that improve computational stability.\n\n### Implementation Guidance\n\nThis subsection provides concrete implementation guidance for building the `SimpleLinearRegression` component, including complete starter code, file organization recommendations, and practical development checkpoints.\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Core Computation | Pure NumPy arrays | NumPy + SciPy for numerical stability |\n| Input Validation | Manual type checking | Schema validation with Pydantic |\n| Testing Framework | Built-in unittest | pytest with parametrized tests |\n| Documentation | Inline docstrings | Sphinx with mathematical notation |\n\n**Recommended File Structure:**\n\nThe simple linear regression component should be organized to support the learning progression while maintaining clean separation of concerns:\n\n```\nproject-root/\n  src/\n    linear_regression/\n      __init__.py                    ← package initialization\n      simple_regression.py           ← SimpleLinearRegression class\n      data_handler.py               ← DataHandler class (from previous milestone)\n      utils.py                      ← shared utilities and constants\n  tests/\n    test_simple_regression.py       ← comprehensive unit tests\n    test_integration.py             ← end-to-end workflow tests\n    fixtures/\n      sample_data.csv              ← test datasets\n  examples/\n    basic_usage.py                 ← simple usage demonstration\n    synthetic_data_demo.py         ← complete workflow example\n  README.md                        ← getting started guide\n```\n\n**Infrastructure Starter Code:**\n\nHere's the complete utilities module that provides shared constants and helper functions:\n\n```python\n\"\"\"\nUtility functions and constants for linear regression implementation.\nProvides shared numerical constants, validation helpers, and common calculations.\n\"\"\"\nimport numpy as np\nfrom typing import Union, Tuple\n\n# Numerical constants\nTOLERANCE = 1e-10\nDEFAULT_LEARNING_RATE = 0.01\nMAX_ITERATIONS = 1000\n\ndef validate_regression_inputs(x: np.ndarray, y: np.ndarray) -> None:\n    \"\"\"\n    Comprehensive validation for regression input data.\n    \n    Args:\n        x: Input features array\n        y: Target values array\n        \n    Raises:\n        ValueError: If data is incompatible for regression\n        TypeError: If inputs are not numeric arrays\n    \"\"\"\n    # Convert to numpy arrays if needed\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    \n    # Check for compatible shapes\n    if x.shape[0] != y.shape[0]:\n        raise ValueError(f\"Feature and target arrays must have same length. \"\n                        f\"Got x: {x.shape[0]}, y: {y.shape[0]}\")\n    \n    # Check for finite values\n    if not np.all(np.isfinite(x)):\n        raise ValueError(\"Input features contain non-finite values (NaN or infinity)\")\n    \n    if not np.all(np.isfinite(y)):\n        raise ValueError(\"Target values contain non-finite values (NaN or infinity)\")\n    \n    # Check for minimum sample size\n    if len(x) < 2:\n        raise ValueError(f\"Need at least 2 samples for regression. Got {len(x)}\")\n\ndef compute_r_squared(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"\n    Calculate R-squared coefficient of determination.\n    \n    Args:\n        y_true: Actual target values\n        y_pred: Predicted target values\n        \n    Returns:\n        R-squared value (can be negative)\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    \n    # Total sum of squares (variance in y_true)\n    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n    \n    # Residual sum of squares (unexplained variance)\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    \n    # Handle edge case where all y_true values are identical\n    if ss_tot < TOLERANCE:\n        return 1.0 if ss_res < TOLERANCE else 0.0\n    \n    return 1.0 - (ss_res / ss_tot)\n\ndef generate_synthetic_data(n_samples: int, true_slope: float, true_intercept: float, \n                          noise_std: float = 1.0, x_range: Tuple[float, float] = (0, 10)) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Generate synthetic linear data for testing and demonstration.\n    \n    Args:\n        n_samples: Number of data points to generate\n        true_slope: True slope parameter\n        true_intercept: True intercept parameter\n        noise_std: Standard deviation of noise to add\n        x_range: Range for x values as (min, max) tuple\n        \n    Returns:\n        Tuple of (x_values, y_values) as numpy arrays\n    \"\"\"\n    np.random.seed(42)  # For reproducible examples\n    \n    # Generate x values uniformly in the specified range\n    x = np.random.uniform(x_range[0], x_range[1], n_samples)\n    \n    # Generate y values with linear relationship plus noise\n    y = true_slope * x + true_intercept + np.random.normal(0, noise_std, n_samples)\n    \n    return x, y\n```\n\n**Core Logic Skeleton:**\n\nHere's the `SimpleLinearRegression` class with complete method signatures and detailed TODO comments that map directly to the algorithm steps described in the design section:\n\n```python\n\"\"\"\nSimple Linear Regression implementation using closed-form ordinary least squares.\n\"\"\"\nimport numpy as np\nfrom typing import Union\nfrom .utils import validate_regression_inputs, compute_r_squared, TOLERANCE\n\nclass SimpleLinearRegression:\n    \"\"\"\n    Single-variable linear regression using the closed-form ordinary least squares solution.\n    \n    This implementation demonstrates the mathematical foundation of linear regression\n    by computing optimal parameters directly through the normal equations rather\n    than iterative optimization.\n    \n    Attributes:\n        slope_ (float): Fitted slope parameter (set after calling fit)\n        intercept_ (float): Fitted intercept parameter (set after calling fit)\n        is_fitted_ (bool): Whether the model has been trained on data\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize unfitted model with default parameter values.\"\"\"\n        self.slope_ = 0.0\n        self.intercept_ = 0.0\n        self.is_fitted_ = False\n    \n    def fit(self, x: Union[list, np.ndarray], y: Union[list, np.ndarray]) -> 'SimpleLinearRegression':\n        \"\"\"\n        Fit linear regression model using closed-form ordinary least squares.\n        \n        Computes optimal slope and intercept parameters that minimize the sum of\n        squared residuals using the analytical normal equation solution.\n        \n        Args:\n            x: Input features (1D array-like)\n            y: Target values (1D array-like)\n            \n        Returns:\n            self: Fitted model instance for method chaining\n            \n        Raises:\n            ValueError: If input data is incompatible or has zero variance\n        \"\"\"\n        # TODO 1: Convert inputs to numpy arrays and validate compatibility\n        # Use validate_regression_inputs() from utils module\n        # Store as self._x_train and self._y_train for potential future use\n        \n        # TODO 2: Calculate sample means for both x and y\n        # These will be used in both slope and intercept calculations\n        # Store as x_mean and y_mean local variables\n        \n        # TODO 3: Compute slope using covariance formula\n        # Formula: slope = Σ((xi - x_mean) * (yi - y_mean)) / Σ((xi - x_mean)²)\n        # Calculate numerator (covariance) and denominator (x variance) separately\n        # Check denominator against TOLERANCE to avoid division by zero\n        \n        # TODO 4: Handle zero variance case\n        # If denominator < TOLERANCE, raise ValueError with descriptive message\n        # \"Cannot fit linear regression: input features have zero variance\"\n        \n        # TODO 5: Calculate intercept using the means\n        # Formula: intercept = y_mean - slope * x_mean\n        # This ensures the line passes through the data centroid\n        \n        # TODO 6: Store fitted parameters and set fitted flag\n        # Set self.slope_, self.intercept_, and self.is_fitted_ = True\n        # Return self to enable method chaining\n        \n        pass  # Remove this line when implementing\n    \n    def predict(self, x: Union[float, list, np.ndarray]) -> np.ndarray:\n        \"\"\"\n        Generate predictions using the fitted linear model.\n        \n        Args:\n            x: Input features for prediction\n            \n        Returns:\n            Predicted target values as numpy array\n            \n        Raises:\n            ValueError: If model has not been fitted\n        \"\"\"\n        # TODO 1: Check if model has been fitted\n        # Raise ValueError(\"Model must be fitted before making predictions\") if not\n        \n        # TODO 2: Convert input to numpy array and validate\n        # Handle both scalar and array inputs uniformly\n        # Check for finite values using np.isfinite()\n        \n        # TODO 3: Apply linear model equation\n        # Formula: y_pred = slope_ * x + intercept_\n        # NumPy broadcasting will handle array operations automatically\n        \n        # TODO 4: Return predictions as numpy array\n        # Ensure return type is consistent regardless of input type\n        \n        pass  # Remove this line when implementing\n    \n    def score(self, x: Union[list, np.ndarray], y: Union[list, np.ndarray]) -> float:\n        \"\"\"\n        Calculate R-squared coefficient of determination for the model.\n        \n        Args:\n            x: Input features for evaluation\n            y: True target values for comparison\n            \n        Returns:\n            R-squared score (proportion of variance explained)\n            \n        Raises:\n            ValueError: If model has not been fitted\n        \"\"\"\n        # TODO 1: Validate that model is fitted\n        # Check self.is_fitted_ flag and raise error if False\n        \n        # TODO 2: Generate predictions for the input data\n        # Use self.predict(x) to get predicted values\n        \n        # TODO 3: Calculate R-squared using utility function\n        # Use compute_r_squared(y, y_pred) from utils module\n        # This handles edge cases like zero variance in targets\n        \n        # TODO 4: Return R-squared value\n        # Value will be between -infinity and 1.0\n        # Negative values indicate worse-than-mean performance\n        \n        pass  # Remove this line when implementing\n    \n    def get_params(self) -> dict:\n        \"\"\"\n        Get fitted model parameters.\n        \n        Returns:\n            Dictionary with slope, intercept, and fitting status\n        \"\"\"\n        return {\n            'slope': self.slope_,\n            'intercept': self.intercept_,\n            'is_fitted': self.is_fitted_\n        }\n    \n    def __repr__(self) -> str:\n        \"\"\"String representation of the model.\"\"\"\n        if self.is_fitted_:\n            return f\"SimpleLinearRegression(slope={self.slope_:.4f}, intercept={self.intercept_:.4f})\"\n        else:\n            return \"SimpleLinearRegression(unfitted)\"\n```\n\n**Language-Specific Hints:**\n\n- Use `np.asarray(data, dtype=float)` to safely convert input data while preserving NumPy arrays and converting lists/scalars appropriately\n- The `@` operator in Python 3.5+ provides matrix multiplication, but for simple linear regression, element-wise operations with `*` and `+` are more transparent\n- NumPy's `np.mean()`, `np.sum()`, and `np.var()` functions automatically handle array operations efficiently\n- Use `np.isfinite()` to check for both NaN and infinity values in a single operation\n- The `typing` module's `Union` type helps document flexible input acceptance while maintaining type safety\n\n**Milestone Checkpoint:**\n\nAfter implementing the `SimpleLinearRegression` class, verify your implementation with these concrete tests:\n\n**Test 1: Perfect Linear Data**\n```python\n# Create perfect linear relationship\nx = np.array([1, 2, 3, 4, 5])\ny = np.array([2, 4, 6, 8, 10])  # y = 2x + 0\n\nmodel = SimpleLinearRegression()\nmodel.fit(x, y)\n\n# Expected results\nassert abs(model.slope_ - 2.0) < 1e-10\nassert abs(model.intercept_ - 0.0) < 1e-10\nassert abs(model.score(x, y) - 1.0) < 1e-10\n```\n\n**Test 2: Real-World Data**\n```python\n# Generate noisy data\nx, y = generate_synthetic_data(n_samples=100, true_slope=3.5, true_intercept=2.1, noise_std=0.5)\n\nmodel = SimpleLinearRegression()\nmodel.fit(x, y)\n\n# Verify reasonable parameter recovery\nassert abs(model.slope_ - 3.5) < 0.2  # Allow for noise\nassert abs(model.intercept_ - 2.1) < 0.5\nassert model.score(x, y) > 0.8  # High R-squared expected with low noise\n```\n\n**Test 3: Error Handling**\n```python\n# Test zero variance detection\nx_constant = np.array([5, 5, 5, 5])\ny_varied = np.array([1, 2, 3, 4])\n\ntry:\n    model.fit(x_constant, y_varied)\n    assert False, \"Should raise ValueError for zero variance\"\nexcept ValueError as e:\n    assert \"zero variance\" in str(e).lower()\n```\n\n**Signs of Success**: Your implementation should handle all test cases above, produce reasonable parameter estimates for noisy data, and provide clear error messages for invalid inputs. The R-squared calculation should return 1.0 for perfect linear relationships and values between 0-1 for typical noisy datasets.\n\n**Signs of Problems**: If parameters are wildly incorrect, check the covariance and variance calculations. If you get division by zero errors, verify the variance checking logic. If R-squared is negative for reasonable data, examine the prediction calculation for bugs.\n\n![Model State Machine](./diagrams/model-state-machine.svg)\n\n\n## Gradient Descent Optimizer Component\n\n> **Milestone(s):** M2: Gradient Descent\n\nThis component implements iterative parameter optimization using gradient descent, providing an alternative to the closed-form solutions developed in Milestone 1. While the `SimpleLinearRegression` component finds the optimal parameters directly through mathematical calculation, the `GradientDescentRegression` component discovers them through an iterative process of gradual improvement. This approach becomes essential when closed-form solutions are computationally expensive or mathematically intractable, and it provides the foundation for understanding how most modern machine learning algorithms optimize their parameters.\n\nThe gradient descent optimizer serves as both a practical alternative to closed-form solutions and an educational bridge to more advanced optimization techniques used in neural networks and other complex models. By implementing gradient descent from scratch, learners develop intuition for how machines \"learn\" through iterative improvement rather than direct calculation.\n\n### Mental Model: Hill Climbing in Reverse\n\nThink of gradient descent as **finding the bottom of a valley while blindfolded**. Imagine you're standing somewhere on a hilly landscape in dense fog, and your goal is to reach the lowest point in the valley. You can't see where you're going, but you can feel the slope of the ground under your feet.\n\nYour strategy is simple but effective: at each step, you feel around with your foot to determine which direction slopes downward most steeply, then take a step in that direction. You repeat this process—feel the slope, step downward—until you can't find any direction that goes further down. At that point, you've likely reached the bottom of the valley (or at least a local depression).\n\nIn our linear regression context:\n- The **landscape** is our cost function surface, where each point represents a different combination of slope and intercept parameters\n- The **height at any point** represents how badly our model performs with those parameters (measured by mean squared error)\n- The **valley bottom** represents the parameter values that minimize our prediction errors\n- **Feeling the slope** corresponds to calculating gradients—the mathematical measure of how steeply the cost function changes as we adjust each parameter\n- **Taking a step downward** means updating our parameters by moving them in the direction that reduces cost most quickly\n- The **step size** is our learning rate—how big a step we take in the downward direction\n\nThis analogy reveals why gradient descent works: by consistently moving in the direction of steepest descent (negative gradient direction), we're guaranteed to reach a local minimum of our cost function. For linear regression, this local minimum is also the global minimum—there's only one valley bottom, and gradient descent will find it regardless of where we start.\n\nThe blindfolded hiker analogy also explains common challenges: if you take steps that are too large (high learning rate), you might overshoot the valley bottom and end up bouncing back and forth across it, or even climbing out of the valley entirely. If your steps are too small (low learning rate), you'll eventually reach the bottom but it might take an impractically long time.\n\n### Cost Function Implementation\n\nThe **cost function** serves as our objective measure of how well our current parameters fit the data. In the hill-climbing analogy, the cost function defines the landscape we're navigating—it assigns a \"height\" (cost value) to every possible combination of parameters. Our goal is to find the parameter values that produce the lowest cost.\n\nFor linear regression, we use **mean squared error** as our cost function because it has mathematical properties that make gradient descent both efficient and reliable. The mean squared error measures the average squared difference between our model's predictions and the actual target values in our training data.\n\nThe mathematical formulation computes cost as the sum of squared residuals divided by the number of samples. For each data point, we calculate the prediction using our current slope and intercept parameters, subtract the actual target value to get the residual, square that residual to eliminate negative values and emphasize larger errors, then average across all data points to get a single cost value.\n\n| Cost Function Property | Explanation | Why Important |\n|------------------------|-------------|---------------|\n| **Always Non-negative** | Squared errors ensure cost ≥ 0 | Provides clear minimum at cost = 0 |\n| **Convex Shape** | Single global minimum, no local minima | Guarantees gradient descent finds optimal solution |\n| **Smooth and Differentiable** | Has well-defined gradients everywhere | Enables reliable gradient calculation |\n| **Emphasizes Large Errors** | Squaring makes big mistakes costlier than small ones | Prioritizes fixing worst predictions first |\n| **Scale Invariant** | Mean normalization makes cost comparable across datasets | Enables consistent learning rate tuning |\n\nThe cost function implementation must handle several practical considerations. First, it needs to be numerically stable when dealing with large prediction errors or extreme parameter values. Second, it should be computationally efficient since it will be called hundreds or thousands of times during training. Third, it must return consistent results across different data scales and sizes.\n\n> **Key Design Insight**: The choice of mean squared error over alternatives like mean absolute error or other loss functions isn't arbitrary. MSE produces gradients that are proportional to the error magnitude, which creates a natural \"acceleration\" effect—the algorithm takes larger steps when far from the optimum and smaller steps when close, leading to efficient convergence.\n\n**Cost Function Computational Flow:**\n\n1. **Parameter Input Validation**: Verify that slope and intercept parameters are finite numbers, not NaN or infinite values that could destabilize computation\n2. **Prediction Generation**: Apply the linear model equation y_pred = slope * x + intercept to generate predictions for all input features simultaneously using vectorized operations\n3. **Residual Calculation**: Compute residuals by subtracting actual targets from predictions, creating a vector of prediction errors\n4. **Error Squaring**: Square each residual to eliminate sign differences and emphasize larger errors quadratically\n5. **Mean Computation**: Sum all squared errors and divide by the number of samples to get the mean squared error\n6. **Numerical Stability Check**: Verify the result is finite and return an error flag if numerical issues are detected\n\nThe cost function serves dual purposes in our system: it provides the objective value that we're trying to minimize, and it serves as a convergence indicator—when the cost stops decreasing significantly between iterations, we know we've reached the optimal parameters.\n\n> **Architecture Decision: Vectorized vs Loop-Based Cost Calculation**\n> - **Context**: Cost function will be called thousands of times during training, making performance critical\n> - **Options Considered**: Element-wise loops, list comprehension, vectorized NumPy operations\n> - **Decision**: Use fully vectorized NumPy operations for all computations\n> - **Rationale**: Vectorized operations are 10-100x faster than Python loops and more readable than manual optimization\n> - **Consequences**: Requires NumPy dependency but provides significant performance gains and numerical stability\n\n### Gradient Calculation\n\n**Gradient calculation** is the mathematical heart of the optimization process. While the cost function tells us how well our current parameters perform, the gradients tell us how to improve them. Gradients represent the partial derivatives of the cost function with respect to each parameter—they indicate both the direction and magnitude of change needed to reduce cost most effectively.\n\nThink of gradients as **vector compass readings** in our parameter space. For simple linear regression with two parameters (slope and intercept), the gradient is a two-dimensional vector where the first component indicates how much the cost function changes as we adjust the slope, and the second component indicates how much it changes as we adjust the intercept. The direction of this vector points toward the steepest uphill direction, so we move in the opposite direction to go downhill.\n\nThe mathematical foundation relies on calculus and the chain rule. We derive the partial derivative of the mean squared error with respect to each parameter, producing formulas that can be computed directly from our current parameter values and training data. For linear regression, these derivatives have closed-form expressions that are computationally efficient to evaluate.\n\n| Parameter | Gradient Formula | Intuitive Meaning |\n|-----------|------------------|-------------------|\n| **Slope** | `2 * mean(residuals * x_values)` | How much cost changes per unit change in slope |\n| **Intercept** | `2 * mean(residuals)` | How much cost changes per unit change in intercept |\n| **Magnitude** | `sqrt(slope_grad² + intercept_grad²)` | Overall strength of improvement signal |\n| **Direction** | `arctan(intercept_grad / slope_grad)` | Direction of steepest descent in parameter space |\n\nThe gradient calculation process transforms our current prediction errors into specific parameter adjustment recommendations. When residuals are predominantly positive (predictions too low), the gradients will recommend increasing the slope and intercept to raise predictions. When residuals are predominantly negative (predictions too high), the gradients will recommend decreasing parameters to lower predictions.\n\n**Gradient Computation Algorithm:**\n\n1. **Current Predictions**: Use existing slope and intercept parameters to generate predictions for all training examples\n2. **Residual Vector**: Compute prediction errors by subtracting actual targets from predictions\n3. **Slope Gradient**: Calculate the correlation between residuals and input features, scaled by the mean—this measures how much the slope should change\n4. **Intercept Gradient**: Calculate the mean of residuals—this measures how much the intercept should change\n5. **Gradient Scaling**: Apply the 2/n scaling factor (where n is the number of samples) to match the cost function derivative\n6. **Numerical Validation**: Check that gradients are finite and within reasonable bounds to prevent numerical instability\n\nThe mathematical elegance of linear regression gradients lies in their interpretability. The slope gradient directly measures the linear correlation between prediction errors and input features—if larger input values tend to have larger errors, the slope needs adjustment. The intercept gradient measures the overall bias in predictions—if predictions are consistently too high or too low, the intercept needs adjustment.\n\n> **Critical Implementation Detail**: Gradient computation must use the same prediction method as the cost function to ensure mathematical consistency. Any discrepancy between how predictions are calculated in the two functions will cause the gradients to point in incorrect directions, preventing convergence.\n\n**Gradient Numerical Properties:**\n\nThe gradient calculation must handle several numerical considerations that can affect training stability. First, gradients can become very large when parameters are far from optimal, potentially causing parameter updates to overshoot. Second, gradients approach zero as we near the optimum, which can cause training to slow down prematurely if we don't choose convergence criteria carefully. Third, the relative magnitudes of slope and intercept gradients depend on the scale of input features, which affects learning rate selection.\n\n| Gradient Magnitude Range | Interpretation | Recommended Action |\n|-------------------------|----------------|-------------------|\n| **Very Large (>1000)** | Parameters far from optimum | Reduce learning rate or check for numerical issues |\n| **Moderate (1-1000)** | Normal optimization progress | Continue with current learning rate |\n| **Small (0.01-1)** | Approaching convergence | Monitor for convergence criteria |\n| **Very Small (<0.01)** | Near optimal or stuck | Check convergence or consider stopping |\n| **Zero** | At optimum or numerical underflow | Stop training—convergence achieved |\n\n### Parameter Update Rules\n\nThe **parameter update rule** is where gradient descent takes its \"step downhill\" in the parameter landscape. This rule combines the gradient information with a learning rate to determine how much to adjust each parameter in each iteration. The update rule represents the core learning mechanism—how the algorithm uses feedback from prediction errors to improve its parameter estimates.\n\nThe fundamental update equation follows the pattern: new_parameter = old_parameter - learning_rate * gradient. The negative sign is crucial because gradients point in the direction of steepest ascent (uphill), but we want to move toward the minimum (downhill). The learning rate scales the gradient to control step size—it determines how aggressively we move in the gradient direction.\n\n**Parameter Update Process:**\n\n1. **Gradient Computation**: Calculate current gradients for slope and intercept using the gradient calculation process described above\n2. **Learning Rate Application**: Multiply each gradient by the learning rate to determine the magnitude of parameter adjustment\n3. **Parameter Adjustment**: Subtract the scaled gradients from current parameters to move in the direction of decreasing cost\n4. **Parameter Validation**: Check that updated parameters remain within reasonable bounds and haven't become infinite or NaN\n5. **History Recording**: Store the new parameters and their associated cost for convergence monitoring and debugging\n6. **Iteration Advancement**: Increment the iteration counter and prepare for the next optimization step\n\nThe learning rate serves as the most critical hyperparameter in gradient descent optimization. It controls the trade-off between convergence speed and stability. A learning rate that's too high causes the algorithm to take steps that are too large, potentially overshooting the minimum and causing the cost to oscillate or even increase. A learning rate that's too low causes the algorithm to take tiny steps, requiring many iterations to reach the optimum and potentially getting stuck in flat regions of the cost surface.\n\n| Learning Rate Range | Behavior | Pros | Cons |\n|---------------------|----------|------|------|\n| **Very High (>1.0)** | Large steps, often overshoots | Fast when far from optimum | Unstable, may diverge |\n| **High (0.1-1.0)** | Moderate steps, some oscillation | Good initial progress | May overshoot near optimum |\n| **Medium (0.01-0.1)** | Steady progress with minor oscillation | Reliable convergence | Moderate convergence speed |\n| **Low (0.001-0.01)** | Small steps, smooth progress | Stable, precise convergence | Slow convergence |\n| **Very Low (<0.001)** | Tiny steps, very slow progress | Maximum stability | Impractically slow |\n\n**Adaptive Learning Rate Strategies:**\n\nWhile our implementation uses a fixed learning rate for simplicity, understanding adaptive strategies provides insight into more advanced optimization. The key insight is that optimal learning rates change during training—we often want larger steps when far from the optimum and smaller steps when close to it.\n\n> **Architecture Decision: Fixed vs Adaptive Learning Rate**\n> - **Context**: Learning rate significantly affects convergence speed and stability, but adaptive methods add complexity\n> - **Options Considered**: Fixed rate, linear decay, exponential decay, adaptive gradient methods\n> - **Decision**: Use fixed learning rate with manual tuning guidance\n> - **Rationale**: Provides clear understanding of learning rate effects without obscuring core gradient descent concepts\n> - **Consequences**: Requires manual tuning but offers transparent, predictable behavior ideal for learning\n\nThe parameter update rule also incorporates momentum concepts implicitly through the iterative process. Each parameter update builds on previous updates, creating a form of \"momentum\" that helps the algorithm maintain direction and speed through flat regions of the cost surface. This momentum effect becomes more pronounced when the gradient consistently points in similar directions across iterations.\n\n**Parameter Update Numerical Considerations:**\n\nThe update rule must handle numerical precision issues that can arise during optimization. Parameters can grow very large if gradients consistently point in one direction, potentially causing overflow. Conversely, very small gradients combined with low learning rates can cause underflow, where parameter changes become too small to register in floating-point arithmetic.\n\n| Numerical Issue | Symptoms | Detection Method | Prevention Strategy |\n|-----------------|----------|------------------|-------------------|\n| **Parameter Overflow** | Parameters become infinite | Check for inf/NaN after update | Gradient clipping, learning rate reduction |\n| **Parameter Underflow** | No parameter change despite non-zero gradients | Monitor parameter change magnitude | Increase learning rate, check precision |\n| **Oscillating Parameters** | Parameters alternate between values | Track parameter history variance | Reduce learning rate, check for numerical instability |\n| **Stuck Parameters** | Parameters stop changing prematurely | Monitor gradient magnitude and parameter changes | Adjust convergence criteria, increase learning rate |\n\n### Convergence Detection\n\n**Convergence detection** determines when gradient descent has found the optimal parameters and training should stop. This mechanism prevents unnecessary computation while ensuring we've reached a satisfactory solution. Effective convergence detection balances between stopping too early (suboptimal results) and running too long (wasted computation).\n\nThe primary convergence criterion monitors the **cost improvement** between consecutive iterations. When the reduction in cost falls below a specified tolerance threshold for several consecutive iterations, we consider the algorithm to have converged. This approach recognizes that near the optimum, cost improvements become progressively smaller until they're negligible for practical purposes.\n\n**Multi-Criteria Convergence Strategy:**\n\nRobust convergence detection uses multiple criteria to handle different convergence scenarios. A single criterion can fail in edge cases—for example, cost-based detection might fail when cost oscillates slightly, while gradient-based detection might fail due to numerical precision issues.\n\n| Convergence Criterion | Formula | Purpose | Failure Modes |\n|-----------------------|---------|---------|---------------|\n| **Cost Improvement** | `abs(cost[i] - cost[i-1]) < tolerance` | Detects when cost stops decreasing | Fails with oscillating costs |\n| **Relative Cost Change** | `abs(cost[i] - cost[i-1]) / cost[i-1] < tolerance` | Handles different cost scales | Fails when cost approaches zero |\n| **Gradient Magnitude** | `sqrt(slope_grad² + intercept_grad²) < tolerance` | Detects when gradients approach zero | Sensitive to numerical precision |\n| **Parameter Stability** | `max(abs(param_change)) < tolerance` | Detects when parameters stop changing | Can miss slow convergence |\n| **Maximum Iterations** | `iteration_count >= max_iterations` | Prevents infinite loops | May stop before convergence |\n\nThe convergence detection algorithm combines these criteria using a logical OR relationship—convergence is declared when any criterion is satisfied. This approach provides robustness against individual criterion failures while maintaining clear stopping conditions.\n\n**Convergence Detection Algorithm:**\n\n1. **Cost History Analysis**: Compare current cost with previous iteration's cost to measure improvement magnitude\n2. **Gradient Magnitude Check**: Calculate the L2 norm of current gradients to detect when they approach zero\n3. **Parameter Change Analysis**: Measure how much parameters changed in the current iteration\n4. **Tolerance Comparison**: Compare each metric against its respective tolerance threshold\n5. **Consecutive Iteration Tracking**: Require convergence criteria to be satisfied for multiple consecutive iterations to avoid false positives\n6. **Iteration Limit Check**: Enforce maximum iteration limit to prevent infinite loops in pathological cases\n7. **Convergence Declaration**: Set convergence flag and record convergence reason for diagnostic purposes\n\n> **Key Design Insight**: Convergence detection must account for the fact that gradient descent can exhibit different behaviors near the optimum. Some problems converge smoothly with monotonically decreasing costs, while others oscillate slightly around the minimum due to discrete parameter updates. Requiring consecutive iterations to meet criteria helps distinguish true convergence from temporary fluctuations.\n\n**Premature vs Delayed Convergence:**\n\nConvergence detection involves balancing two types of errors. Premature convergence stops training before reaching the true optimum, resulting in suboptimal model performance. Delayed convergence continues training long after reaching practical optimality, wasting computational resources without meaningful improvement.\n\n| Convergence Type | Symptoms | Causes | Solutions |\n|------------------|----------|---------|-----------|\n| **Premature** | Training stops early, cost could decrease further | Tolerance too loose, insufficient consecutive checks | Tighten tolerance, require more consecutive iterations |\n| **Delayed** | Training continues with minimal improvement | Tolerance too strict, numerical precision limits | Relax tolerance, add maximum iteration limits |\n| **False Positive** | Declares convergence then cost decreases again | Temporary flat regions, oscillating costs | Require consecutive iteration criteria |\n| **Never Converges** | Training runs indefinitely | Learning rate too high, numerical instability | Implement maximum iterations, check gradients |\n\nThe convergence detection system records detailed information about why training stopped, which becomes crucial for debugging and hyperparameter tuning. This metadata helps users understand whether their model achieved genuine convergence or stopped due to limits and constraints.\n\n> **Architecture Decision: Single vs Multiple Convergence Criteria**\n> - **Context**: Different optimization scenarios may satisfy different convergence conditions first\n> - **Options Considered**: Cost-only, gradient-only, parameter-change-only, combined criteria\n> - **Decision**: Implement multiple criteria with logical OR combination\n> - **Rationale**: Provides robustness against individual criterion failures and handles various convergence patterns\n> - **Consequences**: More complex logic but significantly more reliable convergence detection across different problem types\n\n### Architecture Decision Records\n\nThe gradient descent optimizer component involves several critical design decisions that affect both educational value and practical performance. Each decision represents a trade-off between simplicity, performance, numerical stability, and pedagogical clarity.\n\n> **Decision: Batch vs Stochastic vs Mini-Batch Gradient Descent**\n> - **Context**: Different gradient computation strategies offer different convergence properties and computational characteristics\n> - **Options Considered**: Batch gradient descent (full dataset), stochastic gradient descent (single sample), mini-batch gradient descent (subset of samples)\n> - **Decision**: Implement batch gradient descent using the full dataset for each gradient computation\n> - **Rationale**: Provides deterministic, smooth convergence that's easier to understand and debug; eliminates random variation that could confuse learning process; ensures gradients point in true direction of steepest descent\n> - **Consequences**: Higher memory usage and computational cost per iteration but more predictable convergence behavior ideal for educational purposes\n\n| Gradient Descent Variant | Memory Usage | Computational Cost Per Iteration | Convergence Behavior | Educational Value |\n|--------------------------|--------------|----------------------------------|----------------------|-------------------|\n| **Batch (Chosen)** | High (full dataset) | High | Smooth, deterministic | Excellent for learning |\n| **Stochastic** | Low (single sample) | Low | Noisy, faster initial progress | Confusing for beginners |\n| **Mini-batch** | Medium (subset) | Medium | Balanced noise/smoothness | Good but more complex |\n\n> **Decision: Manual vs Automatic Learning Rate Selection**\n> - **Context**: Learning rate significantly affects convergence but optimal values depend on data characteristics\n> - **Options Considered**: Fixed manual rate, automatic line search, adaptive rate methods, learning rate schedules\n> - **Decision**: Use fixed manual learning rate with clear guidance for selection\n> - **Rationale**: Forces learners to understand learning rate effects through experimentation; provides transparent, predictable behavior; avoids obscuring core concepts with complex adaptive algorithms\n> - **Consequences**: Requires manual tuning and may need adjustment for different datasets, but provides clear cause-and-effect understanding\n\n> **Decision: Numerical Precision Strategy**\n> - **Context**: Floating-point arithmetic can cause numerical instability in gradient computations and parameter updates\n> - **Options Considered**: Single precision (float32), double precision (float64), arbitrary precision arithmetic\n> - **Decision**: Use double precision (float64) throughout with explicit overflow/underflow checking\n> - **Rationale**: Provides sufficient precision for educational datasets while remaining computationally efficient; reduces likelihood of numerical issues that could confuse learning process\n> - **Consequences**: Higher memory usage but significantly more stable numerical behavior and clearer debugging\n\n> **Decision: Gradient Computation Approach**\n> - **Context**: Gradients can be computed analytically, numerically, or through automatic differentiation\n> - **Options Considered**: Analytical derivatives (manual derivation), numerical differentiation (finite differences), automatic differentiation frameworks\n> - **Decision**: Implement analytical derivatives with explicit mathematical formulas\n> - **Rationale**: Provides exact gradients without approximation error; forces understanding of underlying mathematics; offers maximum computational efficiency; enables clear connection between calculus and implementation\n> - **Consequences**: Requires manual derivation and implementation of derivative formulas but provides deepest mathematical understanding\n\n### Common Pitfalls\n\nGradient descent implementation involves several subtle issues that frequently cause problems for learners. Understanding these pitfalls and their solutions is crucial for successful implementation and debugging.\n\n⚠️ **Pitfall: Learning Rate Too High Causing Divergence**\n\n**Description**: The most common failure mode occurs when the learning rate is set too high, causing parameter updates to overshoot the minimum. Instead of converging to optimal parameters, the cost function value increases with each iteration, often growing exponentially until parameters become infinite.\n\n**Why It's Wrong**: Large learning rates cause the algorithm to take steps that are too big, jumping over the minimum point in the cost function. Each step moves further from the optimum rather than closer to it, creating a divergent spiral where parameters grow without bound.\n\n**How to Detect**: Monitor cost function values during training—if costs increase consistently or oscillate wildly between very large values, the learning rate is too high. Parameter values themselves may become extremely large (>1e6) or infinite.\n\n**How to Fix**: Reduce learning rate by factors of 10 until convergence behavior appears. Start with learning rates around 0.001 and increase gradually if convergence is too slow. Implement gradient clipping to prevent parameter explosions.\n\n⚠️ **Pitfall: Learning Rate Too Low Causing Premature Stopping**\n\n**Description**: Excessively low learning rates cause gradient descent to make tiny progress in each iteration. The algorithm may appear to stop improving while still far from the true optimum, leading to premature convergence declarations and suboptimal model performance.\n\n**Why It's Wrong**: Tiny parameter updates mean the algorithm requires thousands or tens of thousands of iterations to reach the optimum. Combined with loose convergence criteria, this often triggers early stopping when improvement per iteration falls below the threshold despite significant potential improvement remaining.\n\n**How to Detect**: Training stops after relatively few iterations with large gradient magnitudes still present. Cost improvements are extremely small but consistent, suggesting more progress is possible. Final cost values are significantly higher than expected.\n\n**How to Fix**: Increase learning rate gradually until reasonable convergence speed appears. Tighten convergence criteria to require smaller improvements before stopping. Monitor both cost improvement and gradient magnitude to distinguish true convergence from slow progress.\n\n⚠️ **Pitfall: Forgetting to Normalize Features**\n\n**Description**: When input features have very different scales (e.g., age in years vs income in dollars), the cost function surface becomes elongated and narrow. Gradient descent struggles with such surfaces because optimal learning rates for different parameters differ dramatically.\n\n**Why It's Wrong**: Features with large scales dominate gradient calculations, causing the algorithm to focus on adjusting parameters for high-scale features while largely ignoring low-scale features. This creates inefficient zigzag paths toward the minimum instead of direct descent.\n\n**How to Detect**: Some parameters change rapidly while others barely change at all during training. Convergence is much slower than expected, with cost decreasing in a zigzag pattern rather than smooth descent. Different features have vastly different ranges in the dataset.\n\n**How to Fix**: Apply z-score normalization to all features before training, scaling each to zero mean and unit variance. Store normalization parameters to apply the same transformation to prediction data. Consider the impact of normalization on parameter interpretation.\n\n⚠️ **Pitfall: Incorrect Gradient Implementation**\n\n**Description**: Mathematical errors in gradient calculation formulas cause the algorithm to move in wrong directions, preventing convergence or causing divergence. Common errors include sign mistakes, missing scaling factors, or inconsistent prediction calculations between cost and gradient functions.\n\n**Why It's Wrong**: Incorrect gradients point in suboptimal directions, causing the algorithm to move away from the minimum instead of toward it. Even small errors can prevent convergence or cause extremely slow progress, making the optimization process unreliable.\n\n**How to Detect**: Cost function fails to decrease consistently despite reasonable learning rates. Parameters move in unexpected directions (e.g., slope increases when it should decrease). Gradient magnitudes don't correlate with expected improvement directions.\n\n**How to Fix**: Verify gradient formulas through numerical differentiation—compute gradients using finite differences and compare with analytical formulas. Test gradient calculations on simple datasets where optimal parameters are known. Ensure prediction calculations are identical in cost and gradient functions.\n\n⚠️ **Pitfall: Inadequate Convergence Criteria**\n\n**Description**: Poorly chosen convergence thresholds either stop training prematurely or allow training to continue indefinitely without meaningful improvement. Single-criterion convergence detection fails in edge cases like oscillating costs or numerical precision limits.\n\n**Why It's Wrong**: Premature stopping produces suboptimal models, while delayed stopping wastes computation. Inadequate criteria fail to distinguish between true convergence, temporary plateaus, and numerical precision limits, leading to unreliable training outcomes.\n\n**How to Detect**: Training stops while significant improvement potential remains, or training continues indefinitely with minimal progress. Convergence behavior varies dramatically across similar datasets or repeated runs.\n\n**How to Fix**: Implement multiple convergence criteria with appropriate thresholds for cost improvement, gradient magnitude, and parameter stability. Require criteria to be satisfied for consecutive iterations to avoid false positives. Always enforce maximum iteration limits as safety nets.\n\n⚠️ **Pitfall: Numerical Instability from Poor Parameter Initialization**\n\n**Description**: Starting parameters that are too large can cause initial cost calculations to overflow, while parameters that are too small may cause underflow in gradient calculations. Poor initialization can also place the algorithm in flat regions of the cost surface.\n\n**Why It's Wrong**: Extreme initial parameter values can cause numerical overflow or underflow in the first few iterations, preventing the algorithm from getting started. Even if numerically stable, poor initialization can significantly slow convergence or cause the algorithm to get stuck.\n\n**How to Detect**: Initial cost calculations produce infinite or NaN values. Training fails to start or makes no progress in early iterations despite reasonable learning rates. Parameters remain close to initial values throughout training.\n\n**How to Fix**: Initialize parameters to small random values (typically between -0.1 and 0.1) or use domain knowledge for reasonable starting points. For linear regression, initializing slope to zero and intercept to the mean of target values often works well. Implement bounds checking for parameter initialization.\n\n### Implementation Guidance\n\nThe gradient descent optimizer bridges mathematical optimization theory with practical machine learning implementation. This section provides complete, working infrastructure code and detailed implementation skeletons that help learners focus on core gradient descent concepts while handling necessary but peripheral concerns like data validation and numerical stability.\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| **Numerical Operations** | `numpy` arrays with basic operations | `scipy.optimize` for comparison and validation |\n| **Gradient Calculation** | Manual analytical derivatives | `autograd` or `jax` for automatic differentiation |\n| **Plotting and Visualization** | `matplotlib` for cost curves and parameter traces | `plotly` for interactive optimization visualization |\n| **Data Storage** | Python lists for training history | `pandas.DataFrame` for structured history tracking |\n| **Numerical Stability** | Manual overflow/underflow checks | `numpy.errstate` context managers |\n\n**Recommended File Structure:**\n\n```\nproject-root/\n  src/\n    models/\n      gradient_descent_regressor.py    ← Core implementation (learner implements)\n      base_regressor.py               ← Base class with shared functionality\n    optimizers/\n      gradient_descent.py             ← Generic optimizer (starter code provided)\n      convergence.py                  ← Convergence detection (starter code provided)\n    utils/\n      numerical_stability.py          ← Overflow/underflow handling (complete)\n      visualization.py                ← Training progress plots (complete)\n      metrics.py                      ← Cost function and evaluation (starter code)\n  tests/\n    test_gradient_descent.py          ← Unit tests for core functionality\n    test_convergence.py               ← Convergence detection tests\n  examples/\n    gradient_descent_demo.py          ← Complete working example\n```\n\n**Complete Infrastructure: Numerical Stability Utilities**\n\n```python\n\"\"\"\nNumerical stability utilities for gradient descent optimization.\nHandles overflow, underflow, and precision issues.\n\"\"\"\n\nimport numpy as np\nfrom typing import Tuple, Optional, Union\nimport warnings\n\n# Global constants for numerical stability\nTOLERANCE = 1e-10\nMAX_PARAMETER_VALUE = 1e8\nMIN_GRADIENT_NORM = 1e-12\nMAX_GRADIENT_NORM = 1e8\n\ndef check_numerical_stability(\n    parameters: np.ndarray,\n    gradients: np.ndarray,\n    cost: float,\n    parameter_names: Optional[list] = None\n) -> Tuple[bool, str]:\n    \"\"\"\n    Comprehensive numerical stability check for gradient descent.\n    \n    Returns:\n        (is_stable, error_message): Stability status and diagnostic message\n    \"\"\"\n    param_names = parameter_names or [f\"param_{i}\" for i in range(len(parameters))]\n    \n    # Check for NaN or infinite parameters\n    if np.any(~np.isfinite(parameters)):\n        invalid_params = [param_names[i] for i in range(len(parameters)) \n                         if not np.isfinite(parameters[i])]\n        return False, f\"Non-finite parameters detected: {invalid_params}\"\n    \n    # Check for parameter overflow\n    if np.any(np.abs(parameters) > MAX_PARAMETER_VALUE):\n        large_params = [param_names[i] for i in range(len(parameters))\n                       if np.abs(parameters[i]) > MAX_PARAMETER_VALUE]\n        return False, f\"Parameter overflow detected: {large_params}\"\n    \n    # Check for NaN or infinite gradients\n    if np.any(~np.isfinite(gradients)):\n        invalid_grads = [f\"{param_names[i]}_grad\" for i in range(len(gradients))\n                        if not np.isfinite(gradients[i])]\n        return False, f\"Non-finite gradients detected: {invalid_grads}\"\n    \n    # Check for gradient explosion\n    gradient_norm = np.linalg.norm(gradients)\n    if gradient_norm > MAX_GRADIENT_NORM:\n        return False, f\"Gradient explosion: norm={gradient_norm:.2e}\"\n    \n    # Check for cost function issues\n    if not np.isfinite(cost) or cost < 0:\n        return False, f\"Invalid cost value: {cost}\"\n    \n    return True, \"Numerically stable\"\n\ndef safe_parameter_update(\n    parameters: np.ndarray,\n    gradients: np.ndarray,\n    learning_rate: float\n) -> Tuple[np.ndarray, bool, str]:\n    \"\"\"\n    Safely update parameters with overflow protection.\n    \n    Returns:\n        (new_parameters, success, message)\n    \"\"\"\n    # Calculate proposed parameter changes\n    parameter_changes = learning_rate * gradients\n    \n    # Check if changes are too large\n    if np.any(np.abs(parameter_changes) > MAX_PARAMETER_VALUE / 10):\n        return parameters, False, \"Parameter change too large - reduce learning rate\"\n    \n    # Apply updates\n    new_parameters = parameters - parameter_changes\n    \n    # Verify results are finite and reasonable\n    if np.any(~np.isfinite(new_parameters)):\n        return parameters, False, \"Parameter update produced non-finite values\"\n    \n    if np.any(np.abs(new_parameters) > MAX_PARAMETER_VALUE):\n        return parameters, False, \"Parameter update caused overflow\"\n    \n    return new_parameters, True, \"Update successful\"\n\ndef initialize_parameters_safely(\n    n_parameters: int,\n    initialization_scale: float = 0.01,\n    random_seed: Optional[int] = None\n) -> np.ndarray:\n    \"\"\"\n    Initialize parameters with safe random values.\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    \n    return np.random.normal(0, initialization_scale, n_parameters)\n```\n\n**Complete Infrastructure: Convergence Detection**\n\n```python\n\"\"\"\nConvergence detection for gradient descent optimization.\nImplements multiple criteria with robust checking.\n\"\"\"\n\nimport numpy as np\nfrom typing import List, Dict, Tuple, Optional\nfrom dataclasses import dataclass\n\n@dataclass\nclass ConvergenceConfig:\n    \"\"\"Configuration for convergence detection criteria.\"\"\"\n    cost_tolerance: float = 1e-6\n    gradient_tolerance: float = 1e-6\n    parameter_tolerance: float = 1e-8\n    relative_tolerance: float = 1e-6\n    consecutive_iterations: int = 3\n    max_iterations: int = 1000\n\nclass ConvergenceDetector:\n    \"\"\"\n    Multi-criteria convergence detection for gradient descent.\n    \"\"\"\n    \n    def __init__(self, config: ConvergenceConfig):\n        self.config = config\n        self.cost_history: List[float] = []\n        self.parameter_history: List[np.ndarray] = []\n        self.gradient_history: List[np.ndarray] = []\n        self.consecutive_count = 0\n        self.converged = False\n        self.convergence_reason = \"\"\n    \n    def check_convergence(\n        self,\n        cost: float,\n        parameters: np.ndarray,\n        gradients: np.ndarray,\n        iteration: int\n    ) -> Tuple[bool, str]:\n        \"\"\"\n        Check all convergence criteria and return decision.\n        \n        Returns:\n            (converged, reason): Convergence status and reason\n        \"\"\"\n        # Store current state\n        self.cost_history.append(cost)\n        self.parameter_history.append(parameters.copy())\n        self.gradient_history.append(gradients.copy())\n        \n        # Check maximum iterations\n        if iteration >= self.config.max_iterations:\n            self.converged = True\n            self.convergence_reason = f\"Maximum iterations ({self.config.max_iterations}) reached\"\n            return True, self.convergence_reason\n        \n        # Need at least 2 points for improvement-based criteria\n        if len(self.cost_history) < 2:\n            return False, \"Insufficient history for convergence check\"\n        \n        # Check each convergence criterion\n        criteria_met = []\n        \n        # Cost improvement criterion\n        cost_improvement = abs(self.cost_history[-2] - self.cost_history[-1])\n        if cost_improvement < self.config.cost_tolerance:\n            criteria_met.append(\"cost_improvement\")\n        \n        # Relative cost improvement criterion\n        if self.cost_history[-1] > 0:\n            relative_improvement = cost_improvement / self.cost_history[-1]\n            if relative_improvement < self.config.relative_tolerance:\n                criteria_met.append(\"relative_cost_improvement\")\n        \n        # Gradient magnitude criterion\n        gradient_norm = np.linalg.norm(gradients)\n        if gradient_norm < self.config.gradient_tolerance:\n            criteria_met.append(\"gradient_magnitude\")\n        \n        # Parameter change criterion\n        if len(self.parameter_history) >= 2:\n            parameter_change = np.linalg.norm(\n                self.parameter_history[-1] - self.parameter_history[-2]\n            )\n            if parameter_change < self.config.parameter_tolerance:\n                criteria_met.append(\"parameter_stability\")\n        \n        # Check if any criteria are met\n        if criteria_met:\n            self.consecutive_count += 1\n            if self.consecutive_count >= self.config.consecutive_iterations:\n                self.converged = True\n                self.convergence_reason = f\"Criteria satisfied for {self.consecutive_count} consecutive iterations: {criteria_met}\"\n                return True, self.convergence_reason\n            else:\n                return False, f\"Convergence criteria met ({criteria_met}) but need {self.config.consecutive_iterations - self.consecutive_count} more consecutive iterations\"\n        else:\n            self.consecutive_count = 0\n            return False, f\"No convergence criteria satisfied. Cost improvement: {cost_improvement:.2e}, Gradient norm: {gradient_norm:.2e}\"\n    \n    def get_convergence_summary(self) -> Dict:\n        \"\"\"Return detailed convergence information for debugging.\"\"\"\n        return {\n            \"converged\": self.converged,\n            \"reason\": self.convergence_reason,\n            \"total_iterations\": len(self.cost_history),\n            \"final_cost\": self.cost_history[-1] if self.cost_history else None,\n            \"final_gradient_norm\": np.linalg.norm(self.gradient_history[-1]) if self.gradient_history else None,\n            \"cost_history\": self.cost_history.copy(),\n            \"consecutive_count\": self.consecutive_count\n        }\n```\n\n**Core Implementation Skeleton: GradientDescentRegression Class**\n\n```python\n\"\"\"\nGradient descent linear regression implementation.\nLearners implement the core optimization logic.\n\"\"\"\n\nimport numpy as np\nfrom typing import Tuple, List, Optional, Dict\nfrom .base_regressor import BaseRegressor  # Provided base class\nfrom .convergence import ConvergenceDetector, ConvergenceConfig\nfrom .numerical_stability import check_numerical_stability, safe_parameter_update\n\nclass GradientDescentRegression(BaseRegressor):\n    \"\"\"\n    Linear regression using gradient descent optimization.\n    \n    This class learns the relationship y = slope * x + intercept by iteratively\n    minimizing mean squared error using gradient descent.\n    \"\"\"\n    \n    def __init__(\n        self,\n        learning_rate: float = 0.01,\n        max_iterations: int = 1000,\n        tolerance: float = 1e-6,\n        random_seed: Optional[int] = None\n    ):\n        \"\"\"\n        Initialize gradient descent regressor.\n        \n        Args:\n            learning_rate: Step size for parameter updates\n            max_iterations: Maximum number of optimization iterations\n            tolerance: Convergence tolerance for cost improvement\n            random_seed: Random seed for parameter initialization\n        \"\"\"\n        # Initialize regression parameters\n        self.slope_ = 0.0\n        self.intercept_ = 0.0\n        self.is_fitted_ = False\n        \n        # Store hyperparameters\n        self.learning_rate = learning_rate\n        self.max_iterations = max_iterations\n        self.tolerance = tolerance\n        self.random_seed = random_seed\n        \n        # Training history for analysis and debugging\n        self.cost_history_: List[float] = []\n        self.parameter_history_: List[Tuple[float, float]] = []\n        \n        # Set up convergence detection\n        conv_config = ConvergenceConfig(\n            cost_tolerance=tolerance,\n            gradient_tolerance=tolerance,\n            max_iterations=max_iterations\n        )\n        self.convergence_detector = ConvergenceDetector(conv_config)\n    \n    def fit(self, x: np.ndarray, y: np.ndarray) -> 'GradientDescentRegression':\n        \"\"\"\n        Train the linear regression model using gradient descent.\n        \n        Args:\n            x: Input features, shape (n_samples,)\n            y: Target values, shape (n_samples,)\n            \n        Returns:\n            self: Fitted regressor instance\n        \"\"\"\n        # TODO 1: Validate input data shapes and types\n        # Hint: Check that x and y have same length, are 1D arrays, contain finite values\n        \n        # TODO 2: Initialize parameters\n        # Hint: Set slope to small random value, intercept to mean of y values\n        # Use self.random_seed for reproducibility\n        \n        # TODO 3: Main optimization loop\n        # for iteration in range(self.max_iterations):\n        #   TODO 3a: Compute current cost using _compute_cost method\n        #   TODO 3b: Compute gradients using _compute_gradients method  \n        #   TODO 3c: Check numerical stability of parameters and gradients\n        #   TODO 3d: Update parameters using gradient descent rule\n        #   TODO 3e: Record current state in history\n        #   TODO 3f: Check convergence criteria\n        #   TODO 3g: Break if converged\n        \n        # TODO 4: Set fitted flag and return self\n        # self.is_fitted_ = True\n        # return self\n        \n        raise NotImplementedError(\"Implement gradient descent training loop\")\n    \n    def _compute_cost(self, x: np.ndarray, y: np.ndarray, slope: float, intercept: float) -> float:\n        \"\"\"\n        Compute mean squared error cost function.\n        \n        Args:\n            x: Input features\n            y: Target values  \n            slope: Current slope parameter\n            intercept: Current intercept parameter\n            \n        Returns:\n            Mean squared error cost\n        \"\"\"\n        # TODO 1: Generate predictions using current parameters\n        # Hint: predictions = slope * x + intercept\n        \n        # TODO 2: Compute residuals (prediction errors)\n        # Hint: residuals = predictions - y\n        \n        # TODO 3: Compute mean squared error\n        # Hint: cost = mean(residuals^2)\n        \n        # TODO 4: Validate cost is finite and non-negative\n        # Hint: Use np.isfinite and check cost >= 0\n        \n        raise NotImplementedError(\"Implement mean squared error calculation\")\n    \n    def _compute_gradients(\n        self, \n        x: np.ndarray, \n        y: np.ndarray, \n        slope: float, \n        intercept: float\n    ) -> Tuple[float, float]:\n        \"\"\"\n        Compute partial derivatives of cost function.\n        \n        Args:\n            x: Input features\n            y: Target values\n            slope: Current slope parameter\n            intercept: Current intercept parameter\n            \n        Returns:\n            (slope_gradient, intercept_gradient): Partial derivatives\n        \"\"\"\n        # TODO 1: Generate predictions using current parameters\n        # Hint: Use same prediction formula as in _compute_cost\n        \n        # TODO 2: Compute residuals\n        # Hint: residuals = predictions - y\n        \n        # TODO 3: Compute slope gradient\n        # Hint: slope_grad = 2 * mean(residuals * x)\n        # This measures correlation between errors and input values\n        \n        # TODO 4: Compute intercept gradient  \n        # Hint: intercept_grad = 2 * mean(residuals)\n        # This measures average bias in predictions\n        \n        # TODO 5: Validate gradients are finite\n        # Hint: Check both gradients with np.isfinite\n        \n        raise NotImplementedError(\"Implement gradient calculation\")\n    \n    def predict(self, x: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Make predictions using fitted model.\n        \n        Args:\n            x: Input features for prediction\n            \n        Returns:\n            Predicted values\n        \"\"\"\n        # TODO 1: Check that model has been fitted\n        # Hint: if not self.is_fitted_: raise ValueError(\"Model not fitted\")\n        \n        # TODO 2: Validate input data\n        # Hint: Check x is 1D array with finite values\n        \n        # TODO 3: Generate predictions using learned parameters\n        # Hint: return self.slope_ * x + self.intercept_\n        \n        raise NotImplementedError(\"Implement prediction method\")\n    \n    def score(self, x: np.ndarray, y: np.ndarray) -> float:\n        \"\"\"\n        Calculate R-squared coefficient of determination.\n        \n        Args:\n            x: Input features\n            y: True target values\n            \n        Returns:\n            R-squared score between 0 and 1\n        \"\"\"\n        # TODO 1: Generate predictions for input data\n        \n        # TODO 2: Compute total sum of squares (TSS)\n        # Hint: TSS = sum((y - mean(y))^2)\n        \n        # TODO 3: Compute residual sum of squares (RSS)  \n        # Hint: RSS = sum((y - predictions)^2)\n        \n        # TODO 4: Compute R-squared\n        # Hint: R² = 1 - RSS/TSS\n        \n        # TODO 5: Handle edge case where TSS = 0 (constant targets)\n        \n        raise NotImplementedError(\"Implement R-squared calculation\")\n    \n    def get_training_history(self) -> Dict:\n        \"\"\"Return complete training history for analysis.\"\"\"\n        return {\n            \"cost_history\": self.cost_history_.copy(),\n            \"parameter_history\": self.parameter_history_.copy(),\n            \"convergence_info\": self.convergence_detector.get_convergence_summary(),\n            \"hyperparameters\": {\n                \"learning_rate\": self.learning_rate,\n                \"max_iterations\": self.max_iterations,\n                \"tolerance\": self.tolerance\n            }\n        }\n```\n\n**Milestone Checkpoint:**\n\nAfter implementing the core gradient descent functionality, learners should be able to run this validation:\n\n```bash\npython -m pytest tests/test_gradient_descent.py -v\npython examples/gradient_descent_demo.py\n```\n\nExpected behavior:\n- Training cost should decrease monotonically (or with minor oscillations)\n- Algorithm should converge within 100-1000 iterations for simple datasets\n- Final R-squared scores should match closed-form solutions within 1e-4 tolerance\n- Parameter values should be reasonable (typically between -100 and 100 for normalized data)\n- Cost history plots should show smooth descent curves\n\n**Signs of successful implementation:**\n- Cost decreases from initial value and eventually plateaus\n- Parameters converge to stable values\n- Predictions closely match those from `SimpleLinearRegression` component\n- Algorithm handles different learning rates gracefully (slower/faster convergence)\n- Convergence detection stops training at appropriate points\n\n**Common debugging steps:**\n1. **Diverging cost**: Reduce learning rate by factor of 10\n2. **Slow convergence**: Increase learning rate or check feature normalization\n3. **Oscillating parameters**: Implement consecutive iteration requirements for convergence\n4. **Wrong final parameters**: Verify gradient calculation formulas and signs\n5. **Infinite values**: Add numerical stability checks and parameter bounds\n\n![Gradient Descent Algorithm Flow](./diagrams/gradient-descent-flow.svg)\n\n![Training Process Sequence](./diagrams/training-sequence.svg)\n\n![Model Training State Machine](./diagrams/model-state-machine.svg)\n\n\n## Multiple Linear Regression Component\n\n> **Milestone(s):** M3: Multiple Linear Regression\n\nThis component represents the culmination of our linear regression implementation, extending from single-variable regression to handle multiple input features simultaneously. The transition from simple to multiple linear regression transforms our system from fitting lines in two-dimensional space to fitting hyperplanes in multi-dimensional space, requiring fundamental changes in data representation, mathematical formulation, and computational approach.\n\n### Mental Model: Multi-Dimensional Plane Fitting\n\n**Extending from fitting lines to fitting hyperplanes through multi-dimensional data**\n\nThink of simple linear regression as being a carpenter trying to position a straight plank (representing y = mx + b) so it lies as close as possible to a scattered collection of nails on a wall. You can visualize this easily: the wall is two-dimensional (x and y axes), the nails are data points, and you're finding the best angle and position for your plank.\n\nMultiple linear regression is like being an architect designing a table surface that must come as close as possible to a collection of points floating in a room. Now you're working in three dimensions if you have two features (x₁, x₂, and y), or even higher dimensions with more features. Instead of a straight line, you're positioning a flat plane (or hyperplane in higher dimensions) through multi-dimensional space.\n\nThe mathematical challenge transforms from finding two parameters (slope and intercept) to finding as many parameters as you have features, plus one intercept term. Instead of the equation y = mx + b, you're solving y = w₁x₁ + w₂x₂ + w₃x₃ + ... + wₙxₙ + b, where each wᵢ represents how much that particular feature contributes to the prediction.\n\nThe intuition remains the same: you're still minimizing the sum of squared distances between your predictions and the actual data points. However, these \"distances\" now exist in multi-dimensional space, and the \"surface\" you're fitting can orient itself along multiple dimensions simultaneously. A feature with a large positive weight means that increases in that feature strongly push predictions upward, while a feature with a negative weight pushes predictions downward.\n\nThis mental model helps explain why feature scaling becomes critical: if one feature ranges from 0-1 while another ranges from 0-10000, the hyperplane will be dramatically tilted toward accommodating the large-scale feature, potentially ignoring the subtle but important patterns in the smaller-scale feature.\n\n### Matrix Formulation\n\n**Converting to matrix form and understanding the design matrix structure**\n\nThe transition from scalar equations to matrix operations represents the fundamental architectural shift that makes multiple linear regression computationally feasible. Rather than handling each feature and parameter individually, we reorganize all our data and parameters into matrices that allow us to perform all computations simultaneously through vectorized operations.\n\nThe design matrix X becomes the cornerstone of our matrix formulation. For a dataset with n samples and p features, X is an n × (p+1) matrix where each row represents one data sample and each column represents one feature, plus an additional column of ones for the intercept term. The first column contains all ones (the intercept column), and subsequent columns contain the actual feature values.\n\n| Component | Dimensions | Content | Purpose |\n|-----------|------------|---------|---------|\n| Design Matrix X | n × (p+1) | [1, x₁, x₂, ..., xₚ] for each sample | Input features with intercept column |\n| Weight Vector w | (p+1) × 1 | [b, w₁, w₂, ..., wₚ] | Model parameters including intercept |\n| Target Vector y | n × 1 | [y₁, y₂, ..., yₙ] | Ground truth values |\n| Prediction Vector ŷ | n × 1 | [ŷ₁, ŷ₂, ..., ŷₙ] | Model predictions |\n\nThe matrix equation y = Xw elegantly captures the entire linear relationship. When we multiply the design matrix X by the weight vector w, we automatically compute predictions for all samples simultaneously. The first element of w (the intercept) gets multiplied by the column of ones, adding the bias term to every prediction. Each subsequent element of w gets multiplied by its corresponding feature column, contributing its weighted influence to the final predictions.\n\nThe cost function transforms from a scalar computation to a matrix operation: Cost = (1/2n)||Xw - y||². This represents the mean squared error across all samples, computed as the squared Euclidean norm of the residual vector (Xw - y). The matrix formulation automatically handles the summation across all samples and features that we would otherwise need to implement with nested loops.\n\nGradient computation becomes a single matrix multiplication: ∇w = (1/n)X^T(Xw - y). The transpose operation X^T effectively rotates our perspective from \"samples × features\" to \"features × samples,\" allowing us to compute how each parameter should change based on all the prediction errors simultaneously. This single matrix operation replaces what would otherwise require separate partial derivative calculations for each parameter.\n\nThe architectural decision to use matrix formulation over scalar loops has profound implications for both computational efficiency and code maintainability. Matrix operations leverage highly optimized linear algebra libraries (like NumPy's underlying BLAS implementations), often achieving order-of-magnitude performance improvements over naive Python loops. Additionally, the matrix approach makes the code more declarative and less prone to indexing errors.\n\n> **The critical insight is that matrix formulation doesn't just make multiple linear regression faster—it makes it conceptually simpler by treating all parameters uniformly and eliminating special-case handling for individual features.**\n\n### Vectorized Gradient Descent\n\n**Implementing batch updates for all parameters simultaneously**\n\nVectorized gradient descent represents the computational heart of multiple linear regression, extending the iterative optimization approach from single-parameter updates to simultaneous updates of entire parameter vectors. This transformation requires careful consideration of numerical stability, convergence criteria, and computational efficiency when dealing with higher-dimensional parameter spaces.\n\nThe parameter update rule generalizes from scalar arithmetic to vector operations: w(t+1) = w(t) - α∇w, where α represents the learning rate and ∇w is the gradient vector computed for all parameters simultaneously. Unlike simple linear regression where we updated slope and intercept separately, vectorized gradient descent updates all components of the weight vector in a single atomic operation.\n\n| Operation | Mathematical Form | Computational Implementation | Complexity |\n|-----------|------------------|----------------------------|------------|\n| Cost Computation | (1/2n)\\\\|Xw - y\\\\|² | `np.mean((X @ weights - y) ** 2) / 2` | O(np) |\n| Gradient Computation | (1/n)X^T(Xw - y) | `X.T @ (X @ weights - y) / n` | O(np²) |\n| Parameter Update | w = w - α∇w | `weights -= learning_rate * gradients` | O(p) |\n| Convergence Check | \\\\|∇w\\\\|₂ < ε | `np.linalg.norm(gradients) < tolerance` | O(p) |\n\nThe batch gradient descent algorithm follows these detailed steps:\n\n1. Initialize the weight vector w with small random values or zeros, ensuring the vector includes space for the intercept term as the first component\n2. Construct the design matrix X by prepending a column of ones to the feature matrix, creating the (n × p+1) structure required for matrix multiplication\n3. For each iteration until convergence or maximum iterations reached:\n4. Compute predictions for all samples using matrix multiplication: ŷ = Xw, leveraging NumPy's optimized linear algebra routines\n5. Calculate the residual vector by subtracting actual targets from predictions: residuals = ŷ - y\n6. Compute the cost as the mean of squared residuals: cost = (1/2n)||residuals||²\n7. Calculate gradients for all parameters simultaneously: ∇w = (1/n)X^T × residuals\n8. Update all parameters using the vectorized update rule: w = w - α∇w\n9. Check convergence by evaluating multiple criteria: cost improvement, gradient magnitude, and parameter change magnitude\n10. Store training history including cost values, parameter vectors, and gradient norms for debugging and analysis\n\nThe convergence detection becomes more sophisticated with multiple parameters because we must consider the behavior of the entire parameter vector rather than individual scalars. We implement multiple convergence criteria that must be satisfied simultaneously:\n\n| Convergence Criterion | Formula | Interpretation | Threshold |\n|----------------------|---------|----------------|-----------|\n| Cost Improvement | \\|cost(t) - cost(t-1)\\| < ε₁ | Optimization progress stalled | 1e-8 |\n| Gradient Magnitude | \\\\|∇w\\\\|₂ < ε₂ | Reached local minimum | 1e-6 |\n| Parameter Change | \\\\|w(t) - w(t-1)\\\\|₂ < ε₃ | Parameters stabilized | 1e-6 |\n| Relative Improvement | \\|cost(t) - cost(t-1)\\|/cost(t-1) < ε₄ | Relative progress minimal | 1e-10 |\n\nThe learning rate selection becomes more critical with multiple features because the parameter space has more dimensions along which the optimization can become unstable. Features with vastly different scales can create ill-conditioned optimization surfaces where the optimal learning rate for one parameter is too large or too small for others. This motivates the crucial importance of feature scaling.\n\nNumerical stability monitoring must track the behavior of all parameters simultaneously. We implement safeguards that detect parameter explosion (any parameter exceeding MAX_PARAMETER_VALUE), gradient explosion (gradient norm exceeding MAX_GRADIENT_NORM), and numerical underflow (gradient norm below MIN_GRADIENT_NORM). When instability is detected, the algorithm can automatically reduce the learning rate or terminate with a diagnostic message.\n\n> **Decision: Batch Gradient Descent vs Stochastic Variants**\n> - **Context**: Multiple approaches exist for computing gradients in multi-dimensional parameter spaces\n> - **Options Considered**: \n>   - Batch gradient descent (full dataset per iteration)\n>   - Stochastic gradient descent (single sample per iteration)\n>   - Mini-batch gradient descent (subset of samples per iteration)\n> - **Decision**: Implement batch gradient descent for the educational implementation\n> - **Rationale**: Batch gradient descent provides the most stable and predictable convergence behavior for learning purposes, with deterministic results that aid debugging and understanding. The computational overhead is acceptable for the dataset sizes typical in educational scenarios\n> - **Consequences**: Enables consistent convergence behavior and reproducible results, but requires more memory and computational resources per iteration compared to stochastic variants\n\n### Feature Scaling and Engineering\n\n**Normalization techniques and handling the intercept term**\n\nFeature scaling represents one of the most critical preprocessing steps in multiple linear regression, transforming raw features with potentially vastly different scales into a normalized space where gradient descent can converge efficiently and stably. The architectural challenge lies in implementing scaling that is both mathematically sound and practically robust while maintaining the ability to transform new data consistently.\n\nThe fundamental problem arises when features exist on dramatically different scales. Consider predicting house prices using features like \"number of bedrooms\" (typically 1-5) and \"lot size in square feet\" (typically 1000-10000). Without scaling, the gradient descent algorithm will be dominated by the large-magnitude feature, potentially ignoring the subtle but important patterns in the smaller-scale feature. The optimization surface becomes elongated and ill-conditioned, requiring impractically small learning rates to avoid divergence.\n\nZ-score normalization (standardization) transforms each feature to have zero mean and unit variance: x_scaled = (x - μ)/σ, where μ is the feature mean and σ is the feature standard deviation. This transformation preserves the relative relationships between data points while ensuring all features contribute equally to the optimization dynamics.\n\n| Scaling Method | Formula | When to Use | Properties |\n|----------------|---------|-------------|------------|\n| Z-score (Standard) | (x - μ)/σ | Default choice, features normally distributed | Mean=0, Std=1, preserves outliers |\n| Min-Max Scaling | (x - min)/(max - min) | Features need specific range [0,1] | Bounded output, sensitive to outliers |\n| Robust Scaling | (x - median)/IQR | Features contain outliers | Uses percentiles, less sensitive to outliers |\n| Unit Vector Scaling | x/\\\\|x\\\\|₂ | Sparse features, text processing | Preserves direction, magnitude=1 |\n\nThe implementation must carefully handle the storage and application of scaling parameters to ensure consistency between training and prediction phases. During training, we compute and store the mean and standard deviation for each feature. During prediction, we must apply exactly the same scaling transformation to new data, which requires persistent storage of these scaling parameters.\n\nThe intercept term requires special architectural consideration because it should never be scaled. The intercept represents the baseline prediction when all features equal zero (in the scaled space), and scaling the intercept would fundamentally alter this interpretation. Our design matrix construction must ensure the first column remains as ones, unaffected by any scaling operations applied to the feature columns.\n\n```python\n# Implementation approach for feature scaling architecture:\nclass FeatureScaler:\n    def __init__(self, method='standard'):\n        self.method = method\n        self.feature_means_ = None\n        self.feature_stds_ = None\n        self.is_fitted_ = False\n    \n    def fit(self, features):\n        # Compute and store scaling parameters\n        # Never include intercept column in scaling calculations\n    \n    def transform(self, features):\n        # Apply stored scaling parameters\n        # Preserve intercept column unchanged\n    \n    def fit_transform(self, features):\n        # Convenience method combining fit and transform\n```\n\nThe scaling operation integrates seamlessly with our Dataset architecture, updating the normalization_stats dictionary with the computed parameters and setting the is_normalized flag to track the scaling state. This design ensures that predictions on new data automatically apply consistent scaling without requiring manual parameter management.\n\nFeature engineering considerations extend beyond simple scaling to include handling of categorical variables, polynomial features, and interaction terms. However, for the educational scope of this implementation, we focus on continuous numerical features with standard scaling as the primary transformation.\n\nThe architectural decision to implement scaling within the DataHandler component rather than the regression model itself reflects the principle that data preprocessing should be separated from model training. This separation enables the scaling logic to be reused across different model types and ensures that scaling parameters are managed consistently throughout the data pipeline.\n\n> **Decision: When to Apply Feature Scaling**\n> - **Context**: Feature scaling can be applied at data loading time or during model fitting\n> - **Options Considered**:\n>   - Scale during data loading (eager scaling)\n>   - Scale during model fitting (lazy scaling)\n>   - Scale on-demand during prediction (dynamic scaling)\n> - **Decision**: Implement scaling as an explicit method call during data preprocessing, before model fitting\n> - **Rationale**: Explicit scaling provides learners with clear visibility into the preprocessing pipeline and enables experimentation with different scaling strategies. It also prevents accidental data leakage by ensuring scaling parameters are computed only from training data\n> - **Consequences**: Requires explicit scaling calls but provides maximum flexibility and educational transparency\n\n### L2 Regularization (Ridge)\n\n**Adding penalty terms to prevent overfitting with many features**\n\nL2 regularization, commonly known as Ridge regression, addresses the fundamental challenge of overfitting that emerges when working with multiple features, particularly when the number of features approaches or exceeds the number of training samples. The regularization mechanism adds a penalty term to the cost function that discourages large parameter values, effectively constraining the model's complexity and improving its generalization to unseen data.\n\nThe mathematical foundation of Ridge regression modifies our standard cost function by adding a penalty term proportional to the sum of squared parameter values: Cost = (1/2n)||Xw - y||² + λ(1/2)||w||², where λ (lambda) represents the regularization strength hyperparameter. The penalty term (1/2)||w||² computes the squared L2 norm of the weight vector, excluding the intercept term which should not be regularized.\n\nThe architectural motivation for including regularization stems from the curse of dimensionality and the bias-variance tradeoff. As we add more features to our model, the parameter space becomes increasingly high-dimensional, creating more opportunities for the model to memorize training data rather than learning generalizable patterns. Regularization acts as a constraint that forces the model to find simpler solutions by preferring smaller parameter values.\n\n| Regularization Aspect | Standard Regression | Ridge Regression | Impact |\n|----------------------|-------------------|------------------|---------|\n| Cost Function | (1/2n)\\\\|Xw - y\\\\|² | (1/2n)\\\\|Xw - y\\\\|² + λ(1/2)\\\\|w\\\\|² | Penalizes large weights |\n| Gradient Computation | (1/n)X^T(Xw - y) | (1/n)X^T(Xw - y) + λw | Shrinks weights toward zero |\n| Parameter Updates | w = w - α∇w | w = w - α(∇w + λw) | Implicit weight decay |\n| Solution Behavior | Can overfit with many features | Controlled complexity | Better generalization |\n\nThe gradient computation incorporates the regularization term by adding λw to the standard gradient: ∇w_regularized = (1/n)X^T(Xw - y) + λw. This addition has the effect of pulling all parameters toward zero during each update step, with the strength of this \"weight decay\" controlled by the regularization parameter λ. Importantly, the intercept term (first element of w) should be excluded from regularization because it represents the baseline prediction and should not be penalized for being large.\n\nThe regularization strength λ represents a crucial hyperparameter that controls the tradeoff between fitting the training data and keeping the model simple. Small λ values (approaching 0) provide minimal regularization and may allow overfitting, while large λ values heavily penalize parameter magnitude and may lead to underfitting. The optimal λ value typically requires experimentation or cross-validation to determine.\n\n| Lambda Value | Regularization Effect | Model Behavior | When to Use |\n|--------------|----------------------|----------------|-------------|\n| λ = 0 | No regularization | Standard linear regression | More samples than features |\n| λ ≈ 0.01 | Light regularization | Slight bias toward simplicity | Mild overfitting signs |\n| λ ≈ 0.1 | Moderate regularization | Balanced complexity control | Moderate overfitting risk |\n| λ ≥ 1.0 | Strong regularization | Heavy bias toward zero weights | High overfitting risk |\n\nThe implementation architecture extends our existing gradient descent framework by modifying the gradient computation and cost calculation to include regularization terms. The regularization strength becomes a configurable parameter of the model, stored in the ModelParameters structure and used consistently throughout the training process.\n\nThe convergence criteria require slight modification under regularization because the additional penalty term affects both the cost function trajectory and the gradient magnitudes. Regularized gradients may not approach zero as closely as unregularized gradients, requiring adjusted convergence thresholds that account for the persistent regularization pressure.\n\nRidge regression provides several architectural advantages beyond overfitting prevention. The regularization term improves the numerical conditioning of the optimization problem, often enabling more stable convergence even with correlated features. Additionally, Ridge regression handles multicollinearity more gracefully than unregularized regression by distributing parameter values across correlated features rather than arbitrarily assigning large weights to individual features.\n\n> **Decision: L2 vs L1 Regularization**\n> - **Context**: Multiple regularization methods exist for controlling model complexity\n> - **Options Considered**:\n>   - L2 regularization (Ridge): penalty = λ||w||²\n>   - L1 regularization (Lasso): penalty = λ||w||₁  \n>   - Elastic Net: penalty = λ₁||w||₁ + λ₂||w||²\n> - **Decision**: Implement L2 regularization (Ridge) for the educational version\n> - **Rationale**: L2 regularization provides smooth, differentiable gradients that work seamlessly with gradient descent optimization. It shrinks parameters toward zero without creating sparsity, making it easier to understand and debug. The mathematical foundation aligns well with the least squares framework already established\n> - **Consequences**: Provides effective overfitting control and improved numerical stability, but does not perform feature selection like L1 regularization would\n\n### Architecture Decision Records\n\n**Decisions about matrix operations, regularization strength, and feature scaling methods**\n\nThe architectural decisions for multiple linear regression fundamentally shape both the educational value and practical implementation of the system. Each decision represents a deliberate tradeoff between simplicity, performance, educational clarity, and extensibility.\n\n> **Decision: NumPy Matrix Operations vs Pure Python Implementation**\n> - **Context**: Multiple approaches exist for implementing matrix operations required for vectorized computations\n> - **Options Considered**:\n>   - Pure Python with nested loops for all matrix operations\n>   - NumPy arrays with explicit loop implementations for educational clarity\n>   - Full NumPy vectorization using optimized linear algebra routines\n> - **Decision**: Use full NumPy vectorization for all matrix operations\n> - **Rationale**: NumPy provides orders of magnitude performance improvement over pure Python, essential for reasonable training times on realistic datasets. The vectorized operations more accurately represent production machine learning implementations. Educational value comes from understanding the mathematical concepts rather than implementing low-level matrix arithmetic\n> - **Consequences**: Requires NumPy dependency but enables practical performance and industry-standard implementation patterns. Students learn to think in terms of matrix operations rather than scalar loops\n\n> **Decision: Default Regularization Strategy**\n> - **Context**: Regularization strength significantly impacts model behavior and requires sensible defaults\n> - **Options Considered**:\n>   - No regularization by default (λ = 0)\n>   - Light regularization by default (λ = 0.01) \n>   - Adaptive regularization based on dataset characteristics\n> - **Decision**: No regularization by default, with easy configuration for Ridge regularization\n> - **Rationale**: Zero regularization maintains consistency with standard linear regression behavior and allows students to observe overfitting effects directly. Regularization should be an explicit choice that students make when they understand the need for it. This approach supports progressive learning where students first master basic concepts before adding complexity\n> - **Consequences**: Students may initially encounter overfitting in high-dimensional scenarios, creating learning opportunities to understand when and why regularization is needed\n\n> **Decision: Feature Scaling Integration Architecture**\n> - **Context**: Feature scaling can be tightly coupled with the model or implemented as separate preprocessing\n> - **Options Considered**:\n>   - Automatic scaling within the model's fit method\n>   - Manual scaling as a separate preprocessing step\n>   - Optional scaling with automatic detection of scale differences\n> - **Decision**: Manual scaling as an explicit preprocessing step with clear API methods\n> - **Rationale**: Explicit preprocessing teaches the importance of feature scaling and prevents hidden transformations that obscure the learning process. Manual control enables experimentation with different scaling strategies and makes the data pipeline transparent. This approach aligns with production machine learning practices where preprocessing is explicitly managed\n> - **Consequences**: Requires students to explicitly consider and implement feature scaling, which increases initial complexity but provides deeper understanding of preprocessing importance\n\n> **Decision: Parameter Initialization Strategy**\n> - **Context**: Initial parameter values significantly affect gradient descent convergence in high-dimensional spaces\n> - **Options Considered**:\n>   - Zero initialization for all parameters\n>   - Small random initialization from normal distribution  \n>   - Xavier/He initialization adapted for linear regression\n> - **Decision**: Small random initialization from normal distribution with scale 0.01\n> - **Rationale**: Random initialization prevents symmetry breaking issues that can occur with zero initialization when features are scaled. Small scale (0.01) ensures parameters start near zero without being exactly zero, providing gentle initial gradients. This approach works well across different dataset scales and feature counts\n> - **Consequences**: Provides robust initialization across various scenarios while remaining simple to understand and implement\n\n| Decision Area | Chosen Approach | Alternative Considered | Trade-off Rationale |\n|---------------|----------------|----------------------|-------------------|\n| Memory Management | Store full design matrix | Compute features on-demand | Favors simplicity over memory efficiency for educational datasets |\n| Convergence Detection | Multiple criteria (cost, gradient, parameters) | Single cost-based criterion | More robust but requires tuning multiple thresholds |\n| Learning Rate Strategy | Fixed learning rate | Adaptive/decaying learning rate | Simpler to understand and debug for learning purposes |\n| Numerical Precision | Standard float64 | Mixed precision computation | Prioritizes numerical stability over computational efficiency |\n\nThe parameter update architecture implements safeguards against numerical instability that becomes more critical with multiple parameters. We implement gradient clipping to prevent explosion, parameter bounds to prevent overflow, and automatic learning rate reduction when instability is detected.\n\nThe error handling architecture extends to include matrix dimension validation, ensuring that all matrix operations receive compatible dimensions. The system validates that the feature matrix dimensions align with the parameter vector dimensions and that all samples have consistent feature counts.\n\n### Common Pitfalls\n\n**Matrix dimension errors, feature scaling issues, and regularization parameter tuning**\n\nMultiple linear regression introduces several categories of errors that are less common or entirely absent in simple linear regression. These pitfalls often stem from the increased complexity of matrix operations, feature interactions, and the additional hyperparameters introduced by regularization and scaling.\n\n⚠️ **Pitfall: Matrix Dimension Mismatches**\nThe most frequent error in multiple linear regression implementations involves incompatible matrix dimensions during mathematical operations. The design matrix X must have dimensions (n_samples × n_features+1), the weight vector w must have dimensions (n_features+1 × 1), and the target vector y must have dimensions (n_samples × 1). Common mistakes include forgetting to add the intercept column to X, creating w with the wrong number of parameters, or attempting to multiply matrices with incompatible inner dimensions. The error typically manifests as NumPy broadcasting errors or explicit dimension mismatch exceptions. To avoid this, always validate matrix shapes before mathematical operations and implement helper functions that construct matrices with correct dimensions. Use assertions to check dimensions at critical points: `assert X.shape[1] == len(weights), f\"Feature count mismatch: X has {X.shape[1]} columns but weights has {len(weights)} elements\"`.\n\n⚠️ **Pitfall: Scaling the Intercept Term**\nA subtle but critical error involves including the intercept column (column of ones) in feature scaling operations. When the intercept column gets scaled, it no longer represents a constant bias term, fundamentally altering the model's mathematical properties and typically causing convergence problems or nonsensical results. This error often occurs when applying scaling transformations to the entire design matrix instead of only the feature columns. The intercept column should always remain as pure ones, unaffected by any scaling operations. Implement scaling operations that explicitly exclude the first column: `scaled_features = scaler.fit_transform(X[:, 1:])`, then reconstruct the design matrix by concatenating the intercept column with the scaled features.\n\n⚠️ **Pitfall: Data Leakage in Feature Scaling**\nFeature scaling parameters (mean and standard deviation) must be computed exclusively from training data, never from the combination of training and test data. Computing scaling parameters from the entire dataset creates data leakage, where information from test samples influences the model training process. This leakage often produces overly optimistic performance estimates that don't reflect real-world generalization. The correct approach computes scaling parameters during training (`scaler.fit(X_train)`), then applies these same parameters to test data (`X_test_scaled = scaler.transform(X_test)`). Never call `fit_transform` on test data; always use the `transform` method with parameters learned from training data.\n\n⚠️ **Pitfall: Regularizing the Intercept Parameter**\nRidge regularization should exclude the intercept term from the penalty calculation because the intercept represents the baseline prediction and should not be constrained toward zero. Including the intercept in regularization can bias the model toward predicting zero regardless of the actual target distribution. This error typically occurs when applying the regularization penalty to the entire weight vector instead of excluding the first element (intercept). Implement regularization that explicitly excludes the intercept: `l2_penalty = lambda_reg * np.sum(weights[1:] ** 2)` and modify gradients accordingly: `regularized_gradients[1:] += lambda_reg * weights[1:]`.\n\n⚠️ **Pitfall: Inappropriate Regularization Strength**\nRegularization strength (λ) requires careful tuning because inappropriate values can lead to severe underfitting or provide no overfitting protection. Very large λ values (≥1.0) can shrink all parameters toward zero, effectively ignoring the training data and producing poor predictions. Very small λ values (≤1e-6) provide minimal regularization benefit. The optimal λ typically falls between 0.001 and 0.1 for most datasets, but requires experimentation or cross-validation. Start with λ = 0.01 as a reasonable default and adjust based on training vs validation performance. Monitor the magnitude of parameter values; if they approach zero, reduce λ; if they grow very large, increase λ.\n\n⚠️ **Pitfall: Learning Rate Too Large for High-Dimensional Spaces**\nLearning rates that work well for simple linear regression often cause divergence in multiple linear regression due to the increased complexity of the parameter space. With more parameters, the optimization surface becomes more complex, and the same learning rate may be too aggressive for stable convergence. Symptoms include oscillating or increasing cost values, parameter values that grow extremely large, or NaN values appearing in gradients or parameters. Start with smaller learning rates (0.001-0.01) for multiple features and implement adaptive learning rate reduction when instability is detected. Monitor parameter magnitudes and cost function behavior; stable training should show monotonically decreasing cost and bounded parameter values.\n\n⚠️ **Pitfall: Inadequate Convergence Criteria**\nSingle convergence criteria (like cost-based stopping) may be insufficient for multiple linear regression because the high-dimensional parameter space can exhibit complex convergence behavior. The cost function may plateau while individual parameters continue changing significantly, or gradients may remain non-zero due to regularization effects. Implement multiple convergence criteria that all must be satisfied: cost improvement below threshold, gradient magnitude below threshold, and parameter change magnitude below threshold. Use relative thresholds in addition to absolute thresholds to handle different problem scales: `abs(cost_change) / cost < relative_tolerance`.\n\n| Error Symptom | Likely Cause | Diagnostic Approach | Fix Strategy |\n|---------------|--------------|-------------------|--------------|\n| \"Cannot multiply matrices\" | Dimension mismatch | Print shapes before operations | Validate and fix matrix construction |\n| Cost increases during training | Learning rate too high or intercept scaled | Monitor parameter magnitudes | Reduce learning rate, check scaling |\n| All parameters near zero | Regularization too strong | Check λ value and parameter history | Reduce regularization strength |\n| Model ignores some features | Poor feature scaling | Check feature value ranges | Apply proper normalization |\n| NaN values in gradients | Numerical overflow | Check parameter bounds | Add gradient clipping, reduce learning rate |\n| Perfect training accuracy, poor test | Data leakage in scaling | Review preprocessing pipeline | Recompute scaling from training only |\n\n### Implementation Guidance\n\nThis section provides concrete implementation guidance for building the multiple linear regression component, focusing on the matrix operations, vectorized computations, and architectural patterns needed to extend from simple linear regression to multi-dimensional feature spaces.\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Matrix Operations | NumPy with basic linear algebra | NumPy with BLAS-optimized routines |\n| Feature Scaling | Manual standardization | Scikit-learn preprocessing (for reference) |\n| Regularization | Basic L2 penalty implementation | Configurable regularization factory |\n| Validation | Simple assertion checks | Comprehensive input validation suite |\n\n**Recommended File Structure:**\n```\nproject-root/\n├── src/\n│   ├── data_handler.py           ← Enhanced with scaling capabilities\n│   ├── simple_regression.py      ← From previous milestone\n│   ├── gradient_descent.py       ← From previous milestone\n│   ├── multiple_regression.py    ← New: Main multiple regression class\n│   └── feature_scaler.py         ← New: Feature scaling utilities\n├── tests/\n│   ├── test_multiple_regression.py\n│   └── test_feature_scaling.py\n├── data/\n│   ├── housing.csv               ← Multi-feature dataset\n│   └── synthetic_multi.csv       ← Generated multi-dimensional data\n└── examples/\n    ├── multiple_regression_demo.py\n    └── regularization_comparison.py\n```\n\n**Complete Feature Scaler Implementation:**\n```python\nimport numpy as np\nfrom typing import Optional, Dict, Any\n\nclass StandardScaler:\n    \"\"\"Z-score normalization for multiple features.\n    \n    Transforms features to have zero mean and unit variance.\n    Preserves intercept column (first column) unchanged.\n    \"\"\"\n    \n    def __init__(self):\n        self.mean_: Optional[np.ndarray] = None\n        self.std_: Optional[np.ndarray] = None\n        self.is_fitted_: bool = False\n        self.n_features_: int = 0\n    \n    def fit(self, X: np.ndarray) -> 'StandardScaler':\n        \"\"\"Compute scaling parameters from training data.\n        \n        Args:\n            X: Feature matrix with shape (n_samples, n_features+1)\n               First column should be intercept (all ones)\n        \n        Returns:\n            self for method chaining\n        \"\"\"\n        if X.ndim != 2:\n            raise ValueError(f\"Expected 2D array, got {X.ndim}D\")\n        \n        # Extract feature columns (exclude intercept)\n        features = X[:, 1:] if X.shape[1] > 1 else X\n        \n        self.mean_ = np.mean(features, axis=0)\n        self.std_ = np.std(features, axis=0)\n        \n        # Handle constant features (std = 0)\n        self.std_[self.std_ == 0] = 1.0\n        \n        self.n_features_ = features.shape[1]\n        self.is_fitted_ = True\n        return self\n    \n    def transform(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Apply scaling to new data using stored parameters.\n        \n        Args:\n            X: Feature matrix with shape (n_samples, n_features+1)\n        \n        Returns:\n            Scaled feature matrix with same shape as input\n        \"\"\"\n        if not self.is_fitted_:\n            raise ValueError(\"Scaler not fitted. Call fit() first.\")\n        \n        X_scaled = X.copy()\n        \n        # Scale only feature columns, preserve intercept\n        if X.shape[1] > 1:\n            features = X[:, 1:]\n            scaled_features = (features - self.mean_) / self.std_\n            X_scaled[:, 1:] = scaled_features\n        \n        return X_scaled\n    \n    def fit_transform(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Fit scaler and transform data in one step.\"\"\"\n        return self.fit(X).transform(X)\n```\n\n**Multiple Linear Regression Core Class Structure:**\n```python\nimport numpy as np\nfrom typing import Optional, Tuple, List, Dict, Any\n\nclass MultipleLinearRegression:\n    \"\"\"Multiple linear regression with gradient descent and optional Ridge regularization.\"\"\"\n    \n    def __init__(self, \n                 learning_rate: float = 0.01,\n                 max_iterations: int = 1000,\n                 tolerance: float = 1e-6,\n                 regularization_strength: float = 0.0):\n        self.learning_rate = learning_rate\n        self.max_iterations = max_iterations\n        self.tolerance = tolerance\n        self.regularization_strength = regularization_strength\n        \n        # Model parameters\n        self.weights_: Optional[np.ndarray] = None\n        self.is_fitted_: bool = False\n        self.n_features_: int = 0\n        \n        # Training history\n        self.cost_history_: List[float] = []\n        self.weight_history_: List[np.ndarray] = []\n        self.gradient_norms_: List[float] = []\n        \n        # Convergence info\n        self.converged_: bool = False\n        self.final_iteration_: int = 0\n        self.convergence_reason_: str = \"\"\n    \n    def fit(self, X: np.ndarray, y: np.ndarray) -> 'MultipleLinearRegression':\n        \"\"\"Train the multiple regression model using gradient descent.\n        \n        Args:\n            X: Feature matrix with shape (n_samples, n_features+1)\n               First column should be intercept (all ones)\n            y: Target vector with shape (n_samples,)\n        \n        Returns:\n            self for method chaining\n        \"\"\"\n        # TODO 1: Validate input dimensions and data types\n        # TODO 2: Initialize weights vector with small random values\n        # TODO 3: Implement gradient descent loop with vectorized operations\n        # TODO 4: Compute cost using matrix operations: (1/2n)||Xw - y||²\n        # TODO 5: Add L2 regularization to cost: + (λ/2)||w[1:]||²\n        # TODO 6: Compute gradients: (1/n)X^T(Xw - y) + λ[0, w1, w2, ...]\n        # TODO 7: Update weights: w = w - α * gradients\n        # TODO 8: Check multiple convergence criteria\n        # TODO 9: Store training history for debugging\n        # TODO 10: Handle numerical instability and divergence\n        pass\n    \n    def predict(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Generate predictions using fitted model.\n        \n        Args:\n            X: Feature matrix with shape (n_samples, n_features+1)\n        \n        Returns:\n            Predictions with shape (n_samples,)\n        \"\"\"\n        # TODO 1: Validate model is fitted\n        # TODO 2: Validate input dimensions match training data\n        # TODO 3: Compute predictions using matrix multiplication: Xw\n        # TODO 4: Return predictions as 1D array\n        pass\n    \n    def score(self, X: np.ndarray, y: np.ndarray) -> float:\n        \"\"\"Calculate R-squared coefficient of determination.\n        \n        Args:\n            X: Feature matrix with shape (n_samples, n_features+1)\n            y: Target vector with shape (n_samples,)\n        \n        Returns:\n            R-squared score between 0 and 1\n        \"\"\"\n        # TODO 1: Generate predictions for input data\n        # TODO 2: Compute total sum of squares: Σ(yi - ȳ)²\n        # TODO 3: Compute residual sum of squares: Σ(yi - ŷi)²\n        # TODO 4: Calculate R² = 1 - RSS/TSS\n        # TODO 5: Handle edge cases (perfect fit, no variance)\n        pass\n    \n    def _compute_cost(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray) -> float:\n        \"\"\"Compute regularized mean squared error cost.\n        \n        Args:\n            X: Feature matrix\n            y: Target vector  \n            weights: Current parameter values\n            \n        Returns:\n            Regularized cost value\n        \"\"\"\n        # TODO 1: Compute predictions: Xw\n        # TODO 2: Compute residuals: predictions - y\n        # TODO 3: Compute MSE: (1/2n) * ||residuals||²\n        # TODO 4: Add L2 penalty: (λ/2) * ||weights[1:]||² (exclude intercept)\n        # TODO 5: Return total cost\n        pass\n    \n    def _compute_gradients(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray) -> np.ndarray:\n        \"\"\"Compute regularized gradients for all parameters.\n        \n        Args:\n            X: Feature matrix\n            y: Target vector\n            weights: Current parameter values\n            \n        Returns:\n            Gradient vector with same shape as weights\n        \"\"\"\n        # TODO 1: Compute predictions: Xw\n        # TODO 2: Compute residuals: predictions - y\n        # TODO 3: Compute base gradients: (1/n) * X^T * residuals\n        # TODO 4: Add L2 regularization: gradients[1:] += λ * weights[1:]\n        # TODO 5: Keep intercept gradient unregularized\n        # TODO 6: Return complete gradient vector\n        pass\n```\n\n**Design Matrix Construction Helper:**\n```python\ndef create_design_matrix(features: np.ndarray) -> np.ndarray:\n    \"\"\"Construct design matrix by adding intercept column.\n    \n    Args:\n        features: Raw feature matrix (n_samples, n_features)\n        \n    Returns:\n        Design matrix (n_samples, n_features+1) with intercept column\n    \"\"\"\n    if features.ndim == 1:\n        features = features.reshape(-1, 1)\n    \n    n_samples = features.shape[0]\n    intercept_column = np.ones((n_samples, 1))\n    \n    return np.hstack([intercept_column, features])\n\ndef validate_regression_data(X: np.ndarray, y: np.ndarray) -> None:\n    \"\"\"Comprehensive validation for regression inputs.\"\"\"\n    if X.ndim != 2:\n        raise ValueError(f\"X must be 2D, got shape {X.shape}\")\n    \n    if y.ndim != 1:\n        raise ValueError(f\"y must be 1D, got shape {y.shape}\")\n    \n    if X.shape[0] != len(y):\n        raise ValueError(f\"Sample count mismatch: X has {X.shape[0]}, y has {len(y)}\")\n    \n    if X.shape[0] < X.shape[1]:\n        raise ValueError(f\"Insufficient samples: {X.shape[0]} samples for {X.shape[1]} features\")\n    \n    # Check for NaN or infinite values\n    if not np.isfinite(X).all():\n        raise ValueError(\"X contains NaN or infinite values\")\n    \n    if not np.isfinite(y).all():\n        raise ValueError(\"y contains NaN or infinite values\")\n```\n\n**Synthetic Data Generation for Testing:**\n```python\ndef generate_multiple_regression_data(n_samples: int = 100,\n                                    n_features: int = 3,\n                                    noise_std: float = 0.1,\n                                    random_state: Optional[int] = None) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Generate synthetic data for multiple linear regression.\n    \n    Creates data following y = X @ true_weights + noise\n    \n    Returns:\n        Tuple of (features, targets) arrays\n    \"\"\"\n    if random_state is not None:\n        np.random.seed(random_state)\n    \n    # Generate random features\n    X = np.random.randn(n_samples, n_features)\n    \n    # Generate true weights\n    true_weights = np.random.randn(n_features) * 2.0\n    true_intercept = np.random.randn() * 0.5\n    \n    # Generate targets with noise\n    y = X @ true_weights + true_intercept + np.random.normal(0, noise_std, n_samples)\n    \n    return X, y\n```\n\n**Milestone Checkpoint:**\nAfter implementing the multiple linear regression component, verify the following behavior:\n\n1. **Matrix Operations Test**: Create a simple 2-feature dataset and verify that matrix multiplication produces correct predictions:\n   ```python\n   X, y = generate_multiple_regression_data(n_samples=50, n_features=2, random_state=42)\n   X_design = create_design_matrix(X)\n   model = MultipleLinearRegression(learning_rate=0.01, max_iterations=1000)\n   model.fit(X_design, y)\n   predictions = model.predict(X_design)\n   r_squared = model.score(X_design, y)\n   print(f\"R-squared: {r_squared:.4f}\")  # Should be > 0.8 for synthetic data\n   ```\n\n2. **Feature Scaling Test**: Compare convergence with and without feature scaling on data with different feature scales:\n   ```python\n   # Test with unscaled features of different magnitudes\n   X_unscaled = np.column_stack([np.random.randn(100), np.random.randn(100) * 1000])\n   # Training should converge much faster after scaling\n   ```\n\n3. **Regularization Test**: Verify that increasing regularization strength shrinks parameter magnitudes:\n   ```python\n   # Compare parameter values with λ = 0.0 vs λ = 1.0\n   # Higher regularization should produce smaller parameter values\n   ```\n\n4. **Convergence Monitoring**: Check that training history shows decreasing cost and stabilizing parameters:\n   ```python\n   plt.plot(model.cost_history_)\n   plt.title(\"Cost Function Over Training\")\n   # Should show monotonic decrease to convergence\n   ```\n\nThe implementation should handle matrices with up to 10-20 features efficiently, converge within 1000 iterations for well-conditioned problems, and provide stable predictions with properly scaled features.\n\n\n## Interactions and Data Flow\n\n> **Milestone(s):** All milestones (M1: Simple Linear Regression - basic training and prediction flow, M2: Gradient Descent - iterative optimization flow, M3: Multiple Linear Regression - vectorized operations flow)\n\nThis section describes the orchestration of all components working together to transform raw data into trained models and generate predictions. Understanding these interactions is crucial because machine learning systems are inherently pipeline-oriented, where data flows through multiple transformation stages, each component depends on specific input formats and contracts, and the sequence of operations determines both correctness and performance.\n\n### Mental Model: The Assembly Line for Intelligence\n\nThink of our linear regression system as an **intelligent assembly line** where raw data enters at one end and emerges as a trained model capable of making predictions. Just like a car assembly line has specific stations (body assembly, engine installation, painting, quality control), our system has specialized components that must work in precise sequence.\n\nThe **DataHandler** acts like the **receiving dock** - it accepts raw materials (CSV files, arrays) and prepares them for processing by checking quality, standardizing formats, and organizing everything properly. The **model components** (SimpleLinearRegression, GradientDescentOptimizer, MultipleLinearRegression) are like **specialized workstations** - each has specific tools and capabilities, and they transform the data in predictable ways. The **prediction pipeline** is like the **shipping department** - it takes the finished product (trained model) and uses it to fulfill new orders (generate predictions for new data).\n\nCritical to this analogy is understanding that unlike a physical assembly line, our data flows through **state transformations** rather than physical modifications. The raw features don't get \"consumed\" - instead, they get **analyzed** to extract mathematical relationships (parameters) that capture the underlying patterns. Once we have these parameters, we can apply them to entirely new data to make predictions.\n\n### Training Data Flow\n\nThe training process represents the core learning pipeline where raw data transforms into a mathematical model capable of making predictions. This flow varies slightly between our three milestone approaches, but follows a consistent pattern of data preparation, parameter learning, and model validation.\n\n#### Step-by-Step Training Process\n\nThe complete training data flow follows these sequential stages:\n\n**Stage 1: Data Ingestion and Initial Validation**\n1. Raw data enters through `DataHandler.load_csv_data()` or direct array input\n2. The system validates basic data properties using `validate_data()` - checking for proper numeric types, compatible array shapes, and absence of pathological values like infinite or NaN entries\n3. Data gets packaged into a `Dataset` structure that includes feature matrix, target vector, metadata about dimensionality, and normalization status\n4. The handler performs regression-specific validation using `validate_regression_inputs()` to ensure we have sufficient samples relative to features (minimum 10:1 ratio) and that the target variable has adequate variance for meaningful fitting\n\n**Stage 2: Feature Preprocessing and Normalization** \n5. For gradient descent and multiple regression (M2/M3), the system applies z-score normalization via `normalize_features()` to prevent features with large scales from dominating the optimization\n6. The `StandardScaler` computes and stores mean and standard deviation statistics for each feature, enabling consistent transformation of future prediction data  \n7. For multiple regression, the system constructs the design matrix using `create_design_matrix()`, adding an intercept column of ones to handle the bias term mathematically\n8. Normalized features and scaling parameters get stored in the updated `Dataset` structure for later use during prediction\n\n**Stage 3: Model Initialization and Parameter Setup**\n9. The chosen model component (SimpleLinearRegression, GradientDescentRegression, or MultipleLinearRegression) initializes its internal parameter storage structures\n10. For closed-form solutions (M1), no initial parameter values are needed since the normal equation computes them directly\n11. For gradient descent approaches (M2/M3), parameters initialize to zeros or small random values, and the `TrainingHistory` structure gets prepared to track the optimization process\n12. The system validates that learning rate, convergence tolerance, and maximum iteration settings are within reasonable bounds\n\n**Stage 4: Parameter Learning Process**\n13. **Closed-form path (M1)**: The SimpleLinearRegression component applies the ordinary least squares normal equation directly, computing optimal slope and intercept in a single mathematical operation\n14. **Iterative optimization path (M2/M3)**: The gradient descent loop begins, alternating between cost computation using `_compute_cost()`, gradient calculation using `_compute_gradients()`, and parameter updates following the gradient descent rule\n15. Each iteration, the system checks multiple convergence criteria through `ConvergenceDetector` - cost improvement below threshold, gradient magnitude below tolerance, or parameter changes below minimum significance\n16. The optimization process records comprehensive training history including cost values, parameter evolution, and gradient norms for debugging and analysis purposes\n\n**Stage 5: Model Finalization and Validation**\n17. Upon convergence or reaching maximum iterations, the system packages final parameters into `ModelParameters` structure with metadata about the fitting method, regularization strength, and feature scaling applied\n18. The model's `is_fitted_` flag gets set to True, enabling prediction capabilities while preventing accidental reuse of untrained models\n19. Training history gets finalized with convergence status, final iteration count, and the specific reason for termination (tolerance reached, max iterations, numerical instability detected)\n20. The system validates the learned parameters for numerical stability - checking for overflow, underflow, or parameters that suggest overfitting or underfitting\n\n#### Data Format Transformations During Training\n\n| Stage | Input Format | Output Format | Key Transformations |\n|-------|--------------|---------------|-------------------|\n| Data Loading | CSV files or raw arrays | `Dataset` structure | Type conversion, shape validation, metadata extraction |\n| Preprocessing | Raw numeric features | Normalized features + scaling stats | Z-score normalization, design matrix construction |\n| Parameter Learning | Prepared feature matrix + targets | `ModelParameters` + `TrainingHistory` | Optimization algorithm application |\n| Model Storage | Optimization results | Fitted model with prediction capability | Parameter packaging, validation flags |\n\n#### Training Flow Variations by Milestone\n\n**Milestone 1 (Simple Linear Regression)**: The training flow emphasizes the closed-form solution path. Data flows from loading through basic validation directly to the normal equation computation. No iterative optimization occurs, making this the fastest training path but limited to single-variable problems without regularization.\n\n**Milestone 2 (Gradient Descent)**: Introduces the iterative optimization flow with comprehensive convergence detection. Data flows through the same preprocessing stages, but then enters the gradient descent loop with extensive history tracking. This milestone emphasizes understanding the optimization dynamics and debugging convergence issues.\n\n**Milestone 3 (Multiple Linear Regression)**: Extends to vectorized operations with feature scaling and regularization. The training flow handles arbitrary feature counts through matrix operations, includes Ridge regularization in the cost function, and demonstrates how proper preprocessing enables stable optimization of high-dimensional problems.\n\n#### Critical Synchronization Points\n\nThe training pipeline has several critical synchronization points where components must coordinate:\n\n**Data-Model Interface**: The model component must validate that incoming data matches its expected dimensionality and format. For single regression, this means ensuring exactly one feature column. For multiple regression, this means validating that the feature count matches what the model was configured to handle.\n\n**Preprocessing-Prediction Consistency**: The normalization statistics computed during training must be preserved and applied identically during prediction. This requires careful coordination between `StandardScaler` state and the model's prediction pipeline.\n\n**Convergence-History Synchronization**: The optimization loop must maintain perfect synchronization between parameter updates and history recording. Any mismatch between `parameter_history_` and `cost_history_` indices would break debugging and analysis capabilities.\n\n### Prediction Data Flow\n\nThe prediction process transforms new input data through the same preprocessing pipeline used during training, then applies the learned model parameters to generate predictions along with confidence metrics and error estimates.\n\n#### Mental Model: The Factory Production Line\n\nThink of prediction as a **factory production line** that's been calibrated during training. The training process was like **setting up and calibrating** all the machinery - determining the exact specifications, adjusting the equipment, and validating the quality control procedures. Now prediction is the **actual production run** - raw materials (new data) flow through the same stations, but instead of learning the settings, we apply the previously learned settings to transform inputs into outputs.\n\nThe key insight is that prediction must **exactly replicate** the training preprocessing pipeline. If training data was normalized, prediction data must be normalized using the same statistics. If training used a design matrix with intercept column, prediction must construct an identical design matrix structure. Any deviation from this preprocessing consistency will produce incorrect predictions.\n\n#### Step-by-Step Prediction Process\n\n**Stage 1: Input Validation and Format Checking**\n1. New data enters through the model's `predict()` method as raw feature arrays\n2. The system validates input data format, checking for correct data types, absence of NaN/infinite values, and appropriate array dimensions\n3. For single regression, validation ensures exactly one feature per sample; for multiple regression, validation confirms feature count matches training dimensionality\n4. The model checks its `is_fitted_` status to prevent prediction attempts on untrained models\n\n**Stage 2: Preprocessing Pipeline Replication**\n5. If the model was trained with feature normalization, prediction data gets transformed using the stored `StandardScaler` statistics via `apply_normalization()`\n6. The same mean and standard deviation values from training get applied to normalize new features, ensuring identical scaling\n7. For multiple regression, the system constructs the design matrix using `create_design_matrix()`, adding the intercept column in the same position as during training\n8. All preprocessing steps must exactly match the training pipeline to maintain mathematical consistency\n\n**Stage 3: Model Parameter Application**\n9. The system retrieves stored model parameters (`slope_`, `intercept_`, or `weights_`) from the fitted model\n10. **Single regression path**: Predictions compute as `y_pred = slope_ * x + intercept_` using scalar arithmetic\n11. **Multiple regression path**: Predictions compute as `y_pred = X @ weights_` using vectorized matrix multiplication where `X` is the design matrix and `weights_` includes the intercept term\n12. The mathematical operations produce raw prediction values for each input sample\n\n**Stage 4: Prediction Packaging and Metadata Generation**\n13. Raw predictions get packaged into a `PredictionResult` structure along with comprehensive metadata\n14. The system computes residuals if ground truth values are provided, calculating `residuals = y_true - y_pred`\n15. Confidence intervals get estimated using prediction variance calculations (simplified bootstrap or analytical approximations)\n16. Performance metrics like mean squared error and mean absolute error get computed when ground truth is available for validation\n\n**Stage 5: Output Validation and Error Checking**\n17. The system validates prediction outputs for numerical stability, checking for overflow, underflow, or impossible values\n18. Predictions get range-checked against reasonable bounds based on training data statistics\n19. The `PredictionResult` structure gets populated with input metadata like sample count, feature dimensionality, and model type for traceability\n20. Final output includes both predictions and comprehensive diagnostic information for debugging and analysis\n\n#### Prediction Data Format Requirements\n\n| Component | Input Requirements | Output Format | Validation Checks |\n|-----------|-------------------|---------------|-------------------|\n| Single Regression | 1D array or 2D array with 1 column | 1D prediction array | Shape, data type, finite values |\n| Multiple Regression | 2D array with n_features columns | 1D prediction array + metadata | Dimension match, normalization compatibility |\n| All Models | Same preprocessing as training | `PredictionResult` structure | Model fitted status, numerical stability |\n\n#### Preprocessing Consistency Requirements\n\nThe prediction pipeline must maintain perfect consistency with the training preprocessing:\n\n**Normalization Consistency**: The `StandardScaler` fitted during training stores `mean_` and `std_` arrays that must be applied identically to prediction data. Using different normalization statistics would shift and scale features incorrectly, producing meaningless predictions.\n\n**Design Matrix Consistency**: For multiple regression, the design matrix construction during prediction must match training exactly - same intercept column position, same feature ordering, same data types. Matrix shape mismatches would cause dimension errors in the prediction computation.\n\n**Data Type Consistency**: Prediction data must use the same numeric precision (float32 vs float64) as training data to prevent subtle numerical differences that accumulate through matrix operations.\n\n#### Prediction Flow Variations by Milestone\n\n**Milestone 1 Prediction Flow**: Simple scalar arithmetic with minimal preprocessing. The flow emphasizes understanding the basic prediction equation and handling edge cases like division by zero or extreme input values.\n\n**Milestone 2 Prediction Flow**: Identical to M1 mathematically, but includes more comprehensive error checking and history tracking for debugging optimization issues. The flow demonstrates how gradient descent parameters produce identical predictions to closed-form solutions.\n\n**Milestone 3 Prediction Flow**: Full vectorized pipeline with matrix operations, feature scaling, and regularization effects. The flow handles arbitrary dimensionality and demonstrates computational efficiency through proper vectorization.\n\n### Component Communication Protocols\n\nThe communication between system components follows strict interface contracts that ensure data consistency, error propagation, and state management throughout the training and prediction pipelines.\n\n#### Mental Model: The Diplomatic Embassy System\n\nThink of component communication as a **diplomatic embassy system** where each component is a sovereign nation with its own internal rules, but they must communicate through **formal diplomatic protocols** to accomplish shared goals. Each component has **ambassadors** (public methods) that handle incoming requests and **treaties** (interface contracts) that specify exactly what information gets exchanged in what format.\n\nThe `DataHandler` acts like the **United Nations** - it speaks everyone's language and provides neutral ground for data exchange. The model components are like **specialized technical agencies** - each has deep expertise in its domain but needs standardized protocols to share information. The prediction system is like an **international trade network** - it must ensure that goods (data) produced in one country (component) can be consumed safely in another.\n\nCritical to this analogy is understanding that **protocol violations are serious diplomatic incidents** - if one component sends data in the wrong format or violates interface contracts, the entire system can fail in unpredictable ways. Therefore, our protocols include extensive **verification** (input validation), **translation services** (data format conversion), and **error reporting mechanisms** (exception handling with context).\n\n#### Interface Contracts and Data Exchange Formats\n\nEach component exposes well-defined interfaces with strict contracts about input requirements, output guarantees, and error conditions:\n\n**DataHandler Interface Contract**\n\n| Method | Input Requirements | Output Guarantees | Error Conditions |\n|--------|-------------------|-------------------|------------------|\n| `load_csv_data()` | Valid file path, existing columns | `Dataset` with validated arrays | File not found, column missing, invalid data types |\n| `validate_data()` | Feature and target arrays | Silent success or exception | Shape mismatch, NaN values, insufficient samples |\n| `normalize_features()` | Numeric feature matrix | `Dataset` with normalized features + stats | Constant features, numerical overflow |\n| `apply_normalization()` | Features + fitted scaler | Consistently scaled feature matrix | Dimension mismatch, unfitted scaler |\n\nThe DataHandler acts as the **data embassy**, translating between external formats (CSV files, raw arrays) and internal formats (`Dataset` structures). It guarantees that any `Dataset` it produces will be mathematically valid for regression - proper shapes, numeric types, no pathological values.\n\n**Model Component Interface Contracts**\n\n| Method | Input Requirements | Output Guarantees | State Changes |\n|--------|-------------------|-------------------|---------------|\n| `fit(x, y)` | Validated feature/target arrays | Fitted model with `is_fitted_=True` | Parameters learned, history recorded |\n| `predict(x)` | Same format as training features | Prediction array matching input samples | No state changes |\n| `score(x, y)` | Features + ground truth targets | R-squared coefficient in [0, 1] | No state changes |\n\nEach model component acts as a **specialized technical agency** with deep expertise in its optimization approach. The interface contracts ensure that regardless of internal implementation differences (closed-form vs gradient descent vs regularization), all models expose identical external interfaces.\n\n**Gradient Descent Optimizer Specific Contracts**\n\n| Internal Method | Input Requirements | Output Guarantees | Side Effects |\n|----------------|-------------------|-------------------|---------------|\n| `_compute_cost()` | Features, targets, current parameters | Non-negative cost value | None |\n| `_compute_gradients()` | Features, targets, current parameters | Gradient vector/tuple | None |\n| `check_convergence()` | Cost history, parameters, gradients | Boolean convergence status + reason | History updates |\n\nThe gradient descent optimizer maintains additional internal contracts for the optimization loop. These methods are prefixed with underscore to indicate internal use, but they follow strict mathematical contracts that enable testing and debugging.\n\n#### Message Formats and Data Structures\n\nThe communication between components uses standardized data structures that act as **diplomatic messages** - each has a specific format that all parties understand:\n\n**Dataset Message Format**\n```\nDataset Structure:\n- features: np.ndarray (n_samples, n_features) - Core feature matrix\n- targets: np.ndarray (n_samples,) - Target values for supervised learning  \n- feature_names: List[str] - Human-readable feature identifiers\n- n_samples: int - Number of training examples\n- n_features: int - Dimensionality of feature space\n- is_normalized: bool - Whether features have been scaled\n- normalization_stats: Dict[str, np.ndarray] - Mean/std for consistent scaling\n```\n\nThe `Dataset` structure acts as the **universal data passport** that can cross component boundaries safely. Every component that receives a `Dataset` can trust its format and mathematical properties.\n\n**ModelParameters Message Format**\n```\nModelParameters Structure:\n- slope_: float - Single regression slope coefficient\n- intercept_: float - Bias term for all regression types  \n- weights_: np.ndarray - Multiple regression weight vector\n- is_fitted_: bool - Training completion status\n- fitting_method_: str - Algorithm used (\"closed_form\", \"gradient_descent\", \"ridge\")\n- regularization_strength_: float - L2 penalty coefficient\n- feature_count_: int - Expected input dimensionality\n```\n\nThe `ModelParameters` structure serves as the **credentials** that prove a model has been properly trained and specify exactly what kind of predictions it can make.\n\n**TrainingHistory Message Format**\n```\nTrainingHistory Structure:\n- cost_history_: List[float] - Optimization objective over time\n- parameter_history_: List[Tuple[float, float]] - Single regression parameter evolution\n- weight_history_: List[np.ndarray] - Multiple regression parameter evolution  \n- gradient_norms_: List[float] - Optimization progress indicators\n- converged_: bool - Whether optimization completed successfully\n- final_iteration_: int - Stopping iteration number\n- convergence_reason_: str - Specific termination condition met\n```\n\nThe `TrainingHistory` structure acts as the **optimization logbook** that records the complete learning process for debugging and analysis.\n\n#### Component Interaction Patterns\n\n**Producer-Consumer Pattern**: The DataHandler produces `Dataset` structures that model components consume during training. This pattern ensures data flows unidirectionally and prevents circular dependencies.\n\n**Command Pattern**: The `fit()` method represents a command that changes model state from unfitted to fitted. The command pattern ensures that training is an atomic operation - either it completes successfully with a valid model, or it fails completely without partial state.\n\n**Factory Pattern**: Different model classes (SimpleLinearRegression, GradientDescentRegression, MultipleLinearRegression) implement the same interface contract but create different internal implementations. This allows the system to be extended with new optimization algorithms without changing client code.\n\n**Observer Pattern**: The gradient descent optimization loop notifies the `TrainingHistory` observer of each iteration's results. This pattern enables comprehensive logging and debugging without cluttering the core optimization logic.\n\n#### Error Propagation and Recovery Protocols\n\nComponent communication includes structured error handling that preserves diagnostic information across component boundaries:\n\n**Data Validation Errors**: When DataHandler detects invalid input data, it raises specific exceptions with detailed context about what validation failed and suggested fixes. These errors should **fail fast** rather than allowing invalid data to corrupt downstream processing.\n\n**Numerical Stability Errors**: When optimization components detect numerical instability (overflow, underflow, divergence), they package diagnostic information about parameter values, gradient magnitudes, and convergence history before propagating the error upward.\n\n**Interface Contract Violations**: When a component receives input that violates its interface contract (wrong dimensions, unfitted model, etc.), it raises immediately with specific information about what was expected versus what was received.\n\n**Graceful Degradation**: In some cases, components can detect problems and recover gracefully - for example, if gradient descent fails to converge, it can still return the best parameters found so far along with a warning about convergence failure.\n\n#### Architecture Decision Records for Communication Design\n\n> **Decision: Immutable Data Structures**\n> - **Context**: Components need to share data safely without accidental modification\n> - **Options Considered**: Mutable shared state, deep copying, immutable structures\n> - **Decision**: Use immutable data structures with explicit copy operations when mutation needed\n> - **Rationale**: Prevents bugs from accidental state modification, makes data flow more predictable, enables safe parallel processing\n> - **Consequences**: Slight memory overhead from copying, but eliminates entire class of state-mutation bugs\n\n> **Decision: Explicit Interface Contracts**\n> - **Context**: Need to ensure components can interoperate reliably across different implementations\n> - **Options Considered**: Duck typing, formal interfaces, documentation-only contracts\n> - **Decision**: Formal interface definitions with runtime validation\n> - **Rationale**: Catches integration errors early, enables confident refactoring, provides clear contracts for testing\n> - **Consequences**: More verbose code, but much more reliable component integration\n\n> **Decision: Structured Error Information**\n> - **Context**: Machine learning systems can fail in complex ways that require detailed debugging\n> - **Options Considered**: Simple exception messages, error codes, structured diagnostic objects\n> - **Decision**: Structured exception objects with diagnostic context and suggested fixes\n> - **Rationale**: ML debugging often requires understanding numerical details, parameter states, and convergence history\n> - **Consequences**: More complex error handling code, but much better debugging experience\n\n![System Component Architecture](./diagrams/system-components.svg)\n\n![Training Process Sequence](./diagrams/training-sequence.svg)\n\n#### Common Pitfalls in Component Communication\n\n⚠️ **Pitfall: Preprocessing Inconsistency Between Training and Prediction**\nThe most common communication failure occurs when prediction data gets preprocessed differently than training data. For example, if training data was normalized but prediction data is not, or if different normalization statistics are used. This breaks the mathematical assumptions of the learned model parameters.\n\n**Detection**: Predictions will have wrong magnitude/scale, or you'll get numerical errors from matrix operations with mismatched dimensions.\n\n**Fix**: Always use the same `StandardScaler` instance for both training and prediction, and validate that preprocessing steps exactly match between training and prediction pipelines.\n\n⚠️ **Pitfall: State Mutation During Prediction**\nSome implementations accidentally modify model state during prediction (e.g., updating history logs, changing parameters). This breaks the prediction contract and can cause race conditions in concurrent environments.\n\n**Detection**: Model behavior changes between prediction calls, or predictions give different results for identical inputs.\n\n**Fix**: Ensure prediction methods are truly read-only - no modifications to any model attributes, no appending to history lists, no parameter updates.\n\n⚠️ **Pitfall: Interface Contract Violations Under Edge Cases**\nComponents often work correctly for normal inputs but violate interface contracts for edge cases like single-sample datasets, constant features, or extreme parameter values.\n\n**Detection**: Intermittent failures that are hard to reproduce, or failures that only occur with specific datasets.\n\n**Fix**: Implement comprehensive input validation in every public method, with specific checks for mathematical edge cases relevant to regression.\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component Layer | Simple Option | Advanced Option |\n|----------------|---------------|-----------------|\n| Data Flow Orchestration | Simple function calls with explicit sequencing | Pipeline classes with step validation and rollback |\n| Inter-Component Communication | Direct method calls with exception handling | Message passing with async queues and retry logic |\n| State Management | Object attributes with manual validation | State machines with transition guards and logging |\n| Error Propagation | Standard Python exceptions with context | Structured error objects with diagnostic payloads |\n| Data Serialization | JSON for simple cases, pickle for arrays | Protocol buffers or Apache Arrow for performance |\n| Logging and Monitoring | Python logging module with structured messages | Dedicated ML monitoring with metrics collection |\n\n#### Recommended File Structure for Communication Management\n\n```\nlinear_regression/\n├── core/\n│   ├── __init__.py\n│   ├── interfaces.py          ← Abstract base classes defining contracts\n│   ├── data_types.py          ← All data structures (Dataset, ModelParameters, etc.)\n│   └── exceptions.py          ← Structured exception classes\n├── pipeline/\n│   ├── __init__.py  \n│   ├── training_pipeline.py   ← Orchestrates training data flow\n│   ├── prediction_pipeline.py ← Orchestrates prediction data flow  \n│   └── validation.py          ← Cross-component validation logic\n├── components/\n│   ├── data_handler.py        ← Data loading and preprocessing\n│   ├── simple_regression.py   ← M1 implementation\n│   ├── gradient_descent.py    ← M2 implementation  \n│   └── multiple_regression.py ← M3 implementation\n└── utils/\n    ├── communication.py       ← Helper functions for component interaction\n    └── debugging.py           ← Diagnostic tools for data flow analysis\n```\n\n#### Core Data Structures Implementation\n\nComplete, ready-to-use data structures that define the communication contracts:\n\n```python\n\"\"\"\nCore data types for component communication.\n\"\"\"\nfrom dataclasses import dataclass, field\nfrom typing import List, Dict, Tuple, Optional, Union\nimport numpy as np\n\n@dataclass\nclass Dataset:\n    \"\"\"Universal data structure for component communication.\"\"\"\n    features: np.ndarray\n    targets: np.ndarray  \n    feature_names: List[str]\n    n_samples: int\n    n_features: int\n    is_normalized: bool = False\n    normalization_stats: Dict[str, np.ndarray] = field(default_factory=dict)\n    \n    def __post_init__(self):\n        \"\"\"Validate data consistency after initialization.\"\"\"\n        if self.features.shape[0] != self.targets.shape[0]:\n            raise ValueError(f\"Sample count mismatch: features {self.features.shape[0]}, targets {self.targets.shape[0]}\")\n        if self.features.shape[1] != self.n_features:\n            raise ValueError(f\"Feature count mismatch: array {self.features.shape[1]}, metadata {self.n_features}\")\n\n@dataclass  \nclass ModelParameters:\n    \"\"\"Trained model parameters with metadata.\"\"\"\n    slope_: float = 0.0\n    intercept_: float = 0.0\n    weights_: Optional[np.ndarray] = None\n    is_fitted_: bool = False\n    fitting_method_: str = \"\"\n    regularization_strength_: float = 0.0\n    feature_count_: int = 0\n    \n    def validate_for_prediction(self, n_features: int) -> None:\n        \"\"\"Ensure model is ready for prediction with given feature count.\"\"\"\n        if not self.is_fitted_:\n            raise ValueError(\"Model must be fitted before making predictions\")\n        if self.feature_count_ != n_features:\n            raise ValueError(f\"Feature count mismatch: trained {self.feature_count_}, input {n_features}\")\n\n@dataclass\nclass TrainingHistory:\n    \"\"\"Complete optimization history for analysis and debugging.\"\"\"\n    cost_history_: List[float] = field(default_factory=list)\n    parameter_history_: List[Tuple[float, float]] = field(default_factory=list) \n    weight_history_: List[np.ndarray] = field(default_factory=list)\n    gradient_norms_: List[float] = field(default_factory=list)\n    learning_rate: float = 0.01\n    max_iterations: int = 1000\n    tolerance: float = 1e-6\n    converged_: bool = False\n    final_iteration_: int = 0\n    convergence_reason_: str = \"\"\n\n@dataclass\nclass PredictionResult:\n    \"\"\"Complete prediction output with diagnostics.\"\"\"\n    predictions: np.ndarray\n    residuals: Optional[np.ndarray] = None\n    confidence_intervals: Optional[np.ndarray] = None  \n    r_squared: Optional[float] = None\n    mean_squared_error: Optional[float] = None\n    mean_absolute_error: Optional[float] = None\n    input_shape: Tuple[int, int] = (0, 0)\n    model_type: str = \"\"\n```\n\n#### Pipeline Orchestration Implementation\n\nComplete training and prediction pipeline orchestrators:\n\n```python\n\"\"\"\nPipeline orchestration for training and prediction workflows.\n\"\"\"\nfrom typing import Union, Optional\nimport numpy as np\nfrom .core.data_types import Dataset, ModelParameters, TrainingHistory, PredictionResult\nfrom .core.exceptions import ValidationError, NumericalInstabilityError\n\nclass TrainingPipeline:\n    \"\"\"Orchestrates the complete training data flow.\"\"\"\n    \n    def __init__(self, data_handler, model_component):\n        self.data_handler = data_handler\n        self.model = model_component\n        self.history = TrainingHistory()\n    \n    def execute_training_flow(self, data_source: Union[str, np.ndarray], \n                            feature_columns: Optional[List[str]] = None,\n                            target_column: Optional[str] = None) -> ModelParameters:\n        \"\"\"\n        Execute complete training pipeline from data source to fitted model.\n        \n        Returns fitted ModelParameters with complete training history.\n        Raises structured exceptions with diagnostic information.\n        \"\"\"\n        try:\n            # TODO 1: Load and validate raw data using data_handler.load_csv_data() or direct array input\n            # TODO 2: Apply preprocessing pipeline (normalization, design matrix construction) if needed\n            # TODO 3: Initialize model component with training configuration\n            # TODO 4: Execute model.fit() with preprocessed data\n            # TODO 5: Validate training results and extract final parameters \n            # TODO 6: Package results with complete training history\n            # TODO 7: Run post-training validation checks for numerical stability\n            pass\n            \n        except Exception as e:\n            # Package diagnostic information with the exception\n            self._add_diagnostic_context(e)\n            raise\n\nclass PredictionPipeline:\n    \"\"\"Orchestrates the complete prediction data flow.\"\"\"\n    \n    def __init__(self, model_parameters: ModelParameters, data_handler):\n        self.parameters = model_parameters\n        self.data_handler = data_handler\n        \n    def execute_prediction_flow(self, new_data: np.ndarray, \n                              ground_truth: Optional[np.ndarray] = None) -> PredictionResult:\n        \"\"\"\n        Execute complete prediction pipeline maintaining preprocessing consistency.\n        \n        Returns PredictionResult with predictions and diagnostic metrics.\n        \"\"\"\n        try:\n            # TODO 1: Validate input data format and dimensions against model requirements\n            # TODO 2: Apply identical preprocessing pipeline used during training\n            # TODO 3: Generate predictions using stored model parameters\n            # TODO 4: Compute confidence intervals and error estimates if possible\n            # TODO 5: Calculate performance metrics if ground truth provided\n            # TODO 6: Package results with comprehensive metadata\n            # TODO 7: Validate output for numerical stability and reasonable ranges\n            pass\n            \n        except Exception as e:\n            self._add_diagnostic_context(e, new_data)\n            raise\n```\n\n#### Component Interface Definitions\n\nAbstract base classes that define the communication contracts:\n\n```python\n\"\"\"\nAbstract interfaces defining component communication contracts.\n\"\"\"\nfrom abc import ABC, abstractmethod\nimport numpy as np\nfrom .data_types import Dataset, ModelParameters, PredictionResult\n\nclass DataHandlerInterface(ABC):\n    \"\"\"Contract for data loading and preprocessing components.\"\"\"\n    \n    @abstractmethod\n    def load_csv_data(self, filename: str, feature_columns: List[str], \n                     target_column: str) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Load data from CSV with validation.\"\"\"\n        pass\n        \n    @abstractmethod  \n    def validate_data(self, features: np.ndarray, targets: np.ndarray) -> None:\n        \"\"\"Validate data meets regression requirements.\"\"\"\n        pass\n        \n    @abstractmethod\n    def normalize_features(self, features: np.ndarray) -> Dataset:\n        \"\"\"Apply z-score normalization and return Dataset.\"\"\"\n        pass\n\nclass RegressionModelInterface(ABC):\n    \"\"\"Contract for all regression model implementations.\"\"\"\n    \n    @abstractmethod\n    def fit(self, x: np.ndarray, y: np.ndarray) -> 'RegressionModelInterface':\n        \"\"\"Train model on data, return self for chaining.\"\"\"\n        pass\n        \n    @abstractmethod\n    def predict(self, x: np.ndarray) -> np.ndarray:\n        \"\"\"Generate predictions for input data.\"\"\"\n        pass\n        \n    @abstractmethod  \n    def score(self, x: np.ndarray, y: np.ndarray) -> float:\n        \"\"\"Calculate R-squared coefficient of determination.\"\"\"\n        pass\n```\n\n#### Error Handling and Diagnostic Classes\n\nStructured exception classes with diagnostic information:\n\n```python\n\"\"\"\nStructured exceptions with ML-specific diagnostic information.\n\"\"\"\n\nclass MLException(Exception):\n    \"\"\"Base class for ML-specific exceptions with diagnostic context.\"\"\"\n    \n    def __init__(self, message: str, context: dict = None, suggested_fix: str = None):\n        super().__init__(message)\n        self.context = context or {}\n        self.suggested_fix = suggested_fix\n        \n    def add_context(self, **kwargs):\n        \"\"\"Add diagnostic information to the exception.\"\"\"\n        self.context.update(kwargs)\n        \n    def __str__(self):\n        base_msg = super().__str__()\n        if self.context:\n            context_str = \", \".join(f\"{k}={v}\" for k, v in self.context.items())\n            base_msg += f\" (Context: {context_str})\"\n        if self.suggested_fix:\n            base_msg += f\" (Suggested fix: {self.suggested_fix})\"\n        return base_msg\n\nclass ValidationError(MLException):\n    \"\"\"Data validation failed with specific diagnostic information.\"\"\"\n    pass\n\nclass NumericalInstabilityError(MLException):  \n    \"\"\"Numerical computation became unstable.\"\"\"\n    pass\n    \nclass ConvergenceError(MLException):\n    \"\"\"Optimization failed to converge within specified criteria.\"\"\"\n    pass\n\nclass InterfaceContractViolation(MLException):\n    \"\"\"Component interface contract was violated.\"\"\"  \n    pass\n```\n\n#### Milestone Checkpoints for Data Flow\n\n**Milestone 1 Checkpoint**: After implementing basic training and prediction flow:\n```bash\n# Test the complete pipeline with synthetic data\npython -m pytest tests/test_training_pipeline.py::test_simple_regression_flow -v\n\n# Expected behavior: \n# - Data loads successfully from CSV or array input\n# - Model trains using closed-form solution  \n# - Predictions match expected values within tolerance\n# - R-squared score is reasonable (> 0.8 for synthetic data with low noise)\n```\n\n**Milestone 2 Checkpoint**: After implementing gradient descent optimization flow:\n```bash  \n# Test iterative optimization pipeline\npython -m pytest tests/test_gradient_descent_pipeline.py::test_convergence_flow -v\n\n# Expected behavior:\n# - Cost decreases monotonically during training\n# - Gradient descent parameters match closed-form solution within tolerance\n# - Training history captures complete optimization trajectory\n# - Convergence detection works correctly for different tolerance settings\n```\n\n**Milestone 3 Checkpoint**: After implementing multiple regression flow:\n```bash\n# Test vectorized operations pipeline  \npython -m pytest tests/test_multiple_regression_pipeline.py::test_feature_scaling_flow -v\n\n# Expected behavior:\n# - Multiple features handled correctly through matrix operations\n# - Feature normalization applied consistently between training/prediction  \n# - Regularization affects parameter values as expected\n# - Performance scales reasonably with feature count\n```\n\n#### Debugging Data Flow Issues\n\n| Symptom | Likely Cause | Diagnostic Steps | Fix |\n|---------|--------------|------------------|-----|\n| Predictions have wrong scale/magnitude | Preprocessing inconsistency between train/predict | Check normalization stats, compare preprocessing steps | Use identical StandardScaler for both pipelines |\n| Matrix dimension errors during prediction | Design matrix construction differs from training | Verify intercept column handling, feature count | Ensure identical design matrix creation |  \n| Model state changes during prediction | Prediction method modifies model attributes | Add assertions for immutability, check history updates | Make prediction methods read-only |\n| Intermittent failures with edge cases | Interface validation missing for boundary conditions | Test with single samples, constant features, extreme values | Add comprehensive input validation |\n| Performance degrades with more features | Inefficient data copying or non-vectorized operations | Profile memory usage and computational complexity | Use in-place operations and proper vectorization |\n\n\n## Error Handling and Edge Cases\n\n> **Milestone(s):** All milestones (M1: Simple Linear Regression - handles basic data validation and division by zero, M2: Gradient Descent - adds optimization failure detection and learning rate issues, M3: Multiple Linear Regression - comprehensive matrix dimension validation and regularization edge cases)\n\nThis section establishes a comprehensive error handling strategy that transforms potential silent failures into informative feedback, enabling learners to debug their implementations effectively while understanding the numerical challenges inherent in machine learning algorithms.\n\n### Mental Model: The Safety Net System\n\nThink of error handling in linear regression like a multi-layered safety net system at a construction site. Just as construction workers need protection at different heights and stages of building, our machine learning system needs protection at different phases of data processing and model training.\n\nThe **first safety net** catches you early - this is input validation that prevents bad data from entering the system, like checking that workers have proper safety equipment before they even start climbing. The **second safety net** operates during computation - this is numerical stability monitoring that detects when mathematical operations are becoming dangerous, like sensors that warn when wind speeds make crane operation unsafe. The **third safety net** handles optimization failures - this is convergence monitoring that recognizes when the learning process itself is going wrong, like supervisors who notice when construction is proceeding in an unsafe direction and call for a strategy change.\n\nEach layer serves a specific purpose: fail early with clear messages when possible, gracefully handle recoverable issues during processing, and provide meaningful diagnostics when fundamental assumptions break down. This layered approach ensures that learners receive actionable feedback rather than mysterious crashes or silent incorrect results.\n\n### Numerical Stability Issues\n\nNumerical stability represents one of the most subtle but critical aspects of machine learning implementation. Unlike logical bugs that produce obvious errors, numerical instability often manifests as gradually degrading results, convergence to incorrect solutions, or sudden catastrophic failures after many successful iterations.\n\nThe **core challenge** stems from the inherent limitations of floating-point arithmetic when combined with the mathematical operations required for linear regression. Small rounding errors can accumulate over many gradient descent iterations, leading to parameter drift even when the algorithm should have converged. Division operations in gradient calculations can produce infinite or undefined results when denominators approach zero. Matrix operations with poorly conditioned data can amplify numerical errors exponentially.\n\nOur approach implements **proactive monitoring** rather than reactive error handling. Instead of waiting for NaN values to propagate through the system, we establish numerical bounds and stability checks at each critical computation point. This allows us to detect emerging instability early and either correct the issue automatically or fail gracefully with diagnostic information.\n\n#### Division by Zero Protection\n\nDivision by zero scenarios occur primarily in three contexts within linear regression: computing the slope in simple linear regression when all x-values are identical, calculating gradients when the feature variance is zero, and computing R-squared when the target variable has no variance.\n\nThe **slope calculation** in simple linear regression requires computing the covariance between features and targets divided by the variance of features. When all feature values are identical (zero variance), this division becomes undefined. Our protection strategy detects this condition by checking whether the feature variance falls below a numerical tolerance threshold, typically `TOLERANCE = 1e-10`. Rather than allowing the division to proceed and produce infinity or NaN, we immediately raise a descriptive error explaining that the feature lacks sufficient variation for linear regression.\n\n**Gradient calculations** face similar challenges when feature columns have zero or near-zero variance after normalization. During gradient descent, we compute partial derivatives that involve dividing by feature statistics. Our stability checker validates that all feature variances exceed the minimum threshold before beginning optimization. If any features are detected as constant, we provide specific guidance about removing these features or adding regularization.\n\n**R-squared computation** requires dividing the explained variance by the total variance of target values. When target values are constant (zero variance), this calculation becomes undefined. We handle this edge case by detecting constant targets during data validation and returning an R-squared value of NaN with an explanatory warning, since the concept of explained variance is meaningless when there is no variance to explain.\n\n| Protection Type | Detection Method | Threshold | Recovery Action | Error Message |\n|---|---|---|---|---|\n| Feature Variance | `np.var(features) < TOLERANCE` | 1e-10 | Immediate failure | \"Feature column {i} has zero variance. Remove constant features or add noise.\" |\n| Target Variance | `np.var(targets) < TOLERANCE` | 1e-10 | Set R² = NaN | \"Target variable is constant. R-squared is undefined.\" |\n| Gradient Division | `denominator < MIN_GRADIENT_NORM` | 1e-12 | Skip update | \"Gradient calculation unstable. Consider feature scaling.\" |\n| Matrix Condition | `np.linalg.cond(X) > 1e12` | 1e12 | Add regularization | \"Design matrix is ill-conditioned. Enable regularization.\" |\n\n#### Overflow and Underflow Detection\n\n**Overflow conditions** occur when parameter values or intermediate calculations exceed the representable range of floating-point numbers. In gradient descent, this typically happens when the learning rate is too large, causing parameter updates that grow exponentially until they reach infinity. We establish parameter bounds of `MAX_PARAMETER_VALUE = 1e8` as a reasonable upper limit for most regression problems. When any parameter exceeds this threshold, we detect potential overflow and reduce the learning rate automatically or terminate with guidance about hyperparameter adjustment.\n\n**Underflow problems** manifest when gradients become so small that they fall below machine precision, effectively stalling the optimization process. We monitor gradient norms using `MIN_GRADIENT_NORM = 1e-12` as the lower threshold. When gradient norms consistently fall below this value but the cost function has not converged, we detect potential underflow and suggest increasing the learning rate or checking for numerical precision issues.\n\n**Intermediate calculation protection** monitors the results of matrix operations, especially during multiple linear regression where matrix multiplication can amplify existing numerical errors. We validate that cost function values remain finite and that matrix operations produce well-defined results. When we detect NaN or infinite values in intermediate calculations, we immediately halt processing and provide diagnostic information about which operation failed.\n\n#### Precision Loss Prevention\n\n**Accumulated rounding errors** represent a subtle but persistent threat to gradient descent convergence. Small floating-point errors in each parameter update can accumulate over hundreds of iterations, eventually causing the optimization to drift away from the true minimum. Our prevention strategy includes monitoring the relative change in parameters between iterations and detecting when updates become smaller than machine precision.\n\n**Feature scaling impact** on numerical stability cannot be overstated. When features have vastly different scales (e.g., age in years vs. income in dollars), the condition number of the design matrix degrades dramatically, leading to numerical instability in both closed-form and iterative solutions. We enforce feature normalization for multiple linear regression and provide warnings when feature scales differ by more than several orders of magnitude.\n\n**Matrix conditioning** becomes critical in multiple linear regression, where we must solve systems involving the design matrix. We compute the condition number using `np.linalg.cond()` and warn when it exceeds reasonable thresholds. Poorly conditioned matrices indicate that small changes in input data can produce large changes in the solution, suggesting the need for regularization or feature reduction.\n\n> **Architecture Decision: Fail-Fast vs. Graceful Degradation**\n> - **Context**: When numerical instability is detected, we must decide whether to immediately terminate with an error or attempt to continue with degraded precision\n> - **Options Considered**: \n>   1. Always fail immediately when any numerical issue is detected\n>   2. Attempt automatic recovery (reduce learning rate, add regularization) \n>   3. Continue with warnings and hope for eventual recovery\n> - **Decision**: Hybrid approach - fail fast for fundamental issues (zero variance), attempt recovery for optimization issues (learning rate adjustment)\n> - **Rationale**: Educational value comes from understanding when problems are fundamental vs. solvable, while preventing silent failures that teach incorrect lessons\n> - **Consequences**: More complex error handling logic, but learners develop intuition about which numerical issues are recoverable vs. fundamental design problems\n\n### Input Validation and Sanitization\n\nInput validation serves as the first line of defense against data-related errors, transforming potentially cryptic runtime failures into immediate, actionable feedback. Rather than allowing invalid data to propagate through the system and cause mysterious failures during matrix operations or optimization, we implement comprehensive validation that catches problems at the point of data ingestion.\n\n#### Array Shape and Type Validation\n\n**Array dimensionality checking** ensures that feature and target arrays have compatible shapes before any mathematical operations begin. The fundamental requirement for regression is that the number of samples in features matches the number of samples in targets. We validate this using `features.shape[0] == targets.shape[0]` and provide specific error messages when mismatches occur, including the actual dimensions detected to help learners diagnose the source of the problem.\n\n**Feature matrix structure** validation becomes particularly important in multiple linear regression, where we expect a 2D array with samples as rows and features as columns. We check that feature arrays have exactly two dimensions and that the number of features is reasonable relative to the number of samples. A common guideline is maintaining at least 10 samples per feature to avoid overfitting, which we enforce through `MIN_RATIO = 10` samples per feature minimum.\n\n**Data type consistency** checking ensures that all input arrays contain numeric data compatible with floating-point operations. We validate that arrays contain numeric dtypes (int, float) rather than strings or objects, and we automatically promote integer arrays to float64 to prevent precision loss during calculations. This prevents runtime errors during mathematical operations and ensures consistent precision across all computations.\n\n| Validation Type | Check Performed | Error Condition | Recovery Action | Error Message |\n|---|---|---|---|---|\n| Shape Compatibility | `features.shape[0] == targets.shape[0]` | Mismatch | Immediate failure | \"Feature samples ({f}) != target samples ({t})\" |\n| Feature Dimensions | `len(features.shape) == 2` | Wrong dimensions | Reshape or fail | \"Features must be 2D array, got {d}D\" |\n| Sample Sufficiency | `n_samples >= MIN_RATIO * n_features` | Too few samples | Warning + continue | \"Only {r:.1f} samples per feature. Consider reducing features.\" |\n| Data Types | `np.issubdtype(arr.dtype, np.number)` | Non-numeric | Attempt conversion | \"Array contains non-numeric data: {dtype}\" |\n\n#### Missing Value and Outlier Detection\n\n**Missing value identification** scans input arrays for NaN, None, or other indicators of missing data. Linear regression algorithms cannot handle missing values directly, so we must detect and address them before training begins. We use `np.isnan()` and `np.isinf()` to identify problematic values and provide specific counts and locations of missing data to help learners understand the scope of the data quality issue.\n\n**Outlier detection** helps identify data points that may disproportionately influence the regression line, especially important for educational purposes where learners need to understand how linear regression responds to extreme values. We implement simple outlier detection using the interquartile range (IQR) method, flagging data points that fall more than 1.5 * IQR beyond the quartiles. While we don't automatically remove outliers, we provide warnings and statistics about their potential impact.\n\n**Range validation** checks whether feature and target values fall within reasonable ranges for the problem domain. Extremely large or small values can cause numerical instability even if they're technically valid. We validate that all values fall within reasonable bounds (e.g., between -1e6 and 1e6) and warn when values appear to be on scales that might cause numerical issues.\n\n#### Data Quality Metrics\n\n**Statistical validation** computes basic descriptive statistics for features and targets to identify potential data quality issues before they cause training problems. We calculate means, standard deviations, minimum and maximum values, and check for degenerate cases where these statistics indicate problematic data distributions.\n\n**Correlation analysis** for multiple regression identifies perfect or near-perfect correlations between features, which can cause numerical instability in matrix operations. We compute the correlation matrix and warn when any feature pairs have correlations above 0.95, suggesting multicollinearity issues that may require feature selection or regularization.\n\n**Distribution checks** identify severely skewed or abnormal distributions that may benefit from transformation before regression analysis. While linear regression doesn't assume normality of features, severely skewed data can impact numerical stability and interpretation of results. We provide warnings when feature distributions are extremely skewed (skewness > 3) or have unusual characteristics.\n\n> **Architecture Decision: Validation Strictness Level**\n> - **Context**: Determining how strict input validation should be - whether to reject data with minor issues or proceed with warnings\n> - **Options Considered**:\n>   1. Strict validation that rejects any data quality issues\n>   2. Permissive validation that proceeds with warnings\n>   3. Configurable validation levels based on user preference\n> - **Decision**: Moderate strictness with clear warnings - fail for fundamental incompatibilities, warn for quality issues that may affect results\n> - **Rationale**: Educational goal is to teach learners to recognize data quality issues while still allowing experimentation with imperfect data\n> - **Consequences**: More complex validation logic with multiple severity levels, but learners develop better intuition about real-world data challenges\n\n### Optimization Failure Recovery\n\nOptimization failure recovery addresses the fundamental challenge that gradient descent, unlike closed-form solutions, can fail to converge or converge to incorrect solutions. These failures often occur gradually over many iterations, making them difficult to detect without systematic monitoring. Our recovery strategy focuses on early detection of optimization problems and providing specific guidance for resolution.\n\n#### Divergence Detection\n\n**Parameter explosion** represents the most dramatic form of optimization failure, where parameters grow without bound due to excessive learning rates or numerical instability. We monitor parameter values at each iteration and trigger divergence detection when any parameter exceeds `MAX_PARAMETER_VALUE = 1e8`. When divergence is detected, we immediately halt optimization and provide specific guidance about reducing the learning rate or checking for data scaling issues.\n\n**Cost function monitoring** tracks the mean squared error at each iteration to detect both divergence (increasing cost) and stagnation (unchanging cost despite continued updates). We maintain a history of recent cost values and compute the trend over the last several iterations. Divergence is indicated by consistently increasing cost over multiple consecutive iterations, while stagnation is indicated by cost changes below the convergence tolerance without meeting other convergence criteria.\n\n**Gradient explosion detection** identifies cases where gradient magnitudes become extremely large, indicating numerical instability in the optimization process. We compute the L2 norm of the gradient vector at each iteration and trigger alerts when it exceeds `MAX_GRADIENT_NORM = 1e8`. Gradient explosion often precedes parameter explosion and can be addressed by reducing the learning rate or improving feature scaling.\n\n| Failure Mode | Detection Criteria | Immediate Action | Recovery Strategy | Diagnostic Message |\n|---|---|---|---|---|\n| Parameter Explosion | `max(abs(parameters)) > MAX_PARAMETER_VALUE` | Halt optimization | Reduce learning rate by 10x | \"Parameters diverging. Try learning_rate = {new_rate}\" |\n| Cost Divergence | `cost[t] > 1.1 * cost[t-5]` for 3+ iterations | Halt optimization | Reset and reduce learning rate | \"Cost increasing. Check data scaling and learning rate.\" |\n| Gradient Explosion | `np.linalg.norm(gradient) > MAX_GRADIENT_NORM` | Halt optimization | Feature scaling + rate reduction | \"Gradient explosion detected. Normalize features and reduce learning rate.\" |\n| Stagnation | `abs(cost[t] - cost[t-10]) < TOLERANCE` without convergence | Continue with warning | Increase learning rate or check tolerance | \"Cost plateaued without convergence. Consider different learning rate.\" |\n\n#### Learning Rate Adaptation\n\n**Automatic learning rate adjustment** provides a safety mechanism when the initial learning rate proves inappropriate for the optimization landscape. Rather than requiring manual hyperparameter tuning, we implement simple adaptive strategies that can rescue failing optimizations. When divergence is detected, we automatically reduce the learning rate by a factor of 10 and restart optimization from the initial parameters.\n\n**Learning rate bounds** establish reasonable limits for automatic adjustment to prevent the system from choosing learning rates that are either too large (causing continued divergence) or too small (causing extremely slow convergence). We maintain bounds between `1e-6` (minimum practical learning rate) and `1.0` (maximum stable learning rate for most problems).\n\n**Convergence rate monitoring** tracks the speed at which the optimization approaches convergence to detect learning rates that are too conservative. When the cost function decreases very slowly over many iterations (less than 1% improvement over 100 iterations), we suggest increasing the learning rate to accelerate convergence while maintaining stability.\n\n#### Convergence Criteria Refinement\n\n**Multi-criteria convergence detection** improves upon simple cost-based stopping conditions by incorporating multiple indicators of optimization completion. We monitor cost improvement, parameter stability, and gradient magnitude simultaneously, requiring satisfaction of convergence criteria across multiple metrics before declaring success. This prevents premature stopping when one metric appears stable while others indicate continued optimization potential.\n\n**Adaptive tolerance adjustment** recognizes that appropriate convergence tolerances depend on the scale and characteristics of the specific dataset. Rather than using fixed tolerances, we compute relative tolerances based on the initial cost function value and parameter magnitudes. This ensures that convergence detection works appropriately across datasets with different scales and characteristics.\n\n**Convergence confidence assessment** provides probabilistic estimates of convergence quality rather than binary success/failure indicators. We track the consistency of convergence signals over multiple recent iterations and provide confidence scores that help learners understand whether optimization terminated due to strong convergence evidence or marginal satisfaction of stopping criteria.\n\nThe `ConvergenceDetector` maintains comprehensive state about the optimization process:\n\n| State Field | Type | Purpose | Update Frequency |\n|---|---|---|---|\n| `cost_history` | List[float] | Tracks cost function values for trend analysis | Every iteration |\n| `parameter_history` | List[np.ndarray] | Stores recent parameter vectors for stability checking | Every iteration |\n| `gradient_history` | List[np.ndarray] | Maintains gradient magnitudes for explosion detection | Every iteration |\n| `consecutive_count` | int | Counts consecutive iterations meeting convergence criteria | Every iteration |\n| `converged` | bool | Indicates whether convergence has been achieved | When criteria met |\n| `convergence_reason` | str | Explains which criteria triggered convergence | When convergence detected |\n\n#### Recovery Strategy Implementation\n\n**Automatic restart mechanisms** allow the optimization to recover from temporary numerical issues without requiring manual intervention. When certain types of failures are detected (particularly learning rate issues), we automatically reset parameters to their initial values and restart optimization with adjusted hyperparameters. This provides a seamless experience for learners while demonstrating the iterative nature of hyperparameter tuning.\n\n**Progressive fallback strategies** implement a hierarchy of recovery attempts when initial optimization fails. We first attempt learning rate reduction, then feature scaling adjustments, and finally recommend regularization if numerical instability persists. Each fallback level provides more aggressive intervention while maintaining the educational goal of understanding why different approaches are necessary.\n\n**Diagnostic information collection** ensures that when optimization ultimately fails despite recovery attempts, learners receive comprehensive information about what was attempted and why it failed. We maintain detailed logs of all recovery actions taken, parameter adjustments made, and the specific failure modes encountered, enabling learners to understand both the automatic recovery process and the underlying mathematical challenges.\n\n> **Architecture Decision: Automatic vs. Manual Recovery**\n> - **Context**: When optimization failures are detected, we must decide whether to attempt automatic recovery or require manual intervention\n> - **Options Considered**:\n>   1. Fully automatic recovery with minimal user visibility\n>   2. Manual recovery requiring explicit user action for each failure\n>   3. Automatic recovery with detailed logging and user override options\n> - **Decision**: Automatic recovery for common issues (learning rate adjustment) with detailed logging, manual intervention required for fundamental problems (data quality)\n> - **Rationale**: Educational value comes from understanding what recovery strategies work and why, while avoiding tedious manual tuning for well-understood issues\n> - **Consequences**: More complex error handling logic, but learners see both automatic recovery in action and receive guidance for manual intervention when necessary\n\n⚠️ **Pitfall: Silent Numerical Failures**\nMany numerical issues in machine learning manifest as gradually degrading performance rather than obvious crashes. For example, when the learning rate is slightly too high, the optimization may appear to converge normally but settle on a suboptimal solution. Our monitoring systems are designed to catch these subtle failures by tracking multiple convergence indicators simultaneously. Always check that your final cost is reasonable relative to the data scale, not just that the optimization \"finished.\"\n\n⚠️ **Pitfall: Ignoring Data Scale Effects**\nA common mistake is implementing numerical stability checks with fixed thresholds that don't account for the natural scale of the data. For instance, checking for parameter values above 1e8 may be appropriate for normalized data but too restrictive for raw financial data where values naturally reach millions. Our validation systems compute relative thresholds based on data characteristics rather than using absolute fixed values.\n\n⚠️ **Pitfall: Over-Aggressive Error Handling**\nWhile comprehensive error checking is important, overly strict validation can prevent learners from experimenting with edge cases that might actually work. For example, automatically rejecting datasets with fewer than 10 samples per feature prevents exploration of regularization techniques that can handle such scenarios. Our approach balances protection against clear numerical issues with allowance for educational experimentation.\n\n### Implementation Guidance\n\nThis subsection provides concrete implementation strategies for the error handling and validation systems described above, organized to support learners at each milestone of the project.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|---|---|---|\n| Validation Framework | Manual checks with numpy functions | Custom validation classes with inheritance |\n| Error Messages | String formatting with problem description | Structured error objects with codes and context |\n| Numerical Monitoring | Basic threshold checking in training loops | Comprehensive monitoring with state machines |\n| Recovery Strategies | Fixed learning rate reduction | Adaptive hyperparameter adjustment algorithms |\n| Logging | Print statements with error details | Structured logging with multiple severity levels |\n\n#### Recommended File Structure\n\nThe error handling system integrates throughout the project structure rather than being isolated in a single module:\n\n```\nproject-root/\n  src/\n    validation/\n      __init__.py              ← validation module exports\n      data_validator.py        ← DataValidator class implementation\n      numerical_validator.py   ← NumericalStabilityChecker class\n      convergence_detector.py  ← ConvergenceDetector class implementation\n    errors/\n      __init__.py              ← custom exception definitions\n      regression_errors.py     ← RegressionError hierarchy\n    models/\n      simple_regression.py     ← integrates validation in fit() method\n      gradient_descent.py      ← integrates numerical monitoring\n      multiple_regression.py   ← comprehensive validation integration\n    utils/\n      monitoring.py            ← TrainingMonitor helper class\n      recovery.py              ← OptimizationRecovery strategies\n  tests/\n    test_validation.py         ← validation system tests\n    test_error_handling.py     ← error scenario tests\n    test_edge_cases.py         ← boundary condition tests\n```\n\n#### Infrastructure Starter Code\n\n**Custom Exception Hierarchy:**\n```python\n\"\"\"\nCustom exception classes for linear regression implementation.\nProvides structured error handling with specific error types and recovery guidance.\n\"\"\"\n\nclass RegressionError(Exception):\n    \"\"\"Base exception for all linear regression errors.\"\"\"\n    \n    def __init__(self, message, error_code=None, suggested_fix=None, context=None):\n        super().__init__(message)\n        self.error_code = error_code\n        self.suggested_fix = suggested_fix\n        self.context = context or {}\n    \n    def __str__(self):\n        base_msg = super().__str__()\n        if self.suggested_fix:\n            return f\"{base_msg}\\nSuggested fix: {self.suggested_fix}\"\n        return base_msg\n\nclass DataValidationError(RegressionError):\n    \"\"\"Raised when input data fails validation checks.\"\"\"\n    pass\n\nclass NumericalInstabilityError(RegressionError):\n    \"\"\"Raised when numerical computations become unstable.\"\"\"\n    pass\n\nclass ConvergenceError(RegressionError):\n    \"\"\"Raised when optimization fails to converge properly.\"\"\"\n    pass\n\nclass MatrixConditionError(RegressionError):\n    \"\"\"Raised when matrix operations encounter conditioning problems.\"\"\"\n    pass\n\n# Usage example for learners:\n# try:\n#     model.fit(X, y)\n# except DataValidationError as e:\n#     print(f\"Data problem: {e}\")\n#     print(f\"Try this: {e.suggested_fix}\")\n#     print(f\"Context: {e.context}\")\n```\n\n**Numerical Constants and Thresholds:**\n```python\n\"\"\"\nNumerical constants for stability checking and validation.\nThese values are chosen based on typical floating-point precision limits\nand practical experience with regression problems.\n\"\"\"\n\n# Fundamental numerical limits\nTOLERANCE = 1e-10                    # General numerical comparison threshold\nMACHINE_EPSILON = np.finfo(float).eps  # Machine precision for floating-point\n\n# Parameter bounds for stability\nMAX_PARAMETER_VALUE = 1e8           # Overflow detection threshold\nMIN_PARAMETER_CHANGE = 1e-12        # Underflow detection for parameter updates\n\n# Gradient monitoring thresholds  \nMIN_GRADIENT_NORM = 1e-12           # Gradient underflow threshold\nMAX_GRADIENT_NORM = 1e8             # Gradient explosion threshold\n\n# Matrix conditioning limits\nMAX_CONDITION_NUMBER = 1e12         # Matrix conditioning threshold\nMIN_VARIANCE_THRESHOLD = 1e-10      # Zero variance detection\n\n# Optimization parameters\nDEFAULT_LEARNING_RATE = 0.01        # Safe starting learning rate\nMAX_LEARNING_RATE = 1.0             # Upper bound for automatic adjustment\nMIN_LEARNING_RATE = 1e-6            # Lower bound for automatic adjustment\nMAX_ITERATIONS = 1000               # Default iteration limit\n\n# Data quality thresholds\nMIN_SAMPLES_PER_FEATURE = 10        # Minimum sample to feature ratio\nMAX_OUTLIER_ZSCORE = 3.0            # Outlier detection threshold\nMAX_CORRELATION_THRESHOLD = 0.95     # Multicollinearity detection\n```\n\n#### Core Logic Skeleton Code\n\n**Comprehensive Data Validation Class:**\n```python\nclass DataValidator:\n    \"\"\"Comprehensive validation for regression input data.\"\"\"\n    \n    def __init__(self, tolerance=TOLERANCE):\n        self.tolerance = tolerance\n        self.validation_results = {}\n    \n    def validate_regression_inputs(self, features, targets):\n        \"\"\"\n        Perform comprehensive validation of features and targets for regression.\n        \n        Args:\n            features: Feature array (n_samples, n_features) or (n_samples,) for simple regression\n            targets: Target array (n_samples,)\n            \n        Raises:\n            DataValidationError: When validation fails with specific guidance\n        \"\"\"\n        # TODO 1: Validate array types and convert to numpy arrays if needed\n        # TODO 2: Check array dimensions and reshape if necessary for compatibility\n        # TODO 3: Validate sample count compatibility between features and targets\n        # TODO 4: Check for missing values (NaN, inf) and provide locations\n        # TODO 5: Validate data types are numeric and convertible to float64\n        # TODO 6: Check for sufficient samples relative to features (avoid overfitting)\n        # TODO 7: Detect zero-variance features that will cause division by zero\n        # TODO 8: Check for extreme outliers that may cause numerical instability\n        # TODO 9: Store validation statistics for later use in training\n        # TODO 10: Return validation summary with warnings and recommendations\n        pass\n    \n    def check_matrix_conditioning(self, X):\n        \"\"\"\n        Check design matrix conditioning for numerical stability.\n        \n        Args:\n            X: Design matrix with intercept column\n            \n        Returns:\n            Tuple[bool, str]: (is_stable, diagnostic_message)\n        \"\"\"\n        # TODO 1: Compute condition number using np.linalg.cond()\n        # TODO 2: Compare against MAX_CONDITION_NUMBER threshold\n        # TODO 3: Check for perfect correlations between features\n        # TODO 4: Identify specific problematic feature combinations\n        # TODO 5: Generate specific recommendations (regularization, feature removal)\n        pass\n```\n\n**Numerical Stability Monitoring:**\n```python\nclass NumericalStabilityChecker:\n    \"\"\"Real-time monitoring of numerical stability during optimization.\"\"\"\n    \n    def __init__(self):\n        self.parameter_history = []\n        self.gradient_history = []\n        self.cost_history = []\n        self.stability_warnings = []\n    \n    def check_numerical_stability(self, parameters, gradients, cost, iteration):\n        \"\"\"\n        Comprehensive stability check for current optimization state.\n        \n        Args:\n            parameters: Current parameter values (slope, intercept) or weights vector\n            gradients: Current gradient values  \n            cost: Current cost function value\n            iteration: Current iteration number\n            \n        Returns:\n            Tuple[bool, str, str]: (is_stable, warning_message, suggested_action)\n        \"\"\"\n        # TODO 1: Check for parameter overflow (values exceeding MAX_PARAMETER_VALUE)\n        # TODO 2: Detect gradient explosion (norm exceeding MAX_GRADIENT_NORM)\n        # TODO 3: Check for gradient underflow (norm below MIN_GRADIENT_NORM)\n        # TODO 4: Validate cost function value is finite and reasonable\n        # TODO 5: Detect cost function divergence (increasing trend over recent iterations)\n        # TODO 6: Check for parameter stagnation (no meaningful updates)\n        # TODO 7: Analyze parameter update magnitudes relative to current values\n        # TODO 8: Update internal history for trend analysis\n        # TODO 9: Generate specific diagnostic messages for detected issues\n        # TODO 10: Recommend specific recovery actions (learning rate, scaling, etc.)\n        pass\n    \n    def suggest_learning_rate_adjustment(self, current_lr, problem_type):\n        \"\"\"\n        Suggest learning rate adjustments based on detected stability issues.\n        \n        Args:\n            current_lr: Current learning rate\n            problem_type: Type of stability issue detected\n            \n        Returns:\n            Tuple[float, str]: (suggested_lr, explanation)\n        \"\"\"\n        # TODO 1: Analyze problem type (divergence, stagnation, explosion)\n        # TODO 2: Compute appropriate adjustment factor based on problem severity\n        # TODO 3: Ensure suggested rate stays within MIN_LEARNING_RATE to MAX_LEARNING_RATE bounds\n        # TODO 4: Generate explanation of why this adjustment should help\n        # TODO 5: Include guidance about monitoring the adjustment effectiveness\n        pass\n```\n\n**Advanced Convergence Detection:**\n```python\nclass ConvergenceDetector:\n    \"\"\"Multi-criteria convergence detection with adaptive thresholds.\"\"\"\n    \n    def __init__(self, config=None):\n        self.config = config or ConvergenceConfig()\n        self.cost_history = []\n        self.parameter_history = []\n        self.gradient_history = []\n        self.consecutive_count = 0\n        self.converged = False\n        self.convergence_reason = \"\"\n    \n    def check_convergence(self, cost, parameters, gradients, iteration):\n        \"\"\"\n        Multi-criteria convergence detection with detailed analysis.\n        \n        Args:\n            cost: Current cost function value\n            parameters: Current parameter vector\n            gradients: Current gradient vector  \n            iteration: Current iteration number\n            \n        Returns:\n            Tuple[bool, str]: (has_converged, detailed_reason)\n        \"\"\"\n        # TODO 1: Update internal histories with current values\n        # TODO 2: Check cost improvement criterion (absolute and relative)\n        # TODO 3: Check parameter stability criterion (small parameter changes)\n        # TODO 4: Check gradient magnitude criterion (approaching zero)\n        # TODO 5: Require multiple consecutive iterations meeting criteria\n        # TODO 6: Detect false convergence (premature stopping)\n        # TODO 7: Analyze convergence quality and confidence\n        # TODO 8: Generate detailed explanation of convergence decision\n        # TODO 9: Update convergence state and reason\n        # TODO 10: Return convergence status with full diagnostic information\n        pass\n    \n    def analyze_convergence_quality(self):\n        \"\"\"\n        Assess the quality and confidence of detected convergence.\n        \n        Returns:\n            Dict: Convergence quality metrics and confidence assessment\n        \"\"\"\n        # TODO 1: Compute convergence confidence score based on multiple criteria\n        # TODO 2: Analyze final gradient norm relative to problem scale\n        # TODO 3: Check cost function stability over recent iterations\n        # TODO 4: Assess parameter stability and reasonable final values\n        # TODO 5: Generate convergence quality report with recommendations\n        pass\n```\n\n#### Milestone Checkpoints\n\n**Milestone 1 Checkpoint - Basic Validation:**\nAfter implementing basic data validation for simple linear regression:\n\n```bash\n# Test basic validation with synthetic data\npython -c \"\nfrom src.validation.data_validator import DataValidator\nfrom src.models.simple_regression import SimpleLinearRegression\nimport numpy as np\n\n# Test normal case\nX = np.array([1, 2, 3, 4, 5])\ny = np.array([2, 4, 6, 8, 10])\nmodel = SimpleLinearRegression()\nmodel.fit(X, y)  # Should work without errors\nprint('✓ Normal case passes')\n\n# Test zero variance detection  \nX_const = np.array([3, 3, 3, 3, 3])\ntry:\n    model.fit(X_const, y)\n    print('✗ Should have detected zero variance')\nexcept Exception as e:\n    print(f'✓ Zero variance detected: {e}')\n\n# Test dimension mismatch\nX_wrong = np.array([1, 2, 3])\ntry:\n    model.fit(X_wrong, y)\n    print('✗ Should have detected dimension mismatch')\nexcept Exception as e:\n    print(f'✓ Dimension mismatch detected: {e}')\n\"\n```\n\nExpected output shows successful validation of normal data and appropriate error detection for problematic cases.\n\n**Milestone 2 Checkpoint - Gradient Descent Stability:**\nAfter implementing numerical stability monitoring for gradient descent:\n\n```bash\n# Test stability monitoring with different learning rates\npython -c \"\nfrom src.models.gradient_descent import GradientDescentRegression\nimport numpy as np\n\n# Generate test data\nnp.random.seed(42)\nX = np.random.randn(100)\ny = 2 * X + 1 + 0.1 * np.random.randn(100)\n\n# Test stable learning rate\nmodel_stable = GradientDescentRegression(learning_rate=0.01)\nmodel_stable.fit(X, y)\nprint(f'✓ Stable training converged: {model_stable.converged_}')\n\n# Test unstable learning rate (should detect divergence)\nmodel_unstable = GradientDescentRegression(learning_rate=10.0)\ntry:\n    model_unstable.fit(X, y)\n    if not model_unstable.converged_:\n        print('✓ Divergence detected correctly')\n    else:\n        print('✗ Should have detected divergence')\nexcept Exception as e:\n    print(f'✓ Divergence caught: {type(e).__name__}')\n\"\n```\n\nThis checkpoint verifies that stability monitoring correctly identifies problematic learning rates and provides appropriate error handling.\n\n**Milestone 3 Checkpoint - Multiple Regression Validation:**\nAfter implementing comprehensive validation for multiple linear regression:\n\n```bash\n# Test matrix conditioning and feature scaling validation\npython -c \"\nfrom src.models.multiple_regression import MultipleLinearRegression\nimport numpy as np\n\n# Generate well-conditioned data\nnp.random.seed(42)\nX = np.random.randn(100, 3)\ny = X @ [2, -1, 0.5] + 0.1 * np.random.randn(100)\n\nmodel = MultipleLinearRegression()\nmodel.fit(X, y)\nprint(f'✓ Well-conditioned case: R² = {model.score(X, y):.3f}')\n\n# Test poorly conditioned data (perfect correlation)\nX_bad = np.random.randn(100, 3)\nX_bad[:, 2] = X_bad[:, 1]  # Perfect correlation\ny_bad = X_bad[:, :2] @ [2, -1] + 0.1 * np.random.randn(100)\n\ntry:\n    model_bad = MultipleLinearRegression()\n    model_bad.fit(X_bad, y_bad)\n    print('⚠ Should warn about conditioning but may still work')\nexcept Exception as e:\n    print(f'✓ Conditioning problem detected: {type(e).__name__}')\n\n# Test insufficient samples\nX_few = np.random.randn(5, 10)  # 5 samples, 10 features\ny_few = np.random.randn(5)\ntry:\n    model_few = MultipleLinearRegression()\n    model_few.fit(X_few, y_few)\n    print('⚠ Should warn about overfitting risk')\nexcept Exception as e:\n    print(f'✓ Insufficient data detected: {type(e).__name__}')\n\"\n```\n\nThis checkpoint tests the full validation system including matrix conditioning, multicollinearity detection, and sample sufficiency checking.\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---|---|---|---|\n| `np.linalg.LinAlgError: SVD did not converge` | Poorly conditioned design matrix | Check `np.linalg.cond(X)` and feature correlations | Add regularization or remove correlated features |\n| Parameters become NaN after few iterations | Learning rate too high or numerical overflow | Print parameter values each iteration, check gradients | Reduce learning rate by 10x, ensure feature scaling |\n| Cost function oscillates wildly | Learning rate too high for optimization landscape | Plot cost history, check gradient magnitudes | Reduce learning rate, verify feature normalization |\n| Optimization never converges (hits max iterations) | Learning rate too small or poor convergence criteria | Check cost improvement rate, examine gradient norms | Increase learning rate or relax convergence tolerance |\n| \"Singular matrix\" error in closed-form solution | Perfect correlation between features or zero variance | Check feature correlation matrix and variances | Remove constant features, check for identical columns |\n| Silent incorrect results (converges to wrong answer) | Numerical precision loss or inappropriate scaling | Compare with closed-form solution, check condition number | Implement feature scaling, use higher precision |\n| Memory usage grows unexpectedly during training | Storing too much history in convergence detection | Monitor history list sizes, check memory profiling | Limit history length, use fixed-size circular buffers |\n| Training extremely slow despite simple data | Inefficient matrix operations or excessive validation | Profile code execution, time individual operations | Vectorize operations, reduce validation frequency |\n\n\n## Testing Strategy and Milestone Checkpoints\n\n> **Milestone(s):** All milestones (M1: Simple Linear Regression - validates closed-form solutions and basic prediction, M2: Gradient Descent - tests iterative optimization and convergence, M3: Multiple Linear Regression - verifies matrix operations and regularization)\n\nThis section defines a comprehensive testing strategy that serves both educational and verification purposes. Unlike production testing that focuses primarily on correctness and performance, our testing approach emphasizes learning validation—ensuring that each implementation milestone demonstrates understanding of the underlying mathematical concepts while building confidence through measurable success criteria.\n\n### Mental Model: The Mathematical Proof Checker\n\nThink of our testing strategy as a mathematical proof checker that validates not just the final answer, but every step of reasoning along the way. Just as a mathematics professor doesn't simply look at whether a student got the right answer to a calculus problem, but examines the work to ensure they understand derivatives, limits, and algebraic manipulation, our testing framework validates both computational correctness and conceptual understanding.\n\nEach test serves as a checkpoint that answers the question: \"Does this implementation demonstrate mastery of the underlying mathematical concept?\" For the closed-form solution in Milestone 1, we verify that the learner understands the normal equation. For gradient descent in Milestone 2, we confirm they grasp optimization principles and convergence detection. For multiple regression in Milestone 3, we ensure they comprehend matrix operations and regularization effects.\n\nThis approach builds confidence progressively—each passing test provides concrete evidence that the learner has mastered a specific concept and is ready to tackle the next level of complexity.\n\n## Unit Testing Approach\n\nThe unit testing strategy treats each component as an isolated mathematical function with precisely defined input-output relationships. We test individual components using carefully constructed synthetic datasets where we know the exact expected outcomes, allowing us to validate both numerical accuracy and algorithmic correctness.\n\n### Synthetic Data Generation Strategy\n\nOur synthetic data generation forms the foundation of predictable, repeatable unit tests. Rather than using random data that might mask subtle bugs through statistical averaging, we construct datasets with known mathematical properties that expose specific edge cases and validate precise algorithmic behavior.\n\n| Test Data Type | Purpose | Construction Method | Expected Properties |\n|---|---|---|---|\n| Perfect Linear Data | Validate noise-free fitting | `y = 2.5 * x + 1.7` exactly | R² = 1.0, residuals = 0 |\n| Controlled Noise Data | Test robustness to realistic conditions | Add Gaussian noise σ=0.1 | R² > 0.95, predictable MSE |\n| Edge Case Data | Expose numerical stability issues | Single point, constant x, large values | Graceful error handling |\n| Multi-collinear Data | Test multiple regression challenges | Features with exact linear dependence | Proper error detection |\n| Standardization Test Data | Validate feature scaling | Mean=100, std=50 known values | Normalized mean=0, std=1 |\n\nThe synthetic data generator provides complete control over every aspect of the dataset, enabling us to test specific mathematical properties in isolation:\n\n```python\ndef generate_perfect_linear_data(n_samples=100, slope=2.5, intercept=1.7):\n    \"\"\"Creates data that perfectly follows y = slope * x + intercept\"\"\"\n    x = np.linspace(-10, 10, n_samples)\n    y = slope * x + intercept  # No noise - perfect relationship\n    return x.reshape(-1, 1), y\n```\n\n### Component-Level Test Structure\n\nEach component receives a dedicated test suite that validates both its mathematical correctness and its adherence to the defined interface contracts. The test structure mirrors the component architecture, ensuring complete coverage of all public methods and critical internal calculations.\n\n**SimpleLinearRegression Unit Tests:**\n\n| Test Method | Validates | Input Data | Expected Output |\n|---|---|---|---|\n| `test_perfect_fit_closed_form` | Normal equation accuracy | Perfect linear data | slope_=2.5, intercept_=1.7, R²=1.0 |\n| `test_prediction_accuracy` | Parameter application | New x values | Exact y = mx + b calculation |\n| `test_score_calculation` | R-squared computation | Known residuals | Precise coefficient of determination |\n| `test_empty_data_handling` | Input validation | Empty arrays | `DataValidationError` with clear message |\n| `test_single_point_edge_case` | Mathematical edge case | One data point | Undefined slope error |\n| `test_constant_x_values` | Division by zero protection | All x identical | `NumericalInstabilityError` |\n\n**GradientDescentRegression Unit Tests:**\n\n| Test Method | Validates | Input Data | Expected Outcome |\n|---|---|---|---|\n| `test_convergence_perfect_data` | Optimization accuracy | Perfect linear relationship | Converged parameters ≈ true values |\n| `test_cost_function_calculation` | MSE computation | Known residuals | Exact mean squared error |\n| `test_gradient_computation` | Partial derivatives | Analytical comparison | Gradients match calculus |\n| `test_learning_rate_sensitivity` | Parameter update behavior | Various learning rates | Stable vs. divergent trajectories |\n| `test_convergence_detection` | Stopping criteria | Controlled cost sequence | Proper termination timing |\n| `test_cost_history_tracking` | Training monitoring | Multi-iteration fit | Complete cost trajectory |\n\n**MultipleLinearRegression Unit Tests:**\n\n| Test Method | Validates | Input Data | Expected Result |\n|---|---|---|---|\n| `test_design_matrix_construction` | Matrix formulation | Multiple features | Proper intercept column addition |\n| `test_vectorized_gradient_computation` | Batch operations | Multi-dimensional data | Correct gradient vector |\n| `test_regularization_effect` | Ridge penalty | High regularization | Reduced parameter magnitudes |\n| `test_feature_scaling_integration` | Normalization compatibility | Different feature scales | Consistent convergence |\n| `test_matrix_dimension_validation` | Shape compatibility | Mismatched dimensions | Clear error messages |\n\n### Mathematical Verification Approach\n\nBeyond functional correctness, our unit tests verify mathematical properties that demonstrate conceptual understanding. Each test validates specific mathematical principles rather than just computational outputs.\n\n**Closed-Form Solution Verification:**\nWe validate that the implemented normal equation produces results identical to manual matrix calculations. For the simple case, we verify:\n- The slope calculation: `slope = Σ[(xi - x̄)(yi - ȳ)] / Σ[(xi - x̄)²]`\n- The intercept calculation: `intercept = ȳ - slope * x̄`\n- The R-squared computation: `R² = 1 - SSres/SStot`\n\n**Gradient Descent Mathematical Consistency:**\nWe test that gradient descent converges to the same solution as the closed-form approach on identical data, within numerical tolerance. This validates both the gradient calculation and the parameter update rule.\n\n**Regularization Effect Verification:**\nFor Ridge regression, we confirm that increasing `regularization_strength` progressively shrinks parameter magnitudes while maintaining reasonable fit quality, demonstrating understanding of the bias-variance tradeoff.\n\n> **Key Insight**: Mathematical verification tests serve dual purposes—they catch implementation bugs while confirming that the learner understands the underlying mathematical relationships. A test that verifies gradient computation teaches the learner about partial derivatives as much as it validates code correctness.\n\n### Error Handling Validation\n\nUnit tests extensively validate error handling behavior, ensuring that the system fails gracefully with informative messages rather than producing silent incorrect results or cryptic crashes.\n\n| Error Condition | Test Validation | Expected Behavior |\n|---|---|---|\n| Null/empty input arrays | Pass None or empty `np.ndarray` | `DataValidationError` with descriptive message |\n| Mismatched array lengths | Different feature/target sizes | `DataValidationError` specifying length mismatch |\n| Non-numeric data | String or boolean values | `DataValidationError` indicating data type issue |\n| Infinite/NaN values | Inject `np.inf` or `np.nan` | `NumericalInstabilityError` with location info |\n| Singular matrix conditions | Perfectly correlated features | `NumericalInstabilityError` suggesting solutions |\n| Gradient descent divergence | Excessive learning rate | `ConvergenceError` with learning rate adjustment advice |\n\n### Common Unit Testing Pitfalls\n\n⚠️ **Pitfall: Testing with Random Data Only**\nMany learners write tests that use `np.random.rand()` to generate test data, making tests non-deterministic and hiding edge cases. Random data can mask bugs through statistical averaging—a slightly incorrect gradient calculation might still produce reasonable results on most random datasets.\n\n*Fix*: Use deterministic synthetic data with known mathematical properties. Set `np.random.seed()` when randomness is necessary, and test specific edge cases with carefully constructed data.\n\n⚠️ **Pitfall: Insufficient Numerical Precision Testing**\nTesting for exact equality (`assert result == 2.5`) fails with floating-point arithmetic. Conversely, using overly loose tolerances (`assert abs(result - 2.5) < 0.1`) can miss significant numerical errors.\n\n*Fix*: Use appropriate tolerances based on the mathematical operation. For closed-form solutions on perfect data, use tight tolerances (`1e-10`). For iterative algorithms, use tolerances related to convergence criteria (`1e-6`).\n\n⚠️ **Pitfall: Not Testing the Learning Process**\nFocusing only on final results misses important learning validation. Testing that gradient descent eventually reaches the right answer doesn't confirm understanding of the optimization process.\n\n*Fix*: Test intermediate states—verify that the cost function decreases monotonically, that gradients shrink as convergence approaches, and that parameter updates follow expected patterns.\n\n## Integration Testing\n\nIntegration testing validates the complete end-to-end system behavior using realistic datasets and workflows. Unlike unit tests that isolate individual components, integration tests verify that components collaborate correctly and that the entire pipeline produces mathematically sound results on real-world data.\n\n### Real Dataset Integration\n\nWe use carefully selected real datasets that exhibit different characteristics and challenges, allowing learners to validate their implementation against data with genuine complexity while maintaining interpretable results.\n\n| Dataset | Source | Characteristics | Integration Test Focus |\n|---|---|---|---|\n| Boston Housing (subset) | Scikit-learn | 506 samples, 13 features | Multiple regression with mixed feature types |\n| California Housing (sample) | Scikit-learn | Geographic data, large feature ranges | Feature normalization necessity |\n| Synthetic Polynomial | Generated | Known polynomial relationship | Basis function extension testing |\n| Small Business Revenue | Custom CSV | Simple business metrics | CSV loading and preprocessing validation |\n\n**Boston Housing Integration Test:**\nThis test validates the complete multiple regression pipeline on a well-understood dataset where published results provide benchmarks for comparison. The test loads the data using `DataHandler`, applies feature normalization, trains both closed-form and gradient descent models, and compares their convergence behavior.\n\n**California Housing Feature Scaling Test:**\nThis integration test deliberately uses the California housing dataset without preprocessing to demonstrate the necessity of feature normalization. The test shows how gradient descent fails to converge on raw data (where longitude ranges from -124 to -114 while median income ranges from 0.5 to 15) but succeeds after applying `StandardScaler`.\n\n**Custom CSV Integration Test:**\nUsing a small, interpretable business dataset (e.g., advertising spend vs. revenue), this test validates the complete data loading pipeline. Learners can manually verify results and understand the business interpretation of coefficients.\n\n### Cross-Validation Integration\n\nIntegration testing includes cross-validation workflows that demonstrate model generalization and help learners understand overfitting concepts. We implement simple train-test splits rather than full k-fold cross-validation to maintain educational focus.\n\n| Validation Approach | Dataset Split | Metrics Computed | Learning Objective |\n|---|---|---|---|\n| Simple Train-Test Split | 80% train, 20% test | Train R², Test R², Gap Analysis | Overfitting detection |\n| Temporal Split | First 70% chronological | Time-based prediction validation | Real-world relevance |\n| Feature Subset Validation | Incremental feature addition | Performance vs. complexity | Feature selection intuition |\n\nThe cross-validation integration tests reveal important machine learning principles:\n- **Generalization Gap**: Comparing training and testing R-squared scores helps learners understand when models memorize rather than learn patterns\n- **Feature Selection Impact**: Testing subsets of features demonstrates how additional predictors can improve or hurt generalization\n- **Regularization Benefits**: Ridge regression typically shows smaller generalization gaps than unregularized models\n\n### Algorithm Comparison Integration\n\nA critical integration test compares the closed-form solution with gradient descent on identical datasets, validating that both approaches converge to the same mathematical solution while demonstrating their different computational characteristics.\n\n| Comparison Aspect | Closed-Form Result | Gradient Descent Result | Expected Relationship |\n|---|---|---|---|\n| Final parameters | Direct calculation | Converged parameters | Within numerical tolerance |\n| Computational time | Instant (matrix ops) | Multiple iterations | Gradient descent slower |\n| Memory usage | Full matrix inversion | Incremental updates | Depends on dataset size |\n| Numerical stability | Matrix conditioning dependent | Learning rate dependent | Different failure modes |\n\nThis comparison teaches learners when to choose each approach:\n- **Small datasets**: Closed-form solution preferred for speed and exactness\n- **Large datasets**: Gradient descent scales better with massive feature counts\n- **Online learning**: Only gradient descent supports incremental updates\n- **Numerical challenges**: Each approach has different stability characteristics\n\n### Pipeline Robustness Testing\n\nIntegration tests validate system behavior under realistic adverse conditions that commonly occur in real-world data science workflows.\n\n| Robustness Test | Adverse Condition | Expected System Response | Learning Value |\n|---|---|---|---|\n| Missing value handling | CSV with empty cells | Graceful error with row identification | Data quality awareness |\n| Mixed data types | Numeric and string columns | Clear type conversion guidance | Data preprocessing necessity |\n| Extreme feature scales | Features differing by 10^6 magnitude | Convergence failure without normalization | Feature scaling importance |\n| Near-singular matrices | Highly correlated features | Numerical instability warning | Multicollinearity understanding |\n| Large datasets | 10,000+ samples | Memory and performance monitoring | Computational complexity awareness |\n\n### End-to-End Workflow Validation\n\nThe most comprehensive integration test validates the complete workflow that a learner would follow in practice: data loading, preprocessing, model training, evaluation, and prediction on new data.\n\n**Complete Workflow Test Sequence:**\n1. Load dataset from CSV using `load_csv_data()`\n2. Validate data using `validate_regression_inputs()`\n3. Split into training and testing sets\n4. Initialize and fit `SimpleLinearRegression` model\n5. Compare with `GradientDescentRegression` on same data\n6. Scale features and apply `MultipleLinearRegression`\n7. Generate predictions on test set\n8. Compute comprehensive evaluation metrics\n9. Visualize results and residuals\n\nThis workflow integration test serves as the final validation that all components work together harmoniously and that the learner can successfully apply their implementation to solve real regression problems.\n\n### Common Integration Testing Pitfalls\n\n⚠️ **Pitfall: Using Only \"Clean\" Academic Datasets**\nMany learners test only on preprocessed datasets like those from scikit-learn, missing the reality of messy real-world data. This creates false confidence when their implementation encounters actual CSV files with missing values, mixed data types, or formatting issues.\n\n*Fix*: Include integration tests with deliberately messy data—CSV files with missing values, inconsistent formatting, mixed data types, and extra columns. Test the complete data cleaning pipeline.\n\n⚠️ **Pitfall: Ignoring Feature Scale Effects**\nTesting primarily on normalized datasets masks the critical importance of feature scaling. Learners may not realize their gradient descent implementation fails on real data with natural feature scales.\n\n*Fix*: Include explicit tests that demonstrate gradient descent failure on unnormalized data followed by success after feature scaling. Make the failure and recovery visible and educational.\n\n⚠️ **Pitfall: Inadequate Cross-Validation Understanding**\nSimply splitting data and comparing metrics doesn't teach the deeper lesson about overfitting and generalization. Many learners miss the connection between model complexity and generalization gap.\n\n*Fix*: Design cross-validation tests that explicitly demonstrate overfitting scenarios. Show how increasing polynomial degree initially improves training performance but eventually hurts test performance.\n\n## Milestone Success Checkpoints\n\nEach milestone defines specific, measurable success criteria that serve both as learning validation and implementation checkpoints. These criteria ensure progressive mastery while building confidence through demonstrable achievements.\n\n### Milestone 1: Simple Linear Regression Success Criteria\n\nMilestone 1 establishes the foundation of linear regression understanding through closed-form solution implementation. Success validation focuses on mathematical correctness and basic prediction capability.\n\n**Mathematical Accuracy Checkpoints:**\n\n| Checkpoint | Test Data | Success Criteria | Validates Understanding |\n|---|---|---|---|\n| Perfect Fit Validation | `y = 3x + 2` (no noise) | slope_ = 3.0 ± 1e-10, intercept_ = 2.0 ± 1e-10 | Normal equation implementation |\n| R-squared Calculation | Known residual pattern | R² = 0.9876 (specific value) | Coefficient of determination concept |\n| Prediction Accuracy | New x values [1, 2, 3] | Predictions [5, 8, 11] exactly | Parameter application |\n| Boston Housing Subset | First 50 samples, single feature | R² > 0.4, reasonable coefficients | Real data applicability |\n\n**Interface Contract Validation:**\n\nThe `SimpleLinearRegression` class must satisfy all interface requirements consistently:\n\n| Method | Pre-fit Behavior | Post-fit Behavior | Error Conditions |\n|---|---|---|---|\n| `fit(x, y)` | Sets is_fitted_=False | Sets is_fitted_=True, returns self | Validates input arrays |\n| `predict(x)` | Raises `RegressionError` | Returns predictions array | Validates x dimensionality |\n| `score(x, y)` | Raises `RegressionError` | Returns R-squared float | Validates fitted state |\n| Attribute access | slope_=None, intercept_=None | Computed values available | Consistent state management |\n\n**Verification Commands:**\n```bash\n# Run Milestone 1 test suite\npython -m pytest tests/test_simple_linear_regression.py -v\n\n# Manual verification script\npython scripts/milestone1_verification.py\n\n# Expected output:\n# ✓ Perfect linear fit: R² = 1.000\n# ✓ Slope accuracy: 2.500 (expected: 2.500)\n# ✓ Intercept accuracy: 1.700 (expected: 1.700)\n# ✓ Prediction validation passed\n# ✓ Boston housing test: R² = 0.423\n```\n\n**Milestone 1 Behavioral Checkpoints:**\n\nBeyond mathematical accuracy, learners must demonstrate understanding of the underlying concepts:\n\n1. **Normal Equation Understanding**: Can explain why the closed-form solution works and its mathematical derivation\n2. **Least Squares Principle**: Understands that linear regression minimizes sum of squared residuals\n3. **R-squared Interpretation**: Can explain what R² = 0.7 means in practical terms\n4. **Prediction Process**: Understands that prediction applies fitted parameters to new data\n5. **Limitation Recognition**: Recognizes when closed-form solution breaks (singular matrices, perfect collinearity)\n\n### Milestone 2: Gradient Descent Success Criteria\n\nMilestone 2 introduces iterative optimization, requiring demonstration of convergence understanding and algorithmic implementation skills.\n\n**Convergence Validation Checkpoints:**\n\n| Checkpoint | Test Configuration | Success Criteria | Demonstrates |\n|---|---|---|---|\n| Basic Convergence | Perfect linear data, lr=0.01 | Converged within 100 iterations | Algorithm correctness |\n| Cost Reduction | Monitor cost_history_ | Monotonic decrease to < 1e-6 | Optimization understanding |\n| Parameter Accuracy | Compare to closed-form solution | Within 1e-4 of analytical solution | Mathematical consistency |\n| Learning Rate Sensitivity | Test lr=[0.001, 0.01, 0.1, 1.0] | Stable convergence for appropriate rates | Hyperparameter awareness |\n| Convergence Detection | Controlled cost plateau | Stops within tolerance window | Termination criteria |\n\n**Gradient Descent Algorithm Validation:**\n\nThe core optimization loop must demonstrate correct implementation of gradient descent principles:\n\n| Algorithm Component | Test Method | Expected Behavior | Mathematical Validation |\n|---|---|---|---|\n| Cost function | Known residuals | Exact MSE calculation | `cost = Σ(yi - ŷi)² / (2n)` |\n| Gradient calculation | Analytical comparison | Match partial derivatives | `∂J/∂m = Σ(ŷi - yi)xi / n` |\n| Parameter updates | Single iteration | Correct update formula | `θ := θ - α∇J(θ)` |\n| Convergence check | Multiple criteria | Combined tolerance evaluation | Cost, gradient, parameter changes |\n\n**Training History Validation:**\n\nThe `TrainingHistory` object must capture complete optimization trajectory:\n\n| History Component | Required Content | Validation Method | Educational Value |\n|---|---|---|---|\n| cost_history_ | Cost at each iteration | Monotonic decrease check | Optimization visualization |\n| parameter_history_ | Parameters at each step | Convergence trajectory | Parameter evolution understanding |\n| converged_ | Boolean convergence flag | Matches stopping criteria | Termination understanding |\n| final_iteration_ | Iteration count | Reasonable convergence speed | Algorithm efficiency |\n| convergence_reason_ | Descriptive string | Matches actual stopping condition | Debugging capability |\n\n**Verification Commands:**\n```bash\n# Run Milestone 2 test suite  \npython -m pytest tests/test_gradient_descent.py -v\n\n# Convergence visualization\npython scripts/milestone2_convergence_analysis.py\n\n# Expected output:\n# ✓ Convergence achieved in 87 iterations\n# ✓ Final cost: 0.000003 (tolerance: 0.000001)\n# ✓ Parameters match closed-form solution\n# ✓ Cost history shows monotonic decrease\n# ✓ All learning rates 0.001-0.1 converged\n```\n\n### Milestone 3: Multiple Linear Regression Success Criteria\n\nMilestone 3 represents the culmination of linear regression mastery, requiring matrix operation competency and regularization understanding.\n\n**Matrix Operation Validation:**\n\n| Matrix Component | Test Data | Success Criteria | Validates |\n|---|---|---|---|\n| Design matrix construction | 3 features + intercept | Shape (n, 4), first column all 1s | Matrix formulation understanding |\n| Vectorized gradient computation | Multiple features | Gradient shape (n_features + 1,) | Batch operation implementation |\n| Parameter vector | Multiple regression fit | weights_ shape matches features | Weight vector concept |\n| Prediction matrix multiplication | Test feature matrix | Correct matrix-vector product | Linear algebra application |\n\n**Multiple Regression Mathematical Validation:**\n\nThe implementation must demonstrate correct handling of multi-dimensional optimization:\n\n| Mathematical Concept | Test Approach | Expected Result | Conceptual Understanding |\n|---|---|---|---|\n| Hyperplane fitting | 3D visualization data | Reasonable plane coefficients | Multi-dimensional relationships |\n| Feature interaction | Orthogonal vs. correlated features | Different convergence behavior | Feature independence effects |\n| Regularization impact | Ridge with λ=[0, 0.1, 1.0, 10.0] | Progressive coefficient shrinkage | Bias-variance tradeoff |\n| Feature scaling necessity | Mixed feature scales | Convergence failure without scaling | Preprocessing importance |\n\n**Regularization Effectiveness Validation:**\n\nRidge regression must demonstrate proper L2 penalty implementation:\n\n| Regularization Strength | Expected Behavior | Test Method | Learning Outcome |\n|---|---|---|---|\n| λ = 0.0 | Identical to unregularized | Parameter comparison | Regularization baseline |\n| λ = 0.1 | Slight coefficient shrinkage | Magnitude reduction check | Gentle penalty effect |\n| λ = 1.0 | Moderate shrinkage | Reduced overfitting evidence | Practical regularization |\n| λ = 10.0 | Strong shrinkage | Near-zero coefficients | Over-regularization recognition |\n\n**Feature Engineering Validation:**\n\nThe system must properly handle feature preprocessing and transformation:\n\n| Feature Processing | Test Scenario | Success Criteria | Teaches |\n|---|---|---|---|\n| Standardization | Different feature scales | Mean ≈ 0, std ≈ 1 for all features | Z-score normalization |\n| Design matrix | Manual verification | Correct intercept column placement | Matrix structure |\n| New data scaling | Apply fitted scaler | Consistent transformation | Preprocessing consistency |\n| Missing value detection | Incomplete feature matrix | Clear error with location info | Data quality validation |\n\n**Verification Commands:**\n```bash\n# Run comprehensive Milestone 3 test suite\npython -m pytest tests/test_multiple_regression.py -v\n\n# Full system integration test\npython scripts/milestone3_full_integration.py\n\n# Expected output:\n# ✓ Design matrix construction: Shape (506, 14)\n# ✓ Vectorized gradients: Shape (14,) \n# ✓ Ridge regularization: λ=0.1 reduces overfitting\n# ✓ Feature scaling: All features normalized\n# ✓ Cross-validation: Test R² = 0.64, Train R² = 0.67\n# ✓ Algorithm comparison: Closed-form vs. GD within tolerance\n```\n\n### Progressive Learning Validation\n\nThe milestone checkpoints build understanding progressively, with each milestone requiring mastery of previous concepts while introducing new complexity layers.\n\n**Concept Mastery Progression:**\n\n| Milestone | Core Concepts | Required Understanding | Builds Foundation For |\n|---|---|---|---|\n| M1: Simple Regression | Linear relationships, least squares | Mathematical optimization principles | Iterative optimization |\n| M2: Gradient Descent | Iterative optimization, convergence | Algorithm design and tuning | Multi-dimensional optimization |\n| M3: Multiple Regression | Matrix operations, regularization | Production ML considerations | Advanced ML algorithms |\n\n**Implementation Skill Development:**\n\nEach milestone develops specific technical competencies that prepare learners for advanced machine learning topics:\n\n1. **Milestone 1**: NumPy array manipulation, basic linear algebra, interface design\n2. **Milestone 2**: Algorithm implementation, convergence monitoring, hyperparameter sensitivity  \n3. **Milestone 3**: Matrix operations, feature engineering, performance optimization\n\n### Common Milestone Checkpoint Pitfalls\n\n⚠️ **Pitfall: Focusing Only on Final Metrics**\nMany learners consider a milestone complete when final R-squared scores look reasonable, missing the deeper learning validation. Getting the right answer without understanding the process defeats the educational purpose.\n\n*Fix*: Include process validation in checkpoints. Test intermediate states, verify gradient calculations manually, and require explanation of why each step works.\n\n⚠️ **Pitfall: Inadequate Edge Case Testing**  \nMilestone checkpoints often test only the \"happy path\" with clean data and good convergence, missing the robust error handling that characterizes professional implementations.\n\n*Fix*: Include edge case validation in every milestone checkpoint. Test division by zero, convergence failure, and malformed input data. Require graceful degradation.\n\n⚠️ **Pitfall: Insufficient Cross-Milestone Integration**\nTesting each milestone in isolation misses the important integration between concepts. Learners may understand gradient descent separately from feature scaling but fail to combine them effectively.\n\n*Fix*: Include integration checkpoints that require combining concepts from multiple milestones. Test the complete workflow end-to-end at each stage.\n\n![Learning Milestone Progression](./diagrams/milestone-progression.svg)\n\n> **Key Insight**: Milestone checkpoints serve dual purposes—they validate implementation correctness while confirming conceptual understanding. A learner who passes all checkpoints has demonstrated not just coding competency, but mastery of linear regression fundamentals that will transfer to advanced machine learning concepts.\n\nThe testing strategy and milestone checkpoints transform abstract learning goals into concrete, measurable achievements. Each checkpoint provides clear evidence of progress while building the foundation for subsequent learning. This structured approach ensures that learners develop both theoretical understanding and practical implementation skills necessary for advanced machine learning study.\n\n### Implementation Guidance\n\nThis section provides the practical implementation support needed to execute the testing strategy effectively, bridging the gap between testing theory and actual code validation.\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|---|---|---|\n| Testing Framework | Python `unittest` (built-in) | `pytest` with fixtures |\n| Assertion Library | Basic `assert` statements | NumPy testing (`np.testing.assert_allclose`) |\n| Test Data Generation | Manual array creation | Property-based testing with `hypothesis` |\n| Visualization | `matplotlib` for simple plots | `seaborn` + `plotly` for interactive analysis |\n| Performance Monitoring | Basic timing with `time.time()` | `pytest-benchmark` for statistical timing |\n| Coverage Analysis | Visual inspection | `pytest-cov` for coverage reports |\n\n**Recommended Testing File Structure:**\n```\nproject-root/\n  src/\n    data_handler.py\n    simple_regression.py  \n    gradient_descent.py\n    multiple_regression.py\n  tests/\n    conftest.py                    ← pytest fixtures and test data\n    test_data_handler.py          ← unit tests for data loading/preprocessing\n    test_simple_regression.py     ← milestone 1 validation tests\n    test_gradient_descent.py      ← milestone 2 convergence tests\n    test_multiple_regression.py   ← milestone 3 matrix operation tests\n    test_integration.py           ← end-to-end integration tests\n  test_data/\n    synthetic_linear.csv          ← known linear relationship\n    boston_housing_subset.csv     ← real data sample\n    messy_data.csv               ← missing values, mixed types\n  scripts/\n    milestone1_verification.py    ← M1 checkpoint validation\n    milestone2_convergence.py     ← M2 optimization analysis\n    milestone3_integration.py     ← M3 full system validation\n```\n\n**Complete Test Infrastructure (Starter Code):**\n\n```python\n# tests/conftest.py - Shared test fixtures and utilities\nimport pytest\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\n@pytest.fixture\ndef perfect_linear_data():\n    \"\"\"Perfect linear relationship for exact testing\"\"\"\n    np.random.seed(42)\n    x = np.linspace(-5, 5, 50)\n    y = 2.5 * x + 1.7  # Known slope and intercept\n    return x.reshape(-1, 1), y\n\n@pytest.fixture  \ndef noisy_linear_data():\n    \"\"\"Linear relationship with controlled noise\"\"\"\n    np.random.seed(42)\n    x = np.linspace(-5, 5, 100)\n    y = 2.5 * x + 1.7 + np.random.normal(0, 0.1, 100)\n    return x.reshape(-1, 1), y\n\n@pytest.fixture\ndef multiple_features_data():\n    \"\"\"Multiple regression test data\"\"\"\n    np.random.seed(42)\n    n_samples, n_features = 200, 3\n    X = np.random.randn(n_samples, n_features)\n    true_weights = np.array([1.5, -2.0, 0.8])\n    y = X @ true_weights + 0.5 + np.random.normal(0, 0.1, n_samples)\n    return X, y, true_weights\n\n@pytest.fixture\ndef edge_case_data():\n    \"\"\"Edge cases that should trigger errors\"\"\"\n    return {\n        'empty_arrays': (np.array([]), np.array([])),\n        'mismatched_lengths': (np.array([1, 2, 3]), np.array([1, 2])),\n        'single_point': (np.array([1]), np.array([2])),\n        'constant_x': (np.array([5, 5, 5, 5]), np.array([1, 2, 3, 4])),\n        'nan_values': (np.array([1, 2, np.nan]), np.array([1, 2, 3])),\n        'infinite_values': (np.array([1, 2, np.inf]), np.array([1, 2, 3]))\n    }\n\ndef assert_regression_parameters_close(actual_slope, actual_intercept, \n                                     expected_slope, expected_intercept, \n                                     tolerance=1e-6):\n    \"\"\"Helper for parameter validation\"\"\"\n    np.testing.assert_allclose(actual_slope, expected_slope, atol=tolerance,\n                              err_msg=f\"Slope mismatch: {actual_slope} vs {expected_slope}\")\n    np.testing.assert_allclose(actual_intercept, expected_intercept, atol=tolerance,\n                              err_msg=f\"Intercept mismatch: {actual_intercept} vs {expected_intercept}\")\n\ndef validate_convergence_history(cost_history, tolerance=1e-6):\n    \"\"\"Validate gradient descent convergence properties\"\"\"\n    assert len(cost_history) > 0, \"Cost history should not be empty\"\n    \n    # Check monotonic decrease (allowing for numerical noise)\n    for i in range(1, len(cost_history)):\n        assert cost_history[i] <= cost_history[i-1] + 1e-12, \\\n            f\"Cost increased at iteration {i}: {cost_history[i]} > {cost_history[i-1]}\"\n    \n    # Check final convergence\n    final_cost = cost_history[-1]\n    assert final_cost < tolerance, f\"Final cost {final_cost} exceeds tolerance {tolerance}\"\n    \n    return True\n```\n\n**Core Testing Skeletons (TODOs for learner implementation):**\n\n```python\n# tests/test_simple_regression.py - Milestone 1 validation\nimport pytest\nimport numpy as np\nfrom src.simple_regression import SimpleLinearRegression\n\nclass TestSimpleLinearRegression:\n    \n    def test_perfect_fit_accuracy(self, perfect_linear_data):\n        \"\"\"Validate closed-form solution on perfect data\"\"\"\n        X, y = perfect_linear_data\n        model = SimpleLinearRegression()\n        \n        # TODO 1: Call model.fit(X, y) and verify it returns self\n        # TODO 2: Check that model.is_fitted_ is True after fitting\n        # TODO 3: Verify model.slope_ equals 2.5 within tolerance 1e-10\n        # TODO 4: Verify model.intercept_ equals 1.7 within tolerance 1e-10\n        # TODO 5: Check model.score(X, y) returns 1.0 (perfect fit)\n        # Hint: Use assert_regression_parameters_close from conftest.py\n        \n    def test_prediction_accuracy(self, perfect_linear_data):\n        \"\"\"Validate prediction calculation\"\"\"\n        X, y = perfect_linear_data\n        model = SimpleLinearRegression()\n        model.fit(X, y)\n        \n        # TODO 1: Create test points X_new = [[0], [1], [2]]  \n        # TODO 2: Call model.predict(X_new) to get predictions\n        # TODO 3: Calculate expected predictions: y = 2.5 * x + 1.7\n        # TODO 4: Verify predictions match expected within tolerance\n        # Hint: For x=0, expect y=1.7; for x=1, expect y=4.2; etc.\n        \n    def test_error_handling(self, edge_case_data):\n        \"\"\"Validate proper error handling\"\"\"\n        model = SimpleLinearRegression()\n        \n        # TODO 1: Test prediction before fitting raises RegressionError\n        # TODO 2: Test empty arrays raise DataValidationError  \n        # TODO 3: Test mismatched array lengths raise DataValidationError\n        # TODO 4: Test constant x values raise NumericalInstabilityError\n        # TODO 5: Verify error messages are descriptive and helpful\n        # Hint: Use pytest.raises(ExceptionType) context manager\n```\n\n```python\n# tests/test_gradient_descent.py - Milestone 2 validation  \nimport pytest\nimport numpy as np\nfrom src.gradient_descent import GradientDescentRegression\n\nclass TestGradientDescentOptimization:\n    \n    def test_convergence_perfect_data(self, perfect_linear_data):\n        \"\"\"Validate gradient descent convergence\"\"\"\n        X, y = perfect_linear_data\n        model = GradientDescentRegression(learning_rate=0.01, max_iterations=1000, tolerance=1e-6)\n        \n        # TODO 1: Fit model and verify convergence (model.converged_ is True)\n        # TODO 2: Check final parameters match true values (2.5, 1.7) within 1e-4\n        # TODO 3: Verify cost_history_ shows monotonic decrease\n        # TODO 4: Check final cost is below tolerance\n        # TODO 5: Verify convergence_reason_ is descriptive\n        # Hint: Use validate_convergence_history from conftest.py\n        \n    def test_learning_rate_sensitivity(self, noisy_linear_data):\n        \"\"\"Test different learning rates\"\"\"\n        X, y = noisy_linear_data\n        learning_rates = [0.001, 0.01, 0.1, 1.0]\n        \n        # TODO 1: Test each learning rate in a loop\n        # TODO 2: Verify rates [0.001, 0.01, 0.1] converge successfully  \n        # TODO 3: Verify rate 1.0 either converges slowly or diverges\n        # TODO 4: Check that smaller learning rates take more iterations\n        # TODO 5: Validate that all converged models reach similar parameters\n        # Hint: Track convergence success and iteration counts\n        \n    def test_gradient_calculation_accuracy(self, perfect_linear_data):\n        \"\"\"Validate gradient computation against analytical solution\"\"\"\n        X, y = perfect_linear_data\n        model = GradientDescentRegression()\n        \n        # TODO 1: Initialize parameters to arbitrary values (slope=0, intercept=0)\n        # TODO 2: Compute gradients using model._compute_gradients()\n        # TODO 3: Calculate analytical gradients manually using calculus formulas\n        # TODO 4: Verify computed gradients match analytical within tolerance\n        # TODO 5: Test gradient calculation at multiple parameter values\n        # Hint: Analytical gradients are ∂J/∂m = Σ(ŷ-y)x/n, ∂J/∂b = Σ(ŷ-y)/n\n```\n\n**Milestone Checkpoint Scripts:**\n\n```python\n# scripts/milestone1_verification.py - Automated M1 checkpoint\nimport numpy as np\nimport sys\nfrom pathlib import Path\n\n# Add src to path for imports\nsys.path.append(str(Path(__file__).parent.parent / 'src'))\n\nfrom simple_regression import SimpleLinearRegression\nfrom data_handler import generate_synthetic_data\n\ndef run_milestone1_checkpoint():\n    \"\"\"Complete Milestone 1 validation checkpoint\"\"\"\n    print(\"🔍 Running Milestone 1 Verification...\")\n    \n    # TODO 1: Generate perfect linear data using generate_synthetic_data()\n    # TODO 2: Create and fit SimpleLinearRegression model\n    # TODO 3: Verify parameters match expected values (slope, intercept)\n    # TODO 4: Test prediction accuracy on new data points\n    # TODO 5: Load and test Boston housing subset data\n    # TODO 6: Print comprehensive success report\n    \n    print(\"✅ Milestone 1: Simple Linear Regression - COMPLETED\")\n    return True\n\nif __name__ == \"__main__\":\n    success = run_milestone1_checkpoint()\n    sys.exit(0 if success else 1)\n```\n\n**Language-Specific Implementation Hints:**\n\n**NumPy Testing Best Practices:**\n- Use `np.testing.assert_allclose()` instead of exact equality for floating-point comparisons\n- Set appropriate `atol` (absolute tolerance) and `rtol` (relative tolerance) based on expected numerical precision\n- Use `np.testing.assert_array_equal()` for integer arrays and shape validation\n\n**Error Testing Patterns:**\n```python\n# Test specific exception types\nwith pytest.raises(DataValidationError, match=\"Array lengths must match\"):\n    model.fit(X_wrong_shape, y)\n\n# Test exception attributes  \nwith pytest.raises(ConvergenceError) as exc_info:\n    model.fit(X, y)\nassert exc_info.value.error_code == \"DIVERGENCE\"\nassert \"learning_rate\" in exc_info.value.suggested_fix\n```\n\n**Gradient Descent Testing Strategies:**\n- Test convergence with multiple random seeds to ensure robustness\n- Verify that cost function value matches manual MSE calculation\n- Test parameter update formula by checking single iteration changes\n- Validate that learning rate scaling affects convergence speed proportionally\n\n**Cross-Validation Implementation:**\n```python\ndef simple_train_test_split(X, y, test_size=0.2, random_state=42):\n    \"\"\"Simple train-test split for integration testing\"\"\"\n    # TODO: Implement random shuffling and splitting\n    # TODO: Return X_train, X_test, y_train, y_test\n    # TODO: Ensure reproducible splits with random_state\n    pass\n```\n\n**Debugging Tips for Testing:**\n\n| Symptom | Likely Cause | Diagnosis | Fix |\n|---|---|---|---|\n| All gradient descent tests timeout | Learning rate too high causing divergence | Check if cost_history_ shows increasing values | Reduce learning rate, add divergence detection |\n| Perfect fit tests fail with small errors | Numerical precision limitations | Compare expected vs actual tolerances | Adjust tolerance to 1e-10 for closed-form, 1e-6 for iterative |\n| Integration tests pass but unit tests fail | Test data inconsistency | Verify same datasets used in both test types | Use shared fixtures from conftest.py |\n| Regularization tests show no effect | Lambda parameter not being applied | Check cost function includes penalty term | Verify _compute_cost() adds L2 penalty |\n\n\n## Debugging Guide\n\n> **Milestone(s):** All milestones (M1: Simple Linear Regression - debugging data issues and basic fitting, M2: Gradient Descent - debugging convergence and optimization problems, M3: Multiple Linear Regression - debugging matrix operations and scaling issues)\n\nDebugging machine learning implementations presents unique challenges that extend beyond traditional software debugging. Unlike typical software bugs where incorrect output is immediately obvious, machine learning bugs often manifest as subtle performance degradation, slow convergence, or mathematically correct but practically useless results. This section provides a systematic approach to identifying, diagnosing, and resolving the most common issues encountered when implementing linear regression from scratch.\n\n### Mental Model: The Medical Diagnostic Process\n\nThink of debugging machine learning code like a doctor diagnosing a patient with mysterious symptoms. Just as a doctor follows a systematic process—gathering symptoms, forming hypotheses, running targeted tests, and prescribing treatment—debugging ML code requires careful observation of symptoms (poor predictions, slow training), hypothesis formation (data issues vs algorithmic problems), targeted investigation (plotting cost curves, inspecting gradients), and systematic fixes. The key insight is that ML bugs rarely have single causes; they often result from combinations of data quality issues, numerical instabilities, and algorithmic choices that interact in complex ways.\n\nThe diagnostic process becomes more complex because machine learning algorithms can appear to work correctly while producing suboptimal results. A gradient descent implementation might converge to some solution, but whether it's the optimal solution requires deeper investigation. This is why systematic debugging approaches and comprehensive validation techniques are essential for ML implementations.\n\n### Common Bug Patterns\n\nUnderstanding common bug patterns helps developers quickly identify and resolve issues without extensive debugging sessions. Each pattern includes the typical symptoms learners observe, the underlying causes, diagnostic techniques for confirming the hypothesis, and step-by-step fixes.\n\n#### Data-Related Bug Patterns\n\n| Bug Pattern | Symptoms | Underlying Cause | Diagnostic Approach | Fix Strategy |\n|-------------|----------|------------------|---------------------|--------------|\n| **Feature-Target Dimension Mismatch** | Array shape errors during fitting, predictions return wrong shapes | Inconsistent array dimensions between features and targets | Check `features.shape` and `targets.shape`, verify they match expected dimensions | Reshape arrays using `.reshape()` or fix data loading logic |\n| **Unnormalized Features** | Gradient descent fails to converge, parameters grow extremely large | Features on vastly different scales cause gradient imbalance | Plot feature distributions, check standard deviations across features | Apply z-score normalization before training |\n| **Missing Data Propagation** | NaN values in predictions, cost function returns NaN | NaN values in input data propagate through calculations | Use `np.isnan()` to check for missing values in datasets | Remove or impute missing values before training |\n| **Data Leakage in Normalization** | Unrealistically high R-squared scores on test data | Normalization statistics computed on entire dataset including test data | Verify normalization computed only on training data | Recompute normalization using only training features |\n| **Constant Feature Columns** | Division by zero errors during normalization | Features with zero variance cause division by zero in z-score calculation | Check `np.std(features, axis=0)` for zero values | Remove constant features or add small epsilon to standard deviation |\n\n#### Algorithmic Bug Patterns\n\n| Bug Pattern | Symptoms | Underlying Cause | Diagnostic Approach | Fix Strategy |\n|-------------|----------|------------------|---------------------|--------------|\n| **Gradient Descent Divergence** | Cost increases instead of decreasing, parameters explode to large values | Learning rate too high causes overshooting | Plot cost history, check if cost oscillates or increases | Reduce learning rate by factor of 10, add gradient clipping |\n| **Premature Convergence** | Training stops too early, suboptimal final parameters | Convergence tolerance too loose or inappropriate stopping criteria | Compare final parameters to analytical solution, check convergence reason | Tighten tolerance, use multiple convergence criteria |\n| **Gradient Computation Errors** | Slow convergence, parameters update in wrong direction | Incorrect partial derivative implementation | Compare computed gradients to numerical gradients using finite differences | Fix gradient calculation, verify against mathematical derivation |\n| **Learning Rate Too Small** | Extremely slow convergence, minimal parameter updates | Conservative learning rate causes tiny steps | Monitor parameter changes per iteration, check gradient magnitudes | Increase learning rate gradually, use adaptive learning rate |\n| **Matrix Dimension Errors** | Broadcasting errors, unexpected array shapes in multiple regression | Incorrect design matrix construction or weight vector dimensions | Print array shapes at each operation, verify matrix multiplication compatibility | Fix design matrix construction, ensure weight vector matches feature count plus one |\n\n#### Numerical Stability Patterns\n\n| Bug Pattern | Symptoms | Underlying Cause | Diagnostic Approach | Fix Strategy |\n|-------------|----------|------------------|---------------------|--------------|\n| **Parameter Overflow** | Parameters become inf or extremely large values | Accumulated numerical errors cause parameter explosion | Check parameter magnitudes during training, monitor for inf values | Add parameter clipping, reduce learning rate, improve numerical precision |\n| **Gradient Underflow** | Parameters stop updating despite not converging | Gradients become too small to affect parameter updates | Monitor gradient magnitudes, check for values near machine epsilon | Increase learning rate, check for numerical precision loss |\n| **Matrix Conditioning Issues** | Unstable solutions in multiple regression, high sensitivity to input changes | Design matrix becomes nearly singular or poorly conditioned | Compute condition number of design matrix using `np.linalg.cond()` | Add regularization, remove redundant features, improve data quality |\n| **Precision Loss in Cost Calculation** | Cost function plateaus prematurely, no further improvement possible | Floating point precision insufficient for small cost differences | Monitor relative cost changes, check if improvements below machine precision | Use double precision, implement relative convergence criteria |\n| **Accumulated Rounding Errors** | Gradual parameter drift, unstable convergence behavior | Small rounding errors accumulate over many iterations | Compare single-precision vs double-precision results, monitor error accumulation | Use higher precision arithmetic, implement periodic parameter normalization |\n\n### ML-Specific Debugging Techniques\n\nMachine learning debugging requires specialized techniques that go beyond traditional software debugging approaches. These techniques focus on understanding algorithm behavior, validating mathematical correctness, and diagnosing performance issues specific to optimization and statistical modeling.\n\n#### Visualization-Based Debugging\n\nVisualization provides immediate insights into algorithm behavior and data quality issues that are difficult to detect through numerical inspection alone. The key is knowing what to plot and how to interpret the results.\n\n**Cost Function Monitoring:** The cost history plot serves as the primary diagnostic tool for gradient descent debugging. A healthy cost curve should decrease monotonically with occasional plateaus. Oscillating costs indicate learning rate issues, while flat costs suggest convergence or numerical precision problems. Plotting both raw cost values and cost differences helps identify convergence patterns and numerical precision limits.\n\n**Parameter Evolution Tracking:** Plotting parameter values over training iterations reveals optimization dynamics. Parameters should generally move toward stable values with decreasing update magnitudes. Oscillating parameters indicate learning rate problems, while parameters that grow without bound suggest gradient computation errors or numerical instabilities.\n\n**Gradient Magnitude Analysis:** Monitoring gradient magnitudes throughout training helps diagnose optimization problems. Gradients should generally decrease as the algorithm approaches optimal parameters. Gradient explosion (rapidly increasing magnitudes) indicates numerical instability, while gradient vanishing (magnitudes near zero) suggests either convergence or underflow issues.\n\n**Residual Analysis:** Plotting prediction residuals against fitted values reveals model assumptions violations and data quality issues. Residuals should appear randomly scattered around zero with constant variance. Patterns in residuals indicate missing nonlinear relationships, heteroscedasticity, or outliers affecting model performance.\n\n#### Mathematical Verification Techniques\n\nMathematical verification ensures implementation correctness by comparing results against known analytical solutions or established numerical methods.\n\n**Gradient Checking with Finite Differences:** The most reliable method for validating gradient computations involves comparing analytical gradients to numerical approximations using finite differences. For each parameter, compute `(cost(θ + ε) - cost(θ - ε)) / (2ε)` with small ε (typically 1e-5) and compare to the analytical gradient. Differences larger than 1e-7 indicate gradient computation errors.\n\n**Closed-Form Solution Validation:** For simple linear regression, compare gradient descent results to the analytical solution computed using normal equations. Parameters should match within numerical precision (typically 1e-10). Significant differences indicate either gradient descent implementation errors or convergence issues.\n\n**Synthetic Data Testing:** Generate synthetic datasets with known parameters, train the model, and verify that recovered parameters match the ground truth. This approach isolates algorithmic issues from data quality problems and provides definitive validation of implementation correctness.\n\n**Invariant Checking:** Verify mathematical invariants during training. For example, the cost function should never increase in properly implemented gradient descent with appropriate learning rates. R-squared values should remain between 0 and 1 for well-specified models. Violations of these invariants indicate fundamental implementation errors.\n\n#### Logging and Inspection Strategies\n\nComprehensive logging enables post-mortem analysis of training failures and provides insights into algorithm behavior during successful runs.\n\n**Multi-Level Logging Architecture:** Implement logging at multiple granularities: summary statistics (final parameters, convergence metrics), iteration-level metrics (cost, gradient norms, parameter updates), and detailed debugging information (intermediate calculations, array shapes, numerical stability checks). This hierarchical approach provides overview information for normal operation and detailed diagnostics for debugging.\n\n**Checkpoint-Based Debugging:** Save complete model state at regular intervals during training, including parameters, gradients, cost values, and intermediate calculations. This enables analysis of training progression and identification of the exact iteration where problems begin. Checkpoints also support training resumption after debugging fixes.\n\n**Interactive Debugging Sessions:** Design code to support step-by-step execution during training iterations. Implement debugging hooks that allow inspection of variables, plotting of intermediate results, and modification of parameters without restarting training. This approach enables real-time diagnosis of convergence problems.\n\n**Performance Profiling Integration:** Combine algorithmic debugging with performance profiling to identify computational bottlenecks. Slow convergence might result from inefficient implementations rather than algorithmic issues. Profile matrix operations, gradient computations, and convergence checking to ensure optimal performance.\n\n### Performance and Convergence Debugging\n\nPerformance and convergence issues represent the most challenging aspects of machine learning debugging because they often result from subtle interactions between data characteristics, algorithmic parameters, and numerical precision considerations.\n\n#### Convergence Diagnosis Framework\n\n**Multi-Criteria Convergence Analysis:** Implement comprehensive convergence detection using multiple criteria simultaneously. Cost-based convergence (relative cost change below threshold) catches optimization completion. Gradient-based convergence (gradient magnitude below threshold) detects mathematical optimality. Parameter-based convergence (parameter change below threshold) identifies stability. Analyzing which criteria trigger convergence provides insights into optimization dynamics.\n\n**Convergence Quality Assessment:** Not all convergence represents successful optimization. Implement convergence quality metrics that evaluate whether the achieved solution represents a global optimum, local optimum, or saddle point. Compare final cost to theoretical minimum (when available), analyze gradient characteristics at convergence, and verify solution stability through parameter perturbation testing.\n\n**Learning Rate Adaptation Strategies:** Implement adaptive learning rate mechanisms that respond to convergence characteristics. If cost oscillates, reduce learning rate. If convergence is extremely slow, increase learning rate. If gradients become too small, adjust precision or modify convergence criteria. Systematic learning rate adaptation prevents many convergence problems while maintaining training efficiency.\n\n**Convergence Failure Recovery:** Design recovery mechanisms for common convergence failures. Gradient explosion triggers learning rate reduction and parameter reinitialization. Premature convergence triggers tolerance tightening and training continuation. Numerical instability triggers precision increase and parameter clipping. These automatic recovery mechanisms enable robust training without manual intervention.\n\n#### Numerical Stability Monitoring\n\n**Real-Time Stability Assessment:** Implement continuous monitoring of numerical stability indicators during training. Track parameter magnitudes, gradient norms, condition numbers, and numerical precision metrics at each iteration. Early detection of stability problems enables preventive action before catastrophic failures occur.\n\n**Overflow and Underflow Protection:** Design comprehensive protection against numerical overflow and underflow. Implement parameter clipping that constrains values to representable ranges. Use numerically stable formulations for mathematical operations. Detect and handle special values (inf, NaN) before they propagate through calculations. These protections ensure training can continue even under adverse numerical conditions.\n\n**Matrix Conditioning Analysis:** For multiple linear regression, monitor design matrix conditioning throughout training. Compute condition numbers and detect near-singularity conditions that cause numerical instability. Implement automatic regularization or feature removal when conditioning becomes problematic. Poor matrix conditioning is a common cause of unstable solutions in multiple regression.\n\n**Precision Management Strategies:** Implement dynamic precision management that adapts to problem requirements. Use double precision when numerical stability is critical. Monitor precision loss during calculations and upgrade precision when necessary. Balance computational efficiency with numerical accuracy based on convergence requirements and stability analysis.\n\n#### Performance Optimization Debugging\n\n**Computational Bottleneck Identification:** Profile training performance to identify computational bottlenecks that cause slow training. Matrix operations, gradient computations, and convergence checking each have different optimization requirements. Use profiling results to guide optimization efforts and ensure training efficiency scales appropriately with problem size.\n\n**Memory Usage Analysis:** Monitor memory consumption during training, especially for large datasets or multiple regression with many features. Identify memory leaks, excessive array copying, and inefficient data structures. Memory pressure can cause performance degradation and numerical instability, making memory analysis essential for robust implementations.\n\n**Algorithm Complexity Verification:** Verify that implementation complexity matches theoretical expectations. Simple linear regression should scale linearly with sample count. Multiple regression should scale with the cube of feature count due to matrix operations. Suboptimal scaling indicates algorithmic inefficiencies that affect training performance.\n\n**Vectorization Optimization:** Ensure maximum utilization of vectorized operations using NumPy. Replace loops with matrix operations wherever possible. Profile vector operations to verify they achieve expected performance gains. Poor vectorization is a common cause of slow training in educational implementations.\n\n![Error Detection and Recovery Flow](./diagrams/error-handling-flow.svg)\n\n#### Common Debugging Scenarios and Solutions\n\n**Scenario 1: Model Fits Training Data Perfectly but Fails on New Data**\n\nThis scenario typically indicates overfitting or data leakage rather than implementation bugs. Diagnostic steps include verifying proper train-test splits, checking for feature normalization leakage, and analyzing model complexity relative to dataset size. Solutions involve implementing proper cross-validation, fixing normalization procedures, and adding regularization if appropriate.\n\n**Scenario 2: Gradient Descent Converges to Different Solutions on Repeated Runs**\n\nThis scenario suggests numerical instability or poor conditioning rather than algorithm randomness. Linear regression has a unique global optimum, so different solutions indicate implementation problems. Diagnostic approaches include checking matrix conditioning, verifying gradient computations, and analyzing learning rate appropriateness. Solutions involve adding regularization, improving numerical precision, or using closed-form solutions for comparison.\n\n**Scenario 3: Training Appears Successful but Predictions Are Nonsensical**\n\nThis scenario often results from unit mismatches, scaling issues, or incorrect prediction implementation. Diagnostic steps include verifying feature scaling consistency between training and prediction, checking prediction function implementation, and analyzing prediction magnitudes relative to training targets. Solutions involve fixing feature scaling, correcting prediction logic, or debugging data preprocessing pipelines.\n\n**Scenario 4: Algorithm Runs Without Errors but Produces Poor Results**\n\nThis scenario requires systematic validation of each component. Start with data quality analysis, verify mathematical implementations against analytical solutions, check parameter initialization, and analyze convergence quality. The problem often lies in subtle implementation details rather than obvious bugs.\n\n### Implementation Guidance\n\nThis section provides concrete tools and techniques for implementing robust debugging capabilities in your linear regression system. The focus is on practical debugging infrastructure that can be integrated throughout development and used for ongoing diagnosis.\n\n#### Technology Recommendations for Debugging\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Visualization | `matplotlib` with basic plotting functions | `matplotlib` + `seaborn` with custom debugging dashboards |\n| Logging | Python `logging` module with file output | `logging` + `wandb` or `tensorboard` for experiment tracking |\n| Numerical Validation | Manual assertion checking | `numpy.testing` module with comprehensive test suites |\n| Profiling | Built-in `time.time()` measurements | `cProfile` + `line_profiler` for detailed performance analysis |\n| Interactive Debugging | Print statements and manual inspection | `ipdb` or `pdb` with breakpoints and variable inspection |\n\n#### Debugging Infrastructure Starter Code\n\n**Debugging Logger Configuration:**\n\n```python\nimport logging\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import List, Dict, Any, Optional, Tuple\nfrom dataclasses import dataclass\nimport json\nimport os\n\n@dataclass\nclass DebugConfig:\n    \"\"\"Configuration for debugging and logging behavior.\"\"\"\n    log_level: str = \"INFO\"\n    save_plots: bool = True\n    plot_dir: str = \"debug_plots\"\n    log_file: str = \"training_debug.log\"\n    checkpoint_interval: int = 100\n    enable_convergence_plots: bool = True\n    enable_gradient_checking: bool = False\n    numerical_precision_threshold: float = 1e-10\n\nclass MLDebugger:\n    \"\"\"Comprehensive debugging utilities for machine learning implementations.\"\"\"\n    \n    def __init__(self, config: DebugConfig):\n        self.config = config\n        self.setup_logging()\n        self.training_history = []\n        self.gradient_history = []\n        self.parameter_history = []\n        \n    def setup_logging(self):\n        \"\"\"Configure logging system for debugging output.\"\"\"\n        logging.basicConfig(\n            level=getattr(logging, self.config.log_level),\n            format='%(asctime)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s',\n            handlers=[\n                logging.FileHandler(self.config.log_file),\n                logging.StreamHandler()\n            ]\n        )\n        self.logger = logging.getLogger(__name__)\n        \n        if self.config.save_plots:\n            os.makedirs(self.config.plot_dir, exist_ok=True)\n    \n    def log_training_iteration(self, iteration: int, cost: float, \n                             parameters: np.ndarray, gradients: np.ndarray):\n        \"\"\"Log detailed information for each training iteration.\"\"\"\n        # TODO: Store iteration data in training_history\n        # TODO: Check for numerical instabilities in parameters and gradients\n        # TODO: Log warnings if parameters or gradients exceed safe ranges\n        # TODO: Store gradient norms and parameter norms for analysis\n        pass\n    \n    def check_gradient_correctness(self, compute_cost_func, compute_gradient_func,\n                                 parameters: np.ndarray, X: np.ndarray, y: np.ndarray,\n                                 epsilon: float = 1e-5) -> Tuple[bool, Dict[str, float]]:\n        \"\"\"Validate gradient computation using finite differences.\"\"\"\n        # TODO: Implement finite difference gradient checking\n        # TODO: Compare analytical gradients to numerical approximations\n        # TODO: Return boolean indicating correctness and detailed error metrics\n        # TODO: Log detailed comparison results for debugging\n        pass\n    \n    def plot_convergence_diagnostics(self, cost_history: List[float],\n                                   parameter_history: List[np.ndarray],\n                                   save_path: Optional[str] = None):\n        \"\"\"Generate comprehensive convergence diagnostic plots.\"\"\"\n        # TODO: Create subplots for cost history, parameter evolution, and gradient norms\n        # TODO: Plot cost reduction rate and detect convergence patterns\n        # TODO: Show parameter trajectories and stability analysis\n        # TODO: Save plots to debug directory if enabled\n        pass\n    \n    def diagnose_convergence_issues(self, cost_history: List[float],\n                                  gradient_history: List[np.ndarray]) -> Dict[str, Any]:\n        \"\"\"Analyze convergence behavior and identify common problems.\"\"\"\n        # TODO: Detect oscillating costs indicating learning rate issues\n        # TODO: Identify premature convergence or stagnation\n        # TODO: Check for gradient explosion or vanishing gradients\n        # TODO: Return diagnostic report with specific recommendations\n        pass\n    \n    def validate_data_quality(self, X: np.ndarray, y: np.ndarray) -> Dict[str, Any]:\n        \"\"\"Comprehensive data quality validation and reporting.\"\"\"\n        # TODO: Check for missing values, infinite values, and data type issues\n        # TODO: Analyze feature scales and distributions\n        # TODO: Detect constant features and highly correlated features\n        # TODO: Validate array shapes and compatibility\n        # TODO: Return detailed data quality report\n        pass\n```\n\n#### Core Debugging Function Skeletons\n\n**Gradient Validation Implementation:**\n\n```python\ndef validate_gradients_with_finite_differences(\n    model,\n    X: np.ndarray,\n    y: np.ndarray,\n    epsilon: float = 1e-5,\n    tolerance: float = 1e-7\n) -> Tuple[bool, Dict[str, float]]:\n    \"\"\"\n    Validate gradient computation using finite difference approximation.\n    \n    This function computes numerical gradients using finite differences and\n    compares them to analytical gradients from the model implementation.\n    \"\"\"\n    # TODO 1: Get current model parameters (slope, intercept or weight vector)\n    # TODO 2: Compute analytical gradients using model's gradient function\n    # TODO 3: For each parameter, compute numerical gradient using finite differences:\n    #         grad_numerical[i] = (cost(params + epsilon*e_i) - cost(params - epsilon*e_i)) / (2*epsilon)\n    #         where e_i is unit vector in direction i\n    # TODO 4: Compare analytical and numerical gradients element-wise\n    # TODO 5: Compute relative and absolute errors for each parameter\n    # TODO 6: Return validation result and detailed error metrics\n    # Hint: Use np.abs(analytical - numerical) / (np.abs(analytical) + 1e-8) for relative error\n    pass\n\ndef diagnose_learning_rate_issues(\n    cost_history: List[float],\n    parameter_history: List[np.ndarray],\n    current_learning_rate: float\n) -> Tuple[str, float, str]:\n    \"\"\"\n    Analyze training behavior to diagnose learning rate problems.\n    \n    Returns diagnosis type, suggested learning rate, and explanation.\n    \"\"\"\n    # TODO 1: Check if cost is increasing - indicates learning rate too high\n    # TODO 2: Check for oscillating costs - indicates learning rate slightly too high\n    # TODO 3: Check for extremely slow convergence - indicates learning rate too low\n    # TODO 4: Analyze parameter update magnitudes relative to parameter values\n    # TODO 5: Return diagnosis type ('too_high', 'too_low', 'oscillating', 'appropriate')\n    # TODO 6: Suggest new learning rate based on diagnosis\n    # TODO 7: Provide detailed explanation of the diagnosis\n    # Hint: Cost increasing for multiple iterations definitely means learning rate too high\n    pass\n\ndef detect_numerical_instabilities(\n    parameters: np.ndarray,\n    gradients: np.ndarray,\n    cost: float,\n    iteration: int\n) -> Tuple[List[str], List[str]]:\n    \"\"\"\n    Real-time detection of numerical stability problems during training.\n    \n    Returns lists of detected issues and suggested fixes.\n    \"\"\"\n    # TODO 1: Check for inf or NaN values in parameters, gradients, and cost\n    # TODO 2: Detect parameter overflow (values exceeding MAX_PARAMETER_VALUE)\n    # TODO 3: Detect gradient explosion (gradient norms exceeding MAX_GRADIENT_NORM)\n    # TODO 4: Detect gradient underflow (gradient norms below MIN_GRADIENT_NORM)\n    # TODO 5: Check for loss of numerical precision in cost calculation\n    # TODO 6: Compile list of detected issues with descriptive names\n    # TODO 7: Generate specific fix recommendations for each detected issue\n    # Hint: Use np.linalg.norm() to compute gradient magnitudes\n    pass\n```\n\n#### Comprehensive Testing and Validation Framework\n\n**Milestone Checkpoint Validation:**\n\n```python\ndef validate_milestone_1_implementation(model_class):\n    \"\"\"\n    Comprehensive validation for Milestone 1: Simple Linear Regression.\n    \n    Tests closed-form solution correctness and basic prediction capability.\n    \"\"\"\n    print(\"=== Milestone 1 Validation: Simple Linear Regression ===\")\n    \n    # TODO 1: Create synthetic linear data with known parameters\n    # TODO 2: Fit model using closed-form solution\n    # TODO 3: Verify recovered parameters match ground truth within tolerance\n    # TODO 4: Test prediction accuracy on new data points\n    # TODO 5: Validate R-squared calculation correctness\n    # TODO 6: Test edge cases (constant features, single data point, etc.)\n    # TODO 7: Report detailed validation results and success/failure status\n    \n    # Expected output: \"✓ All Milestone 1 tests passed\" or specific failure details\n    pass\n\ndef validate_milestone_2_implementation(model_class):\n    \"\"\"\n    Comprehensive validation for Milestone 2: Gradient Descent.\n    \n    Tests iterative optimization convergence and gradient computation correctness.\n    \"\"\"\n    print(\"=== Milestone 2 Validation: Gradient Descent ===\")\n    \n    # TODO 1: Test gradient computation using finite difference validation\n    # TODO 2: Verify convergence to same solution as closed-form method\n    # TODO 3: Test different learning rates and validate convergence behavior\n    # TODO 4: Verify cost function decreases monotonically\n    # TODO 5: Test convergence detection triggers appropriately\n    # TODO 6: Validate training history tracking and parameter evolution\n    # TODO 7: Report convergence quality and optimization effectiveness\n    \n    # Expected behavior: Should converge to within 1e-6 of analytical solution\n    pass\n\ndef validate_milestone_3_implementation(model_class):\n    \"\"\"\n    Comprehensive validation for Milestone 3: Multiple Linear Regression.\n    \n    Tests matrix operations, feature scaling, and regularization.\n    \"\"\"\n    print(\"=== Milestone 3 Validation: Multiple Linear Regression ===\")\n    \n    # TODO 1: Test design matrix construction with multiple features\n    # TODO 2: Validate feature normalization preserves relationships\n    # TODO 3: Test vectorized gradient descent on multi-dimensional problems\n    # TODO 4: Verify regularization affects overfitting appropriately\n    # TODO 5: Test matrix conditioning and numerical stability\n    # TODO 6: Compare results to sklearn LinearRegression for validation\n    # TODO 7: Test scalability with increasing feature counts\n    \n    # Expected behavior: Should match sklearn results within numerical precision\n    pass\n```\n\n#### Debugging Command-Line Interface\n\n**Interactive Debugging Session:**\n\n```python\ndef debug_training_session(model, X, y, debug_config: DebugConfig):\n    \"\"\"\n    Interactive debugging session for training problems.\n    \n    Provides step-by-step diagnosis and fixes for common training issues.\n    \"\"\"\n    debugger = MLDebugger(debug_config)\n    \n    print(\"Starting interactive debugging session...\")\n    print(\"Available commands: 'data', 'gradients', 'convergence', 'parameters', 'quit'\")\n    \n    while True:\n        command = input(\"Debug> \").strip().lower()\n        \n        if command == 'data':\n            # TODO: Run comprehensive data quality analysis\n            # TODO: Display feature distributions, correlation matrix, missing values\n            # TODO: Recommend preprocessing steps if issues detected\n            pass\n            \n        elif command == 'gradients':\n            # TODO: Perform gradient checking with finite differences\n            # TODO: Display gradient correctness results and error analysis\n            # TODO: Suggest fixes if gradient computation errors detected\n            pass\n            \n        elif command == 'convergence':\n            # TODO: Analyze current training progress and convergence quality\n            # TODO: Generate convergence diagnostic plots\n            # TODO: Recommend learning rate or tolerance adjustments\n            pass\n            \n        elif command == 'parameters':\n            # TODO: Display current parameter values and evolution history\n            # TODO: Check for parameter instabilities or unexpected behavior\n            # TODO: Compare to expected parameter ranges or theoretical values\n            pass\n            \n        elif command == 'quit':\n            break\n            \n        else:\n            print(\"Unknown command. Available: 'data', 'gradients', 'convergence', 'parameters', 'quit'\")\n```\n\n#### Language-Specific Debugging Hints\n\n**Python/NumPy Specific Techniques:**\n\n- Use `np.seterr(all='raise')` during development to catch numerical issues immediately rather than allowing NaN propagation\n- Leverage `np.testing.assert_array_almost_equal()` for robust numerical comparisons that account for floating-point precision\n- Use `warnings.filterwarnings('error')` to convert NumPy warnings into exceptions for debugging\n- Implement custom `__repr__` methods for model classes to provide detailed state information during debugging\n- Use `np.savez()` to save complete debugging sessions including arrays, parameters, and metadata for later analysis\n\n**Performance Debugging:**\n\n- Use `%timeit` in Jupyter notebooks to profile individual operations and identify bottlenecks\n- Implement timing decorators that automatically log execution times for critical functions\n- Use `memory_profiler` to track memory usage and detect memory leaks during long training sessions\n- Profile matrix operations separately to ensure they achieve expected O(n³) scaling for multiple regression\n\n#### Milestone Debugging Checkpoints\n\n**After Milestone 1 (Simple Linear Regression):**\n- Run: `python -m pytest tests/test_simple_regression.py -v`\n- Expected: All tests pass, recovered slope/intercept match analytical solution within 1e-10\n- Manual verification: Plot fitted line against data points, visually verify good fit\n- Debug signs: If parameters don't match analytical solution, check closed-form implementation\n\n**After Milestone 2 (Gradient Descent):**\n- Run: `python debug_gradient_descent.py --validate-gradients`\n- Expected: Gradient checking passes, convergence achieved within 1000 iterations\n- Manual verification: Cost should decrease monotonically, final parameters match Milestone 1 results\n- Debug signs: If cost oscillates, reduce learning rate; if convergence is slow, increase learning rate\n\n**After Milestone 3 (Multiple Linear Regression):**\n- Run: `python validate_multiple_regression.py --compare-sklearn`\n- Expected: Results match sklearn LinearRegression within 1e-8, proper scaling handling\n- Manual verification: Feature normalization should result in zero mean, unit variance\n- Debug signs: If results differ from sklearn, check design matrix construction and feature scaling\n\n⚠️ **Pitfall: Debugging in Isolation**\nMany learners debug components in isolation without considering system-wide interactions. Always test the complete pipeline from data loading through prediction, as bugs often emerge from component interfaces and data flow between modules.\n\n⚠️ **Pitfall: Ignoring Numerical Precision**\nFloating-point precision issues cause many subtle bugs in machine learning implementations. Always use appropriate tolerances for comparisons and monitor for precision loss during iterative algorithms.\n\n⚠️ **Pitfall: Over-Relying on Print Statements**\nWhile print statements help with initial debugging, implement comprehensive logging and visualization capabilities for complex debugging scenarios. Print statements become insufficient for optimization problems that require understanding algorithm behavior over many iterations.\n\n\n## Future Extensions and Learning Path\n\n> **Milestone(s):** All milestones (foundation for continued learning beyond M3: Multiple Linear Regression)\n\nThis section outlines the natural progression from our foundational linear regression implementation toward more advanced machine learning concepts. The architecture and design patterns established in our three milestones create a solid foundation for extending into polynomial regression, cross-validation, regularization variants, and eventually more complex algorithms like logistic regression and neural networks.\n\n### Mental Model: The Learning Journey Map\n\nThink of our linear regression implementation as the **foundational base camp** in a mountain climbing expedition toward machine learning mastery. Just as mountaineers establish a well-equipped base camp with proper supplies, communication systems, and safety protocols before attempting higher peaks, our implementation provides the essential tools and patterns needed for more advanced algorithms.\n\nThe base camp (our current implementation) has established critical infrastructure: data handling pipelines, numerical optimization routines, parameter management systems, and debugging tools. From this foundation, we can launch expeditions to nearby peaks (polynomial features, regularization variants) or prepare for more challenging ascents (logistic regression, neural networks). Each extension builds upon the same core principles but adapts the techniques to handle new types of mathematical relationships and optimization challenges.\n\nThe key insight is that **machine learning algorithms share common patterns**: they all involve data preprocessing, parameter optimization, prediction generation, and model evaluation. By mastering these patterns in the linear regression context, learners develop transferable skills that accelerate learning of more complex algorithms.\n\n### Immediate Extensions\n\nThese extensions represent natural next steps that build directly upon our existing architecture without requiring fundamental changes to the core design patterns. Each extension introduces one or two new concepts while leveraging the solid foundation we've established.\n\n#### Polynomial Features and Non-Linear Relationships\n\nOur current implementation handles linear relationships through the equation `y = w₁x₁ + w₂x₂ + ... + wₙxₙ + b`. The most natural extension involves **polynomial feature engineering**, which transforms linear models to capture non-linear patterns by creating new features based on powers and interactions of existing features.\n\nThe mathematical foundation involves expanding our feature space through polynomial basis functions. For a single input feature `x`, we can create polynomial features `[1, x, x², x³, ...]` up to some degree `d`. For multiple input features, we add interaction terms like `x₁x₂`, `x₁²x₂`, etc. This transforms a non-linear problem in the original feature space into a linear problem in the expanded polynomial feature space.\n\n**Implementation Architecture for Polynomial Features:**\n\n| Component | Current Responsibility | Extended Responsibility |\n|-----------|------------------------|-------------------------|\n| `DataHandler` | Load and normalize features | Add polynomial feature generation and interaction terms |\n| `MultipleLinearRegression` | Fit linear combinations of features | Unchanged - still fits linear model, but on expanded features |\n| `StandardScaler` | Normalize individual features | Handle scaling of polynomial features with different magnitude ranges |\n\nThe architectural beauty lies in **reusing our existing linear regression engine unchanged**. The `MultipleLinearRegression` component continues to solve the same linear optimization problem, but now operates on an expanded feature matrix where polynomial relationships appear as linear relationships.\n\n> **Decision: Polynomial Feature Generation Strategy**\n> - **Context**: Need to handle non-linear relationships while preserving our linear regression architecture\n> - **Options Considered**:\n>   1. Modify core regression algorithm to handle non-linear terms directly\n>   2. Create polynomial features as preprocessing step before existing linear regression\n>   3. Build separate polynomial regression class from scratch\n> - **Decision**: Preprocess data to create polynomial features, then use existing linear regression\n> - **Rationale**: Preserves existing architecture, demonstrates feature engineering concepts, maintains mathematical simplicity\n> - **Consequences**: Enables non-linear modeling without code changes to core algorithm, but increases feature dimensionality and risk of overfitting\n\n**Polynomial Feature Generation Process:**\n\n1. **Degree Selection**: Choose maximum polynomial degree `d` based on problem complexity and available data\n2. **Feature Expansion**: For each original feature, generate powers up to degree `d`\n3. **Interaction Terms**: Create cross-products between different original features up to total degree `d`\n4. **Feature Naming**: Maintain interpretability by naming generated features like `x1^2`, `x1*x2`, etc.\n5. **Scaling Considerations**: Apply normalization after polynomial generation since `x²` has very different scale than `x`\n\nThe implementation extends our `DataHandler` with a `PolynomialFeatureGenerator` that transforms the original feature matrix into an expanded polynomial feature matrix. This generator needs to handle the combinatorial explosion of features carefully - for `n` original features and degree `d`, we can generate up to `C(n+d,d)` polynomial features.\n\n**Cross-Validation for Model Selection**\n\nCross-validation represents the next critical concept for preventing overfitting, especially important when working with polynomial features that can easily memorize training data. Our implementation establishes the foundation for cross-validation through our existing train-test evaluation patterns.\n\nThe **k-fold cross-validation** algorithm involves partitioning the dataset into `k` equal-sized subsets, then training `k` different models where each model uses `k-1` subsets for training and the remaining subset for validation. This provides a more robust estimate of model performance than a single train-test split.\n\n**Cross-Validation Architecture Integration:**\n\n| Component | New Responsibility |\n|-----------|-------------------|\n| `DataHandler` | Add `create_cv_splits(k)` method to generate k-fold partitions |\n| `CrossValidator` | New component to orchestrate training multiple models and aggregate results |\n| `ModelSelection` | New component to compare different configurations (polynomial degrees, regularization strengths) |\n\nThe cross-validation process integrates naturally with our existing training pipeline. Each fold reuses our established data loading, preprocessing, model fitting, and evaluation components. The `CrossValidator` component acts as an orchestrator that manages multiple training sessions and aggregates performance metrics.\n\n> **Key Design Insight**: Cross-validation doesn't require changes to our core regression algorithms. Instead, it's a **meta-algorithm** that uses our existing training and evaluation infrastructure multiple times with different data partitions.\n\n**Implementation Steps for Cross-Validation:**\n\n1. **Data Partitioning**: Split dataset into `k` approximately equal-sized folds, preserving target variable distribution\n2. **Fold Training Loop**: For each fold `i`, train model on folds `{1...k} - {i}` and evaluate on fold `i`\n3. **Metric Aggregation**: Collect performance metrics (R-squared, MSE) from each fold and compute mean and standard deviation\n4. **Model Selection**: Compare aggregated metrics across different model configurations\n5. **Final Training**: Train final model on entire dataset using best configuration found via cross-validation\n\n#### Regularization Variants and Robust Linear Models\n\nOur current implementation includes basic L2 regularization (Ridge regression) in the `MultipleLinearRegression` component. The natural extensions involve **L1 regularization (Lasso)** and **Elastic Net** (combined L1 and L2), each addressing different aspects of model complexity and feature selection.\n\n**Mathematical Foundation of Regularization Extensions:**\n\n| Regularization Type | Cost Function | Key Property |\n|-------------------|---------------|--------------|\n| Ridge (L2) | `MSE + α∑w²` | Shrinks coefficients toward zero, keeps all features |\n| Lasso (L1) | `MSE + α∑|w|` | Performs automatic feature selection by setting some coefficients to exactly zero |\n| Elastic Net | `MSE + α₁∑|w| + α₂∑w²` | Balances feature selection (L1) with coefficient shrinkage (L2) |\n\nThe architectural challenge lies in adapting our gradient descent optimizer to handle the non-differentiable L1 penalty. While L2 regularization has a smooth derivative `2αw`, L1 regularization has a non-smooth derivative that requires **subgradient methods** or **proximal gradient descent**.\n\n**Regularization Architecture Extensions:**\n\n| Component | Current Implementation | L1/Elastic Net Extension |\n|-----------|----------------------|--------------------------|\n| `MultipleLinearRegression` | Handles L2 penalty in cost and gradient | Add regularization type parameter and different penalty calculations |\n| `_compute_gradients()` | Simple gradient with L2 penalty | Conditional logic for L1 subgradients or proximal operators |\n| `TrainingHistory` | Tracks cost evolution | Add regularization path tracking to see feature selection process |\n\nThe implementation requires careful handling of the L1 penalty's non-differentiability at zero. Common approaches include **soft thresholding** in proximal gradient methods or **coordinate descent** algorithms that optimize one parameter at a time while holding others fixed.\n\n> **Decision: Regularization Implementation Strategy**\n> - **Context**: Need L1 regularization for feature selection while maintaining gradient descent framework\n> - **Options Considered**:\n>   1. Implement proximal gradient descent with soft thresholding operators\n>   2. Switch to coordinate descent algorithm specifically for Lasso\n>   3. Use subgradient methods with step size scheduling\n> - **Decision**: Implement proximal gradient descent with soft thresholding for L1, keep existing gradient descent for L2\n> - **Rationale**: Proximal methods generalize our existing gradient descent while handling non-smooth penalties elegantly\n> - **Consequences**: Enables feature selection capabilities while preserving familiar optimization framework, but adds complexity to parameter update logic\n\n### Advanced Topics Path\n\nThe foundation established through our linear regression implementation creates natural pathways toward more sophisticated machine learning algorithms. Each advanced topic builds upon core concepts we've mastered while introducing new mathematical frameworks and computational challenges.\n\n#### Logistic Regression and Classification\n\n**Logistic regression** represents the most direct extension from linear regression, transitioning from predicting continuous values to predicting class probabilities. The mathematical connection involves replacing the linear prediction `y = Xw + b` with a probability prediction `p = sigmoid(Xw + b)`, where the sigmoid function `σ(z) = 1/(1 + e^(-z))` maps any real number to the interval [0,1].\n\n**Conceptual Bridge from Linear to Logistic Regression:**\n\n| Aspect | Linear Regression | Logistic Regression | Architectural Similarity |\n|--------|------------------|-------------------|-------------------------|\n| **Prediction Function** | `y = Xw + b` | `p = sigmoid(Xw + b)` | Same linear combination, different output transformation |\n| **Cost Function** | Mean Squared Error | Cross-Entropy Loss | Different loss function, same optimization framework |\n| **Optimization** | Gradient Descent | Gradient Descent | Identical optimization algorithm and convergence detection |\n| **Parameter Updates** | `w -= lr * ∇MSE` | `w -= lr * ∇CrossEntropy` | Same update rule, different gradient computation |\n\nThe architectural transition involves **minimal changes to our existing components**. The `DataHandler` requires binary target encoding, the cost function changes from MSE to cross-entropy, and gradient computation adapts to the new loss function. However, the core optimization loop, convergence detection, parameter management, and evaluation infrastructure remain largely unchanged.\n\n**Implementation Pathway for Logistic Regression:**\n\n1. **Target Encoding**: Extend `DataHandler` to handle binary classification targets (0/1 encoding)\n2. **Sigmoid Activation**: Add activation functions that transform linear outputs to probabilities\n3. **Cross-Entropy Cost**: Replace MSE cost function with log-likelihood maximization\n4. **Gradient Adaptation**: Modify gradient computation for logistic cost function\n5. **Classification Metrics**: Extend evaluation beyond R-squared to accuracy, precision, recall\n\nThe mathematical foundation becomes `Cost = -∑[y*log(p) + (1-y)*log(1-p)]`, where `p = sigmoid(Xw + b)`. The gradient computation yields `∇w = (1/m) * X^T * (p - y)`, which has the same computational pattern as linear regression gradients but derives from a different loss function.\n\n> **Learning Progression Insight**: Logistic regression demonstrates that many machine learning algorithms share the same **optimization backbone** (gradient descent) but differ in their **mathematical modeling assumptions** (linear vs. probabilistic relationships).\n\n#### Neural Networks and Deep Learning Foundations\n\nOur linear regression implementation establishes several foundational concepts that transfer directly to neural networks: **matrix operations**, **gradient computation**, **iterative optimization**, and **parameter management**. A neural network can be understood as a composition of multiple linear transformations with non-linear activation functions.\n\n**Architectural Concepts Bridge:**\n\n| Linear Regression Component | Neural Network Equivalent | Conceptual Connection |\n|---------------------------|----------------------------|----------------------|\n| `weights_` parameter vector | Weight matrices for each layer | Parameters to be optimized |\n| `_compute_gradients()` method | Backpropagation algorithm | Gradient computation for optimization |\n| `fit()` training loop | Training epoch iterations | Iterative parameter updates |\n| `predict()` method | Forward propagation | Input-to-output transformation |\n| `StandardScaler` normalization | Input normalization layers | Data preprocessing for stable training |\n\nA **single-layer neural network** (perceptron) is mathematically identical to our linear regression: `output = activation(Xw + b)`. Multi-layer networks compose multiple such transformations: `output = f₃(f₂(f₁(X)))`, where each `fᵢ` represents a linear transformation followed by a non-linear activation.\n\nThe **backpropagation algorithm** extends our gradient computation using the chain rule. Instead of computing gradients for a single set of parameters, backpropagation computes gradients for parameters in each layer by propagating error derivatives backward through the network.\n\n**Neural Network Implementation Pathway:**\n\n1. **Layer Abstraction**: Extend our linear transformation to a `Layer` class with forward and backward methods\n2. **Activation Functions**: Add non-linear transformations (ReLU, sigmoid, tanh) after linear layers\n3. **Network Composition**: Chain multiple layers with automatic gradient flow between them\n4. **Backpropagation**: Generalize our gradient computation to handle multi-layer parameter updates\n5. **Advanced Optimizers**: Extend beyond basic gradient descent to momentum, Adam, etc.\n\nThe implementation builds upon our established patterns. Each layer maintains its own `weights_` and provides `forward()` and `backward()` methods. The `Network` class orchestrates the forward pass (prediction) and backward pass (gradient computation) across all layers, similar to how our current `fit()` method orchestrates training steps.\n\n#### Advanced Optimization Algorithms\n\nOur basic gradient descent implementation with fixed learning rates provides the foundation for more sophisticated optimization algorithms that adapt learning rates, incorporate momentum, or use second-order information.\n\n**Optimization Algorithm Progression:**\n\n| Algorithm | Key Innovation | Implementation Extension |\n|-----------|----------------|-------------------------|\n| **SGD with Momentum** | Accumulates velocity term to accelerate convergence | Add velocity vector to parameter updates |\n| **AdaGrad** | Adapts learning rate per parameter based on historical gradients | Maintain per-parameter gradient accumulation |\n| **Adam** | Combines momentum with adaptive learning rates | Track both gradient momentum and second moment estimates |\n| **L-BFGS** | Uses quasi-Newton methods with limited memory | Maintain history of recent gradients for second-order approximation |\n\nEach algorithm extends our basic `parameter_update_rule` with additional state variables and more sophisticated update computations. The architectural pattern remains consistent: compute gradients, update parameters based on gradient and algorithm-specific state, check convergence.\n\n**Adam Optimizer Extension Example:**\n\nThe Adam optimizer maintains exponentially decaying averages of gradients (`m_t`) and squared gradients (`v_t`). The update rule becomes:\n```\nm_t = β₁ * m_(t-1) + (1-β₁) * g_t\nv_t = β₂ * v_(t-1) + (1-β₂) * g_t²\nparameters -= lr * m̂_t / (√v̂_t + ε)\n```\n\nThis requires extending our `TrainingHistory` to track momentum terms and adapting our convergence detection to handle adaptive learning rates.\n\n### Production Readiness\n\nTransforming our educational implementation into a production-ready system requires addressing scalability, reliability, maintainability, and performance concerns that were deliberately simplified for learning purposes.\n\n#### Computational Efficiency and Scalability\n\nOur current implementation prioritizes clarity and educational value over computational efficiency. Production deployment requires optimizing for large datasets, distributed computing, and real-time prediction scenarios.\n\n**Performance Optimization Requirements:**\n\n| Current Implementation | Production Requirement | Optimization Strategy |\n|----------------------|------------------------|---------------------|\n| **Full-batch gradient descent** | Handle datasets larger than memory | Implement mini-batch and online learning algorithms |\n| **Pure NumPy operations** | Leverage hardware acceleration | Integrate with optimized BLAS libraries, GPU computation |\n| **Single-threaded training** | Utilize multi-core processors | Parallelize gradient computation and parameter updates |\n| **In-memory data handling** | Handle terabyte-scale datasets | Implement data streaming and out-of-core processing |\n| **Python implementation** | High-performance inference | Compile critical paths to C/C++ or use specialized ML frameworks |\n\nThe **mini-batch gradient descent** extension involves modifying our training loop to process data in small batches rather than the entire dataset. This requires changes to our `DataHandler` for batch generation and our optimization components for accumulating gradients across batches.\n\n**Scalability Architecture Patterns:**\n\n1. **Data Pipeline Streaming**: Replace in-memory arrays with data generators that yield batches from disk or network\n2. **Distributed Parameter Updates**: Implement parameter server architectures for multi-machine training\n3. **Model Checkpointing**: Add serialization for model state to enable training resumption and deployment\n4. **Prediction Serving**: Separate training and inference with optimized prediction pipelines\n5. **Feature Store Integration**: Connect with production feature management systems\n\n#### Robust Production Data Handling\n\nOur current `DataHandler` provides basic CSV loading and validation suitable for controlled educational scenarios. Production systems must handle diverse data sources, streaming updates, missing values, data quality monitoring, and schema evolution.\n\n**Production Data Management Requirements:**\n\n| Component | Current Scope | Production Extension |\n|-----------|---------------|---------------------|\n| **Data Loading** | CSV files with clean data | Multiple formats (JSON, Parquet, databases), streaming ingestion |\n| **Data Validation** | Basic shape and type checking | Comprehensive data quality monitoring, drift detection |\n| **Missing Values** | Assumes complete data | Robust imputation strategies and missing data handling |\n| **Feature Engineering** | Manual polynomial features | Automated feature generation and selection pipelines |\n| **Data Versioning** | No versioning | Track data lineage and enable reproducible training |\n\nThe production `DataHandler` requires integration with **data pipeline orchestration** systems, **feature stores** for consistent feature definitions, and **monitoring systems** for data quality and model performance tracking.\n\n**Enterprise Integration Patterns:**\n\n1. **Feature Store Integration**: Connect with centralized feature management systems for consistent train/serve features\n2. **Data Quality Monitoring**: Implement statistical tests for data drift, schema validation, and anomaly detection\n3. **Model Registry**: Version control for trained models with metadata tracking and deployment management\n4. **A/B Testing Framework**: Infrastructure for controlled model rollouts and performance comparison\n5. **Monitoring and Alerting**: Real-time tracking of prediction accuracy, latency, and system health\n\n#### Model Lifecycle Management\n\nOur educational implementation focuses on training and prediction but lacks the comprehensive model lifecycle management required for production deployment.\n\n**Model Lifecycle Components:**\n\n| Lifecycle Stage | Current Implementation | Production Requirements |\n|----------------|------------------------|-------------------------|\n| **Training** | One-time fit() call | Automated retraining pipelines with data freshness triggers |\n| **Validation** | Basic R-squared metric | Comprehensive model validation including fairness and bias testing |\n| **Deployment** | Direct Python object usage | Containerized deployment with version management and rollback capability |\n| **Monitoring** | No post-deployment tracking | Real-time performance monitoring with automated alerting |\n| **Maintenance** | Manual code updates | Automated model updates based on performance degradation detection |\n\nProduction systems require **MLOps infrastructure** that automates model training, validation, deployment, and monitoring. This includes CI/CD pipelines for model code, automated testing of model performance, and infrastructure for safe model rollouts.\n\n**Production Deployment Architecture:**\n\n1. **Model Serving Infrastructure**: REST/gRPC APIs with load balancing and auto-scaling for prediction requests\n2. **Feature Pipeline Management**: Real-time feature computation and caching for low-latency predictions\n3. **Model Performance Monitoring**: Track prediction accuracy, inference latency, and model drift over time\n4. **Automated Retraining**: Trigger model updates based on performance degradation or new data availability\n5. **Safety and Compliance**: Implement model explainability, audit trails, and bias monitoring for regulated industries\n\n> **Production Readiness Principle**: The transition from educational to production code requires shifting focus from **algorithmic understanding** to **operational reliability**. The core mathematical concepts remain the same, but the supporting infrastructure expands dramatically.\n\n#### Security and Privacy Considerations\n\nProduction machine learning systems must address data privacy, model security, and regulatory compliance requirements that are outside the scope of our educational implementation.\n\n**Security and Privacy Requirements:**\n\n| Aspect | Educational Scope | Production Requirements |\n|--------|------------------|-------------------------|\n| **Data Privacy** | Public datasets | PII protection, encryption at rest and in transit |\n| **Model Security** | Open model parameters | Secure model deployment, protection against adversarial attacks |\n| **Access Control** | No authentication | Role-based access control for training data and models |\n| **Audit Logging** | Basic training history | Comprehensive audit trails for regulatory compliance |\n| **Data Governance** | No retention policies | Data retention, right-to-be-forgotten, consent management |\n\nProduction systems may require **differential privacy** techniques during training, **federated learning** approaches for distributed sensitive data, and **secure multi-party computation** for collaborative model training without data sharing.\n\n### Implementation Guidance\n\nThis implementation guidance provides concrete next steps for extending your linear regression foundation toward more advanced machine learning concepts. The progression follows natural complexity gradients while building upon your established architecture patterns.\n\n#### Technology Stack Evolution\n\n| Component | Current Foundation | Immediate Extensions | Advanced Production |\n|-----------|-------------------|---------------------|-------------------|\n| **Core Computation** | NumPy | SciPy (optimization), Pandas (data) | CuPy (GPU), Dask (distributed) |\n| **Visualization** | Matplotlib | Seaborn (statistical plots) | Plotly (interactive), TensorBoard (training) |\n| **Data Handling** | CSV files | Pandas (multiple formats) | Apache Arrow, Parquet |\n| **Model Persistence** | Pickle | Joblib (NumPy optimization) | MLflow, ONNX |\n| **Web APIs** | None | Flask/FastAPI | Kubernetes, Docker |\n\n#### Project Structure Evolution\n\nYour current project structure provides an excellent foundation for extensions. Here's how to organize advanced features:\n\n```\nlinear_regression_project/\n├── core/                          # Your existing core implementation\n│   ├── data_handler.py           # Current DataHandler\n│   ├── simple_regression.py      # SimpleLinearRegression\n│   ├── gradient_descent.py       # GradientDescentRegression\n│   └── multiple_regression.py    # MultipleLinearRegression\n├── extensions/                    # New extensions directory\n│   ├── __init__.py\n│   ├── polynomial_features.py    # Polynomial feature generation\n│   ├── regularization.py         # L1, Elastic Net extensions\n│   ├── cross_validation.py       # K-fold validation framework\n│   └── model_selection.py        # Hyperparameter tuning\n├── advanced/                      # Advanced algorithm implementations\n│   ├── __init__.py\n│   ├── logistic_regression.py    # Classification extension\n│   ├── neural_network.py         # Basic neural network\n│   └── optimizers.py             # Advanced optimization algorithms\n├── production/                    # Production-ready components\n│   ├── __init__.py\n│   ├── model_serving.py          # REST API for predictions\n│   ├── pipeline.py               # End-to-end ML pipeline\n│   └── monitoring.py             # Model performance tracking\n├── examples/                      # Comprehensive examples\n│   ├── polynomial_regression_demo.py\n│   ├── logistic_regression_demo.py\n│   └── production_pipeline_demo.py\n└── tests/                         # Extended testing\n    ├── test_extensions.py\n    ├── test_advanced.py\n    └── test_production.py\n```\n\n#### Polynomial Features Implementation Starter\n\nThis complete implementation extends your `DataHandler` with polynomial feature generation:\n\n```python\n# extensions/polynomial_features.py\nimport numpy as np\nfrom itertools import combinations_with_replacement\nfrom typing import List, Tuple\nimport warnings\n\nclass PolynomialFeatureGenerator:\n    \"\"\"Generates polynomial features for non-linear regression.\"\"\"\n    \n    def __init__(self, degree: int = 2, include_bias: bool = True, interaction_only: bool = False):\n        \"\"\"Initialize polynomial feature generator.\n        \n        Args:\n            degree: Maximum polynomial degree\n            include_bias: Whether to include constant (bias) term\n            interaction_only: If True, only include interaction terms (no powers)\n        \"\"\"\n        self.degree = degree\n        self.include_bias = include_bias\n        self.interaction_only = interaction_only\n        self.feature_names_: List[str] = []\n        self.n_input_features_: int = 0\n        self.n_output_features_: int = 0\n        \n    def fit(self, X: np.ndarray, feature_names: List[str] = None) -> 'PolynomialFeatureGenerator':\n        \"\"\"Learn the polynomial feature combinations from input data.\"\"\"\n        # TODO 1: Validate input array X for proper shape and data type\n        # TODO 2: Store number of input features for later validation\n        # TODO 3: Generate feature names if not provided (use \"x0\", \"x1\", etc.)\n        # TODO 4: Calculate all polynomial feature combinations up to specified degree\n        # TODO 5: Store feature names for interpretability\n        # TODO 6: Calculate total number of output features for memory planning\n        # Hint: Use itertools.combinations_with_replacement for feature combinations\n        pass\n    \n    def transform(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Transform input features to polynomial features.\"\"\"\n        # TODO 1: Validate that fit() has been called first\n        # TODO 2: Check input feature count matches training data\n        # TODO 3: Initialize output array with correct shape\n        # TODO 4: Add bias column if include_bias is True\n        # TODO 5: Generate polynomial features for each combination\n        # TODO 6: Handle potential overflow in polynomial computation\n        # Hint: Consider using np.column_stack for efficient array building\n        pass\n    \n    def fit_transform(self, X: np.ndarray, feature_names: List[str] = None) -> np.ndarray:\n        \"\"\"Fit and transform in one step.\"\"\"\n        return self.fit(X, feature_names).transform(X)\n    \n    def get_feature_names_out(self, input_features: List[str] = None) -> List[str]:\n        \"\"\"Get descriptive names for output features.\"\"\"\n        # TODO: Return stored feature names for interpretability\n        pass\n\n# Extension to your existing DataHandler\nclass PolynomialDataHandler:\n    \"\"\"Extends DataHandler with polynomial feature generation.\"\"\"\n    \n    def __init__(self, base_handler, degree: int = 2):\n        self.base_handler = base_handler\n        self.poly_generator = PolynomialFeatureGenerator(degree=degree)\n        self.is_fitted_ = False\n        \n    def prepare_polynomial_features(self, features: np.ndarray, fit: bool = True) -> np.ndarray:\n        \"\"\"Prepare polynomial features for training or prediction.\"\"\"\n        # TODO 1: Apply base data handling (normalization, validation)\n        # TODO 2: Generate polynomial features using fit_transform or transform\n        # TODO 3: Apply feature scaling to polynomial features\n        # TODO 4: Store transformation state for consistent prediction preprocessing\n        # Hint: Polynomial features often have very different scales than original features\n        pass\n```\n\n#### Cross-Validation Framework Starter\n\nThis implementation provides a complete cross-validation framework that integrates with your existing models:\n\n```python\n# extensions/cross_validation.py\nimport numpy as np\nfrom typing import List, Tuple, Dict, Any\nfrom sklearn.model_selection import KFold\nimport warnings\n\nclass CrossValidator:\n    \"\"\"K-fold cross-validation for regression models.\"\"\"\n    \n    def __init__(self, k: int = 5, shuffle: bool = True, random_state: int = None):\n        self.k = k\n        self.shuffle = shuffle\n        self.random_state = random_state\n        self.cv_results_: Dict[str, List[float]] = {}\n        \n    def validate_model(self, model_class, X: np.ndarray, y: np.ndarray, \n                      model_params: Dict[str, Any] = None) -> Dict[str, float]:\n        \"\"\"Perform k-fold cross-validation on a model.\"\"\"\n        # TODO 1: Initialize KFold splitter with specified parameters\n        # TODO 2: Initialize result storage for each fold\n        # TODO 3: Iterate through each fold split\n        # TODO 4: Train model on training fold using model_class\n        # TODO 5: Evaluate model on validation fold\n        # TODO 6: Store metrics (R-squared, MSE, etc.) for each fold\n        # TODO 7: Compute aggregate statistics (mean, std) across folds\n        # TODO 8: Return comprehensive results dictionary\n        # Hint: Handle potential fitting failures gracefully with try-catch\n        pass\n    \n    def hyperparameter_search(self, model_class, X: np.ndarray, y: np.ndarray,\n                             param_grid: Dict[str, List[Any]]) -> Dict[str, Any]:\n        \"\"\"Grid search with cross-validation for hyperparameter tuning.\"\"\"\n        # TODO 1: Generate all combinations of parameters from grid\n        # TODO 2: For each parameter combination, run cross-validation\n        # TODO 3: Track best performance and corresponding parameters\n        # TODO 4: Return best parameters and detailed results\n        # Hint: Use itertools.product for parameter combination generation\n        pass\n\ndef simple_train_test_split(X: np.ndarray, y: np.ndarray, \n                          test_size: float = 0.2, random_state: int = None) -> Tuple:\n    \"\"\"Simple implementation of train-test split.\"\"\"\n    # TODO 1: Set random seed if provided for reproducibility\n    # TODO 2: Calculate split indices based on test_size\n    # TODO 3: Randomly shuffle indices if desired\n    # TODO 4: Split features and targets based on calculated indices\n    # TODO 5: Return train and test sets\n    pass\n```\n\n#### Logistic Regression Implementation Skeleton\n\nThis skeleton demonstrates the architectural similarity between linear and logistic regression:\n\n```python\n# advanced/logistic_regression.py\nimport numpy as np\nfrom typing import Optional\nimport warnings\n\nclass LogisticRegression:\n    \"\"\"Binary logistic regression using gradient descent.\"\"\"\n    \n    def __init__(self, learning_rate: float = 0.01, max_iterations: int = 1000, \n                 tolerance: float = 1e-6):\n        # TODO: Initialize parameters identical to your LinearRegression class\n        # Hint: Same optimization parameters, different cost function\n        pass\n    \n    def _sigmoid(self, z: np.ndarray) -> np.ndarray:\n        \"\"\"Compute sigmoid activation function.\"\"\"\n        # TODO 1: Implement sigmoid function: 1 / (1 + exp(-z))\n        # TODO 2: Handle numerical overflow for very large negative z values\n        # TODO 3: Clip extreme values to prevent numerical instability\n        # Hint: Use np.clip to prevent overflow in exponential\n        pass\n    \n    def _compute_cost(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray) -> float:\n        \"\"\"Compute logistic regression cost (cross-entropy loss).\"\"\"\n        # TODO 1: Compute linear combination: z = X @ weights\n        # TODO 2: Apply sigmoid to get probabilities: p = sigmoid(z)\n        # TODO 3: Compute cross-entropy: -[y*log(p) + (1-y)*log(1-p)]\n        # TODO 4: Return average cost across all samples\n        # Hint: Add small epsilon to prevent log(0) errors\n        pass\n    \n    def _compute_gradients(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray) -> np.ndarray:\n        \"\"\"Compute gradients of logistic cost function.\"\"\"\n        # TODO 1: Compute predictions using current weights\n        # TODO 2: Calculate prediction errors (predictions - targets)\n        # TODO 3: Compute gradient: (1/m) * X.T @ errors\n        # TODO 4: Return gradient vector\n        # Hint: Gradient computation is very similar to linear regression!\n        pass\n    \n    def fit(self, X: np.ndarray, y: np.ndarray) -> 'LogisticRegression':\n        \"\"\"Train logistic regression model using gradient descent.\"\"\"\n        # TODO 1: Validate binary classification targets (0 and 1 only)\n        # TODO 2: Add intercept column to feature matrix\n        # TODO 3: Initialize weights (same as linear regression)\n        # TODO 4: Implement gradient descent loop (reuse your existing pattern!)\n        # TODO 5: Track cost history and check convergence\n        # TODO 6: Store final weights and set fitted flag\n        # Hint: Training loop structure identical to LinearRegression\n        pass\n    \n    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Predict class probabilities.\"\"\"\n        # TODO 1: Validate model is fitted\n        # TODO 2: Add intercept column to input features\n        # TODO 3: Compute linear combination with learned weights\n        # TODO 4: Apply sigmoid to get probabilities\n        # Hint: This is your prediction pipeline\n        pass\n    \n    def predict(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Predict binary class labels (0 or 1).\"\"\"\n        # TODO 1: Get probability predictions\n        # TODO 2: Apply threshold (0.5) to convert probabilities to classes\n        # TODO 3: Return binary predictions\n        pass\n```\n\n#### Production API Serving Starter\n\nThis complete implementation shows how to serve your trained models via REST API:\n\n```python\n# production/model_serving.py\nfrom flask import Flask, request, jsonify\nimport numpy as np\nimport pickle\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, Any\nimport traceback\n\napp = Flask(__name__)\nlogging.basicConfig(level=logging.INFO)\n\nclass ModelServer:\n    \"\"\"Production model serving with REST API.\"\"\"\n    \n    def __init__(self):\n        self.models = {}\n        self.prediction_history = []\n        \n    def load_model(self, model_path: str, model_name: str):\n        \"\"\"Load trained model from disk.\"\"\"\n        try:\n            with open(model_path, 'rb') as f:\n                model = pickle.load(f)\n            self.models[model_name] = {\n                'model': model,\n                'loaded_at': datetime.now(),\n                'prediction_count': 0\n            }\n            logging.info(f\"Loaded model {model_name} from {model_path}\")\n        except Exception as e:\n            logging.error(f\"Failed to load model {model_name}: {str(e)}\")\n            raise\n    \n    def predict(self, model_name: str, features: np.ndarray) -> Dict[str, Any]:\n        \"\"\"Make prediction with error handling and logging.\"\"\"\n        # TODO 1: Validate model exists and is loaded\n        # TODO 2: Validate input features shape and data types\n        # TODO 3: Make prediction using loaded model\n        # TODO 4: Log prediction request and response\n        # TODO 5: Update model usage statistics\n        # TODO 6: Return prediction with metadata\n        pass\n\n# Global model server instance\nmodel_server = ModelServer()\n\n@app.route('/health', methods=['GET'])\ndef health_check():\n    \"\"\"Health check endpoint for load balancers.\"\"\"\n    return jsonify({\n        'status': 'healthy',\n        'models_loaded': len(model_server.models),\n        'timestamp': datetime.now().isoformat()\n    })\n\n@app.route('/predict/<model_name>', methods=['POST'])\ndef predict(model_name: str):\n    \"\"\"Main prediction endpoint.\"\"\"\n    try:\n        # TODO 1: Parse JSON request body to get features\n        # TODO 2: Validate required fields are present\n        # TODO 3: Convert features to NumPy array\n        # TODO 4: Call model_server.predict() with features\n        # TODO 5: Return prediction results as JSON\n        # TODO 6: Handle errors gracefully with appropriate HTTP status codes\n        pass\n    except Exception as e:\n        logging.error(f\"Prediction error: {str(e)}\\n{traceback.format_exc()}\")\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/models', methods=['GET'])\ndef list_models():\n    \"\"\"List all loaded models with metadata.\"\"\"\n    # TODO: Return information about all loaded models\n    pass\n\nif __name__ == '__main__':\n    # Load your trained models\n    # model_server.load_model('path/to/your/trained_model.pkl', 'linear_regression_v1')\n    app.run(host='0.0.0.0', port=5000, debug=False)\n```\n\n#### Milestone Checkpoints for Extensions\n\n**Polynomial Features Checkpoint:**\nAfter implementing polynomial feature generation, verify:\n1. **Synthetic Data Test**: Generate quadratic relationship `y = x² + noise`, fit polynomial model with degree=2, should achieve R² > 0.95\n2. **Feature Count Verification**: Input with 3 features and degree=2 should generate 9 polynomial features\n3. **Overfitting Detection**: High-degree polynomials on small datasets should show training R² >> test R²\n\n**Cross-Validation Checkpoint:**\nAfter implementing k-fold validation:\n1. **Stability Test**: 5-fold CV on same data should give similar mean R² across multiple runs\n2. **Hyperparameter Selection**: CV should identify optimal polynomial degree that balances training and validation performance\n3. **Statistical Significance**: Standard deviation across folds should be reasonable (typically < 0.1 for R²)\n\n**Logistic Regression Checkpoint:**\nAfter implementing classification:\n1. **Binary Classification**: Fit model on binary classification data, should achieve > 85% accuracy on linearly separable data\n2. **Probability Calibration**: Predicted probabilities should be well-calibrated (predictions near 0.7 should be correct ~70% of time)\n3. **Decision Boundary**: Visualization should show clear linear decision boundary for 2D data\n\n**Production API Checkpoint:**\nAfter deploying model serving:\n1. **Load Test**: API should handle 100 concurrent prediction requests without errors\n2. **Latency Test**: Single prediction should complete in < 100ms\n3. **Error Handling**: Invalid inputs should return appropriate HTTP error codes with helpful messages\n\n\n## Glossary\n\n> **Milestone(s):** All milestones (foundational terminology that applies across M1: Simple Linear Regression, M2: Gradient Descent, M3: Multiple Linear Regression)\n\nThis glossary provides comprehensive definitions for all technical terms, mathematical concepts, and domain-specific vocabulary used throughout the linear regression implementation. Understanding these terms is essential for grasping both the theoretical foundations and practical implementation details of machine learning algorithms.\n\n### Mental Model: The Language of Machine Learning\n\nThink of this glossary as a translation dictionary for a foreign country you're visiting for the first time. Machine learning has its own specialized vocabulary that can seem intimidating initially, but each term represents a concrete concept with practical implications. Just as learning key phrases helps you navigate a new city, mastering these terms will help you navigate the world of machine learning with confidence. Each definition bridges the gap between mathematical theory and computational implementation.\n\n### Mathematical and Statistical Terms\n\nThe mathematical foundation of linear regression relies on several key statistical and optimization concepts that form the theoretical backbone of the implementation.\n\n| Term | Definition | Context in Project |\n|------|------------|-------------------|\n| **ordinary least squares** | Standard linear regression optimization method that finds the line minimizing the sum of squared residuals between actual and predicted values | Primary method used in M1 for closed-form solution |\n| **normal equations** | Analytical solution to the least squares problem that directly computes optimal parameters without iteration: slope = Σ((x-x̄)(y-ȳ))/Σ((x-x̄)²) | Implemented in `SimpleLinearRegression.fit()` for direct parameter calculation |\n| **least squares** | Optimization principle that minimizes the sum of squared differences between observed and predicted values | Fundamental principle underlying both closed-form and iterative solutions |\n| **coefficient of determination** | Statistical measure (R-squared) representing the proportion of variance in the dependent variable explained by the independent variables, ranging from 0 to 1 | Computed by `score()` method to evaluate model quality |\n| **residuals** | Difference between actual target values and predicted values: residual = y_actual - y_predicted | Used in cost function computation and model evaluation throughout all milestones |\n| **gradient** | Vector of partial derivatives of the cost function with respect to each parameter, indicating the direction of steepest increase | Core of gradient descent algorithm in M2, computed by `_compute_gradients()` |\n| **cost function** | Objective function that measures model performance, specifically mean squared error: (1/2m)Σ(y_pred - y_actual)² | Minimized during training in M2 and M3, implemented in `_compute_cost()` |\n| **learning rate** | Step size parameter controlling how much parameters change during each gradient descent iteration | Critical hyperparameter in M2 and M3, affects convergence speed and stability |\n| **convergence** | State reached when optimization algorithm finds a stable solution where further iterations produce minimal improvement | Detected by multiple criteria in M2 and M3 to determine when to stop training |\n\n### Optimization and Algorithm Terms\n\nThe iterative optimization aspects of gradient descent introduce specialized terminology for understanding how algorithms learn from data.\n\n| Term | Definition | Context in Project |\n|------|------------|-------------------|\n| **gradient descent** | Iterative optimization algorithm that updates parameters by moving in the direction opposite to the gradient | Primary training method in M2 and M3, implemented in `GradientDescentRegression.fit()` |\n| **batch gradient descent** | Variant of gradient descent that computes gradients using the entire training dataset in each iteration | Standard approach used throughout M2 and M3 for stable convergence |\n| **parameter update rule** | Core gradient descent equation: θ_new = θ_old - α * ∇J(θ), where α is learning rate and ∇J is gradient | Fundamental step implemented in all gradient descent components |\n| **convergence detection** | Process of determining when optimization has reached a satisfactory solution using multiple criteria | Implemented with cost tolerance, gradient magnitude, and parameter change thresholds |\n| **gradient magnitude** | L2 norm (Euclidean length) of the gradient vector, indicating how steep the cost function is at current parameters | Used in convergence detection to identify when gradients become sufficiently small |\n| **cost improvement** | Decrease in cost function value between consecutive iterations, indicating training progress | Monitored to ensure algorithm is making progress and to detect convergence |\n| **gradient explosion** | Condition where gradients become extremely large, often indicating numerical instability or inappropriate learning rate | Detected by `NumericalStabilityChecker` to prevent parameter divergence |\n| **parameter divergence** | Situation where parameters grow without bound due to optimization problems, typically from excessive learning rate | Monitored continuously during training to trigger error handling |\n\n### Data Handling and Preprocessing Terms\n\nData preparation and validation introduce terminology related to ensuring data quality and numerical stability.\n\n| Term | Definition | Context in Project |\n|------|------------|-------------------|\n| **z-score normalization** | Standardization technique that transforms features to have zero mean and unit variance: z = (x - μ)/σ | Essential preprocessing step in M3, implemented in `StandardScaler` |\n| **feature scaling** | Process of transforming input features to comparable ranges to improve optimization performance | Critical for multiple linear regression to ensure fair parameter updates |\n| **design matrix** | Feature matrix augmented with a column of ones to represent the intercept term in matrix form | Constructed in M3 to enable vectorized operations: X_design = [1, x1, x2, ...] |\n| **data validation** | Comprehensive process of checking input data for compatibility with regression requirements | Implemented in `DataValidator` to catch issues before training begins |\n| **broadcasting** | NumPy mechanism that automatically aligns arrays of different shapes during arithmetic operations | Enables efficient vectorized operations without explicit loops in M3 |\n| **vectorization** | Technique of using matrix operations instead of explicit loops to improve computational efficiency | Central to M3 implementation for handling multiple features simultaneously |\n| **data leakage** | Problematic situation where information from the future or target variable inappropriately influences training | Prevented through careful data splitting and preprocessing isolation |\n| **fail-fast principle** | Design philosophy of generating immediate, informative errors rather than allowing silent failures | Implemented throughout data validation to help learners identify issues quickly |\n\n### Model Architecture and Component Terms\n\nThe system architecture introduces terminology for understanding how components interact and manage state.\n\n| Term | Definition | Context in Project |\n|------|------------|-------------------|\n| **closed-form solution** | Direct analytical calculation that produces exact results without iteration, using mathematical formulas | Implemented in M1 `SimpleLinearRegression` for educational comparison with iterative methods |\n| **hyperplane** | Multi-dimensional generalization of a plane that separates or fits through data points in feature space | What multiple linear regression fits: a hyperplane through n-dimensional feature space |\n| **regularization** | Technique that adds penalty terms to the cost function to prevent overfitting by constraining parameter complexity | Implemented as L2 regularization in M3 `MultipleLinearRegression` |\n| **Ridge regression** | Linear regression variant that includes L2 regularization: cost = MSE + λΣ(weights²) | Available option in M3 for handling overfitting with many features |\n| **matrix conditioning** | Numerical measure of how sensitive matrix operations are to small changes in input values | Monitored to detect ill-conditioned design matrices that cause numerical instability |\n| **numerical stability** | Property of algorithms that remain accurate despite floating-point precision limitations and computational errors | Critical concern addressed throughout M2 and M3 with stability checking |\n| **overflow protection** | Safeguards that prevent computational values from exceeding representable floating-point ranges | Implemented in parameter update rules to maintain numerical validity |\n| **underflow detection** | Mechanisms that identify when values become too small to represent accurately in floating-point arithmetic | Used in gradient magnitude checking to detect vanishing gradients |\n\n### Testing and Validation Terms\n\nThe testing strategy introduces terminology for ensuring implementation correctness and educational effectiveness.\n\n| Term | Definition | Context in Project |\n|------|------------|-------------------|\n| **unit testing** | Testing approach that validates individual components in isolation using synthetic data with known outcomes | Applied to each component separately: data handling, model fitting, optimization |\n| **integration testing** | End-to-end testing that validates the complete system using real datasets and cross-validation | Ensures components work together correctly across all milestones |\n| **milestone checkpoints** | Specific success criteria and behavioral expectations that must be met after completing each learning stage | Defined for M1, M2, and M3 to provide clear progress indicators |\n| **synthetic data** | Artificially generated datasets with known mathematical properties, ideal for validating implementations | Created by `generate_linear_data()` with controlled noise and known parameters |\n| **cross-validation** | Technique for assessing model generalization by training and testing on different data subsets | Used in integration testing to ensure models generalize beyond training data |\n| **mathematical verification** | Process of validating implementation correctness by comparing results against analytical solutions | Critical for ensuring gradient calculations match finite difference approximations |\n| **edge case testing** | Systematic testing of system behavior under adverse conditions like missing data or extreme values | Ensures robustness and appropriate error handling throughout the system |\n| **finite differences** | Numerical approximation technique for computing derivatives: f'(x) ≈ (f(x+h) - f(x))/h | Used in gradient checking to validate analytical gradient computations |\n| **gradient checking** | Validation technique that compares analytical gradients to numerical approximations for correctness | Essential debugging tool implemented to catch gradient computation errors |\n\n### Debugging and Error Handling Terms\n\nThe debugging infrastructure introduces specialized terminology for diagnosing and resolving machine learning problems.\n\n| Term | Definition | Context in Project |\n|------|------------|-------------------|\n| **convergence validation** | Process of verifying that gradient descent successfully reaches a stable, optimal solution | Implemented with multiple criteria to ensure training quality |\n| **numerical precision** | Accuracy of floating-point computations, limited by machine representation and algorithmic stability | Monitored throughout optimization to prevent precision loss |\n| **parameter clipping** | Technique of constraining parameter values to safe ranges to prevent numerical overflow | Applied when parameters exceed `MAX_PARAMETER_VALUE` threshold |\n| **learning rate adaptation** | Process of automatically adjusting the step size during optimization based on training behavior | Suggested by diagnostic tools when convergence problems are detected |\n| **validation strictness** | Design balance between preventing errors and allowing experimentation for educational purposes | Tuned to provide helpful guidance without being overly restrictive |\n\n### Advanced Concepts and Extensions\n\nTerms related to future extensions and advanced machine learning concepts that build on this foundation.\n\n| Term | Definition | Context in Project |\n|------|------------|-------------------|\n| **polynomial features** | Transformed features that include powers and interactions of original features to capture non-linear relationships | Introduced in future extensions as natural progression from linear models |\n| **k-fold** | Cross-validation method that partitions data into k equal subsets for robust model evaluation | Implemented in advanced testing for model validation |\n| **hyperparameter tuning** | Systematic process of selecting optimal configuration parameters like learning rate and regularization strength | Essential skill for practical machine learning applications |\n| **feature engineering** | Process of creating new features from existing ones to improve model performance and capture domain knowledge | Natural extension of preprocessing concepts introduced in data handling |\n| **sigmoid function** | Logistic function σ(z) = 1/(1 + e^(-z)) that maps real numbers to probabilities between 0 and 1 | Foundation for logistic regression extension |\n| **cross-entropy loss** | Cost function for classification problems that measures the quality of probability predictions | Used in logistic regression as alternative to mean squared error |\n| **overfitting** | Phenomenon where models memorize training data rather than learning generalizable patterns | Addressed by regularization and proper validation techniques |\n| **model serving** | Production deployment of trained models via APIs for real-time predictions | Represents the practical application of the educational implementation |\n| **production readiness** | Set of capabilities required for reliable deployment in real-world environments | Encompasses robustness, scalability, and monitoring requirements |\n\n### Implementation Guidance\n\nThis implementation guidance provides practical direction for understanding and applying the terminology throughout the linear regression project.\n\n#### A. Technology Recommendations Table\n\n| Concept Category | Basic Understanding | Advanced Application |\n|-----------------|-------------------|---------------------|\n| Mathematical Terms | Focus on intuitive understanding through examples | Study formal mathematical proofs and derivations |\n| Algorithm Terms | Implement step-by-step with detailed logging | Optimize for computational efficiency and numerical stability |\n| Data Terms | Practice with small, well-understood datasets | Handle real-world messy data with missing values and outliers |\n| Architecture Terms | Build simple, clear implementations | Design for extensibility and maintainability |\n\n#### B. Recommended Study Approach\n\nThe terminology in this glossary follows a specific learning progression that mirrors the project milestones:\n\n**Milestone 1 Focus Terms:**\n- ordinary least squares, normal equations, least squares\n- coefficient of determination, residuals\n- closed-form solution\n- unit testing, synthetic data\n\n**Milestone 2 Addition Terms:**\n- gradient descent, batch gradient descent, parameter update rule\n- cost function, learning rate, convergence\n- gradient magnitude, convergence detection\n- numerical stability, gradient checking\n\n**Milestone 3 Extension Terms:**\n- z-score normalization, feature scaling, design matrix\n- vectorization, broadcasting, hyperplane\n- regularization, Ridge regression, matrix conditioning\n- cross-validation, hyperparameter tuning\n\n#### C. Terminology Application in Code\n\nEach term in this glossary corresponds to specific implementation patterns:\n\n```python\n# Example of term application in method documentation\ndef _compute_gradients(self, x: np.ndarray, y: np.ndarray, slope: float, intercept: float) -> Tuple[float, float]:\n    \"\"\"\n    Compute partial derivatives of cost function with respect to parameters.\n    \n    This implements the gradient calculation for batch gradient descent,\n    computing the gradient magnitude for convergence detection.\n    \n    Uses vectorization to avoid explicit loops and improve numerical stability.\n    \"\"\"\n    # Implementation follows terminology definitions\n```\n\n#### D. Common Terminology Mistakes\n\n| Mistake | Correct Understanding | Impact |\n|---------|----------------------|--------|\n| Confusing \"gradient\" with \"slope\" | Gradient is vector of partial derivatives; slope is single parameter | Misunderstanding multivariable optimization |\n| Using \"accuracy\" for regression | Use R-squared or mean squared error for regression evaluation | Inappropriate evaluation metrics |\n| Calling feature scaling \"normalization\" | Distinguish z-score normalization from min-max scaling | Different preprocessing effects |\n| Confusing \"convergence\" with \"completion\" | Convergence means reaching stable solution, not just finishing iterations | Misunderstanding optimization quality |\n\n#### E. Debugging with Terminology\n\nWhen encountering issues, use precise terminology to diagnose problems:\n\n- \"Parameter divergence\" indicates learning rate too high\n- \"Gradient explosion\" suggests numerical instability\n- \"Poor convergence\" may indicate insufficient iterations or inappropriate tolerance\n- \"Low R-squared\" suggests model doesn't fit data well\n\n#### F. Milestone Terminology Checkpoints\n\n**After Milestone 1:** Learners should comfortably use terms related to least squares, closed-form solutions, and basic model evaluation.\n\n**After Milestone 2:** Vocabulary should expand to include gradient descent terminology, convergence concepts, and optimization language.\n\n**After Milestone 3:** Complete terminology mastery including matrix operations, regularization, and advanced preprocessing concepts.\n\nEach checkpoint represents not just vocabulary acquisition but deep conceptual understanding that enables effective communication about machine learning concepts and debugging of implementation issues.\n"}