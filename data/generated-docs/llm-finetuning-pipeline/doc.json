{"html":"<h1 id=\"llm-fine-tuning-pipeline-design-document\">LLM Fine-tuning Pipeline: Design Document</h1>\n<h2 id=\"overview\">Overview</h2>\n<p>An end-to-end system for efficiently fine-tuning large language models using parameter-efficient techniques like LoRA and QLoRA, with integrated dataset preparation and comprehensive evaluation. The key architectural challenge is balancing memory efficiency, training stability, and model quality while handling multi-gigabyte models on consumer hardware.</p>\n<blockquote>\n<p>This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.</p>\n</blockquote>\n<h2 id=\"context-and-problem-statement\">Context and Problem Statement</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - this section establishes the foundational understanding needed for the entire fine-tuning pipeline</p>\n</blockquote>\n<p>The landscape of large language models has fundamentally shifted the paradigm of natural language processing, but this revolution comes with a critical constraint: the computational and memory requirements for fine-tuning these billion-parameter models have grown beyond the reach of most practitioners and organizations. While a pre-trained model like Llama 2 7B represents an extraordinary achievement in language understanding, adapting it to specific domains or tasks using traditional fine-tuning approaches requires resources that are prohibitively expensive for most use cases.</p>\n<p>This section explores the core challenges that make traditional fine-tuning impractical for large language models, introduces parameter-efficient alternatives that democratize model customization, and establishes the technical foundation for understanding why LoRA and QLoRA have emerged as the dominant approaches for practical LLM fine-tuning.</p>\n<h3 id=\"the-workshop-apprentice-mental-model\">The Workshop Apprentice Mental Model</h3>\n<p>To understand the essence of fine-tuning large language models, imagine a <strong>master craftsperson</strong> who has spent decades perfecting their trade—let&#39;s say a violin maker who has internalized centuries of woodworking knowledge, acoustic principles, and aesthetic traditions. This craftsperson represents our pre-trained language model: they possess vast, general knowledge accumulated through extensive training on diverse examples.</p>\n<p>Now suppose you want this master craftsperson to specialize in creating instruments for a specific musical tradition—perhaps traditional Irish fiddles with their distinctive sound characteristics and construction techniques. You have two approaches:</p>\n<p><strong>The Traditional Apprenticeship (Full Fine-tuning)</strong>: You could ask the master craftsperson to forget everything they know and start completely over, learning their entire craft again but with exclusive focus on Irish fiddle-making techniques. This approach would require them to re-examine every single skill they&#39;ve developed, from basic wood selection to advanced finishing techniques. While this might produce exceptional Irish fiddle expertise, it&#39;s extraordinarily wasteful—you&#39;re discarding decades of valuable knowledge and starting from scratch. Moreover, the time and resources required would be enormous, and there&#39;s a risk that in focusing so narrowly, they might lose the broader craftsmanship wisdom that made them a master in the first place.</p>\n<p><strong>The Skill Overlay Approach (Parameter-Efficient Fine-tuning)</strong>: Alternatively, you could teach the master craftsperson a set of specialized techniques that build upon their existing knowledge. You might introduce them to specific wood types used in Irish instruments, particular carving patterns that enhance traditional sound qualities, or specialized varnishing techniques that preserve the wood&#39;s resonance characteristics. The craftsperson retains all their foundational knowledge—understanding of wood grain, basic carving principles, acoustic theory—but adds a focused layer of domain-specific expertise. This approach is faster, more efficient, and preserves the valuable general knowledge while adding specialized capabilities.</p>\n<p>In this analogy, <strong>LoRA (Low-Rank Adaptation)</strong> represents the skill overlay approach. Instead of retraining every parameter in the model (equivalent to relearning every craftsmanship skill), LoRA adds small, specialized adaptation layers that modify how the existing knowledge is applied to specific tasks. The base model&#39;s parameters remain frozen—preserving all the general language understanding—while lightweight adapter modules learn task-specific transformations.</p>\n<p><strong>QLoRA</strong> extends this analogy further: imagine that our master craftsperson has accumulated so many tools and materials over their career that their workshop has become prohibitively large and expensive to maintain. QLoRA is like organizing the workshop with an extremely efficient storage system that compresses the most commonly used tools and materials without losing their functionality. The craftsperson can still access everything they need, but the workshop now fits in a much smaller, more affordable space. This compression (4-bit quantization) makes it possible to work with the master craftsperson even if you don&#39;t have access to a vast workshop facility.</p>\n<p>The key insight from this mental model is that <strong>fine-tuning should preserve and build upon existing knowledge rather than replacing it</strong>. Pre-trained language models have learned incredibly sophisticated patterns about language, reasoning, and world knowledge from massive datasets. Parameter-efficient methods recognize this value and ask: &quot;How can we add new capabilities without discarding what the model already knows?&quot;</p>\n<p>This approach has profound implications for both technical implementation and practical deployment. From a technical perspective, it means we need systems that can selectively modify model behavior while keeping most parameters unchanged. From a practical perspective, it means that fine-tuning becomes accessible to organizations and individuals who cannot afford the computational resources required for full model retraining.</p>\n<h3 id=\"the-memory-wall-problem\">The Memory Wall Problem</h3>\n<p>The fundamental challenge facing modern LLM fine-tuning is what we term the <strong>Memory Wall Problem</strong>—the exponential growth in GPU memory requirements that has outpaced the availability and affordability of high-memory hardware. To understand this problem&#39;s severity, we need to examine the memory requirements across different model sizes and training approaches.</p>\n<p>Consider the memory footprint of training a language model. During standard fine-tuning, the GPU must simultaneously hold:</p>\n<ol>\n<li><strong>Model Parameters</strong>: The actual weights of the neural network</li>\n<li><strong>Gradients</strong>: Partial derivatives for each parameter during backpropagation  </li>\n<li><strong>Optimizer States</strong>: Additional parameters maintained by optimizers like Adam (momentum and variance estimates)</li>\n<li><strong>Activation Memory</strong>: Intermediate computations stored for gradient calculation</li>\n<li><strong>Batch Data</strong>: Input tokens and attention masks for the current training batch</li>\n</ol>\n<p>For a 7-billion parameter model using 32-bit floating point precision, the base model weights alone require approximately 28 GB of memory. However, this is just the beginning. During training with the Adam optimizer, each parameter requires storage for the parameter itself, its gradient, and two optimizer state variables (momentum and variance). This quadruples the memory requirement to roughly 112 GB just for the optimization process, before accounting for activation memory and batch data.</p>\n<p>The situation becomes even more challenging with larger models. A 13-billion parameter model requires approximately 208 GB for full fine-tuning, while a 70-billion parameter model would need over 1 TB of GPU memory—far exceeding the capacity of even the most expensive consumer and professional GPUs available today.</p>\n<table>\n<thead>\n<tr>\n<th>Model Size</th>\n<th>Parameters</th>\n<th>Base Model (FP32)</th>\n<th>Full Training Memory</th>\n<th>Accessible Hardware</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>7B</td>\n<td>7 billion</td>\n<td>28 GB</td>\n<td>~112 GB</td>\n<td>Requires 8x A100 (40GB)</td>\n</tr>\n<tr>\n<td>13B</td>\n<td>13 billion</td>\n<td>52 GB</td>\n<td>~208 GB</td>\n<td>Requires 16x A100 (40GB)</td>\n</tr>\n<tr>\n<td>30B</td>\n<td>30 billion</td>\n<td>120 GB</td>\n<td>~480 GB</td>\n<td>Requires 32+ A100 (40GB)</td>\n</tr>\n<tr>\n<td>70B</td>\n<td>70 billion</td>\n<td>280 GB</td>\n<td>~1.1 TB</td>\n<td>Impossible on current hardware</td>\n</tr>\n</tbody></table>\n<p>This memory wall has created a practical barrier that excludes most researchers, developers, and organizations from fine-tuning large language models. Even organizations with substantial computational budgets find the costs prohibitive for experimental work or iterative development.</p>\n<p><strong>Parameter-efficient methods fundamentally reshape this equation</strong> by dramatically reducing the trainable parameter count while preserving model capability. LoRA, for example, typically reduces trainable parameters by 99% or more. Instead of fine-tuning 7 billion parameters, LoRA might adapt only 4-8 million parameters through low-rank decomposition matrices. This reduction has cascading effects throughout the memory hierarchy:</p>\n<ul>\n<li><strong>Reduced Optimizer Memory</strong>: With 99% fewer trainable parameters, optimizer states shrink proportionally</li>\n<li><strong>Simplified Gradient Storage</strong>: Only adapter parameters require gradient computation and storage</li>\n<li><strong>Preserved Base Model</strong>: The frozen base model can be loaded in reduced precision without affecting adaptation quality</li>\n</ul>\n<p>QLoRA amplifies these benefits through 4-bit quantization of the base model. By representing the frozen parameters in 4-bit NormalFloat format instead of 32-bit floating point, the base model memory footprint shrinks by approximately 75%. A 7-billion parameter model that previously required 28 GB can be stored in roughly 7 GB, bringing it within reach of consumer-grade GPUs.</p>\n<p>The combination of LoRA and QLoRA creates a <strong>memory efficiency multiplier effect</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>7B Model Memory</th>\n<th>Reduction Factor</th>\n<th>Consumer GPU Feasible?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Full Fine-tuning</td>\n<td>~112 GB</td>\n<td>1x</td>\n<td>No</td>\n</tr>\n<tr>\n<td>LoRA (FP16 base)</td>\n<td>~16 GB</td>\n<td>7x</td>\n<td>Barely (high-end)</td>\n</tr>\n<tr>\n<td>QLoRA (4-bit base)</td>\n<td>~6 GB</td>\n<td>18x</td>\n<td>Yes (RTX 4090, etc.)</td>\n</tr>\n</tbody></table>\n<p>This transformation is not merely quantitative but qualitative—it changes who can participate in LLM development and what kinds of experimentation become feasible. The memory wall problem had created a practical monopoly where only large corporations and well-funded research institutions could customize large language models. Parameter-efficient methods democratize this capability, enabling widespread innovation and specialization.</p>\n<p>However, this efficiency comes with trade-offs that must be carefully managed. Quantization introduces small quality degradations, rank limitations in LoRA can constrain adaptation capacity, and the interaction between quantization and low-rank adaptation requires careful hyperparameter tuning. Understanding these trade-offs is essential for designing effective fine-tuning pipelines.</p>\n<h3 id=\"existing-fine-tuning-approaches\">Existing Fine-tuning Approaches</h3>\n<p>The evolution of language model fine-tuning has progressed through several distinct approaches, each addressing different aspects of the memory and computational challenges while making various trade-offs between adaptation quality, efficiency, and implementation complexity. Understanding these approaches and their trade-offs is crucial for making informed decisions about fine-tuning strategy.</p>\n<p><strong>Full Fine-tuning</strong> represents the traditional approach where all model parameters are updated during training. This method treats the pre-trained model as an initialization point and allows the optimization process to modify every weight in the network. While conceptually straightforward and theoretically optimal for task adaptation, full fine-tuning suffers from the memory wall problem described above and introduces additional risks.</p>\n<p>The primary concern beyond memory requirements is <strong>catastrophic forgetting</strong>—the tendency for neural networks to lose previously learned capabilities when adapting to new tasks. When all parameters are trainable, aggressive learning rates can cause the model to overwrite important general knowledge encoded in the pre-trained weights. This is particularly problematic for language models, where general linguistic competence is as valuable as task-specific performance.</p>\n<p><strong>Low-Rank Adaptation (LoRA)</strong> introduces a fundamentally different paradigm based on the insight that adaptation to new tasks likely occurs in a much lower-dimensional space than the full parameter space. Instead of updating the original weight matrices directly, LoRA decomposes updates into two smaller matrices that, when multiplied together, approximate the full adaptation.</p>\n<p>For a pre-trained weight matrix W₀ with dimensions d×k, LoRA introduces two trainable matrices: A (d×r) and B (r×k), where r is the adaptation rank and is much smaller than both d and k. During forward passes, the adapted weight becomes W₀ + αBA, where α is a scaling parameter. This decomposition reduces trainable parameters from d×k to r(d+k), achieving dramatic parameter reduction when r &lt;&lt; min(d,k).</p>\n<p>The mathematical foundation of LoRA rests on the hypothesis that adaptation primarily occurs in a low-dimensional subspace of the full parameter space. This hypothesis has empirical support—research has shown that the effective rank of fine-tuning updates is often much lower than the dimensionality of the weight matrices being adapted.</p>\n<p><strong>QLoRA (Quantized LoRA)</strong> combines LoRA with aggressive quantization techniques to achieve maximum memory efficiency while preserving adaptation quality. The key innovation is the introduction of 4-bit NormalFloat (NF4) quantization, specifically designed for neural network weights that follow approximately normal distributions.</p>\n<p>Traditional quantization approaches use uniform quantization intervals, which are suboptimal for normally-distributed weights where most values cluster around zero. NF4 quantization adapts the quantization levels to match the expected weight distribution, providing better precision where weights are most dense while using fewer bits overall.</p>\n<p>QLoRA also introduces <strong>double quantization</strong>, where the quantization constants themselves are quantized using 8-bit precision. This provides additional memory savings with minimal quality impact, as the quantization constants exhibit different statistical properties than the weights themselves.</p>\n<p><strong>Prefix Tuning and P-Tuning v2</strong> represent alternative approaches that modify model behavior by learning task-specific prompts or prefixes rather than adapting the model weights directly. These methods prepend learnable continuous tokens to the input sequence, allowing the frozen model to adapt its behavior based on these learned prompt representations.</p>\n<p>While prompt-based methods avoid modifying model weights entirely, they face limitations in adaptation capacity and can be sensitive to the specific architecture and pre-training approach of the base model. They work particularly well for tasks that can be naturally formulated as continuation or completion problems but may struggle with tasks requiring fundamental changes in model behavior.</p>\n<p><strong>Adapter Networks</strong> insert small neural networks (adapters) between existing layers of the pre-trained model. These adapters typically consist of down-projection and up-projection layers with a bottleneck architecture that reduces dimensionality, applies a non-linear transformation, and then projects back to the original dimensionality.</p>\n<p>The advantage of adapter networks is their architectural flexibility—different adapter designs can target specific aspects of model behavior. However, they introduce additional computational overhead during inference, as the adapter computations add to the forward pass time even after training completes.</p>\n<p>Here&#39;s a comprehensive comparison of these approaches:</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Trainable Parameters</th>\n<th>Memory Efficiency</th>\n<th>Inference Speed</th>\n<th>Adaptation Quality</th>\n<th>Implementation Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Full Fine-tuning</td>\n<td>100%</td>\n<td>Poor (28-112GB for 7B model)</td>\n<td>Baseline</td>\n<td>Excellent</td>\n<td>Simple</td>\n</tr>\n<tr>\n<td>LoRA</td>\n<td>0.1-1%</td>\n<td>Good (16GB for 7B model)</td>\n<td>Baseline</td>\n<td>Very Good</td>\n<td>Moderate</td>\n</tr>\n<tr>\n<td>QLoRA</td>\n<td>0.1-1%</td>\n<td>Excellent (6GB for 7B model)</td>\n<td>Baseline</td>\n<td>Good</td>\n<td>Complex</td>\n</tr>\n<tr>\n<td>Prefix Tuning</td>\n<td>0.01-0.1%</td>\n<td>Excellent</td>\n<td>Slightly slower</td>\n<td>Good</td>\n<td>Moderate</td>\n</tr>\n<tr>\n<td>Adapter Networks</td>\n<td>0.5-2%</td>\n<td>Good</td>\n<td>10-20% slower</td>\n<td>Very Good</td>\n<td>Moderate</td>\n</tr>\n</tbody></table>\n<p>The trade-offs between these approaches create different optimal choices depending on constraints and requirements:</p>\n<blockquote>\n<p><strong>Decision: LoRA + QLoRA as Primary Approach</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to balance memory efficiency, adaptation quality, and practical implementation constraints for a general-purpose fine-tuning pipeline</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Full fine-tuning with gradient checkpointing and model parallelism</li>\n<li>Pure LoRA with 16-bit base model</li>\n<li>QLoRA combining 4-bit quantization with LoRA adaptation</li>\n<li>Prefix tuning for maximum memory efficiency</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement QLoRA as the primary method with LoRA as a fallback option</li>\n<li><strong>Rationale</strong>: QLoRA provides the optimal balance of memory efficiency (enabling consumer GPU usage), adaptation quality (preserving 95%+ of full fine-tuning performance), and inference speed (no overhead after adapter merging). The 4-bit quantization makes fine-tuning accessible while the low-rank adaptation preserves flexibility for various task types.</li>\n<li><strong>Consequences</strong>: Requires implementing 4-bit quantization support and careful hyperparameter tuning, but enables fine-tuning on consumer hardware and democratizes access to LLM customization</li>\n</ul>\n</blockquote>\n<p><strong>Emerging Approaches</strong> continue to push the boundaries of parameter-efficient fine-tuning. AdaLoRA (Adaptive LoRA) dynamically adjusts the rank allocation across different layers and training steps, concentrating adaptation capacity where it&#39;s most needed. DoRA (Weight-Decomposed Low-Rank Adaptation) separates weight updates into magnitude and direction components, providing more fine-grained control over adaptation behavior.</p>\n<p>These emerging methods highlight the active research area of parameter-efficient fine-tuning and suggest that the field will continue evolving toward more sophisticated approaches that balance efficiency, quality, and flexibility.</p>\n<p>The choice of fine-tuning approach ultimately depends on the specific constraints and requirements of each use case. However, the combination of memory efficiency, adaptation quality, and practical accessibility makes QLoRA the most compelling choice for a general-purpose fine-tuning pipeline. Understanding the principles underlying these approaches—low-rank decomposition, quantization, and selective parameter updating—provides the foundation for implementing robust and efficient fine-tuning systems.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The concepts and trade-offs described above translate into specific technical decisions and implementation strategies. This guidance bridges the gap between understanding the approaches conceptually and implementing them effectively.</p>\n<p><strong>A. Technology Recommendations Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Base Framework</td>\n<td>PyTorch + Transformers library</td>\n<td>PyTorch + Custom CUDA kernels</td>\n</tr>\n<tr>\n<td>Quantization</td>\n<td>bitsandbytes (4-bit NF4)</td>\n<td>Custom quantization with TensorRT</td>\n</tr>\n<tr>\n<td>LoRA Implementation</td>\n<td>PEFT library (Parameter Efficient Fine-Tuning)</td>\n<td>Manual low-rank matrix implementation</td>\n</tr>\n<tr>\n<td>Training Orchestration</td>\n<td>HuggingFace Trainer API</td>\n<td>Custom training loop with DeepSpeed</td>\n</tr>\n<tr>\n<td>Memory Optimization</td>\n<td>Gradient checkpointing + mixed precision</td>\n<td>ZeRO optimizer with gradient compression</td>\n</tr>\n<tr>\n<td>Model Export</td>\n<td>HuggingFace format + GGUF conversion</td>\n<td>Custom serialization with optimization passes</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File Structure:</strong></p>\n<p>The fine-tuning pipeline should be organized to separate concerns clearly while maintaining easy discoverability for debugging and extension:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>llm-finetuning-pipeline/\n├── src/\n│   ├── data/\n│   │   ├── __init__.py\n│   │   ├── loaders.py              ← Dataset loading from multiple formats\n│   │   ├── processors.py           ← Chat template and tokenization\n│   │   └── splitters.py            ← Train/validation splitting\n│   ├── models/\n│   │   ├── __init__.py\n│   │   ├── quantization.py         ← 4-bit model loading and config\n│   │   ├── lora_config.py          ← LoRA adapter configuration\n│   │   └── adapters.py             ← Adapter injection and management\n│   ├── training/\n│   │   ├── __init__.py\n│   │   ├── trainer.py              ← Main training orchestration\n│   │   ├── optimization.py         ← Learning rate, gradient accumulation\n│   │   └── callbacks.py            ← Checkpointing, early stopping\n│   ├── evaluation/\n│   │   ├── __init__.py\n│   │   ├── metrics.py              ← Perplexity and task-specific evaluation\n│   │   ├── benchmarks.py           ← Standard benchmark implementations\n│   │   └── exporters.py            ← Model merging and format conversion\n│   └── utils/\n│       ├── __init__.py\n│       ├── memory_utils.py         ← GPU memory monitoring\n│       ├── logging_config.py       ← Structured logging setup\n│       └── config_parser.py        ← Configuration validation\n├── configs/\n│   ├── base_config.yaml            ← Default configuration template\n│   ├── model_configs/              ← Model-specific configurations\n│   └── task_configs/               ← Task-specific training configurations\n├── tests/\n│   ├── unit/                       ← Component-level tests\n│   ├── integration/                ← Cross-component tests\n│   └── fixtures/                   ← Test data and mock models\n└── examples/\n    ├── basic_finetuning.py         ← Simple end-to-end example\n    ├── advanced_config.py          ← Complex configuration example\n    └── evaluation_only.py          ← Model evaluation without training</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code:</strong></p>\n<p>The following utilities handle common infrastructure concerns that aren&#39;t the core learning focus but are necessary for effective implementation:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/utils/memory_utils.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> psutil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MemoryMonitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Tracks GPU and system memory usage throughout training.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.baseline_gpu_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.baseline_system_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.measurements </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> capture_baseline</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record initial memory state before model loading.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            torch.cuda.empty_cache()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.baseline_gpu_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_allocated()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.baseline_system_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.virtual_memory().used</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logger.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Baseline GPU memory: </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.baseline_gpu_memory </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1e9</span><span style=\"color:#F97583\">:.2f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> GB\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logger.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Baseline system memory: </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.baseline_system_memory </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1e9</span><span style=\"color:#F97583\">:.2f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> GB\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> measure_current_usage</span><span style=\"color:#E1E4E8\">(self, stage: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Measure current memory usage and log the difference from baseline.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        current_measurement </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'stage'</span><span style=\"color:#E1E4E8\">: stage,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'timestamp'</span><span style=\"color:#E1E4E8\">: torch.cuda.Event(</span><span style=\"color:#FFAB70\">enable_timing</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available() </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            current_gpu </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_allocated()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            max_gpu </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.max_memory_allocated()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            current_measurement.update({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'gpu_used_gb'</span><span style=\"color:#E1E4E8\">: current_gpu </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1e9</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'gpu_delta_gb'</span><span style=\"color:#E1E4E8\">: (current_gpu </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.baseline_gpu_memory) </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1e9</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'gpu_max_gb'</span><span style=\"color:#E1E4E8\">: max_gpu </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1e9</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        current_system </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.virtual_memory().used</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        current_measurement.update({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'system_used_gb'</span><span style=\"color:#E1E4E8\">: current_system </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1e9</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'system_delta_gb'</span><span style=\"color:#E1E4E8\">: (current_system </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.baseline_system_memory) </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1e9</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'system_available_gb'</span><span style=\"color:#E1E4E8\">: psutil.virtual_memory().available </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1e9</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.measurements.append(current_measurement)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logger.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Memory at </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">stage</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: GPU </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">current_measurement.get(</span><span style=\"color:#9ECBFF\">'gpu_used_gb'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span><span style=\"color:#F97583\">:.2f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> GB, \"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                   f</span><span style=\"color:#9ECBFF\">\"System </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">current_measurement[</span><span style=\"color:#9ECBFF\">'system_used_gb'</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">:.2f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> GB\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> current_measurement</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_peak_usage</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return peak memory usage across all measurements.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.measurements:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        peak_stats </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            peak_stats[</span><span style=\"color:#9ECBFF\">'peak_gpu_gb'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(m.get(</span><span style=\"color:#9ECBFF\">'gpu_used_gb'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> m </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.measurements)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            peak_stats[</span><span style=\"color:#9ECBFF\">'peak_gpu_delta_gb'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(m.get(</span><span style=\"color:#9ECBFF\">'gpu_delta_gb'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> m </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.measurements)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        peak_stats[</span><span style=\"color:#9ECBFF\">'peak_system_gb'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(m[</span><span style=\"color:#9ECBFF\">'system_used_gb'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> m </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.measurements)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        peak_stats[</span><span style=\"color:#9ECBFF\">'peak_system_delta_gb'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(m[</span><span style=\"color:#9ECBFF\">'system_delta_gb'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> m </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.measurements)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> peak_stats</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># src/utils/config_parser.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> yaml</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, List, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> QuantizationConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for 4-bit model quantization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    load_in_4bit: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bnb_4bit_quant_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"nf4\"</span><span style=\"color:#6A737D\">  # nf4 or fp4</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bnb_4bit_compute_dtype: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"float16\"</span><span style=\"color:#6A737D\">  # float16, bfloat16, or float32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bnb_4bit_use_double_quant: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#6A737D\">  # Double quantization for extra memory savings</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> LoRAConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for LoRA adapter settings.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    r: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 16</span><span style=\"color:#6A737D\">  # Rank of the low-rank decomposition</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    alpha: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 32</span><span style=\"color:#6A737D\">  # LoRA scaling parameter (typically 2x rank)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dropout: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#6A737D\">  # Dropout applied to LoRA layers</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    target_modules: Optional[List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">  # Auto-detected if None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bias: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"none\"</span><span style=\"color:#6A737D\">  # \"none\", \"all\", or \"lora_only\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    task_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"CAUSAL_LM\"</span><span style=\"color:#6A737D\">  # Task type for PEFT library</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TrainingConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Main training configuration combining all aspects.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Model settings</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    model_name_or_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    quantization: QuantizationConfig </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">QuantizationConfig)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lora: LoRAConfig </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">LoRAConfig)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Training parameters</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    learning_rate: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2e-4</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_train_epochs: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    per_device_train_batch_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gradient_accumulation_steps: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 8</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    warmup_ratio: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_grad_norm: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Data settings</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_seq_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2048</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    train_data_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    validation_split: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Output settings</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    output_dir: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"./outputs\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    save_steps: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 500</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    eval_steps: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 500</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logging_steps: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> load_config_from_yaml</span><span style=\"color:#E1E4E8\">(config_path: Path) -> TrainingConfig:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Load training configuration from YAML file with validation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> config_path.exists():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> FileNotFoundError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Configuration file not found: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">config_path</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(config_path, </span><span style=\"color:#9ECBFF\">'r'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        yaml_config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> yaml.safe_load(f)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Convert nested dictionaries to appropriate config objects</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#9ECBFF\"> 'quantization'</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> yaml_config:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        yaml_config[</span><span style=\"color:#9ECBFF\">'quantization'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> QuantizationConfig(</span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">yaml_config[</span><span style=\"color:#9ECBFF\">'quantization'</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#9ECBFF\"> 'lora'</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> yaml_config:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        yaml_config[</span><span style=\"color:#9ECBFF\">'lora'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> LoRAConfig(</span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">yaml_config[</span><span style=\"color:#9ECBFF\">'lora'</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> TrainingConfig(</span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">yaml_config)</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton:</strong></p>\n<p>The following skeleton provides the structure for the main fine-tuning coordinator that learners will implement:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/training/finetuning_pipeline.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> transformers </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> AutoTokenizer, AutoModelForCausalLM</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> peft </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> get_peft_model, LoraConfig</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> FineTuningPipeline</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Orchestrates the complete fine-tuning process from data loading to model export.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This class coordinates all components of the fine-tuning pipeline:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    - Dataset preparation and validation</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    - Model loading with quantization</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    - LoRA adapter configuration and injection</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    - Training execution with monitoring</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    - Evaluation and adapter merging</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: TrainingConfig, memory_monitor: MemoryMonitor):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.memory_monitor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> memory_monitor</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokenizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.train_dataset </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.eval_dataset </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> setup_model_and_tokenizer</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Load the base model with quantization and prepare tokenizer.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        This method handles the critical first step of loading the pre-trained model</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        in quantized format and setting up the tokenizer with appropriate special tokens.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load tokenizer from config.model_name_or_path</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Add any missing special tokens (pad_token, eos_token)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Configure quantization parameters using BitsAndBytesConfig</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Load model with quantization config and torch_dtype</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Resize token embeddings if new tokens were added</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Measure memory usage after model loading</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use AutoTokenizer.from_pretrained() and AutoModelForCausalLM.from_pretrained()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Set device_map=\"auto\" for automatic GPU placement</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> setup_lora_adapters</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Configure and inject LoRA adapters into the quantized model.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        This method transforms the frozen base model into a parameter-efficient</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        fine-tuning model by adding trainable low-rank adapter matrices.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create LoraConfig with rank, alpha, dropout, and target modules</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If target_modules is None, auto-detect based on model architecture</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Apply PEFT to the model using get_peft_model()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Print trainable parameter statistics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Verify that base model parameters are frozen</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Measure memory usage after adapter injection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use model.named_modules() to find attention and MLP layers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Different model architectures use different layer names (q_proj, k_proj, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> prepare_datasets</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Load, validate, and preprocess training data for instruction fine-tuning.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        This method handles the complete data preparation pipeline from raw data</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        to tokenized datasets ready for training.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load raw data from config.train_data_path (support JSON, JSONL, CSV)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate that required fields (instruction, response) are present</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Apply chat template to format instruction-response pairs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Tokenize the formatted text with appropriate padding and truncation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Split into train and validation sets based on config.validation_split</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Create DataLoader objects with appropriate batch size and collation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use tokenizer.apply_chat_template() for consistent formatting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Set max_length=config.max_seq_length to control memory usage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> execute_training</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Run the fine-tuning training loop with monitoring and checkpointing.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            str: Path to the best checkpoint directory</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Configure TrainingArguments with learning rate, batch size, etc.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set up learning rate scheduler (linear warmup + cosine decay)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Create Trainer with model, datasets, and training arguments</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Add callbacks for early stopping and memory monitoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Execute training with trainer.train()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Save final checkpoint and return path to best checkpoint</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use save_strategy=\"steps\" and evaluation_strategy=\"steps\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Monitor training loss and validation perplexity for early stopping</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> evaluate_and_merge</span><span style=\"color:#E1E4E8\">(self, checkpoint_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Evaluate the fine-tuned model and merge adapters for deployment.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            checkpoint_path: Path to the best training checkpoint</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Dict containing evaluation metrics and export paths</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load the best checkpoint and set model to evaluation mode</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate perplexity on the validation set</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Run task-specific evaluation metrics (BLEU, ROUGE, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Merge LoRA adapters back into the base model</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Save the merged model in HuggingFace format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Export to GGUF format for inference optimization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use model.merge_and_unload() to combine adapters with base weights</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use torch.no_grad() during evaluation for memory efficiency</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> run_complete_pipeline</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Execute the entire fine-tuning pipeline from start to finish.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Dict containing training results, evaluation metrics, and export paths</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Capture baseline memory measurements</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Setup model and tokenizer with quantization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Configure and inject LoRA adapters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Prepare and validate datasets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Execute training loop with monitoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Evaluate results and merge adapters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return comprehensive results dictionary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Wrap each step with try-catch for error handling and cleanup</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Log memory usage after each major step</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints:</strong></p>\n<ul>\n<li><p><strong>PyTorch Memory Management</strong>: Use <code>torch.cuda.empty_cache()</code> between major pipeline stages to free unused GPU memory. Be aware that PyTorch caches memory allocations, so <code>torch.cuda.memory_allocated()</code> shows actual usage while <code>nvidia-smi</code> shows cached memory.</p>\n</li>\n<li><p><strong>Transformers Library Integration</strong>: The <code>transformers</code> library automatically handles device placement when using <code>device_map=&quot;auto&quot;</code>, but be careful with custom operations that might not respect the device mapping.</p>\n</li>\n<li><p><strong>PEFT Library Conventions</strong>: The PEFT library expects specific naming conventions for target modules. Use <code>print(model.named_modules())</code> to discover the exact layer names for your model architecture before configuring <code>target_modules</code>.</p>\n</li>\n<li><p><strong>Gradient Checkpointing</strong>: Enable with <code>model.gradient_checkpointing_enable()</code> to trade computation time for memory usage. This is particularly useful when combined with quantization.</p>\n</li>\n<li><p><strong>Mixed Precision Training</strong>: Use <code>torch.cuda.amp.autocast()</code> and <code>GradScaler</code> for automatic mixed precision, but ensure your quantization and compute dtypes are compatible.</p>\n</li>\n</ul>\n<p><strong>F. Milestone Checkpoint:</strong></p>\n<p>After implementing the core pipeline, verify correct behavior with these checkpoints:</p>\n<ol>\n<li><p><strong>Memory Efficiency Verification</strong>: Load a 7B model with QLoRA and confirm GPU usage stays below 8GB. Expected output: &quot;Model loaded in 4-bit precision, using ~6GB VRAM&quot;</p>\n</li>\n<li><p><strong>Parameter Efficiency Check</strong>: Print trainable parameter counts and verify less than 1% of total parameters are trainable. Expected: &quot;Trainable params: 8,388,608 || all params: 6,738,415,616 || trainable%: 0.12&quot;</p>\n</li>\n<li><p><strong>Training Smoke Test</strong>: Run one training step and verify loss decreases from random initialization. Expected: Loss should decrease from ~10-11 to ~8-9 after several steps.</p>\n</li>\n<li><p><strong>Adapter Functionality</strong>: Generate text before and after fine-tuning to verify behavioral changes. Expected: Model should show task-specific improvements while retaining general capabilities.</p>\n</li>\n</ol>\n<p><strong>G. Common Setup Issues:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>&quot;CUDA out of memory&quot; during model loading</td>\n<td>Model too large for GPU or quantization not applied</td>\n<td>Check GPU memory with <code>nvidia-smi</code></td>\n<td>Verify <code>load_in_4bit=True</code> and reduce batch size</td>\n</tr>\n<tr>\n<td>&quot;No trainable parameters found&quot;</td>\n<td>LoRA target modules don&#39;t match model architecture</td>\n<td>Print <code>model.named_modules()</code></td>\n<td>Update <code>target_modules</code> to match actual layer names</td>\n</tr>\n<tr>\n<td>Training loss stays constant</td>\n<td>Learning rate too low or wrong optimizer state</td>\n<td>Check learning rate scheduler output</td>\n<td>Increase learning rate or verify gradient flow</td>\n</tr>\n<tr>\n<td>bitsandbytes import error</td>\n<td>CUDA version mismatch with compiled library</td>\n<td>Check CUDA version with <code>nvcc --version</code></td>\n<td>Reinstall bitsandbytes with correct CUDA support</td>\n</tr>\n</tbody></table>\n<h2 id=\"goals-and-non-goals\">Goals and Non-Goals</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - this section establishes the functional and performance targets that guide implementation decisions across the entire fine-tuning pipeline</p>\n</blockquote>\n<h3 id=\"the-resource-optimization-mental-model-the-efficiency-consultant\">The Resource Optimization Mental Model: The Efficiency Consultant</h3>\n<p>Think of our fine-tuning pipeline as an efficiency consultant brought in to transform a massive corporate training program. The consultant&#39;s job is to take a company-wide training initiative that would normally require an enormous conference center, hundreds of full-time trainers, and months of everyone&#39;s undivided attention, and instead make it work with a small meeting room, a few part-time specialists, and minimal disruption to daily operations.</p>\n<p>Just as the efficiency consultant must clearly define what results the streamlined training program must deliver (improved employee skills, measurable competency gains, certification outcomes) while explicitly stating what it won&#39;t attempt (complete career changes, personality transformations, or training in unrelated fields), our fine-tuning pipeline must establish clear success criteria and boundaries. The consultant knows that trying to do everything leads to doing nothing well - the same principle applies to our system architecture.</p>\n<p>The goals we set determine every subsequent design decision, from how we structure our memory optimization to which evaluation metrics we prioritize. Without clear boundaries, we risk building a system that attempts to solve every possible fine-tuning scenario but excels at none.</p>\n<h3 id=\"functional-goals-core-capabilities-the-system-must-provide\">Functional Goals: Core Capabilities the System Must Provide</h3>\n<p>Our fine-tuning pipeline must deliver a complete, reliable pathway from raw training data to a deployable fine-tuned model. The system&#39;s functional goals represent the minimum viable capabilities that define success.</p>\n<p><strong>End-to-End Pipeline Automation</strong></p>\n<p>The system must orchestrate the entire fine-tuning workflow without requiring manual intervention between stages. A user should be able to point the system at a dataset and base model, provide a configuration file, and receive a fine-tuned model ready for deployment. This automation includes data validation, format conversion, model preparation, training execution, and evaluation reporting.</p>\n<p>The pipeline must handle failures gracefully, providing clear error messages that guide users toward resolution. When a training run fails due to out-of-memory conditions, the system should suggest specific configuration adjustments (reduced batch size, lower LoRA rank, or quantization settings) rather than generic error messages.</p>\n<p><strong>Multi-Format Data Ingestion and Standardization</strong></p>\n<p>The data preparation component must accept training data in multiple common formats and convert them to a standardized internal representation. The supported input formats include JSON files with instruction-response pairs, JSONL streams for large datasets, CSV files with configurable column mapping, and Parquet files for efficient columnar access.</p>\n<table>\n<thead>\n<tr>\n<th>Input Format</th>\n<th>Required Fields</th>\n<th>Optional Fields</th>\n<th>Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>JSON</td>\n<td><code>instruction</code>, <code>response</code></td>\n<td><code>input</code>, <code>system</code>, <code>conversation_id</code></td>\n<td>Small structured datasets</td>\n</tr>\n<tr>\n<td>JSONL</td>\n<td><code>instruction</code>, <code>response</code></td>\n<td><code>input</code>, <code>system</code>, <code>conversation_id</code></td>\n<td>Large streaming datasets</td>\n</tr>\n<tr>\n<td>CSV</td>\n<td>Configurable columns</td>\n<td>Any additional columns</td>\n<td>Tabular data from databases</td>\n</tr>\n<tr>\n<td>Parquet</td>\n<td>Configurable columns</td>\n<td>Any additional columns</td>\n<td>Big data workflows</td>\n</tr>\n</tbody></table>\n<p>The system must validate that required fields are present and contain meaningful content, filtering out empty responses, malformed instructions, or samples that exceed token length limits. Data quality metrics should be reported, including duplicate detection rates, average token lengths, and filtering statistics.</p>\n<p><strong>Parameter-Efficient Fine-Tuning with Memory Optimization</strong></p>\n<p>The core functional requirement is enabling fine-tuning of billion-parameter models on consumer hardware through parameter-efficient techniques. The system must implement LoRA (Low-Rank Adaptation) with configurable rank and alpha parameters, automatically detecting appropriate target modules based on the model architecture.</p>\n<table>\n<thead>\n<tr>\n<th>Model Architecture</th>\n<th>Default Target Modules</th>\n<th>Memory Reduction</th>\n<th>Typical Rank Range</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Llama/Mistral</td>\n<td><code>q_proj</code>, <code>v_proj</code>, <code>o_proj</code>, <code>gate_proj</code>, <code>up_proj</code>, <code>down_proj</code></td>\n<td>90-95%</td>\n<td>8-64</td>\n</tr>\n<tr>\n<td>GPT-2/GPT-J</td>\n<td><code>attn.c_attn</code>, <code>attn.c_proj</code>, <code>mlp.c_fc</code>, <code>mlp.c_proj</code></td>\n<td>85-90%</td>\n<td>8-32</td>\n</tr>\n<tr>\n<td>T5/FLAN-T5</td>\n<td><code>q</code>, <code>v</code>, <code>o</code>, <code>wi</code>, <code>wo</code></td>\n<td>90-95%</td>\n<td>8-64</td>\n</tr>\n</tbody></table>\n<p>The system must support QLoRA quantization using 4-bit NormalFloat format, with configurable double quantization for additional memory savings. Memory usage should be monitored throughout the process, with the <code>MemoryMonitor</code> providing baseline measurements and peak usage reporting.</p>\n<p><strong>Flexible Training Configuration and Optimization</strong></p>\n<p>The training loop must support gradient accumulation to simulate large batch sizes on memory-constrained hardware. Learning rate scheduling should include linear warmup followed by cosine or linear decay, with configurable warmup steps and total training duration.</p>\n<p>The system must implement automatic mixed-precision training, using the configured compute dtype for forward and backward passes while storing weights in quantized format. Gradient clipping should prevent training instability, with configurable maximum gradient norms.</p>\n<p>Checkpoint management must save model state at regular intervals, track the best checkpoint based on validation loss, and enable training resumption from any saved checkpoint. Early stopping should halt training when validation loss fails to improve for a configurable number of evaluation steps.</p>\n<p><strong>Comprehensive Evaluation and Model Export</strong></p>\n<p>The evaluation component must calculate perplexity on held-out validation data, comparing fine-tuned performance against the base model. Task-specific evaluation should support configurable metrics relevant to the fine-tuning objective, such as instruction-following accuracy or domain-specific benchmarks.</p>\n<p>The system must merge LoRA adapter weights back into the base model, creating a standalone fine-tuned model that can be deployed without the adapter framework. Export functionality should support both HuggingFace format for compatibility with the transformers ecosystem and GGUF format for inference optimization with tools like llama.cpp.</p>\n<table>\n<thead>\n<tr>\n<th>Export Format</th>\n<th>Target Runtime</th>\n<th>Quantization Support</th>\n<th>Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HuggingFace</td>\n<td>Transformers library</td>\n<td>Native PyTorch precision</td>\n<td>Development and research</td>\n</tr>\n<tr>\n<td>GGUF</td>\n<td>llama.cpp, Ollama</td>\n<td>Multiple quantization levels</td>\n<td>Production inference</td>\n</tr>\n<tr>\n<td>ONNX</td>\n<td>ONNX Runtime</td>\n<td>INT8, FP16 quantization</td>\n<td>Cross-platform deployment</td>\n</tr>\n</tbody></table>\n<h3 id=\"performance-and-resource-goals-memory-speed-and-hardware-requirements\">Performance and Resource Goals: Memory, Speed, and Hardware Requirements</h3>\n<p>The performance goals establish measurable targets for memory efficiency, training speed, and hardware compatibility that make fine-tuning accessible to practitioners with limited resources.</p>\n<p><strong>Memory Efficiency Targets</strong></p>\n<p>The primary performance goal is enabling fine-tuning of 7B parameter models on consumer GPUs with 12-16GB of VRAM. Through the combination of QLoRA quantization and LoRA adaptation, the system should reduce memory requirements by at least 75% compared to full fine-tuning approaches.</p>\n<table>\n<thead>\n<tr>\n<th>Model Size</th>\n<th>Full Fine-tuning VRAM</th>\n<th>QLoRA + LoRA VRAM</th>\n<th>Memory Reduction</th>\n<th>Target Hardware</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>7B params</td>\n<td>48-64GB</td>\n<td>12-16GB</td>\n<td>75-80%</td>\n<td>RTX 3080/4070, V100</td>\n</tr>\n<tr>\n<td>13B params</td>\n<td>96-128GB</td>\n<td>20-24GB</td>\n<td>80-85%</td>\n<td>RTX 4080/4090, A100-40GB</td>\n</tr>\n<tr>\n<td>30B params</td>\n<td>200-256GB</td>\n<td>48-64GB</td>\n<td>75-80%</td>\n<td>A100-80GB, H100</td>\n</tr>\n</tbody></table>\n<p>The <code>MemoryMonitor</code> must track actual memory usage throughout training, providing alerts when usage approaches hardware limits. Peak memory usage should remain within 90% of available VRAM to prevent out-of-memory crashes.</p>\n<p><strong>Training Speed and Throughput</strong></p>\n<p>Training speed targets focus on practical turnaround times for iterative fine-tuning workflows. The system should complete fine-tuning runs on modest datasets (1,000-10,000 samples) within hours rather than days, enabling rapid experimentation and hyperparameter tuning.</p>\n<p>Throughput targets depend on hardware capabilities and model size, but the system should achieve reasonable tokens-per-second rates with gradient accumulation compensating for smaller effective batch sizes on memory-constrained hardware.</p>\n<table>\n<thead>\n<tr>\n<th>Hardware Class</th>\n<th>Expected Tokens/Second</th>\n<th>Typical Training Time (5K samples)</th>\n<th>Batch Size Constraints</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Consumer GPU (12-16GB)</td>\n<td>500-1,000</td>\n<td>2-4 hours</td>\n<td>Micro-batch 1-2, accumulate 8-16</td>\n</tr>\n<tr>\n<td>Professional GPU (24-32GB)</td>\n<td>1,000-2,000</td>\n<td>1-2 hours</td>\n<td>Micro-batch 2-4, accumulate 4-8</td>\n</tr>\n<tr>\n<td>Data Center GPU (40-80GB)</td>\n<td>2,000-4,000</td>\n<td>30-60 minutes</td>\n<td>Micro-batch 4-8, accumulate 2-4</td>\n</tr>\n</tbody></table>\n<p><strong>Hardware Compatibility and Requirements</strong></p>\n<p>The system must support CUDA-compatible GPUs with compute capability 6.1 or higher, ensuring compatibility with the bitsandbytes library for 4-bit quantization. CPU fallback should be available for development and testing, though training performance will be significantly reduced.</p>\n<p>System memory requirements should scale with model size but remain reasonable for desktop workstations. A minimum of 32GB system RAM is recommended for 7B parameter models, with 64GB preferred for larger models to handle data loading and preprocessing overhead.</p>\n<blockquote>\n<p><strong>Critical Performance Insight</strong>: The combination of quantization and parameter-efficient fine-tuning creates a multiplicative memory reduction effect. QLoRA provides a 4x reduction in model weight storage, while LoRA reduces trainable parameters by 10-100x, resulting in overall memory savings that make large model fine-tuning feasible on consumer hardware.</p>\n</blockquote>\n<h3 id=\"non-goals-what-this-system-explicitly-does-not-handle\">Non-Goals: What This System Explicitly Does Not Handle</h3>\n<p>Clearly defining what the system does not attempt is crucial for setting appropriate expectations and maintaining focus on core capabilities. These non-goals help prevent scope creep and guide users toward complementary tools when needed.</p>\n<p><strong>Multi-GPU and Distributed Training</strong></p>\n<p>The initial system design explicitly excludes multi-GPU training and distributed fine-tuning across multiple machines. While these capabilities would improve training speed and enable larger models, they introduce significant complexity in gradient synchronization, memory coordination, and failure handling.</p>\n<p>Users requiring multi-GPU training should consider established frameworks like DeepSpeed or FairScale that specialize in distributed optimization. Our single-GPU focus allows for simpler architecture and more reliable execution on the hardware most practitioners have available.</p>\n<p><strong>Full Fine-Tuning and Alternative Parameter-Efficient Methods</strong></p>\n<p>The system does not support traditional full fine-tuning where all model parameters are updated. This decision reflects both memory constraints and the empirical evidence that parameter-efficient methods often achieve comparable performance with dramatically lower resource requirements.</p>\n<p>Alternative parameter-efficient methods like AdaLoRA, DoRA, or prompt tuning are also excluded from the initial implementation. The focus on LoRA and QLoRA provides a proven, well-supported approach that covers the majority of use cases without the complexity of supporting multiple adapter architectures.</p>\n<p><strong>Custom Model Architectures and Non-Transformer Models</strong></p>\n<p>The system targets standard transformer architectures supported by the HuggingFace transformers library. Custom model architectures, non-transformer models, or heavily modified transformer variants are out of scope.</p>\n<p>This limitation reflects the automatic target module detection logic, which relies on known naming conventions for attention and feed-forward layers. Supporting arbitrary architectures would require manual configuration for each model type, significantly increasing complexity.</p>\n<p><strong>Production Inference Serving and Deployment</strong></p>\n<p>While the system exports models in formats suitable for inference (HuggingFace and GGUF), it does not include inference serving capabilities, API endpoints, or production deployment tools. Users should utilize dedicated inference frameworks like vLLM, Text Generation Inference, or llama.cpp for serving fine-tuned models.</p>\n<p>The export functionality provides the bridge to these inference systems, but the fine-tuning pipeline itself focuses solely on the training and evaluation phases of the model lifecycle.</p>\n<p><strong>Advanced Evaluation and Safety Benchmarks</strong></p>\n<p>The evaluation component provides basic metrics like perplexity and configurable task-specific benchmarks, but it does not include comprehensive safety evaluation, bias detection, or advanced capabilities assessment.</p>\n<p>Users requiring thorough model evaluation should integrate with specialized evaluation frameworks like EleutherAI&#39;s LM Evaluation Harness or custom evaluation pipelines. The system provides the fine-tuned model and basic quality metrics needed to initiate more comprehensive evaluation workflows.</p>\n<p><strong>Data Collection, Annotation, and Active Learning</strong></p>\n<p>The system assumes training data is already collected and formatted. It does not provide data collection tools, annotation interfaces, or active learning workflows to improve dataset quality over time.</p>\n<p>Data preparation focuses on format conversion, validation, and quality filtering of existing datasets. Users need separate tools for data creation, human annotation, or iterative dataset improvement based on model performance.</p>\n<p><strong>Architecture Decision Record: Scope Limitation Strategy</strong></p>\n<blockquote>\n<p><strong>Decision: Focused Single-GPU Implementation</strong></p>\n<ul>\n<li><strong>Context</strong>: Fine-tuning systems can range from simple scripts to comprehensive platforms supporting every possible configuration and deployment scenario</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Comprehensive platform covering all fine-tuning approaches and deployment scenarios</li>\n<li>Focused single-GPU system optimizing for accessibility and reliability</li>\n<li>Modular framework allowing plugin-based extension to additional capabilities</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Focused single-GPU system with clear scope boundaries</li>\n<li><strong>Rationale</strong>: The 80/20 principle applies - most practitioners need reliable fine-tuning on consumer hardware more than they need every possible advanced feature. A focused implementation can achieve higher quality and reliability in core use cases than a sprawling system trying to handle everything. Clear non-goals prevent feature creep and guide architectural decisions toward simplicity and maintainability.</li>\n<li><strong>Consequences</strong>: Some users will need additional tools for advanced scenarios, but the core system remains approachable and reliable for the majority use case. Future extensions can add capabilities without compromising the foundation.</li>\n</ul>\n</blockquote>\n<h3 id=\"goal-achievement-metrics-and-success-criteria\">Goal Achievement Metrics and Success Criteria</h3>\n<p>To ensure the system meets its stated goals, we establish measurable criteria for success across functional and performance dimensions.</p>\n<p><strong>Functional Success Metrics</strong></p>\n<p>Pipeline completeness is measured by successful execution from raw data to deployed model without manual intervention. Success requires 95% of well-formed datasets to process without user intervention, with clear error messages guiding resolution of the remaining 5%.</p>\n<p>Data ingestion success is measured by format support coverage and quality filtering effectiveness. The system should successfully process 90% of common dataset formats found in the wild, with quality metrics showing appropriate filtering of malformed or low-quality samples.</p>\n<p>Memory optimization effectiveness is measured by comparing actual memory usage against theoretical requirements. QLoRA should achieve within 10% of theoretical 4x memory reduction, and LoRA should reduce trainable parameters to less than 1% of total model parameters.</p>\n<p><strong>Performance Success Metrics</strong></p>\n<p>Training speed targets are expressed as wall-clock time for standard benchmark tasks. Fine-tuning a 7B parameter model on 5,000 instruction-response pairs should complete within 4 hours on RTX 4080-class hardware with appropriate hyperparameters.</p>\n<p>Memory efficiency targets require peak VRAM usage to remain within hardware constraints with appropriate configuration. A 7B parameter model should fine-tune successfully on 16GB VRAM with QLoRA rank 16 and appropriate batch size settings.</p>\n<p>Quality preservation requires fine-tuned models to maintain base model capabilities while gaining task-specific performance. Perplexity on held-out general text should not increase by more than 5%, while task-specific metrics should show measurable improvement over the base model.</p>\n<p><strong>Reliability and Usability Metrics</strong></p>\n<p>System reliability is measured by failure recovery and error reporting quality. Training interruptions should resume correctly from checkpoints 100% of the time, and error messages should provide actionable guidance in 90% of failure scenarios.</p>\n<p>Configuration ease is measured by successful setup time for new users. A practitioner familiar with Python and machine learning should be able to fine-tune their first model within 30 minutes of system setup, using provided example configurations and datasets.</p>\n<p>Documentation completeness ensures all supported features have corresponding examples and common pitfalls are addressed with specific solutions rather than generic advice.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Configuration Management</td>\n<td>YAML files with Pydantic validation</td>\n<td>Hydra with hierarchical configs</td>\n</tr>\n<tr>\n<td>Memory Monitoring</td>\n<td>torch.cuda memory functions</td>\n<td>Custom CUDA memory profiler</td>\n</tr>\n<tr>\n<td>Logging</td>\n<td>Python logging with file output</td>\n<td>Weights &amp; Biases integration</td>\n</tr>\n<tr>\n<td>Data Loading</td>\n<td>PyTorch DataLoader with custom datasets</td>\n<td>HuggingFace datasets with streaming</td>\n</tr>\n<tr>\n<td>Model Loading</td>\n<td>transformers.AutoModel with manual config</td>\n<td>Custom model factory with architecture detection</td>\n</tr>\n<tr>\n<td>Evaluation</td>\n<td>Simple perplexity calculation</td>\n<td>Integration with lm-evaluation-harness</td>\n</tr>\n</tbody></table>\n<h4 id=\"configuration-schema-implementation\">Configuration Schema Implementation</h4>\n<p>The system requires a hierarchical configuration structure that captures all tunable parameters while providing sensible defaults for common use cases.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, List, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> yaml</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> QuantizationConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for 4-bit model quantization using bitsandbytes.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    load_in_4bit: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bnb_4bit_quant_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"nf4\"</span><span style=\"color:#6A737D\">  # Use NF4_QUANTIZATION constant</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bnb_4bit_compute_dtype: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"bfloat16\"</span><span style=\"color:#6A737D\">  # Compute dtype for forward pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bnb_4bit_use_double_quant: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#6A737D\">  # Double quantization for extra memory savings</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> LoRAConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for Low-Rank Adaptation parameters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    r: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 16</span><span style=\"color:#6A737D\">  # DEFAULT_LORA_RANK - rank of adaptation matrices</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    alpha: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 32</span><span style=\"color:#6A737D\">  # DEFAULT_LORA_ALPHA - scaling parameter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dropout: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#6A737D\">  # Dropout rate for LoRA layers</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    target_modules: Optional[List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">  # Auto-detected if None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bias: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"none\"</span><span style=\"color:#6A737D\">  # Bias update strategy: \"none\", \"all\", or \"lora_only\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    task_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"CAUSAL_LM\"</span><span style=\"color:#6A737D\">  # Task type for PEFT library</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TrainingConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Complete training configuration combining model, quantization, and training parameters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Model and data paths</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    model_name_or_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#6A737D\">  # HuggingFace model name or local path</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dataset_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#6A737D\">  # Path to training dataset</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    output_dir: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"./output\"</span><span style=\"color:#6A737D\">  # Directory for checkpoints and logs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Component configurations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    quantization: QuantizationConfig </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lora: LoRAConfig </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Training hyperparameters</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    learning_rate: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2e-4</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_train_epochs: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    per_device_train_batch_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gradient_accumulation_steps: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 8</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    warmup_steps: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 512</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Evaluation and checkpointing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    evaluation_strategy: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"steps\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    eval_steps: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 500</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    save_steps: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 500</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logging_steps: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize nested configurations with defaults if not provided.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.quantization </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.quantization </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> QuantizationConfig()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.lora </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.lora </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> LoRAConfig()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> load_config_from_yaml</span><span style=\"color:#E1E4E8\">(config_path: Path) -> TrainingConfig:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Load and validate training configuration from YAML file.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Provides detailed validation and helpful error messages for common</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    configuration mistakes.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Read YAML file and handle file not found errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Parse nested configuration sections (quantization, lora, training)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate required fields (model_name_or_path, dataset_path)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Apply defaults for optional fields using dataclass defaults</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate parameter ranges (learning_rate > 0, rank > 0, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return fully constructed TrainingConfig object</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"memory-monitoring-infrastructure\">Memory Monitoring Infrastructure</h4>\n<p>The memory monitoring system provides detailed tracking of GPU and system memory usage throughout the fine-tuning process, enabling optimization and debugging of memory-related issues.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> psutil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MemoryMonitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Tracks memory usage throughout training for optimization and debugging.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    baseline_gpu_memory: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">  # GPU memory before model loading</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    baseline_system_memory: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#6A737D\">  # System memory at initialization</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    measurements: List[Dict] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Time-series measurements</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> capture_baseline</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Record initial memory state before model loading.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        This baseline enables calculation of memory overhead from each</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        component (quantization, LoRA injection, training loop).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if CUDA is available and get GPU memory info</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Record current GPU memory usage if available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Record system memory usage using psutil</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Store baseline measurements with MEMORY_BASELINE_STAGE marker</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> measure_current_usage</span><span style=\"color:#E1E4E8\">(self, stage: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Measure current memory usage and return detailed statistics.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            stage: Descriptive name for current training stage</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Dictionary with GPU memory, system memory, and delta from baseline</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get current GPU memory allocated and reserved</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Get current system memory usage (RSS and virtual)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate deltas from baseline measurements</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Create measurement record with timestamp and stage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Append to measurements list for historical tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return current usage statistics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_peak_usage</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Return peak memory usage across all recorded measurements.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Useful for determining minimum hardware requirements and</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        optimizing configuration parameters.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Iterate through all measurements to find peak values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Track peak GPU memory, system memory, and deltas</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return dictionary with peak usage statistics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Memory monitoring constants</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MEMORY_BASELINE_STAGE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"baseline\"</span><span style=\"color:#6A737D\">  # Stage marker for initial measurement</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">DEFAULT_MEMORY_LOG_INTERVAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#6A737D\">  # Steps between memory measurements</span></span></code></pre></div>\n\n<h4 id=\"pipeline-orchestration-structure\">Pipeline Orchestration Structure</h4>\n<p>The main pipeline class coordinates all components and manages the end-to-end training workflow, providing a clean interface for users while handling complex component interactions internally.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> transformers </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> AutoTokenizer, AutoModelForCausalLM</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> peft </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> get_peft_model, TaskType</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> FineTuningPipeline</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Main orchestrator for the end-to-end fine-tuning pipeline.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Coordinates data preparation, model loading, LoRA setup, training,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    and evaluation while providing comprehensive logging and error handling.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: TrainingConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.memory_monitor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MemoryMonitor()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokenizer: Optional </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model: Optional </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.peft_model: Optional </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Initialize memory baseline before any model operations</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.memory_monitor.capture_baseline()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> setup_model_and_tokenizer</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Load quantized model and configure tokenizer for instruction tuning.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Handles model loading with quantization configuration and tokenizer</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        setup with appropriate special tokens and padding configuration.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load tokenizer with trust_remote_code and padding configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Add special tokens if missing (pad_token, eos_token)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Create BitsAndBytesConfig from quantization settings</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Load model with quantization config and device mapping</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Measure memory usage after model loading</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Resize token embeddings if tokenizer was modified</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> setup_lora_adapters</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Inject LoRA adapters into the frozen base model.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Automatically detects target modules based on model architecture</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        and injects low-rank adaptation matrices.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Auto-detect target modules if not specified in config</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create LoRA configuration with rank, alpha, and targets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Wrap model with PEFT adapters using get_peft_model</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Print trainable parameter summary for verification</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Measure memory usage after adapter injection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> prepare_datasets</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Load and preprocess training data with proper formatting.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Handles data loading, chat template application, tokenization,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        and train-validation splitting.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load dataset from configured path and format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Apply chat template and instruction formatting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Tokenize with truncation and padding</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Split into training and validation sets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Create DataLoader instances for training loop</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> execute_training</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Run the complete training loop with monitoring and checkpointing.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Path to the best checkpoint based on validation loss</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize trainer with model, datasets, and training arguments</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set up logging and evaluation callbacks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Configure checkpoint saving and early stopping</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Execute training with memory monitoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return path to best checkpoint for evaluation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> evaluate_and_merge</span><span style=\"color:#E1E4E8\">(self, checkpoint_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Evaluate fine-tuned model and merge adapters for deployment.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            checkpoint_path: Path to the best training checkpoint</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Dictionary containing evaluation metrics and export paths</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load checkpoint and run perplexity evaluation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Run task-specific evaluation benchmarks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Merge LoRA adapters into base model weights</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Export merged model in HuggingFace format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Export in GGUF format if requested</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return evaluation results and export paths</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> run_complete_pipeline</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Execute the complete fine-tuning pipeline from data to deployment.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        This is the main entry point for end-to-end training workflows.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Provides comprehensive error handling and progress reporting.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Dictionary containing training metrics, evaluation results,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            and paths to exported models</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Setup model and tokenizer with quantization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Inject LoRA adapters and verify parameter efficiency</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Prepare and validate training datasets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Execute training loop with monitoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Evaluate final model and merge adapters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Generate comprehensive results report</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Clean up GPU memory and temporary files</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> results</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement comprehensive error handling with cleanup</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Include memory state, configuration dump, and recovery suggestions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span></span></code></pre></div>\n\n<h4 id=\"constants-and-configuration-defaults\">Constants and Configuration Defaults</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Quantization constants</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">NF4_QUANTIZATION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"nf4\"</span><span style=\"color:#6A737D\">  # 4-bit NormalFloat quantization type</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">FP4_QUANTIZATION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"fp4\"</span><span style=\"color:#6A737D\">  # Alternative 4-bit quantization</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">INT4_QUANTIZATION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"int4\"</span><span style=\"color:#6A737D\">  # Integer 4-bit quantization</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># LoRA defaults optimized for instruction tuning</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">DEFAULT_LORA_RANK</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 16</span><span style=\"color:#6A737D\">  # Balanced rank for most use cases</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">DEFAULT_LORA_ALPHA</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 32</span><span style=\"color:#6A737D\">  # Standard alpha scaling (2x rank)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">DEFAULT_LORA_DROPOUT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#6A737D\">  # Conservative dropout rate</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Memory monitoring stages</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MEMORY_BASELINE_STAGE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"baseline\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MEMORY_MODEL_LOADED_STAGE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"model_loaded\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MEMORY_LORA_INJECTED_STAGE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"lora_injected\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MEMORY_TRAINING_START_STAGE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"training_start\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MEMORY_TRAINING_PEAK_STAGE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"training_peak\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Training defaults</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">DEFAULT_LEARNING_RATE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2e-4</span><span style=\"color:#6A737D\">  # Conservative learning rate for stability</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">DEFAULT_WARMUP_RATIO</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#6A737D\">  # 10% of training steps for warmup</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">DEFAULT_MAX_LENGTH</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 512</span><span style=\"color:#6A737D\">  # Token limit for memory efficiency</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p>After implementing this Goals and Non-Goals section, verify the following:</p>\n<p><strong>Goal Specification Checkpoint:</strong></p>\n<ol>\n<li>Run configuration validation: <code>python -c &quot;from config import load_config_from_yaml; load_config_from_yaml(&#39;test_config.yaml&#39;)&quot;</code></li>\n<li>Verify memory monitoring works: Initialize <code>MemoryMonitor</code> and call <code>capture_baseline()</code></li>\n<li>Check scope boundaries: Attempt to load multi-GPU config and verify it&#39;s rejected with clear error</li>\n<li>Validate performance targets: Test memory estimation against stated hardware requirements</li>\n</ol>\n<p><strong>Expected Behavior:</strong></p>\n<ul>\n<li>Configuration loading should provide detailed validation errors for missing required fields</li>\n<li>Memory monitoring should report current GPU and system memory usage</li>\n<li>Scope validation should clearly state what features are not supported</li>\n<li>Performance estimation should align with stated memory reduction targets</li>\n</ul>\n<p><strong>Common Issues and Fixes:</strong></p>\n<ul>\n<li>Configuration validation fails silently: Add explicit field validation with descriptive errors</li>\n<li>Memory monitoring returns zero: Check CUDA availability and torch.cuda.memory_allocated()</li>\n<li>Scope creep in implementation: Regularly review code against explicit non-goals list</li>\n</ul>\n<h2 id=\"high-level-architecture\">High-Level Architecture</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - this section establishes the architectural foundation that enables all components to work together effectively throughout the fine-tuning pipeline</p>\n</blockquote>\n<p>The LLM fine-tuning pipeline follows a modular architecture designed around the core constraint of memory efficiency while maintaining training effectiveness. Think of this system like a <strong>precision manufacturing assembly line</strong> where each station performs a specialized transformation on the raw materials (data and models) while carefully monitoring resource consumption at every step. Each component in our pipeline has a specific responsibility, much like how each station in a factory handles one aspect of production - some prepare materials, others perform the actual work, and the final stations verify quality and package the output.</p>\n<p>The architecture addresses the fundamental <strong>memory wall problem</strong> that makes traditional fine-tuning impractical for billion-parameter models on consumer hardware. By decomposing the problem into discrete, well-coordinated components, we can apply different optimization strategies at each stage while maintaining clean interfaces and error propagation paths between subsystems.</p>\n<h3 id=\"component-overview\">Component Overview</h3>\n<p>The fine-tuning pipeline consists of five major subsystems that collaborate to transform raw training data and a base model into a fine-tuned, deployment-ready model. Each component owns specific aspects of the transformation process and maintains clear boundaries with its collaborators.</p>\n<p><img src=\"/api/project/llm-finetuning-pipeline/architecture-doc/asset?path=diagrams%2Fsystem-overview.svg\" alt=\"System Component Overview\"></p>\n<p>The <strong>Dataset Preparation Component</strong> serves as the data ingestion and standardization layer. This component transforms heterogeneous input data from various formats (JSON, JSONL, CSV, Parquet) into a standardized instruction-response format that the training loop can consume efficiently. Think of this component as a <strong>language tutor preparing lesson materials</strong> - it takes raw educational content and structures it into coherent learning exercises with proper formatting, length constraints, and quality standards. The component handles chat template application, ensuring that conversational data matches the specific prompt format expected by the target model family. It also manages tokenization with appropriate padding and truncation strategies, and splits the data into training and validation sets with configurable stratification options.</p>\n<table>\n<thead>\n<tr>\n<th>Responsibility</th>\n<th>Description</th>\n<th>Input</th>\n<th>Output</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Ingestion</td>\n<td>Load training data from multiple file formats</td>\n<td>Raw files (JSON, CSV, Parquet)</td>\n<td>Validated records</td>\n</tr>\n<tr>\n<td>Format Standardization</td>\n<td>Convert to instruction-response pairs</td>\n<td>Raw conversational data</td>\n<td>Structured training samples</td>\n</tr>\n<tr>\n<td>Chat Template Application</td>\n<td>Apply model-specific prompt formatting</td>\n<td>Instruction-response pairs</td>\n<td>Templated prompts</td>\n</tr>\n<tr>\n<td>Tokenization</td>\n<td>Convert text to token sequences with proper masks</td>\n<td>Formatted text</td>\n<td>Tokenized datasets</td>\n</tr>\n<tr>\n<td>Train-Val Splitting</td>\n<td>Partition data with stratification options</td>\n<td>Complete dataset</td>\n<td>Training and validation splits</td>\n</tr>\n<tr>\n<td>Quality Filtering</td>\n<td>Remove duplicates, short samples, overlength data</td>\n<td>Raw training samples</td>\n<td>Clean, filtered dataset</td>\n</tr>\n</tbody></table>\n<p>The <strong>LoRA Configuration Component</strong> handles the setup of parameter-efficient fine-tuning adapters. This component automatically identifies target modules within the model architecture and injects low-rank matrices that will learn task-specific adaptations. The mental model here is a <strong>skill overlay system</strong> - imagine overlaying transparent sheets with new skills onto a master craftsperson&#39;s existing knowledge base, where each sheet contains incremental expertise that enhances specific capabilities without erasing the foundational knowledge. The component manages rank and alpha parameter selection, balancing adapter capacity with memory efficiency, and ensures that the vast majority of the base model parameters remain frozen during training.</p>\n<table>\n<thead>\n<tr>\n<th>Responsibility</th>\n<th>Description</th>\n<th>Input</th>\n<th>Output</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Target Module Detection</td>\n<td>Identify attention and MLP layers for adaptation</td>\n<td>Model architecture</td>\n<td>List of target module names</td>\n</tr>\n<tr>\n<td>Rank-Alpha Selection</td>\n<td>Choose low-rank dimensions and scaling factors</td>\n<td>Task requirements, memory constraints</td>\n<td>LoRA hyperparameters</td>\n</tr>\n<tr>\n<td>Adapter Initialization</td>\n<td>Create and inject low-rank matrices</td>\n<td>Target modules, LoRA config</td>\n<td>Instrumented model</td>\n</tr>\n<tr>\n<td>Parameter Analysis</td>\n<td>Count trainable vs frozen parameters</td>\n<td>Adapted model</td>\n<td>Efficiency metrics</td>\n</tr>\n<tr>\n<td>Memory Estimation</td>\n<td>Predict VRAM usage with adapters</td>\n<td>Model size, LoRA config</td>\n<td>Memory requirements</td>\n</tr>\n</tbody></table>\n<p>The <strong>QLoRA Quantization Component</strong> implements 4-bit quantization using NormalFloat precision to dramatically reduce memory consumption while preserving model quality. Think of this component as a <strong>compression expert</strong> who understands exactly which information can be stored in reduced precision without losing the essential characteristics that make the model effective. This component handles the complex orchestration of mixed-precision training where weights are stored in 4-bit format but computations happen in float16 or bfloat16 precision. It also manages double quantization, where even the quantization constants themselves are quantized for additional memory savings.</p>\n<table>\n<thead>\n<tr>\n<th>Responsibility</th>\n<th>Description</th>\n<th>Input</th>\n<th>Output</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>4-bit Model Loading</td>\n<td>Load base model in NF4 quantized format</td>\n<td>Model checkpoint path</td>\n<td>Quantized model</td>\n</tr>\n<tr>\n<td>Quantization Configuration</td>\n<td>Set up NF4, compute dtype, double quantization</td>\n<td>Memory and quality targets</td>\n<td>Quantization parameters</td>\n</tr>\n<tr>\n<td>Mixed-Precision Setup</td>\n<td>Configure storage vs compute data types</td>\n<td>Hardware capabilities</td>\n<td>Training precision config</td>\n</tr>\n<tr>\n<td>Memory Benchmarking</td>\n<td>Measure actual VRAM usage</td>\n<td>Quantized model</td>\n<td>Memory usage statistics</td>\n</tr>\n<tr>\n<td>Quality-Memory Analysis</td>\n<td>Compare perplexity across quantization levels</td>\n<td>Validation data</td>\n<td>Quality degradation metrics</td>\n</tr>\n</tbody></table>\n<p>The <strong>Training Loop Component</strong> orchestrates the actual fine-tuning process with sophisticated optimization strategies. This component acts like a <strong>personal trainer</strong> who carefully monitors progress, adjusts the training intensity, and knows when to push harder or ease back to avoid injury (catastrophic forgetting). It implements gradient accumulation to simulate large batch sizes across multiple micro-batches, manages learning rate scheduling with warmup and decay phases, and handles checkpoint saving with configurable strategies for model persistence and recovery.</p>\n<table>\n<thead>\n<tr>\n<th>Responsibility</th>\n<th>Description</th>\n<th>Input</th>\n<th>Output</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Training Orchestration</td>\n<td>Coordinate forward/backward passes with accumulation</td>\n<td>Prepared datasets, model</td>\n<td>Trained adapters</td>\n</tr>\n<tr>\n<td>Learning Rate Scheduling</td>\n<td>Manage warmup, decay, and optimization phases</td>\n<td>Training progress</td>\n<td>Dynamic learning rates</td>\n</tr>\n<tr>\n<td>Gradient Accumulation</td>\n<td>Simulate large batches across micro-batches</td>\n<td>Memory constraints</td>\n<td>Effective batch training</td>\n</tr>\n<tr>\n<td>Checkpoint Management</td>\n<td>Save model state and enable resumption</td>\n<td>Training milestones</td>\n<td>Persistent checkpoints</td>\n</tr>\n<tr>\n<td>Loss Monitoring</td>\n<td>Track convergence and detect overfitting</td>\n<td>Training metrics</td>\n<td>Early stopping signals</td>\n</tr>\n<tr>\n<td>Memory Management</td>\n<td>Monitor VRAM usage throughout training</td>\n<td>Training state</td>\n<td>Resource utilization alerts</td>\n</tr>\n</tbody></table>\n<p>The <strong>Evaluation and Merging Component</strong> measures the effectiveness of fine-tuning and prepares the model for deployment. This component functions as an <strong>exam proctor</strong> who objectively measures learning progress against established benchmarks and then packages the knowledge for real-world application. It computes perplexity on validation data, runs task-specific evaluations to measure improvement over the base model, merges the learned LoRA adapter weights back into the base model for standalone inference, and exports the final model in deployment-optimized formats like GGUF for efficient inference engines.</p>\n<table>\n<thead>\n<tr>\n<th>Responsibility</th>\n<th>Description</th>\n<th>Input</th>\n<th>Output</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Perplexity Evaluation</td>\n<td>Measure language modeling performance</td>\n<td>Trained model, validation data</td>\n<td>Perplexity scores</td>\n</tr>\n<tr>\n<td>Task-Specific Benchmarking</td>\n<td>Run domain-relevant evaluation suites</td>\n<td>Model checkpoints</td>\n<td>Performance metrics</td>\n</tr>\n<tr>\n<td>Adapter Merging</td>\n<td>Fuse LoRA weights into base model</td>\n<td>Trained adapters, base model</td>\n<td>Merged standalone model</td>\n</tr>\n<tr>\n<td>Model Export</td>\n<td>Convert to inference-optimized formats</td>\n<td>Merged model</td>\n<td>GGUF/HF format files</td>\n</tr>\n<tr>\n<td>Quality Comparison</td>\n<td>Compare fine-tuned vs base model performance</td>\n<td>Evaluation results</td>\n<td>Improvement analysis</td>\n</tr>\n<tr>\n<td>Deployment Validation</td>\n<td>Verify merged model inference quality</td>\n<td>Exported model files</td>\n<td>Deployment readiness</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Key Architectural Insight</strong>: The separation of quantization, adaptation, and training concerns allows each component to optimize for its specific responsibilities while maintaining composability. The LoRA component focuses purely on parameter efficiency, the quantization component handles memory optimization, and the training component manages convergence - this separation of concerns prevents the complexity explosion that would occur if these responsibilities were mixed.</p>\n</blockquote>\n<h3 id=\"data-flow-and-dependencies\">Data Flow and Dependencies</h3>\n<p>The components interact through a carefully orchestrated data flow that minimizes memory overhead while maintaining training stability. The flow follows a <strong>pipeline pattern</strong> where each stage produces artifacts consumed by subsequent stages, with explicit checkpointing and validation at component boundaries.</p>\n<p><img src=\"/api/project/llm-finetuning-pipeline/architecture-doc/asset?path=diagrams%2Ftraining-flow-sequence.svg\" alt=\"Training Flow Sequence\"></p>\n<p>The data flow begins with the <strong>Dataset Preparation Component</strong> ingesting raw training data and producing tokenized datasets with proper train-validation splits. This component has no dependencies on other pipeline components but requires access to the target model&#39;s tokenizer to ensure consistent token encoding. The tokenizer dependency creates a weak coupling to the model loading process, but this is resolved by loading the tokenizer independently during the data preparation phase.</p>\n<table>\n<thead>\n<tr>\n<th>Data Flow Stage</th>\n<th>Component</th>\n<th>Input Dependencies</th>\n<th>Output Artifacts</th>\n<th>Downstream Consumers</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Ingestion</td>\n<td>Dataset Preparation</td>\n<td>Raw data files, model tokenizer</td>\n<td>Tokenized train/val datasets</td>\n<td>Training Loop</td>\n</tr>\n<tr>\n<td>Model Quantization</td>\n<td>QLoRA Quantization</td>\n<td>Base model checkpoint, hardware info</td>\n<td>4-bit quantized model</td>\n<td>LoRA Configuration</td>\n</tr>\n<tr>\n<td>Adapter Setup</td>\n<td>LoRA Configuration</td>\n<td>Quantized model, efficiency targets</td>\n<td>Adapter-instrumented model</td>\n<td>Training Loop</td>\n</tr>\n<tr>\n<td>Fine-tuning</td>\n<td>Training Loop</td>\n<td>Prepared datasets, adapted model</td>\n<td>Checkpoint series</td>\n<td>Evaluation &amp; Merging</td>\n</tr>\n<tr>\n<td>Deployment Prep</td>\n<td>Evaluation &amp; Merging</td>\n<td>Best checkpoint, validation data</td>\n<td>Merged model, metrics</td>\n<td>External deployment</td>\n</tr>\n</tbody></table>\n<p>The <strong>QLoRA Quantization Component</strong> loads the base model independently and produces a 4-bit quantized version that serves as the foundation for subsequent processing. This component has no dependencies on other pipeline components but must coordinate with the system&#39;s CUDA environment and bitsandbytes library. The quantized model becomes the input for the LoRA configuration process.</p>\n<p>The <strong>LoRA Configuration Component</strong> receives the quantized model and injects low-rank adapters into the target modules. This component depends on the quantized model from the previous stage and produces an adapter-instrumented model ready for training. The component must coordinate with the quantization setup to ensure that adapter parameters are not quantized while maintaining the 4-bit base model weights.</p>\n<p>The <strong>Training Loop Component</strong> orchestrates the convergence process using the prepared datasets and adapter-instrumented model. This component has dependencies on both the Dataset Preparation and LoRA Configuration outputs, making it a convergence point in the pipeline. The training loop produces a series of checkpoints, with the best checkpoint (determined by validation loss) becoming the input for evaluation and merging.</p>\n<p>The <strong>Evaluation and Merging Component</strong> consumes the best training checkpoint along with the validation dataset to produce final performance metrics and a deployment-ready merged model. This component depends on outputs from both the Training Loop (for the checkpoint) and Dataset Preparation (for evaluation data).</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Pipeline Flow Dependencies:\n\nRaw Data + Base Model Checkpoint\n    ↓\nDataset Preparation ← tokenizer ← QLoRA Quantization\n    ↓                                ↓\nTokenized Datasets              4-bit Quantized Model\n    ↓                                ↓\n    ↓                        LoRA Configuration\n    ↓                                ↓\n    └─────→ Training Loop ←─── Adapter Model\n                ↓\n            Checkpoints\n                ↓\n         Evaluation &amp; Merging ← Validation Data\n                ↓\n        Deployed Model + Metrics</code></pre></div>\n\n<p><strong>Inter-component communication</strong> follows a message-passing pattern with well-defined interfaces and error propagation. Each component exposes configuration objects that capture all necessary parameters for its operation, enabling components to validate their inputs before processing begins.</p>\n<table>\n<thead>\n<tr>\n<th>Interface</th>\n<th>Producer</th>\n<th>Consumer</th>\n<th>Data Structure</th>\n<th>Validation Rules</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Training Data</td>\n<td>Dataset Preparation</td>\n<td>Training Loop</td>\n<td><code>torch.utils.data.Dataset</code></td>\n<td>Non-empty, consistent tokenization</td>\n</tr>\n<tr>\n<td>Quantized Model</td>\n<td>QLoRA Quantization</td>\n<td>LoRA Configuration</td>\n<td><code>transformers.PreTrainedModel</code></td>\n<td>4-bit weights, compatible device</td>\n</tr>\n<tr>\n<td>Adapted Model</td>\n<td>LoRA Configuration</td>\n<td>Training Loop</td>\n<td><code>peft.PeftModel</code></td>\n<td>Frozen base, trainable adapters</td>\n</tr>\n<tr>\n<td>Checkpoint Path</td>\n<td>Training Loop</td>\n<td>Evaluation &amp; Merging</td>\n<td><code>str</code> (file path)</td>\n<td>Exists, loadable state dict</td>\n</tr>\n<tr>\n<td>Evaluation Results</td>\n<td>Evaluation &amp; Merging</td>\n<td>External</td>\n<td><code>Dict[str, float]</code></td>\n<td>Valid metrics, improvement scores</td>\n</tr>\n</tbody></table>\n<p><strong>State coordination</strong> between components happens through shared configuration objects and explicit checkpointing. The <code>TrainingConfig</code> object serves as the central coordination mechanism, containing all the parameters needed by each component. This design ensures that components receive consistent configuration while maintaining loose coupling.</p>\n<table>\n<thead>\n<tr>\n<th>Configuration Object</th>\n<th>Scope</th>\n<th>Fields</th>\n<th>Used By</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>TrainingConfig</code></td>\n<td>Global</td>\n<td>model_name_or_path, learning_rate, num_train_epochs</td>\n<td>All components</td>\n</tr>\n<tr>\n<td><code>QuantizationConfig</code></td>\n<td>QLoRA</td>\n<td>load_in_4bit, bnb_4bit_quant_type, compute_dtype</td>\n<td>QLoRA, LoRA</td>\n</tr>\n<tr>\n<td><code>LoRAConfig</code></td>\n<td>Adapters</td>\n<td>r, alpha, target_modules, dropout</td>\n<td>LoRA, Training</td>\n</tr>\n<tr>\n<td><code>DataConfig</code></td>\n<td>Dataset</td>\n<td>train_file, validation_split, max_length</td>\n<td>Dataset Preparation</td>\n</tr>\n</tbody></table>\n<p><strong>Error propagation</strong> follows a fail-fast principle where each component validates its inputs and raises specific exceptions for different failure modes. Components catch and re-raise errors with additional context about the failure location and potential recovery strategies.</p>\n<blockquote>\n<p><strong>Critical Design Decision</strong>: Components communicate through filesystem artifacts (saved datasets, model checkpoints) rather than in-memory objects. This design trades some performance for robustness - if any component fails, the pipeline can resume from the last successful stage without recomputing earlier steps.</p>\n</blockquote>\n<h3 id=\"recommended-file-structure\">Recommended File Structure</h3>\n<p>The codebase organization follows a <strong>domain-driven structure</strong> where each major component lives in its own module with clear separation of concerns. The structure accommodates both the learning progression (implementing components incrementally) and production usage (clear module boundaries for testing and maintenance).</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>llm-fine-tuning-pipeline/\n├── README.md                           ← Quick start guide and examples\n├── requirements.txt                    ← Python dependencies\n├── pyproject.toml                      ← Project configuration and build settings\n├── config/                            ← Configuration templates and examples\n│   ├── example_training_config.yaml   ← Sample training configuration\n│   ├── model_configs/                 ← Model-specific configuration templates\n│   │   ├── llama2_7b.yaml\n│   │   ├── mistral_7b.yaml\n│   │   └── phi2_3b.yaml\n│   └── dataset_configs/               ← Dataset format examples\n│       ├── alpaca_format.json\n│       ├── conversation_format.json\n│       └── instruction_format.json\n├── src/                               ← Main source code\n│   ├── __init__.py\n│   ├── fine_tuning_pipeline.py        ← Main pipeline orchestrator class\n│   ├── config/                        ← Configuration management\n│   │   ├── __init__.py\n│   │   ├── training_config.py         ← TrainingConfig, LoRAConfig, QuantizationConfig\n│   │   ├── data_config.py             ← Dataset configuration objects\n│   │   └── config_loader.py           ← YAML/JSON config file loading\n│   ├── data_preparation/              ← Dataset Preparation Component\n│   │   ├── __init__.py\n│   │   ├── data_loader.py             ← Multi-format data ingestion\n│   │   ├── chat_template.py           ← Model-specific prompt formatting\n│   │   ├── tokenizer_pipeline.py      ← Tokenization with padding/truncation\n│   │   ├── data_splitter.py           ← Train-validation splitting\n│   │   ├── quality_filter.py          ← Data cleaning and validation\n│   │   └── dataset_builder.py         ← Coordinate data preparation stages\n│   ├── lora_config/                   ← LoRA Configuration Component\n│   │   ├── __init__.py\n│   │   ├── target_modules.py          ← Auto-detect adaptation targets\n│   │   ├── adapter_setup.py           ← LoRA injection and initialization\n│   │   ├── parameter_analysis.py      ← Count trainable parameters\n│   │   └── lora_manager.py            ← Coordinate LoRA setup stages\n│   ├── quantization/                  ← QLoRA Quantization Component\n│   │   ├── __init__.py\n│   │   ├── model_loader.py            ← 4-bit NF4 quantized model loading\n│   │   ├── quantization_setup.py      ← Configure quantization parameters\n│   │   ├── memory_monitor.py          ← VRAM usage tracking\n│   │   └── quantization_manager.py    ← Coordinate quantization stages\n│   ├── training/                      ← Training Loop Component\n│   │   ├── __init__.py\n│   │   ├── training_args.py           ← Training hyperparameter management\n│   │   ├── gradient_accumulation.py   ← Micro-batch coordination\n│   │   ├── lr_scheduler.py            ← Learning rate scheduling\n│   │   ├── checkpoint_manager.py      ← Model saving and resumption\n│   │   ├── loss_tracker.py            ← Training metrics and early stopping\n│   │   └── training_orchestrator.py   ← Main training loop coordination\n│   ├── evaluation/                    ← Evaluation and Merging Component\n│   │   ├── __init__.py\n│   │   ├── perplexity_calculator.py   ← Language modeling evaluation\n│   │   ├── task_evaluator.py          ← Domain-specific benchmarking\n│   │   ├── adapter_merger.py          ← LoRA weight merging\n│   │   ├── model_exporter.py          ← GGUF and HF format export\n│   │   └── evaluation_manager.py      ← Coordinate evaluation stages\n│   └── utils/                         ← Shared utilities\n│       ├── __init__.py\n│       ├── logging_setup.py           ← Structured logging configuration\n│       ├── device_utils.py            ← CUDA device management\n│       ├── file_utils.py              ← File I/O and path handling\n│       └── metric_utils.py            ← Common evaluation metrics\n├── scripts/                           ← Executable scripts and examples\n│   ├── run_fine_tuning.py             ← Main pipeline execution script\n│   ├── prepare_data_only.py           ← Run just data preparation\n│   ├── evaluate_model.py              ← Run evaluation on existing checkpoint\n│   ├── export_to_gguf.py              ← Convert model to GGUF format\n│   └── examples/                      ← Example usage scripts\n│       ├── fine_tune_llama2.py\n│       ├── fine_tune_code_model.py\n│       └── resume_training.py\n├── tests/                             ← Test suite\n│   ├── __init__.py\n│   ├── unit/                          ← Component unit tests\n│   │   ├── test_data_preparation.py\n│   │   ├── test_lora_config.py\n│   │   ├── test_quantization.py\n│   │   ├── test_training.py\n│   │   └── test_evaluation.py\n│   ├── integration/                   ← End-to-end integration tests\n│   │   ├── test_pipeline_flow.py\n│   │   ├── test_component_integration.py\n│   │   └── test_checkpoint_recovery.py\n│   ├── fixtures/                      ← Test data and mock models\n│   │   ├── sample_training_data.json\n│   │   ├── mock_model_config.json\n│   │   └── test_checkpoints/\n│   └── performance/                   ← Memory and speed benchmarks\n│       ├── test_memory_usage.py\n│       └── test_training_speed.py\n├── docs/                              ← Additional documentation\n│   ├── api_reference.md               ← Detailed API documentation\n│   ├── configuration_guide.md         ← Configuration options explained\n│   ├── troubleshooting.md             ← Common issues and solutions\n│   └── diagrams/                      ← Architecture diagrams\n└── experiments/                       ← Experimental code and research\n    ├── notebook_examples/             ← Jupyter notebooks for exploration\n    ├── alternative_approaches/        ← Alternative implementation attempts\n    └── benchmark_results/             ← Performance measurement results</code></pre></div>\n\n<p><strong>Module organization principles</strong> guide the file structure decisions. Each component directory contains all the code needed for that component&#39;s responsibilities, minimizing cross-component imports and enabling independent testing. The structure supports <strong>incremental implementation</strong> where learners can implement components one at a time while maintaining a working system.</p>\n<table>\n<thead>\n<tr>\n<th>Directory</th>\n<th>Purpose</th>\n<th>Import Policy</th>\n<th>Testing Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>config/</code></td>\n<td>Configuration classes and loading</td>\n<td>No imports from other components</td>\n<td>Unit tests with mock configs</td>\n</tr>\n<tr>\n<td><code>data_preparation/</code></td>\n<td>Dataset processing pipeline</td>\n<td>Import only from <code>config/</code> and <code>utils/</code></td>\n<td>Unit tests + integration with real data</td>\n</tr>\n<tr>\n<td><code>lora_config/</code></td>\n<td>Adapter setup and management</td>\n<td>Import from <code>config/</code>, <code>utils/</code>, <code>quantization/</code></td>\n<td>Unit tests + model architecture tests</td>\n</tr>\n<tr>\n<td><code>quantization/</code></td>\n<td>4-bit model loading and monitoring</td>\n<td>Import from <code>config/</code>, <code>utils/</code></td>\n<td>Unit tests + memory benchmarking</td>\n</tr>\n<tr>\n<td><code>training/</code></td>\n<td>Training loop and optimization</td>\n<td>Import from all components</td>\n<td>Integration tests + training convergence</td>\n</tr>\n<tr>\n<td><code>evaluation/</code></td>\n<td>Model assessment and export</td>\n<td>Import from all components</td>\n<td>End-to-end tests + quality validation</td>\n</tr>\n<tr>\n<td><code>utils/</code></td>\n<td>Shared infrastructure</td>\n<td>No component imports</td>\n<td>Utility function unit tests</td>\n</tr>\n</tbody></table>\n<p>The <strong>script organization</strong> in the <code>scripts/</code> directory provides both learning examples and production usage patterns. The main <code>run_fine_tuning.py</code> script demonstrates the complete pipeline, while component-specific scripts enable testing and debugging individual stages.</p>\n<p><strong>Configuration management</strong> centralizes all hyperparameters and settings in the <code>config/</code> directory with YAML files for different model families and use cases. This organization makes it easy to reproduce experiments and share working configurations between team members.</p>\n<blockquote>\n<p><strong>Architectural Decision: Filesystem-Based Component Communication</strong></p>\n<ul>\n<li><strong>Context</strong>: Components need to exchange large objects like datasets and model checkpoints, and the pipeline should support resumption after failures</li>\n<li><strong>Options Considered</strong>: In-memory object passing, shared state objects, filesystem artifacts</li>\n<li><strong>Decision</strong>: Use filesystem artifacts for inter-component communication</li>\n<li><strong>Rationale</strong>: Enables pipeline resumption, simplifies debugging (intermediate results are inspectable), supports distributed processing, and reduces memory pressure</li>\n<li><strong>Consequences</strong>: Slight performance overhead from serialization, but significant gains in robustness and debuggability</li>\n</ul>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The implementation follows a progressive complexity approach where each component can be built and tested independently before integration into the full pipeline.</p>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Configuration Management</td>\n<td>YAML + dataclasses</td>\n<td>Hydra configuration framework</td>\n</tr>\n<tr>\n<td>Dataset Loading</td>\n<td>pandas + torch Dataset</td>\n<td>HuggingFace datasets library</td>\n</tr>\n<tr>\n<td>Model Loading</td>\n<td>transformers + bitsandbytes</td>\n<td>Custom quantization with GPTQ</td>\n</tr>\n<tr>\n<td>Training Loop</td>\n<td>HuggingFace Trainer</td>\n<td>Custom PyTorch training loop</td>\n</tr>\n<tr>\n<td>Logging</td>\n<td>Python logging + file output</td>\n<td>Weights &amp; Biases integration</td>\n</tr>\n<tr>\n<td>Memory Monitoring</td>\n<td>torch.cuda.memory_stats</td>\n<td>NVIDIA-ML-Py detailed profiling</td>\n</tr>\n</tbody></table>\n<p><strong>Core Infrastructure Starter Code:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/config/training_config.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, List, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> yaml</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> QuantizationConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for 4-bit quantization settings.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    load_in_4bit: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bnb_4bit_quant_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"nf4\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bnb_4bit_compute_dtype: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"bfloat16\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bnb_4bit_use_double_quant: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> LoRAConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for LoRA adapter parameters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    r: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 16</span><span style=\"color:#6A737D\">  # Default rank for low-rank decomposition</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    alpha: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 32</span><span style=\"color:#6A737D\">  # Scaling parameter for adapter outputs  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dropout: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    target_modules: Optional[List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">  # Auto-detect if None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bias: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"none\"</span><span style=\"color:#6A737D\">  # Options: \"none\", \"all\", \"lora_only\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    task_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"CAUSAL_LM\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TrainingConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Master configuration object for the entire fine-tuning pipeline.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    model_name_or_path: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    quantization: QuantizationConfig </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">QuantizationConfig)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lora: LoRAConfig </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">LoRAConfig)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    learning_rate: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3e-4</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_train_epochs: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    per_device_train_batch_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gradient_accumulation_steps: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 8</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    warmup_ratio: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logging_steps: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    save_steps: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    eval_steps: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    output_dir: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"./output\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    run_name: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> load_config_from_yaml</span><span style=\"color:#E1E4E8\">(config_path: Path) -> TrainingConfig:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Load and validate configuration from YAML file.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(config_path, </span><span style=\"color:#9ECBFF\">'r'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config_dict </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> yaml.safe_load(f)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Extract nested configuration sections</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    quantization_dict </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config_dict.pop(</span><span style=\"color:#9ECBFF\">'quantization'</span><span style=\"color:#E1E4E8\">, {})</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lora_dict </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config_dict.pop(</span><span style=\"color:#9ECBFF\">'lora'</span><span style=\"color:#E1E4E8\">, {})</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Create configuration objects</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    quantization_config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> QuantizationConfig(</span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">quantization_dict)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lora_config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> LoRAConfig(</span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">lora_dict)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> TrainingConfig(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        quantization</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">quantization_config,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        lora</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">lora_config,</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        **</span><span style=\"color:#E1E4E8\">config_dict</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/utils/memory_monitor.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> psutil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MemoryMonitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Tracks GPU and system memory usage throughout the pipeline.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    baseline_gpu_memory: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    baseline_system_memory: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    measurements: List[Dict] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> capture_baseline</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record initial memory state before model loading.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            torch.cuda.empty_cache()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.baseline_gpu_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_allocated()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.baseline_system_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.virtual_memory().used</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> measure_current_usage</span><span style=\"color:#E1E4E8\">(self, stage: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Measure current memory and return usage statistics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        measurement </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"stage\"</span><span style=\"color:#E1E4E8\">: stage,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"timestamp\"</span><span style=\"color:#E1E4E8\">: time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            measurement.update({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"gpu_allocated_mb\"</span><span style=\"color:#E1E4E8\">: torch.cuda.memory_allocated() </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"gpu_reserved_mb\"</span><span style=\"color:#E1E4E8\">: torch.cuda.memory_reserved() </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"gpu_free_mb\"</span><span style=\"color:#E1E4E8\">: (torch.cuda.get_device_properties(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">).total_memory </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                               torch.cuda.memory_reserved()) </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">2</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        measurement[</span><span style=\"color:#9ECBFF\">\"system_memory_mb\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.virtual_memory().used </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">2</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        measurement[</span><span style=\"color:#9ECBFF\">\"system_memory_percent\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.virtual_memory().percent</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.measurements.append(measurement)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> measurement</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_peak_usage</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return peak memory usage across all measurements.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.measurements:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        peak_gpu </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(m.get(</span><span style=\"color:#9ECBFF\">\"gpu_allocated_mb\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> m </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.measurements)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        peak_system </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(m.get(</span><span style=\"color:#9ECBFF\">\"system_memory_mb\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> m </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.measurements)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"peak_gpu_mb\"</span><span style=\"color:#E1E4E8\">: peak_gpu,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"peak_system_mb\"</span><span style=\"color:#E1E4E8\">: peak_system,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"gpu_baseline_mb\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.baseline_gpu_memory </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#F97583\"> if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.baseline_gpu_memory </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"system_baseline_mb\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.baseline_system_memory </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">2</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span></code></pre></div>\n\n<p><strong>Core Pipeline Orchestrator Skeleton:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/fine_tuning_pipeline.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .config.training_config </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TrainingConfig</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .utils.memory_monitor </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> MemoryMonitor</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> FineTuningPipeline</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Main orchestrator for the end-to-end fine-tuning pipeline.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config: TrainingConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    memory_monitor: MemoryMonitor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">MemoryMonitor)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokenizer: Optional </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    model: Optional </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> setup_model_and_tokenizer</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load quantized model and configure tokenizer.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize memory monitoring baseline</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Load tokenizer from config.model_name_or_path</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Apply chat template configuration if available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Load model with quantization configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Monitor memory usage after model loading</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use AutoTokenizer.from_pretrained() and AutoModelForCausalLM.from_pretrained()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Pass quantization config via BitsAndBytesConfig</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> setup_lora_adapters</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Inject LoRA adapters into frozen base model.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Auto-detect target modules if not specified in config</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create PEFT configuration with LoRA parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Wrap model with PEFT adapters using get_peft_model()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify that base model parameters are frozen</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Count and log trainable vs total parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Monitor memory usage after adapter injection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> prepare_datasets</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load and preprocess training data.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load raw training data from configured sources</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Apply chat template formatting for instruction tuning</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Tokenize datasets with proper padding and truncation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Split into training and validation sets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Apply quality filtering and length constraints</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Cache processed datasets for resumption</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> execute_training</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Run training loop and return best checkpoint path.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Set up training arguments with learning rate schedule</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize trainer with gradient accumulation configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Set up logging and checkpoint saving callbacks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Configure early stopping based on validation loss</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Start training loop with memory monitoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Track training metrics and loss convergence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return path to best checkpoint based on validation score</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> evaluate_and_merge</span><span style=\"color:#E1E4E8\">(self, checkpoint_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Evaluate model and merge adapters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load best checkpoint and switch to evaluation mode</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate perplexity on validation set</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Run task-specific evaluation benchmarks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Merge LoRA adapter weights back into base model</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Verify merged model quality matches checkpoint</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Export merged model in HuggingFace and GGUF formats</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return comprehensive evaluation metrics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> run_complete_pipeline</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute full pipeline from data to deployment.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.memory_monitor.capture_baseline()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Pipeline stages with progress tracking</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logging.info(</span><span style=\"color:#9ECBFF\">\"Setting up model and tokenizer...\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.setup_model_and_tokenizer()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logging.info(</span><span style=\"color:#9ECBFF\">\"Configuring LoRA adapters...\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.setup_lora_adapters()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logging.info(</span><span style=\"color:#9ECBFF\">\"Preparing datasets...\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.prepare_datasets()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logging.info(</span><span style=\"color:#9ECBFF\">\"Starting training...\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            best_checkpoint </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.execute_training()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logging.info(</span><span style=\"color:#9ECBFF\">\"Evaluating and merging model...\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            results </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.evaluate_and_merge(best_checkpoint)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Add memory usage summary to results</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            results[</span><span style=\"color:#9ECBFF\">\"memory_usage\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.memory_monitor.get_peak_usage()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> results</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logging.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Pipeline failed: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Add error context and memory state</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint for Architecture Setup:</strong>\nAfter implementing the basic pipeline structure:</p>\n<ol>\n<li>Run <code>python -m pytest tests/unit/test_config.py</code> - should pass configuration loading tests</li>\n<li>Execute <code>python scripts/run_fine_tuning.py --config config/example_training_config.yaml --dry-run</code> - should initialize components without training</li>\n<li>Check output directory contains properly structured component logs</li>\n<li>Verify memory monitoring captures baseline measurements before any model loading</li>\n</ol>\n<p><strong>Language-Specific Implementation Hints:</strong></p>\n<ul>\n<li>Use <code>torch.cuda.is_available()</code> to detect GPU presence before quantization setup</li>\n<li>Import <code>bitsandbytes</code> only when quantization is enabled to avoid CUDA errors on CPU-only systems</li>\n<li>Use <code>transformers.AutoModel.from_pretrained()</code> with <code>device_map=&quot;auto&quot;</code> for automatic device placement</li>\n<li>Enable gradient checkpointing with <code>model.gradient_checkpointing_enable()</code> for memory efficiency</li>\n<li>Use <code>accelerate</code> library for automatic mixed precision and device placement in production</li>\n</ul>\n<h2 id=\"data-model\">Data Model</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - these data structures form the backbone of the entire fine-tuning pipeline, enabling configuration management, training orchestration, and evaluation tracking across all components</p>\n</blockquote>\n<p>The data model serves as the contract between all components in our fine-tuning pipeline, defining exactly how configuration parameters, training samples, and evaluation metrics flow through the system. Think of these data structures as the <strong>architectural blueprints</strong> that every component must understand - just as construction workers need detailed blueprints to coordinate their efforts on a building, our pipeline components need precise data schemas to coordinate the complex process of model fine-tuning.</p>\n<p><img src=\"/api/project/llm-finetuning-pipeline/architecture-doc/asset?path=diagrams%2Fdata-model-diagram.svg\" alt=\"Data Model and Configuration Schema\"></p>\n<p>The data model design follows three core principles that make the system maintainable and extensible. First, <strong>configuration immutability</strong> ensures that once training begins, hyperparameters cannot accidentally change mid-process. Second, <strong>type safety</strong> prevents common errors like mixing up learning rates and rank parameters. Third, <strong>progressive disclosure</strong> means simple configurations use sensible defaults while advanced users can override every parameter.</p>\n<h3 id=\"training-data-structures\">Training Data Structures</h3>\n<p>The training data structures transform raw conversational data into the precise format required by modern instruction-tuned language models. Think of this as the <strong>language tutor&#39;s lesson plan</strong> - raw conversations must be converted into structured instruction-response pairs with proper formatting, special tokens, and attention masks that the model can learn from effectively.</p>\n<p>Modern language models expect training data in a specific conversational format that includes system prompts, user instructions, and assistant responses. The challenge lies in standardizing this format across different data sources while preserving the conversational context that makes fine-tuning effective. Our data structures handle this transformation systematically.</p>\n<table>\n<thead>\n<tr>\n<th>Structure</th>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ConversationSample</code></td>\n<td>conversation_id</td>\n<td><code>str</code></td>\n<td>Unique identifier for tracking multi-turn conversations</td>\n</tr>\n<tr>\n<td></td>\n<td>turns</td>\n<td><code>List[ConversationTurn]</code></td>\n<td>Ordered sequence of conversation exchanges</td>\n</tr>\n<tr>\n<td></td>\n<td>metadata</td>\n<td><code>Dict[str, Any]</code></td>\n<td>Source information, quality scores, domain tags</td>\n</tr>\n<tr>\n<td></td>\n<td>total_tokens</td>\n<td><code>Optional[int]</code></td>\n<td>Token count after tokenization, None if not yet computed</td>\n</tr>\n<tr>\n<td></td>\n<td>is_valid</td>\n<td><code>bool</code></td>\n<td>Whether sample passes quality filters and length constraints</td>\n</tr>\n<tr>\n<td><code>ConversationTurn</code></td>\n<td>role</td>\n<td><code>str</code></td>\n<td>Speaker role: &quot;system&quot;, &quot;user&quot;, &quot;assistant&quot;</td>\n</tr>\n<tr>\n<td></td>\n<td>content</td>\n<td><code>str</code></td>\n<td>The actual text content of this turn</td>\n</tr>\n<tr>\n<td></td>\n<td>turn_index</td>\n<td><code>int</code></td>\n<td>Position in conversation sequence, starting from 0</td>\n</tr>\n<tr>\n<td><code>InstructionSample</code></td>\n<td>instruction</td>\n<td><code>str</code></td>\n<td>The task description or question posed to the model</td>\n</tr>\n<tr>\n<td></td>\n<td>response</td>\n<td><code>str</code></td>\n<td>Expected model output or correct answer</td>\n</tr>\n<tr>\n<td></td>\n<td>system_prompt</td>\n<td><code>Optional[str]</code></td>\n<td>Context or constraints for the instruction</td>\n</tr>\n<tr>\n<td></td>\n<td>input_context</td>\n<td><code>Optional[str]</code></td>\n<td>Additional context data for the instruction</td>\n</tr>\n<tr>\n<td></td>\n<td>sample_id</td>\n<td><code>str</code></td>\n<td>Unique identifier for this instruction-response pair</td>\n</tr>\n<tr>\n<td></td>\n<td>source</td>\n<td><code>str</code></td>\n<td>Origin dataset or generation method</td>\n</tr>\n<tr>\n<td></td>\n<td>quality_score</td>\n<td><code>Optional[float]</code></td>\n<td>Automated quality assessment score 0-1</td>\n</tr>\n<tr>\n<td><code>TokenizedSample</code></td>\n<td>input_ids</td>\n<td><code>List[int]</code></td>\n<td>Token IDs for the complete formatted prompt</td>\n</tr>\n<tr>\n<td></td>\n<td>attention_mask</td>\n<td><code>List[int]</code></td>\n<td>Binary mask indicating which tokens to attend to</td>\n</tr>\n<tr>\n<td></td>\n<td>labels</td>\n<td><code>List[int]</code></td>\n<td>Target token IDs for loss computation, -100 for ignored tokens</td>\n</tr>\n<tr>\n<td></td>\n<td>prompt_length</td>\n<td><code>int</code></td>\n<td>Number of tokens in the instruction portion</td>\n</tr>\n<tr>\n<td></td>\n<td>response_length</td>\n<td><code>int</code></td>\n<td>Number of tokens in the response portion</td>\n</tr>\n<tr>\n<td></td>\n<td>total_length</td>\n<td><code>int</code></td>\n<td>Total sequence length including special tokens</td>\n</tr>\n</tbody></table>\n<p>The conversation-to-instruction transformation follows a systematic process that preserves the natural flow of multi-turn dialogues while creating clear learning objectives. When processing multi-turn conversations, the system identifies natural instruction-response boundaries and creates separate training samples for each exchange, maintaining conversation history as context.</p>\n<p>The tokenization process applies the model&#39;s specific chat template, which varies between model families. For example, Llama models use <code>&lt;s&gt;[INST] instruction [/INST] response &lt;/s&gt;</code> format, while ChatML-formatted models use <code>&lt;|im_start|&gt;user\\ninstruction&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\nresponse&lt;|im_end|&gt;</code>. Our tokenization component automatically detects the appropriate template based on the model configuration.</p>\n<blockquote>\n<p><strong>Critical Design Insight</strong>: The <code>labels</code> field uses -100 for instruction tokens to exclude them from loss computation. This ensures the model only learns to predict response tokens, not repeat instructions verbatim. This technique, called <strong>causal language modeling with attention masks</strong>, is essential for effective instruction tuning.</p>\n</blockquote>\n<p>Quality filtering operates at multiple levels to ensure training data meets minimum standards. The system removes samples with extremely short responses (less than 10 tokens), excessively long sequences that exceed the model&#39;s context window, and conversations with obvious formatting errors or corrupted encoding.</p>\n<blockquote>\n<p><strong>Decision: Unified Conversation Format</strong></p>\n<ul>\n<li><strong>Context</strong>: Different datasets use incompatible conversation formats (ChatML, Alpaca, ShareGPT), making data preparation complex</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Support each format natively with format-specific loaders</li>\n<li>Convert everything to a single intermediate format</li>\n<li>Use HuggingFace&#39;s ChatML standard throughout</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Convert all data to our unified <code>ConversationSample</code> format, then apply model-specific templates during tokenization</li>\n<li><strong>Rationale</strong>: Single format simplifies data processing logic, enables consistent quality filtering, and separates data representation from model-specific formatting</li>\n<li><strong>Consequences</strong>: Requires upfront conversion cost but eliminates format-specific bugs and enables uniform data augmentation</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>Pros</th>\n<th>Cons</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Native format support</td>\n<td>No conversion overhead, preserves original structure</td>\n<td>Complex codebase, format-specific bugs, inconsistent quality control</td>\n</tr>\n<tr>\n<td>Unified intermediate format</td>\n<td>Clean abstraction, consistent processing, easier testing</td>\n<td>Conversion overhead, potential information loss</td>\n</tr>\n<tr>\n<td>HuggingFace ChatML standard</td>\n<td>Industry standard, broad tool support</td>\n<td>Less flexibility for custom formats, external dependency</td>\n</tr>\n</tbody></table>\n<h3 id=\"configuration-objects\">Configuration Objects</h3>\n<p>Configuration objects provide type-safe, validated containers for the numerous hyperparameters that control model quantization, LoRA adaptation, and training behavior. Think of these as the <strong>master control panel</strong> for the fine-tuning process - they centralize all the knobs and dials that determine how the training will proceed, with built-in validation to prevent common configuration mistakes.</p>\n<p>The configuration system uses a hierarchical structure where high-level training configuration contains specialized sub-configurations for quantization and LoRA parameters. This design separates concerns while ensuring all components receive consistent parameter values throughout the training process.</p>\n<table>\n<thead>\n<tr>\n<th>Structure</th>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>QuantizationConfig</code></td>\n<td>load_in_4bit</td>\n<td><code>bool</code></td>\n<td>Whether to quantize the base model to 4-bit precision</td>\n</tr>\n<tr>\n<td></td>\n<td>bnb_4bit_quant_type</td>\n<td><code>str</code></td>\n<td>Quantization algorithm: &quot;nf4&quot; or &quot;fp4&quot;</td>\n</tr>\n<tr>\n<td></td>\n<td>bnb_4bit_compute_dtype</td>\n<td><code>str</code></td>\n<td>Data type for computations: &quot;float16&quot; or &quot;bfloat16&quot;</td>\n</tr>\n<tr>\n<td></td>\n<td>bnb_4bit_use_double_quant</td>\n<td><code>bool</code></td>\n<td>Whether to quantize quantization constants</td>\n</tr>\n<tr>\n<td></td>\n<td>bnb_4bit_quant_storage</td>\n<td><code>str</code></td>\n<td>Storage format for quantized weights</td>\n</tr>\n<tr>\n<td><code>LoRAConfig</code></td>\n<td>r</td>\n<td><code>int</code></td>\n<td>Rank of low-rank decomposition matrices</td>\n</tr>\n<tr>\n<td></td>\n<td>alpha</td>\n<td><code>int</code></td>\n<td>Scaling parameter for adapter weights</td>\n</tr>\n<tr>\n<td></td>\n<td>dropout</td>\n<td><code>float</code></td>\n<td>Dropout probability in adapter layers</td>\n</tr>\n<tr>\n<td></td>\n<td>target_modules</td>\n<td><code>Optional[List[str]]</code></td>\n<td>Layer names to inject adapters, None for auto-detection</td>\n</tr>\n<tr>\n<td></td>\n<td>bias</td>\n<td><code>str</code></td>\n<td>Bias handling: &quot;none&quot;, &quot;all&quot;, or &quot;lora_only&quot;</td>\n</tr>\n<tr>\n<td></td>\n<td>task_type</td>\n<td><code>str</code></td>\n<td>Task category: &quot;CAUSAL_LM&quot;, &quot;SEQ_2_SEQ_LM&quot;, etc.</td>\n</tr>\n<tr>\n<td></td>\n<td>lora_alpha_scaling</td>\n<td><code>bool</code></td>\n<td>Whether to apply alpha/r scaling to adapter outputs</td>\n</tr>\n<tr>\n<td></td>\n<td>init_lora_weights</td>\n<td><code>Union[bool, str]</code></td>\n<td>Weight initialization strategy for adapters</td>\n</tr>\n<tr>\n<td><code>TrainingConfig</code></td>\n<td>model_name_or_path</td>\n<td><code>str</code></td>\n<td>HuggingFace model identifier or local path</td>\n</tr>\n<tr>\n<td></td>\n<td>quantization</td>\n<td><code>QuantizationConfig</code></td>\n<td>4-bit quantization parameters</td>\n</tr>\n<tr>\n<td></td>\n<td>lora</td>\n<td><code>LoRAConfig</code></td>\n<td>Low-rank adaptation configuration</td>\n</tr>\n<tr>\n<td></td>\n<td>learning_rate</td>\n<td><code>float</code></td>\n<td>Base learning rate for optimizer</td>\n</tr>\n<tr>\n<td></td>\n<td>num_train_epochs</td>\n<td><code>int</code></td>\n<td>Number of complete passes through training data</td>\n</tr>\n<tr>\n<td></td>\n<td>per_device_train_batch_size</td>\n<td><code>int</code></td>\n<td>Training samples per GPU per forward pass</td>\n</tr>\n<tr>\n<td></td>\n<td>per_device_eval_batch_size</td>\n<td><code>int</code></td>\n<td>Evaluation samples per GPU per forward pass</td>\n</tr>\n<tr>\n<td></td>\n<td>gradient_accumulation_steps</td>\n<td><code>int</code></td>\n<td>Steps to accumulate before optimizer update</td>\n</tr>\n<tr>\n<td></td>\n<td>warmup_steps</td>\n<td><code>int</code></td>\n<td>Learning rate warmup duration</td>\n</tr>\n<tr>\n<td></td>\n<td>max_seq_length</td>\n<td><code>int</code></td>\n<td>Maximum sequence length in tokens</td>\n</tr>\n<tr>\n<td></td>\n<td>dataloader_num_workers</td>\n<td><code>int</code></td>\n<td>Parallel data loading processes</td>\n</tr>\n<tr>\n<td></td>\n<td>fp16</td>\n<td><code>bool</code></td>\n<td>Enable 16-bit mixed precision training</td>\n</tr>\n<tr>\n<td></td>\n<td>bf16</td>\n<td><code>bool</code></td>\n<td>Enable bfloat16 mixed precision training</td>\n</tr>\n<tr>\n<td></td>\n<td>dataloader_pin_memory</td>\n<td><code>bool</code></td>\n<td>Pin dataset tensors in CPU memory</td>\n</tr>\n<tr>\n<td></td>\n<td>gradient_checkpointing</td>\n<td><code>bool</code></td>\n<td>Trade compute for memory in backpropagation</td>\n</tr>\n<tr>\n<td></td>\n<td>logging_steps</td>\n<td><code>int</code></td>\n<td>Frequency of training metric logging</td>\n</tr>\n<tr>\n<td></td>\n<td>eval_steps</td>\n<td><code>int</code></td>\n<td>Frequency of validation evaluation</td>\n</tr>\n<tr>\n<td></td>\n<td>save_steps</td>\n<td><code>int</code></td>\n<td>Frequency of checkpoint saving</td>\n</tr>\n<tr>\n<td></td>\n<td>save_total_limit</td>\n<td><code>int</code></td>\n<td>Maximum number of checkpoints to retain</td>\n</tr>\n<tr>\n<td></td>\n<td>load_best_model_at_end</td>\n<td><code>bool</code></td>\n<td>Load best checkpoint after training completion</td>\n</tr>\n<tr>\n<td></td>\n<td>metric_for_best_model</td>\n<td><code>str</code></td>\n<td>Metric used for best model selection</td>\n</tr>\n<tr>\n<td></td>\n<td>greater_is_better</td>\n<td><code>bool</code></td>\n<td>Whether higher metric values indicate better performance</td>\n</tr>\n<tr>\n<td></td>\n<td>early_stopping_patience</td>\n<td><code>int</code></td>\n<td>Evaluations to wait before stopping if metric doesn&#39;t improve</td>\n</tr>\n<tr>\n<td></td>\n<td>early_stopping_threshold</td>\n<td><code>float</code></td>\n<td>Minimum improvement required to reset patience counter</td>\n</tr>\n</tbody></table>\n<p>The configuration validation process occurs at object creation time, preventing invalid parameter combinations that would cause training failures hours later. For example, the system validates that <code>gradient_accumulation_steps * per_device_train_batch_size</code> creates a reasonable effective batch size, and that <code>learning_rate</code> is appropriate for the chosen <code>lora.r</code> and <code>lora.alpha</code> combination.</p>\n<p>Quantization configuration requires careful coordination between storage format and compute precision. The <code>bnb_4bit_compute_dtype</code> determines the precision used during forward and backward passes, while quantized weights remain in 4-bit format. The system validates that the chosen compute dtype is compatible with the available hardware and training precision settings.</p>\n<blockquote>\n<p><strong>Decision: Hierarchical Configuration Structure</strong></p>\n<ul>\n<li><strong>Context</strong>: Fine-tuning involves dozens of hyperparameters across quantization, LoRA, and training domains</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Flat configuration with all parameters in one object</li>\n<li>Hierarchical configuration with specialized sub-objects</li>\n<li>Multiple independent configuration files</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Hierarchical structure with <code>TrainingConfig</code> containing <code>QuantizationConfig</code> and <code>LoRAConfig</code></li>\n<li><strong>Rationale</strong>: Groups related parameters logically, enables component-specific validation, allows independent testing of configuration subsystems</li>\n<li><strong>Consequences</strong>: Slightly more complex object construction but dramatically clearer parameter organization and better error messages</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>Pros</th>\n<th>Cons</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Flat configuration</td>\n<td>Simple structure, easy serialization</td>\n<td>Parameter namespace pollution, unclear relationships</td>\n</tr>\n<tr>\n<td>Hierarchical configuration</td>\n<td>Logical grouping, component isolation, clear validation</td>\n<td>More complex construction, nested access patterns</td>\n</tr>\n<tr>\n<td>Multiple configuration files</td>\n<td>Complete separation, independent versioning</td>\n<td>File coordination complexity, inconsistency risks</td>\n</tr>\n</tbody></table>\n<p>LoRA configuration requires balancing adaptation capacity against memory efficiency. The rank <code>r</code> determines how many parameters the adapter can learn - higher ranks enable more complex adaptations but consume more memory and may lead to overfitting on small datasets. The alpha parameter <code>alpha</code> controls how much the adapter outputs influence the base model, with the effective scaling being <code>alpha / r</code>.</p>\n<p>Default configuration values are chosen based on empirical results across diverse fine-tuning tasks. <code>DEFAULT_LORA_RANK = 16</code> provides good adaptation capacity for most instruction-following tasks, while <code>DEFAULT_LORA_ALPHA = 32</code> creates a 2x scaling that prevents adapter outputs from being overwhelmed by pre-trained weights.</p>\n<h3 id=\"evaluation-and-logging\">Evaluation and Logging</h3>\n<p>The evaluation and logging data structures capture training progress, model performance, and system resource utilization throughout the fine-tuning process. Think of this as the <strong>mission control dashboard</strong> for training - these structures provide real-time visibility into whether the training is proceeding successfully and enable data-driven decisions about when to stop, adjust hyperparameters, or investigate problems.</p>\n<p><img src=\"/api/project/llm-finetuning-pipeline/architecture-doc/asset?path=diagrams%2Fevaluation-workflow.svg\" alt=\"Evaluation Workflow\"></p>\n<p>Comprehensive logging serves multiple critical functions: tracking convergence for early stopping decisions, diagnosing training instabilities, measuring resource utilization for cost optimization, and providing reproducible records for experiment comparison. The logging system balances detail with performance, avoiding expensive computations during training while capturing sufficient information for post-training analysis.</p>\n<table>\n<thead>\n<tr>\n<th>Structure</th>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>TrainingMetrics</code></td>\n<td>step</td>\n<td><code>int</code></td>\n<td>Global training step number</td>\n</tr>\n<tr>\n<td></td>\n<td>epoch</td>\n<td><code>float</code></td>\n<td>Current epoch progress (e.g., 1.5 for halfway through epoch 2)</td>\n</tr>\n<tr>\n<td></td>\n<td>loss</td>\n<td><code>float</code></td>\n<td>Training loss for current batch</td>\n</tr>\n<tr>\n<td></td>\n<td>learning_rate</td>\n<td><code>float</code></td>\n<td>Current learning rate after scheduling</td>\n</tr>\n<tr>\n<td></td>\n<td>grad_norm</td>\n<td><code>Optional[float]</code></td>\n<td>Gradient norm before clipping, None if not computed</td>\n</tr>\n<tr>\n<td></td>\n<td>timestamp</td>\n<td><code>float</code></td>\n<td>Unix timestamp when metrics were recorded</td>\n</tr>\n<tr>\n<td></td>\n<td>gpu_memory_used</td>\n<td><code>Optional[float]</code></td>\n<td>GPU memory usage in GB</td>\n</tr>\n<tr>\n<td></td>\n<td>samples_per_second</td>\n<td><code>float</code></td>\n<td>Training throughput metric</td>\n</tr>\n<tr>\n<td><code>EvaluationMetrics</code></td>\n<td>eval_step</td>\n<td><code>int</code></td>\n<td>Evaluation step number</td>\n</tr>\n<tr>\n<td></td>\n<td>eval_loss</td>\n<td><code>float</code></td>\n<td>Average loss on validation set</td>\n</tr>\n<tr>\n<td></td>\n<td>perplexity</td>\n<td><code>float</code></td>\n<td>Validation perplexity computed from eval_loss</td>\n</tr>\n<tr>\n<td></td>\n<td>bleu_score</td>\n<td><code>Optional[float]</code></td>\n<td>BLEU score for generation tasks</td>\n</tr>\n<tr>\n<td></td>\n<td>rouge_scores</td>\n<td><code>Optional[Dict[str, float]]</code></td>\n<td>ROUGE-1, ROUGE-2, ROUGE-L scores</td>\n</tr>\n<tr>\n<td></td>\n<td>exact_match</td>\n<td><code>Optional[float]</code></td>\n<td>Exact string match accuracy</td>\n</tr>\n<tr>\n<td></td>\n<td>eval_samples</td>\n<td><code>int</code></td>\n<td>Number of samples in evaluation set</td>\n</tr>\n<tr>\n<td></td>\n<td>eval_runtime</td>\n<td><code>float</code></td>\n<td>Time spent on this evaluation in seconds</td>\n</tr>\n<tr>\n<td></td>\n<td>eval_samples_per_second</td>\n<td><code>float</code></td>\n<td>Evaluation throughput</td>\n</tr>\n<tr>\n<td></td>\n<td>timestamp</td>\n<td><code>float</code></td>\n<td>Unix timestamp of evaluation completion</td>\n</tr>\n<tr>\n<td><code>CheckpointMetadata</code></td>\n<td>checkpoint_path</td>\n<td><code>str</code></td>\n<td>File system path to saved checkpoint</td>\n</tr>\n<tr>\n<td></td>\n<td>step</td>\n<td><code>int</code></td>\n<td>Training step when checkpoint was created</td>\n</tr>\n<tr>\n<td></td>\n<td>epoch</td>\n<td><code>float</code></td>\n<td>Training epoch when checkpoint was created</td>\n</tr>\n<tr>\n<td></td>\n<td>eval_loss</td>\n<td><code>Optional[float]</code></td>\n<td>Validation loss at checkpoint time, None if not evaluated</td>\n</tr>\n<tr>\n<td></td>\n<td>is_best</td>\n<td><code>bool</code></td>\n<td>Whether this is the best checkpoint so far by the chosen metric</td>\n</tr>\n<tr>\n<td></td>\n<td>model_config</td>\n<td><code>Dict[str, Any]</code></td>\n<td>Serialized model and training configuration</td>\n</tr>\n<tr>\n<td></td>\n<td>adapter_config</td>\n<td><code>Dict[str, Any]</code></td>\n<td>LoRA adapter configuration and state</td>\n</tr>\n<tr>\n<td></td>\n<td>optimizer_state_size</td>\n<td><code>int</code></td>\n<td>Size of optimizer state in bytes</td>\n</tr>\n<tr>\n<td></td>\n<td>save_timestamp</td>\n<td><code>float</code></td>\n<td>Unix timestamp when checkpoint was saved</td>\n</tr>\n<tr>\n<td><code>MemoryMetrics</code></td>\n<td>stage</td>\n<td><code>str</code></td>\n<td>Training stage: &quot;baseline&quot;, &quot;model_loaded&quot;, &quot;training&quot;, &quot;evaluation&quot;</td>\n</tr>\n<tr>\n<td></td>\n<td>gpu_memory_allocated</td>\n<td><code>float</code></td>\n<td>GPU memory allocated by PyTorch in GB</td>\n</tr>\n<tr>\n<td></td>\n<td>gpu_memory_cached</td>\n<td><code>float</code></td>\n<td>GPU memory cached by PyTorch in GB</td>\n</tr>\n<tr>\n<td></td>\n<td>gpu_memory_reserved</td>\n<td><code>float</code></td>\n<td>Total GPU memory reserved by PyTorch in GB</td>\n</tr>\n<tr>\n<td></td>\n<td>system_memory_used</td>\n<td><code>float</code></td>\n<td>System RAM usage in GB</td>\n</tr>\n<tr>\n<td></td>\n<td>timestamp</td>\n<td><code>float</code></td>\n<td>Unix timestamp of memory measurement</td>\n</tr>\n<tr>\n<td></td>\n<td>details</td>\n<td><code>Optional[Dict[str, Any]]</code></td>\n<td>Additional stage-specific memory details</td>\n</tr>\n</tbody></table>\n<p>The training metrics collection occurs at configurable intervals to balance monitoring granularity with training performance. High-frequency metrics like loss and learning rate are logged every few steps, while expensive metrics like gradient norms are computed less frequently. The system automatically adjusts logging frequency based on training speed to maintain roughly constant monitoring overhead.</p>\n<p>Evaluation metrics provide task-specific performance measurements that guide training decisions. Perplexity measures how well the model predicts validation text and correlates strongly with general language modeling capability. Task-specific metrics like BLEU and ROUGE are computed when the validation set includes reference outputs for comparison.</p>\n<blockquote>\n<p><strong>Critical Design Insight</strong>: Memory metrics collection uses PyTorch&#39;s built-in memory profiling rather than system-level tools because GPU memory allocation is more complex than simple usage monitoring. PyTorch&#39;s memory manager pools and caches memory, so system-level measurements can be misleading.</p>\n</blockquote>\n<p>The checkpoint metadata system tracks not just model weights but the complete training context needed for reproducible resumption. This includes optimizer states, random number generator seeds, and data loader positions. The metadata enables intelligent checkpoint management, automatically cleaning up inferior checkpoints while preserving the best models.</p>\n<blockquote>\n<p><strong>Decision: Structured Logging with Typed Metrics</strong></p>\n<ul>\n<li><strong>Context</strong>: Training generates thousands of metric data points that need efficient storage and analysis</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Unstructured logging with print statements and manual parsing</li>\n<li>Structured logging with typed metric objects</li>\n<li>External monitoring systems like WandB with custom schemas</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Typed metric structures with optional external system integration</li>\n<li><strong>Rationale</strong>: Type safety prevents metric recording bugs, structured format enables easy analysis, local storage ensures data availability regardless of external services</li>\n<li><strong>Consequences</strong>: Requires more upfront design but provides reliable, analyzable training records and easier debugging</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>Pros</th>\n<th>Cons</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Unstructured logging</td>\n<td>Simple implementation, no dependencies</td>\n<td>Difficult analysis, error-prone parsing, no validation</td>\n</tr>\n<tr>\n<td>Structured typed metrics</td>\n<td>Type safety, easy analysis, reliable storage</td>\n<td>More complex design, additional abstractions</td>\n</tr>\n<tr>\n<td>External monitoring only</td>\n<td>Rich visualization, cloud storage, collaboration features</td>\n<td>External dependency, data lock-in, network requirements</td>\n</tr>\n</tbody></table>\n<p>Memory monitoring operates continuously throughout training to detect memory leaks and optimization opportunities. The system establishes a baseline measurement before model loading, then tracks memory growth during each training phase. Unexpected memory growth patterns often indicate bugs in data loading, gradient accumulation, or checkpoint handling.</p>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Configuration Parameter Type Mismatches</strong>\nDevelopers frequently confuse integer and float parameters, especially for learning rates and LoRA parameters. For example, setting <code>learning_rate = 5</code> instead of <code>learning_rate = 5e-5</code> creates an absurdly high learning rate that causes immediate training collapse. Similarly, confusing LoRA rank and alpha parameters leads to either ineffective adaptation or unstable training. The solution is comprehensive configuration validation that checks parameter types, ranges, and relationships before training begins.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Token Counting</strong>\nToken length calculations often differ between data preparation and actual training due to tokenizer special tokens, chat template formatting, and padding strategies. This leads to samples that pass length filtering but cause out-of-memory errors during training, or samples that are unnecessarily truncated. The solution is to perform tokenization during data preparation using the exact same tokenizer configuration and special tokens that will be used during training.</p>\n<p>⚠️ <strong>Pitfall: Missing Evaluation Baseline Comparisons</strong>\nTraining metrics show improvement over time, but without baseline comparisons against the unmodified base model, it&#39;s impossible to determine whether fine-tuning actually improved task performance. The solution is to evaluate the base model on the validation set before training begins, storing these baseline metrics alongside training results for direct comparison.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Memory Monitoring Granularity</strong>\nMonitoring only peak memory usage misses important memory allocation patterns that cause training failures. For example, gradient accumulation may work fine for several steps then suddenly cause out-of-memory errors when the optimizer state updates. The solution is to monitor memory at each phase transition (data loading, forward pass, backward pass, optimizer step) to identify exactly where memory problems occur.</p>\n<p>⚠️ <strong>Pitfall: Configuration Serialization Incompatibilities</strong>\nSaving configurations to YAML or JSON often fails when the configuration contains complex objects like tokenizers or custom data types. Additionally, loading configurations from files may not preserve exact object types, leading to subtle runtime errors. The solution is to implement proper serialization methods that handle all configuration object types and validate deserialized configurations match the original types and value ranges.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The data model implementation focuses on creating robust, validated data structures that prevent common configuration errors and provide clear debugging information when problems occur. The implementation balances simplicity for basic use cases with extensibility for advanced configurations.</p>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Configuration Management</td>\n<td>dataclasses with field validation</td>\n<td>Pydantic models with advanced validation</td>\n</tr>\n<tr>\n<td>Serialization</td>\n<td>JSON with custom encoders</td>\n<td>YAML with schema validation</td>\n</tr>\n<tr>\n<td>Validation</td>\n<td>Manual type checking</td>\n<td>Marshmallow schemas with custom validators</td>\n</tr>\n<tr>\n<td>Logging Backend</td>\n<td>Python logging with structured formatters</td>\n<td>structlog with context preservation</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>src/fine_tuning_pipeline/\n  config/\n    __init__.py\n    training_config.py       ← TrainingConfig and sub-configurations\n    data_structures.py       ← Training data structures\n    validation.py           ← Configuration validation functions\n  metrics/\n    __init__.py\n    training_metrics.py     ← TrainingMetrics and logging\n    evaluation_metrics.py   ← EvaluationMetrics and benchmark results\n    memory_monitor.py       ← MemoryMetrics and monitoring utilities\n  utils/\n    serialization.py        ← Configuration save/load utilities\n    type_helpers.py         ← Type validation and conversion utilities</code></pre></div>\n\n<p><strong>Configuration Management Infrastructure (Complete):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Complete configuration management system with validation and serialization.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">This provides the foundation for all pipeline components.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field, asdict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, List, Dict, Any, Union</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> yaml</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> QuantizationType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NF4</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"nf4\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FP4</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"fp4\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ComputeDType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FLOAT16</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"float16\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    BFLOAT16</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"bfloat16\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FLOAT32</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"float32\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> QuantizationConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"4-bit quantization configuration with validation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    load_in_4bit: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bnb_4bit_quant_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"nf4\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bnb_4bit_compute_dtype: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"float16\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bnb_4bit_use_double_quant: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bnb_4bit_quant_storage: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"uint8\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate quantization configuration after creation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        valid_quant_types </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">\"nf4\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"fp4\"</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.bnb_4bit_quant_type </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> valid_quant_types:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Invalid quant_type: </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.bnb_4bit_quant_type</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">. Must be one of </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">valid_quant_types</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        valid_compute_dtypes </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">\"float16\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"bfloat16\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"float32\"</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.bnb_4bit_compute_dtype </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> valid_compute_dtypes:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Invalid compute_dtype: </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.bnb_4bit_compute_dtype</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">. Must be one of </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">valid_compute_dtypes</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> LoRAConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"LoRA adapter configuration with parameter validation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    r: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 16</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    alpha: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dropout: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    target_modules: Optional[List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bias: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"none\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    task_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"CAUSAL_LM\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lora_alpha_scaling: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    init_lora_weights: Union[</span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate LoRA configuration parameters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.r </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> or</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.r </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 512</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"LoRA rank must be between 1 and 512, got </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.r</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.alpha </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"LoRA alpha must be positive, got </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.alpha</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#F97583\"> &#x3C;=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.dropout </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Dropout must be between 0 and 1, got </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.dropout</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        valid_bias_options </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">\"none\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"all\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"lora_only\"</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.bias </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> valid_bias_options:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Invalid bias option: </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.bias</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">. Must be one of </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">valid_bias_options</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">property</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> effective_alpha</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate effective alpha scaling factor.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.alpha </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.r </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.lora_alpha_scaling </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.alpha</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> load_config_from_yaml</span><span style=\"color:#E1E4E8\">(config_path: Path) -> </span><span style=\"color:#9ECBFF\">\"TrainingConfig\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Load and validate training configuration from YAML file.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(config_path, </span><span style=\"color:#9ECBFF\">'r'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config_dict </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> yaml.safe_load(f)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Extract nested configurations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    quantization_dict </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config_dict.pop(</span><span style=\"color:#9ECBFF\">'quantization'</span><span style=\"color:#E1E4E8\">, {})</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lora_dict </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config_dict.pop(</span><span style=\"color:#9ECBFF\">'lora'</span><span style=\"color:#E1E4E8\">, {})</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Create configuration objects</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    quantization_config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> QuantizationConfig(</span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">quantization_dict)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lora_config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> LoRAConfig(</span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">lora_dict)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Create main configuration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    training_config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TrainingConfig(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        quantization</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">quantization_config,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        lora</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">lora_config,</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        **</span><span style=\"color:#E1E4E8\">config_dict</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> training_config</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> save_config_to_yaml</span><span style=\"color:#E1E4E8\">(config: </span><span style=\"color:#9ECBFF\">\"TrainingConfig\"</span><span style=\"color:#E1E4E8\">, config_path: Path) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Save training configuration to YAML file with proper structure.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config_dict </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> asdict(config)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(config_path, </span><span style=\"color:#9ECBFF\">'w'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        yaml.dump(config_dict, f, </span><span style=\"color:#FFAB70\">default_flow_style</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">indent</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Core Training Configuration Structure (Skeleton):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TrainingConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Complete training configuration with validation and defaults.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Model configuration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    model_name_or_path: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    quantization: QuantizationConfig </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">QuantizationConfig)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lora: LoRAConfig </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">LoRAConfig)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Training hyperparameters  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    learning_rate: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 5e-5</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_train_epochs: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    per_device_train_batch_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    per_device_eval_batch_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gradient_accumulation_steps: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 4</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    warmup_steps: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_seq_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2048</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # System configuration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dataloader_num_workers: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fp16: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bf16: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dataloader_pin_memory: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gradient_checkpointing: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Logging and evaluation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logging_steps: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    eval_steps: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 500</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    save_steps: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 500</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    save_total_limit: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    load_best_model_at_end: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metric_for_best_model: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"eval_loss\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    greater_is_better: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Early stopping</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    early_stopping_patience: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    early_stopping_threshold: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.001</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate training configuration after initialization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate learning rate is reasonable (1e-6 to 1e-2)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check that effective batch size is reasonable (grad_accum * batch_size >= 8)  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Verify sequence length doesn't exceed model's max position embeddings</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Validate early stopping patience is positive</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Check that save_steps is multiple of eval_steps for consistent checkpointing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use self.quantization and self.lora to access sub-configurations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">property</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> effective_batch_size</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate effective training batch size accounting for gradient accumulation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return per_device_train_batch_size * gradient_accumulation_steps</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Memory Monitoring Infrastructure (Complete):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Complete memory monitoring system for tracking GPU and system memory usage.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">This provides real-time visibility into memory consumption patterns.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> psutil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, List, Dict, Any</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MemoryMetrics</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Memory usage metrics at a specific point in time.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stage: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gpu_memory_allocated: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gpu_memory_cached: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gpu_memory_reserved: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    system_memory_used: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    details: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MemoryMonitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Comprehensive memory monitoring for training pipeline.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.baseline_gpu_memory: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.baseline_system_memory: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.measurements: List[Dict] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check if CUDA is available</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cuda_available </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.cuda_available:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            torch.cuda.empty_cache()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> capture_baseline</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record initial memory state before model loading.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.cuda_available:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            torch.cuda.empty_cache()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            torch.cuda.synchronize()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.baseline_gpu_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_allocated()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.baseline_system_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.virtual_memory().used</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        baseline_metrics </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.measure_current_usage(</span><span style=\"color:#9ECBFF\">\"baseline\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.measurements.append(baseline_metrics)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> measure_current_usage</span><span style=\"color:#E1E4E8\">(self, stage: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Measure current memory usage and return statistics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # GPU memory measurement</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        gpu_allocated </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        gpu_cached </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        gpu_reserved </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.cuda_available:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            torch.cuda.synchronize()  </span><span style=\"color:#6A737D\"># Ensure all operations complete</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            gpu_allocated </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_allocated() </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># GB</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            gpu_cached </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_cached() </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            gpu_reserved </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_reserved() </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # System memory measurement</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        system_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.virtual_memory().used </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># GB</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metrics </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"stage\"</span><span style=\"color:#E1E4E8\">: stage,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"gpu_memory_allocated\"</span><span style=\"color:#E1E4E8\">: gpu_allocated,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"gpu_memory_cached\"</span><span style=\"color:#E1E4E8\">: gpu_cached,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"gpu_memory_reserved\"</span><span style=\"color:#E1E4E8\">: gpu_reserved, </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"system_memory_used\"</span><span style=\"color:#E1E4E8\">: system_memory,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"timestamp\"</span><span style=\"color:#E1E4E8\">: timestamp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.measurements.append(metrics)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_peak_usage</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return peak memory usage across all measurements.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.measurements:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">\"error\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"No measurements recorded\"</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        peak_gpu </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(m[</span><span style=\"color:#9ECBFF\">\"gpu_memory_allocated\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> m </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.measurements)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        peak_system </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(m[</span><span style=\"color:#9ECBFF\">\"system_memory_used\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> m </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.measurements)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"peak_gpu_memory_gb\"</span><span style=\"color:#E1E4E8\">: peak_gpu,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"peak_system_memory_gb\"</span><span style=\"color:#E1E4E8\">: peak_system,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"measurement_count\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.measurements)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint:</strong>\nAfter implementing the data model structures, verify the following behavior:</p>\n<ol>\n<li><strong>Configuration Validation</strong>: Run <code>python -c &quot;from config import TrainingConfig; TrainingConfig(model_name_or_path=&#39;test&#39;)&quot;</code> - should create valid config with defaults</li>\n<li><strong>Memory Monitoring</strong>: Create MemoryMonitor, call <code>capture_baseline()</code>, load a small model, call <code>measure_current_usage(&#39;model_loaded&#39;)</code> - should show memory increase</li>\n<li><strong>YAML Serialization</strong>: Save config to YAML, load it back, verify all parameters match exactly</li>\n<li><strong>Invalid Configuration Handling</strong>: Try creating LoRAConfig with negative rank - should raise ValueError with clear message</li>\n</ol>\n<p>Expected output: Configuration objects created successfully, memory measurements show realistic GPU/system usage, YAML round-trip preserves all values, validation catches parameter errors before training begins.</p>\n<h2 id=\"dataset-preparation-component\">Dataset Preparation Component</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1 - this section implements the data loading, validation, formatting, and preprocessing capabilities that transform raw training data into tokenized instruction-response pairs ready for fine-tuning</p>\n</blockquote>\n<p>The dataset preparation component serves as the foundation of the entire fine-tuning pipeline, transforming heterogeneous raw data into carefully structured, tokenized samples that enable effective parameter-efficient fine-tuning. This component must handle the complexities of modern conversational AI training data while ensuring compatibility with the specific chat templates and tokenization requirements of different language models.</p>\n<h3 id=\"mental-model-the-language-tutor\">Mental Model: The Language Tutor</h3>\n<p>Think of the dataset preparation component as <strong>The Language Tutor</strong> - an experienced teacher who takes raw educational materials from various sources and transforms them into structured lesson plans optimized for a specific student&#39;s learning style.</p>\n<p>Just as a skilled tutor would:</p>\n<ul>\n<li><strong>Collect diverse materials</strong> from textbooks, articles, and conversations, then standardize them into a consistent format</li>\n<li><strong>Apply pedagogical templates</strong> that match the student&#39;s preferred learning style (visual, auditory, kinesthetic)</li>\n<li><strong>Break complex lessons</strong> into appropriately-sized chunks that fit the student&#39;s attention span</li>\n<li><strong>Create practice sets</strong> by splitting materials into learning exercises and assessment questions</li>\n<li><strong>Filter out distractions</strong> by removing low-quality or irrelevant content that might confuse the learning process</li>\n</ul>\n<p>The dataset preparation component performs analogous transformations:</p>\n<ul>\n<li><strong>Ingests heterogeneous data</strong> from JSON, JSONL, CSV, and Parquet sources into a unified schema</li>\n<li><strong>Applies chat templates</strong> that match the target model&#39;s expected conversation format</li>\n<li><strong>Tokenizes and chunks</strong> text into sequences that fit within the model&#39;s context length limits</li>\n<li><strong>Creates train-validation splits</strong> that enable learning assessment without data leakage</li>\n<li><strong>Filters for quality</strong> by removing duplicates, malformed samples, and content that exceeds token limits</li>\n</ul>\n<p>This mental model emphasizes that dataset preparation is not merely a mechanical transformation, but a thoughtful curation process that directly impacts the quality and effectiveness of the fine-tuning process.</p>\n<p><img src=\"/api/project/llm-finetuning-pipeline/architecture-doc/asset?path=diagrams%2Fdata-preparation-flow.svg\" alt=\"Data Preparation Pipeline\"></p>\n<h3 id=\"data-ingestion-and-validation\">Data Ingestion and Validation</h3>\n<p>The data ingestion subsystem provides the entry point for raw training data, supporting multiple file formats while ensuring data quality and consistency. This subsystem must handle the reality that training data often comes from diverse sources with varying schemas and quality levels.</p>\n<p>The ingestion process begins with <strong>format detection and loading</strong>, where the system automatically identifies file types and applies appropriate parsing strategies. Different formats require different handling approaches due to their structural characteristics and common usage patterns in the machine learning community.</p>\n<table>\n<thead>\n<tr>\n<th>Format</th>\n<th>Typical Use Case</th>\n<th>Parsing Strategy</th>\n<th>Memory Consideration</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>JSON</td>\n<td>Small to medium datasets with complex nested structures</td>\n<td>Load entire file into memory, parse as single object</td>\n<td>Suitable for files under 1GB</td>\n</tr>\n<tr>\n<td>JSONL</td>\n<td>Large datasets with simple record structure</td>\n<td>Stream line-by-line parsing</td>\n<td>Memory-efficient for multi-GB files</td>\n</tr>\n<tr>\n<td>CSV</td>\n<td>Tabular data from databases or spreadsheets</td>\n<td>Pandas or streaming CSV parser</td>\n<td>Moderate memory usage with chunking</td>\n</tr>\n<tr>\n<td>Parquet</td>\n<td>Large-scale datasets with schema enforcement</td>\n<td>Columnar reading with PyArrow</td>\n<td>Most memory-efficient for large datasets</td>\n</tr>\n</tbody></table>\n<p>The <strong>data validation layer</strong> ensures that ingested records contain the required fields for instruction tuning. The validation process operates on a progressive strictness model, where certain fields are mandatory while others provide optional enhancement to the training process.</p>\n<table>\n<thead>\n<tr>\n<th>Field Category</th>\n<th>Required Fields</th>\n<th>Optional Fields</th>\n<th>Validation Rules</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Core Content</td>\n<td><code>instruction</code>, <code>response</code></td>\n<td><code>input_context</code>, <code>system_prompt</code></td>\n<td>Non-empty strings, reasonable length limits</td>\n</tr>\n<tr>\n<td>Metadata</td>\n<td><code>sample_id</code></td>\n<td><code>source</code>, <code>quality_score</code>, <code>conversation_id</code></td>\n<td>Unique IDs, valid score ranges</td>\n</tr>\n<tr>\n<td>Quality Indicators</td>\n<td>None</td>\n<td><code>length_tokens</code>, <code>language</code>, <code>difficulty</code></td>\n<td>Positive integers, ISO language codes</td>\n</tr>\n</tbody></table>\n<p><strong>Quality filtering</strong> removes samples that could negatively impact training effectiveness. The filtering pipeline applies multiple criteria to identify problematic content:</p>\n<ol>\n<li><strong>Duplicate detection</strong> using content hashing to identify exact and near-duplicate instruction-response pairs</li>\n<li><strong>Length validation</strong> ensuring samples fall within reasonable token count ranges for the target model</li>\n<li><strong>Content quality assessment</strong> checking for obvious spam, garbled text, or inappropriate content</li>\n<li><strong>Language consistency</strong> verifying that instruction and response use the same primary language</li>\n<li><strong>Completeness validation</strong> ensuring responses are not truncated or obviously incomplete</li>\n</ol>\n<p>The validation pipeline maintains detailed statistics about the filtering process, enabling users to understand how much data was removed and for what reasons.</p>\n<blockquote>\n<p><strong>Key Design Insight</strong>: The validation pipeline is designed to be conservative - it errs on the side of removing questionable samples rather than allowing low-quality data to pollute the training process. High-quality training data is far more valuable than large quantities of mediocre data.</p>\n</blockquote>\n<p><strong>Architecture Decision: Format-Agnostic Internal Representation</strong></p>\n<blockquote>\n<p><strong>Decision: Standardize on InstructionSample for Internal Processing</strong></p>\n<ul>\n<li><strong>Context</strong>: Raw data comes in many formats with different field names and structures, but downstream components need consistent interfaces</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Pass format-specific dictionaries and handle conversions in each component</li>\n<li>Create format-specific classes for each input type</li>\n<li>Standardize on a single <code>InstructionSample</code> structure with optional fields</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use <code>InstructionSample</code> as the canonical internal representation</li>\n<li><strong>Rationale</strong>: This provides type safety, clear documentation of expected fields, and simplifies downstream component interfaces while maintaining flexibility for optional fields</li>\n<li><strong>Consequences</strong>: Requires upfront conversion cost but eliminates format-handling complexity from all downstream components</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>InstructionSample Field</th>\n<th>Type</th>\n<th>Purpose</th>\n<th>Validation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>instruction</code></td>\n<td>str</td>\n<td>The task or question being asked</td>\n<td>Non-empty, reasonable length</td>\n</tr>\n<tr>\n<td><code>response</code></td>\n<td>str</td>\n<td>The expected response or answer</td>\n<td>Non-empty, reasonable length</td>\n</tr>\n<tr>\n<td><code>system_prompt</code></td>\n<td>Optional[str]</td>\n<td>Context or role definition</td>\n<td>Optional but validated if present</td>\n</tr>\n<tr>\n<td><code>input_context</code></td>\n<td>Optional[str]</td>\n<td>Additional context for the instruction</td>\n<td>Optional supporting information</td>\n</tr>\n<tr>\n<td><code>sample_id</code></td>\n<td>str</td>\n<td>Unique identifier for the sample</td>\n<td>Must be unique within dataset</td>\n</tr>\n<tr>\n<td><code>source</code></td>\n<td>str</td>\n<td>Origin of the data for tracking and filtering</td>\n<td>Helps with data provenance</td>\n</tr>\n<tr>\n<td><code>quality_score</code></td>\n<td>Optional[float]</td>\n<td>Automated or manual quality assessment</td>\n<td>Range 0.0 to 1.0 if provided</td>\n</tr>\n</tbody></table>\n<h3 id=\"chat-template-application\">Chat Template Application</h3>\n<p>The chat template application subsystem transforms instruction-response pairs into the specific conversational format expected by the target language model. This transformation is critical because different model families use incompatible chat formats that affect both training convergence and inference quality.</p>\n<p>Modern language models are trained with specific <strong>chat templates</strong> that define how multi-turn conversations are formatted with special tokens. These templates serve as the &quot;grammar&quot; for how the model expects to see conversational data structured.</p>\n<p><strong>Chat Template Concepts and Variations</strong></p>\n<p>Different model families implement distinct approaches to conversation formatting:</p>\n<table>\n<thead>\n<tr>\n<th>Model Family</th>\n<th>Template Style</th>\n<th>Special Tokens</th>\n<th>Turn Separation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Llama-2-Chat</td>\n<td>XML-style tags</td>\n<td><code>&lt;s&gt;</code>, <code>&lt;/s&gt;</code>, <code>[INST]</code>, <code>[/INST]</code></td>\n<td>Tagged instruction blocks</td>\n</tr>\n<tr>\n<td>ChatML (GPT-4)</td>\n<td>Role-based</td>\n<td>`&lt;</td>\n<td>im_start</td>\n</tr>\n<tr>\n<td>Alpaca</td>\n<td>Instruction format</td>\n<td>Standard tokens</td>\n<td>Fixed template with placeholders</td>\n</tr>\n<tr>\n<td>Vicuna</td>\n<td>Conversation style</td>\n<td>Standard tokens</td>\n<td>USER/ASSISTANT prefixes</td>\n</tr>\n</tbody></table>\n<p>The template application process must handle several complexities:</p>\n<ol>\n<li><strong>System message integration</strong> - Incorporating system prompts that define the model&#39;s role or behavior guidelines</li>\n<li><strong>Multi-turn conversation handling</strong> - Managing instruction-response pairs that are part of longer conversations</li>\n<li><strong>Token efficiency</strong> - Minimizing template overhead to maximize content within context limits</li>\n<li><strong>Special token placement</strong> - Ensuring proper positioning of begin/end markers for training effectiveness</li>\n</ol>\n<p><strong>Template Application Algorithm</strong></p>\n<p>The chat template application follows a systematic process to ensure consistency and correctness:</p>\n<ol>\n<li><strong>Identify the target model&#39;s template format</strong> from the tokenizer configuration or explicit specification</li>\n<li><strong>Extract conversation components</strong> including system prompt, instruction, and response from the <code>InstructionSample</code></li>\n<li><strong>Construct the conversation structure</strong> by organizing components into the model&#39;s expected role sequence</li>\n<li><strong>Apply template formatting</strong> using the model-specific special tokens and delimiters</li>\n<li><strong>Validate template correctness</strong> ensuring proper token pairing and expected sequence structure</li>\n<li><strong>Generate final formatted text</strong> ready for tokenization with appropriate special token placement</li>\n</ol>\n<p><strong>Architecture Decision: Template Auto-Detection vs Explicit Configuration</strong></p>\n<blockquote>\n<p><strong>Decision: Use Auto-Detection with Manual Override Capability</strong></p>\n<ul>\n<li><strong>Context</strong>: Different models require different chat templates, and users may work with custom models or want to experiment with template variations</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Require explicit template specification for every model</li>\n<li>Auto-detect from tokenizer configuration only</li>\n<li>Auto-detect with manual override capability</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Auto-detect from the tokenizer&#39;s <code>chat_template</code> attribute with option to provide custom templates</li>\n<li><strong>Rationale</strong>: Reduces configuration burden for standard models while maintaining flexibility for custom scenarios</li>\n<li><strong>Consequences</strong>: Requires robust template detection logic and clear error messages when auto-detection fails</li>\n</ul>\n</blockquote>\n<h3 id=\"tokenization-and-length-handling\">Tokenization and Length Handling</h3>\n<p>The tokenization subsystem converts formatted text into numerical token sequences that the model can process during training. This conversion must handle the complexities of subword tokenization while respecting length constraints and maintaining proper attention masking.</p>\n<p><strong>Tokenization Process and Considerations</strong></p>\n<p>Modern language models use <strong>subword tokenization</strong> schemes like Byte-Pair Encoding (BPE) or SentencePiece that split text into smaller units than whole words. This approach enables the model to handle out-of-vocabulary words and improves computational efficiency.</p>\n<p>The tokenization process for instruction tuning requires special handling to distinguish between input (instruction) and target (response) portions of the sequence:</p>\n<ol>\n<li><strong>Separate tokenization</strong> of instruction and response components to track boundaries</li>\n<li><strong>Label masking</strong> where instruction tokens receive -100 labels to exclude them from loss calculation</li>\n<li><strong>Attention mask generation</strong> ensuring the model attends to all relevant tokens</li>\n<li><strong>Special token insertion</strong> for begin-of-sequence, end-of-sequence, and padding tokens</li>\n<li><strong>Length validation</strong> ensuring the complete sequence fits within model context limits</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Tokenization Component</th>\n<th>Purpose</th>\n<th>Implementation Considerations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Input Tokenization</td>\n<td>Convert instruction text to token IDs</td>\n<td>Include chat template tokens, track boundary</td>\n</tr>\n<tr>\n<td>Response Tokenization</td>\n<td>Convert response text to token IDs</td>\n<td>Mark as training targets, add EOS token</td>\n</tr>\n<tr>\n<td>Label Generation</td>\n<td>Create loss calculation targets</td>\n<td>Set instruction tokens to -100, response tokens to their IDs</td>\n</tr>\n<tr>\n<td>Attention Masking</td>\n<td>Define which tokens to attend to</td>\n<td>Usually 1 for all non-padding tokens</td>\n</tr>\n<tr>\n<td>Length Management</td>\n<td>Handle sequences exceeding context limit</td>\n<td>Truncation strategies and boundary preservation</td>\n</tr>\n</tbody></table>\n<p><strong>Length Handling Strategies</strong></p>\n<p>When tokenized sequences exceed the model&#39;s context length, the system must apply truncation strategies that preserve training effectiveness:</p>\n<table>\n<thead>\n<tr>\n<th>Strategy</th>\n<th>Application</th>\n<th>Pros</th>\n<th>Cons</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Truncate Response</td>\n<td>Cut response at context limit</td>\n<td>Preserves complete instruction</td>\n<td>May lose important response content</td>\n</tr>\n<tr>\n<td>Truncate Instruction</td>\n<td>Cut instruction, preserve response</td>\n<td>Maintains complete response for learning</td>\n<td>May lose essential context</td>\n</tr>\n<tr>\n<td>Proportional Truncation</td>\n<td>Reduce both instruction and response</td>\n<td>Balanced approach</td>\n<td>Both components may lose information</td>\n</tr>\n<tr>\n<td>Sample Rejection</td>\n<td>Remove samples exceeding limits</td>\n<td>Maintains data quality</td>\n<td>Reduces dataset size</td>\n</tr>\n</tbody></table>\n<p>The length handling pipeline maintains statistics about truncation decisions to help users understand the impact on their dataset.</p>\n<p><strong>Architecture Decision: Token-Level vs Character-Level Length Limits</strong></p>\n<blockquote>\n<p><strong>Decision: Use Token-Level Limits with Character-Level Pre-filtering</strong></p>\n<ul>\n<li><strong>Context</strong>: Model context limits are defined in tokens, but token counting requires tokenization which is computationally expensive</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Use character-level approximations only</li>\n<li>Tokenize everything for exact token counts</li>\n<li>Character-level pre-filtering followed by exact token counting</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Apply character-level filtering (assuming ~4 chars per token) followed by exact tokenization for remaining samples</li>\n<li><strong>Rationale</strong>: Eliminates obviously oversized samples efficiently while ensuring accuracy for borderline cases</li>\n<li><strong>Consequences</strong>: Requires maintaining both character and token thresholds but significantly reduces tokenization overhead</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>TokenizedSample Field</th>\n<th>Type</th>\n<th>Purpose</th>\n<th>Generation Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>input_ids</code></td>\n<td>List[int]</td>\n<td>Token sequence for model input</td>\n<td>Concatenated instruction + response tokens</td>\n</tr>\n<tr>\n<td><code>attention_mask</code></td>\n<td>List[int]</td>\n<td>Attention pattern definition</td>\n<td>1 for content tokens, 0 for padding</td>\n</tr>\n<tr>\n<td><code>labels</code></td>\n<td>List[int]</td>\n<td>Training targets for loss calculation</td>\n<td>-100 for instruction, token IDs for response</td>\n</tr>\n<tr>\n<td><code>prompt_length</code></td>\n<td>int</td>\n<td>Number of tokens in instruction portion</td>\n<td>Used for label masking and analysis</td>\n</tr>\n<tr>\n<td><code>response_length</code></td>\n<td>int</td>\n<td>Number of tokens in response portion</td>\n<td>Training target length tracking</td>\n</tr>\n<tr>\n<td><code>total_length</code></td>\n<td>int</td>\n<td>Complete sequence length</td>\n<td>Context limit validation</td>\n</tr>\n</tbody></table>\n<h3 id=\"train-validation-splitting\">Train-Validation Splitting</h3>\n<p>The train-validation splitting subsystem partitions the processed dataset into separate training and evaluation sets while avoiding data leakage and ensuring representative distributions. This split is crucial for monitoring training progress and preventing overfitting during fine-tuning.</p>\n<p><strong>Splitting Strategy Considerations</strong></p>\n<p>The splitting process must balance several competing objectives:</p>\n<ol>\n<li><strong>Statistical representativeness</strong> - Validation set should reflect the same distribution as training data</li>\n<li><strong>Temporal consistency</strong> - For time-series data, validation should not contain future information</li>\n<li><strong>Source diversity</strong> - Both sets should contain samples from similar data sources</li>\n<li><strong>Size optimization</strong> - Validation set should be large enough for reliable evaluation but not waste training data</li>\n</ol>\n<p><strong>Standard Splitting Approaches</strong></p>\n<table>\n<thead>\n<tr>\n<th>Splitting Method</th>\n<th>Use Case</th>\n<th>Implementation</th>\n<th>Pros</th>\n<th>Cons</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Random Split</td>\n<td>General purpose, mixed-source data</td>\n<td>Shuffle and partition by percentage</td>\n<td>Simple, unbiased</td>\n<td>May not preserve temporal order</td>\n</tr>\n<tr>\n<td>Stratified Split</td>\n<td>Maintaining category proportions</td>\n<td>Split within each category/source</td>\n<td>Preserves distributions</td>\n<td>Requires meaningful stratification keys</td>\n</tr>\n<tr>\n<td>Temporal Split</td>\n<td>Time-ordered data</td>\n<td>Split at time boundary</td>\n<td>Realistic evaluation</td>\n<td>May introduce distribution shift</td>\n</tr>\n<tr>\n<td>Source-Aware Split</td>\n<td>Multi-source datasets</td>\n<td>Ensure sources in both sets</td>\n<td>Balanced source representation</td>\n<td>Complex implementation</td>\n</tr>\n</tbody></table>\n<p><strong>Stratification Implementation</strong></p>\n<p>For instruction tuning datasets, stratification can be applied across multiple dimensions to ensure representative splits:</p>\n<table>\n<thead>\n<tr>\n<th>Stratification Key</th>\n<th>Purpose</th>\n<th>Implementation Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Source</td>\n<td>Maintain source diversity</td>\n<td>Group by <code>source</code> field, split proportionally</td>\n</tr>\n<tr>\n<td>Instruction Type</td>\n<td>Preserve task variety</td>\n<td>Classify instruction types, balance across splits</td>\n</tr>\n<tr>\n<td>Response Length</td>\n<td>Maintain length distribution</td>\n<td>Bin by token count, split within bins</td>\n</tr>\n<tr>\n<td>Quality Score</td>\n<td>Preserve quality distribution</td>\n<td>Bin by quality score, ensure balanced representation</td>\n</tr>\n</tbody></table>\n<p><strong>Architecture Decision: Fixed vs Dynamic Validation Set</strong></p>\n<blockquote>\n<p><strong>Decision: Support Both Fixed and Dynamic Validation Sets</strong></p>\n<ul>\n<li><strong>Context</strong>: Some users want consistent validation sets for reproducibility, others want fresh validation data for each experiment</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Always create fresh random splits</li>\n<li>Always use fixed, predetermined validation sets</li>\n<li>Support both modes with configuration option</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Default to seeded random splits for reproducibility, with option to provide pre-defined validation data</li>\n<li><strong>Rationale</strong>: Balances reproducibility needs with flexibility for different experimental setups</li>\n<li><strong>Consequences</strong>: Requires seed management and validation set consistency checks</li>\n</ul>\n</blockquote>\n<p>The splitting process generates detailed statistics about the resulting partitions:</p>\n<table>\n<thead>\n<tr>\n<th>Split Statistic</th>\n<th>Training Set</th>\n<th>Validation Set</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Sample Count</td>\n<td>Total training samples</td>\n<td>Total validation samples</td>\n<td>Size verification</td>\n</tr>\n<tr>\n<td>Average Token Length</td>\n<td>Mean tokens per sample</td>\n<td>Mean tokens per sample</td>\n<td>Length distribution check</td>\n</tr>\n<tr>\n<td>Source Distribution</td>\n<td>Percentage from each source</td>\n<td>Percentage from each source</td>\n<td>Source balance verification</td>\n</tr>\n<tr>\n<td>Quality Score Distribution</td>\n<td>Mean and std dev of scores</td>\n<td>Mean and std dev of scores</td>\n<td>Quality balance check</td>\n</tr>\n</tbody></table>\n<h3 id=\"architecture-decision-records\">Architecture Decision Records</h3>\n<p>Several key decisions shape the dataset preparation component&#39;s architecture and behavior:</p>\n<p><strong>Architecture Decision: Streaming vs Batch Processing</strong></p>\n<blockquote>\n<p><strong>Decision: Hybrid Approach with Streaming for Large Files</strong></p>\n<ul>\n<li><strong>Context</strong>: Training datasets can range from thousands to millions of samples, requiring different memory management strategies</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Load entire dataset into memory for fast random access</li>\n<li>Stream all processing to minimize memory usage</li>\n<li>Hybrid approach based on dataset size and available memory</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use memory-based processing for datasets under 100K samples, streaming for larger datasets</li>\n<li><strong>Rationale</strong>: Small datasets benefit from fast random access during splitting and shuffling, while large datasets require streaming to avoid memory overflow</li>\n<li><strong>Consequences</strong>: Requires implementing both code paths but optimizes for common use cases while scaling to large datasets</li>\n</ul>\n</blockquote>\n<p><strong>Architecture Decision: Error Handling Philosophy</strong></p>\n<blockquote>\n<p><strong>Decision: Fail-Fast for Configuration Errors, Continue for Data Errors</strong></p>\n<ul>\n<li><strong>Context</strong>: Dataset preparation involves both configuration validation and individual sample processing, each with different error recovery requirements</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Fail immediately on any error</li>\n<li>Continue processing and report all errors at the end</li>\n<li>Fail fast for configuration, continue for recoverable data errors</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Halt immediately for configuration problems (missing files, invalid formats), but continue processing individual samples that fail validation</li>\n<li><strong>Rationale</strong>: Configuration errors indicate fundamental setup problems that require user intervention, while individual sample failures are often due to data quality issues that can be filtered out</li>\n<li><strong>Consequences</strong>: Requires clear error categorization and robust logging to help users distinguish between critical and recoverable errors</li>\n</ul>\n</blockquote>\n<p><strong>Architecture Decision: Memory Optimization Strategy</strong></p>\n<blockquote>\n<p><strong>Decision: Lazy Loading with Configurable Caching</strong></p>\n<ul>\n<li><strong>Context</strong>: Tokenized samples consume significant memory, especially for long sequences, but re-tokenization is computationally expensive</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Pre-tokenize and cache all samples in memory</li>\n<li>Tokenize on-demand with no caching</li>\n<li>Configurable caching based on available memory and dataset size</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement lazy tokenization with LRU cache sized based on available system memory</li>\n<li><strong>Rationale</strong>: Balances memory efficiency with computational performance, adapting to available hardware resources</li>\n<li><strong>Consequences</strong>: Requires memory monitoring and cache management logic but provides optimal performance across different hardware configurations</li>\n</ul>\n</blockquote>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>The dataset preparation component involves several areas where developers commonly encounter issues. Understanding these pitfalls and their solutions is essential for successful implementation.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Chat Template Application</strong></p>\n<p>A frequent error occurs when developers apply chat templates inconsistently across samples or use templates that don&#39;t match the target model. This happens when the template detection logic fails or when mixing data from different model families.</p>\n<p><strong>Why it&#39;s wrong</strong>: Inconsistent templates confuse the model&#39;s learned conversation patterns, leading to poor generation quality and training instability. The model expects specific token sequences to indicate role transitions and conversation structure.</p>\n<p><strong>How to fix</strong>: Implement template validation that verifies the chosen template matches the model&#39;s tokenizer configuration. Add consistency checks that ensure all samples in a dataset use the same template format. Include explicit template specification options for custom models.</p>\n<p>⚠️ <strong>Pitfall: Label Masking Errors</strong></p>\n<p>Developers often incorrectly set up the labels array, either forgetting to mask instruction tokens (-100) or including padding tokens in the loss calculation. This leads to the model learning to predict its own instructions rather than generating appropriate responses.</p>\n<p><strong>Why it&#39;s wrong</strong>: When instruction tokens aren&#39;t masked, the model learns to memorize input patterns rather than learning the input-output mapping. This results in poor response generation and potential overfitting to instruction formats.</p>\n<p><strong>How to fix</strong>: Implement careful label generation that tracks the boundary between instruction and response tokens. Set instruction tokens and padding tokens to -100, and only use response token IDs as training targets. Validate label arrays to ensure they contain the expected number of non-masked tokens.</p>\n<p>⚠️ <strong>Pitfall: Context Length Boundary Violations</strong></p>\n<p>When sequences exceed the model&#39;s context length, naive truncation can cut tokens in the middle of important content or remove essential special tokens. This corrupts the training data and degrades model performance.</p>\n<p><strong>Why it&#39;s wrong</strong>: Cutting sequences at arbitrary boundaries can remove critical response content or leave conversations in invalid states. Missing special tokens break the model&#39;s understanding of conversation structure.</p>\n<p><strong>How to fix</strong>: Implement intelligent truncation that preserves special tokens and attempts to maintain complete thoughts. Use proportional truncation strategies that reduce both instruction and response content rather than arbitrary cutoff points. Consider rejecting samples that cannot be meaningfully truncated.</p>\n<p>⚠️ <strong>Pitfall: Validation Set Data Leakage</strong></p>\n<p>Creating validation sets without considering data relationships can result in near-duplicate samples appearing in both training and validation sets. This leads to overly optimistic evaluation metrics that don&#39;t reflect real-world performance.</p>\n<p><strong>Why it&#39;s wrong</strong>: Data leakage inflates validation scores, making it impossible to detect overfitting or compare model performance accurately. The model appears to perform better than it actually will on unseen data.</p>\n<p><strong>How to fix</strong>: Implement deduplication that considers semantic similarity, not just exact matches. For conversation datasets, ensure that related conversations (same topic, same user session) are kept together in the same split. Use content hashing and similarity measures to detect near-duplicates before splitting.</p>\n<p>⚠️ <strong>Pitfall: Tokenizer Mismatch</strong></p>\n<p>Using a tokenizer that doesn&#39;t match the target model leads to incorrect vocabulary mapping and degraded performance. This often happens when developers use a convenient tokenizer for preprocessing but switch to a different model for training.</p>\n<p><strong>Why it&#39;s wrong</strong>: Different tokenizers produce different token sequences for the same text. Training data tokenized with one tokenizer cannot be correctly interpreted by a model expecting a different tokenizer&#39;s vocabulary.</p>\n<p><strong>How to fix</strong>: Always use the exact tokenizer associated with the target model. Load the tokenizer from the same model checkpoint or repository that will be used for training. Implement tokenizer validation checks that verify vocabulary compatibility.</p>\n<p>⚠️ <strong>Pitfall: Memory Management Failures</strong></p>\n<p>Attempting to load large datasets entirely into memory without considering available system resources leads to out-of-memory crashes or excessive swap usage that severely impacts performance.</p>\n<p><strong>Why it&#39;s wrong</strong>: Large datasets can consume tens of gigabytes of memory when fully loaded, especially after tokenization. This can crash the preprocessing pipeline or make the system unusable due to memory pressure.</p>\n<p><strong>How to fix</strong>: Implement memory monitoring that tracks dataset size and available system memory. Use streaming processing for large datasets and implement checkpointing so that preprocessing progress isn&#39;t lost due to memory issues. Provide clear memory requirements in documentation.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This subsection provides concrete implementation guidance for building the dataset preparation component, including technology recommendations, file structure, and starter code.</p>\n<p><strong>Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Loading</td>\n<td>pandas for CSV/JSON + json module</td>\n<td>Apache Arrow + PyArrow for Parquet</td>\n</tr>\n<tr>\n<td>Text Processing</td>\n<td>Basic string operations + regex</td>\n<td>spaCy or NLTK for advanced text analysis</td>\n</tr>\n<tr>\n<td>Tokenization</td>\n<td>HuggingFace Tokenizers library</td>\n<td>Custom BPE with tokenizers library</td>\n</tr>\n<tr>\n<td>Memory Management</td>\n<td>Python lists and dictionaries</td>\n<td>Memory-mapped files with NumPy</td>\n</tr>\n<tr>\n<td>Data Validation</td>\n<td>Manual type checking + assertions</td>\n<td>Pydantic models for schema validation</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>llm-fine-tuning/\n  src/\n    data_preparation/\n      __init__.py                    ← component exports\n      data_loader.py                ← format detection and loading\n      instruction_formatter.py      ← InstructionSample conversion\n      chat_templates.py             ← template application logic\n      tokenizer_pipeline.py         ← tokenization and length handling\n      dataset_splitter.py           ← train-validation splitting\n      quality_filter.py             ← data validation and filtering\n      utils.py                      ← shared utilities\n    config/\n      data_config.py                ← configuration schemas\n  tests/\n    test_data_preparation/          ← component tests\n  example_data/\n    sample_instruction_data.jsonl   ← test data for development</code></pre></div>\n\n<p><strong>Core Data Structures (Complete Implementation)</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, List, Dict, Any, Union</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> InstructionSample</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Standardized representation of an instruction-response pair.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    instruction: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    response: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    system_prompt: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    input_context: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sample_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"unknown\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    quality_score: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.sample_id:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.sample_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> hash</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.instruction </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.response)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> to_dict</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"instruction\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.instruction,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"response\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.response,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"system_prompt\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.system_prompt,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"input_context\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.input_context,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"sample_id\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.sample_id,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"source\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.source,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"quality_score\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.quality_score</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ConversationTurn</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Single turn in a multi-turn conversation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    role: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#6A737D\">  # \"user\", \"assistant\", \"system\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    content: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    turn_index: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ConversationSample</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Multi-turn conversation representation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    conversation_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    turns: List[ConversationTurn]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metadata: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    total_tokens: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    is_valid: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenizedSample</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Tokenized and formatted sample ready for training.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    input_ids: List[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    attention_mask: List[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    labels: List[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    prompt_length: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    response_length: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    total_length: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate tokenized sample consistency.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        expected_length </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.input_ids)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.attention_mask) </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> expected_length </span><span style=\"color:#F97583\">and</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.labels) </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> expected_length </span><span style=\"color:#F97583\">and</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.total_length </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> expected_length)</span></span></code></pre></div>\n\n<p><strong>Data Loading Infrastructure (Complete Starter Code)</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pandas </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> pd</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Iterator, List, Dict, Any, Union</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pyarrow.parquet </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> pq</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DataLoader</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Handles loading data from multiple formats with format auto-detection.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SUPPORTED_FORMATS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">'.json'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'.jsonl'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'.csv'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'.parquet'</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, chunk_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10000</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.chunk_size </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> chunk_size</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> detect_format</span><span style=\"color:#E1E4E8\">(self, file_path: Path) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Detect file format from extension.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        suffix </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> file_path.suffix.lower()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> suffix </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">SUPPORTED_FORMATS</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Unsupported format: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">suffix</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> suffix</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> load_data</span><span style=\"color:#E1E4E8\">(self, file_path: Path) -> Iterator[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load data with format-specific handling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        format_type </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.detect_format(file_path)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> format_type </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '.json'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            yield from</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._load_json(file_path)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> format_type </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '.jsonl'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            yield from</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._load_jsonl(file_path)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> format_type </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '.csv'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            yield from</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._load_csv(file_path)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> format_type </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '.parquet'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            yield from</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._load_parquet(file_path)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _load_json</span><span style=\"color:#E1E4E8\">(self, file_path: Path) -> Iterator[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load JSON file (assumes array of objects).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(file_path, </span><span style=\"color:#9ECBFF\">'r'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">encoding</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'utf-8'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> json.load(f)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(data, </span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                yield from</span><span style=\"color:#E1E4E8\"> data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                yield</span><span style=\"color:#E1E4E8\"> data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _load_jsonl</span><span style=\"color:#E1E4E8\">(self, file_path: Path) -> Iterator[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load JSONL file line by line.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(file_path, </span><span style=\"color:#9ECBFF\">'r'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">encoding</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'utf-8'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> line_num, line </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(f, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                line </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> line.strip()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> line:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    continue</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    yield</span><span style=\"color:#E1E4E8\"> json.loads(line)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                except</span><span style=\"color:#E1E4E8\"> json.JSONDecodeError </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Warning: Invalid JSON on line </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">line_num</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _load_csv</span><span style=\"color:#E1E4E8\">(self, file_path: Path) -> Iterator[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load CSV file in chunks.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> chunk </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> pd.read_csv(file_path, </span><span style=\"color:#FFAB70\">chunksize</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.chunk_size):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> _, row </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> chunk.iterrows():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                yield</span><span style=\"color:#E1E4E8\"> row.to_dict()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _load_parquet</span><span style=\"color:#E1E4E8\">(self, file_path: Path) -> Iterator[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load Parquet file in batches.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        parquet_file </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pq.ParquetFile(file_path)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> batch </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> parquet_file.iter_batches(</span><span style=\"color:#FFAB70\">batch_size</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.chunk_size):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            df </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> batch.to_pandas()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> _, row </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> df.iterrows():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                yield</span><span style=\"color:#E1E4E8\"> row.to_dict()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> InstructionFormatter</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Converts raw data dictionaries to InstructionSample objects.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Common field mappings from different data formats</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FIELD_MAPPINGS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'instruction'</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">'instruction'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'prompt'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'input'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'question'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'response'</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">'response'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'output'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'answer'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'completion'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'system_prompt'</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">'system'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'system_prompt'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'context'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'input_context'</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">'input_context'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'context'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'background'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, field_mapping: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize with optional custom field mapping.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.custom_mapping </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field_mapping </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> format_sample</span><span style=\"color:#E1E4E8\">(self, raw_data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> InstructionSample:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Convert raw dictionary to InstructionSample.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Extract instruction field using mapping priority</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Extract response field using mapping priority  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Extract optional fields (system_prompt, input_context)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Generate sample_id if not provided</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Set source field from metadata or filename</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Validate required fields are non-empty</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return constructed InstructionSample object</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _extract_field</span><span style=\"color:#E1E4E8\">(self, data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any], target_field: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Extract field using priority mapping.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check custom mapping first</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> target_field </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.custom_mapping:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            custom_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.custom_mapping[target_field]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> custom_key </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> data:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(data[custom_key])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check standard mappings</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        possible_keys </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">FIELD_MAPPINGS</span><span style=\"color:#E1E4E8\">.get(target_field, [])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> key </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> possible_keys:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> key </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> data </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> data[key] </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(data[key])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> None</span></span></code></pre></div>\n\n<p><strong>Quality Filtering Infrastructure (Complete Starter Code)</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> hashlib</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> re</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Set, List, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> QualityFilterStats</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Statistics from quality filtering process.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    total_samples: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    duplicates_removed: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    length_filtered: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    quality_filtered: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    final_count: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> QualityFilter</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Filters instruction samples for quality and consistency.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 min_instruction_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 max_instruction_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2048</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 min_response_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 max_response_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2048</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 min_quality_score: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.min_instruction_length </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> min_instruction_length</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_instruction_length </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_instruction_length</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.min_response_length </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> min_response_length</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_response_length </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_response_length</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.min_quality_score </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> min_quality_score</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.seen_hashes: Set[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.stats </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> QualityFilterStats()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> filter_dataset</span><span style=\"color:#E1E4E8\">(self, samples: List[InstructionSample]) -> Tuple[List[InstructionSample], QualityFilterStats]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Apply all quality filters to the dataset.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.stats </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> QualityFilterStats()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.stats.total_samples </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(samples)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        filtered_samples </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> sample </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> samples:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check for duplicates using content hash</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate instruction and response lengths</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Apply quality score filtering if available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check for obvious spam or corrupted content</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate language consistency between instruction/response</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Add to filtered list if all checks pass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.stats.final_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(filtered_samples)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> filtered_samples, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.stats</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _compute_content_hash</span><span style=\"color:#E1E4E8\">(self, sample: InstructionSample) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute hash for duplicate detection.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        content </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">sample.instruction.strip()</span><span style=\"color:#79B8FF\">}{</span><span style=\"color:#E1E4E8\">sample.response.strip()</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> hashlib.md5(content.encode(</span><span style=\"color:#9ECBFF\">'utf-8'</span><span style=\"color:#E1E4E8\">)).hexdigest()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _is_valid_length</span><span style=\"color:#E1E4E8\">(self, sample: InstructionSample) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if sample meets length requirements.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        inst_len </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(sample.instruction.strip())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        resp_len </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(sample.response.strip())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.min_instruction_length </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#E1E4E8\"> inst_len </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.max_instruction_length </span><span style=\"color:#F97583\">and</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.min_response_length </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#E1E4E8\"> resp_len </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.max_response_length)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _is_quality_content</span><span style=\"color:#E1E4E8\">(self, sample: InstructionSample) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Basic content quality checks.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement spam detection, language detection, completeness checks</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Core Logic Skeleton (for Student Implementation)</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ChatTemplateApplicator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Applies model-specific chat templates to instruction-response pairs.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, tokenizer, custom_template: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokenizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokenizer</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.custom_template </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> custom_template</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._detected_template </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> apply_template</span><span style=\"color:#E1E4E8\">(self, sample: InstructionSample) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Apply chat template to create formatted conversation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Detect or load the appropriate chat template</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Structure the conversation with roles (system, user, assistant)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Apply the template formatting with special tokens</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Validate the formatted output has proper token structure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return the complete formatted string ready for tokenization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use tokenizer.apply_chat_template() if available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Handle system prompts by adding them as first message</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _detect_template_format</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Auto-detect template format from tokenizer.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if tokenizer has chat_template attribute</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Identify template type (ChatML, Llama, Alpaca, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return standardized template identifier</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Fall back to generic template if detection fails</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenizerPipeline</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Handles tokenization with proper length management and label masking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, tokenizer, max_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2048</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokenizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokenizer</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_length </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_length</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.template_applicator </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ChatTemplateApplicator(tokenizer)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> tokenize_sample</span><span style=\"color:#E1E4E8\">(self, sample: InstructionSample) -> TokenizedSample:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Convert InstructionSample to TokenizedSample with proper masking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Apply chat template to format the conversation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Tokenize instruction and response portions separately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Create attention mask (1 for content, 0 for padding)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Create labels array (-100 for instruction, token_ids for response)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Handle length truncation if sequence exceeds max_length</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Add special tokens (BOS, EOS) as needed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return TokenizedSample with all required fields</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Track the boundary between instruction and response tokens</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use tokenizer.pad() method for consistent padding</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _create_labels_array</span><span style=\"color:#E1E4E8\">(self, input_ids: List[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">], prompt_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> List[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create labels array with proper masking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Set instruction tokens (0 to prompt_length) to -100</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set response tokens (prompt_length onwards) to their token IDs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Set padding tokens to -100</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return complete labels array</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DatasetSplitter</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Splits dataset into training and validation sets with stratification.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, val_ratio: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#E1E4E8\">, random_seed: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 42</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.val_ratio </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> val_ratio</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.random_seed </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> random_seed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> split_dataset</span><span style=\"color:#E1E4E8\">(self, samples: List[InstructionSample], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                     stratify_by: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> Tuple[List[InstructionSample], List[InstructionSample]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Split samples into train and validation sets.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Set random seed for reproducible splits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If stratify_by is specified, group samples by that attribute</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Within each group, randomly assign samples to train/val</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Ensure validation ratio is approximately maintained</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return tuple of (train_samples, val_samples)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use sklearn.model_selection.train_test_split if available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: For custom stratification, implement proportional sampling</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint</strong></p>\n<p>After implementing the dataset preparation component, verify the following behavior:</p>\n<ol>\n<li><strong>Data Loading Test</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">   python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_data_preparation/test_data_loader.py</span></span></code></pre></div>\n<p>   Expected: All format types (JSON, JSONL, CSV, Parquet) load correctly and produce InstructionSample objects.</p>\n<ol start=\"2\">\n<li><p><strong>Manual Validation</strong>:</p>\n<ul>\n<li>Load a sample dataset and verify field extraction works correctly</li>\n<li>Apply chat templates and confirm special tokens are properly positioned  </li>\n<li>Check that tokenization produces correct label masking (-100 for instructions, token IDs for responses)</li>\n<li>Verify train-validation splits maintain approximately the specified ratio</li>\n</ul>\n</li>\n<li><p><strong>Quality Check</strong>:</p>\n<ul>\n<li>Process a dataset with known duplicates and confirm they&#39;re removed</li>\n<li>Test length filtering with samples that exceed context limits</li>\n<li>Verify that the pipeline handles malformed data gracefully without crashing</li>\n</ul>\n</li>\n</ol>\n<p><strong>Signs of Problems</strong>:</p>\n<ul>\n<li>Memory usage growing unboundedly (indicates streaming isn&#39;t working)</li>\n<li>Chat templates producing malformed output (check special token placement)</li>\n<li>Label arrays containing unexpected values (verify masking logic)</li>\n<li>Validation sets being too large or too small (check splitting math)</li>\n</ul>\n<h2 id=\"lora-configuration-component\">LoRA Configuration Component</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 2 - this section implements the LoRA adapter setup, target module identification, rank configuration, and parameter efficiency verification that enables memory-efficient fine-tuning</p>\n</blockquote>\n<h3 id=\"mental-model-the-skill-overlay\">Mental Model: The Skill Overlay</h3>\n<p>Think of LoRA as teaching a master craftsperson new specialized techniques without making them forget their core expertise. Imagine you have a master woodworker who already knows thousands of traditional techniques accumulated over years of experience. Rather than starting their education from scratch to learn modern power tool techniques, you create a <strong>skill overlay</strong> - a thin layer of new knowledge that builds on their existing foundation.</p>\n<p>The skill overlay works by identifying specific decision points in their existing workflow where new techniques can be applied. For example, when the craftsperson reaches for a traditional hand saw (a &quot;target module&quot;), the overlay whispers: &quot;consider using the circular saw instead, but apply this specific adjustment to your grip and this modification to your cutting angle.&quot; The craftsperson&#39;s fundamental understanding of wood, joints, and structural principles remains untouched - only specific techniques get enhanced.</p>\n<p>LoRA operates on this same principle with neural networks. The base model retains all its learned language understanding, world knowledge, and reasoning patterns accumulated during pre-training on massive datasets. Instead of modifying these core capabilities (which would require retraining billions of parameters), LoRA creates small &quot;skill overlays&quot; - <strong>low-rank adapter matrices</strong> that learn task-specific modifications to apply at key decision points in the model&#39;s processing.</p>\n<p>When the model encounters a new instruction-following task, these adapters provide learned adjustments: &quot;when processing this type of instruction, adjust your attention patterns this way&quot; or &quot;for this response format, modify your output generation with these learned offsets.&quot; The fundamental language model remains frozen and intact, while the adapters provide targeted improvements for the specific fine-tuning objective.</p>\n<p>This approach preserves the stability and generality of the pre-trained model while enabling efficient specialization. Just as the craftsperson can remove the skill overlay and return to traditional techniques when needed, LoRA adapters can be easily swapped, merged, or removed without affecting the underlying model capabilities.</p>\n<h3 id=\"target-module-identification\">Target Module Identification</h3>\n<p>The <strong>target module identification</strong> subsystem automatically discovers which layers in a transformer model are suitable for LoRA adaptation. This process involves analyzing the model&#39;s architecture to identify linear transformation layers where low-rank adapters can be effectively applied.</p>\n<p>Modern transformer architectures contain numerous linear layers that serve different purposes in the information processing pipeline. The most effective targets for LoRA adaptation are typically the <strong>attention projection matrices</strong> (query, key, value, and output projections) and the <strong>feed-forward network layers</strong> within each transformer block. These layers perform high-dimensional linear transformations that benefit from the low-rank decomposition approach that LoRA provides.</p>\n<p>The identification process begins by traversing the model&#39;s module hierarchy to catalog all linear layers and their roles. For attention mechanisms, this includes the <code>q_proj</code>, <code>k_proj</code>, <code>v_proj</code>, and <code>o_proj</code> layers that transform input embeddings into query, key, value, and output representations. For feed-forward networks, this encompasses the <code>up_proj</code>, <code>down_proj</code>, and <code>gate_proj</code> layers (in models with gated activations) that process information through the MLP blocks.</p>\n<p>Different model architectures use varying naming conventions for these components. Llama-style models typically use names like <code>self_attn.q_proj</code> and <code>mlp.up_proj</code>, while other architectures may use patterns like <code>attention.query</code> or <code>feed_forward.dense_h_to_4h</code>. The target module identifier maintains architecture-specific mapping tables that translate common layer patterns into standardized target specifications.</p>\n<p>The selection strategy also considers the <strong>computational and memory trade-offs</strong> of adapting different layer types. Attention layers often provide the highest impact per parameter when adapted, as they control how the model focuses on different parts of the input sequence. Feed-forward layers offer additional capacity for learning task-specific transformations but consume more adapter parameters due to their larger dimensions.</p>\n<table>\n<thead>\n<tr>\n<th>Target Module Type</th>\n<th>Typical Names</th>\n<th>Impact on Task Performance</th>\n<th>Memory Cost</th>\n<th>Recommended Priority</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Query Projection</td>\n<td><code>q_proj</code>, <code>query</code></td>\n<td>High - controls attention focus</td>\n<td>Medium</td>\n<td>High</td>\n</tr>\n<tr>\n<td>Key Projection</td>\n<td><code>k_proj</code>, <code>key</code></td>\n<td>High - affects attention patterns</td>\n<td>Medium</td>\n<td>High</td>\n</tr>\n<tr>\n<td>Value Projection</td>\n<td><code>v_proj</code>, <code>value</code></td>\n<td>High - determines information flow</td>\n<td>Medium</td>\n<td>High</td>\n</tr>\n<tr>\n<td>Output Projection</td>\n<td><code>o_proj</code>, <code>dense</code></td>\n<td>High - final attention transformation</td>\n<td>Medium</td>\n<td>High</td>\n</tr>\n<tr>\n<td>Feed-Forward Up</td>\n<td><code>up_proj</code>, <code>dense_h_to_4h</code></td>\n<td>Medium - expands representations</td>\n<td>High</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Feed-Forward Down</td>\n<td><code>down_proj</code>, <code>dense_4h_to_h</code></td>\n<td>Medium - compresses representations</td>\n<td>High</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Gate Projection</td>\n<td><code>gate_proj</code></td>\n<td>Medium - controls activation flow</td>\n<td>High</td>\n<td>Medium</td>\n</tr>\n</tbody></table>\n<p>The automatic detection algorithm employs several heuristics to identify target modules reliably across different model architectures. It searches for linear layers with specific dimension patterns, analyzes the module naming hierarchy to identify attention and MLP components, and validates that identified targets have appropriate weight tensor shapes for LoRA decomposition.</p>\n<p><strong>Architecture Decision Record: Target Module Selection Strategy</strong></p>\n<blockquote>\n<p><strong>Decision: Automatic Detection with Architecture-Specific Overrides</strong></p>\n<ul>\n<li><p><strong>Context</strong>: Different model families use varying naming conventions and architectural patterns for their linear layers. A hardcoded target list would break when applied to new architectures, but fully automatic detection might miss optimal adaptation points or include inappropriate layers.</p>\n</li>\n<li><p><strong>Options Considered</strong>:</p>\n<ol>\n<li><strong>Hardcoded Target Lists</strong>: Maintain fixed lists of target module names for each supported architecture</li>\n<li><strong>Fully Automatic Detection</strong>: Use pattern matching and dimension analysis to detect all suitable linear layers</li>\n<li><strong>Hybrid Approach</strong>: Automatic detection with architecture-specific override capabilities</li>\n</ol>\n</li>\n<li><p><strong>Decision</strong>: Implement the hybrid approach with automatic detection as the default and architecture-specific configuration overrides</p>\n</li>\n<li><p><strong>Rationale</strong>: This provides the best balance of flexibility and reliability. Automatic detection handles most cases correctly and adapts to new architectures, while overrides allow fine-tuning for specific model families where manual optimization improves results. The system remains maintainable as new architectures emerge.</p>\n</li>\n<li><p><strong>Consequences</strong>: Requires implementing both the pattern-matching detection logic and the override configuration system. Increases initial complexity but dramatically improves long-term maintainability and compatibility across model families.</p>\n</li>\n</ul>\n</blockquote>\n<p>The detection algorithm also validates that identified target modules meet the requirements for LoRA adaptation. This includes verifying that modules are indeed linear transformations (not embeddings or normalization layers), confirming that they have trainable parameters that can be frozen, and ensuring that their dimensions are suitable for low-rank decomposition.</p>\n<h3 id=\"rank-and-alpha-parameter-selection\">Rank and Alpha Parameter Selection</h3>\n<p>The <strong>rank and alpha parameter selection</strong> process determines the capacity and scaling behavior of LoRA adapters, directly impacting both the quality of fine-tuning and the efficiency gains achieved. These hyperparameters control the fundamental trade-off between adaptation capability and parameter efficiency that makes LoRA practical for large model fine-tuning.</p>\n<p>The <strong>rank parameter</strong> (r) determines the dimensionality of the low-rank decomposition used in each adapter. LoRA replaces direct weight updates ΔW with the product of two smaller matrices: ΔW = BA, where B has dimensions (output_dim, r) and A has dimensions (r, input_dim). The rank value controls how much information the adapter can capture about the task-specific weight modifications needed.</p>\n<p>Lower rank values create more constrained adapters that can only learn simple, low-dimensional modifications to the original weights. This provides strong regularization and memory efficiency but may limit the adapter&#39;s ability to capture complex task-specific patterns. Higher rank values allow adapters to learn more sophisticated transformations but consume more memory and may be prone to overfitting, especially with limited training data.</p>\n<p>The <strong>alpha parameter</strong> (α) controls the scaling applied to the adapter outputs before they&#39;re added to the frozen base model weights. This scaling factor affects the relative influence of the adapted versus original model behavior. The effective learning rate for adapter parameters becomes proportional to α/r, meaning alpha acts as a learning rate multiplier specifically for the adaptation process.</p>\n<table>\n<thead>\n<tr>\n<th>Rank (r)</th>\n<th>Memory per Adapter</th>\n<th>Expressiveness</th>\n<th>Overfitting Risk</th>\n<th>Best Use Cases</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>4-8</td>\n<td>Very Low</td>\n<td>Limited</td>\n<td>Very Low</td>\n<td>Simple task adaptation, limited data</td>\n</tr>\n<tr>\n<td>16-32</td>\n<td>Low</td>\n<td>Moderate</td>\n<td>Low</td>\n<td>General instruction tuning, most tasks</td>\n</tr>\n<tr>\n<td>64-128</td>\n<td>Moderate</td>\n<td>High</td>\n<td>Moderate</td>\n<td>Complex reasoning, domain specialization</td>\n</tr>\n<tr>\n<td>256+</td>\n<td>High</td>\n<td>Very High</td>\n<td>High</td>\n<td>Research experimentation, abundant data</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th>Alpha (α)</th>\n<th>Effective LR Scale</th>\n<th align=\"right\">Adaptation Strength</th>\n<th>Stability</th>\n<th>Recommended Rank Range</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>8-16</td>\n<td>Low</td>\n<td align=\"right\">Subtle</td>\n<td>High</td>\n<td>4-16</td>\n</tr>\n<tr>\n<td>32-64</td>\n<td>Medium</td>\n<td align=\"right\">Balanced</td>\n<td>Medium</td>\n<td>16-64</td>\n</tr>\n<tr>\n<td>128-256</td>\n<td>High</td>\n<td align=\"right\">Strong</td>\n<td>Lower</td>\n<td>64-128</td>\n</tr>\n<tr>\n<td>512+</td>\n<td>Very High</td>\n<td align=\"right\">Aggressive</td>\n<td>Low</td>\n<td>Research only</td>\n</tr>\n</tbody></table>\n<p>The selection process begins by analyzing the <strong>task complexity and data characteristics</strong>. Tasks requiring significant behavioral changes (like learning new response formats or domain-specific knowledge) typically benefit from higher rank values, while tasks focused on style adaptation or minor behavioral adjustments work well with lower ranks. The amount of training data also influences optimal rank selection - more data can support higher ranks without overfitting.</p>\n<p><strong>Model architecture characteristics</strong> also inform rank selection. Larger models with more parameters per layer can typically benefit from higher-rank adapters, as they have more representational capacity to utilize. The original weight matrix dimensions provide upper bounds for useful rank values - using ranks approaching the smaller dimension of the weight matrix provides diminishing returns while consuming excessive memory.</p>\n<p>The alpha parameter selection considers the desired <strong>adaptation aggressiveness</strong> and training stability requirements. Conservative alpha values (8-32) provide stable training with gradual adaptation, suitable for tasks where preserving most of the original model behavior is important. Higher alpha values (64-128) enable more aggressive adaptation for tasks requiring significant behavioral changes.</p>\n<p><strong>Architecture Decision Record: Rank-Alpha Scaling Relationship</strong></p>\n<blockquote>\n<p><strong>Decision: Proportional Scaling with Task-Adaptive Defaults</strong></p>\n<ul>\n<li><p><strong>Context</strong>: The relationship between rank and alpha significantly affects training dynamics and final performance. Fixed alpha values don&#39;t account for different rank choices, while fixed ratios may not suit all task types.</p>\n</li>\n<li><p><strong>Options Considered</strong>:</p>\n<ol>\n<li><strong>Fixed Alpha</strong>: Use α=32 regardless of rank choice</li>\n<li><strong>Fixed Ratio</strong>: Maintain constant α/r ratio (e.g., α=2r)</li>\n<li><strong>Task-Adaptive Scaling</strong>: Adjust α/r ratio based on task characteristics and rank selection</li>\n</ol>\n</li>\n<li><p><strong>Decision</strong>: Implement task-adaptive scaling with configurable α/r ratios and intelligent defaults</p>\n</li>\n<li><p><strong>Rationale</strong>: Task-adaptive scaling provides the best results across different fine-tuning scenarios. Low-rank adapters benefit from higher α/r ratios to compensate for reduced capacity, while high-rank adapters work better with lower ratios to prevent instability.</p>\n</li>\n<li><p><strong>Consequences</strong>: Requires implementing task analysis logic to recommend appropriate scaling factors. Adds complexity but significantly improves out-of-the-box performance across diverse fine-tuning tasks.</p>\n</li>\n</ul>\n</blockquote>\n<p>The system provides <strong>automated rank and alpha recommendations</strong> based on task analysis and resource constraints. This includes analyzing the training data to estimate task complexity, considering available GPU memory to determine maximum feasible rank, evaluating model architecture to suggest optimal target modules, and recommending alpha values that balance adaptation strength with training stability.</p>\n<p>For users who prefer manual control, the configuration system supports explicit rank and alpha specification with validation to ensure parameters are within reasonable ranges. The system also provides guidance on the expected memory usage and training behavior for different parameter combinations.</p>\n<h3 id=\"adapter-initialization-and-injection\">Adapter Initialization and Injection</h3>\n<p>The <strong>adapter initialization and injection</strong> process creates the low-rank matrices and integrates them into the frozen base model to enable parameter-efficient fine-tuning. This process must carefully manage memory allocation, preserve the original model behavior during initialization, and ensure that gradients flow correctly through the adapter pathways during training.</p>\n<p>LoRA adapters consist of two matrices per target module: matrix <strong>A</strong> with dimensions (rank, input_features) and matrix <strong>B</strong> with dimensions (output_features, rank). During forward passes, the adapter computes the low-rank update as: <code>adapter_output = input @ A.T @ B.T</code>, which is then scaled by the alpha parameter and added to the frozen base layer output.</p>\n<p>The <strong>initialization strategy</strong> for these matrices is crucial for training stability and convergence. Matrix A is typically initialized using small random values drawn from a normal or uniform distribution, providing the initial variability needed for gradient-based learning. Matrix B is initialized to zeros, ensuring that the adapter produces zero output initially and preserves the original model behavior at the start of training.</p>\n<p>This zero-initialization approach means that the model begins fine-tuning with exactly the same behavior as the frozen base model. As training progresses, the adapter matrices learn non-zero values that gradually modify the model&#39;s responses toward the target task behavior. This provides a smooth transition from the pre-trained model capabilities to the fine-tuned specialization.</p>\n<table>\n<thead>\n<tr>\n<th>Matrix</th>\n<th>Initialization</th>\n<th>Dimensions</th>\n<th>Purpose</th>\n<th>Gradient Flow</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>A</td>\n<td>Random (std=0.01)</td>\n<td>(rank, input_features)</td>\n<td>Input projection</td>\n<td>Receives gradients from B</td>\n</tr>\n<tr>\n<td>B</td>\n<td>Zeros</td>\n<td>(output_features, rank)</td>\n<td>Output projection</td>\n<td>Receives gradients from loss</td>\n</tr>\n<tr>\n<td>Scaling</td>\n<td>Fixed (α/r)</td>\n<td>Scalar</td>\n<td>Controls adaptation strength</td>\n<td>No gradients</td>\n</tr>\n</tbody></table>\n<p>The <strong>injection process</strong> modifies the target modules to incorporate the adapter computation without affecting the original weight tensors. This typically involves wrapping the original linear layers with adapter-aware implementations that compute both the frozen base transformation and the trainable adapter transformation in parallel.</p>\n<p>The adapter injection maintains careful separation between frozen and trainable parameters. The original model weights are marked as non-trainable and remain unchanged throughout the fine-tuning process. Only the adapter matrices A and B receive gradients and undergo optimization updates. This separation is essential for parameter efficiency and enables easy adapter removal or swapping after training.</p>\n<p><strong>Memory management</strong> during injection requires attention to both GPU memory allocation and memory fragmentation. The system allocates adapter matrices on the same device as the base model weights to avoid expensive CPU-GPU transfers during forward and backward passes. Memory allocation is performed incrementally as each target module is processed, allowing for better memory utilization patterns.</p>\n<p>The injection process also configures the <strong>computation graph</strong> to ensure proper gradient flow through the adapter pathways. This includes registering the adapter parameters with the optimizer, setting up the forward pass computation to combine base and adapter outputs, and ensuring that backward passes correctly accumulate gradients in the adapter matrices.</p>\n<blockquote>\n<p>The key insight for adapter injection is maintaining <strong>computational isolation</strong> between the frozen base model and the trainable adapters while ensuring they compose correctly during forward passes. This isolation enables independent updates to adapter parameters without affecting base model weights and allows for easy adapter composition and removal.</p>\n</blockquote>\n<p><strong>Architecture Decision Record: Adapter Storage and Composition Strategy</strong></p>\n<blockquote>\n<p><strong>Decision: Separate Adapter Modules with Runtime Composition</strong></p>\n<ul>\n<li><p><strong>Context</strong>: Adapters can be implemented as modifications to existing model layers or as separate modules that compose with frozen layers. The choice affects memory usage, computational efficiency, and adapter portability.</p>\n</li>\n<li><p><strong>Options Considered</strong>:</p>\n<ol>\n<li><strong>In-Place Injection</strong>: Modify existing linear layer implementations to include adapter computations directly</li>\n<li><strong>Wrapper Layers</strong>: Create wrapper modules that contain both the frozen layer and its adapters</li>\n<li><strong>Separate Modules</strong>: Store adapters as independent modules that compose with frozen layers at runtime</li>\n</ol>\n</li>\n<li><p><strong>Decision</strong>: Implement separate adapter modules with runtime composition through the PEFT library integration</p>\n</li>\n<li><p><strong>Rationale</strong>: Separate modules provide the cleanest separation of concerns, enable easy adapter swapping and merging, maintain compatibility with existing model architectures, and leverage well-tested PEFT implementations.</p>\n</li>\n<li><p><strong>Consequences</strong>: Requires implementing adapter composition logic but provides maximum flexibility for adapter management and portability across different base models.</p>\n</li>\n</ul>\n</blockquote>\n<p>The adapter injection process includes <strong>validation steps</strong> to ensure correct integration with the target model. This involves verifying that adapter dimensions match the target module dimensions, confirming that only adapter parameters are marked as trainable, testing that forward passes produce expected output shapes, and validating that gradients flow correctly to adapter parameters during backward passes.</p>\n<p>For models that will undergo quantization, the injection process coordinates with the quantization system to ensure adapters remain in full precision while base model weights are quantized. This requires careful device placement and dtype management to maintain training compatibility.</p>\n<h3 id=\"trainable-parameter-analysis\">Trainable Parameter Analysis</h3>\n<p>The <strong>trainable parameter analysis</strong> subsystem provides comprehensive monitoring and validation of the parameter efficiency gains achieved through LoRA adaptation. This analysis is crucial for verifying that the fine-tuning process achieves its memory and computational efficiency goals while maintaining sufficient model capacity for effective learning.</p>\n<p>The analysis begins by cataloging the <strong>complete parameter inventory</strong> of both the base model and the injected adapters. For the base model, this includes counting all weight tensors across transformer layers, embedding layers, and output projection layers. These parameters are marked as frozen and excluded from gradient computation and optimizer updates. For the adapter system, the analysis counts all A and B matrices across all target modules and calculates the total trainable parameters introduced by the LoRA configuration.</p>\n<p>The <strong>parameter efficiency metrics</strong> provide quantitative measures of the memory and computational savings achieved. The primary metric is the trainable parameter ratio: the percentage of total model parameters that require gradient computation during training. Effective LoRA configurations typically achieve ratios below 1%, with many successful fine-tuning runs using only 0.1-0.5% of the original parameter count.</p>\n<table>\n<thead>\n<tr>\n<th>Parameter Category</th>\n<th>Count</th>\n<th>Percentage</th>\n<th>Memory Usage</th>\n<th>Gradient Computation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Base Model Weights</td>\n<td>7.2B</td>\n<td>99.2%</td>\n<td>14.4 GB (fp16)</td>\n<td>Disabled</td>\n</tr>\n<tr>\n<td>LoRA Adapters</td>\n<td>58M</td>\n<td>0.8%</td>\n<td>116 MB (fp16)</td>\n<td>Enabled</td>\n</tr>\n<tr>\n<td>Optimizer States</td>\n<td>58M</td>\n<td>0.8%</td>\n<td>232 MB (Adam)</td>\n<td>Required</td>\n</tr>\n<tr>\n<td>Total Trainable</td>\n<td>58M</td>\n<td>0.8%</td>\n<td>348 MB</td>\n<td>Active</td>\n</tr>\n</tbody></table>\n<p>The analysis also examines the <strong>rank utilization efficiency</strong> across different target modules to identify potential optimization opportunities. Some target modules may be more critical for task adaptation than others, and the analysis can reveal whether rank allocation is appropriately balanced. Modules that consistently show low gradient magnitudes or minimal weight updates may be candidates for rank reduction or removal from the target list.</p>\n<p><strong>Memory footprint analysis</strong> provides detailed breakdowns of GPU memory usage across different components of the fine-tuning system. This includes the quantized base model weights, the full-precision adapter parameters, the optimizer states for trainable parameters, and the activation memory required during forward and backward passes. Understanding these memory components helps optimize batch sizes and identify bottlenecks in the training configuration.</p>\n<p>The analysis tracks <strong>gradient statistics</strong> for adapter parameters throughout training to monitor learning dynamics and identify potential issues. This includes measuring gradient magnitudes, monitoring gradient-to-parameter ratios, detecting gradient explosion or vanishing problems, and analyzing the distribution of updates across different adapter modules.</p>\n<p><strong>Effective rank analysis</strong> examines whether the chosen rank values are being fully utilized by the learned adapters. Low effective rank (where the learned matrices have rank significantly below the configured rank) may indicate that smaller rank values could achieve similar performance with better efficiency. High effective rank utilization suggests that the adapters are making full use of their representational capacity.</p>\n<table>\n<thead>\n<tr>\n<th>Analysis Metric</th>\n<th>Purpose</th>\n<th>Target Range</th>\n<th>Warning Indicators</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Trainable Ratio</td>\n<td>Parameter efficiency</td>\n<td>0.1% - 2.0%</td>\n<td>&gt;5% suggests inefficiency</td>\n</tr>\n<tr>\n<td>Gradient Magnitude</td>\n<td>Learning activity</td>\n<td>1e-5 - 1e-3</td>\n<td>&lt;1e-6 suggests vanishing gradients</td>\n</tr>\n<tr>\n<td>Effective Rank</td>\n<td>Capacity utilization</td>\n<td>70% - 95% of config rank</td>\n<td>&lt;50% suggests over-parameterization</td>\n</tr>\n<tr>\n<td>Memory Overhead</td>\n<td>Resource usage</td>\n<td>&lt;20% of base model</td>\n<td>&gt;50% defeats efficiency purpose</td>\n</tr>\n</tbody></table>\n<p>The analysis system provides <strong>automated recommendations</strong> for parameter optimization based on the observed training dynamics. This includes suggesting rank adjustments based on effective rank utilization, recommending alpha parameter tuning based on gradient statistics, identifying underutilized target modules that could be removed, and proposing memory optimizations based on usage patterns.</p>\n<p><strong>Convergence analysis</strong> examines the relationship between adapter parameters and training loss to ensure that the parameter-efficient approach is not significantly compromising learning effectiveness. This includes comparing convergence rates with full fine-tuning baselines where available, analyzing the relationship between parameter count and final performance, and identifying the minimum parameter configuration that achieves acceptable task performance.</p>\n<blockquote>\n<p>A critical insight from trainable parameter analysis is that <strong>effective parameter efficiency depends on task-adapter alignment</strong> - the degree to which the chosen target modules and rank configurations match the actual learning requirements of the fine-tuning task. Misaligned configurations may require more parameters to achieve the same performance, reducing the efficiency benefits.</p>\n</blockquote>\n<h3 id=\"architecture-decision-records\">Architecture Decision Records</h3>\n<p>This subsection documents the key architectural decisions made in designing the LoRA configuration component, providing context for the design choices and their implications for system behavior and performance.</p>\n<p><strong>Architecture Decision Record: Target Module Selection Granularity</strong></p>\n<blockquote>\n<p><strong>Decision: Layer-Type Based Selection with Individual Override Capability</strong></p>\n<ul>\n<li><p><strong>Context</strong>: LoRA adapters can be applied at different granularities - all linear layers, specific layer types (attention vs MLP), or individual modules. The granularity affects both performance and memory usage, and different tasks may benefit from different targeting strategies.</p>\n</li>\n<li><p><strong>Options Considered</strong>:</p>\n<ol>\n<li><strong>All Linear Layers</strong>: Apply adapters to every linear transformation in the model</li>\n<li><strong>Layer-Type Grouping</strong>: Apply to all attention layers, all MLP layers, or both</li>\n<li><strong>Individual Selection</strong>: Allow specification of exact module names for adapter application</li>\n<li><strong>Hybrid Approach</strong>: Layer-type defaults with individual override capability</li>\n</ol>\n</li>\n<li><p><strong>Decision</strong>: Implement the hybrid approach with intelligent layer-type defaults and per-module override options</p>\n</li>\n<li><p><strong>Rationale</strong>: Layer-type grouping provides good defaults for most use cases while individual selection enables fine-tuned optimization for specific tasks. The hybrid approach accommodates both novice users who want reasonable defaults and expert users who need precise control.</p>\n</li>\n<li><p><strong>Consequences</strong>: Requires implementing both the automatic layer detection and the individual module specification systems. Increases configuration complexity but provides optimal flexibility for different use cases and expertise levels.</p>\n</li>\n</ul>\n</blockquote>\n<p><strong>Architecture Decision Record: Adapter Parameter Initialization Strategy</strong></p>\n<blockquote>\n<p><strong>Decision: Asymmetric Initialization with Configurable Distributions</strong></p>\n<ul>\n<li><p><strong>Context</strong>: LoRA adapter matrices need initialization that balances training stability with learning capability. Different initialization strategies affect convergence speed, final performance, and training stability.</p>\n</li>\n<li><p><strong>Options Considered</strong>:</p>\n<ol>\n<li><strong>Symmetric Random</strong>: Initialize both A and B matrices with small random values</li>\n<li><strong>Zero Initialization</strong>: Initialize both matrices to zero (preserves base model exactly)</li>\n<li><strong>Asymmetric (A random, B zero)</strong>: Initialize A randomly and B to zero</li>\n<li><strong>Orthogonal Initialization</strong>: Use orthogonal matrices for improved gradient flow</li>\n</ol>\n</li>\n<li><p><strong>Decision</strong>: Use asymmetric initialization (A random, B zero) as default with configurable alternatives</p>\n</li>\n<li><p><strong>Rationale</strong>: Asymmetric initialization provides the best balance of training stability and learning capability. Starting with zero adapter output preserves base model behavior initially while random A matrix provides gradient diversity. This approach has proven most reliable across diverse fine-tuning tasks.</p>\n</li>\n<li><p><strong>Consequences</strong>: Requires implementing multiple initialization strategies but provides optimal training dynamics for the majority of use cases while allowing experimentation with alternatives.</p>\n</li>\n</ul>\n</blockquote>\n<p><strong>Architecture Decision Record: Rank Configuration Strategy</strong></p>\n<blockquote>\n<p><strong>Decision: Task-Informed Automatic Defaults with Manual Override</strong></p>\n<ul>\n<li><p><strong>Context</strong>: Rank selection significantly impacts both performance and efficiency, but optimal ranks vary by task complexity, model size, and available training data. Users need guidance for rank selection while retaining control for specific requirements.</p>\n</li>\n<li><p><strong>Options Considered</strong>:</p>\n<ol>\n<li><strong>Fixed Default Ranks</strong>: Use the same rank (e.g., 16) for all configurations</li>\n<li><strong>Model-Size Scaling</strong>: Scale rank proportionally to model parameter count</li>\n<li><strong>Task-Informed Defaults</strong>: Recommend ranks based on task analysis with manual override</li>\n<li><strong>Adaptive Ranking</strong>: Automatically adjust ranks during training based on performance</li>\n</ol>\n</li>\n<li><p><strong>Decision</strong>: Implement task-informed defaults with comprehensive manual override capabilities</p>\n</li>\n<li><p><strong>Rationale</strong>: Task-informed defaults provide good starting points for users while manual overrides enable optimization for specific use cases. Adaptive ranking adds complexity without clear benefits, while fixed defaults ignore important task variations.</p>\n</li>\n<li><p><strong>Consequences</strong>: Requires implementing task analysis logic and rank recommendation algorithms. Increases system complexity but dramatically improves user experience and out-of-the-box performance.</p>\n</li>\n</ul>\n</blockquote>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>This subsection identifies frequent mistakes that developers encounter when implementing or configuring LoRA adapters, providing concrete guidance on recognition, prevention, and correction of these issues.</p>\n<p>⚠️ <strong>Pitfall: Rank Selection Without Task Analysis</strong></p>\n<p>Many developers choose LoRA ranks based on memory constraints alone, without considering the learning requirements of their specific fine-tuning task. This often results in using very low ranks (r=4 or r=8) for complex tasks that require substantial behavioral changes, leading to underfitting and poor performance despite successful training completion.</p>\n<p>The problem manifests as training loss that plateaus early at a suboptimal level, validation metrics that show minimal improvement over the base model, and generated outputs that fail to demonstrate the desired task-specific behaviors. The model appears to be learning (loss decreases initially) but cannot capture the full complexity of the target task due to insufficient adapter capacity.</p>\n<p>To avoid this pitfall, analyze the task complexity before selecting ranks. Tasks requiring significant behavioral changes (like learning new response formats, domain-specific knowledge, or complex reasoning patterns) typically need ranks of 32-64 or higher. Simple style adaptations or minor behavioral adjustments can work with lower ranks (8-16). Consider the amount of training data available - more data can support higher ranks without overfitting.</p>\n<p>⚠️ <strong>Pitfall: Mismatched Alpha-Rank Scaling</strong></p>\n<p>A common mistake is using default alpha values (often α=32) regardless of the chosen rank, which can lead to training instability or ineffective learning. The effective learning rate for adapters scales as α/r, so using fixed alpha with varying ranks creates inconsistent training dynamics.</p>\n<p>Low-rank configurations with high alpha values can cause training instability, gradient explosions, and catastrophic forgetting of base model capabilities. High-rank configurations with low alpha values result in extremely slow adaptation, requiring many more training steps to achieve convergence, and potentially getting stuck in poor local minima.</p>\n<p>Implement proportional alpha scaling as a starting point: use α=2r for most tasks, then adjust based on training behavior. For conservative adaptation (preserving base model behavior), use α=r. For aggressive adaptation (significant behavior changes), use α=4r. Monitor training loss curves and gradient magnitudes to adjust the scaling if needed.</p>\n<p>⚠️ <strong>Pitfall: Incorrect Target Module Identification</strong></p>\n<p>Developers sometimes manually specify target modules without understanding the model architecture, leading to adapter injection into inappropriate layers such as embedding layers, normalization layers, or output heads. This wastes parameters on layers that don&#39;t benefit from adaptation and misses critical transformation layers where adaptation would be most effective.</p>\n<p>Symptoms include unexpectedly high parameter counts for adapters, poor training efficiency (high loss relative to parameter count), and training instability due to adapting normalization or embedding layers. The system may appear to be working but achieves suboptimal results compared to properly targeted configurations.</p>\n<p>Use automatic target module detection as the default, especially when working with new model architectures. When manually specifying targets, focus on attention projection layers (<code>q_proj</code>, <code>k_proj</code>, <code>v_proj</code>, <code>o_proj</code>) and feed-forward layers (<code>up_proj</code>, <code>down_proj</code>). Avoid targeting embedding layers, normalization layers (<code>layer_norm</code>, <code>rms_norm</code>), or final output projections unless specifically needed for the task.</p>\n<p>⚠️ <strong>Pitfall: Forgetting to Freeze Base Model Parameters</strong></p>\n<p>A critical error is failing to properly freeze the base model parameters, which defeats the primary purpose of parameter-efficient fine-tuning. This can happen due to incorrect model loading, improper PEFT configuration, or accidentally enabling gradients for base model weights during training setup.</p>\n<p>The failure manifests as unexpectedly high GPU memory usage (similar to full fine-tuning), dramatically slower training due to computing gradients for billions of parameters, and potential overfitting or catastrophic forgetting as the entire model adapts to the limited fine-tuning dataset. Memory usage may exceed available GPU capacity even with quantization.</p>\n<p>Verify that base model parameters have <code>requires_grad=False</code> after adapter injection. Use the trainable parameter analysis to confirm that less than 2% of total parameters are trainable. Monitor GPU memory usage during training startup - it should be dramatically lower than full fine-tuning. If memory usage is unexpectedly high, check the parameter freezing configuration and PEFT setup.</p>\n<p>⚠️ <strong>Pitfall: Incompatible Quantization and Adapter Precision</strong></p>\n<p>When combining LoRA with quantization, developers sometimes place adapters and base model weights on different devices or use incompatible data types, leading to runtime errors, extremely slow training due to data transfers, or numerical precision issues that affect training stability.</p>\n<p>Problems include CUDA out-of-memory errors despite quantization, very slow forward passes due to CPU-GPU transfers, and training instability due to precision mismatches between 4-bit base weights and float16 adapter computations. Error messages often mention device placement or tensor type mismatches.</p>\n<p>Ensure that adapters are placed on the same device as the quantized base model (typically GPU). Configure adapters to use the same compute precision as the quantization setup (usually float16 or bfloat16). Use the memory monitoring tools to verify that both base model and adapters are allocated on the GPU. Test a small forward pass before starting full training to catch device placement issues early.</p>\n<p>⚠️ <strong>Pitfall: Overestimating Adapter Memory Efficiency</strong></p>\n<p>Users sometimes expect that LoRA adapters eliminate memory constraints entirely, leading to configurations with excessive ranks, too many target modules, or batch sizes that exceed available memory when combined with optimizer states and activation memory during training.</p>\n<p>The issue appears as out-of-memory errors during training (not model loading), unexpectedly slow training due to memory pressure, and batch size requirements that are much smaller than expected. The problem is often not apparent until training begins and optimizer states are allocated.</p>\n<p>Remember that memory efficiency comes from frozen base model weights, but adapters, optimizer states, and activations still consume significant memory. Use the memory analysis tools to estimate total memory requirements including optimizer states (typically 2x adapter parameters for Adam). Start with conservative batch sizes and increase gradually. Monitor memory usage throughout training, not just during model loading.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This subsection provides concrete implementation patterns, starter code, and practical guidance for building the LoRA configuration component using Python and the HuggingFace ecosystem.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>LoRA Implementation</td>\n<td>HuggingFace PEFT library with default configs</td>\n<td>Custom PEFT integration with fine-tuned hyperparameters</td>\n</tr>\n<tr>\n<td>Target Detection</td>\n<td>Manual module lists by model family</td>\n<td>Automatic detection with architecture introspection</td>\n</tr>\n<tr>\n<td>Parameter Analysis</td>\n<td>Basic parameter counting with PyTorch</td>\n<td>Comprehensive analysis with memory profiling</td>\n</tr>\n<tr>\n<td>Configuration Management</td>\n<td>YAML configs with validation</td>\n<td>Dynamic configs with task-informed defaults</td>\n</tr>\n<tr>\n<td>Memory Monitoring</td>\n<td>PyTorch CUDA memory stats</td>\n<td>Detailed profiling with nvtx and custom trackers</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<p>The LoRA configuration component fits into the overall pipeline structure as follows:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>llm-finetuning-pipeline/\n├── src/\n│   ├── components/\n│   │   ├── lora_config/                    ← This component\n│   │   │   ├── __init__.py\n│   │   │   ├── target_detector.py          ← Module identification logic\n│   │   │   ├── adapter_manager.py          ← Adapter initialization and injection\n│   │   │   ├── parameter_analyzer.py       ← Trainable parameter analysis\n│   │   │   ├── rank_selector.py            ← Rank and alpha recommendation\n│   │   │   └── config_validator.py         ← Configuration validation\n│   │   ├── data_prep/                      ← From previous milestone\n│   │   └── quantization/                   ← Next milestone\n│   ├── config/\n│   │   ├── lora_presets.yaml              ← Common LoRA configurations\n│   │   └── model_architectures.yaml       ← Architecture-specific settings\n│   └── utils/\n│       ├── memory_utils.py                ← Memory monitoring utilities\n│       └── model_utils.py                 ← Model introspection helpers\n├── tests/\n│   ├── test_lora_config/\n│   │   ├── test_target_detection.py\n│   │   ├── test_adapter_injection.py\n│   │   └── test_parameter_analysis.py\n│   └── integration/\n│       └── test_lora_pipeline.py\n└── examples/\n    ├── lora_configuration_examples.py\n    └── custom_target_selection.py</code></pre></div>\n\n<h4 id=\"core-configuration-infrastructure\">Core Configuration Infrastructure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/components/lora_config/__init__.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">LoRA Configuration Component</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Provides automatic target module detection, adapter initialization,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">and parameter efficiency analysis for memory-efficient fine-tuning.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, List, Union, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> yaml</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> LoRAConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for LoRA adapters with validation and defaults.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    r: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 16</span><span style=\"color:#6A737D\">  # Rank of the low-rank decomposition</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    alpha: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 32</span><span style=\"color:#6A737D\">  # Scaling parameter for adapter outputs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dropout: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#6A737D\">  # Dropout applied to adapter outputs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    target_modules: Optional[List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">  # Auto-detect if None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bias: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"none\"</span><span style=\"color:#6A737D\">  # Whether to adapt bias parameters</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    task_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"CAUSAL_LM\"</span><span style=\"color:#6A737D\">  # Task type for PEFT</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lora_alpha_scaling: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#6A737D\">  # Whether to apply alpha/r scaling</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    init_lora_weights: Union[</span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#6A737D\">  # Initialization strategy</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate configuration parameters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.r </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> or</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.r </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 512</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Rank must be between 1 and 512, got </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.r</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.alpha </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Alpha must be positive, got </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.alpha</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> &#x3C;=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.dropout </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Dropout must be between 0 and 1, got </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.dropout</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> effective_alpha</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate the effective alpha scaling factor.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.lora_alpha_scaling:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.alpha </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.r</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.alpha</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> from_task_complexity</span><span style=\"color:#E1E4E8\">(cls, complexity: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, available_memory_gb: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'LoRAConfig'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create LoRA config based on task complexity and memory constraints.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement task complexity analysis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate maximum feasible rank from memory constraints</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Recommend alpha based on complexity and rank</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Set appropriate dropout based on expected overfitting risk</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> to_peft_config</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Convert to PEFT library configuration format.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Transform to PEFT LoraConfig dictionary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle target_modules conversion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Apply library-specific parameter mappings</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AdapterMetrics</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Metrics for analyzing adapter parameter efficiency.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    total_base_params: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    total_adapter_params: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    trainable_ratio: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    memory_overhead_mb: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    target_modules_count: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    effective_rank_utilization: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gradient_statistics: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]]</span></span></code></pre></div>\n\n<h4 id=\"target-module-detection\">Target Module Detection</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/components/lora_config/target_detector.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Automatic target module detection for LoRA adapter injection.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch.nn </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> nn</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Set, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> re</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> transformers </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> PreTrainedModel</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TargetModuleDetector</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Detects suitable target modules for LoRA adaptation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Architecture-specific module patterns</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ATTENTION_PATTERNS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">.</span><span style=\"color:#F97583\">*</span><span style=\"color:#85E89D;font-weight:bold\">\\.</span><span style=\"color:#79B8FF\">(?:</span><span style=\"color:#DBEDFF\">q_proj</span><span style=\"color:#F97583\">|</span><span style=\"color:#DBEDFF\">query</span><span style=\"color:#79B8FF\">)$</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">.</span><span style=\"color:#F97583\">*</span><span style=\"color:#85E89D;font-weight:bold\">\\.</span><span style=\"color:#79B8FF\">(?:</span><span style=\"color:#DBEDFF\">k_proj</span><span style=\"color:#F97583\">|</span><span style=\"color:#DBEDFF\">key</span><span style=\"color:#79B8FF\">)$</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">.</span><span style=\"color:#F97583\">*</span><span style=\"color:#85E89D;font-weight:bold\">\\.</span><span style=\"color:#79B8FF\">(?:</span><span style=\"color:#DBEDFF\">v_proj</span><span style=\"color:#F97583\">|</span><span style=\"color:#DBEDFF\">value</span><span style=\"color:#79B8FF\">)$</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">.</span><span style=\"color:#F97583\">*</span><span style=\"color:#85E89D;font-weight:bold\">\\.</span><span style=\"color:#79B8FF\">(?:</span><span style=\"color:#DBEDFF\">o_proj</span><span style=\"color:#F97583\">|</span><span style=\"color:#DBEDFF\">dense</span><span style=\"color:#F97583\">|</span><span style=\"color:#DBEDFF\">out_proj</span><span style=\"color:#79B8FF\">)$</span><span style=\"color:#9ECBFF\">'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MLP_PATTERNS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">.</span><span style=\"color:#F97583\">*</span><span style=\"color:#85E89D;font-weight:bold\">\\.</span><span style=\"color:#79B8FF\">(?:</span><span style=\"color:#DBEDFF\">up_proj</span><span style=\"color:#F97583\">|</span><span style=\"color:#DBEDFF\">dense_h_to_4h</span><span style=\"color:#79B8FF\">)$</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">.</span><span style=\"color:#F97583\">*</span><span style=\"color:#85E89D;font-weight:bold\">\\.</span><span style=\"color:#79B8FF\">(?:</span><span style=\"color:#DBEDFF\">down_proj</span><span style=\"color:#F97583\">|</span><span style=\"color:#DBEDFF\">dense_4h_to_h</span><span style=\"color:#79B8FF\">)$</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">.</span><span style=\"color:#F97583\">*</span><span style=\"color:#85E89D;font-weight:bold\">\\.</span><span style=\"color:#79B8FF\">(?:</span><span style=\"color:#DBEDFF\">gate_proj</span><span style=\"color:#79B8FF\">)$</span><span style=\"color:#9ECBFF\">'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, model: PreTrainedModel):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.architecture </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._detect_architecture()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _detect_architecture</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Detect the model architecture family.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Analyze model class and config to identify architecture</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return standardized architecture name (llama, gpt, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> detect_attention_modules</span><span style=\"color:#E1E4E8\">(self) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Find all attention projection modules.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        attention_modules </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Iterate through model named modules</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Apply attention patterns to find matches</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate that matched modules are Linear layers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Filter out inappropriate modules (embeddings, norms)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return sorted list of module names</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> attention_modules</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> detect_mlp_modules</span><span style=\"color:#E1E4E8\">(self) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Find all MLP/feed-forward modules.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        mlp_modules </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Iterate through model named modules  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Apply MLP patterns to find matches</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate module types and dimensions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle architecture-specific naming variations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return sorted list of module names</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> mlp_modules</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_recommended_targets</span><span style=\"color:#E1E4E8\">(self, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                              include_attention: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                              include_mlp: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                              max_modules: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get recommended target modules based on strategy.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        targets </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> include_attention:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            targets.extend(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.detect_attention_modules())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> include_mlp:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            targets.extend(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.detect_mlp_modules())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Sort by priority (attention > output > mlp)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Limit to max_modules if specified</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate that targets are compatible with LoRA</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> targets</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> analyze_module_dimensions</span><span style=\"color:#E1E4E8\">(self, module_names: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Analyze dimensions of target modules for rank planning.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        dimensions </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> name </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> module_names:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            module </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> dict</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.model.named_modules())[name]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(module, nn.Linear):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                dimensions[name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'in_features'</span><span style=\"color:#E1E4E8\">: module.in_features,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'out_features'</span><span style=\"color:#E1E4E8\">: module.out_features,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'max_useful_rank'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">min</span><span style=\"color:#E1E4E8\">(module.in_features, module.out_features),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'parameter_count'</span><span style=\"color:#E1E4E8\">: module.in_features </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> module.out_features</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> dimensions</span></span></code></pre></div>\n\n<h4 id=\"adapter-management-and-injection\">Adapter Management and Injection</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/components/lora_config/adapter_manager.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">LoRA adapter initialization, injection, and management.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch.nn </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> nn</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> peft </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> LoraConfig, get_peft_model, TaskType</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> transformers </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> PreTrainedModel</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AdapterManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manages LoRA adapter lifecycle and integration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, model: PreTrainedModel, lora_config: LoRAConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.base_model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.lora_config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> lora_config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.peft_model </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> inject_adapters</span><span style=\"color:#E1E4E8\">(self) -> PreTrainedModel:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Inject LoRA adapters into the base model.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Convert our config to PEFT format</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        peft_config </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._create_peft_config()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate target modules exist in model</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check module dimensions for rank compatibility</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Apply PEFT model wrapping with adapters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify adapter injection success</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return PEFT-wrapped model</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _create_peft_config</span><span style=\"color:#E1E4E8\">(self) -> LoraConfig:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Convert internal config to PEFT LoraConfig.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Map LoRAConfig fields to PEFT LoraConfig</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle target_modules auto-detection if None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Set task_type appropriately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Configure initialization strategy</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> freeze_base_parameters</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Ensure base model parameters are frozen.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peft_model </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Must inject adapters before freezing parameters\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Iterate through base model parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Set requires_grad=False for non-adapter params</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify only adapter parameters remain trainable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Log parameter freeze statistics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_adapter_state_dict</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, torch.Tensor]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Extract only the adapter parameters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peft_model </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"No adapters injected\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Filter state_dict to only adapter parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Remove base model weights from dictionary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return clean adapter-only state dict</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> merge_and_unload</span><span style=\"color:#E1E4E8\">(self) -> PreTrainedModel:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Merge adapter weights into base model and return standalone model.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peft_model </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"No adapters to merge\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Use PEFT merge_and_unload functionality</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate merged model produces same outputs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return standalone model without adapter modules</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> estimate_memory_usage</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Estimate memory usage for adapters in MB.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.lora_config.target_modules </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Need to detect targets first</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            from</span><span style=\"color:#E1E4E8\"> .target_detector </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TargetModuleDetector</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            detector </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TargetModuleDetector(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.base_model)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            target_modules </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> detector.get_recommended_targets()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            target_modules </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.lora_config.target_modules</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate adapter parameter count per module</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Account for rank, input/output dimensions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Include optimizer state memory (2x for Adam)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return memory breakdown dictionary</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"parameter-analysis-and-monitoring\">Parameter Analysis and Monitoring</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/components/lora_config/parameter_analyzer.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Analysis and monitoring of parameter efficiency for LoRA adapters.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> transformers </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> PreTrainedModel</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ParameterAnalyzer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Analyzes parameter efficiency and training dynamics for LoRA.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, model: PreTrainedModel):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.base_param_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.adapter_param_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> analyze_parameter_distribution</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Analyze distribution of trainable vs frozen parameters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        analysis </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'base_model'</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#9ECBFF\">'total'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'trainable'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'frozen'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'adapters'</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#9ECBFF\">'total'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'trainable'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'frozen'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'other'</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#9ECBFF\">'total'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'trainable'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'frozen'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Iterate through all named parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Classify parameters as base, adapter, or other</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Count trainable vs frozen in each category</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate memory usage for each category</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return comprehensive analysis dictionary</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> analysis</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> calculate_efficiency_metrics</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate parameter efficiency metrics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        param_analysis </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.analyze_parameter_distribution()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate trainable parameter ratio</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate memory efficiency ratio</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate computational efficiency estimates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Compare against full fine-tuning baseline</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> analyze_gradient_statistics</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Analyze gradient statistics for adapter parameters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> any</span><span style=\"color:#E1E4E8\">(p.requires_grad </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> p </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.model.parameters()):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"No trainable parameters found\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        gradient_stats </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Collect gradients from adapter parameters only</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate magnitude statistics (mean, std, max, min)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Analyze gradient-to-parameter ratios</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Detect gradient vanishing/explosion patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Group statistics by adapter module type</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> gradient_stats</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> estimate_effective_rank</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Estimate the effective rank utilization of adapters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        effective_ranks </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Extract adapter A and B matrices</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Compute SVD to analyze rank utilization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate effective rank as ratio of significant singular values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Compare against configured rank to identify under-utilization</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> effective_ranks</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> generate_efficiency_report</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate a comprehensive efficiency analysis report.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        param_dist </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.analyze_parameter_distribution()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        efficiency </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.calculate_efficiency_metrics()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Format parameter distribution statistics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Include memory usage comparisons</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add recommendations for optimization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate human-readable report string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"Parameter efficiency report placeholder\"</span></span></code></pre></div>\n\n<h4 id=\"rank-selection-and-optimization\">Rank Selection and Optimization</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/components/lora_config/rank_selector.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Intelligent rank and alpha parameter selection for LoRA configuration.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Tuple, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> transformers </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> PreTrainedModel</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RankSelector</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Provides intelligent rank and alpha selection for LoRA configs.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TASK_COMPLEXITY_RANKS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'simple_style'</span><span style=\"color:#E1E4E8\">: (</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">),      </span><span style=\"color:#6A737D\"># Style adaptation, formatting</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'instruction_following'</span><span style=\"color:#E1E4E8\">: (</span><span style=\"color:#79B8FF\">16</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">32</span><span style=\"color:#E1E4E8\">),  </span><span style=\"color:#6A737D\"># General instruction tuning</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'domain_adaptation'</span><span style=\"color:#E1E4E8\">: (</span><span style=\"color:#79B8FF\">32</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">64</span><span style=\"color:#E1E4E8\">),      </span><span style=\"color:#6A737D\"># Domain-specific knowledge</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'complex_reasoning'</span><span style=\"color:#E1E4E8\">: (</span><span style=\"color:#79B8FF\">64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">128</span><span style=\"color:#E1E4E8\">),     </span><span style=\"color:#6A737D\"># Mathematical, logical reasoning</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'code_generation'</span><span style=\"color:#E1E4E8\">: (</span><span style=\"color:#79B8FF\">32</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">64</span><span style=\"color:#E1E4E8\">),       </span><span style=\"color:#6A737D\"># Programming tasks</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'creative_writing'</span><span style=\"color:#E1E4E8\">: (</span><span style=\"color:#79B8FF\">16</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">32</span><span style=\"color:#E1E4E8\">),      </span><span style=\"color:#6A737D\"># Creative text generation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, model: PreTrainedModel, available_memory_gb: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.available_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> available_memory_gb</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model_size </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._estimate_model_size()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _estimate_model_size</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Estimate model size in billions of parameters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        total_params </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> sum</span><span style=\"color:#E1E4E8\">(p.numel() </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> p </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.model.parameters())</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> total_params </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1e9</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> recommend_rank_for_task</span><span style=\"color:#E1E4E8\">(self, task_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, dataset_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Recommend rank and alpha for a specific task.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Look up base rank range for task type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Adjust for dataset size (more data supports higher rank)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Consider model size (larger models can utilize higher ranks)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Apply memory constraints to limit maximum rank</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return (recommended_rank, recommended_alpha) tuple</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> analyze_memory_constraints</span><span style=\"color:#E1E4E8\">(self, rank: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, target_modules: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Analyze memory requirements for given rank and targets.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate adapter parameter count for each target module</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Estimate memory for adapter weights (fp16/fp32)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Estimate optimizer state memory (Adam = 2x parameters)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Include activation memory estimates for training</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return memory breakdown and total requirements</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> find_optimal_rank</span><span style=\"color:#E1E4E8\">(self, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                         task_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                         target_modules: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                         max_memory_usage: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.8</span><span style=\"color:#E1E4E8\">) -> Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Find optimal rank within memory constraints.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Start with task-based recommendation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Binary search to find maximum feasible rank</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate memory usage under threshold</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Select appropriate alpha for chosen rank</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return (rank, alpha, memory_analysis)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_configuration</span><span style=\"color:#E1E4E8\">(self, rank: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, alpha: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, target_modules: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate that a rank/alpha configuration is feasible.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check rank against module dimensions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify memory requirements are within limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate alpha/rank ratio for training stability</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return True if configuration is valid</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing the LoRA configuration component, verify the following behavior:</p>\n<p><strong>Unit Test Validation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#79B8FF\">cd</span><span style=\"color:#9ECBFF\"> llm-finetuning-pipeline</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_lora_config/</span><span style=\"color:#79B8FF\"> -v</span></span></code></pre></div>\n\n<p><strong>Expected Output:</strong></p>\n<ul>\n<li>All target module detection tests pass</li>\n<li>Adapter injection creates correct parameter counts</li>\n<li>Memory analysis shows &lt;2% trainable parameters</li>\n<li>Rank selection respects memory constraints</li>\n</ul>\n<p><strong>Manual Verification:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test with a small model (e.g., DistilBERT or small Llama)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> src.components.lora_config </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> LoRAConfig, AdapterManager, TargetModuleDetector</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> transformers </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> AutoModel</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> AutoModel.from_pretrained(</span><span style=\"color:#9ECBFF\">\"distilbert-base-uncased\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test automatic target detection</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">detector </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TargetModuleDetector(model)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">targets </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> detector.get_recommended_targets()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Found </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(targets)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> target modules: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">targets[:</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">...\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test adapter injection</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">lora_config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> LoRAConfig(</span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">16</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">alpha</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">32</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">target_modules</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">targets)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">adapter_manager </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> AdapterManager(model, lora_config)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">peft_model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> adapter_manager.inject_adapters()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Verify parameter efficiency</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> src.components.lora_config </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ParameterAnalyzer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">analyzer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ParameterAnalyzer(peft_model)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">efficiency </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> analyzer.calculate_efficiency_metrics()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Trainable parameter ratio: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">efficiency[</span><span style=\"color:#9ECBFF\">'trainable_ratio'</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">:.2%</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should see &#x3C;2% trainable parameters and successful adapter injection</span></span></code></pre></div>\n\n<p><strong>Signs of Success:</strong></p>\n<ul>\n<li>Target detection finds appropriate attention/MLP modules</li>\n<li>Adapter injection completes without errors</li>\n<li>Parameter analysis shows dramatic efficiency gains</li>\n<li>Memory usage is much lower than full fine-tuning</li>\n<li>Model forward pass works correctly with adapters</li>\n</ul>\n<p><strong>Common Issues to Debug:</strong></p>\n<ul>\n<li>Module name mismatches between architectures</li>\n<li>Memory allocation failures during injection</li>\n<li>Incorrect parameter freezing (too many trainable params)</li>\n<li>PEFT library version compatibility issues</li>\n</ul>\n<h2 id=\"qlora-quantization-component\">QLoRA Quantization Component</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 3 - this section implements the 4-bit quantization system using NormalFloat format, double quantization strategies, and mixed-precision training coordination that enables fine-tuning of large language models within consumer hardware memory constraints.</p>\n</blockquote>\n<p>The QLoRA quantization component represents the most aggressive memory optimization strategy in our fine-tuning pipeline, enabling models that would normally require 80GB of VRAM to fit comfortably within 16GB consumer hardware. This component implements the cutting-edge quantization techniques introduced in the QLoRA paper, combining 4-bit NormalFloat quantization with double quantization and mixed-precision training to achieve unprecedented memory efficiency without catastrophic quality degradation.</p>\n<h3 id=\"mental-model-the-compression-expert\">Mental Model: The Compression Expert</h3>\n<p>Think of the quantization component as a <strong>compression expert</strong> working in a high-end photography studio. The expert receives massive RAW image files from photographers (our 32-bit floating point model weights) and must compress them to fit on limited storage devices (GPU VRAM) while preserving the essential visual information needed for professional printing (model inference quality).</p>\n<p>The compression expert uses three sophisticated techniques. First, they analyze the histogram of pixel values and discover that most photographs have a roughly normal distribution of brightness values - this insight leads them to use a <strong>specialized compression format</strong> (NormalFloat) that allocates bits more efficiently for typical photographic content rather than using a generic compression algorithm. Second, they apply <strong>nested compression</strong> (double quantization) by compressing even the compression metadata itself, squeezing out every possible byte. Finally, during editing workflows, they work with <strong>high-quality intermediate files</strong> (float16 computation) while keeping the compressed originals in storage (4-bit weights), only decompressing sections as needed for active editing.</p>\n<p>This analogy captures the core insight of QLoRA: by understanding the statistical properties of neural network weights and carefully orchestrating when to use different precisions, we can achieve 75% memory reduction while maintaining the essential information needed for effective fine-tuning.</p>\n<h3 id=\"normalfloat-4-bit-quantization\">NormalFloat 4-bit Quantization</h3>\n<p>The foundation of our memory optimization strategy rests on <strong>NormalFloat 4-bit quantization</strong>, a technique that exploits the statistical properties of neural network weight distributions to achieve superior compression ratios compared to uniform quantization schemes. Traditional 4-bit quantization divides the weight range uniformly into 16 buckets, but this approach wastes precious bits on extreme values that rarely occur in practice while providing insufficient precision for the common values near zero where most neural network weights cluster.</p>\n<p>NormalFloat quantization solves this problem by pre-computing optimal quantization levels based on the assumption that neural network weights follow approximately normal distributions. The <code>NF4_QUANTIZATION</code> format allocates more quantization levels to values near zero and fewer levels to extreme values, matching the actual distribution of weights found in transformer models. This distributional awareness allows NF4 to represent the same weight tensor with higher effective precision using only 4 bits per parameter.</p>\n<p>The quantization process begins with <strong>statistical analysis</strong> of the weight tensor to determine appropriate scaling factors and zero points. Our <code>QuantizationConfig</code> captures the essential parameters that control this process:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>load_in_4bit</code></td>\n<td><code>bool</code></td>\n<td>Primary flag enabling 4-bit quantization during model loading</td>\n</tr>\n<tr>\n<td><code>bnb_4bit_quant_type</code></td>\n<td><code>str</code></td>\n<td>Quantization format, typically &quot;nf4&quot; for NormalFloat</td>\n</tr>\n<tr>\n<td><code>bnb_4bit_compute_dtype</code></td>\n<td><code>str</code></td>\n<td>Data type for forward pass computations (fp16/bf16)</td>\n</tr>\n<tr>\n<td><code>bnb_4bit_use_double_quant</code></td>\n<td><code>bool</code></td>\n<td>Enable quantization of quantization constants</td>\n</tr>\n<tr>\n<td><code>bnb_4bit_quant_storage</code></td>\n<td><code>str</code></td>\n<td>Storage format for quantized weights on disk</td>\n</tr>\n</tbody></table>\n<p>The quantization component implements a <strong>two-phase loading strategy</strong> that first loads the full-precision model into system memory, applies the NF4 quantization transformation, and then moves the compressed weights to GPU memory. This approach ensures that quantization happens with full precision arithmetic, avoiding the quality degradation that would result from quantizing already-quantized weights.</p>\n<p>During the <strong>quantization transformation</strong>, each weight tensor undergoes the following process: the component calculates tensor-wise statistics to determine optimal scaling factors, applies the NF4 quantization mapping to convert 32-bit floats to 4-bit indices, stores the quantization metadata (scales and zero points) alongside the compressed indices, and registers dequantization callbacks that will be triggered during forward passes.</p>\n<blockquote>\n<p><strong>Critical Design Insight</strong>: The choice of NF4 over uniform quantization isn&#39;t just about better compression - it&#39;s about preserving the gradient flow characteristics that enable effective fine-tuning. Uniform quantization can create artificial &quot;quantization barriers&quot; that interfere with gradient-based optimization, while NF4&#39;s distribution-aware approach maintains smoother optimization landscapes.</p>\n</blockquote>\n<p>The quantization component provides <strong>quality monitoring</strong> capabilities that track the approximation error introduced by 4-bit compression. The <code>MemoryMonitor</code> component measures both the memory savings achieved and the statistical properties of the quantization error, enabling developers to validate that their specific model and dataset combination maintains acceptable quality under 4-bit compression.</p>\n<h3 id=\"double-quantization-strategy\">Double Quantization Strategy</h3>\n<p>Building upon the NF4 foundation, <strong>double quantization</strong> represents the next level of memory optimization by applying quantization recursively to the quantization metadata itself. While NF4 quantization compresses the weight values to 4 bits, the scaling factors and zero points required for dequantization are typically stored in full 32-bit precision. For large models, these quantization constants can consume significant memory - double quantization addresses this by quantizing the constants themselves.</p>\n<p>The double quantization process operates in <strong>hierarchical layers</strong>. The first quantization level compresses the original weight tensors using NF4, producing 4-bit indices and 32-bit quantization constants. The second quantization level then analyzes the distribution of these quantization constants across the model and applies another round of NF4 quantization to compress them further. This creates a nested structure where dequantization requires two levels of lookup and scaling operations.</p>\n<p>Our implementation enables double quantization through the <code>bnb_4bit_use_double_quant</code> flag in the <code>QuantizationConfig</code>. When enabled, the component performs additional analysis during model loading to identify patterns in the quantization constants and determine optimal secondary quantization parameters.</p>\n<p>The <strong>memory savings calculation</strong> for double quantization follows a predictable pattern. For a model with <code>N</code> parameters, standard NF4 quantization requires <code>N/2</code> bytes for compressed weights plus approximately <code>N/2048</code> bytes for quantization constants (assuming block-wise quantization with 1024-element blocks). Double quantization further compresses the constants by roughly 75%, yielding total memory usage of approximately <code>N/2 + N/8192</code> bytes - a marginal but meaningful improvement for billion-parameter models.</p>\n<p>However, double quantization introduces <strong>computational overhead</strong> during the forward pass. Each dequantization operation now requires two sequential lookup and scaling steps, increasing the latency of memory-bound operations. Our implementation provides detailed profiling capabilities to measure this overhead and determine whether the memory savings justify the computational cost for specific deployment scenarios.</p>\n<table>\n<thead>\n<tr>\n<th>Quantization Level</th>\n<th>Memory Usage</th>\n<th>Dequant Overhead</th>\n<th>Recommended For</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>None</td>\n<td>100%</td>\n<td>0%</td>\n<td>Unlimited VRAM</td>\n</tr>\n<tr>\n<td>NF4 Only</td>\n<td>~25%</td>\n<td>Low</td>\n<td>Most use cases</td>\n</tr>\n<tr>\n<td>Double Quantized</td>\n<td>~23%</td>\n<td>Medium</td>\n<td>Extreme memory constraints</td>\n</tr>\n</tbody></table>\n<p>The double quantization component implements <strong>adaptive block sizing</strong> that analyzes the distribution characteristics of quantization constants to determine optimal grouping strategies. Rather than using fixed block sizes, it can dynamically adjust the granularity of secondary quantization to minimize approximation error while maximizing memory savings.</p>\n<blockquote>\n<p><strong>Decision: Double Quantization by Default</strong></p>\n<ul>\n<li><strong>Context</strong>: Double quantization provides additional memory savings with modest computational overhead, but adds complexity to the dequantization path and debugging process.</li>\n<li><strong>Options Considered</strong>: Always enable, never enable, or make it configurable with smart defaults</li>\n<li><strong>Decision</strong>: Enable by default for 4-bit quantization but provide explicit configuration control</li>\n<li><strong>Rationale</strong>: The memory savings (2-3% additional reduction) are meaningful for edge deployment scenarios, and the computational overhead is acceptable for fine-tuning workloads where memory efficiency is prioritized over raw inference speed</li>\n<li><strong>Consequences</strong>: Slightly more complex error handling and debugging, but enables fine-tuning of larger models on the same hardware</li>\n</ul>\n</blockquote>\n<h3 id=\"mixed-precision-training-setup\">Mixed-Precision Training Setup</h3>\n<p>The quantization component&#39;s most sophisticated capability lies in orchestrating <strong>mixed-precision training</strong> that seamlessly combines 4-bit weight storage with higher-precision computation. This approach addresses the fundamental challenge of quantized fine-tuning: while 4-bit weights save enormous amounts of memory, performing gradient computations in 4-bit precision would catastrophically degrade training stability and convergence quality.</p>\n<p>Mixed-precision training operates on the principle of <strong>precision specialization</strong> - using the minimum precision necessary for each operation while maintaining overall training effectiveness. Weights remain stored in 4-bit NF4 format to minimize memory consumption, but forward pass computations dynamically dequantize weights to float16 or bfloat16 precision for stable numerical operations. Gradients are computed and accumulated in the higher precision format, then the LoRA adapter updates are applied without ever modifying the quantized base model weights.</p>\n<p>The precision orchestration happens through <strong>automatic dequantization hooks</strong> registered during model loading. When a quantized linear layer receives input during the forward pass, it triggers the following sequence: dequantize the 4-bit weights to the configured compute dtype (fp16/bf16), perform the matrix multiplication using the higher-precision weights and activations, and discard the temporary dequantized weights to free memory immediately after the operation.</p>\n<p>Our <code>QuantizationConfig</code> provides precise control over the compute precision through the <code>bnb_4bit_compute_dtype</code> field:</p>\n<table>\n<thead>\n<tr>\n<th>Compute Dtype</th>\n<th>Memory Usage</th>\n<th>Numerical Stability</th>\n<th>Hardware Support</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>float16</code></td>\n<td>Lower</td>\n<td>Good</td>\n<td>Universal</td>\n</tr>\n<tr>\n<td><code>bfloat16</code></td>\n<td>Lower</td>\n<td>Better</td>\n<td>Modern GPUs only</td>\n</tr>\n<tr>\n<td><code>float32</code></td>\n<td>Higher</td>\n<td>Best</td>\n<td>Universal</td>\n</tr>\n</tbody></table>\n<p>The <strong>gradient flow</strong> in mixed-precision quantized training follows a carefully orchestrated path that ensures training stability while maintaining memory efficiency. During backpropagation, gradients flow through the higher-precision forward pass computations, accumulate in the optimizer state (typically float32), and apply updates only to the LoRA adapter parameters. The quantized base model weights remain frozen throughout training, eliminating the need to maintain optimizer states for the massive base model parameters.</p>\n<p>This design creates a <strong>memory usage profile</strong> that scales primarily with the adapter size rather than the base model size. A 7B parameter model with 4-bit quantization and rank-16 LoRA adapters requires approximately 4.5GB for quantized weights, 0.1GB for adapter parameters and optimizer states, and 2-4GB for activation storage during training - fitting comfortably within 8GB VRAM while maintaining training effectiveness comparable to full-precision fine-tuning.</p>\n<p>The component implements <strong>automatic mixed-precision configuration</strong> that selects optimal compute dtypes based on hardware capabilities and training requirements. Modern Ampere and newer GPUs benefit from bfloat16 computation due to better numerical stability and hardware acceleration, while older hardware defaults to float16 to ensure compatibility.</p>\n<blockquote>\n<p><strong>Critical Implementation Detail</strong>: The timing of dequantization operations is crucial for memory efficiency. Our implementation uses a &quot;just-in-time&quot; dequantization strategy that decompresses weights immediately before use and discards the decompressed values immediately after, rather than maintaining persistent caches that would defeat the memory savings of quantization.</p>\n</blockquote>\n<h3 id=\"memory-usage-monitoring\">Memory Usage Monitoring</h3>\n<p>The quantization component integrates comprehensive <strong>memory usage monitoring</strong> through the <code>MemoryMonitor</code> system that tracks GPU and system memory consumption across all stages of model loading, quantization, and training. This monitoring capability serves both operational and analytical purposes - providing real-time feedback during training and generating detailed reports for optimization and debugging.</p>\n<p>Memory monitoring operates through <strong>strategic measurement points</strong> that capture memory usage at critical transitions in the quantization pipeline. The <code>MemoryMonitor</code> automatically captures baseline measurements before model loading, tracks memory allocation during quantization transformation, monitors peak usage during training, and analyzes memory efficiency metrics throughout the fine-tuning process.</p>\n<p>The monitoring system maintains detailed memory metrics through structured data collection:</p>\n<table>\n<thead>\n<tr>\n<th>Metric Category</th>\n<th>Measurements</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>GPU Allocated</td>\n<td>PyTorch tensor memory</td>\n<td>Active model and gradient storage</td>\n</tr>\n<tr>\n<td>GPU Cached</td>\n<td>CUDA memory pool</td>\n<td>Available for immediate allocation</td>\n</tr>\n<tr>\n<td>GPU Reserved</td>\n<td>Total CUDA allocation</td>\n<td>Maximum memory claimed from system</td>\n</tr>\n<tr>\n<td>System Memory</td>\n<td>Process RSS/VMS</td>\n<td>Host memory for data loading and processing</td>\n</tr>\n<tr>\n<td>Quantization Savings</td>\n<td>Before/after comparison</td>\n<td>Compression effectiveness validation</td>\n</tr>\n</tbody></table>\n<p>The <code>MemoryMonitor.measure_current_usage()</code> method provides <strong>real-time memory profiling</strong> that can be called at any point during training to capture current memory state. This enables detection of memory leaks, validation of quantization effectiveness, and identification of memory bottlenecks that could impact training stability.</p>\n<p>The monitoring component implements <strong>memory efficiency analysis</strong> that calculates and reports key optimization metrics. The quantization compression ratio compares pre and post-quantization memory usage to validate expected savings. Parameter efficiency metrics show the ratio of trainable adapter parameters to total model parameters. Peak memory tracking identifies the maximum memory usage across the entire training run for capacity planning.</p>\n<p>Memory monitoring data feeds into <strong>automated optimization recommendations</strong> that suggest configuration adjustments based on observed usage patterns. If peak memory usage approaches hardware limits, the system recommends reducing batch size or gradient accumulation steps. If quantization savings are lower than expected, it suggests investigating model architecture compatibility with 4-bit compression.</p>\n<p>The component provides <strong>memory usage visualization</strong> through integration with monitoring frameworks like Weights &amp; Biases. Memory consumption graphs show the impact of each optimization technique, enabling practitioners to understand the memory budget allocation between base model storage, adapter parameters, optimizer states, and activation caching.</p>\n<table>\n<thead>\n<tr>\n<th>Memory Category</th>\n<th>Typical 7B Model (4-bit)</th>\n<th>Percentage</th>\n<th>Optimization Opportunity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Quantized Weights</td>\n<td>4.5GB</td>\n<td>60%</td>\n<td>Already optimized</td>\n</tr>\n<tr>\n<td>Activations</td>\n<td>2.0GB</td>\n<td>27%</td>\n<td>Gradient checkpointing</td>\n</tr>\n<tr>\n<td>Adapter Params</td>\n<td>0.1GB</td>\n<td>1%</td>\n<td>Rank tuning</td>\n</tr>\n<tr>\n<td>Optimizer State</td>\n<td>0.2GB</td>\n<td>3%</td>\n<td>8-bit optimizers</td>\n</tr>\n<tr>\n<td>System Overhead</td>\n<td>0.7GB</td>\n<td>9%</td>\n<td>OS/driver optimization</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Monitoring Best Practice</strong>: Memory measurements should be taken at consistent points in the training loop (e.g., after optimizer steps) to ensure comparable readings. GPU memory allocation can be highly dynamic during forward/backward passes, making timing-sensitive measurements unreliable for optimization decisions.</p>\n</blockquote>\n<h3 id=\"architecture-decision-records\">Architecture Decision Records</h3>\n<p>The quantization component embodies several critical architectural decisions that balance memory efficiency, training stability, and implementation complexity. These decisions were made based on extensive experimentation with large language models and analysis of real-world deployment constraints.</p>\n<blockquote>\n<p><strong>Decision: NormalFloat Over Uniform Quantization</strong></p>\n<ul>\n<li><strong>Context</strong>: 4-bit quantization requires choosing between uniform quantization (equal spacing) and distribution-aware quantization schemes like NormalFloat that allocate precision based on weight distributions</li>\n<li><strong>Options Considered</strong>: Uniform INT4, NormalFloat (NF4), and dynamic range quantization</li>\n<li><strong>Decision</strong>: Implement NormalFloat as the primary quantization format</li>\n<li><strong>Rationale</strong>: Neural network weights exhibit approximately normal distributions, making NF4&#39;s distribution-aware bit allocation 2-3x more effective at preserving model quality compared to uniform quantization for the same bit budget</li>\n<li><strong>Consequences</strong>: Higher implementation complexity but significantly better quality preservation, enabling fine-tuning of larger models without catastrophic performance degradation</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Quality Loss</th>\n<th>Implementation</th>\n<th>Memory Usage</th>\n<th>Hardware Support</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Uniform INT4</td>\n<td>High</td>\n<td>Simple</td>\n<td>4 bits/param</td>\n<td>Universal</td>\n</tr>\n<tr>\n<td>NormalFloat</td>\n<td>Low</td>\n<td>Complex</td>\n<td>4 bits/param</td>\n<td>bitsandbytes only</td>\n</tr>\n<tr>\n<td>Dynamic Range</td>\n<td>Medium</td>\n<td>Medium</td>\n<td>4-8 bits/param</td>\n<td>Limited</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Just-in-Time Dequantization Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Quantized weights must be dequantized for computation, but keeping decompressed weights in memory defeats the purpose of quantization</li>\n<li><strong>Options Considered</strong>: Persistent dequantized cache, just-in-time dequantization, and hybrid caching strategies</li>\n<li><strong>Decision</strong>: Implement just-in-time dequantization with immediate disposal of decompressed weights</li>\n<li><strong>Rationale</strong>: Maintains maximum memory efficiency by keeping weights in 4-bit format except during actual computation, despite the computational overhead of repeated dequantization</li>\n<li><strong>Consequences</strong>: Slight performance impact from repeated dequantization operations, but enables training of larger models by maintaining consistent memory footprint</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Mixed-Precision Training with Configurable Compute Dtype</strong></p>\n<ul>\n<li><strong>Context</strong>: Training stability requires higher precision than 4-bit for gradient computation, but full float32 reduces memory benefits</li>\n<li><strong>Options Considered</strong>: Fixed float16 computation, fixed bfloat16 computation, or configurable compute precision</li>\n<li><strong>Decision</strong>: Implement configurable compute dtype with intelligent hardware-based defaults</li>\n<li><strong>Rationale</strong>: Different hardware generations have varying capabilities for mixed-precision computation, and some models benefit more from bfloat16&#39;s extended range versus float16&#39;s precision</li>\n<li><strong>Consequences</strong>: More complex configuration surface but optimal performance across diverse hardware environments</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Automatic Double Quantization with Override</strong></p>\n<ul>\n<li><strong>Context</strong>: Double quantization provides additional memory savings but adds computational overhead and implementation complexity</li>\n<li><strong>Options Considered</strong>: Never use double quantization, always enable it, or make it configurable with smart defaults</li>\n<li><strong>Decision</strong>: Enable double quantization by default but provide explicit configuration override</li>\n<li><strong>Rationale</strong>: The 2-3% additional memory savings are meaningful for edge deployment scenarios, and the computational overhead is acceptable for fine-tuning workloads that prioritize memory efficiency</li>\n<li><strong>Consequences</strong>: Slight additional complexity in error handling and debugging, but enables training of larger models on severely memory-constrained hardware</li>\n</ul>\n</blockquote>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>The quantization component introduces several subtle failure modes that can cause memory issues, training instability, or compatibility problems. Understanding these pitfalls is essential for successful deployment of quantized fine-tuning systems.</p>\n<p>⚠️ <strong>Pitfall: CUDA Version Incompatibility with bitsandbytes</strong></p>\n<p>The most common quantization failure occurs when the installed CUDA version doesn&#39;t match the bitsandbytes library&#39;s compiled CUDA version. This manifests as cryptic import errors or runtime crashes during model loading, often with unhelpful error messages about missing CUDA functions.</p>\n<p>The root cause lies in bitsandbytes&#39; use of custom CUDA kernels for quantization operations. These kernels are compiled against specific CUDA versions and cannot load if the runtime CUDA version differs significantly. The issue is particularly common in environments where CUDA was upgraded after installing bitsandbytes via pip.</p>\n<p>To diagnose this issue, check the CUDA version used to compile bitsandbytes using <code>python -c &quot;import bitsandbytes; print(bitsandbytes.cuda_setup.main())&quot;</code> and compare it against the system CUDA version via <code>nvidia-smi</code>. If they differ, reinstall bitsandbytes from source or use conda to ensure version compatibility.</p>\n<p>⚠️ <strong>Pitfall: Optimizer State Memory Explosion</strong></p>\n<p>Despite quantizing model weights to 4-bit, training can still exhaust GPU memory due to <strong>optimizer state memory explosion</strong>. Standard optimizers like Adam maintain momentum and variance estimates in full float32 precision for every trainable parameter, which can consume more memory than the quantized weights themselves.</p>\n<p>This pitfall is particularly insidious because it manifests after training begins successfully, causing out-of-memory errors during the first optimizer step rather than during model loading. The memory explosion occurs because optimizer states are allocated only when gradients are first computed, creating a delayed failure mode.</p>\n<p>The solution involves using memory-efficient optimizers like 8-bit Adam (<code>adamw_bnb_8bit</code>) that quantize optimizer states, or ensuring that only LoRA adapter parameters receive gradients while base model parameters remain frozen. Our implementation automatically freezes base model parameters during quantized training to prevent this issue.</p>\n<p>⚠️ <strong>Pitfall: Mixing Quantized and Unquantized Model Components</strong></p>\n<p>Loading a pre-quantized model checkpoint and attempting to apply additional quantization leads to <strong>double quantization errors</strong> that can corrupt weights and cause training failure. This occurs when saved checkpoints already contain 4-bit quantized weights but the loading process attempts to apply quantization again.</p>\n<p>The issue manifests as models that load successfully but produce nonsensical outputs or fail to converge during training. Weight histograms will show unnatural distributions with discrete jumps rather than smooth curves, indicating corrupted quantization.</p>\n<p>Prevention requires careful checkpoint metadata management that records the quantization state of saved models. Our implementation stores quantization configuration alongside model weights and validates compatibility during loading to prevent double quantization scenarios.</p>\n<p>⚠️ <strong>Pitfall: Insufficient Compute Dtype Precision</strong></p>\n<p>Setting <code>bnb_4bit_compute_dtype</code> to <code>float16</code> on older GPUs or with models that have large dynamic ranges can cause <strong>numerical instability</strong> manifesting as loss spikes, gradient explosions, or training divergence. This occurs because float16&#39;s limited range (±65,504) and precision cannot handle the full range of values encountered during forward passes of large models.</p>\n<p>The failure mode typically appears as intermittent loss spikes rather than consistent poor performance, making it difficult to diagnose. Gradient norms may show periodic extreme values indicating overflow conditions in the float16 computation path.</p>\n<p>The solution involves upgrading to <code>bfloat16</code> if hardware supports it, or falling back to <code>float32</code> compute dtype for maximum stability. Our implementation provides automatic dtype selection based on hardware capabilities, but manual override may be necessary for problematic models.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Memory Budget for Quantization Overhead</strong></p>\n<p>Quantization introduces <strong>temporary memory overhead</strong> during model loading that can cause out-of-memory errors even when the final quantized model would fit in available memory. This occurs because the quantization process briefly requires both the original float32 weights and the 4-bit compressed weights in memory simultaneously.</p>\n<p>The memory spike happens during the quantization transformation phase and can be 2-3x larger than the final memory footprint. Systems that precisely allocate memory based on expected quantized model size may fail during this transitional phase.</p>\n<p>Prevention requires reserving additional memory headroom during quantization (typically 20-30% beyond the expected final memory usage) and implementing gradual quantization strategies that process the model in chunks rather than loading everything simultaneously.</p>\n<p><img src=\"/api/project/llm-finetuning-pipeline/architecture-doc/asset?path=diagrams%2Fmemory-optimization-layers.svg\" alt=\"Memory Optimization Stack\"></p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides concrete implementation patterns for building the QLoRA quantization component, focusing on the bitsandbytes integration, memory monitoring infrastructure, and mixed-precision training coordination.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Quantization Backend</td>\n<td>bitsandbytes with defaults</td>\n<td>Custom CUDA kernels with hardware-specific optimization</td>\n</tr>\n<tr>\n<td>Memory Monitoring</td>\n<td>PyTorch memory stats</td>\n<td>NVIDIA ML-Python integration with detailed profiling</td>\n</tr>\n<tr>\n<td>Configuration Management</td>\n<td>Python dataclasses</td>\n<td>Hydra/OmegaConf for hierarchical configuration</td>\n</tr>\n<tr>\n<td>Compute Dtype Selection</td>\n<td>Fixed float16</td>\n<td>Hardware-aware automatic dtype selection</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  src/fine_tuning/\n    quantization/\n      __init__.py                 ← component exports\n      config.py                   ← QuantizationConfig and related types\n      memory_monitor.py           ← MemoryMonitor implementation\n      quantized_model_loader.py   ← model loading with quantization\n      mixed_precision_trainer.py  ← training coordination\n      utils.py                    ← quantization utilities and helpers\n    config/\n      quantization_presets.yaml   ← pre-configured quantization settings\n  tests/\n    test_quantization_config.py   ← configuration validation tests\n    test_memory_monitoring.py     ← memory tracking functionality tests\n    test_quantized_loading.py     ← model loading integration tests</code></pre></div>\n\n<h4 id=\"quantization-configuration-infrastructure\">Quantization Configuration Infrastructure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> yaml</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> QuantizationConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for 4-bit quantization with bitsandbytes integration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    load_in_4bit: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bnb_4bit_quant_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"nf4\"</span><span style=\"color:#6A737D\">  # NormalFloat quantization</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bnb_4bit_compute_dtype: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"float16\"</span><span style=\"color:#6A737D\">  # Computation precision</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bnb_4bit_use_double_quant: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#6A737D\">  # Quantize quantization constants</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bnb_4bit_quant_storage: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"uint8\"</span><span style=\"color:#6A737D\">  # Storage format for quantized weights</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> to_bitsandbytes_config</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Convert to bitsandbytes BitsAndBytesConfig format.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Import BitsAndBytesConfig from bitsandbytes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Map compute_dtype string to torch dtype</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return configured BitsAndBytesConfig object</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: torch.float16, torch.bfloat16 are the typical compute dtypes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> estimate_memory_reduction</span><span style=\"color:#E1E4E8\">(self, base_model_params: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Estimate memory savings from quantization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Calculate original model memory (params * 4 bytes for fp32)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate quantized memory (params * 0.5 bytes for 4-bit)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Account for quantization constant overhead</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return dict with original_mb, quantized_mb, reduction_ratio</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> from_model_size</span><span style=\"color:#E1E4E8\">(cls, model_size_gb: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, available_memory_gb: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'QuantizationConfig'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create optimized config based on model size and available memory.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Determine if quantization is necessary based on memory ratio</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Enable double quantization for very large models</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Select compute dtype based on available memory headroom</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return configured QuantizationConfig instance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> detect_optimal_compute_dtype</span><span style=\"color:#E1E4E8\">() -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Detect the best compute dtype for current hardware.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check CUDA capability version</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Test bfloat16 support with torch.cuda.is_bf16_supported()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return \"bfloat16\" for modern GPUs, \"float16\" for older ones</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Ampere (8.0) and newer support efficient bfloat16</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_quantization_compatibility</span><span style=\"color:#E1E4E8\">(model_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validate that model architecture supports bitsandbytes quantization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load model config from HuggingFace</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check architecture type (llama, mistral, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Verify bitsandbytes supports this architecture</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return compatibility boolean</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"memory-monitoring-system\">Memory Monitoring System</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> psutil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MemoryMetrics</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Memory usage snapshot at a specific point in time.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stage: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gpu_memory_allocated: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#6A737D\">  # MB</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gpu_memory_cached: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#6A737D\">     # MB </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gpu_memory_reserved: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#6A737D\">   # MB</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    system_memory_used: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#6A737D\">    # MB</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    details: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MemoryMonitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Tracks GPU and system memory usage throughout quantization and training.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    baseline_gpu_memory: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    baseline_system_memory: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    measurements: List[Dict] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> capture_baseline</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record initial memory state before model loading.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Clear GPU memory cache with torch.cuda.empty_cache()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Record GPU memory allocated/cached/reserved</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Record system memory usage with psutil.Process().memory_info()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Store baseline values in instance variables</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> measure_current_usage</span><span style=\"color:#E1E4E8\">(self, stage: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Measure current memory usage and return statistics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get current GPU memory stats from torch.cuda</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Convert bytes to MB for readability</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Get current system memory usage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Calculate deltas from baseline if available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Store measurement in self.measurements list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return current usage dict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_peak_usage</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return peak memory usage across all measurements.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Iterate through all measurements in self.measurements</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Find maximum values for each memory category</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return dict with peak values and when they occurred</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> estimate_quantization_savings</span><span style=\"color:#E1E4E8\">(self, before_stage: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, after_stage: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate memory savings achieved by quantization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Find measurements matching before_stage and after_stage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate absolute and relative memory reductions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return savings statistics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> generate_memory_report</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate human-readable memory usage report.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Format baseline and peak usage statistics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate quantization effectiveness metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Include recommendations for memory optimization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return formatted report string</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> setup_memory_monitoring</span><span style=\"color:#E1E4E8\">() -> MemoryMonitor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Initialize memory monitoring with baseline measurements.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    monitor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MemoryMonitor()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    monitor.capture_baseline()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> monitor</span></span></code></pre></div>\n\n<h4 id=\"quantized-model-loading\">Quantized Model Loading</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> transformers </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tuple, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> gc</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> QuantizedModelLoader</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Handles loading and quantization of large language models.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: QuantizationConfig, memory_monitor: MemoryMonitor):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.memory_monitor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> memory_monitor</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> load_model_and_tokenizer</span><span style=\"color:#E1E4E8\">(self, model_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load model with 4-bit quantization and compatible tokenizer.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create BitsAndBytesConfig from self.config</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Measure memory before loading</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Load tokenizer first (minimal memory impact)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Load model with quantization config</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Measure memory after loading</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Force garbage collection to clean up loading overhead</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return model and tokenizer tuple</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use device_map=\"auto\" for automatic GPU placement</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> verify_quantization_quality</span><span style=\"color:#E1E4E8\">(self, model: AutoModelForCausalLM) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Verify that quantization didn't catastrophically degrade model quality.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check that model parameters are actually quantized</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Run a simple forward pass with dummy input</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Verify output tensors have reasonable values (not NaN/Inf)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return quality validation results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> analyze_quantized_layers</span><span style=\"color:#E1E4E8\">(self, model: AutoModelForCausalLM) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Analyze which layers were quantized and their memory usage.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Iterate through model.named_modules()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Identify quantized linear layers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate memory usage per layer type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return analysis dictionary</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> setup_quantized_model</span><span style=\"color:#E1E4E8\">(model_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, config: QuantizationConfig) -> Tuple[AutoModelForCausalLM, AutoTokenizer, MemoryMonitor]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Complete setup for quantized model with monitoring.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    monitor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> setup_memory_monitoring()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    loader </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> QuantizedModelLoader(config, monitor)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Load and quantize the model</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    model, tokenizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> loader.load_model_and_tokenizer(model_name)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Verify quantization was successful</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    quality_report </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> loader.verify_quantization_quality(model)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> model, tokenizer, monitor</span></span></code></pre></div>\n\n<h4 id=\"mixed-precision-training-coordinator\">Mixed-Precision Training Coordinator</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> torch.cuda.amp </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> autocast, GradScaler</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Dict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch.nn </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> nn</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MixedPrecisionTrainer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Coordinates mixed-precision training with quantized models.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: QuantizationConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.compute_dtype </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._resolve_compute_dtype()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.use_amp </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.compute_dtype </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> torch.float32</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.scaler </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> GradScaler() </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.use_amp </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _resolve_compute_dtype</span><span style=\"color:#E1E4E8\">(self) -> torch.dtype:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Convert string compute dtype to torch dtype.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create mapping from config strings to torch dtypes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Handle \"auto\" selection based on hardware capabilities</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate dtype is supported on current device</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return resolved torch dtype</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward_pass_context</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return appropriate context manager for forward pass.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Return autocast context if using mixed precision</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Return nullcontext if using full precision</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: autocast('cuda', dtype=self.compute_dtype) for mixed precision</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> backward_and_step</span><span style=\"color:#E1E4E8\">(self, loss: torch.Tensor, optimizer: Any, model: nn.Module) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute backward pass and optimizer step with proper scaling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Scale loss if using gradient scaler</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Compute gradients with scaled loss</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Unscale gradients before optimizer step</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check for gradient overflow and skip step if necessary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Update gradient scaler</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return step statistics (loss, grad_norm, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> configure_optimizer_for_quantized_training</span><span style=\"color:#E1E4E8\">(self, model: nn.Module) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Configure memory-efficient optimizer for quantized model training.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Filter for only trainable parameters (LoRA adapters)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create 8-bit AdamW optimizer if available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Set appropriate learning rate and weight decay</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return configured optimizer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> create_training_coordinator</span><span style=\"color:#E1E4E8\">(quantization_config: QuantizationConfig, model: nn.Module) -> MixedPrecisionTrainer:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Create and configure mixed-precision training coordinator.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    trainer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MixedPrecisionTrainer(quantization_config)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Verify compute dtype compatibility with model</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    test_input </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.randn(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">dtype</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">trainer.compute_dtype, </span><span style=\"color:#FFAB70\">device</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">next</span><span style=\"color:#E1E4E8\">(model.parameters()).device)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> trainer</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p>After implementing the QLoRA quantization component, verify the following behavior:</p>\n<p><strong>Memory Usage Validation:</strong></p>\n<ul>\n<li>Run <code>python -c &quot;from src.fine_tuning.quantization import setup_quantized_model; model, tokenizer, monitor = setup_quantized_model(&#39;microsoft/DialoGPT-medium&#39;, QuantizationConfig()); print(monitor.generate_memory_report())&quot;</code></li>\n<li>Expected: Memory usage should be approximately 25% of the original model size</li>\n<li>Troubleshooting: If memory usage is higher than expected, check that <code>load_in_4bit=True</code> and verify bitsandbytes installation</li>\n</ul>\n<p><strong>Quantization Quality Check:</strong></p>\n<ul>\n<li>Load a quantized model and generate text with the same prompt as the original model</li>\n<li>Expected: Output quality should be comparable with minimal degradation</li>\n<li>Warning signs: Repetitive text, nonsensical outputs, or generation failures indicate quantization issues</li>\n</ul>\n<p><strong>Mixed-Precision Training Stability:</strong></p>\n<ul>\n<li>Start a short training run (10 steps) and monitor loss values and gradient norms</li>\n<li>Expected: Loss should decrease and gradients should remain finite</li>\n<li>Red flags: Loss spikes to infinity, gradient norms exceeding 10^6, or immediate divergence</li>\n</ul>\n<p><strong>Memory Monitoring Accuracy:</strong></p>\n<ul>\n<li>Compare reported memory usage with <code>nvidia-smi</code> output during model loading</li>\n<li>Expected: Memory measurements should match within 10% of system measurements</li>\n<li>Issues: Large discrepancies indicate measurement timing problems or cache clearing issues</li>\n</ul>\n<h2 id=\"training-loop-component\">Training Loop Component</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 4 - this section implements the training orchestration system that coordinates the fine-tuning process with gradient accumulation, learning rate scheduling, checkpoint management, loss tracking, and early stopping mechanisms.</p>\n</blockquote>\n<p><img src=\"/api/project/llm-finetuning-pipeline/architecture-doc/asset?path=diagrams%2Ftraining-state-machine.svg\" alt=\"Training Loop State Machine\"></p>\n<h3 id=\"mental-model-the-personal-trainer\">Mental Model: The Personal Trainer</h3>\n<p>Think of the training loop as a <strong>dedicated personal trainer</strong> working with an athlete to master new skills. The trainer doesn&#39;t just tell the athlete to &quot;get better&quot; - they create a structured program with carefully planned progressions, monitor performance metrics, adjust intensity based on progress, and know when to push harder or ease up to prevent burnout.</p>\n<p>Just as a personal trainer breaks down complex movements into manageable sets and reps, the training loop breaks down the massive task of learning from thousands of examples into digestible micro-batches. The trainer tracks performance over time, celebrates improvements, and intervenes when progress stalls. They save snapshots of the athlete&#39;s best performances and can roll back to previous training states if an injury or setback occurs.</p>\n<p>The training loop embodies this same nurturing but systematic approach. It feeds the model small batches of examples repeatedly, accumulates gradients like a trainer accumulates evidence of improvement, adjusts the learning intensity through scheduling, and maintains detailed records of progress. When the model shows signs of overtraining (like an athlete showing fatigue), the training loop implements early stopping to preserve peak performance.</p>\n<p>This mental model helps us understand why each component of the training loop matters: gradient accumulation simulates the effect of larger training sessions, learning rate scheduling prevents the model from making overly aggressive changes that could disrupt previous learning, checkpointing preserves the best versions of model knowledge, and loss tracking provides the quantitative feedback needed to make informed training decisions.</p>\n<h3 id=\"gradient-accumulation-strategy\">Gradient Accumulation Strategy</h3>\n<p><strong>Gradient accumulation</strong> serves as the cornerstone technique for simulating large effective batch sizes when GPU memory constraints prevent loading large batches directly. In parameter-efficient fine-tuning, this becomes even more critical because we&#39;re working with resource-constrained environments where even quantized models push memory limits.</p>\n<p>The fundamental insight behind gradient accumulation is that gradient computation is linear - we can split a large batch into smaller micro-batches, compute gradients for each micro-batch separately, accumulate (sum) these gradients, and then perform a single optimizer step. This mathematically equivalent to processing the entire large batch at once, but with dramatically reduced peak memory usage.</p>\n<p>Consider a scenario where we want an effective batch size of 32 samples, but our GPU can only fit 4 samples at a time. The gradient accumulation strategy divides this into 8 micro-batches of 4 samples each. The training loop processes each micro-batch sequentially, accumulating gradients without clearing them between batches, and only performs the optimizer step and gradient clearing after processing all 8 micro-batches.</p>\n<p>The implementation requires careful coordination of several PyTorch mechanisms. During each micro-batch forward pass, we must scale the loss by the number of accumulation steps to ensure the final accumulated gradient has the correct magnitude. We also need to manage the model&#39;s training state to prevent automatic gradient clearing between micro-batches.</p>\n<p>Here&#39;s how the gradient accumulation process unfolds step by step:</p>\n<ol>\n<li><strong>Initialize accumulation counters</strong> and set the model to training mode while ensuring gradients start cleared from any previous iteration</li>\n<li><strong>Enter the micro-batch loop</strong> where each iteration processes one micro-batch worth of samples from the current training batch</li>\n<li><strong>Perform forward pass</strong> on the micro-batch, computing loss values and scaling them by the inverse of the total accumulation steps</li>\n<li><strong>Execute backward pass</strong> using the scaled loss, which computes gradients and adds them to any existing accumulated gradients in parameter tensors</li>\n<li><strong>Check memory usage</strong> and optionally clear intermediate activations to prevent memory buildup during long accumulation sequences</li>\n<li><strong>Continue accumulation loop</strong> until all micro-batches for the current effective batch have been processed</li>\n<li><strong>Apply gradient clipping</strong> to the accumulated gradients to prevent training instability from gradient explosion</li>\n<li><strong>Perform optimizer step</strong> using the accumulated gradients to update model parameters</li>\n<li><strong>Clear accumulated gradients</strong> and reset counters in preparation for the next effective batch</li>\n<li><strong>Update learning rate scheduler</strong> based on the completion of one effective training step</li>\n</ol>\n<p>The gradient accumulation strategy interacts closely with mixed-precision training in QLoRA setups. When using 4-bit quantized weights with float16 computation, gradient accumulation must handle the precision transitions carefully. The accumulated gradients themselves are typically stored in float32 for numerical stability, even when forward pass computations use lower precision.</p>\n<p>Memory management during gradient accumulation requires particular attention. While the strategy reduces peak memory usage compared to large batches, it still accumulates gradients in GPU memory throughout the accumulation process. In extreme memory-constrained scenarios, we may need to implement gradient checkpointing alongside accumulation to reduce activation memory usage during the backward passes.</p>\n<blockquote>\n<p><strong>Key Design Insight</strong>: Gradient accumulation transforms memory constraints from a hard limit into a time-space tradeoff. We exchange longer training time for reduced memory usage, making large-scale fine-tuning feasible on consumer hardware.</p>\n</blockquote>\n<p>The strategy also affects learning dynamics subtly but importantly. Gradient accumulation introduces slightly different noise characteristics compared to true large batch training because the accumulated gradients represent an exact sum rather than a stochastic sample. This generally leads to more stable training but can occasionally reduce the beneficial regularization effect of gradient noise.</p>\n<p><strong>Architecture Decision Record: Gradient Accumulation Implementation</strong></p>\n<blockquote>\n<p><strong>Decision: Implement gradient accumulation at the training loop level rather than using framework-provided utilities</strong></p>\n<ul>\n<li><strong>Context</strong>: Multiple frameworks offer gradient accumulation features, but we need fine-grained control over memory management and compatibility with QLoRA quantization</li>\n<li><strong>Options Considered</strong>: <ul>\n<li>Use Transformers library&#39;s gradient accumulation in <code>TrainingArguments</code></li>\n<li>Implement custom gradient accumulation in our training loop</li>\n<li>Use PyTorch&#39;s native gradient accumulation utilities</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Implement custom gradient accumulation with explicit control over each step</li>\n<li><strong>Rationale</strong>: Custom implementation allows precise memory management during quantized training, better integration with our checkpoint system, and explicit handling of precision transitions in mixed-precision QLoRA training</li>\n<li><strong>Consequences</strong>: Requires more implementation effort but provides necessary control over memory usage patterns and ensures compatibility with our quantization and adapter strategies</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Gradient Accumulation Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Memory Control</th>\n<th>QLoRA Compatibility</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Transformers TrainingArguments</td>\n<td>Minimal code, battle-tested</td>\n<td>Limited memory control, potential quantization conflicts</td>\n<td>Low</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Custom loop implementation</td>\n<td>Full control, optimized for our use case</td>\n<td>More implementation complexity, potential bugs</td>\n<td>High</td>\n<td>High</td>\n</tr>\n<tr>\n<td>PyTorch native utilities</td>\n<td>Good balance, framework support</td>\n<td>Less control than custom, may not handle our edge cases</td>\n<td>Medium</td>\n<td>Medium</td>\n</tr>\n</tbody></table>\n<h3 id=\"learning-rate-scheduling\">Learning Rate Scheduling</h3>\n<p><strong>Learning rate scheduling</strong> serves as the sophisticated mechanism for controlling how aggressively the model updates its parameters throughout the training process. In the context of parameter-efficient fine-tuning with LoRA adapters, learning rate scheduling becomes even more nuanced because we&#39;re adapting a pre-trained model rather than training from scratch.</p>\n<p>The fundamental challenge in fine-tuning lies in balancing knowledge retention with new knowledge acquisition. Too high a learning rate causes <strong>catastrophic forgetting</strong> where the model loses its pre-trained capabilities while learning the new task. Too low a learning rate results in insufficient adaptation, where the model fails to learn the new task effectively. Learning rate scheduling navigates this delicate balance by starting with careful warm-up phases and implementing strategic decay patterns.</p>\n<p><strong>Warmup scheduling</strong> addresses the initialization instability problem that commonly occurs at the beginning of fine-tuning. When LoRA adapters are first initialized, they contain random weights that produce large, potentially destructive gradient updates. The warmup phase gradually increases the learning rate from near-zero to the target value over a specified number of steps, allowing the adapter weights to stabilize before full-strength training begins.</p>\n<p>The warmup process typically follows a linear schedule where the learning rate increases proportionally with the step number until reaching the peak learning rate. During this phase, the model makes small, cautious updates that prevent early training instability while allowing the LoRA adapters to find reasonable initial directions for adaptation.</p>\n<p><strong>Decay scheduling</strong> manages the later phases of training when the model approaches convergence. As training progresses and loss improvements become smaller, maintaining high learning rates can cause the model to oscillate around optimal parameter values rather than settling into them. Decay schedules systematically reduce the learning rate to enable fine-grained convergence.</p>\n<p><strong>Cosine decay scheduling</strong> has emerged as particularly effective for fine-tuning applications. This schedule follows a cosine curve that starts at the peak learning rate after warmup and smoothly decreases to a minimum value (often 10% of the peak rate) by the end of training. The cosine schedule provides faster initial decay when large improvements are still possible, followed by more gradual decay as the model approaches convergence.</p>\n<p>The mathematical formulation for cosine decay with warmup involves several phases:</p>\n<ol>\n<li><strong>Warmup phase</strong> (steps 0 to warmup_steps): learning rate increases linearly from 0 to peak_learning_rate</li>\n<li><strong>Cosine decay phase</strong> (steps warmup_steps to total_steps): learning rate follows cosine decay from peak_learning_rate to min_learning_rate</li>\n<li><strong>Optional constant phase</strong>: some schedules maintain the minimum learning rate for final stabilization</li>\n</ol>\n<p><strong>Linear decay scheduling</strong> provides an alternative approach where the learning rate decreases linearly after the warmup phase. While simpler to implement and understand, linear decay can sometimes lead to more abrupt transitions between high and low learning rate regimes, potentially causing training instability.</p>\n<p><strong>Constant scheduling with warmup</strong> maintains the peak learning rate throughout most of training, only applying warmup at the beginning. This approach works well for tasks where the optimal learning rate remains consistent throughout training, but it requires careful tuning to avoid late-training instability.</p>\n<p>The interaction between learning rate scheduling and LoRA&#39;s alpha parameter requires special consideration. The effective learning rate for LoRA adapters is scaled by the ratio alpha/rank, meaning that the scheduled learning rate gets further modified by the LoRA configuration. This interaction must be accounted for when selecting appropriate peak learning rates and warmup schedules.</p>\n<p><strong>Architecture Decision Record: Learning Rate Scheduling Strategy</strong></p>\n<blockquote>\n<p><strong>Decision: Implement cosine decay with linear warmup as the primary scheduling strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Fine-tuning requires careful balance between adaptation and knowledge retention, with different optimal learning rates at different training phases</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Linear warmup with cosine decay</li>\n<li>Linear warmup with linear decay</li>\n<li>Exponential warmup with polynomial decay</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Linear warmup with cosine decay, configurable warmup ratio and minimum learning rate</li>\n<li><strong>Rationale</strong>: Cosine decay provides smooth convergence characteristics that work well with pre-trained models, linear warmup is stable and predictable, and the combination has strong empirical results in fine-tuning literature</li>\n<li><strong>Consequences</strong>: Requires tuning of warmup ratio and minimum learning rate, but provides robust convergence characteristics across different tasks and model sizes</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Learning Rate Schedule</th>\n<th>Convergence Quality</th>\n<th>Stability</th>\n<th>Tuning Complexity</th>\n<th>Memory Retention</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Linear warmup + Cosine decay</td>\n<td>High</td>\n<td>High</td>\n<td>Medium</td>\n<td>High</td>\n</tr>\n<tr>\n<td>Linear warmup + Linear decay</td>\n<td>Medium</td>\n<td>High</td>\n<td>Low</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Exponential warmup + Polynomial decay</td>\n<td>Medium</td>\n<td>Medium</td>\n<td>High</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Constant with warmup</td>\n<td>Low</td>\n<td>Medium</td>\n<td>Low</td>\n<td>Low</td>\n</tr>\n</tbody></table>\n<p>The learning rate scheduling system must integrate closely with the gradient accumulation strategy. Since gradient accumulation changes the effective frequency of parameter updates, the scheduler must track &quot;effective steps&quot; (completed accumulation cycles) rather than individual forward passes. This ensures that learning rate changes correspond to actual parameter updates rather than individual micro-batch processes.</p>\n<h3 id=\"checkpoint-and-state-management\">Checkpoint and State Management</h3>\n<p><strong>Checkpoint and state management</strong> forms the reliability backbone of the fine-tuning pipeline, ensuring that training progress is preserved against hardware failures, preemption events, and experimental iterations. In parameter-efficient fine-tuning contexts, checkpoint management becomes more sophisticated because we must handle both base model state and adapter-specific configurations.</p>\n<p>The checkpoint system must preserve multiple categories of training state. <strong>Model state</strong> includes the LoRA adapter weights (the base model weights remain frozen and unchanged). <strong>Optimizer state</strong> contains momentum buffers, variance estimates, and other stateful information needed for optimizers like AdamW to resume training effectively. <strong>Scheduler state</strong> tracks the current step count, learning rate value, and any internal scheduler parameters. <strong>Training metadata</strong> captures the current epoch, step count, best validation loss, and configuration parameters.</p>\n<p><strong>Checkpoint timing strategies</strong> determine when to save training state. <strong>Step-interval checkpointing</strong> saves state every N training steps, providing regular snapshots of training progress. This strategy ensures minimal progress loss during failures but can generate large numbers of checkpoint files during long training runs. <strong>Epoch-based checkpointing</strong> saves state at the end of each training epoch, aligning checkpoints with natural training boundaries but potentially losing more progress during failures.</p>\n<p><strong>Evaluation-triggered checkpointing</strong> saves state after each validation evaluation, particularly when validation metrics improve. This strategy focuses checkpoint storage on the most promising model states but may miss intermediate improvements or create irregular checkpoint intervals.</p>\n<p>The <strong>best model tracking</strong> mechanism maintains a separate record of the checkpoint with the lowest validation loss (or highest task-specific metric). This enables easy retrieval of the optimal model state even if training continues past the optimal point or experiences later degradation.</p>\n<p><strong>Storage optimization</strong> becomes critical when dealing with large models and frequent checkpointing. <strong>Incremental checkpointing</strong> saves only the changes since the last checkpoint, dramatically reducing storage requirements for LoRA adapters where only a small fraction of parameters change between checkpoints. <strong>Compression techniques</strong> can reduce checkpoint file sizes by 50-80% with minimal impact on loading times.</p>\n<p><strong>Checkpoint cleanup policies</strong> manage disk space by automatically removing older checkpoints. <strong>Keep-last-N policies</strong> maintain only the most recent N checkpoints, ensuring bounded storage usage. <strong>Keep-best-plus-recent policies</strong> preserve the best checkpoint plus the last few recent checkpoints, balancing optimization needs with failure recovery.</p>\n<p>The checkpoint loading and resumption process requires careful state reconstruction. When resuming training, the system must reload the model state, optimizer state, scheduler state, and training metadata in the correct order. <strong>State validation</strong> checks ensure that the loaded checkpoint matches the current training configuration, preventing subtle errors from configuration changes between training sessions.</p>\n<p><strong>Architecture Decision Record: Checkpoint Storage Format</strong></p>\n<blockquote>\n<p><strong>Decision: Use separate files for model weights, optimizer state, and metadata with atomic write operations</strong></p>\n<ul>\n<li><strong>Context</strong>: Checkpoint corruption during writes can destroy training progress, and different checkpoint components have different update frequencies and storage requirements</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Single monolithic checkpoint file</li>\n<li>Separate files for different state components</li>\n<li>Database-backed checkpoint storage</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Separate files with atomic writes and validation checksums</li>\n<li><strong>Rationale</strong>: Separate files enable partial checkpoint loading, reduce corruption risk through atomic operations, and allow different compression strategies for different components. Validation checksums detect corruption early.</li>\n<li><strong>Consequences</strong>: Slightly more complex checkpoint management code, but dramatically improved reliability and flexibility for partial loading scenarios</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Checkpoint Strategy</th>\n<th>Reliability</th>\n<th>Storage Efficiency</th>\n<th>Loading Speed</th>\n<th>Implementation Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Monolithic files</td>\n<td>Medium</td>\n<td>Low</td>\n<td>Fast</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Separate component files</td>\n<td>High</td>\n<td>High</td>\n<td>Medium</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Database storage</td>\n<td>High</td>\n<td>Medium</td>\n<td>Slow</td>\n<td>High</td>\n</tr>\n<tr>\n<td>Compressed monolithic</td>\n<td>Medium</td>\n<td>High</td>\n<td>Medium</td>\n<td>Medium</td>\n</tr>\n</tbody></table>\n<p><strong>Checkpoint validation</strong> ensures that saved checkpoints are complete and uncorrupted. The validation process includes checksum verification of file contents, consistency checks between model architecture and saved weights, and verification that optimizer state dimensions match the current model configuration.</p>\n<p><strong>Distributed checkpoint handling</strong> becomes relevant when scaling to multi-GPU scenarios. Each process must coordinate checkpoint timing to ensure consistent global state, and checkpoint files must account for distributed optimizer states and model sharding.</p>\n<h3 id=\"loss-tracking-and-early-stopping\">Loss Tracking and Early Stopping</h3>\n<p><strong>Loss tracking and early stopping</strong> provide the monitoring and intervention mechanisms that prevent overfitting and enable efficient resource utilization during fine-tuning. These systems work together to continuously assess training progress and make intelligent decisions about when to continue, modify, or terminate training.</p>\n<p><strong>Comprehensive loss tracking</strong> monitors multiple categories of metrics throughout training. <strong>Training loss</strong> measures how well the model fits the training data at each step, providing immediate feedback on learning progress. <strong>Validation loss</strong> evaluates model performance on held-out data, serving as the primary indicator of generalization quality and overfitting detection. <strong>Learning rate tracking</strong> records the current learning rate value to correlate training behavior with schedule changes. <strong>Gradient norm tracking</strong> monitors the magnitude of gradient updates to detect training instability or convergence.</p>\n<p>The loss tracking system implements <strong>smoothing and statistical analysis</strong> to distinguish meaningful trends from random fluctuations. <strong>Exponential moving averages</strong> smooth noisy training loss curves to reveal underlying trends. <strong>Validation loss analysis</strong> looks for patterns like consistent improvement, stagnation, or deterioration over multiple evaluation cycles.</p>\n<p><strong>Memory-efficient logging</strong> becomes important during long training runs that may generate thousands of metric data points. <strong>Hierarchical sampling</strong> logs every step initially but reduces frequency for older data points. <strong>Statistical summaries</strong> replace individual data points with summary statistics (mean, min, max, variance) over time windows.</p>\n<p><strong>Early stopping mechanisms</strong> implement sophisticated policies for training termination based on validation performance. <strong>Patience-based stopping</strong> allows validation loss to stop improving for a configurable number of evaluation cycles before terminating training. This approach accounts for natural training fluctuations while preventing excessive training on non-improving models.</p>\n<p><strong>Improvement threshold stopping</strong> requires validation loss improvements to exceed a minimum threshold to be considered significant. This prevents early stopping due to tiny, potentially meaningless improvements and focuses on substantial learning progress.</p>\n<p><strong>Combined patience and threshold stopping</strong> requires both significant improvement magnitude and recent improvement timing. This robust approach combines the benefits of both individual strategies while reducing false positive termination.</p>\n<p>The early stopping system must handle <strong>validation evaluation scheduling</strong> carefully. <strong>Fixed interval evaluation</strong> runs validation every N training steps, providing regular performance snapshots but potentially missing rapid changes. <strong>Adaptive evaluation</strong> increases evaluation frequency when validation loss shows interesting patterns (improvement or degradation) and reduces frequency during stable periods.</p>\n<p><strong>Plateau detection algorithms</strong> identify when training has reached a local optimum or learning has stagnated. <strong>Statistical plateau detection</strong> uses statistical tests to determine when validation loss distributions have become statistically indistinguishable across recent evaluation windows. <strong>Slope-based plateau detection</strong> measures the slope of validation loss curves and triggers when slopes approach zero for extended periods.</p>\n<p><strong>Architecture Decision Record: Early Stopping Configuration</strong></p>\n<blockquote>\n<p><strong>Decision: Implement patience-based early stopping with configurable improvement thresholds and validation scheduling</strong></p>\n<ul>\n<li><strong>Context</strong>: Fine-tuning runs can be expensive and may overfit if allowed to continue indefinitely, but premature stopping can prevent full adaptation</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Simple patience-based stopping</li>\n<li>Threshold-based stopping only</li>\n<li>Combined patience and threshold with adaptive evaluation</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Combined approach with configurable patience, improvement threshold, and evaluation interval</li>\n<li><strong>Rationale</strong>: Combined approach provides robustness against both premature stopping and excessive training. Configurable parameters allow adaptation to different task characteristics and resource constraints.</li>\n<li><strong>Consequences</strong>: More configuration complexity but better training efficiency and overfitting prevention across diverse tasks</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Early Stopping Strategy</th>\n<th>False Positive Rate</th>\n<th>Resource Efficiency</th>\n<th>Configuration Complexity</th>\n<th>Overfitting Prevention</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Patience only</td>\n<td>Medium</td>\n<td>Medium</td>\n<td>Low</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Threshold only</td>\n<td>High</td>\n<td>Low</td>\n<td>Low</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Combined patience + threshold</td>\n<td>Low</td>\n<td>High</td>\n<td>Medium</td>\n<td>High</td>\n</tr>\n<tr>\n<td>Adaptive evaluation + combined</td>\n<td>Very Low</td>\n<td>Very High</td>\n<td>High</td>\n<td>Very High</td>\n</tr>\n</tbody></table>\n<p><strong>Loss divergence detection</strong> identifies training instability before it becomes catastrophic. <strong>Gradient explosion detection</strong> monitors gradient norms for sudden increases that indicate numerical instability. <strong>Loss explosion detection</strong> watches for rapid increases in training loss that suggest learning rate problems or batch corruption.</p>\n<p><strong>Recovery mechanisms</strong> can sometimes salvage training runs that show signs of instability. <strong>Learning rate reduction</strong> automatically decreases the learning rate when loss divergence is detected. <strong>Checkpoint rollback</strong> reverts to a previous stable checkpoint and resumes training with modified hyperparameters.</p>\n<h3 id=\"architecture-decision-records\">Architecture Decision Records</h3>\n<p>This section consolidates the key architectural decisions that shape the training loop&#39;s design and behavior, providing the rationale and trade-offs for each major choice.</p>\n<p><strong>Decision: Training Step Definition and Progress Tracking</strong></p>\n<blockquote>\n<p><strong>Decision: Define training steps as completed gradient accumulation cycles rather than individual forward passes</strong></p>\n<ul>\n<li><strong>Context</strong>: Gradient accumulation creates a distinction between micro-batch forward passes and actual parameter updates, affecting how we measure training progress</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Count individual forward passes as steps</li>\n<li>Count completed accumulation cycles as steps</li>\n<li>Maintain separate counters for both metrics</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Use completed accumulation cycles as the primary step metric with secondary micro-batch tracking</li>\n<li><strong>Rationale</strong>: Accumulation cycles correspond to actual parameter updates and learning progress. Scheduler updates and checkpoint intervals should align with meaningful learning steps rather than arbitrary micro-batch boundaries.</li>\n<li><strong>Consequences</strong>: Requires careful coordination between accumulation logic and progress tracking, but provides more meaningful progress metrics and better alignment with learning rate scheduling</li>\n</ul>\n</blockquote>\n<p><strong>Decision: Memory Management During Training</strong></p>\n<blockquote>\n<p><strong>Decision: Implement proactive memory monitoring with automatic garbage collection triggers</strong></p>\n<ul>\n<li><strong>Context</strong>: QLoRA training pushes GPU memory limits, and memory leaks or accumulation can cause out-of-memory failures mid-training</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Reactive memory management (handle OOM when it occurs)</li>\n<li>Proactive monitoring with periodic cleanup</li>\n<li>Continuous memory optimization with automatic adjustment</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Proactive monitoring with configurable memory thresholds and automatic cleanup triggers</li>\n<li><strong>Rationale</strong>: Proactive approach prevents training failures rather than recovering from them. Configurable thresholds allow adaptation to different hardware configurations. Automatic cleanup reduces manual intervention requirements.</li>\n<li><strong>Consequences</strong>: Adds monitoring overhead but significantly improves training reliability on memory-constrained systems</li>\n</ul>\n</blockquote>\n<p><strong>Decision: Checkpoint Metadata and Versioning</strong></p>\n<blockquote>\n<p><strong>Decision: Include comprehensive metadata and configuration snapshots in all checkpoints</strong></p>\n<ul>\n<li><strong>Context</strong>: Training experiments often involve iterative hyperparameter adjustments, and checkpoint compatibility across configuration changes affects reproducibility</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Minimal checkpoints with only model weights</li>\n<li>Checkpoints with basic metadata (step, loss)</li>\n<li>Comprehensive checkpoints with full configuration snapshots</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Comprehensive metadata including configuration hashes, dependency versions, and training environment information</li>\n<li><strong>Rationale</strong>: Comprehensive metadata enables reproducible research, supports checkpoint compatibility validation, and facilitates experiment tracking. The storage overhead is minimal compared to model weights.</li>\n<li><strong>Consequences</strong>: Larger checkpoint files and more complex checkpoint loading logic, but dramatically improved experiment reproducibility and debugging capabilities</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Architecture Decision Area</th>\n<th>Complexity Impact</th>\n<th>Performance Impact</th>\n<th>Maintenance Impact</th>\n<th>Chosen Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Step definition strategy</td>\n<td>Medium</td>\n<td>Low</td>\n<td>Low</td>\n<td>Accumulation cycles</td>\n</tr>\n<tr>\n<td>Memory management approach</td>\n<td>High</td>\n<td>Medium</td>\n<td>Medium</td>\n<td>Proactive monitoring</td>\n</tr>\n<tr>\n<td>Checkpoint metadata strategy</td>\n<td>Medium</td>\n<td>Low</td>\n<td>Low</td>\n<td>Comprehensive metadata</td>\n</tr>\n<tr>\n<td>Early stopping configuration</td>\n<td>High</td>\n<td>High</td>\n<td>Medium</td>\n<td>Combined patience + threshold</td>\n</tr>\n</tbody></table>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>This section identifies the most frequent mistakes that developers encounter when implementing training loop components, providing concrete descriptions of each pitfall and specific guidance for avoiding or correcting them.</p>\n<p>⚠️ <strong>Pitfall: Learning Rate Scale Confusion with Gradient Accumulation</strong></p>\n<p>Many developers forget to adjust their learning rate when implementing gradient accumulation, leading to either under-training or training instability. When you accumulate gradients across N micro-batches, the effective batch size increases by a factor of N, but the gradient magnitude also increases by the same factor. If you don&#39;t adjust the learning rate accordingly, the model receives N times stronger updates than intended.</p>\n<p>The specific mistake is using the same learning rate that worked for batch size 4 when implementing gradient accumulation that creates an effective batch size of 32. The accumulated gradients are 8 times larger, causing the model to make overly aggressive parameter updates that can lead to catastrophic forgetting or numerical instability.</p>\n<p>To fix this, either scale your learning rate down by the accumulation factor (divide by N) or scale your loss by the accumulation factor during the forward pass. The loss scaling approach is generally preferred because it maintains the same numerical range for gradients and works better with mixed-precision training.</p>\n<p>⚠️ <strong>Pitfall: Optimizer State Corruption During Checkpoint Resume</strong></p>\n<p>A subtle but devastating error occurs when the optimizer state becomes misaligned with model parameters during checkpoint resumption. This happens when developers save model state and optimizer state separately, then reload them in the wrong order or with different model configurations. The optimizer maintains momentum and variance estimates for each parameter, and misalignment causes convergence problems that are difficult to diagnose.</p>\n<p>The symptom appears as training that resumes successfully but shows degraded performance, erratic loss curves, or failure to converge from the resumed checkpoint. The model architecture and weights load correctly, but the optimizer&#39;s internal state references the wrong parameters or contains stale statistics.</p>\n<p>To prevent this, always save and load model and optimizer states as a coordinated unit. Validate that the number of trainable parameters matches between save and load time. Include optimizer state validation checks that compare parameter shapes and counts. When in doubt, restart optimizer state from scratch rather than risking misaligned momentum buffers.</p>\n<p>⚠️ <strong>Pitfall: Early Stopping on Training Loss Instead of Validation Loss</strong></p>\n<p>Novice developers sometimes implement early stopping based on training loss rather than validation loss, completely defeating the purpose of early stopping. Training loss typically decreases monotonically throughout training (especially with gradient accumulation smoothing), so early stopping on training loss either never triggers or triggers prematurely due to temporary loss spikes.</p>\n<p>The fundamental error is misunderstanding that early stopping should prevent overfitting, which requires monitoring generalization performance via validation loss. Training loss measures memorization of the training set, not the model&#39;s ability to generalize to new data.</p>\n<p>Always implement early stopping based on validation loss or other held-out evaluation metrics. Set up regular validation evaluation during training (every few epochs or every N steps). Track validation loss trends over multiple evaluation cycles to distinguish random fluctuation from genuine performance degradation.</p>\n<p>⚠️ <strong>Pitfall: Gradient Accumulation Without Proper Loss Scaling</strong></p>\n<p>Developers often implement gradient accumulation by simply not clearing gradients between micro-batches, but forget to scale the loss values appropriately. This results in accumulated gradients that are N times larger than intended, where N is the number of accumulation steps. The oversized gradients cause training instability, poor convergence, or complete training failure.</p>\n<p>The mistake manifests as training that appears to work initially but shows unstable loss curves, poor final performance, or catastrophic forgetting of pre-trained capabilities. The gradient norms become abnormally large, and learning rate schedules that normally work become completely inappropriate.</p>\n<p>To fix this, divide your loss by the number of accumulation steps during the forward pass, before calling backward(). This ensures that the accumulated gradients have the same magnitude as if you had processed a single large batch. Always verify gradient norms remain in reasonable ranges after implementing accumulation.</p>\n<p>⚠️ <strong>Pitfall: Memory Leaks from Retained Computation Graphs</strong></p>\n<p>In PyTorch, computation graphs can accumulate during training loops if intermediate tensors retain references to gradient computation. This is particularly problematic with gradient accumulation, where multiple forward passes occur before graph cleanup. The leaked computation graphs consume GPU memory progressively until out-of-memory errors occur.</p>\n<p>The symptom is training that starts successfully but shows steadily increasing memory usage even when batch sizes and model sizes remain constant. Memory usage grows linearly with training steps rather than remaining stable after initial allocation.</p>\n<p>To prevent this, ensure that loss values are converted to Python scalars (using .item()) before logging or storing them. Avoid keeping references to intermediate tensors outside the training loop. Use del statements to explicitly remove large temporary tensors. Enable gradient checkpointing to reduce activation memory usage during backward passes.</p>\n<p>⚠️ <strong>Pitfall: Checkpoint Corruption from Concurrent Writes</strong></p>\n<p>File system races can corrupt checkpoints when training processes write checkpoint data concurrently or when external processes (monitoring tools, backup systems) access checkpoint files during writes. This leads to partially written or corrupted checkpoints that fail to load during recovery attempts.</p>\n<p>The corruption typically manifests as load errors with cryptic messages about truncated files, mismatched tensor shapes, or deserialization failures. These errors only appear when attempting to resume training, making them particularly frustrating to debug.</p>\n<p>To prevent corruption, always use atomic write operations for checkpoints. Write to temporary files first, then rename them to the final checkpoint name after successful completion. Implement file locking or write coordination if multiple processes might access checkpoint directories. Add checksum validation to detect corrupted checkpoints early.</p>\n<p>⚠️ <strong>Pitfall: Validation Set Data Leakage Through Preprocessing</strong></p>\n<p>Subtle data leakage occurs when preprocessing steps (normalization statistics, vocabulary building, or feature scaling) use information from both training and validation sets. In the context of LLM fine-tuning, this often happens with tokenizer training or chat template standardization that incorporates validation data.</p>\n<p>The leakage compromises the validation set&#39;s ability to provide unbiased performance estimates, leading to overly optimistic early stopping decisions and poor generalization to truly unseen data. The training appears to converge well, but deployed models show performance degradation.</p>\n<p>To avoid leakage, ensure all preprocessing decisions use only training data. Compute normalization statistics, vocabulary, and templates from training samples only, then apply these fixed parameters to validation data. Maintain strict separation between training and validation data throughout the entire pipeline.</p>\n<table>\n<thead>\n<tr>\n<th>Pitfall Category</th>\n<th>Detection Difficulty</th>\n<th>Impact Severity</th>\n<th>Prevention Complexity</th>\n<th>Most Common Cause</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Learning rate scaling</td>\n<td>Medium</td>\n<td>High</td>\n<td>Low</td>\n<td>Forgetting accumulation effects</td>\n</tr>\n<tr>\n<td>Optimizer state corruption</td>\n<td>High</td>\n<td>Very High</td>\n<td>Medium</td>\n<td>Checkpoint loading order</td>\n</tr>\n<tr>\n<td>Wrong loss for early stopping</td>\n<td>Low</td>\n<td>Medium</td>\n<td>Low</td>\n<td>Conceptual misunderstanding</td>\n</tr>\n<tr>\n<td>Loss scaling in accumulation</td>\n<td>Medium</td>\n<td>High</td>\n<td>Low</td>\n<td>Incomplete accumulation implementation</td>\n</tr>\n<tr>\n<td>Memory leaks</td>\n<td>Medium</td>\n<td>High</td>\n<td>Medium</td>\n<td>Computation graph retention</td>\n</tr>\n<tr>\n<td>Checkpoint corruption</td>\n<td>High</td>\n<td>Very High</td>\n<td>Medium</td>\n<td>Concurrent file access</td>\n</tr>\n<tr>\n<td>Validation data leakage</td>\n<td>Very High</td>\n<td>High</td>\n<td>High</td>\n<td>Preprocessing on combined data</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This implementation guidance provides the concrete technical foundation for building the training loop component, including complete starter code for infrastructure components and detailed skeleton code for core training logic.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Training Framework</td>\n<td>HuggingFace Transformers Trainer</td>\n<td>Custom PyTorch training loop with Accelerate</td>\n</tr>\n<tr>\n<td>Learning Rate Scheduling</td>\n<td>transformers.get_cosine_schedule_with_warmup</td>\n<td>Custom schedulers with complex warmup strategies</td>\n</tr>\n<tr>\n<td>Checkpoint Management</td>\n<td>torch.save/torch.load with simple intervals</td>\n<td>Advanced checkpoint management with metadata and compression</td>\n</tr>\n<tr>\n<td>Logging and Monitoring</td>\n<td>Python logging + simple file outputs</td>\n<td>Weights &amp; Biases or TensorBoard integration</td>\n</tr>\n<tr>\n<td>Memory Monitoring</td>\n<td>Basic torch.cuda.memory_summary()</td>\n<td>nvidia-ml-py for detailed GPU monitoring</td>\n</tr>\n<tr>\n<td>Early Stopping</td>\n<td>Simple validation loss patience</td>\n<td>Multi-metric early stopping with statistical tests</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>training/\n├── __init__.py\n├── loop.py                    ← TrainingLoop main class\n├── schedulers.py             ← Learning rate scheduling utilities\n├── checkpoints.py            ← Checkpoint management and metadata\n├── metrics.py                ← Loss tracking and early stopping\n├── memory.py                 ← Memory monitoring utilities\n├── configs/\n│   ├── __init__.py\n│   └── training_config.py    ← TrainingConfig and related classes\n└── tests/\n    ├── test_loop.py\n    ├── test_schedulers.py\n    └── test_checkpoints.py</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Memory Monitoring Utilities (training/memory.py)</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> psutil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MemoryMetrics</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stage: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gpu_memory_allocated: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gpu_memory_cached: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gpu_memory_reserved: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    system_memory_used: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    details: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MemoryMonitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    baseline_gpu_memory: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    baseline_system_memory: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    measurements: List[Dict] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> capture_baseline</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record initial memory state before model loading.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            torch.cuda.empty_cache()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            torch.cuda.synchronize()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.baseline_gpu_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_allocated()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.baseline_system_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.virtual_memory().used</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Take initial measurement</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        baseline_metrics </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.measure_current_usage(</span><span style=\"color:#9ECBFF\">\"baseline\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.measurements.append(baseline_metrics.</span><span style=\"color:#79B8FF\">__dict__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> measure_current_usage</span><span style=\"color:#E1E4E8\">(self, stage: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> MemoryMetrics:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Measure current memory usage and return detailed statistics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # GPU memory measurement</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            torch.cuda.synchronize()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            gpu_allocated </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_allocated() </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#6A737D\">  # Convert to GB</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            gpu_cached </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_reserved() </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            gpu_reserved </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.max_memory_reserved() </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Detailed GPU memory info</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            memory_summary </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_summary()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            details </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"memory_summary\"</span><span style=\"color:#E1E4E8\">: memory_summary,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"device_count\"</span><span style=\"color:#E1E4E8\">: torch.cuda.device_count(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"current_device\"</span><span style=\"color:#E1E4E8\">: torch.cuda.current_device()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            gpu_allocated </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> gpu_cached </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> gpu_reserved </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            details </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">\"gpu_available\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # System memory measurement</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        system_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.virtual_memory()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        system_used_gb </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> system_memory.used </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        details.update({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"system_memory_percent\"</span><span style=\"color:#E1E4E8\">: system_memory.percent,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"system_memory_available_gb\"</span><span style=\"color:#E1E4E8\">: system_memory.available </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metrics </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MemoryMetrics(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            stage</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">stage,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            gpu_memory_allocated</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">gpu_allocated,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            gpu_memory_cached</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">gpu_cached,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            gpu_memory_reserved</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">gpu_reserved,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            system_memory_used</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">system_used_gb,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">timestamp,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            details</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">details</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Store measurement</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.measurements.append(metrics.</span><span style=\"color:#79B8FF\">__dict__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_peak_usage</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return peak memory usage across all measurements.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.measurements:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        peak_gpu </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(m[</span><span style=\"color:#9ECBFF\">\"gpu_memory_allocated\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> m </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.measurements)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        peak_system </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(m[</span><span style=\"color:#9ECBFF\">\"system_memory_used\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> m </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.measurements)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"peak_gpu_memory_gb\"</span><span style=\"color:#E1E4E8\">: peak_gpu,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"peak_system_memory_gb\"</span><span style=\"color:#E1E4E8\">: peak_system,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"measurements_count\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.measurements)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> cleanup_old_measurements</span><span style=\"color:#E1E4E8\">(self, keep_last: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Remove old measurements to prevent unbounded memory growth.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.measurements) </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> keep_last:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.measurements </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.measurements[</span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\">keep_last:]</span></span></code></pre></div>\n\n<p><strong>Checkpoint Management Infrastructure (training/checkpoints.py)</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> hashlib</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> shutil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional, List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, asdict</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CheckpointMetadata</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    checkpoint_path: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    step: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    epoch: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    eval_loss: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    is_best: </span><span style=\"color:#79B8FF\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    model_config: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    adapter_config: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    optimizer_state_size: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    save_timestamp: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> to_dict</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> asdict(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> from_dict</span><span style=\"color:#E1E4E8\">(cls, data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#9ECBFF\">'CheckpointMetadata'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">data)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CheckpointManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, checkpoint_dir: Path, keep_last: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#E1E4E8\">, keep_best: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.checkpoint_dir </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Path(checkpoint_dir)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.checkpoint_dir.mkdir(</span><span style=\"color:#FFAB70\">parents</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">exist_ok</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.keep_last </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> keep_last</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.keep_best </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> keep_best</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.checkpoints: List[CheckpointMetadata] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.best_checkpoint: Optional[CheckpointMetadata] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Load existing checkpoint metadata</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._load_checkpoint_registry()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> save_checkpoint</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        self,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        model_state: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, torch.Tensor],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        optimizer_state: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        scheduler_state: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        step: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        epoch: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        eval_loss: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        model_config: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        adapter_config: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ) -> CheckpointMetadata:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Save a complete checkpoint with atomic write operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        checkpoint_name </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"checkpoint-step-</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">step</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        checkpoint_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.checkpoint_dir </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> checkpoint_name</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Create checkpoint directory</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        checkpoint_path.mkdir(</span><span style=\"color:#FFAB70\">exist_ok</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Determine if this is the best checkpoint</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        is_best </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._is_best_checkpoint(eval_loss)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Prepare checkpoint data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        checkpoint_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"model_state\"</span><span style=\"color:#E1E4E8\">: model_state,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"optimizer_state\"</span><span style=\"color:#E1E4E8\">: optimizer_state,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"scheduler_state\"</span><span style=\"color:#E1E4E8\">: scheduler_state,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"step\"</span><span style=\"color:#E1E4E8\">: step,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"epoch\"</span><span style=\"color:#E1E4E8\">: epoch,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"eval_loss\"</span><span style=\"color:#E1E4E8\">: eval_loss,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"timestamp\"</span><span style=\"color:#E1E4E8\">: timestamp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use atomic write operations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        temp_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> checkpoint_path </span><span style=\"color:#F97583\">/</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\".tmp_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">timestamp</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Save main checkpoint file</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            torch.save(checkpoint_data, temp_path </span><span style=\"color:#F97583\">/</span><span style=\"color:#9ECBFF\"> \"training_state.pt\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Save metadata</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            metadata </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CheckpointMetadata(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                checkpoint_path</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(checkpoint_path),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                step</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">step,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                epoch</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">epoch,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                eval_loss</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">eval_loss,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                is_best</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">is_best,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                model_config</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">model_config </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> {},</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                adapter_config</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">adapter_config </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> {},</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                optimizer_state_size</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._estimate_state_size(optimizer_state),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                save_timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">timestamp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(temp_path </span><span style=\"color:#F97583\">/</span><span style=\"color:#9ECBFF\"> \"metadata.json\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'w'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                json.dump(metadata.to_dict(), f, </span><span style=\"color:#FFAB70\">indent</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Atomic move from temp to final location</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> temp_file </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> temp_path.iterdir():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                final_file </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> checkpoint_path </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> temp_file.name</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                shutil.move(</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(temp_file), </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(final_file))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            temp_path.rmdir()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Cleanup on failure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> temp_path.exists():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                shutil.rmtree(temp_path)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> RuntimeError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Checkpoint save failed: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Update registry</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.checkpoints.append(metadata)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> is_best:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.best_checkpoint </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> metadata</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Cleanup old checkpoints</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._cleanup_old_checkpoints()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Save updated registry</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._save_checkpoint_registry()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> metadata</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> load_checkpoint</span><span style=\"color:#E1E4E8\">(self, checkpoint_path: Optional[Path] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load a checkpoint with validation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> checkpoint_path </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.checkpoints:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"No checkpoints available to load\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            checkpoint_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Path(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.checkpoints[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].checkpoint_path)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Load and validate metadata</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metadata_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> checkpoint_path </span><span style=\"color:#F97583\">/</span><span style=\"color:#9ECBFF\"> \"metadata.json\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> metadata_path.exists():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Checkpoint metadata not found: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">metadata_path</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(metadata_path, </span><span style=\"color:#9ECBFF\">'r'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            metadata_dict </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> json.load(f)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metadata </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CheckpointMetadata.from_dict(metadata_dict)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Load training state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        state_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> checkpoint_path </span><span style=\"color:#F97583\">/</span><span style=\"color:#9ECBFF\"> \"training_state.pt\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> state_path.exists():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Checkpoint state not found: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">state_path</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        checkpoint_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.load(state_path, </span><span style=\"color:#FFAB70\">map_location</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'cpu'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        checkpoint_data[</span><span style=\"color:#9ECBFF\">\"metadata\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> metadata</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> checkpoint_data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_best_checkpoint_path</span><span style=\"color:#E1E4E8\">(self) -> Optional[Path]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get path to the best checkpoint based on evaluation loss.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.best_checkpoint </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Path(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.best_checkpoint.checkpoint_path)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _is_best_checkpoint</span><span style=\"color:#E1E4E8\">(self, eval_loss: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Determine if this checkpoint is the best so far.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> eval_loss </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.best_checkpoint </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.best_checkpoint.eval_loss </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> eval_loss </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.best_checkpoint.eval_loss</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _estimate_state_size</span><span style=\"color:#E1E4E8\">(self, state: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Estimate the size of optimizer state in bytes.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Quick estimation based on tensor sizes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            total_elements </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> value </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> state.values():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(value, torch.Tensor):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    total_elements </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> value.numel()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                elif</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(value, </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    # Recursive estimation for nested dictionaries</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    for</span><span style=\"color:#E1E4E8\"> nested_value </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> value.values():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                        if</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(nested_value, torch.Tensor):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                            total_elements </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> nested_value.numel()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Assume float32 (4 bytes per element) for rough estimation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> total_elements </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 4</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _cleanup_old_checkpoints</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Remove old checkpoints according to retention policy.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.checkpoints) </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.keep_last:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Sort by step number</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sorted_checkpoints </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> sorted</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.checkpoints, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\"> x: x.step)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Determine which checkpoints to remove</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        checkpoints_to_remove </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sorted_checkpoints[:</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.keep_last]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> checkpoint </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> checkpoints_to_remove:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Don't remove the best checkpoint if keep_best is True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.keep_best </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> checkpoint.is_best:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                continue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Remove checkpoint directory</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            checkpoint_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Path(checkpoint.checkpoint_path)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> checkpoint_path.exists():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                shutil.rmtree(checkpoint_path)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Remove from registry</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.checkpoints.remove(checkpoint)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _save_checkpoint_registry</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Save the checkpoint registry to disk.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        registry_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.checkpoint_dir </span><span style=\"color:#F97583\">/</span><span style=\"color:#9ECBFF\"> \"checkpoint_registry.json\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        registry_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"checkpoints\"</span><span style=\"color:#E1E4E8\">: [cp.to_dict() </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> cp </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.checkpoints],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"best_checkpoint\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.best_checkpoint.to_dict() </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.best_checkpoint </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(registry_path, </span><span style=\"color:#9ECBFF\">'w'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            json.dump(registry_data, f, </span><span style=\"color:#FFAB70\">indent</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _load_checkpoint_registry</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load existing checkpoint registry from disk.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        registry_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.checkpoint_dir </span><span style=\"color:#F97583\">/</span><span style=\"color:#9ECBFF\"> \"checkpoint_registry.json\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> registry_path.exists():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(registry_path, </span><span style=\"color:#9ECBFF\">'r'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                registry_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> json.load(f)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.checkpoints </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                CheckpointMetadata.from_dict(cp_data) </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                for</span><span style=\"color:#E1E4E8\"> cp_data </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> registry_data.get(</span><span style=\"color:#9ECBFF\">\"checkpoints\"</span><span style=\"color:#E1E4E8\">, [])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            best_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> registry_data.get(</span><span style=\"color:#9ECBFF\">\"best_checkpoint\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> best_data:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.best_checkpoint </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CheckpointMetadata.from_dict(best_data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Warning: Could not load checkpoint registry: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.checkpoints </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.best_checkpoint </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span></code></pre></div>\n\n<h4 id=\"core-training-loop-skeleton\">Core Training Loop Skeleton</h4>\n<p><strong>Main Training Loop Class (training/loop.py)</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch.nn </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> nn</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> torch.utils.data </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> DataLoader</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> transformers </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> PreTrainedModel, PreTrainedTokenizer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .memory </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> MemoryMonitor, MemoryMetrics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .checkpoints </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> CheckpointManager, CheckpointMetadata</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .metrics </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> MetricsTracker, TrainingMetrics, EvaluationMetrics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .schedulers </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> CosineScheduleWithWarmup</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TrainingMetrics</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    step: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    epoch: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    loss: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    learning_rate: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    grad_norm: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gpu_memory_used: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    samples_per_second: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TrainingLoop</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        self,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        model: PreTrainedModel,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tokenizer: PreTrainedTokenizer,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        train_dataloader: DataLoader,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        eval_dataloader: DataLoader,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        optimizer: torch.optim.Optimizer,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config: </span><span style=\"color:#9ECBFF\">'TrainingConfig'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokenizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokenizer</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.train_dataloader </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> train_dataloader</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.eval_dataloader </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> eval_dataloader</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.optimizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> optimizer</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Initialize components</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.memory_monitor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MemoryMonitor()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.checkpoint_manager </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CheckpointManager(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            checkpoint_dir</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">Path(config.output_dir) </span><span style=\"color:#F97583\">/</span><span style=\"color:#9ECBFF\"> \"checkpoints\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            keep_last</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">config.save_total_limit,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            keep_best</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.metrics_tracker </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MetricsTracker()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Initialize scheduler</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        total_steps </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.calculate_total_training_steps()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        warmup_steps </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> int</span><span style=\"color:#E1E4E8\">(total_steps </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> config.warmup_ratio)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.scheduler </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CosineScheduleWithWarmup(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            optimizer</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">optimizer,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            num_warmup_steps</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">warmup_steps,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            num_training_steps</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">total_steps,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            min_lr_ratio</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">config.min_learning_rate_ratio</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Training state</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.global_step </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_epoch </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.best_eval_loss </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> float</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'inf'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.epochs_without_improvement </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Setup logging</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> train</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute the complete training loop and return path to best checkpoint.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger.info(</span><span style=\"color:#9ECBFF\">\"Starting training loop\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.memory_monitor.capture_baseline()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Set model to training mode and prepare for gradient accumulation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize training metrics tracking and logging</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Enter main epoch loop for specified number of training epochs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: For each epoch, iterate through training batches with progress tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Implement gradient accumulation loop for each effective batch</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Perform forward pass on micro-batch and scale loss appropriately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Execute backward pass and accumulate gradients without clearing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: After accumulation cycle, clip gradients and perform optimizer step</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Update learning rate scheduler and clear accumulated gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Log training metrics and monitor memory usage periodically</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 11: Run evaluation at configured intervals and update best metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 12: Save checkpoints based on evaluation results and step intervals</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 13: Check early stopping conditions based on validation performance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 14: Handle training completion and return best checkpoint path</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._execute_training_loop()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Training failed: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement proper error recovery and cleanup</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _execute_training_loop</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Core training loop implementation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Training preparation</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model.train()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Main training loop</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> epoch </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.config.num_train_epochs)):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.current_epoch </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> epoch</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            epoch_start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Epoch training loop</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            epoch_loss </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._train_epoch()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Evaluation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._should_evaluate(epoch):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                eval_metrics </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._evaluate()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Update best metrics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> eval_metrics.eval_loss </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.best_eval_loss:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">.best_eval_loss </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> eval_metrics.eval_loss</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">.epochs_without_improvement </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">.epochs_without_improvement </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Save checkpoint</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                checkpoint </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._save_checkpoint(eval_metrics.eval_loss)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Check early stopping</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._should_stop_early():</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">.logger.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Early stopping triggered after </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">epoch </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1}</span><span style=\"color:#9ECBFF\"> epochs\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    break</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            epoch_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> epoch_start_time</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.logger.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Epoch </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">epoch </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1}</span><span style=\"color:#9ECBFF\"> completed in </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">epoch_time</span><span style=\"color:#F97583\">:.2f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">s, loss: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">epoch_loss</span><span style=\"color:#F97583\">:.6f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Return path to best checkpoint</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        best_checkpoint_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.checkpoint_manager.get_best_checkpoint_path()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> best_checkpoint_path:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(best_checkpoint_path)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> RuntimeError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"No valid checkpoints were saved during training\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _train_epoch</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Train for one complete epoch with gradient accumulation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        epoch_loss </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        num_batches </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> batch_idx, batch </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.train_dataloader):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            batch_start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Move batch to appropriate device (GPU/CPU)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize gradient accumulation for this effective batch</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Split batch into micro-batches based on accumulation steps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Process each micro-batch in accumulation loop</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Forward pass: compute loss and scale by accumulation steps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Backward pass: accumulate gradients without clearing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: After all micro-batches, clip gradients if configured</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Perform optimizer step and scheduler update</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Clear accumulated gradients for next batch</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Log metrics and update progress tracking</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            step_loss </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._train_step(batch)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            epoch_loss </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> step_loss</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            num_batches </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Memory monitoring</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.global_step </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.logging_steps </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                memory_metrics </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.memory_monitor.measure_current_usage(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"step_</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.global_step</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Log training metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                training_metrics </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TrainingMetrics(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    step</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.global_step,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    epoch</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current_epoch </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> (batch_idx </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.train_dataloader)),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    loss</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">step_loss,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    learning_rate</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.scheduler.get_last_lr()[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    grad_norm</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#6A737D\"># Will be populated by gradient clipping</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.time(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    gpu_memory_used</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">memory_metrics.gpu_memory_allocated,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    samples_per_second</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.config.per_device_train_batch_size </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (time.time() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> batch_start_time)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.metrics_tracker.log_training_metrics(training_metrics)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> epoch_loss </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(num_batches, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _train_step</span><span style=\"color:#E1E4E8\">(self, batch: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, torch.Tensor]) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute one training step with gradient accumulation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Move batch tensors to model device</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize accumulation variables and clear any existing gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate number of micro-batches based on accumulation steps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Split current batch into micro-batches for accumulation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: For each micro-batch, perform forward pass and compute loss</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Scale loss by number of accumulation steps to maintain gradient magnitude</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Perform backward pass to accumulate gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: After all micro-batches, apply gradient clipping if configured</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Perform optimizer step to update model parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Update learning rate scheduler and clear gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 11: Increment global step counter and return average loss</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        total_loss </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        effective_batch_size </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.calculate_effective_batch_size()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        accumulation_steps </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.gradient_accumulation_steps</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Clear gradients from previous step</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.optimizer.zero_grad()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Gradient accumulation loop</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> acc_step </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(accumulation_steps):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Extract micro-batch</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            micro_batch </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._extract_micro_batch(batch, acc_step, accumulation_steps)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Forward pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            outputs </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.model(</span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">micro_batch)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            loss </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> outputs.loss</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Scale loss for accumulation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            scaled_loss </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> loss </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> accumulation_steps</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            total_loss </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> loss.item()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Backward pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            scaled_loss.backward()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Gradient clipping</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        grad_norm </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.max_grad_norm </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            grad_norm </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.nn.utils.clip_grad_norm_(</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.model.parameters(), </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.config.max_grad_norm</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ).item()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Optimizer step</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.optimizer.step()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.scheduler.step()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Update global step</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.global_step </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> total_loss </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> accumulation_steps</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _evaluate</span><span style=\"color:#E1E4E8\">(self) -> EvaluationMetrics:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Run evaluation on validation dataset.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Set model to evaluation mode and disable gradient computation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize evaluation metrics tracking (loss, perplexity, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Iterate through evaluation dataset without gradient accumulation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: For each batch, perform forward pass and collect predictions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Compute loss values and accumulate for final averages</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Calculate task-specific metrics (perplexity, accuracy, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Aggregate all metrics and compute evaluation statistics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Restore model to training mode before returning</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Log evaluation results and update metrics tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Return comprehensive evaluation metrics object</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model.eval()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        total_eval_loss </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        total_samples </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        eval_start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#E1E4E8\"> torch.no_grad():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> batch </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.eval_dataloader:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Move batch to device</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                batch </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {k: v.to(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.model.device) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> k, v </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> batch.items()}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Forward pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                outputs </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.model(</span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">batch)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                loss </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> outputs.loss</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                total_eval_loss </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> loss.item()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                total_samples </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> batch[</span><span style=\"color:#9ECBFF\">'input_ids'</span><span style=\"color:#E1E4E8\">].size(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Calculate final metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        avg_eval_loss </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> total_eval_loss </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.eval_dataloader)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        perplexity </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.exp(torch.tensor(avg_eval_loss)).item()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        eval_runtime </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> eval_start_time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        eval_metrics </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> EvaluationMetrics(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            eval_step</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.global_step,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            eval_loss</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">avg_eval_loss,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            perplexity</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">perplexity,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            bleu_score</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#6A737D\"># </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement task-specific metrics</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            rouge_scores</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            exact_match</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            eval_samples</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">total_samples,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            eval_runtime</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">eval_runtime,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            eval_samples_per_second</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">total_samples </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> eval_runtime,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Restore training mode</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model.train()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> eval_metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> calculate_effective_batch_size</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate the effective batch size accounting for gradient accumulation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.per_device_train_batch_size </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.gradient_accumulation_steps</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> calculate_total_training_steps</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate total number of training steps for scheduler setup.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        steps_per_epoch </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.train_dataloader) </span><span style=\"color:#F97583\">//</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.gradient_accumulation_steps</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> steps_per_epoch </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.num_train_epochs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _should_evaluate</span><span style=\"color:#E1E4E8\">(self, epoch: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Determine if evaluation should be performed at current point.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.evaluation_strategy </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"epoch\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.evaluation_strategy </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"steps\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.global_step </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.eval_steps </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _should_stop_early</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if early stopping conditions are met.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.early_stopping_patience:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.epochs_without_improvement </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.early_stopping_patience</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _save_checkpoint</span><span style=\"color:#E1E4E8\">(self, eval_loss: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> CheckpointMetadata:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Save training checkpoint with current state.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Extract model state dict (adapter weights only for LoRA)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Extract optimizer state dict with all momentum/variance buffers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Extract scheduler state dict for proper training resumption</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Prepare model and adapter configuration for checkpoint metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Call checkpoint manager to save state with atomic operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Update checkpoint registry and cleanup old checkpoints if needed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return checkpoint metadata for further processing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Get model state (only adapter weights for LoRA)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> hasattr</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.model, </span><span style=\"color:#9ECBFF\">'get_adapter_state_dict'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            model_state </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.model.get_adapter_state_dict()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            model_state </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.model.state_dict()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Save checkpoint</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        checkpoint </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.checkpoint_manager.save_checkpoint(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            model_state</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">model_state,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            optimizer_state</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.optimizer.state_dict(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            scheduler_state</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.scheduler.state_dict(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            step</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.global_step,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            epoch</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current_epoch,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            eval_loss</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">eval_loss,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            model_config</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.model.config.to_dict() </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> hasattr</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.model.config, </span><span style=\"color:#9ECBFF\">'to_dict'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {},</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            adapter_config</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">getattr</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.config, </span><span style=\"color:#9ECBFF\">'lora'</span><span style=\"color:#E1E4E8\">, {})</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> checkpoint</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing the training loop component, verify the following behavior:</p>\n<p><strong>Basic Functionality Test:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> training/tests/test_loop.py::test_training_loop_initialization</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> training/tests/test_loop.py::test_gradient_accumulation</span></span></code></pre></div>\n\n<p><strong>Expected Output:</strong></p>\n<ul>\n<li>Training loop initializes without errors</li>\n<li>Gradient accumulation produces mathematically equivalent results to larger batches</li>\n<li>Memory monitoring captures baseline and training memory usage</li>\n<li>Checkpoint manager saves and loads training state correctly</li>\n</ul>\n<p><strong>Manual Verification Steps:</strong></p>\n<ol>\n<li><p><strong>Start a short training run</strong> (1 epoch, small dataset) and verify that:</p>\n<ul>\n<li>Training loss decreases over steps</li>\n<li>Learning rate follows the expected schedule (warmup then decay)</li>\n<li>GPU memory usage stabilizes after initial allocation</li>\n<li>Checkpoints are saved at the expected intervals</li>\n</ul>\n</li>\n<li><p><strong>Test checkpoint resumption</strong> by:</p>\n<ul>\n<li>Stopping training mid-epoch</li>\n<li>Resuming from the latest checkpoint</li>\n<li>Verifying that training continues from the correct step and loss value</li>\n</ul>\n</li>\n<li><p><strong>Verify gradient accumulation</strong> by:</p>\n<ul>\n<li>Training with batch_size=4, accumulation_steps=1</li>\n<li>Training with batch_size=2, accumulation_steps=2</li>\n<li>Confirming that final model weights are nearly identical</li>\n</ul>\n</li>\n</ol>\n<p><strong>Warning Signs:</strong></p>\n<ul>\n<li>Memory usage increasing throughout training (indicates memory leaks)</li>\n<li>Loss values becoming NaN or infinity (gradient explosion)</li>\n<li>Training loss not decreasing after several steps (learning rate too low)</li>\n<li>Checkpoint loading errors (state dict mismatches)</li>\n</ul>\n<h2 id=\"evaluation-and-merging-component\">Evaluation and Merging Component</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 5 - this section implements the evaluation system that measures fine-tuned model performance against baseline metrics, executes LoRA adapter merging for deployment, and exports models to inference-optimized formats</p>\n</blockquote>\n<p>After successfully training a model with LoRA adapters, the evaluation and merging component serves as the final checkpoint that validates whether the fine-tuning process achieved its intended objectives. This component measures model performance against both language modeling benchmarks and task-specific metrics, then seamlessly transforms the trained adapters into deployable model formats. The evaluation process provides quantitative evidence of learning progress while the merging functionality consolidates the distributed adapter weights back into a unified model suitable for production inference.</p>\n<h3 id=\"mental-model-the-exam-proctor\">Mental Model: The Exam Proctor</h3>\n<p>Think of the evaluation and merging component as a meticulous exam proctor overseeing a comprehensive graduation assessment. Just as a proctor administers standardized tests to measure student learning objectively, this component runs systematic evaluations to quantify how much the model has learned during fine-tuning. The proctor doesn&#39;t just check if answers are correct—they measure improvement over baseline performance, ensure the assessment covers relevant skills, and verify that learning transferred appropriately to new scenarios.</p>\n<p>When the examination concludes successfully, the proctor certifies the student&#39;s readiness for real-world application. Similarly, once evaluation confirms the fine-tuned model meets quality thresholds, the merging process consolidates the learned adaptations into a unified credential—the merged model—that can operate independently in production environments. The proctor&#39;s final responsibility involves preparing official transcripts in various formats that different institutions can accept, just as model export creates deployment-ready formats optimized for different inference engines.</p>\n<p>The evaluation proctor maintains strict standards throughout the assessment process. They verify that test conditions match real-world scenarios, prevent data leakage between training and evaluation sets, and ensure consistent measurement protocols across different runs. This objectivity prevents the common pitfall of models that appear impressive on training data but fail to generalize to practical applications.</p>\n<h3 id=\"perplexity-and-language-modeling-metrics\">Perplexity and Language Modeling Metrics</h3>\n<p>Perplexity serves as the fundamental language modeling metric that measures how well the fine-tuned model predicts validation text sequences. Conceptually, perplexity quantifies the model&#39;s &quot;surprise&quot; when encountering validation tokens—lower perplexity indicates the model assigns higher probability mass to the actual next tokens, suggesting better language understanding. The mathematical relationship between cross-entropy loss and perplexity provides a interpretable measure where perplexity of 1.0 represents perfect prediction while higher values indicate increasing uncertainty.</p>\n<p>The evaluation system computes perplexity by running inference on the held-out validation set using the same tokenization and attention masking strategies employed during training. This consistency ensures that perplexity measurements reflect genuine model improvements rather than preprocessing artifacts. The system calculates perplexity separately for prompt tokens and response tokens to distinguish between the model&#39;s comprehension of instructions versus its generation capabilities.</p>\n<p>Beyond basic perplexity calculation, the system tracks perplexity trends across different validation data categories. For instruction-tuning datasets, this means measuring performance separately on different instruction types, response lengths, and domain categories. These granular measurements reveal whether fine-tuning improved performance uniformly or created specialized strengths in particular areas.</p>\n<table>\n<thead>\n<tr>\n<th>Perplexity Metric</th>\n<th>Calculation Method</th>\n<th>Interpretation Range</th>\n<th>Quality Threshold</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Overall Perplexity</strong></td>\n<td>exp(mean cross-entropy loss)</td>\n<td>1.0 (perfect) to ∞ (random)</td>\n<td>&lt;20 for good instruction following</td>\n</tr>\n<tr>\n<td><strong>Prompt Perplexity</strong></td>\n<td>exp(loss on instruction tokens only)</td>\n<td>1.0 to ∞</td>\n<td>Should remain stable vs base model</td>\n</tr>\n<tr>\n<td><strong>Response Perplexity</strong></td>\n<td>exp(loss on response tokens only)</td>\n<td>1.0 to ∞</td>\n<td>Primary target for improvement</td>\n</tr>\n<tr>\n<td><strong>Length-Conditional Perplexity</strong></td>\n<td>exp(loss grouped by response length)</td>\n<td>1.0 to ∞</td>\n<td>Consistent across length buckets</td>\n</tr>\n<tr>\n<td><strong>Domain-Specific Perplexity</strong></td>\n<td>exp(loss grouped by instruction category)</td>\n<td>1.0 to ∞</td>\n<td>Balanced improvement across domains</td>\n</tr>\n</tbody></table>\n<p>The system implements perplexity calculation through careful attention mask handling that excludes padding tokens from loss computation while properly accounting for causal attention patterns. Token-level loss values are accumulated across the validation set using numerically stable log-sum-exp operations to prevent floating-point overflow when processing long sequences.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: Perplexity alone can be misleading because a model might achieve low perplexity by memorizing training patterns without developing genuine understanding. The evaluation system therefore complements perplexity with task-specific metrics that measure functional capabilities rather than just statistical fit.</p>\n</blockquote>\n<p>Perplexity comparison against the base model baseline establishes whether fine-tuning provided meaningful improvements. The system loads both the fine-tuned model and original base model to run identical evaluation protocols, generating side-by-side perplexity measurements that quantify the learning delta. Significant perplexity reduction on validation data while maintaining reasonable performance on held-out general language modeling benchmarks indicates successful adaptation without catastrophic forgetting.</p>\n<p>The evaluation system also implements perplexity-based early stopping validation by tracking validation perplexity trends throughout training. When validation perplexity begins increasing while training loss continues decreasing, this signals overfitting that requires training termination or checkpoint rollback to the best-performing state.</p>\n<h3 id=\"task-specific-evaluation\">Task-Specific Evaluation</h3>\n<p>While perplexity measures general language modeling capability, task-specific evaluation assesses whether the fine-tuned model actually performs better on the intended downstream applications. This evaluation dimension runs domain-relevant benchmarks that test functional capabilities like instruction following, factual accuracy, reasoning consistency, and response appropriateness. Unlike perplexity&#39;s focus on token prediction probability, task-specific metrics evaluate the semantic quality and practical utility of generated responses.</p>\n<p>The evaluation system supports multiple task-specific measurement protocols depending on the fine-tuning objective. For instruction-tuning applications, this includes exact match scoring for factual questions, BLEU and ROUGE metrics for text generation quality, and semantic similarity measurements using embedding-based comparisons. Each metric captures different aspects of model performance that collectively provide comprehensive quality assessment.</p>\n<table>\n<thead>\n<tr>\n<th>Task-Specific Metric</th>\n<th>Measurement Method</th>\n<th>Score Range</th>\n<th>Typical Improvement Target</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Exact Match Accuracy</strong></td>\n<td>Percentage of responses matching reference exactly</td>\n<td>0-100%</td>\n<td>+10-20% over base model</td>\n</tr>\n<tr>\n<td><strong>BLEU Score</strong></td>\n<td>N-gram overlap with reference responses</td>\n<td>0-100</td>\n<td>+5-15 points over baseline</td>\n</tr>\n<tr>\n<td><strong>ROUGE-L F1</strong></td>\n<td>Longest common subsequence with references</td>\n<td>0-1.0</td>\n<td>+0.05-0.15 over baseline</td>\n</tr>\n<tr>\n<td><strong>Semantic Similarity</strong></td>\n<td>Cosine similarity of response embeddings</td>\n<td>-1.0-1.0</td>\n<td>+0.1-0.3 over base model</td>\n</tr>\n<tr>\n<td><strong>Instruction Following Rate</strong></td>\n<td>Percentage following instruction format/constraints</td>\n<td>0-100%</td>\n<td>+20-40% over base model</td>\n</tr>\n<tr>\n<td><strong>Factual Consistency</strong></td>\n<td>Percentage of factually accurate statements</td>\n<td>0-100%</td>\n<td>Maintain base model level</td>\n</tr>\n</tbody></table>\n<p>The benchmark evaluation process involves generating responses from both the fine-tuned model and base model using identical prompts and generation parameters. This controlled comparison isolates the impact of fine-tuning by removing confounding variables like different sampling strategies or temperature settings. The system generates multiple responses per prompt when evaluating stochastic metrics, then reports both mean performance and confidence intervals.</p>\n<p>For instruction-following evaluation, the system implements specialized scoring functions that assess whether generated responses adhere to explicit constraints in the instruction prompts. This includes checking output format requirements, length constraints, style specifications, and content guidelines. Instruction-following metrics often provide more actionable feedback than perplexity because they directly measure the behaviors that fine-tuning aimed to improve.</p>\n<p>The evaluation system integrates with external benchmark suites like HellaSwag, MMLU, or domain-specific evaluation datasets relevant to the fine-tuning task. This integration involves formatting the fine-tuned model&#39;s responses according to each benchmark&#39;s expected input/output schema and computing official benchmark scores for comparison with published baselines.</p>\n<blockquote>\n<p><strong>Design Principle</strong>: Task-specific evaluation must use held-out data that was never seen during training or validation to provide genuine generalization assessment. The system enforces strict data isolation by checking for overlap between training samples and evaluation prompts.</p>\n</blockquote>\n<p>Beyond aggregate scores, the evaluation system performs detailed error analysis by categorizing failure modes and identifying systematic weaknesses in the fine-tuned model. This analysis examines which types of instructions the model handles well versus poorly, whether errors stem from factual knowledge gaps or reasoning failures, and how response quality varies across different domains or complexity levels.</p>\n<p>The comparative evaluation framework generates comprehensive reports showing before-and-after performance across all measured dimensions. These reports include statistical significance testing to determine whether observed improvements are meaningful rather than random variation. The system flags cases where fine-tuning improved some metrics while degrading others, highlighting potential trade-offs in model capabilities.</p>\n<h3 id=\"lora-adapter-merging\">LoRA Adapter Merging</h3>\n<p>Once evaluation confirms that fine-tuning achieved satisfactory performance improvements, the adapter merging process consolidates the learned LoRA parameters back into the base model weights. This merging operation transforms the distributed parameter structure—where base model weights remain frozen and adaptation occurs through separate low-rank matrices—into a unified parameter set that contains all learned adaptations directly in the model weights.</p>\n<p>The mathematical foundation of LoRA adapter merging involves reconstructing the full-rank weight updates from the low-rank decomposition, then adding these updates to the corresponding base model parameters. For each target module with LoRA adapters, the system computes the full adaptation matrix by multiplying the down-projection matrix A with the up-projection matrix B, scales this result by the alpha parameter, then adds it to the original frozen weights.</p>\n<table>\n<thead>\n<tr>\n<th>Merging Operation</th>\n<th>Mathematical Formula</th>\n<th>Memory Requirement</th>\n<th>Computational Cost</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Adapter Weight Reconstruction</strong></td>\n<td>ΔW = α * B @ A</td>\n<td>rank × (input_dim + output_dim)</td>\n<td>O(rank × input_dim × output_dim)</td>\n</tr>\n<tr>\n<td><strong>Base Weight Update</strong></td>\n<td>W_new = W_base + ΔW</td>\n<td>full parameter size</td>\n<td>O(input_dim × output_dim)</td>\n</tr>\n<tr>\n<td><strong>Bias Integration</strong></td>\n<td>b_new = b_base + Δb (if enabled)</td>\n<td>output_dim</td>\n<td>O(output_dim)</td>\n</tr>\n<tr>\n<td><strong>Layer Normalization Merge</strong></td>\n<td>ln_new = ln_base + Δln (if targeted)</td>\n<td>hidden_dim</td>\n<td>O(hidden_dim)</td>\n</tr>\n</tbody></table>\n<p>The merging process requires careful attention to parameter dtypes and device placement to avoid numerical precision loss or memory overflow. The system performs merging operations in float32 precision even when the base model uses lower precision formats, then converts the merged weights back to the target precision after integration. This approach prevents accumulation of quantization errors during the weight combination process.</p>\n<p>Adapter merging operates module-by-module to manage memory usage efficiently. Rather than loading all parameters simultaneously, the system iterates through target modules, loads the relevant LoRA matrices and base weights, performs the merging computation, stores the updated weights, then releases intermediate tensors before proceeding to the next module. This streaming approach enables merging of models that exceed available GPU memory.</p>\n<p>The system implements verification procedures that confirm merged model behavior matches the original LoRA-adapted model within numerical precision tolerances. This verification involves running identical forward passes through both the unmerged LoRA model and merged model, comparing the output activations layer-by-layer to detect any discrepancies that might indicate merging errors.</p>\n<blockquote>\n<p><strong>Critical Implementation Detail</strong>: The merging process must account for different LoRA configurations across target modules. Some modules might use different ranks, alpha scaling factors, or dropout rates, requiring per-module parameter handling rather than uniform merging operations.</p>\n</blockquote>\n<p>After successful merging, the system performs cleanup operations that remove all LoRA-specific parameters, metadata, and configuration artifacts from the model state. The resulting merged model contains only standard transformer parameters and can be used with any inference framework that supports the base model architecture, eliminating dependencies on PEFT or LoRA-specific libraries.</p>\n<p>The merging component also supports partial merging operations where only a subset of adapters are integrated into the base model. This capability enables experimentation with different adapter combinations or selective integration of adapters that showed positive evaluation results while excluding those that degraded performance on certain metrics.</p>\n<h3 id=\"model-export-and-deployment\">Model Export and Deployment</h3>\n<p>The model export functionality transforms the merged model into deployment-ready formats optimized for different inference environments and hardware platforms. This transformation process involves converting between different serialization formats, applying additional quantization for inference efficiency, and generating auxiliary files required for model loading and tokenization in deployment systems.</p>\n<p>The primary export target is the HuggingFace format that maintains full compatibility with the transformers library and associated inference tools. This export involves saving the merged model weights using the <code>save_pretrained</code> method, preserving the model configuration, tokenizer settings, and generation parameters in the standard directory structure expected by HuggingFace pipelines.</p>\n<p>For deployment scenarios requiring maximum inference efficiency, the system supports export to GGUF format compatible with llama.cpp and similar optimized inference engines. GGUF export involves additional quantization steps that convert float16 or float32 weights to 4-bit or 8-bit integer formats using quantization schemes optimized for fast CPU or mobile inference.</p>\n<table>\n<thead>\n<tr>\n<th>Export Format</th>\n<th>Target Runtime</th>\n<th>Quantization Options</th>\n<th>Inference Speed</th>\n<th>Model Size Reduction</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>HuggingFace</strong></td>\n<td>transformers, vLLM, text-generation-inference</td>\n<td>fp16, bf16, int8</td>\n<td>Baseline</td>\n<td>50% (fp16)</td>\n</tr>\n<tr>\n<td><strong>GGUF Q4_0</strong></td>\n<td>llama.cpp, ollama</td>\n<td>4-bit symmetric</td>\n<td>2-3x faster</td>\n<td>75% reduction</td>\n</tr>\n<tr>\n<td><strong>GGUF Q8_0</strong></td>\n<td>llama.cpp CPU inference</td>\n<td>8-bit symmetric</td>\n<td>1.5-2x faster</td>\n<td>50% reduction</td>\n</tr>\n<tr>\n<td><strong>ONNX</strong></td>\n<td>ONNXRuntime, server deployments</td>\n<td>fp16, int8</td>\n<td>Variable</td>\n<td>50-75% reduction</td>\n</tr>\n<tr>\n<td><strong>TensorRT</strong></td>\n<td>NVIDIA inference servers</td>\n<td>fp16, int8, int4</td>\n<td>3-5x faster</td>\n<td>50-85% reduction</td>\n</tr>\n</tbody></table>\n<p>The GGUF export process requires integration with external conversion utilities that handle the complex format transformation and quantization procedures. The system manages this integration by preparing the merged model in the expected input format, invoking the conversion tools with appropriate parameters, then validating the resulting GGUF file through test inference runs.</p>\n<p>Quality validation during export ensures that format conversion and additional quantization do not degrade model performance beyond acceptable thresholds. The system runs comparative evaluations between the original merged model and each exported format, measuring perplexity and task-specific metrics to quantify quality preservation across different export targets.</p>\n<p>The export system generates deployment packages that include not only the converted model weights but also all auxiliary files required for inference deployment. This includes tokenizer configurations, generation parameter presets, model metadata, and example usage scripts tailored to the target deployment environment.</p>\n<blockquote>\n<p><strong>Deployment Consideration</strong>: Different export formats involve trade-offs between inference speed, memory usage, and model quality. The system provides guidance on format selection based on deployment constraints and performance requirements.</p>\n</blockquote>\n<p>For production deployment scenarios, the export process includes security scanning that checks for potential vulnerabilities in the model weights or metadata. This scanning identifies suspicious patterns that might indicate backdoors, data leakage, or other security concerns that require attention before deployment.</p>\n<p>The system maintains export provenance by generating detailed metadata that tracks the complete lineage from original base model through fine-tuning to final export. This provenance information includes training configuration, evaluation results, merging parameters, and export settings, enabling reproducible deployments and audit trails for model governance.</p>\n<h3 id=\"architecture-decision-records\">Architecture Decision Records</h3>\n<p>The evaluation and merging component involves several critical design decisions that significantly impact both the accuracy of performance measurement and the efficiency of model deployment. These decisions balance evaluation completeness against computational cost while ensuring that merged models maintain the quality improvements achieved during fine-tuning.</p>\n<blockquote>\n<p><strong>Decision: Evaluation Frequency During Training</strong></p>\n<ul>\n<li><strong>Context</strong>: Evaluation can be performed after every epoch, at fixed step intervals, or only at the end of training, with trade-offs between training speed and monitoring granularity</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Every epoch evaluation for complete monitoring</li>\n<li>Fixed step interval evaluation (e.g., every 500 steps)</li>\n<li>End-of-training evaluation only</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Configurable evaluation frequency with default every 0.25 epochs for instruction tuning tasks</li>\n<li><strong>Rationale</strong>: Instruction tuning often shows rapid initial improvement followed by slower convergence, requiring frequent early monitoring to detect overfitting while allowing longer intervals during stable training phases</li>\n<li><strong>Consequences</strong>: Enables early stopping and optimal checkpoint selection at the cost of 10-15% training time overhead for evaluation runs</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Training Speed Impact</th>\n<th>Overfitting Detection</th>\n<th>Early Stopping Effectiveness</th>\n<th>Resource Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Every Epoch</strong></td>\n<td>-20% slower</td>\n<td>Excellent</td>\n<td>Optimal</td>\n<td>High GPU/memory</td>\n</tr>\n<tr>\n<td><strong>Every 0.25 Epochs</strong></td>\n<td>-10% slower</td>\n<td>Good</td>\n<td>Effective</td>\n<td>Moderate</td>\n</tr>\n<tr>\n<td><strong>End Only</strong></td>\n<td>No impact</td>\n<td>None</td>\n<td>Not available</td>\n<td>Minimal</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Perplexity Calculation Methodology</strong></p>\n<ul>\n<li><strong>Context</strong>: Perplexity can be calculated across all tokens, excluding instruction tokens, or with different weighting schemes for prompts versus responses</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Standard perplexity across all non-padding tokens</li>\n<li>Response-only perplexity excluding instruction tokens</li>\n<li>Weighted perplexity with higher emphasis on response quality</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Separate calculation of prompt perplexity, response perplexity, and overall perplexity with primary optimization target on response perplexity</li>\n<li><strong>Rationale</strong>: Instruction tuning should maintain instruction comprehension (stable prompt perplexity) while improving response generation (decreasing response perplexity), requiring separate measurement</li>\n<li><strong>Consequences</strong>: Provides granular insight into model improvements at the cost of additional computation and storage for multiple perplexity metrics</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Adapter Merging Memory Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Merging adapters can be performed in-memory for speed or with checkpoint streaming for memory efficiency, particularly important for large models</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Full in-memory merging for maximum speed</li>\n<li>Checkpoint-based streaming merging for memory efficiency  </li>\n<li>Hybrid approach with in-memory merging for smaller modules</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Module-wise streaming merging with configurable batch size for simultaneous module processing</li>\n<li><strong>Rationale</strong>: Large language models often exceed GPU memory even without gradients and optimizer states, requiring streaming approaches that balance memory usage with reasonable merging speed</li>\n<li><strong>Consequences</strong>: Enables merging of arbitrarily large models at the cost of increased merging time and implementation complexity</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Strategy</th>\n<th>Memory Usage</th>\n<th>Merging Speed</th>\n<th>Model Size Limit</th>\n<th>Implementation Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Full In-Memory</strong></td>\n<td>Very High</td>\n<td>Fastest</td>\n<td>GPU memory limit</td>\n<td>Simple</td>\n</tr>\n<tr>\n<td><strong>Module Streaming</strong></td>\n<td>Low</td>\n<td>Moderate</td>\n<td>Unlimited</td>\n<td>Moderate</td>\n</tr>\n<tr>\n<td><strong>Hybrid Batching</strong></td>\n<td>Configurable</td>\n<td>Fast</td>\n<td>Flexible</td>\n<td>Complex</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Export Format Priority and Quality Validation</strong></p>\n<ul>\n<li><strong>Context</strong>: Different deployment environments require different export formats, each with specific quantization and optimization trade-offs</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>HuggingFace format only for maximum compatibility</li>\n<li>GGUF format priority for inference efficiency</li>\n<li>Multiple format support with quality validation</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Multi-format export with HuggingFace as primary and GGUF as secondary, including automated quality validation for each format</li>\n<li><strong>Rationale</strong>: Production deployments require flexibility between research compatibility (HuggingFace) and inference efficiency (GGUF), while quality validation prevents deployment of degraded models</li>\n<li><strong>Consequences</strong>: Comprehensive deployment support at the cost of longer export times and increased storage requirements for multiple format versions</li>\n</ul>\n</blockquote>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>The evaluation and merging component presents several common failure modes that can invalidate performance measurements or corrupt model deployments. Understanding these pitfalls helps developers implement robust evaluation procedures and avoid deployment disasters.</p>\n<p>⚠️ <strong>Pitfall: Evaluation Data Leakage</strong>\nMany developers accidentally include training samples in evaluation datasets or use evaluation data that shares significant overlap with training content. This contamination leads to artificially inflated performance metrics that don&#39;t reflect genuine generalization capability. The symptom appears as unrealistically high evaluation scores that don&#39;t match real-world performance after deployment. To prevent this issue, implement strict data provenance tracking that verifies evaluation samples were never seen during training, and use content-based deduplication to identify near-duplicates that might bias evaluation results.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Evaluation Protocols</strong>\nComparing base model and fine-tuned model performance using different generation parameters, tokenization settings, or prompt formats produces meaningless evaluation results. This often happens when developers use different inference scripts or model loading procedures for baseline versus fine-tuned evaluation. The fix requires implementing unified evaluation pipelines that apply identical generation settings, prompt templates, and post-processing steps to both models being compared.</p>\n<p>⚠️ <strong>Pitfall: Numerical Precision Loss During Merging</strong>\nAdapter merging operations can introduce numerical errors when performed in reduced precision or with incorrect dtype handling. This problem manifests as subtle performance degradation in the merged model compared to the unmerged LoRA version, particularly noticeable in tasks requiring precise numerical reasoning. The solution involves performing all merging computations in float32 precision regardless of storage format, then converting to target precision only after weight integration is complete.</p>\n<p>⚠️ <strong>Pitfall: Missing Validation of Merged Model Equivalence</strong>\nDevelopers often assume that merged models behave identically to their unmerged LoRA counterparts without verification, leading to deployment of models with unexpected behavior changes. This issue can result from incorrect merging implementations, precision issues, or configuration mismatches. The prevention strategy requires implementing automated verification that runs identical prompts through both unmerged and merged models, comparing outputs for numerical equivalence within expected tolerance bounds.</p>\n<p>⚠️ <strong>Pitfall: Export Format Compatibility Assumptions</strong>\nConverting models between formats (HuggingFace to GGUF, for example) without validating compatibility can produce models that fail to load or generate incorrect outputs in the target inference environment. This often occurs because different formats have varying support for model architectures, attention mechanisms, or tokenization schemes. The solution involves implementing comprehensive compatibility testing that loads exported models in their target runtime environments and validates inference behavior before deployment approval.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Task-Specific Benchmark Coverage</strong>\nFocusing evaluation exclusively on perplexity or a narrow set of task-specific metrics can miss important capability degradations or improvements in areas not covered by the evaluation suite. This limitation becomes apparent when deployed models perform poorly on user tasks that weren&#39;t represented in the evaluation benchmarks. The mitigation approach involves implementing comprehensive evaluation suites that cover diverse instruction types, reasoning patterns, and domain knowledge areas relevant to the intended deployment scenarios.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Base Model Comparison Context</strong>\nEvaluating fine-tuned models in isolation without comparing against base model performance makes it impossible to distinguish actual improvements from measurement noise or evaluation artifacts. This problem leads to overconfidence in fine-tuning effectiveness when apparent improvements might be within normal variance bounds. The correction requires implementing side-by-side evaluation protocols that measure both models using identical procedures and report improvement deltas with statistical significance assessments.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The evaluation and merging component requires careful integration of model loading, metric computation, adapter manipulation, and format conversion capabilities. The implementation balances evaluation completeness with computational efficiency while ensuring reliable model deployment preparation.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Perplexity Calculation</strong></td>\n<td>Manual cross-entropy computation with PyTorch</td>\n<td>HuggingFace evaluate library with custom metrics</td>\n</tr>\n<tr>\n<td><strong>Task-Specific Metrics</strong></td>\n<td>Basic BLEU/ROUGE with nltk/rouge-score</td>\n<td>Comprehensive evaluation with evaluate + custom benchmarks</td>\n</tr>\n<tr>\n<td><strong>Adapter Merging</strong></td>\n<td>Manual weight combination with PyTorch operations</td>\n<td>PEFT library merge_and_unload with verification</td>\n</tr>\n<tr>\n<td><strong>Model Export</strong></td>\n<td>HuggingFace save_pretrained for basic export</td>\n<td>llama.cpp integration + multiple format support</td>\n</tr>\n<tr>\n<td><strong>Benchmark Integration</strong></td>\n<td>Custom evaluation scripts with manual scoring</td>\n<td>Integration with lm-evaluation-harness framework</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<p>The evaluation and merging component integrates with the overall pipeline structure while maintaining clear separation of evaluation, merging, and export responsibilities:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>src/\n  evaluation/\n    __init__.py                    ← evaluation component exports\n    metrics/\n      __init__.py\n      perplexity.py               ← perplexity calculation utilities\n      task_specific.py            ← BLEU, ROUGE, exact match metrics\n      benchmark_runner.py         ← external benchmark integration\n    evaluator.py                  ← main evaluation orchestration\n    comparator.py                 ← base vs fine-tuned comparison\n  merging/\n    __init__.py\n    adapter_merger.py             ← LoRA weight merging operations\n    model_validator.py            ← merged model verification\n    export_manager.py             ← multi-format model export\n  utils/\n    model_loader.py               ← unified model loading utilities\n    format_converter.py           ← format conversion helpers\ntests/\n  test_evaluation.py              ← evaluation pipeline tests\n  test_merging.py                 ← adapter merging tests</code></pre></div>\n\n<h4 id=\"core-evaluation-infrastructure\">Core Evaluation Infrastructure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Evaluation infrastructure for measuring fine-tuned model performance.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Provides perplexity calculation, task-specific metrics, and baseline comparison.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch.nn.functional </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> F</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Tuple, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> transformers </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> PreTrainedModel, PreTrainedTokenizer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..data_model </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> EvaluationMetrics, InstructionSample, TokenizedSample</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PerplexityResult</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Results from perplexity calculation with detailed breakdowns.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    overall_perplexity: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    prompt_perplexity: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    response_perplexity: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    token_count: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    prompt_token_count: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    response_token_count: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    loss_values: List[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PerplexityCalculator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Calculates perplexity metrics for language model evaluation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Supports separate calculation for prompt and response tokens.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, model: PreTrainedModel, tokenizer: PreTrainedTokenizer):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokenizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokenizer</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model.eval()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> calculate_perplexity</span><span style=\"color:#E1E4E8\">(self, samples: List[TokenizedSample]) -> PerplexityResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Calculate perplexity across a dataset of tokenized samples.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            samples: List of tokenized instruction-response pairs</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            PerplexityResult with overall and component perplexities</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize loss accumulators for overall, prompt, and response</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize token counters for each component</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Set model to evaluation mode and disable gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Iterate through samples in batches for memory efficiency</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: For each batch, compute forward pass and extract logits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Calculate cross-entropy loss for prompt tokens (labels = input_ids)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Calculate cross-entropy loss for response tokens only  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Accumulate losses and token counts with proper masking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Convert accumulated losses to perplexity using exp()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Return PerplexityResult with all computed metrics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _compute_batch_loss</span><span style=\"color:#E1E4E8\">(self, batch: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, torch.Tensor]) -> Tuple[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Compute loss for a single batch with prompt/response separation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Tuple of (prompt_loss, response_loss, prompt_tokens, response_tokens)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Move batch tensors to model device</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Run forward pass to get logits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Shift logits and labels for causal language modeling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Create masks for prompt vs response tokens</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Calculate cross-entropy loss for each component</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return losses and token counts</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TaskSpecificEvaluator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Evaluates model performance on task-specific metrics like BLEU, ROUGE, exact match.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Compares fine-tuned model against baseline model performance.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, model: PreTrainedModel, tokenizer: PreTrainedTokenizer,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 baseline_model: Optional[PreTrainedModel] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokenizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokenizer</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.baseline_model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> baseline_model</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> evaluate_instruction_following</span><span style=\"color:#E1E4E8\">(self, samples: List[InstructionSample], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                     max_new_tokens: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 512</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Evaluate instruction following capability using multiple metrics.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            samples: Evaluation samples with reference responses</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            max_new_tokens: Maximum tokens to generate per response</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Dictionary of metric name to score mappings</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate responses from fine-tuned model for all samples</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Generate baseline responses if baseline model available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate exact match accuracy against references</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Calculate BLEU scores using n-gram overlap</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Calculate ROUGE-L scores using longest common subsequence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Calculate semantic similarity using embedding comparisons</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Assess instruction format compliance and constraint following</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Compile results into comprehensive metrics dictionary</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _generate_response</span><span style=\"color:#E1E4E8\">(self, instruction: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, model: PreTrainedModel) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate a single response using specified model and generation parameters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Apply chat template to format instruction</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Tokenize formatted prompt</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Generate response with temperature=0.7, top_p=0.9</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Decode generated tokens and extract response portion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return cleaned response text</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ModelComparator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Compares fine-tuned model performance against baseline model.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Provides statistical significance testing and improvement quantification.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, fine_tuned_model: PreTrainedModel, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 baseline_model: PreTrainedModel, tokenizer: PreTrainedTokenizer):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.fine_tuned_model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> fine_tuned_model</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.baseline_model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> baseline_model</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokenizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokenizer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> run_comparative_evaluation</span><span style=\"color:#E1E4E8\">(self, eval_samples: List[InstructionSample]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Run comprehensive comparison between fine-tuned and baseline models.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Detailed comparison results with improvement measurements</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Calculate perplexity for both models using identical samples</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Run task-specific evaluation for both models</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Generate side-by-side response comparisons</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Calculate improvement deltas for each metric</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Perform statistical significance testing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Generate detailed comparison report</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"lora-adapter-merging-infrastructure\">LoRA Adapter Merging Infrastructure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">LoRA adapter merging functionality for consolidating fine-tuned parameters.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Handles weight combination, verification, and cleanup operations.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Optional, List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> transformers </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> PreTrainedModel</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> peft </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> PeftModel, get_peft_model_state_dict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AdapterMerger</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Handles merging of LoRA adapters back into base model weights.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Provides verification and cleanup functionality.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, peft_model: PeftModel):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.peft_model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> peft_model</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.base_model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> peft_model.get_base_model()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> merge_adapters</span><span style=\"color:#E1E4E8\">(self, verification: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">) -> PreTrainedModel:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Merge LoRA adapter weights into base model parameters.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            verification: Whether to verify merged model equivalence</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Standalone model with merged weights</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Extract adapter state dict from PEFT model</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Identify all target modules with LoRA adapters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: For each target module, compute full-rank weight update</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Add computed updates to base model parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Handle different adapter configurations per module</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Clean up PEFT-specific parameters and metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: If verification enabled, run equivalence testing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Return standalone merged model</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _merge_module_weights</span><span style=\"color:#E1E4E8\">(self, module_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, base_weight: torch.Tensor,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                            lora_A: torch.Tensor, lora_B: torch.Tensor, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                            alpha: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, scaling: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Merge LoRA matrices into base weight for a single module.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Updated weight tensor with LoRA adaptations integrated</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Ensure all tensors are in float32 for numerical stability</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Compute full-rank update: delta_W = alpha * (B @ A) * scaling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Add delta_W to base_weight</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Convert result back to original dtype if needed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return merged weight tensor</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> verify_merger_equivalence</span><span style=\"color:#E1E4E8\">(self, merged_model: PreTrainedModel, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                test_inputs: List[torch.Tensor]) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Verify that merged model produces equivalent outputs to original PEFT model.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            merged_model: Model with merged adapters</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            test_inputs: List of test input tensors for verification</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            True if outputs are numerically equivalent within tolerance</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Set both models to evaluation mode</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Disable gradients for inference</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Run identical forward passes through both models</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Compare output tensors with numerical tolerance (1e-5)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Check intermediate activations if full verification needed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return True if all outputs match within tolerance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ModelExporter</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Exports merged models to various deployment formats.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Supports HuggingFace, GGUF, and other inference-optimized formats.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, model: PreTrainedModel, tokenizer: PreTrainedTokenizer):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokenizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokenizer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> export_huggingface</span><span style=\"color:#E1E4E8\">(self, output_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, push_to_hub: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Export model in HuggingFace format for transformers compatibility.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            output_path: Local directory path for model export</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            push_to_hub: Whether to upload to HuggingFace Hub</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Path to exported model directory</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create output directory structure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Save model weights using save_pretrained</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Save tokenizer configuration and vocabulary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Generate model card with training details</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Save generation configuration and parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: If push_to_hub enabled, upload to repository</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return path to exported model</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> export_gguf</span><span style=\"color:#E1E4E8\">(self, output_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, quantization: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"q4_0\"</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Export model in GGUF format for llama.cpp inference.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            output_path: Path for GGUF model file</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            quantization: Quantization type (q4_0, q8_0, f16)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Path to exported GGUF file</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Prepare model in format expected by conversion script</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Save temporary HuggingFace format for conversion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Invoke llama.cpp conversion utility with quantization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Validate GGUF file can be loaded by llama.cpp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Clean up temporary files</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return path to GGUF model file</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_export_quality</span><span style=\"color:#E1E4E8\">(self, exported_model_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                               export_format: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Validate that exported model maintains acceptable quality.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Quality metrics comparing exported vs original model</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load exported model in appropriate format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Prepare test dataset for quality comparison</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Run inference with both original and exported models</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Calculate perplexity and task-specific metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Compare quality metrics and flag significant degradation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return comprehensive quality comparison results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing the evaluation and merging component, verify the following behaviors to ensure correct functionality:</p>\n<p><strong>Evaluation Validation:</strong></p>\n<ol>\n<li>Run <code>python -m evaluation.evaluator --model-path ./checkpoints/best --eval-data ./data/validation.jsonl</code> to execute comprehensive evaluation</li>\n<li>Verify that perplexity calculations produce reasonable values (typically 5-50 for instruction-tuned models)</li>\n<li>Check that task-specific metrics show improvement over baseline model performance</li>\n<li>Confirm evaluation reports include both aggregate scores and detailed per-category breakdowns</li>\n</ol>\n<p><strong>Adapter Merging Verification:</strong></p>\n<ol>\n<li>Execute <code>python -m merging.adapter_merger --checkpoint ./checkpoints/best --output ./merged_model</code> to merge adapters</li>\n<li>Verify that merged model directory contains standard HuggingFace format files (pytorch_model.bin, config.json, tokenizer files)</li>\n<li>Load merged model and run test inference to confirm generation capability</li>\n<li>Compare inference outputs between merged and unmerged models to verify equivalence</li>\n</ol>\n<p><strong>Export Quality Testing:</strong></p>\n<ol>\n<li>Run <code>python -m merging.export_manager --model-path ./merged_model --formats huggingface,gguf</code> to export multiple formats</li>\n<li>Load exported HuggingFace model and verify it generates coherent responses</li>\n<li>Test GGUF model with llama.cpp to confirm compatibility and inference speed</li>\n<li>Compare perplexity scores across original, merged, and exported models to validate quality preservation</li>\n</ol>\n<p><strong>Expected Output Patterns:</strong></p>\n<ul>\n<li>Evaluation reports should show 10-30% improvement in task-specific metrics compared to base model</li>\n<li>Merged model should produce identical outputs to unmerged LoRA model within 1e-5 numerical tolerance</li>\n<li>GGUF export should reduce model size by 60-75% while maintaining within 5% of original perplexity</li>\n<li>Export validation should confirm successful loading in target inference environments</li>\n</ul>\n<h2 id=\"interactions-and-data-flow\">Interactions and Data Flow</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - this section describes how the components work together throughout the complete fine-tuning pipeline, from initial data loading through final model deployment</p>\n</blockquote>\n<p>The fine-tuning pipeline orchestrates a complex sequence of operations where each component transforms data and coordinates state with downstream components. Think of this as <strong>The Assembly Line Conductor</strong> - a factory floor manager who ensures that raw materials (training data) flow smoothly through specialized workstations (components), with each station receiving exactly what it needs when it needs it, while maintaining quality control and handling disruptions gracefully. The conductor doesn&#39;t just pass materials along; they coordinate timing, manage shared resources, handle bottlenecks, and ensure that the final product (fine-tuned model) meets all specifications.</p>\n<p>The pipeline&#39;s architecture follows a <strong>producer-consumer pattern with state coordination</strong>, where each component produces outputs that become inputs for downstream components, while sharing configuration and state information through well-defined interfaces. Unlike a simple linear pipeline, this system requires bidirectional communication for error propagation, progress reporting, and dynamic resource management. The <code>FineTuningPipeline</code> class serves as the central coordinator, maintaining shared state and orchestrating component interactions while ensuring data consistency and error recovery.</p>\n<h3 id=\"end-to-end-pipeline-flow\">End-to-End Pipeline Flow</h3>\n<p>The complete pipeline execution follows a carefully orchestrated sequence that transforms raw training data into a deployment-ready fine-tuned model. This flow involves five major phases, each with multiple internal steps and coordination points.</p>\n<h4 id=\"phase-1-configuration-and-initialization\">Phase 1: Configuration and Initialization</h4>\n<p>The pipeline begins with configuration loading and system initialization, establishing the foundation for all subsequent operations. The <code>FineTuningPipeline</code> coordinates this startup sequence to ensure all components receive consistent configuration and system resources are properly allocated.</p>\n<table>\n<thead>\n<tr>\n<th>Step</th>\n<th>Operation</th>\n<th>Component</th>\n<th>Input</th>\n<th>Output</th>\n<th>State Changes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1.1</td>\n<td>Load configuration</td>\n<td>Pipeline</td>\n<td>YAML config file</td>\n<td><code>TrainingConfig</code> object</td>\n<td>Configuration validated and stored</td>\n</tr>\n<tr>\n<td>1.2</td>\n<td>Initialize memory monitor</td>\n<td><code>MemoryMonitor</code></td>\n<td>System specs</td>\n<td>Baseline memory metrics</td>\n<td>Baseline memory state captured</td>\n</tr>\n<tr>\n<td>1.3</td>\n<td>Validate hardware compatibility</td>\n<td>Pipeline</td>\n<td>GPU specs, memory</td>\n<td>Compatibility report</td>\n<td>Hardware constraints established</td>\n</tr>\n<tr>\n<td>1.4</td>\n<td>Initialize logging system</td>\n<td>Pipeline</td>\n<td>Log configuration</td>\n<td>Logger instances</td>\n<td>Logging infrastructure active</td>\n</tr>\n<tr>\n<td>1.5</td>\n<td>Create output directories</td>\n<td>Pipeline</td>\n<td>Output paths</td>\n<td>Directory structure</td>\n<td>File system prepared</td>\n</tr>\n</tbody></table>\n<p>The initialization phase establishes critical invariants that subsequent phases depend on. The memory monitor captures baseline measurements before any model loading occurs, enabling accurate tracking of memory usage throughout the pipeline. Hardware validation ensures that the quantization configuration and model size are compatible with available GPU memory, preventing out-of-memory failures during training.</p>\n<blockquote>\n<p><strong>Critical Insight</strong>: The initialization phase must establish all shared state and resource constraints before any component begins processing. Attempting to modify system-level configuration after model loading can lead to inconsistent behavior and resource conflicts.</p>\n</blockquote>\n<h4 id=\"phase-2-model-and-data-preparation\">Phase 2: Model and Data Preparation</h4>\n<p>This phase loads the base model with quantization and prepares the training dataset, requiring careful coordination between memory management and data processing. The order of operations is critical - quantization must occur during model loading, and LoRA adapters must be injected before the training data loader is created.</p>\n<table>\n<thead>\n<tr>\n<th>Step</th>\n<th>Operation</th>\n<th>Component</th>\n<th>Input</th>\n<th>Output</th>\n<th>Dependencies</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>2.1</td>\n<td>Load quantized base model</td>\n<td>QLoRA Component</td>\n<td>Model name, quant config</td>\n<td>Quantized model</td>\n<td>Memory baseline established</td>\n</tr>\n<tr>\n<td>2.2</td>\n<td>Initialize tokenizer</td>\n<td>Pipeline</td>\n<td>Model name, special tokens</td>\n<td>Configured tokenizer</td>\n<td>Model loaded successfully</td>\n</tr>\n<tr>\n<td>2.3</td>\n<td>Detect target modules</td>\n<td>LoRA Component</td>\n<td>Model architecture</td>\n<td>Target module list</td>\n<td>Model architecture analyzed</td>\n</tr>\n<tr>\n<td>2.4</td>\n<td>Configure LoRA adapters</td>\n<td>LoRA Component</td>\n<td>Rank, alpha, targets</td>\n<td><code>LoRAConfig</code> object</td>\n<td>Target modules identified</td>\n</tr>\n<tr>\n<td>2.5</td>\n<td>Inject adapters</td>\n<td>LoRA Component</td>\n<td>Base model, LoRA config</td>\n<td>PEFT model with adapters</td>\n<td>LoRA configuration validated</td>\n</tr>\n<tr>\n<td>2.6</td>\n<td>Load and validate training data</td>\n<td>Dataset Component</td>\n<td>Data file paths</td>\n<td>Raw data samples</td>\n<td>File system accessible</td>\n</tr>\n<tr>\n<td>2.7</td>\n<td>Apply data formatting</td>\n<td>Dataset Component</td>\n<td>Raw samples, tokenizer</td>\n<td><code>InstructionSample</code> objects</td>\n<td>Data structure validated</td>\n</tr>\n<tr>\n<td>2.8</td>\n<td>Create train/val split</td>\n<td>Dataset Component</td>\n<td>Formatted samples</td>\n<td>Split datasets</td>\n<td>Data quality metrics computed</td>\n</tr>\n<tr>\n<td>2.9</td>\n<td>Initialize data loaders</td>\n<td>Pipeline</td>\n<td>Split datasets, batch size</td>\n<td>PyTorch DataLoaders</td>\n<td>Memory available for batching</td>\n</tr>\n</tbody></table>\n<p>The model preparation sequence is particularly sensitive to memory management. The quantized model loading triggers the largest single memory allocation, and the memory monitor tracks this carefully. If quantization fails due to insufficient memory, the pipeline can attempt fallback configurations or provide detailed error diagnostics.</p>\n<p><strong>Data preparation coordination</strong> requires the tokenizer from the model loading step, creating a dependency chain that must be respected. The chat template application depends on the specific model&#39;s tokenizer configuration, and the train/validation split must occur after quality filtering to ensure representative sampling.</p>\n<h4 id=\"phase-3-training-orchestration\">Phase 3: Training Orchestration</h4>\n<p>The training phase represents the most complex component interaction, with the training loop coordinating between data loading, forward/backward passes, gradient accumulation, evaluation, and checkpointing. The <code>TrainingOrchestrator</code> manages this coordination while monitoring for failure conditions.</p>\n<table>\n<thead>\n<tr>\n<th>Training Step</th>\n<th>Operations</th>\n<th>Components Involved</th>\n<th>Data Flow</th>\n<th>State Updates</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Training Step Setup</td>\n<td>Load batch, move to GPU, prepare inputs</td>\n<td>DataLoader, Training Loop</td>\n<td>Raw batch → GPU tensors</td>\n<td>Step counter incremented</td>\n</tr>\n<tr>\n<td>Forward Pass</td>\n<td>Model inference, loss calculation</td>\n<td>PEFT Model, Loss Function</td>\n<td>Input tensors → loss scalar</td>\n<td>Gradients computed</td>\n</tr>\n<tr>\n<td>Backward Pass</td>\n<td>Gradient computation, accumulation</td>\n<td>Optimizer, PEFT Model</td>\n<td>Loss → parameter gradients</td>\n<td>Gradient buffers updated</td>\n</tr>\n<tr>\n<td>Optimizer Step</td>\n<td>Parameter updates, scheduler step</td>\n<td>Optimizer, LR Scheduler</td>\n<td>Gradients → parameter updates</td>\n<td>Learning rate updated</td>\n</tr>\n<tr>\n<td>Evaluation Trigger</td>\n<td>Check evaluation schedule</td>\n<td>Training Loop</td>\n<td>Step count → evaluation decision</td>\n<td>Evaluation flag set</td>\n</tr>\n<tr>\n<td>Checkpoint Save</td>\n<td>Model state serialization</td>\n<td>Checkpoint Manager</td>\n<td>Model state → checkpoint file</td>\n<td>Best model tracking updated</td>\n</tr>\n</tbody></table>\n<p>The training orchestration must handle <strong>gradient accumulation</strong> across multiple micro-batches, which requires careful state management to ensure that gradients accumulate correctly before the optimizer step. The effective batch size calculation impacts learning rate scaling and convergence behavior.</p>\n<p><strong>Evaluation coordination</strong> occurs at scheduled intervals, requiring the training loop to pause, switch the model to evaluation mode, run validation samples through the evaluation component, and then resume training. This state transition must preserve the training state exactly.</p>\n<blockquote>\n<p><strong>Design Decision</strong>: <strong>Synchronous vs Asynchronous Evaluation</strong></p>\n<ul>\n<li><strong>Context</strong>: Evaluation can be time-consuming and might benefit from background processing</li>\n<li><strong>Options Considered</strong>: <ul>\n<li>Synchronous evaluation that blocks training progress</li>\n<li>Asynchronous evaluation using separate processes</li>\n<li>Cached evaluation using model fingerprinting</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Synchronous evaluation with configurable frequency</li>\n<li><strong>Rationale</strong>: Model state consistency is critical for accurate evaluation, and the complexity of state synchronization across processes outweighs performance benefits for the target use case</li>\n<li><strong>Consequences</strong>: Training pauses during evaluation, but evaluation results are guaranteed to reflect the exact model state at that training step</li>\n</ul>\n</blockquote>\n<h4 id=\"phase-4-evaluation-and-quality-assessment\">Phase 4: Evaluation and Quality Assessment</h4>\n<p>The evaluation phase performs comprehensive model assessment using multiple metrics and comparison strategies. This phase coordinates between perplexity calculation, task-specific evaluation, and baseline comparison to provide a complete picture of fine-tuning effectiveness.</p>\n<table>\n<thead>\n<tr>\n<th>Evaluation Type</th>\n<th>Component</th>\n<th>Input</th>\n<th>Output</th>\n<th>Dependencies</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Perplexity Calculation</td>\n<td><code>PerplexityCalculator</code></td>\n<td>Validation samples, model</td>\n<td><code>PerplexityResult</code></td>\n<td>Model in eval mode</td>\n</tr>\n<tr>\n<td>Task-Specific Metrics</td>\n<td><code>TaskSpecificEvaluator</code></td>\n<td>Test samples, model</td>\n<td>Accuracy, BLEU, ROUGE scores</td>\n<td>Generation working properly</td>\n</tr>\n<tr>\n<td>Baseline Comparison</td>\n<td><code>ModelComparator</code></td>\n<td>Fine-tuned model, base model</td>\n<td>Improvement metrics</td>\n<td>Both models loaded</td>\n</tr>\n<tr>\n<td>Memory Impact Analysis</td>\n<td><code>MemoryMonitor</code></td>\n<td>Current usage, baseline</td>\n<td>Memory efficiency report</td>\n<td>Memory tracking active</td>\n</tr>\n<tr>\n<td>Adapter Analysis</td>\n<td><code>ParameterAnalyzer</code></td>\n<td>PEFT model</td>\n<td>Efficiency metrics</td>\n<td>LoRA adapters injected</td>\n</tr>\n</tbody></table>\n<p>The evaluation coordination requires loading both the fine-tuned model and the baseline model simultaneously for direct comparison, which creates additional memory pressure. The pipeline must monitor available memory and potentially implement evaluation strategies that avoid loading both models simultaneously if memory is constrained.</p>\n<p><strong>Quality thresholds</strong> guide decision-making about whether to continue training, stop early, or adjust hyperparameters. The evaluation component provides recommendations based on convergence patterns and performance relative to baseline metrics.</p>\n<h4 id=\"phase-5-model-merging-and-export\">Phase 5: Model Merging and Export</h4>\n<p>The final phase merges LoRA adapters back into the base model and exports the result in deployment-ready formats. This phase requires careful verification to ensure that the merged model maintains equivalent behavior to the adapter-based model.</p>\n<table>\n<thead>\n<tr>\n<th>Export Step</th>\n<th>Operation</th>\n<th>Component</th>\n<th>Input</th>\n<th>Output</th>\n<th>Verification</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Adapter Merging</td>\n<td>Combine LoRA weights with base weights</td>\n<td><code>AdapterMerger</code></td>\n<td>PEFT model</td>\n<td>Merged model</td>\n<td>Numerical equivalence check</td>\n</tr>\n<tr>\n<td>Merged Model Validation</td>\n<td>Test output equivalence</td>\n<td><code>ModelComparator</code></td>\n<td>Original + merged models</td>\n<td>Validation report</td>\n<td>Output comparison passed</td>\n</tr>\n<tr>\n<td>HuggingFace Export</td>\n<td>Save in HF format</td>\n<td><code>ModelExporter</code></td>\n<td>Merged model, tokenizer</td>\n<td>HF model files</td>\n<td>Format validation</td>\n</tr>\n<tr>\n<td>GGUF Export</td>\n<td>Convert to GGUF format</td>\n<td><code>ModelExporter</code></td>\n<td>Merged model</td>\n<td>GGUF file</td>\n<td>llama.cpp compatibility</td>\n</tr>\n<tr>\n<td>Quality Verification</td>\n<td>Test exported model quality</td>\n<td><code>ModelExporter</code></td>\n<td>Exported model</td>\n<td>Quality metrics</td>\n<td>Performance threshold met</td>\n</tr>\n<tr>\n<td>Deployment Package</td>\n<td>Bundle model and metadata</td>\n<td>Pipeline</td>\n<td>All export artifacts</td>\n<td>Deployment package</td>\n<td>Complete package validation</td>\n</tr>\n</tbody></table>\n<p><strong>Merger verification</strong> is critical because floating-point arithmetic during weight combination can introduce numerical differences. The verification process runs identical inputs through both the adapter-based and merged models, comparing outputs within acceptable tolerance bounds.</p>\n<h3 id=\"inter-component-communication\">Inter-Component Communication</h3>\n<p>Components communicate through well-defined interfaces that specify data formats, method signatures, and state management protocols. This communication follows both synchronous method calls for direct data transformation and asynchronous signaling for progress reporting and error propagation.</p>\n<h4 id=\"primary-data-interfaces\">Primary Data Interfaces</h4>\n<p>The pipeline defines standardized data structures that flow between components, ensuring type safety and consistent data representation throughout the system.</p>\n<table>\n<thead>\n<tr>\n<th>Interface</th>\n<th>Producer Component</th>\n<th>Consumer Component</th>\n<th>Data Structure</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Raw Data → Formatted Data</td>\n<td>Dataset Preparation</td>\n<td>Training Loop</td>\n<td><code>List[InstructionSample]</code></td>\n<td>Standardized instruction-response pairs</td>\n</tr>\n<tr>\n<td>Formatted Data → Tokenized Data</td>\n<td>Dataset Preparation</td>\n<td>Training Loop</td>\n<td><code>List[TokenizedSample]</code></td>\n<td>GPU-ready tensor data</td>\n</tr>\n<tr>\n<td>Model Configuration → Quantized Model</td>\n<td>QLoRA Quantization</td>\n<td>LoRA Configuration</td>\n<td><code>PreTrainedModel</code> + <code>QuantizationConfig</code></td>\n<td>Memory-optimized base model</td>\n</tr>\n<tr>\n<td>LoRA Configuration → Adapted Model</td>\n<td>LoRA Configuration</td>\n<td>Training Loop</td>\n<td><code>PeftModel</code></td>\n<td>Model with trainable adapters</td>\n</tr>\n<tr>\n<td>Training State → Checkpoints</td>\n<td>Training Loop</td>\n<td>Evaluation Component</td>\n<td><code>CheckpointMetadata</code></td>\n<td>Serialized model state</td>\n</tr>\n<tr>\n<td>Evaluation Results → Decisions</td>\n<td>Evaluation Component</td>\n<td>Training Loop</td>\n<td><code>EvaluationMetrics</code></td>\n<td>Performance assessment</td>\n</tr>\n<tr>\n<td>Final Model → Export Formats</td>\n<td>Evaluation Component</td>\n<td>Model Export</td>\n<td><code>PreTrainedModel</code></td>\n<td>Deployment-ready model</td>\n</tr>\n</tbody></table>\n<p><strong>Data transformation protocols</strong> ensure that each component receives data in the expected format. For example, the dataset preparation component guarantees that all <code>InstructionSample</code> objects include required fields and pass quality validation before being passed to the tokenization stage.</p>\n<h4 id=\"configuration-propagation\">Configuration Propagation</h4>\n<p>Configuration information flows from the central <code>TrainingConfig</code> object to individual components, with each component extracting relevant parameters and validating compatibility with its requirements.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Configuration Extract</th>\n<th>Validation Requirements</th>\n<th>Error Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>QLoRA Quantization</td>\n<td><code>quantization: QuantizationConfig</code></td>\n<td>GPU compatibility, model size limits</td>\n<td>Fallback to FP16, detailed error reporting</td>\n</tr>\n<tr>\n<td>LoRA Configuration</td>\n<td><code>lora: LoRAConfig</code></td>\n<td>Target modules exist, rank feasibility</td>\n<td>Auto-detect targets, adjust rank recommendations</td>\n</tr>\n<tr>\n<td>Training Loop</td>\n<td>Learning rate, batch size, epochs</td>\n<td>Memory constraints, convergence parameters</td>\n<td>Gradient accumulation adjustment, LR scaling</td>\n</tr>\n<tr>\n<td>Dataset Preparation</td>\n<td>Data paths, quality filters</td>\n<td>File accessibility, format compatibility</td>\n<td>Format auto-detection, quality threshold adjustment</td>\n</tr>\n<tr>\n<td>Evaluation</td>\n<td>Metrics configuration, baseline model</td>\n<td>Evaluation data availability</td>\n<td>Skip unavailable metrics, baseline-free evaluation</td>\n</tr>\n</tbody></table>\n<p><strong>Configuration validation</strong> occurs at component initialization, allowing early detection of incompatible settings before expensive operations like model loading begin. Components provide detailed error messages that include suggested fixes or alternative configurations.</p>\n<h4 id=\"progress-and-error-signaling\">Progress and Error Signaling</h4>\n<p>Components communicate status updates and error conditions through a combination of return values, exception propagation, and event signaling.</p>\n<table>\n<thead>\n<tr>\n<th>Signal Type</th>\n<th>Components Involved</th>\n<th>Information Carried</th>\n<th>Handler Actions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Progress Updates</td>\n<td>All components → Pipeline coordinator</td>\n<td>Step counts, completion percentages</td>\n<td>Update progress bars, log milestones</td>\n</tr>\n<tr>\n<td>Memory Warnings</td>\n<td>Memory Monitor → All components</td>\n<td>Current usage, threshold breaches</td>\n<td>Trigger garbage collection, reduce batch sizes</td>\n</tr>\n<tr>\n<td>Training Metrics</td>\n<td>Training Loop → Logging/Monitoring</td>\n<td>Loss values, learning rates, throughput</td>\n<td>Log to WandB/TensorBoard, trigger early stopping</td>\n</tr>\n<tr>\n<td>Quality Alerts</td>\n<td>Evaluation Component → Training Loop</td>\n<td>Performance degradation, convergence issues</td>\n<td>Adjust learning rate, trigger checkpointing</td>\n</tr>\n<tr>\n<td>Error Conditions</td>\n<td>Any component → Pipeline coordinator</td>\n<td>Exception details, recovery suggestions</td>\n<td>Attempt recovery, clean shutdown, detailed reporting</td>\n</tr>\n</tbody></table>\n<p><strong>Error propagation</strong> follows a structured approach where components catch and wrap exceptions with context-specific information before re-raising them. This allows the pipeline coordinator to make informed decisions about recovery strategies.</p>\n<h4 id=\"method-call-patterns\">Method Call Patterns</h4>\n<p>Component interactions follow consistent patterns that simplify testing and maintenance while ensuring predictable behavior.</p>\n<table>\n<thead>\n<tr>\n<th>Pattern</th>\n<th>Example Methods</th>\n<th>Purpose</th>\n<th>Error Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Configuration Validation</td>\n<td><code>validate_configuration()</code></td>\n<td>Pre-flight checks</td>\n<td>Return detailed validation errors</td>\n</tr>\n<tr>\n<td>Resource Initialization</td>\n<td><code>setup_model_and_tokenizer()</code></td>\n<td>One-time setup operations</td>\n<td>Clean up partial state on failure</td>\n</tr>\n<tr>\n<td>Data Transformation</td>\n<td><code>tokenize_sample()</code>, <code>apply_template()</code></td>\n<td>Convert data between formats</td>\n<td>Validate inputs, provide fallbacks</td>\n</tr>\n<tr>\n<td>State Queries</td>\n<td><code>get_peak_usage()</code>, <code>calculate_efficiency_metrics()</code></td>\n<td>Non-mutating information retrieval</td>\n<td>Never fail, return best available data</td>\n</tr>\n<tr>\n<td>State Mutations</td>\n<td><code>inject_adapters()</code>, <code>merge_adapters()</code></td>\n<td>Modify component state</td>\n<td>Atomic operations, rollback on failure</td>\n</tr>\n<tr>\n<td>Batch Processing</td>\n<td><code>filter_dataset()</code>, <code>split_dataset()</code></td>\n<td>Process collections efficiently</td>\n<td>Progress reporting, partial results on interruption</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Architecture Decision</strong>: <strong>Synchronous vs Asynchronous Method Calls</strong></p>\n<ul>\n<li><strong>Context</strong>: Component operations vary widely in execution time, from millisecond tokenization to minute-scale model loading</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Pure synchronous calls with blocking behavior</li>\n<li>Async/await patterns for long-running operations  </li>\n<li>Thread-based background processing</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Synchronous calls with progress callbacks for long-running operations</li>\n<li><strong>Rationale</strong>: Maintains simple control flow and error handling while providing responsiveness through progress reporting. GPU operations are inherently synchronous, and the added complexity of async coordination outweighs benefits.</li>\n<li><strong>Consequences</strong>: UI responsiveness depends on progress callback frequency, but error handling and state management remain straightforward.</li>\n</ul>\n</blockquote>\n<h3 id=\"state-coordination-and-dependencies\">State Coordination and Dependencies</h3>\n<p>The pipeline maintains both shared global state and component-specific local state, with carefully managed dependencies to ensure consistency and enable recovery from failures.</p>\n<h4 id=\"global-state-management\">Global State Management</h4>\n<p>The <code>FineTuningPipeline</code> maintains global state that multiple components need to access or coordinate around. This state is managed through a centralized coordinator to prevent inconsistencies and race conditions.</p>\n<table>\n<thead>\n<tr>\n<th>State Category</th>\n<th>Data Structure</th>\n<th>Owner</th>\n<th>Readers</th>\n<th>Writers</th>\n<th>Synchronization</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Configuration</td>\n<td><code>TrainingConfig</code></td>\n<td>Pipeline</td>\n<td>All components</td>\n<td>Pipeline only</td>\n<td>Immutable after initialization</td>\n</tr>\n<tr>\n<td>Memory Metrics</td>\n<td><code>MemoryMonitor</code></td>\n<td>Memory Monitor</td>\n<td>All components</td>\n<td>Memory Monitor only</td>\n<td>Thread-safe updates</td>\n</tr>\n<tr>\n<td>Training Progress</td>\n<td><code>TrainingMetrics</code></td>\n<td>Training Loop</td>\n<td>Evaluation, Logging</td>\n<td>Training Loop only</td>\n<td>Atomic metric updates</td>\n</tr>\n<tr>\n<td>Model Checkpoints</td>\n<td><code>CheckpointMetadata</code></td>\n<td>Checkpoint Manager</td>\n<td>Training, Evaluation</td>\n<td>Checkpoint Manager only</td>\n<td>File-system locking</td>\n</tr>\n<tr>\n<td>Evaluation Results</td>\n<td><code>EvaluationMetrics</code></td>\n<td>Evaluation Component</td>\n<td>Training Loop, Logging</td>\n<td>Evaluation Component only</td>\n<td>Event-driven updates</td>\n</tr>\n</tbody></table>\n<p><strong>State access patterns</strong> follow the principle of single-writer, multiple-reader to avoid consistency issues. Components that need to coordinate state changes do so through the pipeline coordinator rather than direct communication.</p>\n<h4 id=\"component-dependency-graph\">Component Dependency Graph</h4>\n<p>Components have both initialization dependencies (what must be set up before this component can initialize) and runtime dependencies (what must be available during operation).</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Initialization Dependencies</th>\n<th>Runtime Dependencies</th>\n<th>Failure Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>QLoRA Quantization</td>\n<td>Memory baseline, GPU availability</td>\n<td>CUDA drivers, sufficient VRAM</td>\n<td>Cannot load model - pipeline abort</td>\n</tr>\n<tr>\n<td>LoRA Configuration</td>\n<td>Quantized model loaded</td>\n<td>Model architecture stable</td>\n<td>Cannot create adapters - pipeline abort</td>\n</tr>\n<tr>\n<td>Dataset Preparation</td>\n<td>File system access, tokenizer</td>\n<td>None after initialization</td>\n<td>Cannot create training data - pipeline abort</td>\n</tr>\n<tr>\n<td>Training Loop</td>\n<td>PEFT model, training data, optimizer</td>\n<td>GPU memory, data loader</td>\n<td>Cannot train - pipeline abort</td>\n</tr>\n<tr>\n<td>Evaluation</td>\n<td>Trained model, validation data</td>\n<td>Model in eval mode, generation working</td>\n<td>Cannot evaluate - continue with limited metrics</td>\n</tr>\n</tbody></table>\n<p><strong>Dependency resolution</strong> occurs during pipeline initialization, with each component declaring its dependencies and the pipeline coordinator ensuring proper ordering. If dependencies cannot be satisfied, the pipeline provides detailed error messages explaining what&#39;s missing and potential solutions.</p>\n<h4 id=\"state-persistence-and-recovery\">State Persistence and Recovery</h4>\n<p>The pipeline implements comprehensive checkpointing that captures both component state and coordination state, enabling recovery from various failure modes.</p>\n<table>\n<thead>\n<tr>\n<th>Checkpoint Type</th>\n<th>Frequency</th>\n<th>Contents</th>\n<th>Recovery Scenario</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Full Model Checkpoint</td>\n<td>Every N steps</td>\n<td>Model weights, optimizer state, LoRA adapters</td>\n<td>Training interruption, system restart</td>\n</tr>\n<tr>\n<td>Configuration Snapshot</td>\n<td>At initialization</td>\n<td>Complete training configuration, validation</td>\n<td>Configuration validation, reproducibility</td>\n</tr>\n<tr>\n<td>Progress Checkpoint</td>\n<td>Every step</td>\n<td>Training metrics, evaluation results</td>\n<td>Progress monitoring, performance analysis</td>\n</tr>\n<tr>\n<td>Component State</td>\n<td>On state changes</td>\n<td>Component-specific persistent state</td>\n<td>Component failure, partial recovery</td>\n</tr>\n<tr>\n<td>Dependency Metadata</td>\n<td>At setup completion</td>\n<td>Component initialization order, versions</td>\n<td>Debugging initialization failures</td>\n</tr>\n</tbody></table>\n<p><strong>Recovery coordination</strong> follows a hierarchical approach where the pipeline coordinator determines what level of recovery is possible based on available checkpoint data and current system state.</p>\n<h4 id=\"memory-coordination\">Memory Coordination</h4>\n<p>Memory management requires coordination across all components because GPU memory is a shared, limited resource that can cause system-wide failures if not managed carefully.</p>\n<table>\n<thead>\n<tr>\n<th>Memory Pool</th>\n<th>Manager</th>\n<th>Allocation Strategy</th>\n<th>Overflow Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Base Model Weights</td>\n<td>QLoRA Quantization</td>\n<td>Single large allocation at startup</td>\n<td>Fallback quantization levels, detailed OOM reporting</td>\n</tr>\n<tr>\n<td>LoRA Adapters</td>\n<td>LoRA Configuration</td>\n<td>Small allocations per target module</td>\n<td>Reduce target modules, warn about capacity limits</td>\n</tr>\n<tr>\n<td>Training Batches</td>\n<td>Training Loop</td>\n<td>Dynamic allocation per batch</td>\n<td>Reduce batch size, increase gradient accumulation</td>\n</tr>\n<tr>\n<td>Optimizer States</td>\n<td>Training Loop</td>\n<td>Parallel to model parameters</td>\n<td>Use memory-efficient optimizers, offload to CPU</td>\n</tr>\n<tr>\n<td>Evaluation Buffers</td>\n<td>Evaluation Component</td>\n<td>Temporary allocation during eval</td>\n<td>Skip memory-intensive metrics, batch evaluation</td>\n</tr>\n</tbody></table>\n<p><strong>Memory pressure detection</strong> uses the <code>MemoryMonitor</code> to track allocation patterns and predict when out-of-memory conditions might occur. Components receive memory pressure signals and can adjust their resource usage before failures occur.</p>\n<blockquote>\n<p><strong>Critical Design Principle</strong>: <strong>Graceful Degradation Under Memory Pressure</strong></p>\n<p>The pipeline is designed to maintain functionality even when memory constraints prevent optimal operation. Components implement fallback strategies that trade performance or features for memory efficiency, allowing the pipeline to complete training even on resource-constrained hardware.</p>\n</blockquote>\n<h4 id=\"inter-component-error-recovery\">Inter-Component Error Recovery</h4>\n<p>When component failures occur, the recovery strategy depends on the failure type, the point in the pipeline where it occurred, and what state can be preserved.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Type</th>\n<th>Detection Method</th>\n<th>Recovery Strategy</th>\n<th>State Preservation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Loading Error</td>\n<td>Exception during file I/O</td>\n<td>Skip corrupted samples, continue with valid data</td>\n<td>Maintain data quality metrics</td>\n</tr>\n<tr>\n<td>Model Loading OOM</td>\n<td>CUDA out-of-memory exception</td>\n<td>Retry with more aggressive quantization</td>\n<td>Preserve configuration for retry</td>\n</tr>\n<tr>\n<td>Training Step Failure</td>\n<td>Loss becomes NaN or infinite</td>\n<td>Restore last checkpoint, reduce learning rate</td>\n<td>Roll back to last stable state</td>\n</tr>\n<tr>\n<td>Evaluation Failure</td>\n<td>Generation timeout or error</td>\n<td>Skip current evaluation, continue training</td>\n<td>Maintain training progress</td>\n</tr>\n<tr>\n<td>Export Failure</td>\n<td>File system or format error</td>\n<td>Retry with different format, manual intervention</td>\n<td>Preserve merged model in memory</td>\n</tr>\n</tbody></table>\n<p><strong>Error context propagation</strong> ensures that when failures occur, sufficient information is available to diagnose the root cause and determine appropriate recovery actions. Each component adds context-specific information to exceptions before propagating them to the coordinator.</p>\n<h3 id=\"common-pitfalls-in-pipeline-orchestration\">Common Pitfalls in Pipeline Orchestration</h3>\n<p>⚠️ <strong>Pitfall: Configuration Mutation After Initialization</strong></p>\n<p>Many developers attempt to modify configuration objects after components have been initialized, leading to inconsistent behavior where some components operate with old configuration while others use new settings. This is particularly problematic with memory-related settings that affect resource allocation.</p>\n<p>The problem occurs because components often cache configuration-derived values during initialization. For example, the QLoRA quantization component calculates memory allocation sizes based on the quantization configuration, and changing the configuration afterward doesn&#39;t trigger recalculation.</p>\n<p><strong>Solution</strong>: Make configuration objects immutable after the initialization phase completes. If configuration changes are needed, restart the pipeline with new configuration rather than attempting in-place modification.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Component Initialization Order</strong></p>\n<p>Attempting to initialize components in the wrong order leads to subtle bugs where dependencies are not properly established. For example, injecting LoRA adapters before the base model is fully loaded can result in adapter injection failures or incorrect target module detection.</p>\n<p>This happens because developers often focus on individual component implementation without understanding the dependency graph. The base model architecture must be fully analyzed before LoRA target modules can be identified, and the tokenizer configuration must be stable before chat templates can be applied.</p>\n<p><strong>Solution</strong>: Implement explicit dependency declarations for each component and use topological sorting to determine initialization order automatically. Validate that all dependencies are satisfied before allowing component initialization to proceed.</p>\n<p>⚠️ <strong>Pitfall: Memory Pressure Cascade Failures</strong></p>\n<p>When one component encounters memory pressure and fails, it often triggers a cascade of failures in other components that depend on it. For example, if batch size needs to be reduced due to memory constraints, this affects gradient accumulation calculations, learning rate scaling, and convergence behavior.</p>\n<p>The cascade occurs because components make assumptions about resource availability based on initial configuration. When these assumptions are violated, components don&#39;t have fallback strategies and simply fail rather than adapting.</p>\n<p><strong>Solution</strong>: Implement memory pressure signaling throughout the pipeline, allowing components to adjust their resource usage proactively. Design components with fallback strategies that maintain functionality even when optimal resource allocation is not available.</p>\n<p>⚠️ <strong>Pitfall: State Synchronization Race Conditions</strong></p>\n<p>When multiple components need to coordinate state changes, race conditions can occur if synchronization is not properly managed. This is particularly common between the training loop and evaluation component, where evaluation state must be synchronized with training progress.</p>\n<p>The problem manifests as evaluation results that don&#39;t correspond to the reported training step, or checkpoint metadata that doesn&#39;t match the actual model state. This makes debugging and reproducibility extremely difficult.</p>\n<p><strong>Solution</strong>: Use a centralized state coordinator that manages all cross-component state changes through atomic operations. Implement state versioning so that components can verify they&#39;re operating on consistent state snapshots.</p>\n<p>⚠️ <strong>Pitfall: Incomplete Error Recovery State Cleanup</strong></p>\n<p>When recovery from component failures is attempted, incomplete cleanup of partial state can leave the pipeline in an inconsistent condition. For example, if LoRA adapter injection fails partway through, some modules might have adapters while others don&#39;t, leading to training instability.</p>\n<p>This occurs because error handling code often focuses on the immediate failure without considering all the state changes that occurred before the failure point. Partial state can persist and cause problems in subsequent operations.</p>\n<p><strong>Solution</strong>: Implement transactional state changes where possible, with automatic rollback on failure. For complex operations that can&#39;t be made atomic, maintain explicit cleanup procedures that restore all affected components to a known good state.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides practical implementation patterns and starter code for orchestrating the fine-tuning pipeline components.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Pipeline Orchestration</td>\n<td>Class-based coordinator with method chaining</td>\n<td>State machine with event-driven transitions</td>\n</tr>\n<tr>\n<td>Configuration Management</td>\n<td>YAML files with Pydantic validation</td>\n<td>Hydra with composition and overrides</td>\n</tr>\n<tr>\n<td>Progress Reporting</td>\n<td>Simple print statements with progress bars</td>\n<td>Rich console with structured logging</td>\n</tr>\n<tr>\n<td>Error Handling</td>\n<td>Try-catch blocks with context managers</td>\n<td>Custom exception hierarchy with recovery strategies</td>\n</tr>\n<tr>\n<td>State Persistence</td>\n<td>Pickle-based checkpoints</td>\n<td>HuggingFace Accelerate state management</td>\n</tr>\n<tr>\n<td>Memory Monitoring</td>\n<td>PyTorch CUDA memory functions</td>\n<td>nvidia-ml-py with detailed GPU metrics</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>llm_fine_tuning_pipeline/\n├── core/\n│   ├── pipeline.py              ← FineTuningPipeline coordinator\n│   ├── config.py                ← Configuration data structures  \n│   ├── state_manager.py         ← Global state coordination\n│   └── memory_monitor.py        ← Memory usage tracking\n├── components/\n│   ├── dataset_preparation.py   ← Data loading and preprocessing\n│   ├── lora_configuration.py    ← LoRA adapter management\n│   ├── quantization.py          ← QLoRA quantization handling\n│   ├── training_loop.py         ← Training orchestration\n│   └── evaluation.py            ← Model evaluation and export\n├── utils/\n│   ├── checkpoint_manager.py    ← Checkpoint saving and loading\n│   ├── error_recovery.py        ← Error handling utilities\n│   └── progress_tracking.py     ← Progress reporting helpers\n├── tests/\n│   ├── integration/             ← End-to-end pipeline tests\n│   └── component/               ← Individual component tests\n└── examples/\n    ├── basic_fine_tuning.py     ← Simple pipeline usage\n    └── advanced_config.yaml     ← Complex configuration example</code></pre></div>\n\n<h4 id=\"pipeline-coordinator-infrastructure\">Pipeline Coordinator Infrastructure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional, List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> contextlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> contextmanager</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .config </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TrainingConfig</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .memory_monitor </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> MemoryMonitor</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .state_manager </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> StateManager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> components.dataset_preparation </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> DatasetPreparation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> components.lora_configuration </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> LoRAConfiguration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> components.quantization </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> QLoRAQuantization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> components.training_loop </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TrainingLoop</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> components.evaluation </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> EvaluationComponent</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> FineTuningPipeline</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Central coordinator for the LLM fine-tuning pipeline.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Orchestrates component initialization, data flow, and state management</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    throughout the complete fine-tuning process from raw data to deployed model.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: TrainingConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.memory_monitor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MemoryMonitor()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.state_manager </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> StateManager()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._setup_logging()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Component instances - initialized during setup</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.dataset_prep: Optional[DatasetPreparation] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.quantization: Optional[QLoRAQuantization] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.lora_config: Optional[LoRAConfiguration] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.training_loop: Optional[TrainingLoop] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.evaluation: Optional[EvaluationComponent] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Shared state</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokenizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.training_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.validation_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> pipeline_context</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Context manager for pipeline execution with proper cleanup.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.memory_monitor.capture_baseline()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.state_manager.initialize()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.logger.info(</span><span style=\"color:#9ECBFF\">\"Pipeline context initialized\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            yield</span><span style=\"color:#79B8FF\"> self</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Pipeline execution failed: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._cleanup_on_failure()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        finally</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._final_cleanup()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.logger.info(</span><span style=\"color:#9ECBFF\">\"Pipeline context cleaned up\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _setup_logging</span><span style=\"color:#E1E4E8\">(self) -> logging.Logger:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize structured logging for the pipeline.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Configure logging based on config.logging settings</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Set up file handlers, console handlers, and formatters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Configure log levels and filtering</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize WandB or TensorBoard integration if specified</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _cleanup_on_failure</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Clean up resources when pipeline fails.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Clear GPU memory and cached tensors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Close file handles and temporary resources</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Save partial state for debugging and recovery</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Send failure notifications if configured</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _final_cleanup</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Final cleanup regardless of success or failure.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Clear all GPU memory allocations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Close logging handlers and flush buffers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate final resource usage report</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Clean up temporary files and directories</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> load_config_from_yaml</span><span style=\"color:#E1E4E8\">(config_path: Path) -> TrainingConfig:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Load and validate pipeline configuration from YAML file.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Parse YAML file with proper error handling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate all required fields are present</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check for configuration consistency (e.g., memory limits vs model size)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Apply default values for optional fields</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return fully validated TrainingConfig object</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"state-manager-implementation\">State Manager Implementation</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional, Callable</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PipelineState</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UNINITIALIZED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"uninitialized\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INITIALIZING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"initializing\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DATA_PREPARATION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"data_preparation\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MODEL_LOADING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"model_loading\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ADAPTER_CONFIGURATION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"adapter_configuration\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TRAINING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"training\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EVALUATION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"evaluation\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EXPORT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"export\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COMPLETED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"completed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"failed\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StateSnapshot</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Immutable snapshot of pipeline state at a point in time.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    state: PipelineState</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: datetime</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metrics: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metadata: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StateManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Thread-safe manager for global pipeline state coordination.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.RLock()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._current_state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> PipelineState.</span><span style=\"color:#79B8FF\">UNINITIALIZED</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._state_history: List[StateSnapshot] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._state_callbacks: Dict[PipelineState, List[Callable]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._shared_data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> initialize</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize the state manager for a new pipeline run.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Clear any previous state and initialize fresh</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Set up state transition validation rules</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize shared data storage with thread safety</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Register default state change callbacks</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> transition_to</span><span style=\"color:#E1E4E8\">(self, new_state: PipelineState, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                     metrics: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                     metadata: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Thread-safe state transition with validation and callbacks.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate that transition from current_state to new_state is allowed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create state snapshot with timestamp and provided data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Update current state and add snapshot to history</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Execute registered callbacks for the new state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Log state transition with appropriate detail level</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_current_state</span><span style=\"color:#E1E4E8\">(self) -> StateSnapshot:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get immutable snapshot of current pipeline state.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create and return StateSnapshot of current state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Include copy of current metrics and metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Ensure returned snapshot is fully immutable</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register_state_callback</span><span style=\"color:#E1E4E8\">(self, state: PipelineState, callback: Callable):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register callback to execute when entering specified state.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add callback to the appropriate state's callback list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate callback signature is compatible</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle duplicate registrations appropriately</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> set_shared_data</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, value: Any):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Thread-safe storage of shared data across components.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store value in shared data dictionary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate key format and value serializability</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Log data updates for debugging purposes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_shared_data</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, default: Any </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Thread-safe retrieval of shared data.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return value from shared data or default if key missing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Consider implementing key expiration if needed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Log data access for debugging purposes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span></code></pre></div>\n\n<h4 id=\"core-pipeline-flow-implementation\">Core Pipeline Flow Implementation</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> FineTuningPipeline</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # ... (previous methods) ...</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> run_complete_pipeline</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute the complete fine-tuning pipeline from start to finish.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.pipeline_context():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Phase 1: Initialize components and validate configuration</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.state_manager.transition_to(PipelineState.</span><span style=\"color:#79B8FF\">INITIALIZING</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Call setup_model_and_tokenizer() and handle failures</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Call setup_lora_adapters() with dependency checking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate memory constraints are satisfied</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Phase 2: Prepare training data</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.state_manager.transition_to(PipelineState.</span><span style=\"color:#79B8FF\">DATA_PREPARATION</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Call prepare_datasets() with progress tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate data quality and format compliance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Log dataset statistics and splits</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Phase 3: Execute training loop</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.state_manager.transition_to(PipelineState.</span><span style=\"color:#79B8FF\">TRAINING</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Call execute_training() with checkpoint management</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Monitor training progress and handle early stopping</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Save best checkpoint path for evaluation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Phase 4: Evaluate and export model</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.state_manager.transition_to(PipelineState.</span><span style=\"color:#79B8FF\">EVALUATION</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Call evaluate_and_merge() with quality validation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate comprehensive evaluation report</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Export model in requested formats</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.state_manager.transition_to(PipelineState.</span><span style=\"color:#79B8FF\">COMPLETED</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._generate_completion_report()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.state_manager.transition_to(PipelineState.</span><span style=\"color:#79B8FF\">FAILED</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                                                metadata</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"error\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(e)})</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Pipeline failed: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">exc_info</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                raise</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> setup_model_and_tokenizer</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize quantized model and tokenizer with memory monitoring.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.state_manager.transition_to(PipelineState.</span><span style=\"color:#79B8FF\">MODEL_LOADING</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize QLoRAQuantization component with config.quantization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Load base model with quantization and monitor memory usage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize tokenizer and configure special tokens for instruction tuning</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store model and tokenizer in shared state for component access</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate model loaded successfully and is in expected precision</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Log model statistics including parameter count and memory usage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> setup_lora_adapters</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Configure and inject LoRA adapters into the quantized base model.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.model </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Base model must be loaded before LoRA adapter setup\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize LoRAConfiguration component with config.lora</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Detect target modules based on model architecture</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create and inject LoRA adapters with specified rank and alpha</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Freeze base model parameters and verify only adapters are trainable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate and log parameter efficiency metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store adapted model in shared state for training loop access</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> prepare_datasets</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load, format, and split training data for the fine-tuning process.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.tokenizer </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Tokenizer must be initialized before dataset preparation\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize DatasetPreparation component with data file paths</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Load raw data and apply quality filtering with statistics tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Format samples using instruction-response templates and chat formatting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Tokenize formatted samples with proper attention masks and padding</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Split into training and validation sets with stratification if requested</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create PyTorch DataLoaders with appropriate batching and shuffling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store training and validation data in shared state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Log comprehensive dataset statistics and quality metrics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> execute_training</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Run the training loop with progress monitoring and checkpoint management.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> all</span><span style=\"color:#E1E4E8\">([</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.model, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.training_data, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.validation_data]):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Model and datasets must be prepared before training\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize TrainingLoop component with model, data, and training config</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Set up optimizer, learning rate scheduler, and gradient accumulation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Configure checkpoint saving frequency and early stopping criteria</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Execute training loop with progress reporting and metric logging</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Monitor for convergence, overfitting, or training instability</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle training interruption gracefully with state preservation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return path to best checkpoint based on validation performance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> evaluate_and_merge</span><span style=\"color:#E1E4E8\">(self, checkpoint_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Evaluate trained model performance and merge adapters for deployment.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize EvaluationComponent with trained model and baseline</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate perplexity on validation set with detailed breakdown</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Run task-specific evaluation metrics if configured</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Compare performance against baseline model quantitatively</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Merge LoRA adapters into base model with verification</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Export merged model in HuggingFace and GGUF formats as requested</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate exported models maintain acceptable quality</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return comprehensive evaluation results and export metadata</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _generate_completion_report</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate comprehensive report of pipeline execution and results.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Collect metrics from all pipeline phases</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate resource usage statistics and efficiency metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate model performance summary with baseline comparisons</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Include export information and deployment recommendations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Format report for human readability and programmatic access</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>After Dataset Preparation (Milestone 1):</strong></p>\n<ul>\n<li>Run: <code>python -m llm_fine_tuning_pipeline.examples.basic_fine_tuning --data-only</code></li>\n<li>Expected: Dataset statistics logged, train/validation splits created, tokenized samples ready</li>\n<li>Verify: Check that <code>InstructionSample</code> objects have all required fields and <code>TokenizedSample</code> objects have proper attention masks</li>\n</ul>\n<p><strong>After LoRA Configuration (Milestone 2):</strong></p>\n<ul>\n<li>Run: <code>python -m llm_fine_tuning_pipeline.examples.basic_fine_tuning --setup-only</code></li>\n<li>Expected: LoRA adapters injected, parameter efficiency report generated, memory usage logged</li>\n<li>Verify: Confirm that only 0.1-1% of parameters are trainable and target modules are correctly identified</li>\n</ul>\n<p><strong>After Training Completion (Milestone 4):</strong></p>\n<ul>\n<li>Run: Complete training pipeline and check final metrics</li>\n<li>Expected: Training loss converged, validation metrics improved over baseline, checkpoints saved</li>\n<li>Verify: Best checkpoint can be loaded successfully and generates coherent responses</li>\n</ul>\n<p><strong>After Evaluation and Export (Milestone 5):</strong></p>\n<ul>\n<li>Run: Full pipeline with export to both HuggingFace and GGUF formats</li>\n<li>Expected: Merged model shows equivalent performance to adapter version, exports load correctly</li>\n<li>Verify: Use llama.cpp to load GGUF model and confirm it generates reasonable outputs</li>\n</ul>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis Method</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Pipeline hangs during initialization</td>\n<td>Circular component dependencies</td>\n<td>Check dependency graph in state manager</td>\n<td>Reorder component initialization sequence</td>\n</tr>\n<tr>\n<td>Memory usage grows unexpectedly</td>\n<td>Memory leak in component interactions</td>\n<td>Use memory profiler during each phase</td>\n<td>Add explicit cleanup in component destructors</td>\n</tr>\n<tr>\n<td>Training metrics don&#39;t match between runs</td>\n<td>Non-deterministic component initialization</td>\n<td>Check random seed propagation</td>\n<td>Set seeds in each component initialization</td>\n</tr>\n<tr>\n<td>Evaluation results inconsistent</td>\n<td>Model state not synchronized between training and eval</td>\n<td>Verify model.eval() calls and shared state</td>\n<td>Use state manager to coordinate mode switches</td>\n</tr>\n<tr>\n<td>Export format validation fails</td>\n<td>Precision loss during adapter merging</td>\n<td>Compare outputs before/after merging</td>\n<td>Increase numerical tolerance or use higher precision</td>\n</tr>\n</tbody></table>\n<h2 id=\"error-handling-and-edge-cases\">Error Handling and Edge Cases</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - robust error handling and recovery strategies are essential throughout the entire fine-tuning pipeline, from data preparation through model export</p>\n</blockquote>\n<p>When fine-tuning large language models on resource-constrained hardware, failures are not exceptional cases but expected realities. The parameter-efficient fine-tuning pipeline must gracefully handle hardware limitations, training instabilities, data quality issues, and infrastructure failures while preserving training progress and enabling recovery. This section establishes comprehensive error handling strategies that maintain system reliability even under adverse conditions.</p>\n<h3 id=\"mental-model-the-safety-net-system\">Mental Model: The Safety Net System</h3>\n<p>Think of error handling in the fine-tuning pipeline like a comprehensive safety net system in a high-wire performance. Just as acrobats use multiple layers of protection—primary safety nets, backup harnesses, spotters, and emergency protocols—our pipeline employs layered defensive strategies. The primary nets catch common failures like memory exhaustion before they cause crashes. Backup systems preserve training state when hardware fails. Recovery protocols restore operation from the last known good state. This multi-layered approach ensures that even when individual components fail, the entire performance doesn&#39;t come crashing down, and the show can continue from where it left off.</p>\n<p>The key insight is that in machine learning pipelines, graceful degradation is often more valuable than perfect operation. A system that can detect memory pressure and automatically reduce batch size is more useful than one that crashes with an out-of-memory error. A training loop that saves frequent checkpoints and recovers seamlessly from hardware failures enables longer, more reliable fine-tuning runs than one that requires manual intervention after every GPU driver crash.</p>\n<h3 id=\"hardware-and-memory-failures\">Hardware and Memory Failures</h3>\n<p>Hardware failures represent the most common and challenging class of errors in GPU-based fine-tuning. Memory exhaustion, GPU driver crashes, thermal throttling, and sudden power losses can interrupt training at any moment. The pipeline must detect these conditions early, implement preventive measures, and recover gracefully when prevention fails.</p>\n<h4 id=\"memory-pressure-detection-and-management\">Memory Pressure Detection and Management</h4>\n<p>Memory management forms the foundation of reliable fine-tuning on consumer hardware. The system continuously monitors GPU and system memory usage, implements adaptive batch sizing, and triggers emergency cleanup when memory pressure approaches critical thresholds.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Responsibility</th>\n<th>Detection Method</th>\n<th>Recovery Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>MemoryMonitor</code></td>\n<td>Track baseline and current usage</td>\n<td>CUDA memory queries every training step</td>\n<td>Log warnings when usage exceeds 85% of capacity</td>\n</tr>\n<tr>\n<td><code>AdaptiveBatchManager</code></td>\n<td>Adjust effective batch size</td>\n<td>Monitor allocation failures and OOM exceptions</td>\n<td>Reduce gradient accumulation steps by half</td>\n</tr>\n<tr>\n<td><code>EmergencyGC</code></td>\n<td>Force garbage collection</td>\n<td>Detect fragmented memory pools</td>\n<td>Clear CUDA cache and trigger Python GC</td>\n</tr>\n<tr>\n<td><code>GradientClipping</code></td>\n<td>Prevent memory spikes</td>\n<td>Monitor gradient norm statistics</td>\n<td>Clip gradients more aggressively during pressure</td>\n</tr>\n</tbody></table>\n<p>The memory monitoring system establishes baseline usage during model loading and tracks deviations throughout training. When memory usage exceeds 85% of available GPU memory, the system triggers graduated response protocols. First, it reduces gradient accumulation steps to decrease peak memory usage. If pressure continues, it enables more aggressive gradient clipping to reduce optimizer state size. As a last resort, it forces garbage collection and CUDA cache clearing between training steps.</p>\n<blockquote>\n<p><strong>Critical Design Principle:</strong> Memory management must be proactive rather than reactive. By the time an out-of-memory error occurs, the training state may already be corrupted. Early detection and gradual adaptation prevent catastrophic failures while maintaining training progress.</p>\n</blockquote>\n<p><strong>Memory Failure Recovery Strategies:</strong></p>\n<ol>\n<li><p><strong>Graduated Response Protocol</strong>: The system implements a series of escalating interventions as memory pressure increases. At 85% usage, it logs warnings and reduces batch size. At 90% usage, it forces garbage collection between steps. At 95% usage, it triggers emergency checkpoint saving and prepares for graceful shutdown.</p>\n</li>\n<li><p><strong>Dynamic Batch Size Adaptation</strong>: When memory allocation fails during batch processing, the system automatically reduces the effective batch size by decreasing gradient accumulation steps. This maintains the same learning dynamics while reducing peak memory requirements.</p>\n</li>\n<li><p><strong>Checkpoint-Based Recovery</strong>: If an out-of-memory error forces training termination, the system automatically restores from the most recent checkpoint and resumes with reduced memory settings. This enables continuous training despite memory constraints.</p>\n</li>\n<li><p><strong>Memory Fragmentation Management</strong>: Long-running training jobs can suffer from GPU memory fragmentation. The system periodically clears CUDA cache and defragments memory pools to maintain efficient allocation patterns.</p>\n</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Memory Pressure Level</th>\n<th>Detection Threshold</th>\n<th>Automatic Actions</th>\n<th>Manual Interventions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Normal</td>\n<td>&lt; 80% GPU memory</td>\n<td>Standard operation</td>\n<td>None required</td>\n</tr>\n<tr>\n<td>Elevated</td>\n<td>80-85% GPU memory</td>\n<td>Log warnings, monitor closely</td>\n<td>Consider reducing model size</td>\n</tr>\n<tr>\n<td>High</td>\n<td>85-90% GPU memory</td>\n<td>Reduce gradient accumulation by 25%</td>\n<td>Enable gradient checkpointing</td>\n</tr>\n<tr>\n<td>Critical</td>\n<td>90-95% GPU memory</td>\n<td>Force GC, reduce batch size by 50%</td>\n<td>Emergency checkpoint save</td>\n</tr>\n<tr>\n<td>Emergency</td>\n<td>&gt; 95% GPU memory</td>\n<td>Immediate checkpoint and graceful shutdown</td>\n<td>Restart with lower settings</td>\n</tr>\n</tbody></table>\n<h4 id=\"gpu-driver-and-hardware-recovery\">GPU Driver and Hardware Recovery</h4>\n<p>GPU driver crashes and hardware failures require different recovery strategies than memory management issues. These failures often result in complete loss of GPU state, requiring model reloading and training resumption from disk-based checkpoints.</p>\n<p>The system implements comprehensive hardware failure detection through multiple monitoring channels. CUDA runtime errors indicate driver instability or hardware malfunctions. Thermal monitoring prevents overheating-related failures. Power monitoring detects unstable power delivery that can corrupt GPU state.</p>\n<p><strong>Hardware Failure Types and Recovery:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Failure Type</th>\n<th>Detection Method</th>\n<th>Recovery Strategy</th>\n<th>Prevention Measures</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Driver Crash</td>\n<td>CUDA runtime exceptions</td>\n<td>Restart training process, reload from checkpoint</td>\n<td>Regular driver updates, stability testing</td>\n</tr>\n<tr>\n<td>Thermal Throttling</td>\n<td>GPU temperature monitoring</td>\n<td>Reduce batch size, increase cooling wait times</td>\n<td>Better case ventilation, undervolting</td>\n</tr>\n<tr>\n<td>Power Instability</td>\n<td>Sudden disconnection events</td>\n<td>Resume from last checkpoint with lower power target</td>\n<td>Stable PSU, power monitoring</td>\n</tr>\n<tr>\n<td>Memory Corruption</td>\n<td>Checksum validation failures</td>\n<td>Reload model from disk, skip corrupted batch</td>\n<td>ECC memory when available, lower memory speeds</td>\n</tr>\n<tr>\n<td>Hardware Degradation</td>\n<td>Increased error rates over time</td>\n<td>Automatic fallback to CPU processing for affected operations</td>\n<td>Regular hardware diagnostics</td>\n</tr>\n</tbody></table>\n<p>The hardware recovery system maintains redundant state information to enable rapid restoration. Model weights, optimizer states, and training metadata are stored both in GPU memory and on persistent storage. When hardware failures occur, the system can quickly restore the complete training state without manual intervention.</p>\n<blockquote>\n<p><strong>Decision: Automatic vs Manual Recovery</strong></p>\n<ul>\n<li><strong>Context</strong>: Hardware failures can be transient (driver glitches) or persistent (failing hardware), requiring different recovery approaches</li>\n<li><strong>Options Considered</strong>: Always manual recovery, always automatic recovery, hybrid approach with failure pattern analysis</li>\n<li><strong>Decision</strong>: Implement automatic recovery for transient failures with manual override for persistent issues</li>\n<li><strong>Rationale</strong>: Automatic recovery handles common transient issues without user intervention, while persistent failure detection prevents infinite retry loops</li>\n<li><strong>Consequences</strong>: Faster recovery from common issues, but requires sophisticated failure classification logic</li>\n</ul>\n</blockquote>\n<h4 id=\"power-management-and-thermal-protection\">Power Management and Thermal Protection</h4>\n<p>Power and thermal management prevent hardware damage while maintaining training stability. The system monitors GPU power consumption, temperature, and fan speeds to detect potential hardware stress before it causes failures.</p>\n<p>When thermal limits approach, the system automatically reduces computational load by decreasing batch sizes or introducing cooling delays between training steps. This prevents thermal throttling events that can cause unpredictable performance degradation or hardware damage.</p>\n<p>Power management involves monitoring both instantaneous power consumption and sustained power delivery. Sudden power spikes can indicate unstable power supplies or inadequate cooling. The system can automatically reduce power targets or enable power limiting to maintain stable operation.</p>\n<h3 id=\"training-instability\">Training Instability</h3>\n<p>Training instability manifests through gradient explosions, loss spikes, NaN propagation, and convergence failures. These issues can corrupt model weights, waste computational resources, and prevent successful fine-tuning completion. The pipeline implements comprehensive stability monitoring and automatic correction mechanisms.</p>\n<h4 id=\"gradient-explosion-detection-and-recovery\">Gradient Explosion Detection and Recovery</h4>\n<p>Gradient explosions represent one of the most common training instabilities in language model fine-tuning. Large gradients can corrupt model weights, cause optimizer state overflow, and propagate NaN values throughout the network. The system continuously monitors gradient norms and implements multi-stage intervention protocols.</p>\n<table>\n<thead>\n<tr>\n<th>Gradient Norm Range</th>\n<th>Classification</th>\n<th>Automatic Actions</th>\n<th>Recovery Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>&lt; 1.0</td>\n<td>Normal</td>\n<td>Continue training</td>\n<td>None required</td>\n</tr>\n<tr>\n<td>1.0 - 5.0</td>\n<td>Elevated</td>\n<td>Log warnings, monitor closely</td>\n<td>Consider gradient clipping adjustment</td>\n</tr>\n<tr>\n<td>5.0 - 20.0</td>\n<td>High</td>\n<td>Apply gradient clipping, reduce learning rate temporarily</td>\n<td>Skip current batch if clipping insufficient</td>\n</tr>\n<tr>\n<td>20.0 - 100.0</td>\n<td>Critical</td>\n<td>Emergency gradient clipping, learning rate reduction</td>\n<td>Revert to previous checkpoint if instability persists</td>\n</tr>\n<tr>\n<td>&gt; 100.0</td>\n<td>Explosion</td>\n<td>Skip batch, reload from recent checkpoint</td>\n<td>Investigate data quality and hyperparameter settings</td>\n</tr>\n</tbody></table>\n<p>The gradient monitoring system calculates both global gradient norms and per-layer gradient statistics to identify localized instabilities. When gradients exceed normal ranges, the system can apply targeted interventions rather than global corrections.</p>\n<p><strong>Gradient Explosion Recovery Protocol:</strong></p>\n<ol>\n<li><p><strong>Real-time Monitoring</strong>: The system calculates gradient norms after each backward pass and maintains moving averages to establish stability baselines. Sudden spikes beyond three standard deviations trigger intervention protocols.</p>\n</li>\n<li><p><strong>Adaptive Clipping</strong>: When gradient norms exceed established thresholds, the system automatically adjusts clipping parameters to maintain stability while preserving training dynamics. This prevents the need for manual hyperparameter tuning.</p>\n</li>\n<li><p><strong>Learning Rate Adjustment</strong>: Persistent gradient instability triggers temporary learning rate reduction to improve training stability. The system gradually restores the original learning rate as stability returns.</p>\n</li>\n<li><p><strong>Batch Skipping</strong>: When gradient explosions cannot be controlled through clipping, the system skips the problematic batch and continues with the next training sample. This prevents single bad examples from corrupting the entire training run.</p>\n</li>\n<li><p><strong>Checkpoint Restoration</strong>: If gradient explosions persist despite interventions, the system automatically reverts to the most recent stable checkpoint and resumes training with adjusted hyperparameters.</p>\n</li>\n</ol>\n<h4 id=\"loss-spike-management\">Loss Spike Management</h4>\n<p>Loss spikes indicate sudden degradation in model performance, often caused by bad training examples, learning rate issues, or optimizer state corruption. The system monitors both absolute loss values and relative changes to detect performance degradation.</p>\n<p>When loss spikes occur, the system first determines whether the spike represents a temporary fluctuation or sustained degradation. Temporary spikes may be handled through batch skipping or learning rate adjustment. Sustained degradation typically requires checkpoint restoration.</p>\n<p><strong>Loss Monitoring Strategy:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Loss Change Pattern</th>\n<th>Classification</th>\n<th>Likely Cause</th>\n<th>Recovery Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Single step spike, then recovery</td>\n<td>Transient</td>\n<td>Bad training example</td>\n<td>Continue monitoring, log warning</td>\n</tr>\n<tr>\n<td>Multiple consecutive increases</td>\n<td>Trend degradation</td>\n<td>Learning rate too high</td>\n<td>Reduce learning rate by 50%</td>\n</tr>\n<tr>\n<td>Sudden jump with no recovery</td>\n<td>Performance collapse</td>\n<td>Optimizer state corruption</td>\n<td>Restore from checkpoint</td>\n</tr>\n<tr>\n<td>Gradual increase over epochs</td>\n<td>Overfitting onset</td>\n<td>Insufficient regularization</td>\n<td>Enable early stopping</td>\n</tr>\n<tr>\n<td>Oscillating high/low values</td>\n<td>Training instability</td>\n<td>Batch size or accumulation issues</td>\n<td>Adjust gradient accumulation</td>\n</tr>\n</tbody></table>\n<p>The loss monitoring system maintains both short-term and long-term loss statistics to distinguish between normal training fluctuations and genuine instability. Exponential moving averages provide stable baselines for detecting significant deviations.</p>\n<h4 id=\"nan-and-infinity-propagation\">NaN and Infinity Propagation</h4>\n<p>NaN (Not a Number) and infinity values can propagate through neural networks, corrupting gradients and model weights. Once NaN values enter the computation graph, they typically spread rapidly, making recovery difficult without checkpoint restoration.</p>\n<p>The system implements comprehensive NaN detection at multiple computation stages. Forward pass monitoring catches NaN values in activations before they corrupt gradients. Gradient monitoring detects NaN values in parameter updates before they modify model weights. Loss monitoring identifies NaN values in objective function computation.</p>\n<p><strong>NaN Recovery Protocol:</strong></p>\n<ol>\n<li><p><strong>Early Detection</strong>: The system checks for NaN values in activations, gradients, and loss computations at each training step. Early detection prevents propagation throughout the network.</p>\n</li>\n<li><p><strong>Immediate Isolation</strong>: When NaN values are detected, the system immediately stops gradient computation and parameter updates for the current batch to prevent corruption.</p>\n</li>\n<li><p><strong>Source Identification</strong>: The system analyzes which operations or data examples caused the NaN values to help prevent recurrence.</p>\n</li>\n<li><p><strong>State Restoration</strong>: If NaN values have already corrupted model parameters, the system restores from the most recent checkpoint before NaN propagation began.</p>\n</li>\n<li><p><strong>Prevention Measures</strong>: After NaN recovery, the system implements additional safeguards such as stricter gradient clipping, learning rate reduction, or problematic data filtering.</p>\n</li>\n</ol>\n<blockquote>\n<p><strong>Architecture Decision: NaN Handling Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: NaN values can emerge from various sources (overflow, underflow, invalid operations) and spread rapidly through networks</li>\n<li><strong>Options Considered</strong>: Ignore and continue training, immediate checkpoint restoration, gradual intervention escalation</li>\n<li><strong>Decision</strong>: Implement immediate detection with graduated intervention based on NaN scope and source</li>\n<li><strong>Rationale</strong>: Early detection prevents widespread corruption, while graduated intervention avoids unnecessary training interruption for recoverable issues</li>\n<li><strong>Consequences</strong>: Requires comprehensive monitoring overhead but prevents catastrophic training failures</li>\n</ul>\n</blockquote>\n<h3 id=\"data-quality-issues\">Data Quality Issues</h3>\n<p>Data quality problems can manifest during training rather than preprocessing, requiring runtime detection and handling. Corrupted files, encoding issues, malformed JSON, and inconsistent formatting can interrupt training or degrade model performance. The pipeline implements robust data validation and fallback mechanisms.</p>\n<h4 id=\"runtime-data-validation\">Runtime Data Validation</h4>\n<p>The data loading system performs comprehensive validation beyond initial preprocessing checks. Runtime validation catches corruption that occurs after preprocessing, such as file system errors, network interruptions during streaming, or memory corruption affecting data structures.</p>\n<table>\n<thead>\n<tr>\n<th>Validation Level</th>\n<th>Check Type</th>\n<th>Detection Method</th>\n<th>Recovery Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>File Integrity</td>\n<td>Checksum verification</td>\n<td>Compare stored and computed hashes</td>\n<td>Reload from backup or skip corrupted files</td>\n</tr>\n<tr>\n<td>Format Consistency</td>\n<td>Schema validation</td>\n<td>JSON/JSONL structure verification</td>\n<td>Attempt format correction or skip invalid samples</td>\n</tr>\n<tr>\n<td>Content Quality</td>\n<td>Text analysis</td>\n<td>Length, character encoding, language detection</td>\n<td>Filter problematic samples, apply corrections</td>\n</tr>\n<tr>\n<td>Token Validity</td>\n<td>Tokenization verification</td>\n<td>Check for unknown tokens, length limits</td>\n<td>Re-tokenize with updated tokenizer, truncate oversized samples</td>\n</tr>\n<tr>\n<td>Label Accuracy</td>\n<td>Response validation</td>\n<td>Check instruction-response consistency</td>\n<td>Skip samples with mismatched or corrupted labels</td>\n</tr>\n</tbody></table>\n<p>The validation system maintains statistics on data quality issues to identify systemic problems. High corruption rates may indicate storage issues, network problems, or preprocessing errors that require immediate attention.</p>\n<p><strong>Data Corruption Recovery:</strong></p>\n<ol>\n<li><p><strong>Redundant Storage</strong>: Critical training data is stored with redundant copies and checksums to enable corruption detection and recovery. The system automatically switches to backup copies when corruption is detected.</p>\n</li>\n<li><p><strong>Graceful Degradation</strong>: When data corruption affects small portions of the training set, the system continues training with the remaining valid data rather than terminating the entire process.</p>\n</li>\n<li><p><strong>Real-time Filtering</strong>: The data loader implements real-time quality filtering to remove corrupted or malformed samples during training without interrupting the process.</p>\n</li>\n<li><p><strong>Automatic Reprocessing</strong>: When systematic data corruption is detected, the system can automatically trigger reprocessing of affected data sources with updated validation rules.</p>\n</li>\n</ol>\n<h4 id=\"streaming-data-handling\">Streaming Data Handling</h4>\n<p>For large datasets that exceed memory capacity, streaming data loading introduces additional failure modes. Network interruptions, storage failures, and concurrent access issues can disrupt data flow during training.</p>\n<p>The streaming system implements robust error handling with automatic retry mechanisms, local caching, and fallback data sources. When streaming failures occur, the system can temporarily switch to cached data or alternative sources while attempting to restore the primary stream.</p>\n<p><strong>Streaming Failure Recovery:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Failure Type</th>\n<th>Detection Method</th>\n<th>Immediate Response</th>\n<th>Long-term Resolution</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Network Timeout</td>\n<td>Connection monitoring</td>\n<td>Retry with exponential backoff</td>\n<td>Switch to local cache</td>\n</tr>\n<tr>\n<td>Storage Unavailable</td>\n<td>File system errors</td>\n<td>Attempt alternative paths</td>\n<td>Enable redundant storage</td>\n</tr>\n<tr>\n<td>Concurrent Access</td>\n<td>File locking conflicts</td>\n<td>Queue requests, implement retry logic</td>\n<td>Coordinate access patterns</td>\n</tr>\n<tr>\n<td>Bandwidth Saturation</td>\n<td>Transfer rate monitoring</td>\n<td>Reduce concurrent streams</td>\n<td>Implement traffic shaping</td>\n</tr>\n<tr>\n<td>Data Corruption</td>\n<td>Integrity checking</td>\n<td>Skip corrupted chunks, request retransmission</td>\n<td>Verify storage subsystem health</td>\n</tr>\n</tbody></table>\n<h4 id=\"format-inconsistency-handling\">Format Inconsistency Handling</h4>\n<p>Training datasets often contain samples with inconsistent formatting, especially when combining multiple data sources. The system must handle variations in chat templates, instruction formatting, and response structures without losing valuable training data.</p>\n<p>The format handling system implements automatic format detection and normalization. When inconsistent formats are detected, the system attempts automatic conversion to the standard format. If conversion fails, samples are quarantined for manual review rather than corrupting the training process.</p>\n<h3 id=\"checkpoint-and-state-recovery\">Checkpoint and State Recovery</h3>\n<p>Checkpoint corruption, incomplete saves, and state inconsistencies can prevent training resumption after interruptions. The pipeline implements robust checkpointing with verification, redundancy, and automatic repair mechanisms.</p>\n<h4 id=\"checkpoint-integrity-management\">Checkpoint Integrity Management</h4>\n<p>The checkpointing system ensures data integrity through multiple verification layers. Checksums verify file integrity during save and load operations. Metadata validation confirms checkpoint consistency with training configuration. State verification ensures that restored models produce expected outputs.</p>\n<table>\n<thead>\n<tr>\n<th>Checkpoint Component</th>\n<th>Integrity Check</th>\n<th>Verification Method</th>\n<th>Recovery Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Model Weights</td>\n<td>SHA-256 checksum</td>\n<td>Compare saved and computed hashes</td>\n<td>Restore from backup checkpoint</td>\n</tr>\n<tr>\n<td>Optimizer State</td>\n<td>Size and structure validation</td>\n<td>Verify tensor dimensions and types</td>\n<td>Reinitialize optimizer state if corrupted</td>\n</tr>\n<tr>\n<td>Training Metadata</td>\n<td>JSON schema validation</td>\n<td>Check required fields and data types</td>\n<td>Reconstruct from training logs</td>\n</tr>\n<tr>\n<td>Random State</td>\n<td>Sequence verification</td>\n<td>Test random number generation</td>\n<td>Reseed with deterministic values</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td>Hash comparison</td>\n<td>Verify against original config</td>\n<td>Restore from configuration backup</td>\n</tr>\n</tbody></table>\n<p>The integrity management system maintains multiple checkpoint generations to provide recovery options when the most recent checkpoint is corrupted. Automatic backup rotation ensures that recent good checkpoints are always available.</p>\n<p><strong>Checkpoint Corruption Recovery:</strong></p>\n<ol>\n<li><p><strong>Multi-level Verification</strong>: The system performs integrity checks at multiple levels—file system, binary data, and semantic correctness—to detect different types of corruption.</p>\n</li>\n<li><p><strong>Automatic Repair</strong>: When minor corruption is detected, the system attempts automatic repair using redundant information or reconstructing missing components from available data.</p>\n</li>\n<li><p><strong>Graceful Fallback</strong>: If the most recent checkpoint is corrupted, the system automatically falls back to the previous verified checkpoint and resumes training from that point.</p>\n</li>\n<li><p><strong>Progressive Verification</strong>: During checkpoint loading, the system performs progressive verification to identify exactly which components are corrupted and preserve as much valid state as possible.</p>\n</li>\n</ol>\n<h4 id=\"state-synchronization\">State Synchronization</h4>\n<p>Training state includes not only model weights but also optimizer states, learning rate schedules, random number generators, and data loader positions. Inconsistencies between these components can cause subtle training issues or prevent proper resumption.</p>\n<p>The state synchronization system ensures that all training components remain synchronized across checkpoint save and restore operations. When inconsistencies are detected, the system can either repair the inconsistencies or reinitialize affected components.</p>\n<p><strong>State Component Dependencies:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Dependencies</th>\n<th>Synchronization Requirements</th>\n<th>Recovery Priority</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Model Weights</td>\n<td>None</td>\n<td>Must match training configuration</td>\n<td>Highest - core model state</td>\n</tr>\n<tr>\n<td>Optimizer State</td>\n<td>Model architecture, training step</td>\n<td>Must align with current model parameters</td>\n<td>High - affects convergence</td>\n</tr>\n<tr>\n<td>Learning Rate Schedule</td>\n<td>Training step, epoch count</td>\n<td>Must reflect actual training progress</td>\n<td>Medium - can be reconstructed</td>\n</tr>\n<tr>\n<td>Random State</td>\n<td>Training step, epoch</td>\n<td>Should ensure reproducible behavior</td>\n<td>Low - affects reproducibility only</td>\n</tr>\n<tr>\n<td>Data Loader Position</td>\n<td>Dataset configuration, shuffle state</td>\n<td>Must prevent data duplication or skipping</td>\n<td>Medium - affects training distribution</td>\n</tr>\n</tbody></table>\n<h4 id=\"distributed-training-state-recovery\">Distributed Training State Recovery</h4>\n<p>When training involves multiple GPUs or nodes, state recovery becomes more complex. The system must handle partial failures where some processes succeed while others fail, maintain consistency across distributed components, and coordinate recovery across all participating nodes.</p>\n<p>The distributed recovery system implements coordinator-based recovery protocols. A designated coordinator node manages the recovery process, verifying state consistency across all participants and coordinating synchronized resumption.</p>\n<p><strong>Distributed Recovery Protocol:</strong></p>\n<ol>\n<li><p><strong>Failure Detection</strong>: Each training node monitors the health of other participants and reports failures to the coordinator. The coordinator maintains a global view of system health.</p>\n</li>\n<li><p><strong>State Collection</strong>: When recovery is triggered, the coordinator collects checkpoint information from all surviving nodes and determines the most recent consistent state across the entire system.</p>\n</li>\n<li><p><strong>Consistency Verification</strong>: The coordinator verifies that all nodes have compatible checkpoint states and identifies any inconsistencies that must be resolved before resumption.</p>\n</li>\n<li><p><strong>Synchronized Restart</strong>: The coordinator orchestrates synchronized restart across all nodes, ensuring that training resumes from the same global state on all participants.</p>\n</li>\n<li><p><strong>Health Monitoring</strong>: After resumption, the coordinator continues monitoring node health and checkpoint consistency to detect any ongoing issues.</p>\n</li>\n</ol>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Insufficient Memory Headroom</strong>\nMany implementations monitor memory usage but don&#39;t account for temporary allocations during gradient computation and optimizer updates. This causes unexpected out-of-memory errors even when steady-state memory usage appears safe. Always maintain at least 15% memory headroom and monitor peak allocation patterns, not just average usage.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Error Recovery States</strong>\nWhen recovering from failures, different system components may restore to different temporal states, causing subtle inconsistencies. For example, the model may restore from step 1000 while the learning rate schedule restores from step 1050. Always verify temporal consistency across all training components during recovery.</p>\n<p>⚠️ <strong>Pitfall: Gradient Clipping After Explosion Detection</strong>\nChecking for gradient explosions after applying gradient clipping is ineffective because clipping masks the underlying instability. Monitor raw gradient norms before clipping and adjust clipping parameters dynamically based on explosion frequency.</p>\n<p>⚠️ <strong>Pitfall: Ignoring NaN in Validation Metrics</strong>\nNaN values in validation metrics often indicate training instability, but they&#39;re frequently ignored because validation continues to run. Monitor validation metrics for NaN values and treat them as serious indicators of training problems requiring intervention.</p>\n<p>⚠️ <strong>Pitfall: Checkpoint Frequency vs Recovery Time Trade-offs</strong>\nSaving checkpoints too frequently can significantly slow training, while saving too infrequently increases recovery time after failures. Implement adaptive checkpoint frequency based on training stability and progress rate rather than fixed intervals.</p>\n<p>⚠️ <strong>Pitfall: Hardware-Specific Recovery Assumptions</strong>\nRecovery strategies often assume specific hardware behaviors (e.g., CUDA error types, memory allocation patterns) that don&#39;t generalize across different GPU models or drivers. Implement hardware-agnostic error detection with hardware-specific recovery adaptations.</p>\n<p>⚠️ <strong>Pitfall: Data Corruption Propagation Through Caches</strong>\nCorrupted data can persist in various caching layers (tokenizer caches, data loader buffers, model attention caches) even after the original corruption is detected. Implement comprehensive cache invalidation when data corruption is detected.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Memory Monitoring</td>\n<td><code>torch.cuda.memory_allocated()</code> with periodic logging</td>\n<td>Custom CUDA memory profiler with real-time alerts</td>\n</tr>\n<tr>\n<td>Error Logging</td>\n<td>Python <code>logging</code> module with file handlers</td>\n<td>Structured logging with ELK stack integration</td>\n</tr>\n<tr>\n<td>Checkpoint Storage</td>\n<td>Local filesystem with file locking</td>\n<td>Distributed storage with consistency guarantees</td>\n</tr>\n<tr>\n<td>Health Monitoring</td>\n<td>Simple heartbeat signals</td>\n<td>Comprehensive metrics collection with Prometheus</td>\n</tr>\n<tr>\n<td>Recovery Coordination</td>\n<td>File-based coordination</td>\n<td>Distributed consensus with etcd or Consul</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>src/\n  pipeline/\n    error_handling/\n      __init__.py\n      memory_monitor.py          ← Memory pressure detection and adaptive management\n      training_stability.py     ← Gradient explosion and loss spike handling  \n      data_validation.py        ← Runtime data quality checking and recovery\n      checkpoint_manager.py     ← Checkpoint integrity and state recovery\n      hardware_monitor.py       ← GPU health monitoring and failure detection\n      recovery_coordinator.py   ← Orchestrates recovery across components\n    utils/\n      error_types.py            ← Custom exception classes for different failure modes\n      monitoring_utils.py       ← Shared monitoring and alerting utilities\n      state_utils.py           ← State synchronization and verification helpers</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p>Here&#39;s a complete memory monitoring implementation that tracks GPU memory usage and implements graduated response protocols:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> psutil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MemoryMetrics</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Memory usage measurements at a specific point in time\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stage: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gpu_memory_allocated: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gpu_memory_cached: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gpu_memory_reserved: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    system_memory_used: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    details: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MemoryMonitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Monitors GPU and system memory usage with adaptive management\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    baseline_gpu_memory: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    baseline_system_memory: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    measurements: List[MemoryMetrics] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _lock: threading.Lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">threading.Lock)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _alert_thresholds: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'warning'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.8</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'high'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.85</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'critical'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.9</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'emergency'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.95</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> capture_baseline</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record initial memory state before model loading\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                torch.cuda.empty_cache()  </span><span style=\"color:#6A737D\"># Clear any existing allocations</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.baseline_gpu_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_allocated()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            process </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.Process()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.baseline_system_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> process.memory_info().rss</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logging.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Memory baseline captured - GPU: </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.baseline_gpu_memory </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1e9</span><span style=\"color:#F97583\">:.2f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">GB, \"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                        f</span><span style=\"color:#9ECBFF\">\"System: </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.baseline_system_memory </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1e9</span><span style=\"color:#F97583\">:.2f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">GB\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> measure_current_usage</span><span style=\"color:#E1E4E8\">(self, stage: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Measure current memory usage and return statistics\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                gpu_allocated </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_allocated()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                gpu_cached </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_cached()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                gpu_reserved </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_reserved()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                gpu_allocated </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> gpu_cached </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> gpu_reserved </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            process </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.Process()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            system_used </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> process.memory_info().rss</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            metrics </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MemoryMetrics(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                stage</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">stage,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                gpu_memory_allocated</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">gpu_allocated,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                gpu_memory_cached</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">gpu_cached,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                gpu_memory_reserved</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">gpu_reserved,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                system_memory_used</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">system_used,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">timestamp,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                details</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'gpu_utilization'</span><span style=\"color:#E1E4E8\">: gpu_allocated </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> torch.cuda.max_memory_allocated() </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available() </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'system_utilization'</span><span style=\"color:#E1E4E8\">: system_used </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> psutil.virtual_memory().total</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.measurements.append(metrics)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Check for memory pressure and trigger alerts</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._check_memory_pressure(metrics)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'gpu_allocated_gb'</span><span style=\"color:#E1E4E8\">: gpu_allocated </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1e9</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'gpu_cached_gb'</span><span style=\"color:#E1E4E8\">: gpu_cached </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1e9</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'system_used_gb'</span><span style=\"color:#E1E4E8\">: system_used </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1e9</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'gpu_utilization'</span><span style=\"color:#E1E4E8\">: metrics.details[</span><span style=\"color:#9ECBFF\">'gpu_utilization'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'system_utilization'</span><span style=\"color:#E1E4E8\">: metrics.details[</span><span style=\"color:#9ECBFF\">'system_utilization'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_peak_usage</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return peak memory usage across all measurements\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.measurements:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        peak_gpu </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(m.gpu_memory_allocated </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> m </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.measurements)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        peak_system </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(m.system_memory_used </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> m </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.measurements)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'peak_gpu_gb'</span><span style=\"color:#E1E4E8\">: peak_gpu </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1e9</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'peak_system_gb'</span><span style=\"color:#E1E4E8\">: peak_system </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1e9</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'peak_gpu_utilization'</span><span style=\"color:#E1E4E8\">: peak_gpu </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> torch.cuda.max_memory_allocated() </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available() </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _check_memory_pressure</span><span style=\"color:#E1E4E8\">(self, metrics: MemoryMetrics) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check for memory pressure and trigger appropriate responses\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        gpu_utilization </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> metrics.details[</span><span style=\"color:#9ECBFF\">'gpu_utilization'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> gpu_utilization </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._alert_thresholds[</span><span style=\"color:#9ECBFF\">'emergency'</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logging.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"EMERGENCY: GPU memory usage at </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">gpu_utilization</span><span style=\"color:#F97583\">:.1%</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> - immediate action required\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Trigger emergency protocols</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> gpu_utilization </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._alert_thresholds[</span><span style=\"color:#9ECBFF\">'critical'</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logging.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"CRITICAL: GPU memory usage at </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">gpu_utilization</span><span style=\"color:#F97583\">:.1%</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> - reducing batch size\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Trigger batch size reduction</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> gpu_utilization </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._alert_thresholds[</span><span style=\"color:#9ECBFF\">'high'</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logging.warning(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"HIGH: GPU memory usage at </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">gpu_utilization</span><span style=\"color:#F97583\">:.1%</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> - monitoring closely\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Increase monitoring frequency</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> gpu_utilization </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._alert_thresholds[</span><span style=\"color:#9ECBFF\">'warning'</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logging.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"WARNING: GPU memory usage at </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">gpu_utilization</span><span style=\"color:#F97583\">:.1%</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p>Here&#39;s a comprehensive checkpoint integrity manager:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> hashlib</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> shutil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional, List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CheckpointMetadata</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Metadata about a saved checkpoint\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    checkpoint_path: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    step: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    epoch: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    eval_loss: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    is_best: </span><span style=\"color:#79B8FF\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    model_config: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    adapter_config: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    optimizer_state_size: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    save_timestamp: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CheckpointManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manages checkpoint saving, loading, and integrity verification\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, checkpoint_dir: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, max_checkpoints: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.checkpoint_dir </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Path(checkpoint_dir)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.checkpoint_dir.mkdir(</span><span style=\"color:#FFAB70\">parents</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">exist_ok</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_checkpoints </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_checkpoints</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> save_checkpoint</span><span style=\"color:#E1E4E8\">(self, model, optimizer, step: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, epoch: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                       eval_loss: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, is_best: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">) -> CheckpointMetadata:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Save checkpoint with integrity verification\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create checkpoint filename with timestamp and step number</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Save model state dict with error handling for disk space</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Save optimizer state dict separately for memory efficiency</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Generate and save integrity checksums for all files</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Create checkpoint metadata JSON with all relevant information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Verify saved files can be loaded correctly before considering save complete</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Clean up old checkpoints if maximum count exceeded</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Return CheckpointMetadata object with save information</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> load_checkpoint</span><span style=\"color:#E1E4E8\">(self, checkpoint_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load checkpoint with integrity verification\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Verify checkpoint directory exists and contains required files</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Load and validate checkpoint metadata JSON</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Verify file integrity using saved checksums</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Load model state dict with error handling for corrupted weights</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Load optimizer state dict with compatibility checking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Validate loaded state dimensions match expected model architecture</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return complete checkpoint state or raise appropriate exceptions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> verify_checkpoint_integrity</span><span style=\"color:#E1E4E8\">(self, checkpoint_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Verify checkpoint files are not corrupted\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check that all required checkpoint files exist</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate JSON metadata structure and required fields</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compute and compare file checksums against saved values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Attempt to load state dicts without applying to verify format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return True if all integrity checks pass, False otherwise</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> find_latest_valid_checkpoint</span><span style=\"color:#E1E4E8\">(self) -> Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Find the most recent checkpoint that passes integrity checks\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: List all checkpoint directories sorted by creation time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check each checkpoint starting from most recent</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return path to first checkpoint that passes integrity verification</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return None if no valid checkpoints found</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p>Here&#39;s the training stability monitor that detects and handles gradient explosions and loss spikes:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> collections </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> deque</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TrainingStabilityMonitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Monitors training for instabilities and implements recovery strategies\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, gradient_clip_norm: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">, loss_spike_threshold: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2.0</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.gradient_clip_norm </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> gradient_clip_norm</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.loss_spike_threshold </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> loss_spike_threshold</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.gradient_history </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> deque(</span><span style=\"color:#FFAB70\">maxlen</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.loss_history </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> deque(</span><span style=\"color:#FFAB70\">maxlen</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">) </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.nan_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.explosion_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> check_gradient_stability</span><span style=\"color:#E1E4E8\">(self, model: torch.nn.Module) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Monitor gradient norms and detect explosions\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Calculate total gradient norm across all model parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate per-layer gradient norms for localized explosion detection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compare current gradient norm to historical statistics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Detect gradient explosions using threshold and statistical analysis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Check for NaN or infinite values in gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Update gradient history statistics for future comparisons</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return stability report with recommendations for intervention</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use torch.nn.utils.clip_grad_norm_ for gradient norm calculation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> handle_gradient_explosion</span><span style=\"color:#E1E4E8\">(self, model: torch.nn.Module, optimizer: torch.optim.Optimizer) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Handle detected gradient explosion with graduated intervention\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Log gradient explosion event with severity assessment</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Apply emergency gradient clipping with reduced threshold</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Optionally skip optimizer step if explosion is severe</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Adjust learning rate temporarily to improve stability  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Clear problematic gradients and reset gradient accumulation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Update explosion count and consider checkpoint restoration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return True if training can continue, False if recovery needed</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> check_loss_stability</span><span style=\"color:#E1E4E8\">(self, current_loss: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Monitor loss values for spikes and degradation\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Add current loss to historical tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate loss statistics (mean, std, percentiles) over recent history</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Detect loss spikes using threshold and statistical analysis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Identify loss degradation trends using moving averages</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Check for NaN or infinite loss values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Assess whether loss spike indicates temporary or sustained issues</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return stability assessment with recommended recovery actions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> detect_nan_propagation</span><span style=\"color:#E1E4E8\">(self, model: torch.nn.Module, loss: torch.Tensor) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Detect NaN values in model parameters, gradients, or loss\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check loss tensor for NaN or infinite values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Scan model parameters for NaN or infinite weights</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check gradients for NaN or infinite values if they exist</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Identify which layers or operations introduced NaN values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Log detailed information about NaN source and scope</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Update NaN occurrence statistics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return True if NaN detected, False otherwise</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> should_restore_checkpoint</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Determine if training instability requires checkpoint restoration\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Assess frequency and severity of recent gradient explosions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Evaluate sustained loss degradation patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check NaN occurrence frequency and propagation scope</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Consider cumulative instability indicators over training history</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return True if checkpoint restoration recommended, False otherwise</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints\">Language-Specific Hints</h4>\n<p><strong>Memory Management in PyTorch:</strong></p>\n<ul>\n<li>Use <code>torch.cuda.empty_cache()</code> to clear GPU memory cache, but note this only releases cached memory, not actively allocated memory</li>\n<li>Monitor both <code>torch.cuda.memory_allocated()</code> (actively used) and <code>torch.cuda.memory_cached()</code> (available for reuse)</li>\n<li>Use <code>torch.cuda.memory_summary()</code> for detailed memory debugging information</li>\n<li>Enable <code>PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512</code> to reduce memory fragmentation</li>\n</ul>\n<p><strong>Error Handling Best Practices:</strong></p>\n<ul>\n<li>Catch specific CUDA exceptions like <code>torch.cuda.OutOfMemoryError</code> rather than generic exceptions</li>\n<li>Use context managers for resource cleanup: <code>with torch.cuda.device(gpu_id):</code></li>\n<li>Implement exponential backoff for transient failures: <code>time.sleep(min(2**attempt, 60))</code></li>\n<li>Log stack traces with <code>logging.exception()</code> to preserve error context</li>\n</ul>\n<p><strong>Checkpoint Safety:</strong></p>\n<ul>\n<li>Use atomic writes by saving to temporary files then renaming: <code>torch.save(state, temp_path); os.rename(temp_path, final_path)</code></li>\n<li>Verify disk space before saving: <code>shutil.disk_usage(checkpoint_dir).free &gt; estimated_size</code></li>\n<li>Use <code>pickle_protocol=4</code> for better compatibility across Python versions</li>\n<li>Save checksums separately: <code>hashlib.sha256(checkpoint_data).hexdigest()</code></li>\n</ul>\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing error handling components, verify the following behavior:</p>\n<p><strong>Memory Monitoring Verification:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run training with memory monitoring enabled</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> train.py</span><span style=\"color:#79B8FF\"> --enable-memory-monitoring</span><span style=\"color:#79B8FF\"> --log-level</span><span style=\"color:#9ECBFF\"> DEBUG</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected output should show:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Baseline memory measurements before model loading</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Regular memory usage reports during training</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Warnings when memory usage exceeds thresholds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Automatic batch size reduction under memory pressure</span></span></code></pre></div>\n\n<p><strong>Error Recovery Testing:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test checkpoint recovery</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> test_recovery.py</span><span style=\"color:#79B8FF\"> --simulate-failure</span><span style=\"color:#79B8FF\"> --checkpoint-path</span><span style=\"color:#9ECBFF\"> ./checkpoints/</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected behavior:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Training resumes from last valid checkpoint</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Model produces consistent outputs before and after recovery</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Optimizer state restored correctly with matching learning rates</span></span></code></pre></div>\n\n<p><strong>Training Stability Validation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Monitor training stability with problematic data</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> train.py</span><span style=\"color:#79B8FF\"> --unstable-data</span><span style=\"color:#79B8FF\"> --enable-stability-monitoring</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected automatic handling:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Gradient explosions detected and handled with clipping adjustments</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Loss spikes logged with appropriate recovery actions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - NaN values caught before propagating through the network</span></span></code></pre></div>\n\n<p><strong>Signs of Proper Implementation:</strong></p>\n<ul>\n<li>Training continues smoothly despite memory pressure and hardware hiccups</li>\n<li>Recovery from failures happens automatically without manual intervention</li>\n<li>Detailed logs provide clear information about what went wrong and how it was handled</li>\n<li>Checkpoint integrity verification prevents loading of corrupted states</li>\n</ul>\n<p><strong>Common Issues and Fixes:</strong></p>\n<ul>\n<li>If memory monitoring doesn&#39;t trigger: Check that thresholds are set appropriately for your hardware</li>\n<li>If checkpoint recovery fails: Verify file permissions and disk space availability  </li>\n<li>If gradient explosions aren&#39;t caught: Ensure monitoring happens before gradient clipping</li>\n<li>If NaN detection is too slow: Implement sampling-based checks rather than full parameter scans</li>\n</ul>\n<h2 id=\"testing-strategy\">Testing Strategy</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - comprehensive testing validates the correctness, reliability, and performance of each component and their interactions throughout the fine-tuning pipeline</p>\n</blockquote>\n<p>Testing a parameter-efficient fine-tuning pipeline presents unique challenges that traditional software testing approaches don&#39;t fully address. Unlike conventional applications where we can easily verify outputs against expected values, machine learning systems involve probabilistic behaviors, hardware-dependent performance characteristics, and complex interactions between numerical computations that can lead to subtle but critical failures.</p>\n<h3 id=\"mental-model-the-quality-assurance-laboratory\">Mental Model: The Quality Assurance Laboratory</h3>\n<p>Think of our testing strategy as operating a multi-layered quality assurance laboratory for a precision manufacturing process. Just as a manufacturing QA lab has different stations - incoming material inspection, component testing, assembly verification, final product evaluation, and performance benchmarking - our testing approach operates at multiple levels of granularity and concern.</p>\n<p>The <strong>unit testing station</strong> examines individual components in isolation, like testing whether a single gear meshes properly or a sensor reads accurately. The <strong>integration testing station</strong> verifies that multiple components work together correctly, similar to testing whether the gear assembly rotates smoothly under load. The <strong>end-to-end testing station</strong> validates the complete manufacturing pipeline, ensuring that raw materials transform into finished products meeting all specifications. Finally, the <strong>performance testing station</strong> measures whether the entire system operates within acceptable speed, resource, and quality parameters under various operating conditions.</p>\n<p>What makes ML system testing particularly challenging is that we&#39;re not just testing deterministic mechanical processes - we&#39;re testing systems that involve probabilistic computations, hardware-dependent optimizations, and emergent behaviors that only become apparent when components interact under realistic workloads.</p>\n<h2 id=\"unit-testing-strategy\">Unit Testing Strategy</h2>\n<p>Unit testing in the context of LLM fine-tuning focuses on validating individual components in complete isolation from the complexities of model loading, GPU memory management, and distributed computation. Each component should be testable with predictable inputs and verifiable outputs, allowing us to catch logic errors, configuration problems, and boundary condition failures before they manifest in expensive training runs.</p>\n<h3 id=\"core-component-testing-framework\">Core Component Testing Framework</h3>\n<p>The foundation of our unit testing approach rests on the principle that every component should be testable with mock dependencies and synthetic data that capture the essential characteristics of real training scenarios without requiring actual model weights or GPU resources. This enables fast, deterministic testing that can run in continuous integration environments.</p>\n<p>Our testing framework establishes several categories of unit tests, each targeting different aspects of component behavior:</p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Focus Area</th>\n<th>Example Components</th>\n<th>Key Validation Points</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Structure Validation</td>\n<td>Configuration objects and data formats</td>\n<td><code>LoRAConfig</code>, <code>QuantizationConfig</code>, <code>InstructionSample</code></td>\n<td>Field validation, type checking, constraint enforcement</td>\n</tr>\n<tr>\n<td>Transformation Logic</td>\n<td>Data processing and formatting functions</td>\n<td><code>apply_template</code>, <code>tokenize_sample</code>, <code>format_sample</code></td>\n<td>Input-output correctness, edge case handling, error propagation</td>\n</tr>\n<tr>\n<td>Analysis and Calculation</td>\n<td>Computational functions without model dependencies</td>\n<td><code>analyze_parameter_distribution</code>, <code>calculate_efficiency_metrics</code></td>\n<td>Mathematical correctness, numerical precision, boundary conditions</td>\n</tr>\n<tr>\n<td>Configuration Generation</td>\n<td>Dynamic configuration creation</td>\n<td><code>from_task_complexity</code>, <code>recommend_rank_for_task</code></td>\n<td>Logic correctness, constraint satisfaction, consistency</td>\n</tr>\n</tbody></table>\n<h3 id=\"data-structure-and-configuration-testing\">Data Structure and Configuration Testing</h3>\n<p>Configuration objects form the backbone of the entire pipeline, and errors in configuration validation can lead to training failures that only manifest after expensive model loading and initialization. Our unit tests for configuration objects validate both positive and negative cases comprehensively.</p>\n<p>The <code>LoRAConfig</code> testing suite verifies that rank and alpha parameters satisfy mathematical constraints, target modules are properly validated against known patterns, and edge cases like zero rank or negative alpha are handled gracefully. We test configuration serialization and deserialization to ensure that saved configurations can be reliably restored, and we verify that the <code>effective_alpha</code> calculation produces mathematically correct scaling factors under various parameter combinations.</p>\n<p>For <code>QuantizationConfig</code> objects, tests verify that quantization type strings map to valid bitsandbytes configurations, compute dtype selections are compatible with the specified quantization format, and memory estimation functions produce reasonable predictions for various model sizes. The testing includes validation of the <code>to_bitsandbytes_config</code> method to ensure that our internal configuration representation correctly translates to the format expected by the underlying quantization library.</p>\n<p>Data structure testing for <code>InstructionSample</code> and <code>ConversationSample</code> objects focuses on validation logic, ensuring that required fields are present, conversation turns maintain proper role sequencing, and quality scores fall within expected ranges. These tests use synthetic conversation data that captures the diversity of real training scenarios while remaining deterministic and fast to execute.</p>\n<h3 id=\"transformation-and-processing-logic-testing\">Transformation and Processing Logic Testing</h3>\n<p>The data transformation pipeline contains numerous functions that convert between different representations of training data. These transformations must be tested exhaustively because errors in tokenization, template application, or format conversion can silently corrupt training data, leading to poor model performance that&#39;s difficult to diagnose.</p>\n<p>Template application testing uses a comprehensive suite of instruction-response pairs that exercise different conversation patterns, special character handling, and edge cases like empty responses or extremely long instructions. The test suite includes validation of chat template consistency, ensuring that the same logical conversation structure produces identical formatted output across multiple invocations.</p>\n<p>Tokenization testing validates the <code>tokenize_sample</code> function against known tokenizer behaviors, ensuring that attention masks are correctly generated, special tokens are properly inserted, and length constraints are enforced consistently. These tests use pre-computed expected outputs for specific tokenizer configurations, allowing us to detect regressions in tokenization logic or changes in underlying tokenizer behavior.</p>\n<p>The data loading and format conversion functions are tested with synthetic datasets that include various edge cases: malformed JSON records, missing required fields, inconsistent field types, and boundary conditions like empty datasets or extremely large individual samples. These tests validate both the happy path behavior and the error handling characteristics of the data ingestion pipeline.</p>\n<h3 id=\"analysis-and-calculation-testing\">Analysis and Calculation Testing</h3>\n<p>Mathematical and analytical functions within the pipeline must be tested for numerical correctness, handling of edge cases, and consistency across different input scales. These functions often involve floating-point computations that can be sensitive to numerical precision issues or input scaling problems.</p>\n<p>Parameter analysis functions like <code>analyze_parameter_distribution</code> are tested with synthetic model structures that have known parameter counts and distributions. Test cases include models with different architectures, unusual parameter distributions, and edge cases like models with no trainable parameters or extremely sparse parameter structures.</p>\n<p>Memory estimation functions are validated against empirically measured memory usage patterns, with test cases covering different model sizes, quantization configurations, and LoRA parameter combinations. While these tests can&#39;t perfectly predict actual memory usage due to hardware-dependent factors, they validate the mathematical correctness of the estimation algorithms and ensure consistency across different configuration scenarios.</p>\n<p>Efficiency calculation functions are tested for mathematical correctness using synthetic parameter distributions with known expected outcomes. These tests validate that efficiency ratios are calculated correctly, parameter counts are aggregated properly, and edge cases like division by zero or negative parameter counts are handled gracefully.</p>\n<h3 id=\"mock-infrastructure-and-test-utilities\">Mock Infrastructure and Test Utilities</h3>\n<p>Supporting the unit test suite requires a comprehensive mocking infrastructure that can simulate the interfaces and behaviors of external dependencies without requiring actual GPU resources or network access. This infrastructure enables fast, deterministic testing while maintaining high fidelity to actual component interactions.</p>\n<p>The mock tokenizer implementation provides deterministic tokenization behavior for test scenarios, with configurable vocabulary sizes, special token handling, and length constraints. This allows tokenization tests to focus on the logic of our tokenization pipeline rather than the specific behavior of different tokenizer implementations.</p>\n<p>Mock model objects simulate the interface characteristics of actual transformer models, providing parameter iteration, module inspection, and basic forward pass capabilities without requiring actual model weights. These mock objects enable testing of model analysis functions, target module detection, and parameter counting logic.</p>\n<p>The test data generation utilities create synthetic datasets with controllable characteristics: specific token length distributions, conversation turn patterns, quality score ranges, and error injection capabilities. This synthetic data enables comprehensive testing of data processing pipelines while maintaining reproducibility and fast execution times.</p>\n<h2 id=\"integration-and-end-to-end-testing\">Integration and End-to-End Testing</h2>\n<p>Integration testing validates the correctness of component interactions and data flow between different parts of the fine-tuning pipeline. Unlike unit tests that isolate individual functions, integration tests verify that components work together correctly, handle shared state appropriately, and propagate errors and configuration changes properly through the system.</p>\n<h3 id=\"component-integration-testing\">Component Integration Testing</h3>\n<p>The first level of integration testing focuses on pairs and small groups of components, verifying that their interfaces align correctly and that data transformations preserve essential properties across component boundaries. These tests use larger-scale test data than unit tests but still avoid loading full-scale models or executing actual training loops.</p>\n<p>Data preparation and tokenization integration testing verifies the complete flow from raw data files through chat template application to final tokenized datasets. These tests use realistic data samples that exercise the full range of supported input formats and conversation patterns. The tests validate that data quality statistics are computed correctly, that train-validation splits maintain proper distributions, and that tokenization parameters are applied consistently across the entire dataset.</p>\n<p>LoRA configuration and model analysis integration testing verifies that target module detection works correctly with actual model architectures, that rank and alpha parameter recommendations are consistent with memory constraints, and that the adapter injection process preserves model structure. These tests use smaller pre-trained models that can be loaded quickly while still exercising the full adapter configuration pipeline.</p>\n<p>Quantization and memory monitoring integration testing validates that memory usage predictions align with actual memory consumption, that quantization configurations are applied correctly, and that the combination of quantization and LoRA adapters produces expected memory efficiency gains. These tests require GPU resources but use smaller models to minimize execution time while maintaining test coverage.</p>\n<h3 id=\"pipeline-state-management-testing\">Pipeline State Management Testing</h3>\n<p>The fine-tuning pipeline maintains complex shared state across components, including configuration objects, intermediate data products, and training progress information. Integration tests validate that this state management works correctly under various execution scenarios and failure modes.</p>\n<p>State coordination testing verifies that the <code>StateManager</code> correctly handles component transitions, maintains consistency of shared data, and provides proper isolation between concurrent operations. These tests exercise various execution patterns: normal forward progress, component restart scenarios, and error recovery situations.</p>\n<p>Configuration propagation testing ensures that changes to configuration objects are properly reflected across all dependent components. When a user modifies LoRA rank parameters, for example, the tests verify that memory estimates are updated, target module configurations are recalculated, and training loop parameters are adjusted accordingly.</p>\n<p>Checkpoint and recovery testing validates that training state can be saved and restored correctly, including model weights, optimizer states, LoRA adapter parameters, and training progress metadata. These tests exercise various checkpoint timing scenarios and verify that restored training runs continue exactly where they left off.</p>\n<h3 id=\"data-flow-validation-testing\">Data Flow Validation Testing</h3>\n<p>End-to-end data flow testing traces training samples through the complete pipeline transformation sequence, from raw input data to final model outputs, ensuring that data integrity is preserved and that transformations are applied correctly at each stage.</p>\n<p>The complete data flow test suite loads realistic training datasets, applies the full data preparation pipeline, configures LoRA adapters and quantization, and executes abbreviated training runs with thorough validation at each step. These tests verify that tokenized data maintains proper alignment with original instruction-response pairs, that gradient computations are applied only to adapter parameters, and that evaluation metrics reflect expected model behavior changes.</p>\n<p>Cross-component data consistency testing validates that shared data structures remain consistent as they pass between components. For example, when <code>TokenizedSample</code> objects are created by the data preparation component, the integration tests verify that the tokenization parameters used are identical to those expected by the training loop, and that attention masks are compatible with the model architecture configuration.</p>\n<p>Error propagation testing validates that errors occurring in any component are properly caught, contextualized, and propagated through the system with sufficient information for debugging. These tests inject various types of failures - configuration errors, data corruption, resource exhaustion - and verify that the resulting error messages provide actionable diagnostic information.</p>\n<h3 id=\"resource-management-integration-testing\">Resource Management Integration Testing</h3>\n<p>The fine-tuning pipeline must coordinate access to limited GPU memory and compute resources across multiple components. Integration testing validates that this resource management works correctly under various load scenarios and resource constraints.</p>\n<p>Memory coordination testing verifies that memory usage predictions align with actual consumption, that components properly release GPU memory when finished, and that the combination of quantization and LoRA adapters achieves expected efficiency gains. These tests exercise various model sizes and configuration combinations to validate resource management across different scales.</p>\n<p>Compute resource sharing testing validates that components properly coordinate access to GPU compute resources, that operations are properly queued and executed in sequence when necessary, and that resource contention doesn&#39;t lead to deadlocks or performance degradation.</p>\n<p>Hardware failure simulation testing verifies that the pipeline handles various hardware-related failures gracefully: GPU out-of-memory conditions, CUDA driver issues, thermal throttling events, and temporary network connectivity problems when accessing model repositories.</p>\n<h2 id=\"milestone-validation-checkpoints\">Milestone Validation Checkpoints</h2>\n<p>Each milestone in the fine-tuning pipeline development process requires specific validation checkpoints that verify not only the correctness of individual components but also their readiness for integration with subsequent milestone deliverables. These checkpoints serve as quality gates that prevent architectural problems from propagating to later stages of development.</p>\n<h3 id=\"milestone-1-dataset-preparation-validation\">Milestone 1: Dataset Preparation Validation</h3>\n<p>The dataset preparation milestone validation focuses on verifying that training data is correctly loaded, formatted, and prepared for consumption by the training pipeline. This checkpoint must validate both the technical correctness of the data transformations and the statistical properties of the resulting datasets.</p>\n<p>Data loading validation verifies that all supported input formats (JSON, JSONL, CSV, Parquet) are correctly parsed and converted to standardized <code>InstructionSample</code> objects. The validation process tests various file sizes, encoding formats, and data quality scenarios to ensure robust data ingestion. Expected behavior includes proper handling of malformed records, consistent field mapping across formats, and accurate preservation of metadata through the loading process.</p>\n<p>Chat template application validation ensures that instruction-response pairs are correctly formatted according to the target model&#39;s expected conversation format. The checkpoint validates that special tokens are inserted correctly, that multi-turn conversations maintain proper role sequencing, and that template application is deterministic and reversible. Tests include comparison of template outputs against known-good examples and validation of template consistency across different conversation lengths and complexity levels.</p>\n<p>Tokenization validation verifies that text is correctly converted to token sequences with proper attention masks, padding, and truncation handling. The checkpoint includes validation of token length distributions, verification that tokenization parameters are applied consistently, and testing of boundary conditions like empty inputs or extremely long sequences. Expected outputs include correctly formatted <code>TokenizedSample</code> objects with token counts that match statistical expectations for the input data.</p>\n<p>Train-validation split validation ensures that data partitioning maintains proper distribution characteristics and prevents data leakage between training and validation sets. The checkpoint verifies that stratification parameters are applied correctly, that split ratios match configuration specifications, and that the resulting datasets have appropriate statistical properties for effective training and evaluation.</p>\n<h3 id=\"milestone-2-lora-configuration-validation\">Milestone 2: LoRA Configuration Validation</h3>\n<p>LoRA configuration validation focuses on verifying that adapter setup produces the expected parameter efficiency gains while maintaining sufficient model capacity for effective fine-tuning. This checkpoint validates both the mathematical correctness of the LoRA implementation and the practical effectiveness of the configuration choices.</p>\n<p>Target module detection validation verifies that the automatic identification of suitable layers for LoRA adapter injection works correctly across different model architectures. The checkpoint tests detection logic against various transformer architectures, validates that detected modules have appropriate dimensions for low-rank decomposition, and ensures that module selection follows established best practices for parameter-efficient fine-tuning.</p>\n<p>Rank and alpha parameter validation ensures that the selected hyperparameters produce reasonable trade-offs between model capacity and parameter efficiency. The checkpoint includes validation of the mathematical relationship between rank, alpha, and effective learning rate scaling, verification that parameter count reductions meet efficiency targets, and testing of rank selection logic under various memory constraint scenarios.</p>\n<p>Adapter injection validation verifies that LoRA matrices are correctly inserted into the target model layers and that the resulting model maintains expected forward pass behavior. The checkpoint includes comparison of model outputs before and after adapter injection (which should be identical with untrained adapters), validation of parameter freezing for base model weights, and verification that gradient computation is limited to adapter parameters only.</p>\n<p>Parameter efficiency validation confirms that the configured LoRA setup achieves the expected parameter count reduction while maintaining sufficient capacity for the target fine-tuning task. The checkpoint measures actual trainable parameter counts, validates efficiency ratios against targets, and verifies that memory usage predictions align with practical measurements.</p>\n<h3 id=\"milestone-3-qlora-quantization-validation\">Milestone 3: QLoRA Quantization Validation</h3>\n<p>Quantization validation focuses on verifying that 4-bit quantization achieves significant memory reduction while maintaining acceptable model quality and training stability. This checkpoint requires actual GPU testing to validate memory usage and numerical precision characteristics.</p>\n<p>NormalFloat quantization validation verifies that 4-bit quantization is correctly applied to model weights and that the resulting quantized model maintains reasonable output quality compared to the full-precision baseline. The checkpoint includes comparison of model outputs at various quantization configurations, validation of quantization parameter settings, and measurement of actual memory usage reduction achieved.</p>\n<p>Mixed-precision training validation ensures that the combination of 4-bit weight storage with float16/bfloat16 computation produces stable training dynamics without significant accuracy degradation. The checkpoint includes validation of forward pass correctness, gradient computation stability, and compatibility between quantization settings and training loop configuration.</p>\n<p>Memory efficiency validation measures actual GPU memory usage with different quantization configurations and validates that usage aligns with theoretical predictions. The checkpoint includes testing with various model sizes, validation of memory usage under different batch sizes and sequence lengths, and verification that quantization achieves targeted memory reduction goals.</p>\n<p>Double quantization validation verifies that the optional double quantization feature provides additional memory savings without compromising training stability or model quality. The checkpoint includes measurement of incremental memory savings, validation of numerical precision preservation, and testing of compatibility with various quantization configurations.</p>\n<h3 id=\"milestone-4-training-loop-validation\">Milestone 4: Training Loop Validation</h3>\n<p>Training loop validation focuses on verifying that the fine-tuning process produces expected learning dynamics, maintains training stability, and generates reliable checkpoints for model recovery and evaluation. This checkpoint requires extended testing with realistic training scenarios.</p>\n<p>Gradient accumulation validation ensures that simulated larger batch sizes through gradient accumulation produce training dynamics equivalent to actual large batch training. The checkpoint includes comparison of loss curves with different accumulation strategies, validation of learning rate scaling relationships, and verification that effective batch size calculations are mathematically correct.</p>\n<p>Learning rate scheduling validation verifies that warmup, decay, and other scheduling strategies produce expected learning dynamics and convergence characteristics. The checkpoint includes testing of various scheduling configurations, validation of learning rate transitions, and comparison of convergence behavior against established fine-tuning best practices.</p>\n<p>Checkpoint management validation ensures that training state is correctly saved and restored, enabling reliable training interruption and resumption. The checkpoint includes testing of various checkpoint timing scenarios, validation of state restoration accuracy, and verification that resumed training continues with identical dynamics to uninterrupted training.</p>\n<p>Loss tracking and early stopping validation verifies that training progress monitoring works correctly and that early stopping criteria prevent overfitting while allowing sufficient learning. The checkpoint includes validation of loss calculation accuracy, testing of early stopping trigger conditions, and verification that stopped training produces better validation performance than continued training.</p>\n<h3 id=\"milestone-5-evaluation-and-merging-validation\">Milestone 5: Evaluation and Merging Validation</h3>\n<p>The final milestone validation focuses on verifying that fine-tuned models demonstrate measurable improvement over baseline models and that adapter merging produces deployable models with maintained performance characteristics.</p>\n<p>Perplexity evaluation validation ensures that the fine-tuned model shows improved language modeling performance on validation data compared to the base model. The checkpoint includes validation of perplexity calculation accuracy, comparison of prompt versus response perplexity improvements, and verification that perplexity improvements correlate with task-specific performance gains.</p>\n<p>Task-specific evaluation validation verifies that the fine-tuned model demonstrates improved performance on domain-relevant benchmarks and instruction-following tasks. The checkpoint includes comparison against baseline model performance, validation of evaluation metric calculations, and verification that performance improvements are statistically significant and practically meaningful.</p>\n<p>Adapter merging validation ensures that combining LoRA adapter weights with base model parameters produces a standalone model with equivalent performance to the adapter-enabled version. The checkpoint includes numerical precision validation of weight merging operations, equivalence testing of model outputs before and after merging, and verification that merged models maintain expected performance characteristics.</p>\n<p>Model export validation verifies that exported models in various formats (HuggingFace, GGUF) load correctly in target inference environments and maintain acceptable performance characteristics. The checkpoint includes compatibility testing with target inference frameworks, validation of format conversion accuracy, and verification that exported models produce outputs consistent with the original fine-tuned model.</p>\n<h2 id=\"performance-and-memory-testing\">Performance and Memory Testing</h2>\n<p>Performance and memory testing for LLM fine-tuning systems requires specialized approaches that account for the unique characteristics of parameter-efficient training, quantization effects, and GPU memory management. Unlike traditional application performance testing, these tests must validate not just speed and throughput, but also memory efficiency, numerical precision preservation, and training stability under resource constraints.</p>\n<h3 id=\"memory-efficiency-validation-framework\">Memory Efficiency Validation Framework</h3>\n<p>The foundation of performance testing rests on comprehensive memory monitoring that tracks GPU memory usage patterns throughout all phases of the fine-tuning pipeline. Our <code>MemoryMonitor</code> system captures baseline measurements, tracks peak usage during training, and validates that memory consumption aligns with theoretical predictions based on model size, quantization settings, and LoRA configuration parameters.</p>\n<p>Memory efficiency testing follows a structured progression through different scales and configurations. Initial tests use smaller models (1-3B parameters) to validate that memory monitoring infrastructure works correctly and that basic efficiency calculations are accurate. These tests establish baseline memory consumption patterns for different quantization and adapter configurations without requiring extensive GPU resources.</p>\n<p>Progressive scaling tests validate memory efficiency across model sizes, starting with small models and scaling up to larger configurations as memory efficiency is validated. Each scale tests the relationship between theoretical memory predictions and actual usage, identifies memory usage patterns that don&#39;t scale linearly, and validates that quantization and adapter configurations maintain their efficiency characteristics across different model sizes.</p>\n<table>\n<thead>\n<tr>\n<th>Model Size Category</th>\n<th>Parameter Range</th>\n<th>Memory Test Focus</th>\n<th>Expected Efficiency Gain</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Small Scale</td>\n<td>1-3B parameters</td>\n<td>Baseline validation, infrastructure testing</td>\n<td>4x reduction with 4-bit quantization</td>\n</tr>\n<tr>\n<td>Medium Scale</td>\n<td>7-13B parameters</td>\n<td>Scaling validation, configuration optimization</td>\n<td>4x quantization + 99% LoRA parameter reduction</td>\n</tr>\n<tr>\n<td>Large Scale</td>\n<td>30B+ parameters</td>\n<td>Practical deployment validation, resource limits</td>\n<td>Combined efficiency enabling single-GPU training</td>\n</tr>\n</tbody></table>\n<h3 id=\"training-performance-benchmarking\">Training Performance Benchmarking</h3>\n<p>Training performance testing validates that parameter-efficient fine-tuning achieves acceptable training speeds while maintaining the memory efficiency gains that make large model fine-tuning practical. These tests measure tokens per second, effective throughput with gradient accumulation, and training stability under different batch size and sequence length configurations.</p>\n<p>Throughput benchmarking compares training speed across different efficiency configurations, measuring the trade-offs between memory savings and computational overhead. Pure full-precision training serves as a baseline, with comparisons against 4-bit quantized training, LoRA-only training, and combined QLoRA configurations. These benchmarks account for the overhead of quantization/dequantization operations and the computational characteristics of low-rank matrix operations.</p>\n<p>Gradient accumulation performance testing validates that simulated larger batch sizes don&#39;t introduce disproportionate overhead compared to actual large batch training. These tests measure the relationship between accumulation steps and training throughput, identify optimal accumulation strategies for different hardware configurations, and validate that gradient accumulation maintains training stability characteristics equivalent to large batch training.</p>\n<p>Memory pressure testing validates training performance under various GPU memory utilization levels, ensuring that the system maintains stable performance even when operating close to memory limits. These tests deliberately stress memory usage through larger batch sizes, longer sequence lengths, and concurrent operations to validate that memory management remains stable under realistic high-utilization scenarios.</p>\n<h3 id=\"quantization-impact-assessment\">Quantization Impact Assessment</h3>\n<p>Quantization introduces trade-offs between memory efficiency and numerical precision that must be carefully validated to ensure that memory savings don&#39;t compromise training effectiveness or model quality. Our testing framework measures these trade-offs across different quantization configurations and model architectures.</p>\n<p>Numerical precision testing compares model outputs and training dynamics between full-precision and quantized configurations. These tests measure the magnitude of differences in forward pass outputs, gradient computations, and parameter updates to quantify the precision impact of different quantization strategies. The testing includes validation that precision differences remain within acceptable bounds for effective fine-tuning.</p>\n<p>Training stability assessment validates that quantized training maintains stable learning dynamics without gradient explosions, loss spikes, or convergence failures. These tests run extended training sequences with various quantization configurations, monitoring gradient norms, loss trajectory smoothness, and parameter update characteristics to ensure that quantization doesn&#39;t introduce training instabilities.</p>\n<p>Quality preservation testing measures the impact of quantization on final model performance, validating that memory efficiency gains don&#39;t come at the cost of significant quality degradation. These tests compare fine-tuned model performance between full-precision and quantized training runs, measuring perplexity differences, task-specific performance impacts, and the relationship between quantization aggressiveness and quality preservation.</p>\n<h3 id=\"hardware-compatibility-and-scalability-testing\">Hardware Compatibility and Scalability Testing</h3>\n<p>The fine-tuning pipeline must work reliably across different GPU hardware configurations, CUDA versions, and system environments. Performance testing validates not only optimal-case performance but also compatibility and graceful degradation across different hardware scenarios.</p>\n<p>GPU architecture testing validates performance characteristics across different GPU generations and memory configurations. Tests cover various NVIDIA architectures (Ampere, Ada Lovelace, Hopper) to validate that quantization implementations work correctly and that performance characteristics scale appropriately with different compute capabilities and memory bandwidths.</p>\n<p>CUDA compatibility testing ensures that the quantization and training infrastructure works correctly across different CUDA toolkit versions and driver configurations. These tests validate that bitsandbytes integration remains stable, that quantization operations produce consistent results, and that performance characteristics are maintained across different CUDA environments.</p>\n<p>Multi-GPU scaling testing validates that the pipeline can effectively utilize multiple GPU configurations when available, while gracefully degrading to single-GPU operation when necessary. These tests measure scaling efficiency for data parallel training, validate memory usage distribution across multiple GPUs, and ensure that multi-GPU configurations maintain training stability and convergence characteristics.</p>\n<p>System resource testing validates performance under various system memory, storage, and CPU configurations, ensuring that the pipeline performs acceptably even on systems with limited resources beyond GPU capabilities. These tests identify bottlenecks in data loading, checkpoint saving, and evaluation phases that might not be apparent during pure training performance testing.</p>\n<h3 id=\"benchmark-comparison-framework\">Benchmark Comparison Framework</h3>\n<p>To provide meaningful performance context, our testing framework includes comparisons against established fine-tuning approaches and publicly available benchmarks. These comparisons validate that our parameter-efficient approach provides practical advantages over alternative methods while maintaining competitive training effectiveness.</p>\n<p>Baseline comparison testing measures training performance and memory usage against full fine-tuning approaches, quantifying the practical benefits of parameter-efficient methods. These tests provide concrete measurements of memory reduction, training speed differences, and final model quality comparisons to validate that efficiency gains justify any trade-offs in training characteristics.</p>\n<p>Industry benchmark alignment validates performance against published results for similar fine-tuning approaches, ensuring that our implementation achieves performance characteristics comparable to research implementations and commercial fine-tuning services. These comparisons provide confidence that the implementation represents state-of-the-art parameter-efficient fine-tuning practices.</p>\n<p>Regression testing maintains performance baselines across development iterations, ensuring that code changes don&#39;t inadvertently degrade performance characteristics or introduce memory usage regressions. These tests establish automated performance monitoring that alerts to significant changes in training throughput, memory efficiency, or model quality outcomes.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>Our testing infrastructure is built around comprehensive monitoring, synthetic data generation, and automated validation that provides high confidence in system reliability while minimizing the computational cost of test execution.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Unit Testing</td>\n<td>pytest with custom fixtures</td>\n<td>pytest + hypothesis for property-based testing</td>\n</tr>\n<tr>\n<td>Memory Monitoring</td>\n<td>torch.cuda.memory_* functions</td>\n<td>Custom CUDA memory profiler with timeline visualization</td>\n</tr>\n<tr>\n<td>Performance Benchmarking</td>\n<td>Simple timing with time.time()</td>\n<td>torch.profiler with detailed kernel analysis</td>\n</tr>\n<tr>\n<td>Mock Infrastructure</td>\n<td>unittest.mock with manual setup</td>\n<td>pytest-mock with automatic dependency injection</td>\n</tr>\n<tr>\n<td>Test Data Generation</td>\n<td>Static JSON files with test cases</td>\n<td>Synthetic data generators with configurable distributions</td>\n</tr>\n<tr>\n<td>Validation Framework</td>\n<td>Manual assertion checking</td>\n<td>Automated validation with statistical significance testing</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>tests/\n├── unit/\n│   ├── test_data_preparation.py      # Dataset loading, formatting, tokenization\n│   ├── test_lora_config.py          # LoRA configuration and parameter analysis\n│   ├── test_quantization.py         # Quantization configuration and memory estimation\n│   ├── test_training_loop.py        # Training orchestration and checkpoint management\n│   └── test_evaluation.py           # Evaluation metrics and model merging\n├── integration/\n│   ├── test_component_interactions.py   # Component interface validation\n│   ├── test_data_flow.py               # End-to-end data transformation\n│   ├── test_state_management.py        # Pipeline state coordination\n│   └── test_resource_management.py     # Memory and GPU resource handling\n├── performance/\n│   ├── test_memory_efficiency.py       # Memory usage and optimization validation\n│   ├── test_training_performance.py    # Training speed and throughput benchmarking\n│   ├── test_quantization_impact.py     # Quantization trade-off analysis\n│   └── test_hardware_compatibility.py  # Cross-platform and cross-hardware validation\n├── fixtures/\n│   ├── synthetic_datasets.py           # Test data generation utilities\n│   ├── mock_models.py                  # Lightweight model mocks for testing\n│   ├── memory_monitoring.py            # Memory usage tracking infrastructure\n│   └── performance_benchmarks.py       # Performance measurement utilities\n└── milestone_validation/\n    ├── test_milestone_1_data_prep.py   # Dataset preparation milestone validation\n    ├── test_milestone_2_lora.py        # LoRA configuration milestone validation\n    ├── test_milestone_3_quantization.py # Quantization milestone validation\n    ├── test_milestone_4_training.py     # Training loop milestone validation\n    └── test_milestone_5_evaluation.py   # Evaluation and merging milestone validation</code></pre></div>\n\n<h4 id=\"core-testing-infrastructure\">Core Testing Infrastructure</h4>\n<p>The testing infrastructure provides complete, working utilities for memory monitoring, performance benchmarking, and test data generation that can be used immediately across all test categories.</p>\n<p><strong>Memory Monitoring Infrastructure (Complete Implementation)</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/fixtures/memory_monitoring.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> psutil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> contextlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> contextmanager</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MemoryMetrics</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stage: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gpu_memory_allocated: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gpu_memory_cached: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gpu_memory_reserved: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    system_memory_used: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    details: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MemoryMonitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.baseline_gpu_memory: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.baseline_system_memory: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.measurements: List[MemoryMetrics] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> capture_baseline</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record initial memory state before model loading\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            torch.cuda.empty_cache()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.baseline_gpu_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_allocated()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.baseline_system_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.virtual_memory().used</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> measure_current_usage</span><span style=\"color:#E1E4E8\">(self, stage: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Measure current memory and return usage statistics\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            gpu_allocated </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_allocated()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            gpu_cached </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_cached()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            gpu_reserved </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_reserved()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            gpu_allocated </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> gpu_cached </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> gpu_reserved </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        system_used </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.virtual_memory().used</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metrics </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MemoryMetrics(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            stage</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">stage,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            gpu_memory_allocated</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">gpu_allocated </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#6A737D\"># Convert to GB</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            gpu_memory_cached</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">gpu_cached </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            gpu_memory_reserved</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">gpu_reserved </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            system_memory_used</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">system_used </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">timestamp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.measurements.append(metrics)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'gpu_allocated_gb'</span><span style=\"color:#E1E4E8\">: metrics.gpu_memory_allocated,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'gpu_cached_gb'</span><span style=\"color:#E1E4E8\">: metrics.gpu_memory_cached,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'gpu_reserved_gb'</span><span style=\"color:#E1E4E8\">: metrics.gpu_memory_reserved,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'system_used_gb'</span><span style=\"color:#E1E4E8\">: metrics.system_memory_used,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'gpu_peak_gb'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">max</span><span style=\"color:#E1E4E8\">(m.gpu_memory_allocated </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> m </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.measurements)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_peak_usage</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return peak memory usage across all measurements\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.measurements:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'peak_gpu_allocated'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">max</span><span style=\"color:#E1E4E8\">(m.gpu_memory_allocated </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> m </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.measurements),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'peak_gpu_cached'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">max</span><span style=\"color:#E1E4E8\">(m.gpu_memory_cached </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> m </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.measurements),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'peak_gpu_reserved'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">max</span><span style=\"color:#E1E4E8\">(m.gpu_memory_reserved </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> m </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.measurements),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'peak_system_used'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">max</span><span style=\"color:#E1E4E8\">(m.system_memory_used </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> m </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.measurements)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> monitor_stage</span><span style=\"color:#E1E4E8\">(self, stage_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Context manager for monitoring memory during a specific stage\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.measure_current_usage(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">stage_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">_start\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            yield</span><span style=\"color:#79B8FF\"> self</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        finally</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.measure_current_usage(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">stage_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">_end\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Performance Benchmarking Infrastructure (Complete Implementation)</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/fixtures/performance_benchmarks.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> statistics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Any, Callable</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> contextlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> contextmanager</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PerformanceBenchmark</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    operation_name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    execution_times: List[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    throughput_metrics: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resource_usage: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">property</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> mean_time</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> statistics.mean(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.execution_times)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">property</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> std_time</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> statistics.stdev(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.execution_times) </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.execution_times) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> else</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">property</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> median_time</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> statistics.median(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.execution_times)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PerformanceProfiler</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.benchmarks: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, PerformanceBenchmark] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> profile_operation</span><span style=\"color:#E1E4E8\">(self, operation_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, throughput_unit: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"items/sec\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Profile a single operation with timing and resource tracking\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Clear GPU cache for consistent measurements</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            torch.cuda.synchronize()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            torch.cuda.empty_cache()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.perf_counter()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start_gpu_mem </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_allocated() </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available() </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            yield</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        finally</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                torch.cuda.synchronize()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            end_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.perf_counter()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            end_gpu_mem </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_allocated() </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available() </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            execution_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> end_time </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            gpu_memory_delta </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (end_gpu_mem </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_gpu_mem) </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#6A737D\">  # MB</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> operation_name </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.benchmarks:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.benchmarks[operation_name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> PerformanceBenchmark(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    operation_name</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">operation_name,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    execution_times</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    throughput_metrics</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{},</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    resource_usage</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.benchmarks[operation_name].execution_times.append(execution_time)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.benchmarks[operation_name].resource_usage[</span><span style=\"color:#9ECBFF\">'gpu_memory_delta_mb'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> gpu_memory_delta</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> benchmark_repeated_operation</span><span style=\"color:#E1E4E8\">(self, operation_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, operation_func: Callable, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                   num_iterations: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs) -> PerformanceBenchmark:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Benchmark an operation multiple times for statistical reliability\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> _ </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(num_iterations):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.profile_operation(operation_name):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                operation_func(</span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.benchmarks[operation_name]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> compare_operations</span><span style=\"color:#E1E4E8\">(self, operation_names: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compare performance metrics across multiple operations\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        comparison </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> name </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> operation_names:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> name </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.benchmarks:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                benchmark </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.benchmarks[name]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                comparison[name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'mean_time'</span><span style=\"color:#E1E4E8\">: benchmark.mean_time,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'median_time'</span><span style=\"color:#E1E4E8\">: benchmark.median_time,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'std_time'</span><span style=\"color:#E1E4E8\">: benchmark.std_time,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'relative_speed'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#6A737D\">  # Will be calculated relative to baseline</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Calculate relative speeds (baseline = first operation)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> operation_names </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> operation_names[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> comparison:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            baseline_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> comparison[operation_names[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]][</span><span style=\"color:#9ECBFF\">'mean_time'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> name </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> operation_names:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> name </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> comparison:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    comparison[name][</span><span style=\"color:#9ECBFF\">'relative_speed'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> baseline_time </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> comparison[name][</span><span style=\"color:#9ECBFF\">'mean_time'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> comparison</span></span></code></pre></div>\n\n<p><strong>Synthetic Test Data Generation (Complete Implementation)</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/fixtures/synthetic_datasets.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> random</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Any, Iterator</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, asdict</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SyntheticConversation</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    instruction: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    response: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    system_prompt: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"You are a helpful assistant.\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    quality_score: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"synthetic\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> generate_instruction_response_pairs</span><span style=\"color:#E1E4E8\">(num_samples: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                       min_instruction_tokens: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                       max_instruction_tokens: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 50</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                       min_response_tokens: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                       max_response_tokens: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 200</span><span style=\"color:#E1E4E8\">) -> List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Generate synthetic instruction-response pairs with controllable characteristics\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Template instruction patterns</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    instruction_templates </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"Explain the concept of </span><span style=\"color:#79B8FF\">{topic}</span><span style=\"color:#9ECBFF\"> in simple terms.\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"Write a </span><span style=\"color:#79B8FF\">{length}</span><span style=\"color:#9ECBFF\"> summary of </span><span style=\"color:#79B8FF\">{topic}</span><span style=\"color:#9ECBFF\">.\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"List </span><span style=\"color:#79B8FF\">{count}</span><span style=\"color:#9ECBFF\"> key points about </span><span style=\"color:#79B8FF\">{topic}</span><span style=\"color:#9ECBFF\">.\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"Compare and contrast </span><span style=\"color:#79B8FF\">{topic1}</span><span style=\"color:#9ECBFF\"> and </span><span style=\"color:#79B8FF\">{topic2}</span><span style=\"color:#9ECBFF\">.\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"Provide step-by-step instructions for </span><span style=\"color:#79B8FF\">{topic}</span><span style=\"color:#9ECBFF\">.\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Sample topics and parameters</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    topics </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">\"machine learning\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"data structures\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"algorithms\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"software engineering\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">              \"artificial intelligence\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"databases\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"web development\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"cybersecurity\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lengths </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">\"brief\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"detailed\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"comprehensive\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    counts </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">\"3\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"5\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"7\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"10\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    conversations </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(num_samples):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Generate instruction</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        template </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> random.choice(instruction_templates)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">{topic1}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> template </span><span style=\"color:#F97583\">and</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">{topic2}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> template:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            topic1, topic2 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> random.sample(topics, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            instruction </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> template.format(</span><span style=\"color:#FFAB70\">topic1</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">topic1, </span><span style=\"color:#FFAB70\">topic2</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">topic2)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">{count}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> template:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            instruction </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> template.format(</span><span style=\"color:#FFAB70\">topic</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">random.choice(topics), </span><span style=\"color:#FFAB70\">count</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">random.choice(counts))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">{length}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> template:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            instruction </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> template.format(</span><span style=\"color:#FFAB70\">topic</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">random.choice(topics), </span><span style=\"color:#FFAB70\">length</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">random.choice(lengths))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            instruction </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> template.format(</span><span style=\"color:#FFAB70\">topic</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">random.choice(topics))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Generate response (simplified - in practice would use more sophisticated generation)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        response_length </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> random.randint(min_response_tokens, max_response_tokens)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        response_words </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">\"This\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"is\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"a\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"synthetic\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"response\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"about\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"the\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"topic\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> (response_length </span><span style=\"color:#F97583\">//</span><span style=\"color:#79B8FF\"> 8</span><span style=\"color:#F97583\"> +</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        response </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \" \"</span><span style=\"color:#E1E4E8\">.join(response_words[:response_length]) </span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\"> \".\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        conversations.append({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"instruction\"</span><span style=\"color:#E1E4E8\">: instruction,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"response\"</span><span style=\"color:#E1E4E8\">: response,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"system_prompt\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"You are a helpful assistant.\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"sample_id\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"synthetic_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#F97583\">:06d</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"source\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"synthetic_generator\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"quality_score\"</span><span style=\"color:#E1E4E8\">: random.uniform(</span><span style=\"color:#79B8FF\">0.7</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> conversations</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> generate_conversation_dataset</span><span style=\"color:#E1E4E8\">(output_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, num_samples: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Generate a complete synthetic dataset and save to file\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    conversations </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> generate_instruction_response_pairs(num_samples)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(output_path, </span><span style=\"color:#9ECBFF\">'w'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> conv </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> conversations:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            json.dump(conv, f)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            f.write(</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> output_path</span></span></code></pre></div>\n\n<h4 id=\"unit-test-implementation-examples\">Unit Test Implementation Examples</h4>\n<p><strong>Data Preparation Unit Tests (Core Implementation)</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/unit/test_data_preparation.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pytest</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> tempfile</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> unittest.mock </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Mock, patch</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Import the data preparation components from your implementation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># from src.data_preparation import DataLoader, InstructionFormatter, TokenizationPipeline</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_data_loader_handles_multiple_formats</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test that DataLoader correctly loads and validates data from different file formats\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create temporary test files in different formats (JSON, JSONL, CSV)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Initialize DataLoader and test loading from each format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Validate that all formats produce identical InstructionSample objects</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Test error handling for malformed files and unsupported formats</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_chat_template_application</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test that chat templates are applied correctly for different conversation patterns\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Create mock tokenizer with known chat template</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Test template application on single-turn conversations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Test template application on multi-turn conversations  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Validate that special tokens are inserted correctly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Test template consistency across multiple applications</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_tokenization_with_length_constraints</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test that tokenization handles length constraints and attention masks correctly\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 11: Create mock tokenizer with predictable behavior</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 12: Test tokenization of samples within length limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 13: Test truncation behavior for samples exceeding limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 14: Validate attention mask generation for different input lengths</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 15: Test padding behavior for batch processing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_train_validation_split_stratification</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test that dataset splitting maintains proper distribution characteristics\"\"\"</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 16: Generate synthetic dataset with known distribution characteristics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 17: Apply train-validation split with stratification parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 18: Validate that split ratios match configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 19: Test that stratification preserves important distribution properties</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 20: Verify no data leakage between training and validation sets</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<p><strong>LoRA Configuration Unit Tests (Core Implementation)</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/unit/test_lora_config.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pytest</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> unittest.mock </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Mock, MagicMock</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Import LoRA configuration components from your implementation  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># from src.lora_config import LoRAConfig, TargetModuleDetector, RankSelector</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_lora_config_validation</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test that LoRA configuration validates parameters correctly\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Test valid LoRA configurations are accepted</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Test invalid rank values (negative, zero) are rejected</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Test invalid alpha values are handled appropriately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Test target module list validation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Test effective_alpha calculation accuracy</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_target_module_detection</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test automatic detection of suitable modules for LoRA adapter injection\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Create mock transformer model with known architecture</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Test detection of attention modules (Q, K, V, O projections)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Test detection of MLP/feed-forward modules  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Validate detected modules have appropriate dimensions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 11: Test architecture-specific detection patterns</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_rank_selection_logic</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test that rank selection considers task complexity and memory constraints\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 12: Test rank selection for different task complexity levels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 13: Test memory constraint handling in rank selection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 14: Validate that recommended ranks produce expected parameter counts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 15: Test edge cases like extremely limited memory or very simple tasks</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_parameter_efficiency_calculations</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test that parameter efficiency metrics are calculated correctly\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 16: Create mock model with known parameter distribution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 17: Apply LoRA configuration and measure parameter changes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 18: Validate trainable parameter ratio calculations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 19: Test memory usage estimation accuracy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 20: Verify efficiency metrics match theoretical expectations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"integration-test-implementation-examples\">Integration Test Implementation Examples</h4>\n<p><strong>Component Interaction Integration Tests</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/integration/test_component_interactions.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pytest</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_data_preparation_to_training_integration</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test complete flow from data loading to training-ready datasets\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load realistic test dataset using data preparation components</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Apply full data preparation pipeline (loading, formatting, tokenization)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate that output format matches training loop expectations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Test that tokenization parameters are consistent across pipeline</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Verify that data quality statistics are computed correctly</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_lora_quantization_compatibility</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test that LoRA adapters work correctly with quantized models\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Load small model with 4-bit quantization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Apply LoRA adapter configuration to quantized model</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Validate that adapter injection preserves quantization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Test forward pass correctness with combined LoRA + quantization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Verify that gradient computation works correctly</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_training_checkpoint_integration</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test that checkpointing works correctly with LoRA adapters and quantization\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 11: Set up training configuration with LoRA and quantization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 12: Execute short training run and save checkpoint</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 13: Load checkpoint and verify model state restoration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 14: Continue training from checkpoint and verify consistency</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 15: Test checkpoint compatibility across different configurations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-validation-checkpoints\">Milestone Validation Checkpoints</h4>\n<p><strong>Milestone Validation Implementation</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/milestone_validation/test_milestone_1_data_prep.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pytest</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_milestone_1_complete_data_preparation</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Complete validation checkpoint for Milestone 1: Dataset Preparation\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Expected behavior after Milestone 1:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # - All supported data formats load correctly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # - Chat templates are applied consistently  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # - Tokenization produces proper attention masks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # - Train/validation splits maintain distribution properties</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load test dataset from multiple formats</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate data loading statistics match expectations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Apply chat template and verify output format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Execute tokenization pipeline and validate token distributions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Perform train-validation split and verify no data leakage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Success criteria:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#6A737D\">  # Replace with actual validation logic</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # - Data loading completes without errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # - Template application is deterministic and reversible</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # - Tokenization produces expected token count distributions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # - Split datasets have appropriate size ratios and statistical properties</span></span></code></pre></div>\n\n<h4 id=\"performance-test-implementation-examples\">Performance Test Implementation Examples</h4>\n<p><strong>Memory Efficiency Performance Tests</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/performance/test_memory_efficiency.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pytest</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> tests.fixtures.memory_monitoring </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> MemoryMonitor</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_quantization_memory_reduction</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test that 4-bit quantization achieves expected memory savings\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    monitor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MemoryMonitor()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    monitor.capture_baseline()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load model in full precision and measure memory usage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Load same model with 4-bit quantization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compare memory usage and validate reduction ratio</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Test memory usage with different model sizes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate that memory predictions align with actual usage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Expected: ~75% memory reduction with 4-bit quantization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_lora_parameter_efficiency</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test that LoRA adapters achieve expected parameter count reduction\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Configure model with LoRA adapters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Count trainable parameters vs total parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Validate that trainable ratio is less than 1%</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Test efficiency across different rank configurations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Verify memory overhead of adapters is minimal</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@pytest.mark.slow</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_combined_efficiency_large_model</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test combined LoRA + quantization efficiency on large models\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 11: Load large model (7B+ parameters) with combined optimizations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 12: Measure actual memory usage vs theoretical predictions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 13: Validate that combination enables single-GPU training</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 14: Test training stability with combined optimizations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<p>This comprehensive testing strategy ensures that the LLM fine-tuning pipeline is thoroughly validated at every level, from individual component correctness to full-scale performance characteristics, providing confidence in the system&#39;s reliability and effectiveness across diverse training scenarios.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The testing infrastructure requires a carefully orchestrated approach that balances thorough validation with practical execution time and resource requirements. The implementation focuses on providing complete, working test utilities that can be immediately integrated into the development workflow.</p>\n<p><strong>Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Unit Testing Framework</td>\n<td>pytest with basic fixtures</td>\n<td>pytest + hypothesis for property-based testing</td>\n</tr>\n<tr>\n<td>Memory Profiling</td>\n<td>torch.cuda.memory_* APIs</td>\n<td>NVIDIA NSight Systems with custom profiling hooks</td>\n</tr>\n<tr>\n<td>Performance Benchmarking</td>\n<td>Manual timing with time.perf_counter()</td>\n<td>torch.profiler with kernel-level analysis and visualization</td>\n</tr>\n<tr>\n<td>Mock Infrastructure</td>\n<td>unittest.mock with manual setup</td>\n<td>pytest-mock with automatic dependency injection and fixture management</td>\n</tr>\n<tr>\n<td>Test Data Management</td>\n<td>Static JSON files with hardcoded test cases</td>\n<td>Synthetic data generators with parameterizable distributions</td>\n</tr>\n<tr>\n<td>CI/CD Integration</td>\n<td>Manual test execution</td>\n<td>GitHub Actions with GPU runners and automated performance regression detection</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>tests/\n├── conftest.py                          # Shared pytest fixtures and configuration\n├── unit/\n│   ├── data_preparation/\n│   │   ├── test_data_loading.py         # Data format parsing and validation\n│   │   ├── test_chat_templates.py       # Template application and conversation formatting  \n│   │   ├── test_tokenization.py         # Token encoding and attention mask generation\n│   │   └── test_data_splitting.py       # Train-validation partitioning and stratification\n│   ├── lora_configuration/\n│   │   ├── test_config_validation.py    # LoRA parameter validation and constraint checking\n│   │   ├── test_target_detection.py     # Automatic module identification for adapter injection\n│   │   ├── test_rank_selection.py       # Rank and alpha parameter optimization\n│   │   └── test_efficiency_analysis.py  # Parameter count and memory usage calculations\n│   ├── quantization/\n│   │   ├── test_nf4_config.py          # NormalFloat quantization configuration\n│   │   ├── test_memory_estimation.py    # Memory usage prediction and validation\n│   │   └── test_precision_impact.py     # Numerical precision preservation testing\n│   ├── training_loop/\n│   │   ├── test_gradient_accumulation.py # Batch size simulation and scaling\n│   │   ├── test_lr_scheduling.py        # Learning rate warmup and decay strategies\n│   │   ├── test_checkpointing.py        # Training state persistence and recovery\n│   │   └── test_early_stopping.py       # Convergence monitoring and training termination\n│   └── evaluation/\n│       ├── test_perplexity_calc.py      # Language modeling performance measurement\n│       ├── test_task_evaluation.py      # Domain-specific benchmark assessment\n│       ├── test_adapter_merging.py      # LoRA weight combination and standalone model creation\n│       └── test_model_export.py         # Format conversion and deployment preparation\n├── integration/\n│   ├── test_data_to_training_flow.py    # Complete data preparation to training pipeline\n│   ├── test_lora_quantization_compat.py # LoRA and quantization interaction validation\n│   ├── test_checkpoint_recovery.py      # Training interruption and resumption scenarios\n│   └── test_evaluation_pipeline.py      # Training completion to model deployment flow\n├── performance/\n│   ├── test_memory_benchmarks.py        # GPU and system memory usage profiling\n│   ├── test_training_throughput.py      # Tokens per second and batch processing speed\n│   ├── test_quantization_overhead.py    # Performance impact of 4-bit operations\n│   └── test_scaling_characteristics.py  # Performance across different model sizes\n├── milestone_validation/\n│   ├── validate_milestone_1.py          # Dataset preparation milestone checkpoint\n│   ├── validate_milestone_2.py          # LoRA configuration milestone checkpoint  \n│   ├── validate_milestone_3.py          # Quantization setup milestone checkpoint\n│   ├── validate_milestone_4.py          # Training loop milestone checkpoint\n│   └── validate_milestone_5.py          # Evaluation and merging milestone checkpoint\n└── fixtures/\n    ├── synthetic_data_generator.py       # Configurable test dataset creation\n    ├── mock_model_factory.py            # Lightweight transformer model mocks\n    ├── memory_profiling_utils.py        # GPU memory tracking and analysis\n    └── performance_measurement.py        # Benchmarking and timing utilities</code></pre></div>\n\n<p><strong>Complete Memory Monitoring Infrastructure</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/fixtures/memory_profiling_utils.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> psutil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Callable</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> contextlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> contextmanager</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MemorySnapshot</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stage: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gpu_allocated_gb: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gpu_cached_gb: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gpu_reserved_gb: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    system_memory_gb: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    process_memory_gb: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ContinuousMemoryMonitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Continuous memory monitoring with background thread collection\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, sampling_interval: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.sampling_interval </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sampling_interval</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.snapshots: List[MemorySnapshot] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.monitoring </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.monitor_thread: Optional[threading.Thread] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_stage </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"idle\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> start_monitoring</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Start background memory monitoring thread\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.monitoring </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.monitor_thread </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Thread(</span><span style=\"color:#FFAB70\">target</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._monitoring_loop, </span><span style=\"color:#FFAB70\">daemon</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.monitor_thread.start()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> stop_monitoring</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Stop background monitoring and return collected snapshots\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.monitoring </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.monitor_thread:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.monitor_thread.join(</span><span style=\"color:#FFAB70\">timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.snapshots.copy()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> set_stage</span><span style=\"color:#E1E4E8\">(self, stage_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Update current monitoring stage label\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_stage </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> stage_name</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _monitoring_loop</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Background thread loop for continuous memory sampling\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.monitoring:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                snapshot </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._capture_snapshot()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.snapshots.append(snapshot)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Memory monitoring error: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            time.sleep(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.sampling_interval)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _capture_snapshot</span><span style=\"color:#E1E4E8\">(self) -> MemorySnapshot:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Capture single memory usage snapshot\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            gpu_allocated </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_allocated() </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            gpu_cached </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_cached() </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            gpu_reserved </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_reserved() </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            gpu_allocated </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> gpu_cached </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> gpu_reserved </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        system_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.virtual_memory()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        process_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.Process().memory_info()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> MemorySnapshot(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">timestamp,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            stage</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current_stage,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            gpu_allocated_gb</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">gpu_allocated,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            gpu_cached_gb</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">gpu_cached,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            gpu_reserved_gb</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">gpu_reserved,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            system_memory_gb</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">system_memory.used </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            process_memory_gb</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">process_memory.rss </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_peak_usage_by_stage</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate peak memory usage grouped by monitoring stage\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        stage_peaks </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> snapshot </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.snapshots:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> snapshot.stage </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> stage_peaks:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                stage_peaks[snapshot.stage] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'peak_gpu_allocated'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'peak_gpu_cached'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'peak_system_memory'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            peaks </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> stage_peaks[snapshot.stage]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            peaks[</span><span style=\"color:#9ECBFF\">'peak_gpu_allocated'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(peaks[</span><span style=\"color:#9ECBFF\">'peak_gpu_allocated'</span><span style=\"color:#E1E4E8\">], snapshot.gpu_allocated_gb)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            peaks[</span><span style=\"color:#9ECBFF\">'peak_gpu_cached'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(peaks[</span><span style=\"color:#9ECBFF\">'peak_gpu_cached'</span><span style=\"color:#E1E4E8\">], snapshot.gpu_cached_gb)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            peaks[</span><span style=\"color:#9ECBFF\">'peak_system_memory'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(peaks[</span><span style=\"color:#9ECBFF\">'peak_system_memory'</span><span style=\"color:#E1E4E8\">], snapshot.system_memory_gb)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> stage_peaks</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> profile_memory_usage</span><span style=\"color:#E1E4E8\">(stage_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, continuous: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Context manager for memory usage profiling during specific operations\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> continuous:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        monitor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ContinuousMemoryMonitor(</span><span style=\"color:#FFAB70\">sampling_interval</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.05</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        monitor.set_stage(stage_name)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        monitor.start_monitoring()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            yield</span><span style=\"color:#E1E4E8\"> monitor</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        finally</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            snapshots </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> monitor.stop_monitoring()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Single point-in-time measurements</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        class</span><span style=\"color:#B392F0\"> SimpleMonitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.start_usage </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._measure_current()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.end_usage </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            def</span><span style=\"color:#B392F0\"> _measure_current</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'gpu_allocated'</span><span style=\"color:#E1E4E8\">: torch.cuda.memory_allocated() </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available() </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'system_memory'</span><span style=\"color:#E1E4E8\">: psutil.virtual_memory().used </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            def</span><span style=\"color:#B392F0\"> get_delta</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.end_usage </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._measure_current()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'gpu_delta_gb'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.end_usage[</span><span style=\"color:#9ECBFF\">'gpu_allocated'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.start_usage[</span><span style=\"color:#9ECBFF\">'gpu_allocated'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'system_delta_gb'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.end_usage[</span><span style=\"color:#9ECBFF\">'system_memory'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.start_usage[</span><span style=\"color:#9ECBFF\">'system_memory'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        monitor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SimpleMonitor()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            yield</span><span style=\"color:#E1E4E8\"> monitor</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        finally</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span></code></pre></div>\n\n<p><strong>Complete Performance Benchmarking Infrastructure</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/fixtures/performance_measurement.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> statistics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Any, Callable, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, asdict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> contextlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> BenchmarkResult</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    operation_name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mean_duration: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    std_duration: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    min_duration: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_duration: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    iterations: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    throughput_items_per_sec: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    memory_delta_mb: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gpu_utilization: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PerformanceBenchmark</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Comprehensive performance benchmarking with statistical analysis\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, warmup_iterations: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">, measurement_iterations: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.warmup_iterations </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> warmup_iterations</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.measurement_iterations </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> measurement_iterations</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.results: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, BenchmarkResult] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> benchmark_function</span><span style=\"color:#E1E4E8\">(self, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                         function: Callable,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                         function_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                         *</span><span style=\"color:#E1E4E8\">args,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                         items_processed: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                         **</span><span style=\"color:#E1E4E8\">kwargs) -> BenchmarkResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Benchmark a function with statistical analysis of execution times\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Warmup iterations to stabilize performance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> _ </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.warmup_iterations):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                torch.cuda.empty_cache()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                torch.cuda.synchronize()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            function(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Measurement iterations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        execution_times </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        memory_deltas </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> _ </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.measurement_iterations):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Pre-measurement cleanup</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                torch.cuda.empty_cache()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                torch.cuda.synchronize()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                start_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_allocated()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                start_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Time the operation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.perf_counter()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            function(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                torch.cuda.synchronize()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            end_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.perf_counter()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Memory measurement</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                end_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_allocated()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                memory_delta </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (end_memory </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_memory) </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># MB</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                memory_deltas.append(memory_delta)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            execution_times.append(end_time </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Statistical analysis</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        mean_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> statistics.mean(execution_times)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        std_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> statistics.stdev(execution_times) </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(execution_times) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> else</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        min_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> min</span><span style=\"color:#E1E4E8\">(execution_times) </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        max_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(execution_times)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Calculate throughput if items processed is provided</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        throughput </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> items_processed </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> mean_time </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> items_processed </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Average memory delta</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        avg_memory_delta </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> statistics.mean(memory_deltas) </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> memory_deltas </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> BenchmarkResult(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            operation_name</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">function_name,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            mean_duration</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">mean_time,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            std_duration</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">std_time,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            min_duration</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">min_time,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            max_duration</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">max_time,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            iterations</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.measurement_iterations,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            throughput_items_per_sec</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">throughput,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            memory_delta_mb</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">avg_memory_delta</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.results[function_name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> result</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @contextmanager</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> benchmark_context</span><span style=\"color:#E1E4E8\">(self, operation_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, items_processed: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Context manager for benchmarking code blocks\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        execution_times </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        memory_deltas </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> iteration </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.warmup_iterations </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.measurement_iterations):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Preparation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                torch.cuda.empty_cache()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                torch.cuda.synchronize()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                start_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_allocated()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                start_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Timing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.perf_counter()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            yield</span><span style=\"color:#E1E4E8\"> iteration  </span><span style=\"color:#6A737D\"># Allow caller to execute their code</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                torch.cuda.synchronize()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            end_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.perf_counter()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Skip warmup iterations for statistics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> iteration </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.warmup_iterations:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                execution_times.append(end_time </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    end_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_allocated()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    memory_deltas.append((end_memory </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_memory) </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Calculate statistics and store result</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> execution_times:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            mean_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> statistics.mean(execution_times)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> BenchmarkResult(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                operation_name</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">operation_name,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                mean_duration</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">mean_time,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                std_duration</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">statistics.stdev(execution_times) </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(execution_times) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> else</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                min_duration</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">min</span><span style=\"color:#E1E4E8\">(execution_times),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                max_duration</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">max</span><span style=\"color:#E1E4E8\">(execution_times),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                iterations</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(execution_times),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                throughput_items_per_sec</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">items_processed </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> mean_time </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> items_processed </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                memory_delta_mb</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">statistics.mean(memory_deltas) </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> memory_deltas </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.results[operation_name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> compare_benchmarks</span><span style=\"color:#E1E4E8\">(self, baseline_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compare all benchmarks relative to a baseline operation\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> baseline_name </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.results:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Baseline '</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">baseline_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">' not found in results\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        baseline_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.results[baseline_name].mean_duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        comparisons </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> name, result </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.results.items():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            comparisons[name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> baseline_time </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> result.mean_duration  </span><span style=\"color:#6A737D\"># Relative speedup</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> comparisons</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> export_results</span><span style=\"color:#E1E4E8\">(self, output_path: Path):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Export benchmark results to JSON for analysis and reporting\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        serializable_results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {name: asdict(result) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> name, result </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.results.items()}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(output_path, </span><span style=\"color:#9ECBFF\">'w'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            json.dump({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'benchmark_config'</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'warmup_iterations'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.warmup_iterations,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'measurement_iterations'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.measurement_iterations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                },</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'results'</span><span style=\"color:#E1E4E8\">: serializable_results,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'summary'</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'total_operations'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.results),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'fastest_operation'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">min</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.results.keys(), </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\"> x: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.results[x].mean_duration),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'slowest_operation'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">max</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.results.keys(), </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\"> x: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.results[x].mean_duration)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }, f, </span><span style=\"color:#FFAB70\">indent</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Milestone Validation Checkpoint Implementation</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/milestone_validation/validate_milestone_1.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pytest</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> tempfile</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_milestone_1_dataset_preparation_complete</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Complete validation checkpoint for Milestone 1: Dataset Preparation</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This test validates that all dataset preparation capabilities are working</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    correctly and ready for integration with subsequent pipeline components.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize data preparation components</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # data_loader = DataLoader()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # formatter = InstructionFormatter() </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # tokenizer_pipeline = TokenizationPipeline()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # splitter = DatasetSplitter()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Test data loading from multiple formats</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    with</span><span style=\"color:#E1E4E8\"> tempfile.TemporaryDirectory() </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> temp_dir:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        temp_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Path(temp_dir)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create test datasets in different formats</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # json_data = create_test_json_dataset(temp_path / \"test.json\", num_samples=100)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # jsonl_data = create_test_jsonl_dataset(temp_path / \"test.jsonl\", num_samples=100) </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # csv_data = create_test_csv_dataset(temp_path / \"test.csv\", num_samples=100)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Load data from each format and validate consistency</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # json_samples = list(data_loader.load_data(temp_path / \"test.json\"))</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # jsonl_samples = list(data_loader.load_data(temp_path / \"test.jsonl\"))</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # csv_samples = list(data_loader.load_data(temp_path / \"test.csv\"))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Validation checkpoints for data loading</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # assert len(json_samples) == 100, \"JSON loading should produce 100 samples\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # assert len(jsonl_samples) == 100, \"JSONL loading should produce 100 samples\"  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # assert len(csv_samples) == 100, \"CSV loading should produce 100 samples\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Test chat template application</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # for sample in json_samples[:10]:  # Test subset for performance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #     formatted_text = formatter.apply_template(sample)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #     assert isinstance(formatted_text, str),</span></span></code></pre></div>\n\n\n<h2 id=\"debugging-guide\">Debugging Guide</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - debugging skills and systematic troubleshooting approaches are essential throughout the entire fine-tuning pipeline, from data preparation through model export</p>\n</blockquote>\n<p>The debugging process for LLM fine-tuning can feel like being a detective investigating a crime scene where the evidence keeps changing. Unlike traditional software debugging where stack traces point directly to the problem, fine-tuning issues often manifest as subtle performance degradations, mysterious memory exhaustion, or training instability that emerges hours into a long-running process. The challenge lies in distinguishing between normal training variance and genuine problems that require intervention.</p>\n<p>Think of debugging a fine-tuning pipeline like diagnosing a complex mechanical system where multiple components interact in non-obvious ways. A memory leak in the data loader might not surface until the third epoch. A misconfigured chat template might produce syntactically valid but semantically meaningless training data. A LoRA rank that&#39;s too low might cause the model to converge to a local minimum that looks reasonable on training metrics but fails catastrophically on evaluation.</p>\n<p>The key insight for effective debugging is that LLM fine-tuning failures rarely have single root causes. Instead, they emerge from the interaction of configuration choices, data quality issues, hardware constraints, and implementation bugs. This section provides systematic approaches to isolate these interacting factors and identify the true root cause.</p>\n<blockquote>\n<p><strong>Critical Principle: Establish Baselines First</strong></p>\n<p>Before debugging any fine-tuning issue, establish known-good baselines for memory usage, loss curves, and model outputs. Many apparent &quot;bugs&quot; are actually normal behavior that wasn&#39;t properly characterized during initial setup.</p>\n</blockquote>\n<h3 id=\"symptom-cause-fix-tables\">Symptom-Cause-Fix Tables</h3>\n<p>The following tables provide quick reference guides for the most common fine-tuning issues. Each entry follows the pattern of observable symptom, underlying technical cause, and specific remediation steps. These tables are organized by the pipeline stage where symptoms typically first appear.</p>\n<h4 id=\"data-preparation-issues\">Data Preparation Issues</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Root Cause</th>\n<th>Diagnostic Steps</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>DataLoader runs out of memory during iteration</td>\n<td>Chat template expansion creates unexpectedly long sequences</td>\n<td>Check <code>TokenizedSample.total_length</code> distribution, examine template-applied samples</td>\n<td>Implement dynamic batching based on sequence length, reduce <code>max_length</code> in tokenization</td>\n</tr>\n<tr>\n<td>Training loss starts abnormally high (&gt;10.0)</td>\n<td>Chat templates not properly applied, model seeing raw instruction text</td>\n<td>Manually inspect first few batches with <code>tokenizer.decode()</code>, verify special tokens</td>\n<td>Fix <code>apply_template()</code> implementation, ensure EOS tokens between turns</td>\n</tr>\n<tr>\n<td>Validation loss significantly higher than training loss from epoch 1</td>\n<td>Train-validation split has distribution mismatch</td>\n<td>Compare instruction types, lengths, and domains between splits</td>\n<td>Re-split with stratification by instruction category or length bins</td>\n</tr>\n<tr>\n<td>Tokenizer produces unexpected token counts</td>\n<td>Model tokenizer doesn&#39;t match base model, subword vocabulary mismatch</td>\n<td>Compare <code>tokenizer.vocab_size</code> with model config, test tokenization of known phrases</td>\n<td>Load correct tokenizer for base model, verify tokenizer and model compatibility</td>\n</tr>\n<tr>\n<td>Some samples have <code>labels</code> filled with -100</td>\n<td>Prompt-response separation failed, entire sequence marked as prompt</td>\n<td>Check <code>prompt_length</code> vs <code>total_length</code> ratio, examine tokenization boundaries</td>\n<td>Fix prompt-response splitting in <code>tokenize_sample()</code>, ensure response has positive labels</td>\n</tr>\n<tr>\n<td>Training data appears corrupted with random characters</td>\n<td>Encoding mismatch during file loading, UTF-8 vs ASCII issues</td>\n<td>Check file encoding with <code>chardet</code>, examine raw file bytes</td>\n<td>Specify correct encoding in <code>load_data()</code>, handle encoding errors gracefully</td>\n</tr>\n</tbody></table>\n<h4 id=\"lora-configuration-issues\">LoRA Configuration Issues</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Root Cause</th>\n<th>Diagnostic Steps</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Training loss doesn&#39;t decrease after multiple epochs</td>\n<td>LoRA rank too low for task complexity, insufficient adapter capacity</td>\n<td>Check <code>effective_rank()</code> utilization, compare against task complexity metrics</td>\n<td>Increase rank from 16 to 32-64, adjust alpha proportionally</td>\n</tr>\n<tr>\n<td>GPU memory usage higher than expected despite LoRA</td>\n<td>Base model parameters not frozen, full gradients computed</td>\n<td>Verify <code>requires_grad=False</code> for base parameters, check <code>AdapterMetrics.trainable_ratio</code></td>\n<td>Ensure <code>freeze_base_parameters()</code> called after adapter injection</td>\n</tr>\n<tr>\n<td>Adapter parameters not updating (gradients zero)</td>\n<td>Target modules incorrectly identified, LoRA not injected properly</td>\n<td>Inspect <code>target_modules</code> list, verify modules exist in model architecture</td>\n<td>Use <code>detect_attention_modules()</code> and <code>detect_mlp_modules()</code> for auto-detection</td>\n</tr>\n<tr>\n<td>Training becomes unstable with gradient explosions</td>\n<td>LoRA alpha too high relative to rank, effective learning rate too large</td>\n<td>Monitor <code>grad_norm</code> in training logs, calculate <code>effective_alpha()</code></td>\n<td>Reduce alpha or increase rank to maintain alpha/rank ratio around 2:1</td>\n</tr>\n<tr>\n<td>Model outputs degrade compared to base model</td>\n<td>Adapter weights initialized improperly, breaking pre-trained representations</td>\n<td>Compare base model vs adapted model outputs on same prompts before training</td>\n<td>Use proper weight initialization in <code>init_lora_weights</code>, start with smaller alpha</td>\n</tr>\n<tr>\n<td>Adapter injection fails with shape mismatch errors</td>\n<td>Target module dimensions don&#39;t support chosen rank</td>\n<td>Check <code>analyze_module_dimensions()</code> output, verify rank &lt; module width</td>\n<td>Reduce rank or exclude problematic modules from <code>target_modules</code></td>\n</tr>\n</tbody></table>\n<h4 id=\"quantization-issues\">Quantization Issues</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Root Cause</th>\n<th>Diagnostic Steps</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Model loading fails with CUDA out of memory</td>\n<td>Quantization not applied, model loading in full precision</td>\n<td>Check <code>load_in_4bit=True</code> in config, verify bitsandbytes installation</td>\n<td>Ensure <code>QuantizationConfig.load_in_4bit=True</code>, install compatible bitsandbytes version</td>\n</tr>\n<tr>\n<td>Training slower than expected despite quantization</td>\n<td>Compute dtype mismatch, dequantization overhead in forward pass</td>\n<td>Monitor GPU utilization, check <code>bnb_4bit_compute_dtype</code> setting</td>\n<td>Set compute dtype to <code>torch.bfloat16</code>, enable Flash Attention if available</td>\n</tr>\n<tr>\n<td>Numerical instability with loss spikes</td>\n<td>Mixed precision conflicts with 4-bit quantization</td>\n<td>Check loss curves for sudden jumps, monitor gradient norms</td>\n<td>Use <code>torch.bfloat16</code> instead of <code>float16</code>, enable gradient clipping</td>\n</tr>\n<tr>\n<td>Model outputs significantly degraded after quantization</td>\n<td>Quantization parameters inappropriate for model architecture</td>\n<td>Compare base model outputs before/after quantization on test prompts</td>\n<td>Try FP4 instead of NF4, disable double quantization, use higher compute dtype</td>\n</tr>\n<tr>\n<td>Bitsandbytes import errors or CUDA kernel failures</td>\n<td>CUDA version incompatibility with bitsandbytes build</td>\n<td>Check CUDA version with <code>nvcc --version</code>, verify bitsandbytes CUDA support</td>\n<td>Install bitsandbytes version matching CUDA installation</td>\n</tr>\n<tr>\n<td>Memory usage not reduced as expected</td>\n<td>Double quantization not enabled, optimizer states still in full precision</td>\n<td>Check actual memory with <code>torch.cuda.memory_allocated()</code>, verify config flags</td>\n<td>Enable <code>bnb_4bit_use_double_quant=True</code>, use paged optimizers for large models</td>\n</tr>\n</tbody></table>\n<h4 id=\"training-loop-issues\">Training Loop Issues</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Root Cause</th>\n<th>Diagnostic Steps</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Training loss oscillates wildly</td>\n<td>Learning rate too high for adapter configuration</td>\n<td>Plot loss curves, check <code>learning_rate</code> vs <code>effective_alpha()</code></td>\n<td>Reduce learning rate by 2-4x, implement warmup scheduling</td>\n</tr>\n<tr>\n<td>Training stops with NaN loss</td>\n<td>Gradient explosion, numerical overflow in mixed precision</td>\n<td>Check for NaN in model parameters and gradients</td>\n<td>Enable gradient clipping, reduce learning rate, check for corrupted data samples</td>\n</tr>\n<tr>\n<td>GPU utilization low despite batch size</td>\n<td>Gradient accumulation misconfigured, effective batch size too small</td>\n<td>Check <code>effective_batch_size()</code> vs hardware capability</td>\n<td>Increase gradient accumulation steps, optimize data loading pipeline</td>\n</tr>\n<tr>\n<td>Training extremely slow</td>\n<td>Data loading bottleneck, synchronous preprocessing</td>\n<td>Profile training step timing, check data loader queue utilization</td>\n<td>Enable <code>dataloader_num_workers</code>, implement async data preprocessing</td>\n</tr>\n<tr>\n<td>Checkpoints corrupted or unloadable</td>\n<td>Interrupt during checkpoint writing, insufficient disk space</td>\n<td>Check disk space, verify checkpoint file integrity</td>\n<td>Implement atomic checkpoint writing, verify disk space before saving</td>\n</tr>\n<tr>\n<td>Memory usage grows throughout training</td>\n<td>Memory leak in data preprocessing, accumulated gradients</td>\n<td>Monitor memory usage over time, check for unreleased tensors</td>\n<td>Clear optimizer state periodically, fix data loader memory leaks</td>\n</tr>\n</tbody></table>\n<h4 id=\"evaluation-and-export-issues\">Evaluation and Export Issues</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Root Cause</th>\n<th>Diagnostic Steps</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Perplexity calculation returns NaN or infinity</td>\n<td>Division by zero in loss computation, empty validation batches</td>\n<td>Check validation dataset size, examine loss computation implementation</td>\n<td>Ensure non-empty validation set, add numerical stability checks in perplexity calculation</td>\n</tr>\n<tr>\n<td>Fine-tuned model worse than base model</td>\n<td>Overfitting, catastrophic forgetting, poor hyperparameter choices</td>\n<td>Compare training vs validation metrics, test on diverse prompts</td>\n<td>Reduce learning rate, implement early stopping, increase regularization</td>\n</tr>\n<tr>\n<td>Adapter merging produces incorrect outputs</td>\n<td>Weight merging logic error, scaling factor miscalculation</td>\n<td>Test merged vs unmerged models on identical inputs</td>\n<td>Verify merging implementation, check alpha scaling in merge operation</td>\n</tr>\n<tr>\n<td>GGUF export fails with format errors</td>\n<td>Model architecture not supported, tokenizer incompatibility</td>\n<td>Check model architecture compatibility, verify tokenizer config</td>\n<td>Use compatible model architecture, export tokenizer separately if needed</td>\n</tr>\n<tr>\n<td>Exported model significantly slower than expected</td>\n<td>Quantization not preserved in export, inefficient format conversion</td>\n<td>Profile inference speed, check exported model precision</td>\n<td>Preserve quantization in export, use appropriate GGUF quantization settings</td>\n</tr>\n<tr>\n<td>Quality degradation after export</td>\n<td>Format conversion introduces precision loss</td>\n<td>Compare outputs before/after export on test set</td>\n<td>Use higher precision export, implement export quality validation</td>\n</tr>\n</tbody></table>\n<h3 id=\"debugging-tools-and-techniques\">Debugging Tools and Techniques</h3>\n<p>Effective debugging of LLM fine-tuning requires a combination of system-level monitoring, model-specific analysis tools, and careful experimental design. The challenge is distinguishing between hardware issues, implementation bugs, and fundamental algorithmic problems. Think of this toolkit as a medical diagnostic suite - different tools reveal different aspects of the system&#39;s health.</p>\n<h4 id=\"gpu-profiling-and-memory-analysis\">GPU Profiling and Memory Analysis</h4>\n<p>Understanding GPU behavior during fine-tuning is critical because many issues manifest as performance problems rather than explicit errors. The GPU operates as a complex pipeline where memory allocation, kernel execution, and data transfer can all become bottlenecks.</p>\n<table>\n<thead>\n<tr>\n<th>Tool</th>\n<th>Purpose</th>\n<th>Usage Pattern</th>\n<th>Key Metrics</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>nvidia-smi</code></td>\n<td>Real-time GPU monitoring</td>\n<td>Run in separate terminal with <code>-l 1</code> for continuous updates</td>\n<td>GPU utilization %, memory usage MB, temperature, power draw</td>\n</tr>\n<tr>\n<td><code>torch.profiler</code></td>\n<td>PyTorch kernel profiling</td>\n<td>Wrap training steps, export Chrome trace format</td>\n<td>Kernel execution time, memory allocations, CUDA API calls</td>\n</tr>\n<tr>\n<td><code>py-spy</code></td>\n<td>Python CPU profiling</td>\n<td>Sample running training process</td>\n<td>Function call frequency, CPU hotspots, GIL contention</td>\n</tr>\n<tr>\n<td><code>torch.cuda.memory</code></td>\n<td>CUDA memory debugging</td>\n<td>Call at checkpoints during training</td>\n<td>Allocated vs cached memory, memory fragmentation</td>\n</tr>\n<tr>\n<td><code>wandb.log</code></td>\n<td>Training metrics streaming</td>\n<td>Log every training step</td>\n<td>Loss curves, learning rate, gradient norms, hardware metrics</td>\n</tr>\n</tbody></table>\n<p>The <code>MemoryMonitor</code> class provides systematic memory tracking throughout the pipeline. It establishes baseline measurements before model loading and tracks usage at each stage:</p>\n<table>\n<thead>\n<tr>\n<th>Stage</th>\n<th>Memory Measurement</th>\n<th>Typical Patterns</th>\n<th>Warning Signs</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Baseline</td>\n<td>System memory before any model loading</td>\n<td>1-4 GB system RAM usage</td>\n<td>High baseline indicates memory leaks from previous runs</td>\n</tr>\n<tr>\n<td>Model Loading</td>\n<td>GPU memory after base model + quantization</td>\n<td>40-60% of available VRAM for 7B models</td>\n<td>&gt;80% usage indicates insufficient VRAM for training</td>\n</tr>\n<tr>\n<td>Adapter Injection</td>\n<td>Additional memory for LoRA matrices</td>\n<td>1-5% increase over base model</td>\n<td>Large increase suggests incorrect target module selection</td>\n</tr>\n<tr>\n<td>Training Start</td>\n<td>Memory after first forward pass</td>\n<td>Additional 20-40% for optimizer states</td>\n<td>OOM on first step indicates batch size too large</td>\n</tr>\n<tr>\n<td>Training Steady</td>\n<td>Memory during sustained training</td>\n<td>Stable usage with minor fluctuations</td>\n<td>Continuous growth indicates memory leak</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Performance Insight: The CUDA Context Trap</strong></p>\n<p>PyTorch creates a CUDA context that reserves GPU memory even before model loading. This &quot;invisible&quot; memory usage can cause OOM errors that appear to be model-related but are actually context overhead. Always measure baseline GPU memory before any PyTorch operations.</p>\n</blockquote>\n<h4 id=\"training-visualization-and-analysis\">Training Visualization and Analysis</h4>\n<p>Training fine-tuning models requires monitoring multiple interconnected metrics simultaneously. Unlike traditional machine learning where accuracy provides clear feedback, LLM fine-tuning involves balancing multiple objectives that can conflict.</p>\n<table>\n<thead>\n<tr>\n<th>Visualization</th>\n<th>Purpose</th>\n<th>Update Frequency</th>\n<th>Interpretation Guidelines</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Loss curves (train/val)</td>\n<td>Monitor convergence and overfitting</td>\n<td>Every training step</td>\n<td>Smooth downward trend expected, validate gap indicates overfitting</td>\n</tr>\n<tr>\n<td>Learning rate schedule</td>\n<td>Verify optimizer behavior</td>\n<td>Every optimizer step</td>\n<td>Should follow configured schedule, sudden changes indicate bugs</td>\n</tr>\n<tr>\n<td>Gradient norms</td>\n<td>Detect training instability</td>\n<td>Every backward pass</td>\n<td>Values &gt;1.0 may indicate instability, sudden spikes suggest gradient explosion</td>\n</tr>\n<tr>\n<td>Memory usage over time</td>\n<td>Identify memory leaks</td>\n<td>Every epoch</td>\n<td>Should be stable after initial allocation, growth indicates leaks</td>\n</tr>\n<tr>\n<td>Token throughput</td>\n<td>Measure training efficiency</td>\n<td>Every logging interval</td>\n<td>Consistent throughput expected, drops indicate bottlenecks</td>\n</tr>\n<tr>\n<td>Perplexity validation</td>\n<td>Assess model quality</td>\n<td>Every evaluation</td>\n<td>Should decrease over training, sudden increases suggest overfitting</td>\n</tr>\n</tbody></table>\n<p>The <code>TrainingMetrics</code> and <code>EvaluationMetrics</code> structures provide standardized data collection for these visualizations. Key patterns to watch for:</p>\n<p><strong>Healthy Training Patterns:</strong></p>\n<ul>\n<li>Training loss decreases smoothly with minor fluctuations</li>\n<li>Validation loss follows training loss with small gap (1-2x training loss)</li>\n<li>Gradient norms remain stable between 0.1-1.0</li>\n<li>Memory usage stable after initial allocation</li>\n<li>Perplexity decreases consistently during evaluation</li>\n</ul>\n<p><strong>Warning Signs:</strong></p>\n<ul>\n<li>Loss curves show sudden spikes or plateaus</li>\n<li>Validation loss diverges significantly from training loss</li>\n<li>Gradient norms show sudden spikes &gt;10.0</li>\n<li>Memory usage grows continuously throughout training</li>\n<li>Perplexity increases or oscillates wildly</li>\n</ul>\n<h4 id=\"model-behavior-analysis\">Model Behavior Analysis</h4>\n<p>Understanding how fine-tuning changes model behavior requires systematic comparison between base and adapted models. This analysis helps distinguish between successful learning and problematic changes like catastrophic forgetting.</p>\n<table>\n<thead>\n<tr>\n<th>Analysis Type</th>\n<th>Methodology</th>\n<th>Success Indicators</th>\n<th>Failure Indicators</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Output quality comparison</td>\n<td>Generate responses to fixed test prompts</td>\n<td>Improved relevance, maintained fluency</td>\n<td>Degraded language quality, nonsensical outputs</td>\n</tr>\n<tr>\n<td>Instruction following</td>\n<td>Test adherence to specific formatting requests</td>\n<td>Better following of instructions</td>\n<td>Ignoring instructions, reverting to base behavior</td>\n</tr>\n<tr>\n<td>Domain knowledge retention</td>\n<td>Query general knowledge outside training domain</td>\n<td>Maintains base model capabilities</td>\n<td>Loss of general knowledge, catastrophic forgetting</td>\n</tr>\n<tr>\n<td>Response consistency</td>\n<td>Multiple generations for same prompt</td>\n<td>Consistent high-quality responses</td>\n<td>High variance, occasional nonsensical outputs</td>\n</tr>\n<tr>\n<td>Length and formatting</td>\n<td>Analyze response characteristics</td>\n<td>Appropriate length, proper formatting</td>\n<td>Extremely short/long responses, format violations</td>\n</tr>\n</tbody></table>\n<p>The <code>ModelComparator</code> class enables systematic before-and-after analysis:</p>\n<table>\n<thead>\n<tr>\n<th>Comparison Method</th>\n<th>Base Model Response</th>\n<th>Fine-tuned Response</th>\n<th>Evaluation Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Factual QA</td>\n<td>Accurate but generic</td>\n<td>Accurate with domain-specific details</td>\n<td>Accuracy maintained, specificity improved</td>\n</tr>\n<tr>\n<td>Creative writing</td>\n<td>Coherent but bland</td>\n<td>Coherent with target style</td>\n<td>Coherence maintained, style successfully adapted</td>\n</tr>\n<tr>\n<td>Code generation</td>\n<td>Syntactically correct</td>\n<td>Syntactically correct with conventions</td>\n<td>Correctness maintained, conventions learned</td>\n</tr>\n<tr>\n<td>Instruction following</td>\n<td>Partial compliance</td>\n<td>Full compliance with formatting</td>\n<td>Instruction adherence significantly improved</td>\n</tr>\n</tbody></table>\n<h4 id=\"systematic-debugging-methodology\">Systematic Debugging Methodology</h4>\n<p>When facing a fine-tuning issue, follow this systematic approach to isolate the root cause:</p>\n<p><strong>Phase 1: Information Gathering (5-10 minutes)</strong></p>\n<ol>\n<li>Record exact error message, loss values, and training metrics at failure point</li>\n<li>Capture hardware state (GPU memory, temperature, utilization)</li>\n<li>Document recent changes to configuration, data, or code</li>\n<li>Check system logs for hardware errors or resource limits</li>\n<li>Verify all dependencies and versions match known-working configurations</li>\n</ol>\n<p><strong>Phase 2: Minimal Reproduction (15-30 minutes)</strong></p>\n<ol>\n<li>Create smallest possible dataset that reproduces the issue (10-100 samples)</li>\n<li>Test with minimal configuration (smallest model, lowest rank, shortest sequences)</li>\n<li>Run base model without adapters to isolate LoRA-specific issues</li>\n<li>Test data loading and tokenization independently from training</li>\n<li>Verify issue persists across different random seeds</li>\n</ol>\n<p><strong>Phase 3: Component Isolation (20-40 minutes)</strong></p>\n<ol>\n<li>Test data preparation component independently with sample data</li>\n<li>Verify LoRA configuration produces expected parameter counts</li>\n<li>Test quantization setup with base model inference</li>\n<li>Run single training step to isolate training loop issues</li>\n<li>Test evaluation metrics with known-good checkpoints</li>\n</ol>\n<p><strong>Phase 4: Root Cause Analysis (Variable duration)</strong></p>\n<ol>\n<li>Compare configurations against known-working baselines</li>\n<li>Analyze data distribution differences between working and failing cases</li>\n<li>Profile memory usage and computation patterns</li>\n<li>Test incremental configuration changes to isolate problematic settings</li>\n<li>Validate assumptions about model architecture and data formats</li>\n</ol>\n<h3 id=\"effective-logging-strategies\">Effective Logging Strategies</h3>\n<p>Logging for LLM fine-tuning requires balancing information richness with performance impact. Unlike traditional applications where detailed logging has minimal overhead, fine-tuning can process thousands of samples per second where excessive logging becomes a bottleneck.</p>\n<p>Think of logging strategy like flight data recording - capture enough information to diagnose any failure, but don&#39;t let the recording system interfere with the flight. The key is logging the right information at the right frequency to enable post-hoc analysis without impacting training performance.</p>\n<h4 id=\"structured-logging-schema\">Structured Logging Schema</h4>\n<p>All logging should follow a structured format that enables automated analysis and alerting. The logging schema supports both human debugging and automated monitoring systems:</p>\n<table>\n<thead>\n<tr>\n<th>Log Level</th>\n<th>Purpose</th>\n<th>Frequency</th>\n<th>Example Content</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ERROR</td>\n<td>System failures requiring immediate attention</td>\n<td>Only on failures</td>\n<td>&quot;CUDA out of memory during forward pass: allocated 15.2GB, requested 2.1GB additional&quot;</td>\n</tr>\n<tr>\n<td>WARNING</td>\n<td>Concerning patterns that don&#39;t stop execution</td>\n<td>Per epoch or significant events</td>\n<td>&quot;Validation loss increased for 3 consecutive evaluations, early stopping in 2 more&quot;</td>\n</tr>\n<tr>\n<td>INFO</td>\n<td>Major pipeline milestones and configuration</td>\n<td>Component boundaries</td>\n<td>&quot;Loaded model with 4-bit quantization: 6.7B params reduced to 3.8GB VRAM&quot;</td>\n</tr>\n<tr>\n<td>DEBUG</td>\n<td>Detailed information for troubleshooting</td>\n<td>Configurable intervals</td>\n<td>&quot;Batch 150: loss=2.341, lr=3.2e-5, grad_norm=0.87, memory=12.3GB&quot;</td>\n</tr>\n</tbody></table>\n<h4 id=\"training-progress-logging\">Training Progress Logging</h4>\n<p>Training progress requires logging multiple interconnected metrics that reveal both immediate training health and longer-term trends:</p>\n<table>\n<thead>\n<tr>\n<th>Metric Category</th>\n<th>Logging Frequency</th>\n<th>Key Fields</th>\n<th>Alerting Thresholds</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Loss and convergence</td>\n<td>Every training step</td>\n<td><code>step</code>, <code>epoch</code>, <code>loss</code>, <code>learning_rate</code>, <code>grad_norm</code></td>\n<td>Loss &gt;10.0, grad_norm &gt;5.0, NaN values</td>\n</tr>\n<tr>\n<td>Performance metrics</td>\n<td>Every logging interval</td>\n<td><code>samples_per_second</code>, <code>tokens_per_second</code>, <code>gpu_utilization</code></td>\n<td>&lt;50% target throughput, &lt;70% GPU utilization</td>\n</tr>\n<tr>\n<td>Memory tracking</td>\n<td>Every epoch</td>\n<td><code>gpu_memory_used</code>, <code>gpu_memory_cached</code>, <code>system_memory_used</code></td>\n<td>&gt;95% GPU memory, continuous growth</td>\n</tr>\n<tr>\n<td>Model quality</td>\n<td>Every evaluation</td>\n<td><code>eval_loss</code>, <code>perplexity</code>, <code>task_metrics</code></td>\n<td>Perplexity increase &gt;20%, eval_loss divergence</td>\n</tr>\n</tbody></table>\n<p>The <code>TrainingMetrics</code> structure captures training state at each step:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>step: 1247\nepoch: 2.34\nloss: 2.183\nlearning_rate: 2.8e-5\ngrad_norm: 0.92\ntimestamp: 1701234567.89\ngpu_memory_used: 11.2\nsamples_per_second: 3.4</code></pre></div>\n\n<h4 id=\"component-specific-logging\">Component-Specific Logging</h4>\n<p>Each pipeline component requires specialized logging to capture component-specific issues and state:</p>\n<p><strong>Data Preparation Logging:</strong></p>\n<ul>\n<li>Sample validation results and filtering statistics</li>\n<li>Tokenization length distributions and truncation rates</li>\n<li>Chat template application success/failure rates</li>\n<li>Train-validation split characteristics and balance</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Event</th>\n<th>Log Level</th>\n<th>Message Template</th>\n<th>Fields</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Dataset loaded</td>\n<td>INFO</td>\n<td>&quot;Loaded {total_samples} samples from {source_file}&quot;</td>\n<td><code>total_samples</code>, <code>source_file</code>, <code>format</code></td>\n</tr>\n<tr>\n<td>Quality filtering</td>\n<td>INFO</td>\n<td>&quot;Filtered {filtered_count}/{total_samples}: {duplicate_pct}% duplicates, {length_pct}% length&quot;</td>\n<td><code>QualityFilterStats</code> fields</td>\n</tr>\n<tr>\n<td>Tokenization complete</td>\n<td>INFO</td>\n<td>&quot;Tokenized dataset: avg_length={avg_len}, max_length={max_len}, truncated={trunc_pct}%&quot;</td>\n<td>Length statistics</td>\n</tr>\n<tr>\n<td>Template errors</td>\n<td>WARNING</td>\n<td>&quot;Chat template failed for {failed_count} samples: {error_summary}&quot;</td>\n<td>Error counts and types</td>\n</tr>\n</tbody></table>\n<p><strong>LoRA Configuration Logging:</strong></p>\n<ul>\n<li>Target module detection results and parameter counts</li>\n<li>Rank and alpha configuration with efficiency metrics</li>\n<li>Adapter injection success and parameter freezing verification</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Event</th>\n<th>Log Level</th>\n<th>Message Template</th>\n<th>Fields</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Target detection</td>\n<td>INFO</td>\n<td>&quot;Detected {attention_count} attention, {mlp_count} MLP modules for adaptation&quot;</td>\n<td>Module counts by type</td>\n</tr>\n<tr>\n<td>Adapter injection</td>\n<td>INFO</td>\n<td>&quot;Injected rank-{rank} adapters: {trainable_params}/{total_params} ({ratio:.2%}) trainable&quot;</td>\n<td><code>AdapterMetrics</code> summary</td>\n</tr>\n<tr>\n<td>Parameter analysis</td>\n<td>DEBUG</td>\n<td>&quot;Adapter efficiency: {memory_mb}MB overhead, {effective_rank:.1f} avg effective rank&quot;</td>\n<td>Efficiency metrics</td>\n</tr>\n</tbody></table>\n<p><strong>Quantization Logging:</strong></p>\n<ul>\n<li>Memory reduction achieved and quantization parameters</li>\n<li>Compatibility verification and performance impact</li>\n<li>Quantization quality assessment</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Event</th>\n<th>Log Level</th>\n<th>Message Template</th>\n<th>Fields</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Quantization setup</td>\n<td>INFO</td>\n<td>&quot;Applied {quant_type} quantization: {original_gb}GB -&gt; {quantized_gb}GB ({reduction:.1%} reduction)&quot;</td>\n<td>Memory statistics</td>\n</tr>\n<tr>\n<td>Quality check</td>\n<td>INFO</td>\n<td>&quot;Quantization quality: {perplexity_increase:.2%} perplexity increase on validation set&quot;</td>\n<td>Quality metrics</td>\n</tr>\n</tbody></table>\n<h4 id=\"error-context-logging\">Error Context Logging</h4>\n<p>When errors occur, comprehensive context logging enables effective debugging. Error logs should capture not just the immediate failure, but the system state leading to the failure:</p>\n<table>\n<thead>\n<tr>\n<th>Error Category</th>\n<th>Context Information</th>\n<th>Example Fields</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Memory errors</td>\n<td>GPU/system memory state, recent allocations, model size</td>\n<td><code>available_memory</code>, <code>requested_memory</code>, <code>largest_free_block</code></td>\n</tr>\n<tr>\n<td>Training instability</td>\n<td>Recent loss values, gradient norms, learning rate</td>\n<td><code>loss_history</code>, <code>grad_norm_history</code>, <code>lr_schedule_position</code></td>\n</tr>\n<tr>\n<td>Data errors</td>\n<td>Sample information, tokenization state, validation results</td>\n<td><code>sample_id</code>, <code>sequence_length</code>, <code>validation_errors</code></td>\n</tr>\n<tr>\n<td>Configuration errors</td>\n<td>Full configuration dump, compatibility checks</td>\n<td><code>full_config</code>, <code>version_info</code>, <code>hardware_capabilities</code></td>\n</tr>\n</tbody></table>\n<p><strong>Error Log Format Example:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>ERROR [2024-01-15 14:32:17] Training step failed\nContext:\n  step: 1247\n  epoch: 2.34\n  recent_losses: [2.18, 2.21, 2.19, 2.17]\n  grad_norm: 15.7\n  gpu_memory: 15.2GB / 16.0GB\n  last_lr: 2.8e-5\n  sample_id: &quot;conversation_4821&quot;\n  sequence_length: 2048\nError: CUDA out of memory during backward pass\nStack trace: [full stack trace]\nRecovery action: Reducing batch size and restarting from last checkpoint</code></pre></div>\n\n<h4 id=\"performance-and-bottleneck-logging\">Performance and Bottleneck Logging</h4>\n<p>Performance logging helps identify bottlenecks and optimization opportunities. This logging focuses on throughput, resource utilization, and timing analysis:</p>\n<table>\n<thead>\n<tr>\n<th>Performance Area</th>\n<th>Metrics</th>\n<th>Logging Frequency</th>\n<th>Analysis Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data loading</td>\n<td>Samples/second, queue utilization, preprocessing time</td>\n<td>Every 100 steps</td>\n<td>Identify data pipeline bottlenecks</td>\n</tr>\n<tr>\n<td>Model computation</td>\n<td>Tokens/second, GPU utilization, kernel timing</td>\n<td>Every logging interval</td>\n<td>Optimize computational efficiency</td>\n</tr>\n<tr>\n<td>Memory management</td>\n<td>Allocation patterns, fragmentation, garbage collection</td>\n<td>Every epoch</td>\n<td>Prevent memory issues</td>\n</tr>\n<tr>\n<td>I/O operations</td>\n<td>Checkpoint save time, log write latency</td>\n<td>Every significant I/O</td>\n<td>Optimize storage performance</td>\n</tr>\n</tbody></table>\n<h4 id=\"logging-configuration-and-management\">Logging Configuration and Management</h4>\n<p>Logging configuration should be environment-aware and performance-conscious. Development environments need detailed debugging information, while production training runs prioritize performance with selective detailed logging:</p>\n<table>\n<thead>\n<tr>\n<th>Environment</th>\n<th>Log Level</th>\n<th>File Output</th>\n<th>Structured Format</th>\n<th>Performance Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Development</td>\n<td>DEBUG</td>\n<td>Local files + console</td>\n<td>JSON for analysis tools</td>\n<td>High detail, moderate performance cost</td>\n</tr>\n<tr>\n<td>Training</td>\n<td>INFO</td>\n<td>Remote logging + local backup</td>\n<td>Structured with metadata</td>\n<td>Balanced detail and performance</td>\n</tr>\n<tr>\n<td>Production</td>\n<td>WARNING</td>\n<td>Centralized logging system</td>\n<td>Compact binary format</td>\n<td>Minimal performance impact</td>\n</tr>\n</tbody></table>\n<p><strong>Adaptive Logging Strategy:</strong></p>\n<ul>\n<li>Start with detailed logging during initial epochs to catch early issues</li>\n<li>Reduce logging frequency after stable training is established</li>\n<li>Increase logging detail when issues are detected (circuit breaker pattern)</li>\n<li>Implement log sampling for high-frequency events to control volume</li>\n</ul>\n<blockquote>\n<p><strong>Logging Performance Principle: Log Smart, Not Hard</strong></p>\n<p>The cost of logging a single training step can equal the cost of processing several samples. Design logging to provide maximum debugging value with minimal performance impact by using sampling, buffering, and adaptive detail levels.</p>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Memory Profiling</td>\n<td><code>torch.cuda.memory_stats()</code> + manual tracking</td>\n<td><code>torch.profiler</code> with Chrome trace export</td>\n</tr>\n<tr>\n<td>Training Visualization</td>\n<td>WandB free tier with basic charts</td>\n<td>Custom TensorBoard + Grafana dashboard</td>\n</tr>\n<tr>\n<td>Log Management</td>\n<td>Python <code>logging</code> module to local files</td>\n<td>ELK stack (Elasticsearch + Logstash + Kibana)</td>\n</tr>\n<tr>\n<td>Performance Monitoring</td>\n<td><code>nvidia-smi</code> + manual observation</td>\n<td>Prometheus + GPU exporters + alerting</td>\n</tr>\n<tr>\n<td>Error Tracking</td>\n<td>Print statements + manual analysis</td>\n<td>Sentry for error aggregation and alerting</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>fine_tuning_pipeline/\n├── debugging/\n│   ├── __init__.py\n│   ├── memory_monitor.py          # MemoryMonitor and GPU tracking\n│   ├── training_monitor.py        # TrainingStabilityMonitor and loss tracking\n│   ├── performance_profiler.py    # Performance analysis and bottleneck detection\n│   ├── error_handler.py           # Structured error logging and recovery\n│   └── diagnostic_tools.py        # Debugging utilities and analysis functions\n├── logging_config/\n│   ├── development.yaml           # Detailed logging for development\n│   ├── training.yaml              # Balanced logging for training runs\n│   └── production.yaml            # Minimal logging for production\n├── monitoring/\n│   ├── dashboards/                # Grafana/TensorBoard dashboard configs\n│   ├── alerts/                    # Alerting rules and thresholds\n│   └── metrics_collectors.py      # Custom metrics collection\n└── tools/\n    ├── log_analyzer.py            # Automated log analysis and issue detection\n    ├── memory_analyzer.py         # Memory usage analysis and optimization\n    └── performance_benchmark.py   # Performance baseline establishment</code></pre></div>\n\n<h4 id=\"core-debugging-infrastructure\">Core Debugging Infrastructure</h4>\n<p><strong>Memory Monitoring System (Complete Implementation):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> psutil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MemoryMetrics</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stage: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gpu_memory_allocated: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gpu_memory_cached: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gpu_memory_reserved: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    system_memory_used: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    details: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MemoryMonitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.baseline_gpu_memory: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.baseline_system_memory: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.measurements: List[Dict] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.peak_gpu_usage: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.peak_system_usage: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> capture_baseline</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record initial memory state before model loading.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            torch.cuda.empty_cache()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.baseline_gpu_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_allocated()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.baseline_gpu_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.baseline_system_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.virtual_memory().used</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.measurements.clear()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> measure_current_usage</span><span style=\"color:#E1E4E8\">(self, stage: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Measure current memory and return usage statistics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            gpu_allocated </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_allocated()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            gpu_cached </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_cached()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            gpu_reserved </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_reserved()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            gpu_allocated </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> gpu_cached </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> gpu_reserved </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        system_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.virtual_memory().used</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Convert to MB for readability</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        gpu_allocated_mb </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> gpu_allocated </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\"> **</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        gpu_cached_mb </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> gpu_cached </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\"> **</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        gpu_reserved_mb </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> gpu_reserved </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\"> **</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        system_memory_mb </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> system_memory </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\"> **</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metrics </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MemoryMetrics(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            stage</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">stage,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            gpu_memory_allocated</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">gpu_allocated_mb,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            gpu_memory_cached</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">gpu_cached_mb,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            gpu_memory_reserved</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">gpu_reserved_mb,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            system_memory_used</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">system_memory_mb,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.time(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            details</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"baseline_gpu_delta\"</span><span style=\"color:#E1E4E8\">: gpu_allocated_mb </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.baseline_gpu_memory </span><span style=\"color:#F97583\">or</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\"> **</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"baseline_system_delta\"</span><span style=\"color:#E1E4E8\">: system_memory_mb </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.baseline_system_memory </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\"> **</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"gpu_utilization_pct\"</span><span style=\"color:#E1E4E8\">: gpu_allocated </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> gpu_reserved </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> gpu_reserved </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> else</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.measurements.append(metrics.</span><span style=\"color:#79B8FF\">__dict__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.peak_gpu_usage </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.peak_gpu_usage, gpu_allocated_mb)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.peak_system_usage </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.peak_system_usage, system_memory_mb)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"stage\"</span><span style=\"color:#E1E4E8\">: stage,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"gpu_allocated_mb\"</span><span style=\"color:#E1E4E8\">: gpu_allocated_mb,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"gpu_cached_mb\"</span><span style=\"color:#E1E4E8\">: gpu_cached_mb,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"system_memory_mb\"</span><span style=\"color:#E1E4E8\">: system_memory_mb,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"baseline_delta_mb\"</span><span style=\"color:#E1E4E8\">: metrics.details[</span><span style=\"color:#9ECBFF\">\"baseline_gpu_delta\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_peak_usage</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return peak memory usage across all measurements.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"peak_gpu_mb\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.peak_gpu_usage,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"peak_system_mb\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.peak_system_usage,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"baseline_gpu_mb\"</span><span style=\"color:#E1E4E8\">: (</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.baseline_gpu_memory </span><span style=\"color:#F97583\">or</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\"> **</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"baseline_system_mb\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.baseline_system_memory </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\"> **</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"total_measurements\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.measurements)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span></code></pre></div>\n\n<p><strong>Training Stability Monitor (Complete Implementation):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> collections </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> deque</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> math</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional, Deque</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TrainingStabilityMonitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, gradient_clip_norm: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">, loss_spike_threshold: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2.0</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.gradient_clip_norm </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> gradient_clip_norm</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.loss_spike_threshold </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> loss_spike_threshold</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.gradient_history: Deque[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> deque(</span><span style=\"color:#FFAB70\">maxlen</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.loss_history: Deque[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> deque(</span><span style=\"color:#FFAB70\">maxlen</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">50</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.instability_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> check_gradient_stability</span><span style=\"color:#E1E4E8\">(self, model) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Monitor gradient norms and detect explosions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        total_norm </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        param_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> param </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> model.parameters():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> param.grad </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                param_norm </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> param.grad.data.norm(</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                total_norm </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> param_norm.item() </span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                param_count </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> param_count </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">\"status\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"no_gradients\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"total_norm\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        total_norm </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> total_norm </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">. </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.gradient_history.append(total_norm)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Calculate recent trend</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        recent_grads </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> list</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.gradient_history)[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">:]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        avg_recent </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> sum</span><span style=\"color:#E1E4E8\">(recent_grads) </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(recent_grads) </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> recent_grads </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Detection logic</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        is_explosion </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> total_norm </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 10.0</span><span style=\"color:#F97583\"> or</span><span style=\"color:#E1E4E8\"> total_norm </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> avg_recent </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 5.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        is_vanishing </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> total_norm </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 1e-6</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        is_unstable </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(recent_grads) </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#F97583\"> and</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(recent_grads) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> min</span><span style=\"color:#E1E4E8\">(recent_grads) </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 10.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> is_explosion:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.instability_count </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"status\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"explosion\"</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> is_explosion </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> \"vanishing\"</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> is_vanishing </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> \"unstable\"</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> is_unstable </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> \"stable\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"total_norm\"</span><span style=\"color:#E1E4E8\">: total_norm,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"avg_recent_norm\"</span><span style=\"color:#E1E4E8\">: avg_recent,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"instability_count\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.instability_count,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"should_clip\"</span><span style=\"color:#E1E4E8\">: total_norm </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.gradient_clip_norm,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"clip_factor\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">min</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.gradient_clip_norm </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> total_norm) </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> total_norm </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.gradient_clip_norm </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> handle_gradient_explosion</span><span style=\"color:#E1E4E8\">(self, model, optimizer) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Handle detected gradient explosion with graduated intervention.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        stability </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.check_gradient_stability(model)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> stability[</span><span style=\"color:#9ECBFF\">\"status\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"explosion\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Graduated response based on severity</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> stability[</span><span style=\"color:#9ECBFF\">\"total_norm\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 100.0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Severe explosion - zero gradients and reduce learning rate</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                for</span><span style=\"color:#E1E4E8\"> param </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> model.parameters():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    if</span><span style=\"color:#E1E4E8\"> param.grad </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        param.grad.zero_()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Reduce learning rate by half</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                for</span><span style=\"color:#E1E4E8\"> param_group </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> optimizer.param_groups:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    param_group[</span><span style=\"color:#9ECBFF\">'lr'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">*=</span><span style=\"color:#79B8FF\"> 0.5</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#6A737D\">  # Indicate intervention taken</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            elif</span><span style=\"color:#E1E4E8\"> stability[</span><span style=\"color:#9ECBFF\">\"total_norm\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 10.0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Moderate explosion - clip gradients aggressively</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                torch.nn.utils.clip_grad_norm_(model.parameters(), </span><span style=\"color:#FFAB70\">max_norm</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#6A737D\">  # No intervention needed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> check_loss_stability</span><span style=\"color:#E1E4E8\">(self, current_loss: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Monitor loss values for spikes and degradation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> math.isnan(current_loss) </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> math.isinf(current_loss):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"status\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"invalid\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"loss\"</span><span style=\"color:#E1E4E8\">: current_loss,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"requires_intervention\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.loss_history.append(current_loss)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.loss_history) </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">\"status\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"insufficient_data\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"loss\"</span><span style=\"color:#E1E4E8\">: current_loss}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        recent_losses </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> list</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.loss_history)[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">:]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        avg_recent </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> sum</span><span style=\"color:#E1E4E8\">(recent_losses) </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(recent_losses)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check for sudden spikes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        is_spike </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> current_loss </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> avg_recent </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.loss_spike_threshold</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check for consistent increase (potential divergence)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(recent_losses) </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            increasing_trend </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> all</span><span style=\"color:#E1E4E8\">(recent_losses[i] </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#E1E4E8\"> recent_losses[i</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            increasing_trend </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"status\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"spike\"</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> is_spike </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> \"diverging\"</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> increasing_trend </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> \"stable\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"loss\"</span><span style=\"color:#E1E4E8\">: current_loss,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"avg_recent\"</span><span style=\"color:#E1E4E8\">: avg_recent,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"requires_intervention\"</span><span style=\"color:#E1E4E8\">: is_spike </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> increasing_trend,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"spike_factor\"</span><span style=\"color:#E1E4E8\">: current_loss </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> avg_recent </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> avg_recent </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> else</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> detect_nan_propagation</span><span style=\"color:#E1E4E8\">(self, model, loss) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Detect NaN values in model parameters, gradients, or loss.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check loss</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> math.isnan(loss) </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> math.isinf(loss):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check model parameters and gradients</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> param </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> model.parameters():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> torch.isnan(param).any() </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> torch.isinf(param).any():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> param.grad </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> (torch.isnan(param.grad).any() </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> torch.isinf(param.grad).any()):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> should_restore_checkpoint</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Determine if training instability requires checkpoint restoration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.instability_count </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#F97583\"> or</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">([l </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> l </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.loss_history </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> math.isnan(l) </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> math.isinf(l)]) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span></span></code></pre></div>\n\n<h4 id=\"debugging-tool-skeletons\">Debugging Tool Skeletons</h4>\n<p><strong>Performance Profiler (Implementation Structure):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch.profiler</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> contextlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PerformanceProfiler</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, output_dir: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"./profiles\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.output_dir </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> output_dir</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_profile: Optional[torch.profiler.profile] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.timing_stack </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> profile_training_step</span><span style=\"color:#E1E4E8\">(self, step: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Profile a complete training step with detailed breakdown.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize torch.profiler with GPU and CPU tracking enabled</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Start profiling context manager</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Yield control to training step execution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Stop profiling and export trace to Chrome format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Extract key timing metrics and return summary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use torch.profiler.profile(activities=[CPU, CUDA], record_shapes=True)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @contextmanager</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> time_operation</span><span style=\"color:#E1E4E8\">(self, operation_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Time a specific operation and track in performance metrics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Record start time and push operation onto timing stack</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Yield control to operation execution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate elapsed time and pop operation from stack</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Store timing data with operation context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Check for performance regressions against baselines</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> analyze_bottlenecks</span><span style=\"color:#E1E4E8\">(self, profile_data: Dict) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Analyze profiling data to identify performance bottlenecks.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Extract GPU kernel execution times from profile</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Identify top time-consuming operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate GPU utilization and memory bandwidth</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Detect common bottleneck patterns (data loading, computation, memory)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Generate recommendations for performance optimization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Error Analysis System (Implementation Structure):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> traceback</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> sys</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ErrorCategory</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MEMORY_ERROR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"memory\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TRAINING_INSTABILITY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"training\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DATA_ERROR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"data\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CONFIGURATION_ERROR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"config\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HARDWARE_ERROR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"hardware\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ErrorContext</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    category: ErrorCategory</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    message: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stack_trace: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    system_state: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    recovery_actions: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ErrorAnalyzer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.error_patterns </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._load_error_patterns()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.recovery_strategies </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._load_recovery_strategies()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> analyze_exception</span><span style=\"color:#E1E4E8\">(self, exception: </span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">, context: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> ErrorContext:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Analyze exception and provide structured error context.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Classify exception type into error categories</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Extract relevant system state (memory, GPU, training metrics)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Match error pattern against known issues database</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Generate specific recovery recommendations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Create structured error context for logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> suggest_recovery_actions</span><span style=\"color:#E1E4E8\">(self, error_category: ErrorCategory, context: Dict) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Suggest specific recovery actions based on error analysis.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Look up category-specific recovery strategies</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Filter strategies based on current system state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Prioritize actions by likelihood of success</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Include specific parameter adjustments or config changes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return ordered list of recovery actions to attempt</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _load_error_patterns</span><span style=\"color:#E1E4E8\">(self) -> Dict:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load database of known error patterns and classifications.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"cuda.*out of memory\"</span><span style=\"color:#E1E4E8\">: ErrorCategory.</span><span style=\"color:#79B8FF\">MEMORY_ERROR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"gradient.*explosion\"</span><span style=\"color:#E1E4E8\">: ErrorCategory.</span><span style=\"color:#79B8FF\">TRAINING_INSTABILITY</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"tokenizer.*encode.*failed\"</span><span style=\"color:#E1E4E8\">: ErrorCategory.</span><span style=\"color:#79B8FF\">DATA_ERROR</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Expand with comprehensive error pattern database</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span></code></pre></div>\n\n<h4 id=\"milestone-validation-checkpoints\">Milestone Validation Checkpoints</h4>\n<p><strong>After Dataset Preparation (Milestone 1):</strong></p>\n<ul>\n<li>Run <code>python -m debugging.memory_monitor</code> to establish baseline memory usage</li>\n<li>Verify data loading completes without memory growth over multiple epochs</li>\n<li>Check tokenization produces expected sequence lengths and no truncation warnings</li>\n<li>Confirm chat template application succeeds on 100% of samples</li>\n</ul>\n<p><strong>After LoRA Configuration (Milestone 2):</strong></p>\n<ul>\n<li>Validate adapter injection with <code>python -m debugging.diagnostic_tools --check-adapters</code></li>\n<li>Confirm trainable parameter ratio &lt;1% with <code>ParameterAnalyzer.analyze_parameter_distribution()</code></li>\n<li>Test gradient flow through adapters with dummy backward pass</li>\n<li>Verify base model parameters remain frozen during adapter training</li>\n</ul>\n<p><strong>After Training Implementation (Milestone 4):</strong></p>\n<ul>\n<li>Monitor training stability for first 100 steps with <code>TrainingStabilityMonitor</code></li>\n<li>Confirm loss decreases smoothly without spikes or plateaus</li>\n<li>Validate checkpoint saving/loading maintains training state consistency</li>\n<li>Test early stopping triggers correctly on validation loss plateau</li>\n</ul>\n<p><strong>After Complete Pipeline (Milestone 5):</strong></p>\n<ul>\n<li>Run end-to-end memory profiling to identify peak usage and potential leaks</li>\n<li>Validate exported model produces identical outputs to merged checkpoint</li>\n<li>Confirm performance benchmarks meet throughput requirements</li>\n<li>Test recovery from various failure modes (OOM, gradient explosion, corruption)</li>\n</ul>\n<h2 id=\"future-extensions\">Future Extensions</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - this section outlines the evolution path for the fine-tuning pipeline, describing scalability enhancements, alternative parameter-efficient techniques, and advanced evaluation methods that build upon the foundational system</p>\n</blockquote>\n<p>The current fine-tuning pipeline provides a solid foundation for parameter-efficient model adaptation, but several exciting extensions can enhance its capabilities, scalability, and evaluation depth. These future enhancements fall into three primary categories: scalability improvements that enable training larger models on multiple GPUs, alternative adapter methods that offer different trade-offs between efficiency and performance, and advanced evaluation techniques that provide deeper insights into model capabilities and safety.</p>\n<h3 id=\"mental-model-the-workshop-evolution\">Mental Model: The Workshop Evolution</h3>\n<p>Think of the current fine-tuning pipeline as a skilled craftsperson&#39;s workshop equipped with essential tools for creating custom adaptations. The future extensions represent the natural evolution of this workshop: expanding to multiple workstations for larger projects (multi-GPU training), acquiring specialized tools for different crafting techniques (alternative adapter methods), and developing sophisticated quality assessment methods (advanced evaluation). Just as a master craftsperson continuously expands their capabilities while maintaining their core expertise, these extensions build upon the existing pipeline&#39;s strengths while opening new possibilities for scale and precision.</p>\n<p>The extensions maintain backward compatibility with the existing system while providing optional enhanced capabilities. Users can choose to adopt specific extensions based on their requirements, whether that&#39;s scaling to larger models, experimenting with cutting-edge adapter techniques, or implementing comprehensive safety evaluations.</p>\n<h3 id=\"scalability-extensions\">Scalability Extensions</h3>\n<p>Multi-GPU training and distributed fine-tuning represent the most significant scalability enhancements for the pipeline. These extensions enable training larger models that cannot fit on a single GPU and dramatically reduce training time through parallelization. The scalability extensions maintain the parameter-efficiency principles while distributing computation across multiple devices.</p>\n<h4 id=\"distributed-training-architecture\">Distributed Training Architecture</h4>\n<p>The distributed training extension adds a coordination layer above the existing training components. This coordination layer manages multiple GPU workers, each running a copy of the training loop with synchronized gradient updates. The architecture supports both data parallelism, where different workers process different batches of the same dataset, and model parallelism, where different parts of the model reside on different GPUs.</p>\n<p><strong>Data parallelism</strong> represents the more straightforward extension, where each GPU worker loads the full model and processes different subsets of training data. Workers periodically synchronize their gradients using an all-reduce operation that combines gradients across all workers before applying optimizer updates. This approach scales well for models that fit on individual GPUs but benefit from larger effective batch sizes.</p>\n<p><strong>Model parallelism</strong> becomes necessary when individual model layers exceed single GPU memory capacity. The pipeline extension partitions the model across multiple GPUs, with different layers residing on different devices. Activations flow between GPUs as they pass through the network, requiring careful scheduling to minimize communication overhead and maintain high GPU utilization.</p>\n<table>\n<thead>\n<tr>\n<th>Parallelism Strategy</th>\n<th>Memory Scaling</th>\n<th>Communication Overhead</th>\n<th>Implementation Complexity</th>\n<th>Best Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Parallelism</td>\n<td>Linear per worker</td>\n<td>Gradient sync only</td>\n<td>Low</td>\n<td>Models that fit on single GPU</td>\n</tr>\n<tr>\n<td>Pipeline Parallelism</td>\n<td>Linear with layers</td>\n<td>Activation passing</td>\n<td>Medium</td>\n<td>Very large models with sequential structure</td>\n</tr>\n<tr>\n<td>Tensor Parallelism</td>\n<td>Linear with model width</td>\n<td>High-frequency sync</td>\n<td>High</td>\n<td>Massive transformer layers</td>\n</tr>\n<tr>\n<td>Hybrid Approaches</td>\n<td>Best of multiple</td>\n<td>Balanced</td>\n<td>Very High</td>\n<td>Largest possible models</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Gradient Synchronization Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Distributed training requires coordinating gradient updates across multiple workers to maintain training convergence</li>\n<li><strong>Options Considered</strong>: Synchronous all-reduce, asynchronous parameter servers, elastic averaging</li>\n<li><strong>Decision</strong>: Implement synchronous all-reduce with gradient compression</li>\n<li><strong>Rationale</strong>: Synchronous training provides deterministic convergence behavior critical for reproducible fine-tuning results, while gradient compression reduces communication bandwidth requirements</li>\n<li><strong>Consequences</strong>: Enables linear scaling with worker count while maintaining training stability, but requires workers to synchronize at each step</li>\n</ul>\n</blockquote>\n<h4 id=\"memory-efficient-distributed-quantization\">Memory-Efficient Distributed Quantization</h4>\n<p>Combining QLoRA quantization with distributed training requires careful coordination to maintain memory efficiency gains. The extension implements <strong>gradient synchronization in higher precision</strong> while maintaining 4-bit model weights on each worker. This approach prevents quantization errors from accumulating across gradient synchronization rounds while preserving the memory benefits of quantized storage.</p>\n<p>The distributed quantization system also supports <strong>heterogeneous GPU configurations</strong> where workers have different memory capacities. Workers with larger memory can process larger micro-batches or maintain larger gradient accumulation buffers, while the coordination layer balances workload distribution to prevent bottlenecks from slower workers.</p>\n<p><strong>Quantization-aware load balancing</strong> distributes model components across workers based on their quantized memory footprint rather than parameter count. Since different layer types compress differently under 4-bit quantization, this dynamic allocation ensures more even memory utilization across workers.</p>\n<h4 id=\"multi-gpu-lora-coordination\">Multi-GPU LoRA Coordination</h4>\n<p>Distributing LoRA adapters across multiple GPUs requires sophisticated coordination to maintain the low-rank structure while enabling parallel computation. The extension implements <strong>adapter sharding strategies</strong> that split individual LoRA matrices across workers or distribute entire adapter modules to different GPUs based on the target module layout.</p>\n<p><strong>Cross-GPU adapter communication</strong> becomes critical when adapter matrices span multiple devices. The system implements efficient communication patterns that minimize adapter weight transfers during forward and backward passes. For adapters that require cross-GPU computation, the system batches communication operations to reduce latency overhead.</p>\n<p>The distributed LoRA system also supports <strong>dynamic adapter scaling</strong> where the effective rank can be increased by adding more workers, each contributing additional low-rank components to the adaptation. This approach enables scaling adapter capacity with available hardware resources.</p>\n<h3 id=\"alternative-adapter-methods\">Alternative Adapter Methods</h3>\n<p>While LoRA provides an excellent balance of efficiency and performance, several emerging adapter techniques offer different trade-offs that may be beneficial for specific use cases. These alternative methods can be integrated into the existing pipeline architecture as additional options alongside LoRA.</p>\n<h4 id=\"adalora-adaptive-low-rank-adaptation\">AdaLoRA: Adaptive Low-Rank Adaptation</h4>\n<p><strong>AdaLoRA</strong> extends basic LoRA by dynamically adjusting the rank allocation across different modules during training. Instead of using a fixed rank for all adapter modules, AdaLoRA starts with a higher initial rank and progressively prunes less important singular value directions based on gradient information and importance scores.</p>\n<p>The adaptive rank allocation algorithm monitors the <strong>singular value importance</strong> of each adapter matrix throughout training. Modules that show high gradient activity in their singular value directions retain higher ranks, while modules with low activity have their ranks reduced through progressive pruning. This dynamic allocation results in more efficient parameter utilization compared to fixed-rank LoRA.</p>\n<p><strong>Rank budget management</strong> becomes a key component of the AdaLoRA extension. Users specify a total parameter budget rather than per-module ranks, and the system automatically allocates this budget across modules based on their learning dynamics. This approach often achieves better performance than LoRA with the same total parameter count.</p>\n<table>\n<thead>\n<tr>\n<th>Adapter Method</th>\n<th>Rank Strategy</th>\n<th>Parameter Efficiency</th>\n<th>Training Complexity</th>\n<th>Best Use Cases</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>LoRA</td>\n<td>Fixed rank per module</td>\n<td>High with proper tuning</td>\n<td>Low</td>\n<td>General fine-tuning, proven baselines</td>\n</tr>\n<tr>\n<td>AdaLoRA</td>\n<td>Dynamic rank allocation</td>\n<td>Higher than LoRA</td>\n<td>Medium</td>\n<td>Tasks requiring varied module adaptation</td>\n</tr>\n<tr>\n<td>DoRA</td>\n<td>Weight decomposition</td>\n<td>Competitive with LoRA</td>\n<td>Medium</td>\n<td>Tasks needing magnitude and direction control</td>\n</tr>\n<tr>\n<td>QLoRA</td>\n<td>Fixed rank + quantization</td>\n<td>Highest memory efficiency</td>\n<td>Low</td>\n<td>Memory-constrained environments</td>\n</tr>\n<tr>\n<td>LoRA+</td>\n<td>Different learning rates</td>\n<td>Similar to LoRA</td>\n<td>Low</td>\n<td>Fine-grained optimization control</td>\n</tr>\n</tbody></table>\n<h4 id=\"dora-weight-decomposed-low-rank-adaptation\">DoRA: Weight-Decomposed Low-Rank Adaptation</h4>\n<p><strong>DoRA (Weight-Decomposed Low-Rank Adaptation)</strong> decomposes weight updates into magnitude and directional components, applying low-rank adaptation specifically to the directional component while allowing full-rank updates to the magnitude. This decomposition often provides better performance than standard LoRA because magnitude and direction updates have different optimization dynamics.</p>\n<p>The DoRA extension modifies the adapter injection process to <strong>separate magnitude and direction updates</strong>. For each target module weight matrix W, DoRA decomposes updates into magnitude scaling factors and directional changes. The low-rank adaptation applies only to the directional component, while magnitude updates use learned scaling parameters.</p>\n<p><strong>Magnitude-direction optimization</strong> requires different learning rate schedules for the two components. The magnitude parameters typically benefit from lower learning rates since they control the overall scale of activations, while directional parameters can use higher learning rates since they operate in the normalized space.</p>\n<h4 id=\"lora-and-multi-learning-rate-extensions\">LoRA+ and Multi-Learning-Rate Extensions</h4>\n<p><strong>LoRA+</strong> introduces different learning rates for the A and B matrices in LoRA decomposition, based on the observation that these matrices have different roles in the adaptation process. The A matrix (which receives the input) and B matrix (which produces the output) benefit from different optimization dynamics.</p>\n<p>The multi-learning-rate extension allows fine-grained control over adaptation speed across different components. <strong>Matrix-specific learning rates</strong> can be tuned based on the singular value spectrum of the target modules, with matrices corresponding to larger singular values receiving lower learning rates to prevent over-adaptation.</p>\n<p><strong>Layer-wise learning rate adaptation</strong> extends this concept to automatically adjust learning rates based on the depth of the target modules. Deeper layers, which often require more delicate adaptation to avoid catastrophic forgetting, receive progressively lower learning rates.</p>\n<blockquote>\n<p><strong>Decision: Adapter Method Plugin Architecture</strong></p>\n<ul>\n<li><strong>Context</strong>: Supporting multiple adapter methods requires extensible architecture without complicating the core pipeline</li>\n<li><strong>Options Considered</strong>: Monolithic implementation with all methods, plugin-based architecture, separate pipelines per method</li>\n<li><strong>Decision</strong>: Implement plugin-based adapter architecture with common interfaces</li>\n<li><strong>Rationale</strong>: Plugin architecture enables easy addition of new methods while maintaining code clarity and allowing users to include only needed adapters</li>\n<li><strong>Consequences</strong>: Enables rapid experimentation with new techniques while keeping the core pipeline focused, but requires careful interface design for compatibility</li>\n</ul>\n</blockquote>\n<h3 id=\"advanced-evaluation\">Advanced Evaluation</h3>\n<p>Standard perplexity and task-specific metrics provide important baseline assessments, but advanced evaluation techniques offer deeper insights into model capabilities, safety characteristics, and real-world performance. These advanced evaluation methods help practitioners understand not just whether fine-tuning improved performance, but how and why the improvements occurred.</p>\n<h4 id=\"human-evaluation-integration\">Human Evaluation Integration</h4>\n<p><strong>Human evaluation systems</strong> provide ground-truth assessment of model outputs that automated metrics cannot capture. The advanced evaluation extension integrates with human evaluation platforms to collect structured feedback on model responses across multiple dimensions including accuracy, helpfulness, harmfulness, and coherence.</p>\n<p>The human evaluation component implements <strong>comparative assessment protocols</strong> where human raters compare outputs from the fine-tuned model against baseline models and reference responses. This comparative approach reduces absolute rating bias and provides more reliable relative performance measurements.</p>\n<p><strong>Inter-rater reliability tracking</strong> ensures evaluation quality by monitoring agreement between human evaluators. The system implements multiple rater assignment for critical evaluations and provides calibration exercises to maintain consistent evaluation standards across different raters.</p>\n<table>\n<thead>\n<tr>\n<th>Evaluation Method</th>\n<th>Cost</th>\n<th>Scalability</th>\n<th>Reliability</th>\n<th>Coverage</th>\n<th>Best Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Human Evaluation</td>\n<td>High</td>\n<td>Low</td>\n<td>High with calibration</td>\n<td>Deep but narrow</td>\n<td>Critical safety assessment</td>\n</tr>\n<tr>\n<td>Automated Benchmarks</td>\n<td>Low</td>\n<td>High</td>\n<td>Medium</td>\n<td>Broad but shallow</td>\n<td>Rapid iteration feedback</td>\n</tr>\n<tr>\n<td>Model-Based Evaluation</td>\n<td>Medium</td>\n<td>High</td>\n<td>Variable</td>\n<td>Moderate</td>\n<td>Scalable quality assessment</td>\n</tr>\n<tr>\n<td>Adversarial Testing</td>\n<td>Medium</td>\n<td>Medium</td>\n<td>High</td>\n<td>Targeted</td>\n<td>Safety and robustness validation</td>\n</tr>\n<tr>\n<td>Real-World Deployment</td>\n<td>Variable</td>\n<td>Variable</td>\n<td>Highest</td>\n<td>Complete</td>\n<td>Final validation</td>\n</tr>\n</tbody></table>\n<h4 id=\"safety-and-alignment-benchmarks\">Safety and Alignment Benchmarks</h4>\n<p><strong>Safety evaluation suites</strong> assess model behavior across multiple risk dimensions including harmful content generation, bias amplification, and prompt injection vulnerabilities. The safety evaluation component integrates established benchmarks like TruthfulQA for truthfulness assessment and custom evaluation sets for domain-specific safety concerns.</p>\n<p><strong>Alignment measurement</strong> evaluates how well the fine-tuned model follows intended behaviors and avoids unintended behaviors. This includes measuring instruction following accuracy, refusal behavior for inappropriate requests, and consistency of responses across semantically similar prompts.</p>\n<p>The safety evaluation system implements <strong>adversarial testing protocols</strong> that attempt to elicit problematic behaviors through sophisticated prompting techniques. These tests help identify failure modes that might not appear in standard evaluation but could emerge in real-world deployment scenarios.</p>\n<h4 id=\"multi-dimensional-performance-analysis\">Multi-Dimensional Performance Analysis</h4>\n<p><strong>Capability decomposition</strong> breaks down overall model performance into specific skill areas such as reasoning, knowledge recall, language understanding, and generation quality. This decomposition helps practitioners understand which aspects of model performance improved through fine-tuning and which may have degraded.</p>\n<p><strong>Performance stability analysis</strong> measures how consistently the model performs across different prompting strategies, input formats, and context lengths. Models that show high variance in performance may require additional training or different adapter configurations.</p>\n<p>The multi-dimensional analysis includes <strong>forgetting assessment</strong> that measures how fine-tuning affected performance on capabilities not directly targeted by the training data. This assessment helps identify catastrophic forgetting and guides decisions about continual learning strategies.</p>\n<h4 id=\"model-based-evaluation-systems\">Model-Based Evaluation Systems</h4>\n<p><strong>LLM-as-judge evaluation</strong> uses larger, more capable models to assess the quality of fine-tuned model outputs. This approach scales better than human evaluation while providing more nuanced assessment than simple automated metrics. The system implements multiple judge models to reduce bias from any single evaluator.</p>\n<p><strong>Constitutional AI evaluation</strong> applies structured principles to assess whether model outputs align with specified behavioral guidelines. This evaluation method can be customized for different use cases and provides interpretable scores across multiple alignment dimensions.</p>\n<p>The model-based evaluation system includes <strong>chain-of-thought evaluation</strong> where judge models provide detailed reasoning for their assessments. This reasoning helps practitioners understand specific strengths and weaknesses in model outputs and guides targeted improvements.</p>\n<h4 id=\"specialized-domain-evaluations\">Specialized Domain Evaluations</h4>\n<p>For domain-specific fine-tuning, the advanced evaluation system supports <strong>custom benchmark integration</strong> that allows practitioners to evaluate models on proprietary or specialized datasets relevant to their specific use cases. The system provides tools for creating evaluation datasets, defining custom metrics, and comparing performance against domain-specific baselines.</p>\n<p><strong>Expert evaluation protocols</strong> facilitate assessment by subject matter experts in specialized domains. The system provides interfaces for expert evaluators to assess technical accuracy, domain appropriateness, and practical utility of model outputs in their specific fields.</p>\n<blockquote>\n<p><strong>Decision: Evaluation Framework Design</strong></p>\n<ul>\n<li><strong>Context</strong>: Advanced evaluation requires balancing comprehensiveness with practicality while maintaining reproducible results</li>\n<li><strong>Options Considered</strong>: Monolithic evaluation suite, modular evaluation components, third-party integration only</li>\n<li><strong>Decision</strong>: Implement modular evaluation framework with standardized interfaces and optional third-party integrations</li>\n<li><strong>Rationale</strong>: Modular design enables users to select relevant evaluation methods while maintaining result comparability across different evaluation components</li>\n<li><strong>Consequences</strong>: Provides flexibility for different evaluation needs while ensuring consistent methodology, but requires careful interface design for cross-method compatibility</li>\n</ul>\n</blockquote>\n<h3 id=\"common-pitfalls-in-extensions\">Common Pitfalls in Extensions</h3>\n<p>⚠️ <strong>Pitfall: Distributed Training Without Proper Synchronization</strong>\nImplementing distributed training without careful attention to gradient synchronization can lead to divergent training dynamics where different workers learn contradictory adaptations. This occurs when gradient updates are applied inconsistently across workers or when communication delays cause workers to update based on stale gradient information. To avoid this, implement synchronous gradient aggregation with verification that all workers receive identical gradient updates before proceeding to the next training step.</p>\n<p>⚠️ <strong>Pitfall: Memory Optimization Assumptions in Multi-GPU Setup</strong>\nAssuming that memory optimizations scale linearly across multiple GPUs often leads to unexpected out-of-memory errors. Memory usage patterns differ between single-GPU and multi-GPU training due to additional communication buffers, gradient synchronization overhead, and NCCL library memory requirements. Always benchmark actual memory usage in the target multi-GPU configuration rather than extrapolating from single-GPU measurements.</p>\n<p>⚠️ <strong>Pitfall: Alternative Adapter Method Hyperparameter Transfer</strong>\nDirectly transferring hyperparameters optimized for LoRA to alternative adapter methods like AdaLoRA or DoRA typically yields suboptimal results because different methods have different parameter sensitivity profiles. Each adapter method requires its own hyperparameter optimization process. Budget additional time for hyperparameter tuning when experimenting with alternative adapter techniques.</p>\n<p>⚠️ <strong>Pitfall: Evaluation Method Selection Bias</strong>\nChoosing evaluation methods that favor the specific type of fine-tuning performed can create misleading performance assessments. For example, evaluating instruction-tuned models only on instruction-following tasks while ignoring general language modeling capabilities can mask degradation in other areas. Implement comprehensive evaluation suites that assess multiple capability dimensions, including areas not directly targeted by fine-tuning.</p>\n<p>⚠️ <strong>Pitfall: Human Evaluation Without Proper Calibration</strong>\nConducting human evaluations without proper rater calibration and inter-rater reliability checks can produce inconsistent and unreliable results. Human evaluators often have different standards and may drift in their evaluation criteria over time. Implement regular calibration exercises, measure inter-rater agreement, and provide clear evaluation guidelines with concrete examples to maintain evaluation quality.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The future extensions build upon the existing pipeline architecture through well-defined extension points and plugin interfaces. This implementation approach ensures backward compatibility while enabling selective adoption of advanced capabilities based on specific requirements.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Extension Category</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Distributed Training</td>\n<td>PyTorch DistributedDataParallel + NCCL</td>\n<td>DeepSpeed ZeRO + Gradient Compression</td>\n</tr>\n<tr>\n<td>Alternative Adapters</td>\n<td>Custom LoRA variants</td>\n<td>PEFT library extension framework</td>\n</tr>\n<tr>\n<td>Advanced Evaluation</td>\n<td>HuggingFace Evaluate + custom metrics</td>\n<td>Comprehensive evaluation frameworks (HELM, Eleuther)</td>\n</tr>\n<tr>\n<td>Human Evaluation</td>\n<td>Simple rating interface + CSV export</td>\n<td>Integration with Scale AI or similar platforms</td>\n</tr>\n<tr>\n<td>Safety Benchmarks</td>\n<td>Basic bias detection + manual review</td>\n<td>Comprehensive safety suites (HELM Safety, TruthfulQA)</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-extension-architecture\">Recommended Extension Architecture</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>extensions/\n├── distributed/\n│   ├── __init__.py\n│   ├── multi_gpu_trainer.py      # DistributedDataParallel integration\n│   ├── gradient_sync.py          # Synchronization protocols\n│   └── memory_coordinator.py     # Cross-GPU memory management\n├── adapters/\n│   ├── __init__.py\n│   ├── ada_lora.py               # AdaLoRA implementation\n│   ├── dora.py                   # DoRA implementation\n│   ├── lora_plus.py              # LoRA+ implementation\n│   └── adapter_factory.py        # Plugin registration system\n├── evaluation/\n│   ├── __init__.py\n│   ├── human_eval.py             # Human evaluation coordination\n│   ├── safety_benchmarks.py     # Safety evaluation suite\n│   ├── model_judge.py            # LLM-as-judge evaluation\n│   └── domain_specific.py        # Custom domain evaluations\n└── interfaces/\n    ├── __init__.py\n    ├── adapter_interface.py      # Common adapter API\n    ├── evaluator_interface.py    # Common evaluation API\n    └── trainer_interface.py      # Extended trainer API</code></pre></div>\n\n<h4 id=\"distributed-training-infrastructure\">Distributed Training Infrastructure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Multi-GPU training extension that maintains compatibility with existing pipeline.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Supports both data parallelism and pipeline parallelism strategies.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch.distributed </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> dist</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> torch.nn.parallel </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> DistributedDataParallel</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DistributedTrainingConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for distributed training setup.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 world_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 local_rank: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 backend: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"nccl\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 gradient_compression: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 sync_frequency: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.world_size </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> world_size</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.local_rank </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> local_rank</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.backend </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> backend</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.gradient_compression </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> gradient_compression</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.sync_frequency </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sync_frequency</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MultiGPUTrainer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Extended trainer supporting distributed training across multiple GPUs.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, base_trainer, distributed_config: DistributedTrainingConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.base_trainer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> base_trainer</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.distributed_config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> distributed_config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.process_group </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> initialize_distributed</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize distributed training environment.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize distributed process group using torch.distributed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Set device based on local_rank</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Wrap model with DistributedDataParallel</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Configure gradient synchronization settings</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Set up memory monitoring for distributed context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> synchronize_gradients</span><span style=\"color:#E1E4E8\">(self, model: torch.nn.Module) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Synchronize gradients across all workers with optional compression.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Collect gradients from all model parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Apply gradient compression if enabled</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Perform all-reduce operation across workers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Apply synchronized gradients back to model</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return synchronization statistics (timing, compression ratio)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> distributed_evaluation</span><span style=\"color:#E1E4E8\">(self, eval_dataset) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Run evaluation across multiple GPUs and aggregate results.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Distribute evaluation dataset across workers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Run evaluation on local subset</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Gather evaluation results from all workers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Aggregate metrics and compute final scores</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return comprehensive evaluation results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"alternative-adapter-plugin-system\">Alternative Adapter Plugin System</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Plugin architecture for alternative adapter methods.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Enables easy integration of new parameter-efficient techniques.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> torch </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> nn</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AdapterInterface</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Common interface for all adapter methods.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> inject_adapters</span><span style=\"color:#E1E4E8\">(self, model: nn.Module, config: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> nn.Module:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Inject adapters into the target model.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_trainable_parameters</span><span style=\"color:#E1E4E8\">(self) -> List[nn.Parameter]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return list of trainable adapter parameters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> merge_adapters</span><span style=\"color:#E1E4E8\">(self, model: nn.Module) -> nn.Module:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Merge adapter weights into base model.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> save_adapters</span><span style=\"color:#E1E4E8\">(self, path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Save adapter weights to disk.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AdaLoRAAdapter</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">AdapterInterface</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"AdaLoRA implementation with dynamic rank allocation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, initial_rank: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, target_rank: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, rank_allocation_steps: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.initial_rank </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> initial_rank</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.target_rank </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> target_rank</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.rank_allocation_steps </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> rank_allocation_steps</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_step </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.adapter_modules </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> inject_adapters</span><span style=\"color:#E1E4E8\">(self, model: nn.Module, config: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> nn.Module:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Inject AdaLoRA adapters with dynamic rank capability.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Identify target modules for adaptation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create high-rank initial adapters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize importance tracking for each adapter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Set up pruning schedule for rank reduction</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Register forward hooks for importance computation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> update_ranks</span><span style=\"color:#E1E4E8\">(self, importance_scores: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, torch.Tensor]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Update adapter ranks based on importance scores.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Compute importance scores for each adapter module</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Determine new rank allocation based on importance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Prune less important singular value directions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Redistribute rank budget across modules</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return updated rank allocation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DoRAAdapter</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">AdapterInterface</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"DoRA implementation with magnitude-direction decomposition.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, rank: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, magnitude_lr_scale: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.rank </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> rank</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.magnitude_lr_scale </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> magnitude_lr_scale</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.magnitude_params </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.direction_params </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> inject_adapters</span><span style=\"color:#E1E4E8\">(self, model: nn.Module, config: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> nn.Module:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Inject DoRA adapters with magnitude-direction separation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Identify target modules for adaptation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create magnitude scaling parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create low-rank direction adaptation matrices</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Set up different learning rates for magnitude vs direction</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement forward pass with decomposed updates</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AdapterFactory</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Factory for creating and registering adapter implementations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _adapters: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">type</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"lora\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#6A737D\"># Existing LoRA implementation</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"adalora\"</span><span style=\"color:#E1E4E8\">: AdaLoRAAdapter,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"dora\"</span><span style=\"color:#E1E4E8\">: DoRAAdapter,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_adapter</span><span style=\"color:#E1E4E8\">(cls, adapter_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs) -> AdapterInterface:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create adapter instance of specified type.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate adapter type is registered</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create adapter instance with provided configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return configured adapter interface</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register_adapter</span><span style=\"color:#E1E4E8\">(cls, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, adapter_class: </span><span style=\"color:#79B8FF\">type</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register new adapter implementation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate adapter class implements AdapterInterface</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add adapter to registry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Log successful registration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"advanced-evaluation-framework\">Advanced Evaluation Framework</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Comprehensive evaluation framework supporting multiple evaluation methods.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Integrates automated metrics, human evaluation, and safety assessments.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Any, Optional, Callable</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> transformers </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> PreTrainedModel, PreTrainedTokenizer</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> EvaluationInterface</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Common interface for all evaluation methods.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> evaluate</span><span style=\"color:#E1E4E8\">(self, model: PreTrainedModel, dataset: List[Dict]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Run evaluation and return comprehensive metrics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_evaluation_name</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return human-readable name for this evaluation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SafetyEvaluator</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">EvaluationInterface</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Comprehensive safety evaluation including bias, toxicity, and truthfulness.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, safety_datasets: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.safety_datasets </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> safety_datasets</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.bias_detector </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.toxicity_classifier </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> evaluate</span><span style=\"color:#E1E4E8\">(self, model: PreTrainedModel, dataset: List[Dict]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Run comprehensive safety evaluation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test for various forms of bias (gender, racial, religious)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Evaluate toxicity and harmfulness of generated content</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Assess truthfulness and factual accuracy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test prompt injection and jailbreak resistance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Measure consistency of safety behaviors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return detailed safety metrics with specific failure examples</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> evaluate_bias</span><span style=\"color:#E1E4E8\">(self, model: PreTrainedModel, prompts: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Evaluate model bias across multiple protected characteristics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate responses for bias evaluation prompts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Analyze responses for biased language and assumptions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Compute bias scores across different demographic groups</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return bias metrics with confidence intervals</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> HumanEvaluator</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">EvaluationInterface</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Human evaluation coordinator with inter-rater reliability tracking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, evaluation_platform: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, num_raters: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.evaluation_platform </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> evaluation_platform</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.num_raters </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> num_raters</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.calibration_results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> evaluate</span><span style=\"color:#E1E4E8\">(self, model: PreTrainedModel, dataset: List[Dict]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Coordinate human evaluation with quality controls.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate model responses for evaluation dataset</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create evaluation tasks for human raters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Distribute tasks to calibrated raters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Collect and validate rater responses</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Compute inter-rater agreement scores</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return human evaluation metrics with reliability measures</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> calibrate_raters</span><span style=\"color:#E1E4E8\">(self, calibration_set: List[Dict]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calibrate human raters on known examples.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Present calibration examples to raters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Compare rater responses to ground truth</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Provide feedback and additional training if needed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Measure and track calibration accuracy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return calibration scores for each rater</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ModelJudgeEvaluator</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">EvaluationInterface</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"LLM-as-judge evaluation with multiple judge models for reliability.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, judge_models: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], evaluation_criteria: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.judge_models </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> judge_models</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.evaluation_criteria </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> evaluation_criteria</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.loaded_judges </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> evaluate</span><span style=\"color:#E1E4E8\">(self, model: PreTrainedModel, dataset: List[Dict]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Evaluate using multiple LLM judges for comprehensive assessment.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Load and prepare judge models</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate responses from target model</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create evaluation prompts for each criterion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Collect judgments from all judge models</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Aggregate judgments and compute agreement scores</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return judge-based metrics with confidence measures</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> single_judge_evaluation</span><span style=\"color:#E1E4E8\">(self, judge_model: PreTrainedModel, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                              response: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, criteria: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get detailed evaluation from single judge model.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Format evaluation prompt with response and criteria</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate judge response with reasoning</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Parse numerical scores and qualitative feedback</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return structured evaluation results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ComprehensiveEvaluationSuite</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Orchestrates multiple evaluation methods for complete assessment.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.evaluators: List[EvaluationInterface] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.evaluation_results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> add_evaluator</span><span style=\"color:#E1E4E8\">(self, evaluator: EvaluationInterface) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Add evaluation method to the suite.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate evaluator implements required interface</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add to evaluator list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Log successful addition</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> run_comprehensive_evaluation</span><span style=\"color:#E1E4E8\">(self, model: PreTrainedModel, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                   eval_datasets: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, List[Dict]]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Run all registered evaluations and aggregate results.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Run each evaluator on appropriate datasets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Collect and organize all evaluation results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Compute meta-metrics across evaluation methods</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate comprehensive evaluation report</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return complete evaluation results with summary</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints-for-extensions\">Milestone Checkpoints for Extensions</h4>\n<p><strong>Distributed Training Milestone:</strong></p>\n<ul>\n<li>Command: <code>python -m torch.distributed.launch --nprocs_per_node=2 train_distributed.py --config distributed_config.yaml</code></li>\n<li>Expected: Training progress logged from multiple processes with synchronized loss curves</li>\n<li>Verification: Check that model checkpoints are identical across all workers</li>\n<li>Memory: Monitor total GPU memory usage scales appropriately with worker count</li>\n</ul>\n<p><strong>Alternative Adapter Milestone:</strong></p>\n<ul>\n<li>Command: <code>python test_adapters.py --adapter_type adalora --dataset test_set.jsonl</code></li>\n<li>Expected: AdaLoRA adapters successfully inject and train with dynamic rank adjustment</li>\n<li>Verification: Confirm adapter rank decreases over training steps based on importance scores</li>\n<li>Performance: Validate AdaLoRA achieves competitive or better performance than standard LoRA</li>\n</ul>\n<p><strong>Advanced Evaluation Milestone:</strong></p>\n<ul>\n<li>Command: <code>python comprehensive_eval.py --model fine_tuned_model --eval_suite complete</code></li>\n<li>Expected: Multiple evaluation methods run successfully and generate detailed reports</li>\n<li>Verification: Safety evaluation identifies potential issues, human evaluation shows improved ratings</li>\n<li>Output: Comprehensive evaluation report comparing fine-tuned model against baseline across all metrics</li>\n</ul>\n<h2 id=\"glossary\">Glossary</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - this comprehensive glossary provides essential definitions and explanations for technical terms, acronyms, and domain-specific concepts used throughout the entire fine-tuning pipeline</p>\n</blockquote>\n<p>This glossary serves as the definitive reference for understanding the specialized vocabulary, technical concepts, and methodological approaches that form the foundation of the LLM fine-tuning pipeline. Each entry provides not only a definition but also contextual information about how the concept applies within our system architecture.</p>\n<h3 id=\"core-fine-tuning-concepts\">Core Fine-tuning Concepts</h3>\n<p><strong>Parameter-efficient fine-tuning</strong> refers to methods that adapt large language models using far fewer trainable parameters than traditional full fine-tuning approaches. Instead of updating all billions of parameters in a model, these techniques introduce small adapter components that learn task-specific modifications while keeping the base model frozen. This dramatically reduces memory requirements, training time, and computational costs while often achieving comparable performance to full fine-tuning.</p>\n<p><strong>Low-rank adaptation</strong> is the mathematical foundation underlying LoRA techniques, where weight updates are decomposed into the product of two smaller matrices. Rather than learning a full-rank weight matrix W of dimensions d×k, the system learns matrices A (d×r) and B (r×k) where r &lt;&lt; min(d,k). The rank parameter r controls the capacity and expressiveness of the adaptation - higher ranks can capture more complex patterns but require more memory and parameters.</p>\n<p><strong>Catastrophic forgetting</strong> describes the phenomenon where neural networks lose previously learned knowledge when trained on new tasks. In the context of LLM fine-tuning, this manifests as degraded performance on general language tasks after specializing on domain-specific data. Parameter-efficient methods like LoRA help mitigate this by preserving the original model weights while learning additive modifications.</p>\n<p><strong>Instruction tuning</strong> represents a fine-tuning paradigm that teaches language models to follow human instructions and generate appropriate responses. Unlike traditional language modeling that predicts the next token in arbitrary text, instruction tuning uses structured datasets containing instruction-response pairs, system prompts, and conversational contexts to develop models that can understand and execute diverse user requests.</p>\n<h3 id=\"memory-and-quantization\">Memory and Quantization</h3>\n<p><strong>Memory wall problem</strong> describes the fundamental constraint where GPU memory requirements for training large language models exceed available hardware capacity. Modern LLMs with billions of parameters require hundreds of gigabytes of memory for full fine-tuning when accounting for model weights, optimizer states, gradients, and activation tensors. This creates an insurmountable barrier for most practitioners using consumer or mid-range professional hardware.</p>\n<p><strong>Quantization</strong> reduces the numerical precision of model weights and activations to decrease memory consumption. Instead of storing parameters as 32-bit floating-point numbers, quantization represents them using fewer bits (typically 8, 4, or even 2 bits) with minimal impact on model performance. The challenge lies in maintaining numerical stability and avoiding significant quality degradation during the quantization process.</p>\n<p><strong>NormalFloat quantization</strong> (NF4) is a specialized 4-bit quantization format optimized for neural network weight distributions. Unlike uniform quantization that divides the value range into equal intervals, NF4 uses quantization levels that follow a normal distribution, better matching the actual distribution of neural network parameters. This alignment results in lower quantization error and better preserved model quality.</p>\n<p><strong>Double quantization</strong> applies quantization recursively to the quantization constants themselves, achieving additional memory savings. The first level quantizes model weights from float32 to 4-bit representations with scaling factors. The second level then quantizes these scaling factors, typically reducing them from float32 to 8-bit integers, providing incremental but meaningful memory reduction.</p>\n<p><strong>Mixed-precision training</strong> balances memory efficiency with numerical stability by using different precision formats for storage versus computation. Weights are stored in low precision (4-bit) for memory efficiency, but during forward and backward passes, computations are performed in higher precision (float16 or bfloat16) to maintain numerical accuracy and gradient stability.</p>\n<h3 id=\"lora-architecture-and-configuration\">LoRA Architecture and Configuration</h3>\n<p><strong>Target modules</strong> specify which layers in the neural network receive LoRA adapter injections. Common targets include attention projection matrices (query, key, value, and output projections) and feed-forward network layers. The choice of target modules significantly impacts both the effectiveness of adaptation and the memory overhead, as each targeted layer requires its own set of low-rank matrices.</p>\n<p><img src=\"/api/project/llm-finetuning-pipeline/architecture-doc/asset?path=diagrams%2Flora-architecture.svg\" alt=\"LoRA Adapter Architecture\"></p>\n<p><strong>Rank</strong> represents the dimensionality of the low-rank decomposition and serves as the primary hyperparameter controlling adapter capacity. Higher ranks (64-128) can capture more complex adaptations but require proportionally more parameters and memory. Lower ranks (8-32) are more memory-efficient but may underfitfor complex domain adaptations. The optimal rank depends on task complexity, dataset size, and available computational resources.</p>\n<p><strong>Alpha parameter</strong> functions as a scaling factor that controls the magnitude of adapter contributions to the final model outputs. The effective learning rate for LoRA adapters equals the base learning rate multiplied by alpha divided by rank. This scaling mechanism allows fine-grained control over how aggressively the adapters modify the base model behavior during training.</p>\n<p><strong>Adapter injection</strong> describes the process of inserting LoRA matrices into the computational graph of the frozen base model. During forward passes, the original linear transformation Wx is augmented with the low-rank term α(BAx)/r, where B and A are the learned adapter matrices. This injection preserves the base model&#39;s weights while enabling task-specific modifications through the adapter pathway.</p>\n<p><strong>Effective rank</strong> measures the actual dimensionality utilized by learned LoRA adapters, which may be lower than the configured rank parameter. Singular value decomposition of the learned BA product reveals how many dimensions contribute significantly to the adaptation. Low effective rank suggests the chosen rank is oversized, while effective rank approaching the configured rank indicates full utilization.</p>\n<p><strong>Trainable parameter ratio</strong> quantifies the efficiency of parameter-efficient fine-tuning by comparing the number of trainable adapter parameters to the total model parameter count. Typical LoRA configurations achieve ratios below 1%, meaning less than one percent of the model&#39;s parameters receive gradient updates during training.</p>\n<h3 id=\"training-dynamics-and-optimization\">Training Dynamics and Optimization</h3>\n<p><strong>Gradient accumulation</strong> simulates larger effective batch sizes by accumulating gradients across multiple micro-batches before performing optimizer updates. This technique enables training with batch sizes that exceed GPU memory capacity, as only one micro-batch&#39;s activations need to reside in memory at a time. The effective batch size equals micro-batch size multiplied by accumulation steps.</p>\n<p><strong>Learning rate scheduling</strong> systematically adjusts the learning rate throughout training to optimize convergence and final performance. Common schedules include linear warmup followed by cosine decay, where the learning rate gradually increases from zero to a target value, then smoothly decreases following a cosine curve. Proper scheduling prevents training instability while maximizing final model quality.</p>\n<p><strong>Warmup scheduling</strong> gradually increases the learning rate from zero to the target value over the initial training steps. This prevents optimization instability that can occur when large learning rates are applied to randomly initialized parameters or when fine-tuning pre-trained models. Warmup duration typically ranges from 3-10% of total training steps.</p>\n<p><strong>Cosine decay</strong> reduces the learning rate following a cosine curve from the peak value to near zero over the training duration. This schedule provides aggressive learning rate reduction in later training phases, encouraging convergence to sharp minima that often generalize better than the wider minima found with constant learning rates.</p>\n<p><strong>Early stopping</strong> monitors validation performance throughout training and terminates the process when improvement ceases. The mechanism tracks the best validation loss and stops training when the loss fails to improve for a configurable number of evaluation epochs. This prevents overfitting and reduces computational waste on unproductive training.</p>\n<p><strong>Checkpoint management</strong> systematically saves training state at regular intervals to enable recovery from failures and selection of the best-performing model. Checkpoints include model weights, optimizer states, learning rate scheduler state, and training metadata. The system typically maintains multiple checkpoints while automatically removing older saves to manage disk space.</p>\n<h3 id=\"data-handling-and-tokenization\">Data Handling and Tokenization</h3>\n<p><strong>Chat template</strong> defines the model-specific format for structuring conversational data with special tokens that delineate different parts of the conversation. Each model family (Llama, ChatML, Alpaca) uses distinct templates with specific tokens for system prompts, user instructions, assistant responses, and conversation boundaries. Proper template application is crucial for effective instruction tuning.</p>\n<p><strong>Subword tokenization</strong> encodes text by splitting it into smaller meaningful units that balance vocabulary efficiency with semantic preservation. Modern tokenizers like SentencePiece and Byte-Pair Encoding create vocabularies of 30,000-50,000 subword units that can represent any text while avoiding the sparsity issues of word-level tokenization and the excessive sequence length of character-level approaches.</p>\n<p><strong>Causal language modeling</strong> predicts the next token in a sequence using only preceding context, enforcing left-to-right information flow through attention masks. During training, the model learns to predict each position in the target sequence given all previous positions, enabling autoregressive generation during inference.</p>\n<p><strong>Data leakage</strong> occurs when validation or test data contains information that also appears in training data, leading to overoptimistic performance estimates. In instruction tuning contexts, leakage can be subtle, such as paraphrased instructions or responses that convey identical information despite different wording. Proper data partitioning and deduplication are essential to prevent leakage.</p>\n<h3 id=\"evaluation-and-quality-assessment\">Evaluation and Quality Assessment</h3>\n<p><strong>Perplexity</strong> measures model uncertainty when predicting validation tokens, calculated as the exponential of the average cross-entropy loss. Lower perplexity indicates better language modeling capability, as the model assigns higher probability to the actual next tokens in the validation data. Perplexity serves as a fundamental metric for assessing language model quality.</p>\n<p><strong>Task-specific evaluation</strong> assesses functional capabilities like instruction following, reasoning, or domain expertise rather than just statistical language modeling performance. These evaluations use benchmarks relevant to the fine-tuning objectives, such as question-answering accuracy, instruction compliance rates, or domain-specific knowledge assessments.</p>\n<p><strong>Baseline comparison</strong> evaluates fine-tuned model performance against the original base model to quantify the improvements achieved through adaptation. Side-by-side comparisons on identical prompts reveal whether fine-tuning enhanced the desired capabilities without causing degradation in general language abilities.</p>\n<p><strong>Adapter merging</strong> combines learned LoRA weights with base model parameters to create a single standalone model. The merging process computes W&#39; = W + α(BA)/r for each adapted layer, where W represents original weights and BA represents the learned adaptation. Merged models eliminate the need for separate adapter management during inference.</p>\n<p><strong>Export format</strong> refers to deployment-ready model serializations optimized for specific inference environments. Popular formats include HuggingFace&#39;s safetensors for general-purpose deployment and GGUF for quantized inference in environments like llama.cpp. Each format involves different trade-offs between file size, loading speed, and inference performance.</p>\n<h3 id=\"system-architecture-and-pipeline-management\">System Architecture and Pipeline Management</h3>\n<p><strong>Pipeline orchestration</strong> coordinates the sequential execution of data preparation, model loading, adapter configuration, training, and evaluation components. The orchestrator manages dependencies between stages, handles error propagation, and maintains consistent state throughout the multi-stage process.</p>\n<p><strong>State coordination</strong> manages shared state and configuration across pipeline components, ensuring consistent behavior and proper initialization ordering. The state management system tracks pipeline progress, stores intermediate results, and enables components to access shared configuration and metrics.</p>\n<p><strong>Memory pressure</strong> describes situations where GPU memory usage approaches hardware limits, requiring careful resource management to prevent out-of-memory failures. The system monitors memory consumption throughout training and implements strategies like gradient checkpointing and dynamic batch sizing to operate within available memory constraints.</p>\n<p><strong>Component isolation</strong> maintains independence between pipeline components while enabling necessary coordination and communication. Each component has well-defined interfaces, manages its own internal state, and communicates through structured APIs rather than direct state sharing.</p>\n<h3 id=\"error-handling-and-system-reliability\">Error Handling and System Reliability</h3>\n<p><strong>Gradient explosion</strong> occurs when gradient magnitudes become extremely large during backpropagation, causing training instability and potential numerical overflow. Detection involves monitoring gradient norms across training steps, while mitigation uses gradient clipping to limit maximum gradient magnitudes to stable ranges.</p>\n<p><strong>Training instability</strong> encompasses various pathological behaviors during fine-tuning, including loss spikes, gradient explosions, and NaN propagation. Instability often results from inappropriate learning rates, numerical precision issues, or corrupted training data. Recovery strategies include checkpoint restoration, learning rate reduction, and gradient scaling adjustments.</p>\n<p><strong>NaN propagation</strong> describes the spread of Not-a-Number values through neural network computations, typically originating from numerical overflow, underflow, or invalid operations like division by zero. NaN values contaminate subsequent computations and require immediate detection and mitigation to prevent training failure.</p>\n<p><strong>Checkpoint corruption</strong> involves data integrity issues in saved training states, potentially caused by hardware failures, interrupted write operations, or filesystem errors. The system implements integrity verification through checksums and maintains multiple checkpoint versions to ensure recovery capability.</p>\n<h3 id=\"advanced-techniques-and-extensions\">Advanced Techniques and Extensions</h3>\n<p><strong>Distributed training</strong> scales fine-tuning across multiple GPUs or nodes to accelerate training and handle larger models. Common parallelization strategies include data parallelism (replicating the model across devices), model parallelism (distributing layers across devices), and pipeline parallelism (sequential processing across devices).</p>\n<p><strong>Gradient synchronization</strong> coordinates gradient updates across distributed workers, ensuring consistent model updates despite parallel computation. Synchronization strategies range from synchronous all-reduce operations to asynchronous parameter servers, each with different trade-offs between consistency and training speed.</p>\n<p><strong>Adaptive rank allocation</strong> dynamically adjusts LoRA ranks based on the measured importance of different adaptation components. This technique can reduce memory usage by allocating higher ranks to more critical adaptations while using lower ranks for less important modifications.</p>\n<p><strong>Human evaluation calibration</strong> trains human raters on standardized examples to ensure consistent and reliable assessment of model outputs. Calibration processes measure inter-rater reliability and establish scoring rubrics that minimize subjective variation in human judgments.</p>\n<p><strong>Safety benchmarking</strong> systematically tests fine-tuned models for harmful or biased behaviors across various demographic groups and sensitive topics. Safety evaluation helps identify potential misuse risks and ensures responsible deployment of fine-tuned models in production environments.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The terminology and concepts defined in this glossary form the foundation for implementing an effective LLM fine-tuning pipeline. Understanding these definitions enables clear communication about system requirements, design decisions, and technical trade-offs throughout the development process.</p>\n<h4 id=\"key-concept-categories\">Key Concept Categories</h4>\n<table>\n<thead>\n<tr>\n<th>Category</th>\n<th>Core Concepts</th>\n<th>Implementation Priority</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Memory Optimization</td>\n<td>Quantization, Mixed-precision, Parameter-efficient fine-tuning</td>\n<td>Critical - enables training on limited hardware</td>\n</tr>\n<tr>\n<td>Adaptation Techniques</td>\n<td>LoRA, Rank, Alpha parameter, Target modules</td>\n<td>High - core functionality of the system</td>\n</tr>\n<tr>\n<td>Training Dynamics</td>\n<td>Gradient accumulation, Learning rate scheduling, Early stopping</td>\n<td>High - ensures stable and effective training</td>\n</tr>\n<tr>\n<td>Data Handling</td>\n<td>Chat templates, Tokenization, Data leakage prevention</td>\n<td>Medium - essential for data quality</td>\n</tr>\n<tr>\n<td>Evaluation</td>\n<td>Perplexity, Task-specific metrics, Baseline comparison</td>\n<td>Medium - validates training effectiveness</td>\n</tr>\n<tr>\n<td>System Reliability</td>\n<td>Error handling, Checkpoint management, State coordination</td>\n<td>Low - important for production deployment</td>\n</tr>\n</tbody></table>\n<h4 id=\"terminology-usage-guidelines\">Terminology Usage Guidelines</h4>\n<p>When implementing the fine-tuning pipeline, maintain consistency in terminology usage across code comments, documentation, and user interfaces. Use the precise terms defined in this glossary rather than informal synonyms or abbreviations. This consistency improves code maintainability and reduces confusion among team members.</p>\n<p>For error messages and user-facing output, prefer descriptive terminology that helps users understand the system state and required actions. For example, report &quot;gradient explosion detected&quot; rather than generic &quot;training instability,&quot; and specify &quot;adapter merging in progress&quot; rather than ambiguous &quot;model processing.&quot;</p>\n<h4 id=\"technical-communication-standards\">Technical Communication Standards</h4>\n<p>Documentation and code comments should reference these glossary terms to establish precise meaning and avoid ambiguity. When introducing new concepts not covered in this glossary, provide clear definitions and consider updating the glossary to maintain completeness.</p>\n<p>Team communications about system architecture, performance optimization, and debugging should leverage this shared vocabulary to ensure accurate understanding of technical issues and proposed solutions.</p>\n","toc":[{"level":1,"text":"LLM Fine-tuning Pipeline: Design Document","id":"llm-fine-tuning-pipeline-design-document"},{"level":2,"text":"Overview","id":"overview"},{"level":2,"text":"Context and Problem Statement","id":"context-and-problem-statement"},{"level":3,"text":"The Workshop Apprentice Mental Model","id":"the-workshop-apprentice-mental-model"},{"level":3,"text":"The Memory Wall Problem","id":"the-memory-wall-problem"},{"level":3,"text":"Existing Fine-tuning Approaches","id":"existing-fine-tuning-approaches"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Goals and Non-Goals","id":"goals-and-non-goals"},{"level":3,"text":"The Resource Optimization Mental Model: The Efficiency Consultant","id":"the-resource-optimization-mental-model-the-efficiency-consultant"},{"level":3,"text":"Functional Goals: Core Capabilities the System Must Provide","id":"functional-goals-core-capabilities-the-system-must-provide"},{"level":3,"text":"Performance and Resource Goals: Memory, Speed, and Hardware Requirements","id":"performance-and-resource-goals-memory-speed-and-hardware-requirements"},{"level":3,"text":"Non-Goals: What This System Explicitly Does Not Handle","id":"non-goals-what-this-system-explicitly-does-not-handle"},{"level":3,"text":"Goal Achievement Metrics and Success Criteria","id":"goal-achievement-metrics-and-success-criteria"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Configuration Schema Implementation","id":"configuration-schema-implementation"},{"level":4,"text":"Memory Monitoring Infrastructure","id":"memory-monitoring-infrastructure"},{"level":4,"text":"Pipeline Orchestration Structure","id":"pipeline-orchestration-structure"},{"level":4,"text":"Constants and Configuration Defaults","id":"constants-and-configuration-defaults"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":2,"text":"High-Level Architecture","id":"high-level-architecture"},{"level":3,"text":"Component Overview","id":"component-overview"},{"level":3,"text":"Data Flow and Dependencies","id":"data-flow-and-dependencies"},{"level":3,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Data Model","id":"data-model"},{"level":3,"text":"Training Data Structures","id":"training-data-structures"},{"level":3,"text":"Configuration Objects","id":"configuration-objects"},{"level":3,"text":"Evaluation and Logging","id":"evaluation-and-logging"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Dataset Preparation Component","id":"dataset-preparation-component"},{"level":3,"text":"Mental Model: The Language Tutor","id":"mental-model-the-language-tutor"},{"level":3,"text":"Data Ingestion and Validation","id":"data-ingestion-and-validation"},{"level":3,"text":"Chat Template Application","id":"chat-template-application"},{"level":3,"text":"Tokenization and Length Handling","id":"tokenization-and-length-handling"},{"level":3,"text":"Train-Validation Splitting","id":"train-validation-splitting"},{"level":3,"text":"Architecture Decision Records","id":"architecture-decision-records"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"LoRA Configuration Component","id":"lora-configuration-component"},{"level":3,"text":"Mental Model: The Skill Overlay","id":"mental-model-the-skill-overlay"},{"level":3,"text":"Target Module Identification","id":"target-module-identification"},{"level":3,"text":"Rank and Alpha Parameter Selection","id":"rank-and-alpha-parameter-selection"},{"level":3,"text":"Adapter Initialization and Injection","id":"adapter-initialization-and-injection"},{"level":3,"text":"Trainable Parameter Analysis","id":"trainable-parameter-analysis"},{"level":3,"text":"Architecture Decision Records","id":"architecture-decision-records"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Core Configuration Infrastructure","id":"core-configuration-infrastructure"},{"level":4,"text":"Target Module Detection","id":"target-module-detection"},{"level":4,"text":"Adapter Management and Injection","id":"adapter-management-and-injection"},{"level":4,"text":"Parameter Analysis and Monitoring","id":"parameter-analysis-and-monitoring"},{"level":4,"text":"Rank Selection and Optimization","id":"rank-selection-and-optimization"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":2,"text":"QLoRA Quantization Component","id":"qlora-quantization-component"},{"level":3,"text":"Mental Model: The Compression Expert","id":"mental-model-the-compression-expert"},{"level":3,"text":"NormalFloat 4-bit Quantization","id":"normalfloat-4-bit-quantization"},{"level":3,"text":"Double Quantization Strategy","id":"double-quantization-strategy"},{"level":3,"text":"Mixed-Precision Training Setup","id":"mixed-precision-training-setup"},{"level":3,"text":"Memory Usage Monitoring","id":"memory-usage-monitoring"},{"level":3,"text":"Architecture Decision Records","id":"architecture-decision-records"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Quantization Configuration Infrastructure","id":"quantization-configuration-infrastructure"},{"level":4,"text":"Memory Monitoring System","id":"memory-monitoring-system"},{"level":4,"text":"Quantized Model Loading","id":"quantized-model-loading"},{"level":4,"text":"Mixed-Precision Training Coordinator","id":"mixed-precision-training-coordinator"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":2,"text":"Training Loop Component","id":"training-loop-component"},{"level":3,"text":"Mental Model: The Personal Trainer","id":"mental-model-the-personal-trainer"},{"level":3,"text":"Gradient Accumulation Strategy","id":"gradient-accumulation-strategy"},{"level":3,"text":"Learning Rate Scheduling","id":"learning-rate-scheduling"},{"level":3,"text":"Checkpoint and State Management","id":"checkpoint-and-state-management"},{"level":3,"text":"Loss Tracking and Early Stopping","id":"loss-tracking-and-early-stopping"},{"level":3,"text":"Architecture Decision Records","id":"architecture-decision-records"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Training Loop Skeleton","id":"core-training-loop-skeleton"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":2,"text":"Evaluation and Merging Component","id":"evaluation-and-merging-component"},{"level":3,"text":"Mental Model: The Exam Proctor","id":"mental-model-the-exam-proctor"},{"level":3,"text":"Perplexity and Language Modeling Metrics","id":"perplexity-and-language-modeling-metrics"},{"level":3,"text":"Task-Specific Evaluation","id":"task-specific-evaluation"},{"level":3,"text":"LoRA Adapter Merging","id":"lora-adapter-merging"},{"level":3,"text":"Model Export and Deployment","id":"model-export-and-deployment"},{"level":3,"text":"Architecture Decision Records","id":"architecture-decision-records"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Core Evaluation Infrastructure","id":"core-evaluation-infrastructure"},{"level":4,"text":"LoRA Adapter Merging Infrastructure","id":"lora-adapter-merging-infrastructure"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":2,"text":"Interactions and Data Flow","id":"interactions-and-data-flow"},{"level":3,"text":"End-to-End Pipeline Flow","id":"end-to-end-pipeline-flow"},{"level":4,"text":"Phase 1: Configuration and Initialization","id":"phase-1-configuration-and-initialization"},{"level":4,"text":"Phase 2: Model and Data Preparation","id":"phase-2-model-and-data-preparation"},{"level":4,"text":"Phase 3: Training Orchestration","id":"phase-3-training-orchestration"},{"level":4,"text":"Phase 4: Evaluation and Quality Assessment","id":"phase-4-evaluation-and-quality-assessment"},{"level":4,"text":"Phase 5: Model Merging and Export","id":"phase-5-model-merging-and-export"},{"level":3,"text":"Inter-Component Communication","id":"inter-component-communication"},{"level":4,"text":"Primary Data Interfaces","id":"primary-data-interfaces"},{"level":4,"text":"Configuration Propagation","id":"configuration-propagation"},{"level":4,"text":"Progress and Error Signaling","id":"progress-and-error-signaling"},{"level":4,"text":"Method Call Patterns","id":"method-call-patterns"},{"level":3,"text":"State Coordination and Dependencies","id":"state-coordination-and-dependencies"},{"level":4,"text":"Global State Management","id":"global-state-management"},{"level":4,"text":"Component Dependency Graph","id":"component-dependency-graph"},{"level":4,"text":"State Persistence and Recovery","id":"state-persistence-and-recovery"},{"level":4,"text":"Memory Coordination","id":"memory-coordination"},{"level":4,"text":"Inter-Component Error Recovery","id":"inter-component-error-recovery"},{"level":3,"text":"Common Pitfalls in Pipeline Orchestration","id":"common-pitfalls-in-pipeline-orchestration"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Pipeline Coordinator Infrastructure","id":"pipeline-coordinator-infrastructure"},{"level":4,"text":"State Manager Implementation","id":"state-manager-implementation"},{"level":4,"text":"Core Pipeline Flow Implementation","id":"core-pipeline-flow-implementation"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Error Handling and Edge Cases","id":"error-handling-and-edge-cases"},{"level":3,"text":"Mental Model: The Safety Net System","id":"mental-model-the-safety-net-system"},{"level":3,"text":"Hardware and Memory Failures","id":"hardware-and-memory-failures"},{"level":4,"text":"Memory Pressure Detection and Management","id":"memory-pressure-detection-and-management"},{"level":4,"text":"GPU Driver and Hardware Recovery","id":"gpu-driver-and-hardware-recovery"},{"level":4,"text":"Power Management and Thermal Protection","id":"power-management-and-thermal-protection"},{"level":3,"text":"Training Instability","id":"training-instability"},{"level":4,"text":"Gradient Explosion Detection and Recovery","id":"gradient-explosion-detection-and-recovery"},{"level":4,"text":"Loss Spike Management","id":"loss-spike-management"},{"level":4,"text":"NaN and Infinity Propagation","id":"nan-and-infinity-propagation"},{"level":3,"text":"Data Quality Issues","id":"data-quality-issues"},{"level":4,"text":"Runtime Data Validation","id":"runtime-data-validation"},{"level":4,"text":"Streaming Data Handling","id":"streaming-data-handling"},{"level":4,"text":"Format Inconsistency Handling","id":"format-inconsistency-handling"},{"level":3,"text":"Checkpoint and State Recovery","id":"checkpoint-and-state-recovery"},{"level":4,"text":"Checkpoint Integrity Management","id":"checkpoint-integrity-management"},{"level":4,"text":"State Synchronization","id":"state-synchronization"},{"level":4,"text":"Distributed Training State Recovery","id":"distributed-training-state-recovery"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Language-Specific Hints","id":"language-specific-hints"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":2,"text":"Testing Strategy","id":"testing-strategy"},{"level":3,"text":"Mental Model: The Quality Assurance Laboratory","id":"mental-model-the-quality-assurance-laboratory"},{"level":2,"text":"Unit Testing Strategy","id":"unit-testing-strategy"},{"level":3,"text":"Core Component Testing Framework","id":"core-component-testing-framework"},{"level":3,"text":"Data Structure and Configuration Testing","id":"data-structure-and-configuration-testing"},{"level":3,"text":"Transformation and Processing Logic Testing","id":"transformation-and-processing-logic-testing"},{"level":3,"text":"Analysis and Calculation Testing","id":"analysis-and-calculation-testing"},{"level":3,"text":"Mock Infrastructure and Test Utilities","id":"mock-infrastructure-and-test-utilities"},{"level":2,"text":"Integration and End-to-End Testing","id":"integration-and-end-to-end-testing"},{"level":3,"text":"Component Integration Testing","id":"component-integration-testing"},{"level":3,"text":"Pipeline State Management Testing","id":"pipeline-state-management-testing"},{"level":3,"text":"Data Flow Validation Testing","id":"data-flow-validation-testing"},{"level":3,"text":"Resource Management Integration Testing","id":"resource-management-integration-testing"},{"level":2,"text":"Milestone Validation Checkpoints","id":"milestone-validation-checkpoints"},{"level":3,"text":"Milestone 1: Dataset Preparation Validation","id":"milestone-1-dataset-preparation-validation"},{"level":3,"text":"Milestone 2: LoRA Configuration Validation","id":"milestone-2-lora-configuration-validation"},{"level":3,"text":"Milestone 3: QLoRA Quantization Validation","id":"milestone-3-qlora-quantization-validation"},{"level":3,"text":"Milestone 4: Training Loop Validation","id":"milestone-4-training-loop-validation"},{"level":3,"text":"Milestone 5: Evaluation and Merging Validation","id":"milestone-5-evaluation-and-merging-validation"},{"level":2,"text":"Performance and Memory Testing","id":"performance-and-memory-testing"},{"level":3,"text":"Memory Efficiency Validation Framework","id":"memory-efficiency-validation-framework"},{"level":3,"text":"Training Performance Benchmarking","id":"training-performance-benchmarking"},{"level":3,"text":"Quantization Impact Assessment","id":"quantization-impact-assessment"},{"level":3,"text":"Hardware Compatibility and Scalability Testing","id":"hardware-compatibility-and-scalability-testing"},{"level":3,"text":"Benchmark Comparison Framework","id":"benchmark-comparison-framework"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Core Testing Infrastructure","id":"core-testing-infrastructure"},{"level":4,"text":"Unit Test Implementation Examples","id":"unit-test-implementation-examples"},{"level":4,"text":"Integration Test Implementation Examples","id":"integration-test-implementation-examples"},{"level":4,"text":"Milestone Validation Checkpoints","id":"milestone-validation-checkpoints"},{"level":4,"text":"Performance Test Implementation Examples","id":"performance-test-implementation-examples"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Debugging Guide","id":"debugging-guide"},{"level":3,"text":"Symptom-Cause-Fix Tables","id":"symptom-cause-fix-tables"},{"level":4,"text":"Data Preparation Issues","id":"data-preparation-issues"},{"level":4,"text":"LoRA Configuration Issues","id":"lora-configuration-issues"},{"level":4,"text":"Quantization Issues","id":"quantization-issues"},{"level":4,"text":"Training Loop Issues","id":"training-loop-issues"},{"level":4,"text":"Evaluation and Export Issues","id":"evaluation-and-export-issues"},{"level":3,"text":"Debugging Tools and Techniques","id":"debugging-tools-and-techniques"},{"level":4,"text":"GPU Profiling and Memory Analysis","id":"gpu-profiling-and-memory-analysis"},{"level":4,"text":"Training Visualization and Analysis","id":"training-visualization-and-analysis"},{"level":4,"text":"Model Behavior Analysis","id":"model-behavior-analysis"},{"level":4,"text":"Systematic Debugging Methodology","id":"systematic-debugging-methodology"},{"level":3,"text":"Effective Logging Strategies","id":"effective-logging-strategies"},{"level":4,"text":"Structured Logging Schema","id":"structured-logging-schema"},{"level":4,"text":"Training Progress Logging","id":"training-progress-logging"},{"level":4,"text":"Component-Specific Logging","id":"component-specific-logging"},{"level":4,"text":"Error Context Logging","id":"error-context-logging"},{"level":4,"text":"Performance and Bottleneck Logging","id":"performance-and-bottleneck-logging"},{"level":4,"text":"Logging Configuration and Management","id":"logging-configuration-and-management"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Core Debugging Infrastructure","id":"core-debugging-infrastructure"},{"level":4,"text":"Debugging Tool Skeletons","id":"debugging-tool-skeletons"},{"level":4,"text":"Milestone Validation Checkpoints","id":"milestone-validation-checkpoints"},{"level":2,"text":"Future Extensions","id":"future-extensions"},{"level":3,"text":"Mental Model: The Workshop Evolution","id":"mental-model-the-workshop-evolution"},{"level":3,"text":"Scalability Extensions","id":"scalability-extensions"},{"level":4,"text":"Distributed Training Architecture","id":"distributed-training-architecture"},{"level":4,"text":"Memory-Efficient Distributed Quantization","id":"memory-efficient-distributed-quantization"},{"level":4,"text":"Multi-GPU LoRA Coordination","id":"multi-gpu-lora-coordination"},{"level":3,"text":"Alternative Adapter Methods","id":"alternative-adapter-methods"},{"level":4,"text":"AdaLoRA: Adaptive Low-Rank Adaptation","id":"adalora-adaptive-low-rank-adaptation"},{"level":4,"text":"DoRA: Weight-Decomposed Low-Rank Adaptation","id":"dora-weight-decomposed-low-rank-adaptation"},{"level":4,"text":"LoRA+ and Multi-Learning-Rate Extensions","id":"lora-and-multi-learning-rate-extensions"},{"level":3,"text":"Advanced Evaluation","id":"advanced-evaluation"},{"level":4,"text":"Human Evaluation Integration","id":"human-evaluation-integration"},{"level":4,"text":"Safety and Alignment Benchmarks","id":"safety-and-alignment-benchmarks"},{"level":4,"text":"Multi-Dimensional Performance Analysis","id":"multi-dimensional-performance-analysis"},{"level":4,"text":"Model-Based Evaluation Systems","id":"model-based-evaluation-systems"},{"level":4,"text":"Specialized Domain Evaluations","id":"specialized-domain-evaluations"},{"level":3,"text":"Common Pitfalls in Extensions","id":"common-pitfalls-in-extensions"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Extension Architecture","id":"recommended-extension-architecture"},{"level":4,"text":"Distributed Training Infrastructure","id":"distributed-training-infrastructure"},{"level":4,"text":"Alternative Adapter Plugin System","id":"alternative-adapter-plugin-system"},{"level":4,"text":"Advanced Evaluation Framework","id":"advanced-evaluation-framework"},{"level":4,"text":"Milestone Checkpoints for Extensions","id":"milestone-checkpoints-for-extensions"},{"level":2,"text":"Glossary","id":"glossary"},{"level":3,"text":"Core Fine-tuning Concepts","id":"core-fine-tuning-concepts"},{"level":3,"text":"Memory and Quantization","id":"memory-and-quantization"},{"level":3,"text":"LoRA Architecture and Configuration","id":"lora-architecture-and-configuration"},{"level":3,"text":"Training Dynamics and Optimization","id":"training-dynamics-and-optimization"},{"level":3,"text":"Data Handling and Tokenization","id":"data-handling-and-tokenization"},{"level":3,"text":"Evaluation and Quality Assessment","id":"evaluation-and-quality-assessment"},{"level":3,"text":"System Architecture and Pipeline Management","id":"system-architecture-and-pipeline-management"},{"level":3,"text":"Error Handling and System Reliability","id":"error-handling-and-system-reliability"},{"level":3,"text":"Advanced Techniques and Extensions","id":"advanced-techniques-and-extensions"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Key Concept Categories","id":"key-concept-categories"},{"level":4,"text":"Terminology Usage Guidelines","id":"terminology-usage-guidelines"},{"level":4,"text":"Technical Communication Standards","id":"technical-communication-standards"}],"title":"LLM Fine-tuning Pipeline: Design Document","markdown":"# LLM Fine-tuning Pipeline: Design Document\n\n\n## Overview\n\nAn end-to-end system for efficiently fine-tuning large language models using parameter-efficient techniques like LoRA and QLoRA, with integrated dataset preparation and comprehensive evaluation. The key architectural challenge is balancing memory efficiency, training stability, and model quality while handling multi-gigabyte models on consumer hardware.\n\n\n> This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.\n\n\n## Context and Problem Statement\n\n> **Milestone(s):** All milestones - this section establishes the foundational understanding needed for the entire fine-tuning pipeline\n\nThe landscape of large language models has fundamentally shifted the paradigm of natural language processing, but this revolution comes with a critical constraint: the computational and memory requirements for fine-tuning these billion-parameter models have grown beyond the reach of most practitioners and organizations. While a pre-trained model like Llama 2 7B represents an extraordinary achievement in language understanding, adapting it to specific domains or tasks using traditional fine-tuning approaches requires resources that are prohibitively expensive for most use cases.\n\nThis section explores the core challenges that make traditional fine-tuning impractical for large language models, introduces parameter-efficient alternatives that democratize model customization, and establishes the technical foundation for understanding why LoRA and QLoRA have emerged as the dominant approaches for practical LLM fine-tuning.\n\n### The Workshop Apprentice Mental Model\n\nTo understand the essence of fine-tuning large language models, imagine a **master craftsperson** who has spent decades perfecting their trade—let's say a violin maker who has internalized centuries of woodworking knowledge, acoustic principles, and aesthetic traditions. This craftsperson represents our pre-trained language model: they possess vast, general knowledge accumulated through extensive training on diverse examples.\n\nNow suppose you want this master craftsperson to specialize in creating instruments for a specific musical tradition—perhaps traditional Irish fiddles with their distinctive sound characteristics and construction techniques. You have two approaches:\n\n**The Traditional Apprenticeship (Full Fine-tuning)**: You could ask the master craftsperson to forget everything they know and start completely over, learning their entire craft again but with exclusive focus on Irish fiddle-making techniques. This approach would require them to re-examine every single skill they've developed, from basic wood selection to advanced finishing techniques. While this might produce exceptional Irish fiddle expertise, it's extraordinarily wasteful—you're discarding decades of valuable knowledge and starting from scratch. Moreover, the time and resources required would be enormous, and there's a risk that in focusing so narrowly, they might lose the broader craftsmanship wisdom that made them a master in the first place.\n\n**The Skill Overlay Approach (Parameter-Efficient Fine-tuning)**: Alternatively, you could teach the master craftsperson a set of specialized techniques that build upon their existing knowledge. You might introduce them to specific wood types used in Irish instruments, particular carving patterns that enhance traditional sound qualities, or specialized varnishing techniques that preserve the wood's resonance characteristics. The craftsperson retains all their foundational knowledge—understanding of wood grain, basic carving principles, acoustic theory—but adds a focused layer of domain-specific expertise. This approach is faster, more efficient, and preserves the valuable general knowledge while adding specialized capabilities.\n\nIn this analogy, **LoRA (Low-Rank Adaptation)** represents the skill overlay approach. Instead of retraining every parameter in the model (equivalent to relearning every craftsmanship skill), LoRA adds small, specialized adaptation layers that modify how the existing knowledge is applied to specific tasks. The base model's parameters remain frozen—preserving all the general language understanding—while lightweight adapter modules learn task-specific transformations.\n\n**QLoRA** extends this analogy further: imagine that our master craftsperson has accumulated so many tools and materials over their career that their workshop has become prohibitively large and expensive to maintain. QLoRA is like organizing the workshop with an extremely efficient storage system that compresses the most commonly used tools and materials without losing their functionality. The craftsperson can still access everything they need, but the workshop now fits in a much smaller, more affordable space. This compression (4-bit quantization) makes it possible to work with the master craftsperson even if you don't have access to a vast workshop facility.\n\nThe key insight from this mental model is that **fine-tuning should preserve and build upon existing knowledge rather than replacing it**. Pre-trained language models have learned incredibly sophisticated patterns about language, reasoning, and world knowledge from massive datasets. Parameter-efficient methods recognize this value and ask: \"How can we add new capabilities without discarding what the model already knows?\"\n\nThis approach has profound implications for both technical implementation and practical deployment. From a technical perspective, it means we need systems that can selectively modify model behavior while keeping most parameters unchanged. From a practical perspective, it means that fine-tuning becomes accessible to organizations and individuals who cannot afford the computational resources required for full model retraining.\n\n### The Memory Wall Problem\n\nThe fundamental challenge facing modern LLM fine-tuning is what we term the **Memory Wall Problem**—the exponential growth in GPU memory requirements that has outpaced the availability and affordability of high-memory hardware. To understand this problem's severity, we need to examine the memory requirements across different model sizes and training approaches.\n\nConsider the memory footprint of training a language model. During standard fine-tuning, the GPU must simultaneously hold:\n\n1. **Model Parameters**: The actual weights of the neural network\n2. **Gradients**: Partial derivatives for each parameter during backpropagation  \n3. **Optimizer States**: Additional parameters maintained by optimizers like Adam (momentum and variance estimates)\n4. **Activation Memory**: Intermediate computations stored for gradient calculation\n5. **Batch Data**: Input tokens and attention masks for the current training batch\n\nFor a 7-billion parameter model using 32-bit floating point precision, the base model weights alone require approximately 28 GB of memory. However, this is just the beginning. During training with the Adam optimizer, each parameter requires storage for the parameter itself, its gradient, and two optimizer state variables (momentum and variance). This quadruples the memory requirement to roughly 112 GB just for the optimization process, before accounting for activation memory and batch data.\n\nThe situation becomes even more challenging with larger models. A 13-billion parameter model requires approximately 208 GB for full fine-tuning, while a 70-billion parameter model would need over 1 TB of GPU memory—far exceeding the capacity of even the most expensive consumer and professional GPUs available today.\n\n| Model Size | Parameters | Base Model (FP32) | Full Training Memory | Accessible Hardware |\n|------------|------------|-------------------|---------------------|-------------------|\n| 7B | 7 billion | 28 GB | ~112 GB | Requires 8x A100 (40GB) |\n| 13B | 13 billion | 52 GB | ~208 GB | Requires 16x A100 (40GB) |\n| 30B | 30 billion | 120 GB | ~480 GB | Requires 32+ A100 (40GB) |\n| 70B | 70 billion | 280 GB | ~1.1 TB | Impossible on current hardware |\n\nThis memory wall has created a practical barrier that excludes most researchers, developers, and organizations from fine-tuning large language models. Even organizations with substantial computational budgets find the costs prohibitive for experimental work or iterative development.\n\n**Parameter-efficient methods fundamentally reshape this equation** by dramatically reducing the trainable parameter count while preserving model capability. LoRA, for example, typically reduces trainable parameters by 99% or more. Instead of fine-tuning 7 billion parameters, LoRA might adapt only 4-8 million parameters through low-rank decomposition matrices. This reduction has cascading effects throughout the memory hierarchy:\n\n- **Reduced Optimizer Memory**: With 99% fewer trainable parameters, optimizer states shrink proportionally\n- **Simplified Gradient Storage**: Only adapter parameters require gradient computation and storage\n- **Preserved Base Model**: The frozen base model can be loaded in reduced precision without affecting adaptation quality\n\nQLoRA amplifies these benefits through 4-bit quantization of the base model. By representing the frozen parameters in 4-bit NormalFloat format instead of 32-bit floating point, the base model memory footprint shrinks by approximately 75%. A 7-billion parameter model that previously required 28 GB can be stored in roughly 7 GB, bringing it within reach of consumer-grade GPUs.\n\nThe combination of LoRA and QLoRA creates a **memory efficiency multiplier effect**:\n\n| Approach | 7B Model Memory | Reduction Factor | Consumer GPU Feasible? |\n|----------|----------------|------------------|----------------------|\n| Full Fine-tuning | ~112 GB | 1x | No |\n| LoRA (FP16 base) | ~16 GB | 7x | Barely (high-end) |\n| QLoRA (4-bit base) | ~6 GB | 18x | Yes (RTX 4090, etc.) |\n\nThis transformation is not merely quantitative but qualitative—it changes who can participate in LLM development and what kinds of experimentation become feasible. The memory wall problem had created a practical monopoly where only large corporations and well-funded research institutions could customize large language models. Parameter-efficient methods democratize this capability, enabling widespread innovation and specialization.\n\nHowever, this efficiency comes with trade-offs that must be carefully managed. Quantization introduces small quality degradations, rank limitations in LoRA can constrain adaptation capacity, and the interaction between quantization and low-rank adaptation requires careful hyperparameter tuning. Understanding these trade-offs is essential for designing effective fine-tuning pipelines.\n\n### Existing Fine-tuning Approaches\n\nThe evolution of language model fine-tuning has progressed through several distinct approaches, each addressing different aspects of the memory and computational challenges while making various trade-offs between adaptation quality, efficiency, and implementation complexity. Understanding these approaches and their trade-offs is crucial for making informed decisions about fine-tuning strategy.\n\n**Full Fine-tuning** represents the traditional approach where all model parameters are updated during training. This method treats the pre-trained model as an initialization point and allows the optimization process to modify every weight in the network. While conceptually straightforward and theoretically optimal for task adaptation, full fine-tuning suffers from the memory wall problem described above and introduces additional risks.\n\nThe primary concern beyond memory requirements is **catastrophic forgetting**—the tendency for neural networks to lose previously learned capabilities when adapting to new tasks. When all parameters are trainable, aggressive learning rates can cause the model to overwrite important general knowledge encoded in the pre-trained weights. This is particularly problematic for language models, where general linguistic competence is as valuable as task-specific performance.\n\n**Low-Rank Adaptation (LoRA)** introduces a fundamentally different paradigm based on the insight that adaptation to new tasks likely occurs in a much lower-dimensional space than the full parameter space. Instead of updating the original weight matrices directly, LoRA decomposes updates into two smaller matrices that, when multiplied together, approximate the full adaptation.\n\nFor a pre-trained weight matrix W₀ with dimensions d×k, LoRA introduces two trainable matrices: A (d×r) and B (r×k), where r is the adaptation rank and is much smaller than both d and k. During forward passes, the adapted weight becomes W₀ + αBA, where α is a scaling parameter. This decomposition reduces trainable parameters from d×k to r(d+k), achieving dramatic parameter reduction when r << min(d,k).\n\nThe mathematical foundation of LoRA rests on the hypothesis that adaptation primarily occurs in a low-dimensional subspace of the full parameter space. This hypothesis has empirical support—research has shown that the effective rank of fine-tuning updates is often much lower than the dimensionality of the weight matrices being adapted.\n\n**QLoRA (Quantized LoRA)** combines LoRA with aggressive quantization techniques to achieve maximum memory efficiency while preserving adaptation quality. The key innovation is the introduction of 4-bit NormalFloat (NF4) quantization, specifically designed for neural network weights that follow approximately normal distributions.\n\nTraditional quantization approaches use uniform quantization intervals, which are suboptimal for normally-distributed weights where most values cluster around zero. NF4 quantization adapts the quantization levels to match the expected weight distribution, providing better precision where weights are most dense while using fewer bits overall.\n\nQLoRA also introduces **double quantization**, where the quantization constants themselves are quantized using 8-bit precision. This provides additional memory savings with minimal quality impact, as the quantization constants exhibit different statistical properties than the weights themselves.\n\n**Prefix Tuning and P-Tuning v2** represent alternative approaches that modify model behavior by learning task-specific prompts or prefixes rather than adapting the model weights directly. These methods prepend learnable continuous tokens to the input sequence, allowing the frozen model to adapt its behavior based on these learned prompt representations.\n\nWhile prompt-based methods avoid modifying model weights entirely, they face limitations in adaptation capacity and can be sensitive to the specific architecture and pre-training approach of the base model. They work particularly well for tasks that can be naturally formulated as continuation or completion problems but may struggle with tasks requiring fundamental changes in model behavior.\n\n**Adapter Networks** insert small neural networks (adapters) between existing layers of the pre-trained model. These adapters typically consist of down-projection and up-projection layers with a bottleneck architecture that reduces dimensionality, applies a non-linear transformation, and then projects back to the original dimensionality.\n\nThe advantage of adapter networks is their architectural flexibility—different adapter designs can target specific aspects of model behavior. However, they introduce additional computational overhead during inference, as the adapter computations add to the forward pass time even after training completes.\n\nHere's a comprehensive comparison of these approaches:\n\n| Method | Trainable Parameters | Memory Efficiency | Inference Speed | Adaptation Quality | Implementation Complexity |\n|--------|---------------------|-------------------|----------------|-------------------|-------------------------|\n| Full Fine-tuning | 100% | Poor (28-112GB for 7B model) | Baseline | Excellent | Simple |\n| LoRA | 0.1-1% | Good (16GB for 7B model) | Baseline | Very Good | Moderate |\n| QLoRA | 0.1-1% | Excellent (6GB for 7B model) | Baseline | Good | Complex |\n| Prefix Tuning | 0.01-0.1% | Excellent | Slightly slower | Good | Moderate |\n| Adapter Networks | 0.5-2% | Good | 10-20% slower | Very Good | Moderate |\n\nThe trade-offs between these approaches create different optimal choices depending on constraints and requirements:\n\n> **Decision: LoRA + QLoRA as Primary Approach**\n> - **Context**: Need to balance memory efficiency, adaptation quality, and practical implementation constraints for a general-purpose fine-tuning pipeline\n> - **Options Considered**: \n>   1. Full fine-tuning with gradient checkpointing and model parallelism\n>   2. Pure LoRA with 16-bit base model\n>   3. QLoRA combining 4-bit quantization with LoRA adaptation\n>   4. Prefix tuning for maximum memory efficiency\n> - **Decision**: Implement QLoRA as the primary method with LoRA as a fallback option\n> - **Rationale**: QLoRA provides the optimal balance of memory efficiency (enabling consumer GPU usage), adaptation quality (preserving 95%+ of full fine-tuning performance), and inference speed (no overhead after adapter merging). The 4-bit quantization makes fine-tuning accessible while the low-rank adaptation preserves flexibility for various task types.\n> - **Consequences**: Requires implementing 4-bit quantization support and careful hyperparameter tuning, but enables fine-tuning on consumer hardware and democratizes access to LLM customization\n\n**Emerging Approaches** continue to push the boundaries of parameter-efficient fine-tuning. AdaLoRA (Adaptive LoRA) dynamically adjusts the rank allocation across different layers and training steps, concentrating adaptation capacity where it's most needed. DoRA (Weight-Decomposed Low-Rank Adaptation) separates weight updates into magnitude and direction components, providing more fine-grained control over adaptation behavior.\n\nThese emerging methods highlight the active research area of parameter-efficient fine-tuning and suggest that the field will continue evolving toward more sophisticated approaches that balance efficiency, quality, and flexibility.\n\nThe choice of fine-tuning approach ultimately depends on the specific constraints and requirements of each use case. However, the combination of memory efficiency, adaptation quality, and practical accessibility makes QLoRA the most compelling choice for a general-purpose fine-tuning pipeline. Understanding the principles underlying these approaches—low-rank decomposition, quantization, and selective parameter updating—provides the foundation for implementing robust and efficient fine-tuning systems.\n\n### Implementation Guidance\n\nThe concepts and trade-offs described above translate into specific technical decisions and implementation strategies. This guidance bridges the gap between understanding the approaches conceptually and implementing them effectively.\n\n**A. Technology Recommendations Table:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Base Framework | PyTorch + Transformers library | PyTorch + Custom CUDA kernels |\n| Quantization | bitsandbytes (4-bit NF4) | Custom quantization with TensorRT |\n| LoRA Implementation | PEFT library (Parameter Efficient Fine-Tuning) | Manual low-rank matrix implementation |\n| Training Orchestration | HuggingFace Trainer API | Custom training loop with DeepSpeed |\n| Memory Optimization | Gradient checkpointing + mixed precision | ZeRO optimizer with gradient compression |\n| Model Export | HuggingFace format + GGUF conversion | Custom serialization with optimization passes |\n\n**B. Recommended File Structure:**\n\nThe fine-tuning pipeline should be organized to separate concerns clearly while maintaining easy discoverability for debugging and extension:\n\n```\nllm-finetuning-pipeline/\n├── src/\n│   ├── data/\n│   │   ├── __init__.py\n│   │   ├── loaders.py              ← Dataset loading from multiple formats\n│   │   ├── processors.py           ← Chat template and tokenization\n│   │   └── splitters.py            ← Train/validation splitting\n│   ├── models/\n│   │   ├── __init__.py\n│   │   ├── quantization.py         ← 4-bit model loading and config\n│   │   ├── lora_config.py          ← LoRA adapter configuration\n│   │   └── adapters.py             ← Adapter injection and management\n│   ├── training/\n│   │   ├── __init__.py\n│   │   ├── trainer.py              ← Main training orchestration\n│   │   ├── optimization.py         ← Learning rate, gradient accumulation\n│   │   └── callbacks.py            ← Checkpointing, early stopping\n│   ├── evaluation/\n│   │   ├── __init__.py\n│   │   ├── metrics.py              ← Perplexity and task-specific evaluation\n│   │   ├── benchmarks.py           ← Standard benchmark implementations\n│   │   └── exporters.py            ← Model merging and format conversion\n│   └── utils/\n│       ├── __init__.py\n│       ├── memory_utils.py         ← GPU memory monitoring\n│       ├── logging_config.py       ← Structured logging setup\n│       └── config_parser.py        ← Configuration validation\n├── configs/\n│   ├── base_config.yaml            ← Default configuration template\n│   ├── model_configs/              ← Model-specific configurations\n│   └── task_configs/               ← Task-specific training configurations\n├── tests/\n│   ├── unit/                       ← Component-level tests\n│   ├── integration/                ← Cross-component tests\n│   └── fixtures/                   ← Test data and mock models\n└── examples/\n    ├── basic_finetuning.py         ← Simple end-to-end example\n    ├── advanced_config.py          ← Complex configuration example\n    └── evaluation_only.py          ← Model evaluation without training\n```\n\n**C. Infrastructure Starter Code:**\n\nThe following utilities handle common infrastructure concerns that aren't the core learning focus but are necessary for effective implementation:\n\n```python\n# src/utils/memory_utils.py\nimport torch\nimport psutil\nfrom typing import Dict, Tuple\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass MemoryMonitor:\n    \"\"\"Tracks GPU and system memory usage throughout training.\"\"\"\n    \n    def __init__(self):\n        self.baseline_gpu_memory = None\n        self.baseline_system_memory = None\n        self.measurements = []\n    \n    def capture_baseline(self) -> None:\n        \"\"\"Record initial memory state before model loading.\"\"\"\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            self.baseline_gpu_memory = torch.cuda.memory_allocated()\n        self.baseline_system_memory = psutil.virtual_memory().used\n        logger.info(f\"Baseline GPU memory: {self.baseline_gpu_memory / 1e9:.2f} GB\")\n        logger.info(f\"Baseline system memory: {self.baseline_system_memory / 1e9:.2f} GB\")\n    \n    def measure_current_usage(self, stage: str) -> Dict[str, float]:\n        \"\"\"Measure current memory usage and log the difference from baseline.\"\"\"\n        current_measurement = {\n            'stage': stage,\n            'timestamp': torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None\n        }\n        \n        if torch.cuda.is_available():\n            current_gpu = torch.cuda.memory_allocated()\n            max_gpu = torch.cuda.max_memory_allocated()\n            current_measurement.update({\n                'gpu_used_gb': current_gpu / 1e9,\n                'gpu_delta_gb': (current_gpu - self.baseline_gpu_memory) / 1e9,\n                'gpu_max_gb': max_gpu / 1e9\n            })\n        \n        current_system = psutil.virtual_memory().used\n        current_measurement.update({\n            'system_used_gb': current_system / 1e9,\n            'system_delta_gb': (current_system - self.baseline_system_memory) / 1e9,\n            'system_available_gb': psutil.virtual_memory().available / 1e9\n        })\n        \n        self.measurements.append(current_measurement)\n        logger.info(f\"Memory at {stage}: GPU {current_measurement.get('gpu_used_gb', 0):.2f} GB, \"\n                   f\"System {current_measurement['system_used_gb']:.2f} GB\")\n        return current_measurement\n\n    def get_peak_usage(self) -> Dict[str, float]:\n        \"\"\"Return peak memory usage across all measurements.\"\"\"\n        if not self.measurements:\n            return {}\n        \n        peak_stats = {}\n        if torch.cuda.is_available():\n            peak_stats['peak_gpu_gb'] = max(m.get('gpu_used_gb', 0) for m in self.measurements)\n            peak_stats['peak_gpu_delta_gb'] = max(m.get('gpu_delta_gb', 0) for m in self.measurements)\n        \n        peak_stats['peak_system_gb'] = max(m['system_used_gb'] for m in self.measurements)\n        peak_stats['peak_system_delta_gb'] = max(m['system_delta_gb'] for m in self.measurements)\n        \n        return peak_stats\n\n# src/utils/config_parser.py\nimport yaml\nfrom dataclasses import dataclass, field\nfrom typing import Optional, List, Dict, Any\nfrom pathlib import Path\n\n@dataclass\nclass QuantizationConfig:\n    \"\"\"Configuration for 4-bit model quantization.\"\"\"\n    load_in_4bit: bool = True\n    bnb_4bit_quant_type: str = \"nf4\"  # nf4 or fp4\n    bnb_4bit_compute_dtype: str = \"float16\"  # float16, bfloat16, or float32\n    bnb_4bit_use_double_quant: bool = True  # Double quantization for extra memory savings\n\n@dataclass\nclass LoRAConfig:\n    \"\"\"Configuration for LoRA adapter settings.\"\"\"\n    r: int = 16  # Rank of the low-rank decomposition\n    alpha: int = 32  # LoRA scaling parameter (typically 2x rank)\n    dropout: float = 0.1  # Dropout applied to LoRA layers\n    target_modules: Optional[List[str]] = None  # Auto-detected if None\n    bias: str = \"none\"  # \"none\", \"all\", or \"lora_only\"\n    task_type: str = \"CAUSAL_LM\"  # Task type for PEFT library\n\n@dataclass\nclass TrainingConfig:\n    \"\"\"Main training configuration combining all aspects.\"\"\"\n    # Model settings\n    model_name_or_path: str = \"\"\n    quantization: QuantizationConfig = field(default_factory=QuantizationConfig)\n    lora: LoRAConfig = field(default_factory=LoRAConfig)\n    \n    # Training parameters\n    learning_rate: float = 2e-4\n    num_train_epochs: int = 3\n    per_device_train_batch_size: int = 1\n    gradient_accumulation_steps: int = 8\n    warmup_ratio: float = 0.1\n    max_grad_norm: float = 1.0\n    \n    # Data settings\n    max_seq_length: int = 2048\n    train_data_path: str = \"\"\n    validation_split: float = 0.1\n    \n    # Output settings\n    output_dir: str = \"./outputs\"\n    save_steps: int = 500\n    eval_steps: int = 500\n    logging_steps: int = 10\n\ndef load_config_from_yaml(config_path: Path) -> TrainingConfig:\n    \"\"\"Load training configuration from YAML file with validation.\"\"\"\n    if not config_path.exists():\n        raise FileNotFoundError(f\"Configuration file not found: {config_path}\")\n    \n    with open(config_path, 'r') as f:\n        yaml_config = yaml.safe_load(f)\n    \n    # Convert nested dictionaries to appropriate config objects\n    if 'quantization' in yaml_config:\n        yaml_config['quantization'] = QuantizationConfig(**yaml_config['quantization'])\n    \n    if 'lora' in yaml_config:\n        yaml_config['lora'] = LoRAConfig(**yaml_config['lora'])\n    \n    return TrainingConfig(**yaml_config)\n```\n\n**D. Core Logic Skeleton:**\n\nThe following skeleton provides the structure for the main fine-tuning coordinator that learners will implement:\n\n```python\n# src/training/finetuning_pipeline.py\nfrom dataclasses import dataclass\nfrom typing import Optional, Dict, Any\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import get_peft_model, LoraConfig\nimport torch\n\nclass FineTuningPipeline:\n    \"\"\"\n    Orchestrates the complete fine-tuning process from data loading to model export.\n    \n    This class coordinates all components of the fine-tuning pipeline:\n    - Dataset preparation and validation\n    - Model loading with quantization\n    - LoRA adapter configuration and injection\n    - Training execution with monitoring\n    - Evaluation and adapter merging\n    \"\"\"\n    \n    def __init__(self, config: TrainingConfig, memory_monitor: MemoryMonitor):\n        self.config = config\n        self.memory_monitor = memory_monitor\n        self.tokenizer = None\n        self.model = None\n        self.train_dataset = None\n        self.eval_dataset = None\n    \n    def setup_model_and_tokenizer(self) -> None:\n        \"\"\"\n        Load the base model with quantization and prepare tokenizer.\n        \n        This method handles the critical first step of loading the pre-trained model\n        in quantized format and setting up the tokenizer with appropriate special tokens.\n        \"\"\"\n        # TODO 1: Load tokenizer from config.model_name_or_path\n        # TODO 2: Add any missing special tokens (pad_token, eos_token)\n        # TODO 3: Configure quantization parameters using BitsAndBytesConfig\n        # TODO 4: Load model with quantization config and torch_dtype\n        # TODO 5: Resize token embeddings if new tokens were added\n        # TODO 6: Measure memory usage after model loading\n        # Hint: Use AutoTokenizer.from_pretrained() and AutoModelForCausalLM.from_pretrained()\n        # Hint: Set device_map=\"auto\" for automatic GPU placement\n        pass\n    \n    def setup_lora_adapters(self) -> None:\n        \"\"\"\n        Configure and inject LoRA adapters into the quantized model.\n        \n        This method transforms the frozen base model into a parameter-efficient\n        fine-tuning model by adding trainable low-rank adapter matrices.\n        \"\"\"\n        # TODO 1: Create LoraConfig with rank, alpha, dropout, and target modules\n        # TODO 2: If target_modules is None, auto-detect based on model architecture\n        # TODO 3: Apply PEFT to the model using get_peft_model()\n        # TODO 4: Print trainable parameter statistics\n        # TODO 5: Verify that base model parameters are frozen\n        # TODO 6: Measure memory usage after adapter injection\n        # Hint: Use model.named_modules() to find attention and MLP layers\n        # Hint: Different model architectures use different layer names (q_proj, k_proj, etc.)\n        pass\n    \n    def prepare_datasets(self) -> None:\n        \"\"\"\n        Load, validate, and preprocess training data for instruction fine-tuning.\n        \n        This method handles the complete data preparation pipeline from raw data\n        to tokenized datasets ready for training.\n        \"\"\"\n        # TODO 1: Load raw data from config.train_data_path (support JSON, JSONL, CSV)\n        # TODO 2: Validate that required fields (instruction, response) are present\n        # TODO 3: Apply chat template to format instruction-response pairs\n        # TODO 4: Tokenize the formatted text with appropriate padding and truncation\n        # TODO 5: Split into train and validation sets based on config.validation_split\n        # TODO 6: Create DataLoader objects with appropriate batch size and collation\n        # Hint: Use tokenizer.apply_chat_template() for consistent formatting\n        # Hint: Set max_length=config.max_seq_length to control memory usage\n        pass\n    \n    def execute_training(self) -> str:\n        \"\"\"\n        Run the fine-tuning training loop with monitoring and checkpointing.\n        \n        Returns:\n            str: Path to the best checkpoint directory\n        \"\"\"\n        # TODO 1: Configure TrainingArguments with learning rate, batch size, etc.\n        # TODO 2: Set up learning rate scheduler (linear warmup + cosine decay)\n        # TODO 3: Create Trainer with model, datasets, and training arguments\n        # TODO 4: Add callbacks for early stopping and memory monitoring\n        # TODO 5: Execute training with trainer.train()\n        # TODO 6: Save final checkpoint and return path to best checkpoint\n        # Hint: Use save_strategy=\"steps\" and evaluation_strategy=\"steps\"\n        # Hint: Monitor training loss and validation perplexity for early stopping\n        pass\n    \n    def evaluate_and_merge(self, checkpoint_path: str) -> Dict[str, Any]:\n        \"\"\"\n        Evaluate the fine-tuned model and merge adapters for deployment.\n        \n        Args:\n            checkpoint_path: Path to the best training checkpoint\n            \n        Returns:\n            Dict containing evaluation metrics and export paths\n        \"\"\"\n        # TODO 1: Load the best checkpoint and set model to evaluation mode\n        # TODO 2: Calculate perplexity on the validation set\n        # TODO 3: Run task-specific evaluation metrics (BLEU, ROUGE, etc.)\n        # TODO 4: Merge LoRA adapters back into the base model\n        # TODO 5: Save the merged model in HuggingFace format\n        # TODO 6: Export to GGUF format for inference optimization\n        # Hint: Use model.merge_and_unload() to combine adapters with base weights\n        # Hint: Use torch.no_grad() during evaluation for memory efficiency\n        pass\n    \n    def run_complete_pipeline(self) -> Dict[str, Any]:\n        \"\"\"\n        Execute the entire fine-tuning pipeline from start to finish.\n        \n        Returns:\n            Dict containing training results, evaluation metrics, and export paths\n        \"\"\"\n        # TODO 1: Capture baseline memory measurements\n        # TODO 2: Setup model and tokenizer with quantization\n        # TODO 3: Configure and inject LoRA adapters\n        # TODO 4: Prepare and validate datasets\n        # TODO 5: Execute training loop with monitoring\n        # TODO 6: Evaluate results and merge adapters\n        # TODO 7: Return comprehensive results dictionary\n        # Hint: Wrap each step with try-catch for error handling and cleanup\n        # Hint: Log memory usage after each major step\n        pass\n```\n\n**E. Language-Specific Hints:**\n\n- **PyTorch Memory Management**: Use `torch.cuda.empty_cache()` between major pipeline stages to free unused GPU memory. Be aware that PyTorch caches memory allocations, so `torch.cuda.memory_allocated()` shows actual usage while `nvidia-smi` shows cached memory.\n\n- **Transformers Library Integration**: The `transformers` library automatically handles device placement when using `device_map=\"auto\"`, but be careful with custom operations that might not respect the device mapping.\n\n- **PEFT Library Conventions**: The PEFT library expects specific naming conventions for target modules. Use `print(model.named_modules())` to discover the exact layer names for your model architecture before configuring `target_modules`.\n\n- **Gradient Checkpointing**: Enable with `model.gradient_checkpointing_enable()` to trade computation time for memory usage. This is particularly useful when combined with quantization.\n\n- **Mixed Precision Training**: Use `torch.cuda.amp.autocast()` and `GradScaler` for automatic mixed precision, but ensure your quantization and compute dtypes are compatible.\n\n**F. Milestone Checkpoint:**\n\nAfter implementing the core pipeline, verify correct behavior with these checkpoints:\n\n1. **Memory Efficiency Verification**: Load a 7B model with QLoRA and confirm GPU usage stays below 8GB. Expected output: \"Model loaded in 4-bit precision, using ~6GB VRAM\"\n\n2. **Parameter Efficiency Check**: Print trainable parameter counts and verify less than 1% of total parameters are trainable. Expected: \"Trainable params: 8,388,608 || all params: 6,738,415,616 || trainable%: 0.12\"\n\n3. **Training Smoke Test**: Run one training step and verify loss decreases from random initialization. Expected: Loss should decrease from ~10-11 to ~8-9 after several steps.\n\n4. **Adapter Functionality**: Generate text before and after fine-tuning to verify behavioral changes. Expected: Model should show task-specific improvements while retaining general capabilities.\n\n**G. Common Setup Issues:**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| \"CUDA out of memory\" during model loading | Model too large for GPU or quantization not applied | Check GPU memory with `nvidia-smi` | Verify `load_in_4bit=True` and reduce batch size |\n| \"No trainable parameters found\" | LoRA target modules don't match model architecture | Print `model.named_modules()` | Update `target_modules` to match actual layer names |\n| Training loss stays constant | Learning rate too low or wrong optimizer state | Check learning rate scheduler output | Increase learning rate or verify gradient flow |\n| bitsandbytes import error | CUDA version mismatch with compiled library | Check CUDA version with `nvcc --version` | Reinstall bitsandbytes with correct CUDA support |\n\n\n## Goals and Non-Goals\n\n> **Milestone(s):** All milestones - this section establishes the functional and performance targets that guide implementation decisions across the entire fine-tuning pipeline\n\n### The Resource Optimization Mental Model: The Efficiency Consultant\n\nThink of our fine-tuning pipeline as an efficiency consultant brought in to transform a massive corporate training program. The consultant's job is to take a company-wide training initiative that would normally require an enormous conference center, hundreds of full-time trainers, and months of everyone's undivided attention, and instead make it work with a small meeting room, a few part-time specialists, and minimal disruption to daily operations.\n\nJust as the efficiency consultant must clearly define what results the streamlined training program must deliver (improved employee skills, measurable competency gains, certification outcomes) while explicitly stating what it won't attempt (complete career changes, personality transformations, or training in unrelated fields), our fine-tuning pipeline must establish clear success criteria and boundaries. The consultant knows that trying to do everything leads to doing nothing well - the same principle applies to our system architecture.\n\nThe goals we set determine every subsequent design decision, from how we structure our memory optimization to which evaluation metrics we prioritize. Without clear boundaries, we risk building a system that attempts to solve every possible fine-tuning scenario but excels at none.\n\n### Functional Goals: Core Capabilities the System Must Provide\n\nOur fine-tuning pipeline must deliver a complete, reliable pathway from raw training data to a deployable fine-tuned model. The system's functional goals represent the minimum viable capabilities that define success.\n\n**End-to-End Pipeline Automation**\n\nThe system must orchestrate the entire fine-tuning workflow without requiring manual intervention between stages. A user should be able to point the system at a dataset and base model, provide a configuration file, and receive a fine-tuned model ready for deployment. This automation includes data validation, format conversion, model preparation, training execution, and evaluation reporting.\n\nThe pipeline must handle failures gracefully, providing clear error messages that guide users toward resolution. When a training run fails due to out-of-memory conditions, the system should suggest specific configuration adjustments (reduced batch size, lower LoRA rank, or quantization settings) rather than generic error messages.\n\n**Multi-Format Data Ingestion and Standardization**\n\nThe data preparation component must accept training data in multiple common formats and convert them to a standardized internal representation. The supported input formats include JSON files with instruction-response pairs, JSONL streams for large datasets, CSV files with configurable column mapping, and Parquet files for efficient columnar access.\n\n| Input Format | Required Fields | Optional Fields | Use Case |\n|--------------|----------------|-----------------|-----------|\n| JSON | `instruction`, `response` | `input`, `system`, `conversation_id` | Small structured datasets |\n| JSONL | `instruction`, `response` | `input`, `system`, `conversation_id` | Large streaming datasets |\n| CSV | Configurable columns | Any additional columns | Tabular data from databases |\n| Parquet | Configurable columns | Any additional columns | Big data workflows |\n\nThe system must validate that required fields are present and contain meaningful content, filtering out empty responses, malformed instructions, or samples that exceed token length limits. Data quality metrics should be reported, including duplicate detection rates, average token lengths, and filtering statistics.\n\n**Parameter-Efficient Fine-Tuning with Memory Optimization**\n\nThe core functional requirement is enabling fine-tuning of billion-parameter models on consumer hardware through parameter-efficient techniques. The system must implement LoRA (Low-Rank Adaptation) with configurable rank and alpha parameters, automatically detecting appropriate target modules based on the model architecture.\n\n| Model Architecture | Default Target Modules | Memory Reduction | Typical Rank Range |\n|-------------------|------------------------|------------------|-------------------|\n| Llama/Mistral | `q_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj` | 90-95% | 8-64 |\n| GPT-2/GPT-J | `attn.c_attn`, `attn.c_proj`, `mlp.c_fc`, `mlp.c_proj` | 85-90% | 8-32 |\n| T5/FLAN-T5 | `q`, `v`, `o`, `wi`, `wo` | 90-95% | 8-64 |\n\nThe system must support QLoRA quantization using 4-bit NormalFloat format, with configurable double quantization for additional memory savings. Memory usage should be monitored throughout the process, with the `MemoryMonitor` providing baseline measurements and peak usage reporting.\n\n**Flexible Training Configuration and Optimization**\n\nThe training loop must support gradient accumulation to simulate large batch sizes on memory-constrained hardware. Learning rate scheduling should include linear warmup followed by cosine or linear decay, with configurable warmup steps and total training duration.\n\nThe system must implement automatic mixed-precision training, using the configured compute dtype for forward and backward passes while storing weights in quantized format. Gradient clipping should prevent training instability, with configurable maximum gradient norms.\n\nCheckpoint management must save model state at regular intervals, track the best checkpoint based on validation loss, and enable training resumption from any saved checkpoint. Early stopping should halt training when validation loss fails to improve for a configurable number of evaluation steps.\n\n**Comprehensive Evaluation and Model Export**\n\nThe evaluation component must calculate perplexity on held-out validation data, comparing fine-tuned performance against the base model. Task-specific evaluation should support configurable metrics relevant to the fine-tuning objective, such as instruction-following accuracy or domain-specific benchmarks.\n\nThe system must merge LoRA adapter weights back into the base model, creating a standalone fine-tuned model that can be deployed without the adapter framework. Export functionality should support both HuggingFace format for compatibility with the transformers ecosystem and GGUF format for inference optimization with tools like llama.cpp.\n\n| Export Format | Target Runtime | Quantization Support | Use Case |\n|---------------|----------------|---------------------|-----------|\n| HuggingFace | Transformers library | Native PyTorch precision | Development and research |\n| GGUF | llama.cpp, Ollama | Multiple quantization levels | Production inference |\n| ONNX | ONNX Runtime | INT8, FP16 quantization | Cross-platform deployment |\n\n### Performance and Resource Goals: Memory, Speed, and Hardware Requirements\n\nThe performance goals establish measurable targets for memory efficiency, training speed, and hardware compatibility that make fine-tuning accessible to practitioners with limited resources.\n\n**Memory Efficiency Targets**\n\nThe primary performance goal is enabling fine-tuning of 7B parameter models on consumer GPUs with 12-16GB of VRAM. Through the combination of QLoRA quantization and LoRA adaptation, the system should reduce memory requirements by at least 75% compared to full fine-tuning approaches.\n\n| Model Size | Full Fine-tuning VRAM | QLoRA + LoRA VRAM | Memory Reduction | Target Hardware |\n|------------|----------------------|-------------------|------------------|----------------|\n| 7B params | 48-64GB | 12-16GB | 75-80% | RTX 3080/4070, V100 |\n| 13B params | 96-128GB | 20-24GB | 80-85% | RTX 4080/4090, A100-40GB |\n| 30B params | 200-256GB | 48-64GB | 75-80% | A100-80GB, H100 |\n\nThe `MemoryMonitor` must track actual memory usage throughout training, providing alerts when usage approaches hardware limits. Peak memory usage should remain within 90% of available VRAM to prevent out-of-memory crashes.\n\n**Training Speed and Throughput**\n\nTraining speed targets focus on practical turnaround times for iterative fine-tuning workflows. The system should complete fine-tuning runs on modest datasets (1,000-10,000 samples) within hours rather than days, enabling rapid experimentation and hyperparameter tuning.\n\nThroughput targets depend on hardware capabilities and model size, but the system should achieve reasonable tokens-per-second rates with gradient accumulation compensating for smaller effective batch sizes on memory-constrained hardware.\n\n| Hardware Class | Expected Tokens/Second | Typical Training Time (5K samples) | Batch Size Constraints |\n|----------------|----------------------|-----------------------------------|----------------------|\n| Consumer GPU (12-16GB) | 500-1,000 | 2-4 hours | Micro-batch 1-2, accumulate 8-16 |\n| Professional GPU (24-32GB) | 1,000-2,000 | 1-2 hours | Micro-batch 2-4, accumulate 4-8 |\n| Data Center GPU (40-80GB) | 2,000-4,000 | 30-60 minutes | Micro-batch 4-8, accumulate 2-4 |\n\n**Hardware Compatibility and Requirements**\n\nThe system must support CUDA-compatible GPUs with compute capability 6.1 or higher, ensuring compatibility with the bitsandbytes library for 4-bit quantization. CPU fallback should be available for development and testing, though training performance will be significantly reduced.\n\nSystem memory requirements should scale with model size but remain reasonable for desktop workstations. A minimum of 32GB system RAM is recommended for 7B parameter models, with 64GB preferred for larger models to handle data loading and preprocessing overhead.\n\n> **Critical Performance Insight**: The combination of quantization and parameter-efficient fine-tuning creates a multiplicative memory reduction effect. QLoRA provides a 4x reduction in model weight storage, while LoRA reduces trainable parameters by 10-100x, resulting in overall memory savings that make large model fine-tuning feasible on consumer hardware.\n\n### Non-Goals: What This System Explicitly Does Not Handle\n\nClearly defining what the system does not attempt is crucial for setting appropriate expectations and maintaining focus on core capabilities. These non-goals help prevent scope creep and guide users toward complementary tools when needed.\n\n**Multi-GPU and Distributed Training**\n\nThe initial system design explicitly excludes multi-GPU training and distributed fine-tuning across multiple machines. While these capabilities would improve training speed and enable larger models, they introduce significant complexity in gradient synchronization, memory coordination, and failure handling.\n\nUsers requiring multi-GPU training should consider established frameworks like DeepSpeed or FairScale that specialize in distributed optimization. Our single-GPU focus allows for simpler architecture and more reliable execution on the hardware most practitioners have available.\n\n**Full Fine-Tuning and Alternative Parameter-Efficient Methods**\n\nThe system does not support traditional full fine-tuning where all model parameters are updated. This decision reflects both memory constraints and the empirical evidence that parameter-efficient methods often achieve comparable performance with dramatically lower resource requirements.\n\nAlternative parameter-efficient methods like AdaLoRA, DoRA, or prompt tuning are also excluded from the initial implementation. The focus on LoRA and QLoRA provides a proven, well-supported approach that covers the majority of use cases without the complexity of supporting multiple adapter architectures.\n\n**Custom Model Architectures and Non-Transformer Models**\n\nThe system targets standard transformer architectures supported by the HuggingFace transformers library. Custom model architectures, non-transformer models, or heavily modified transformer variants are out of scope.\n\nThis limitation reflects the automatic target module detection logic, which relies on known naming conventions for attention and feed-forward layers. Supporting arbitrary architectures would require manual configuration for each model type, significantly increasing complexity.\n\n**Production Inference Serving and Deployment**\n\nWhile the system exports models in formats suitable for inference (HuggingFace and GGUF), it does not include inference serving capabilities, API endpoints, or production deployment tools. Users should utilize dedicated inference frameworks like vLLM, Text Generation Inference, or llama.cpp for serving fine-tuned models.\n\nThe export functionality provides the bridge to these inference systems, but the fine-tuning pipeline itself focuses solely on the training and evaluation phases of the model lifecycle.\n\n**Advanced Evaluation and Safety Benchmarks**\n\nThe evaluation component provides basic metrics like perplexity and configurable task-specific benchmarks, but it does not include comprehensive safety evaluation, bias detection, or advanced capabilities assessment.\n\nUsers requiring thorough model evaluation should integrate with specialized evaluation frameworks like EleutherAI's LM Evaluation Harness or custom evaluation pipelines. The system provides the fine-tuned model and basic quality metrics needed to initiate more comprehensive evaluation workflows.\n\n**Data Collection, Annotation, and Active Learning**\n\nThe system assumes training data is already collected and formatted. It does not provide data collection tools, annotation interfaces, or active learning workflows to improve dataset quality over time.\n\nData preparation focuses on format conversion, validation, and quality filtering of existing datasets. Users need separate tools for data creation, human annotation, or iterative dataset improvement based on model performance.\n\n**Architecture Decision Record: Scope Limitation Strategy**\n\n> **Decision: Focused Single-GPU Implementation**\n> - **Context**: Fine-tuning systems can range from simple scripts to comprehensive platforms supporting every possible configuration and deployment scenario\n> - **Options Considered**: \n>   1. Comprehensive platform covering all fine-tuning approaches and deployment scenarios\n>   2. Focused single-GPU system optimizing for accessibility and reliability\n>   3. Modular framework allowing plugin-based extension to additional capabilities\n> - **Decision**: Focused single-GPU system with clear scope boundaries\n> - **Rationale**: The 80/20 principle applies - most practitioners need reliable fine-tuning on consumer hardware more than they need every possible advanced feature. A focused implementation can achieve higher quality and reliability in core use cases than a sprawling system trying to handle everything. Clear non-goals prevent feature creep and guide architectural decisions toward simplicity and maintainability.\n> - **Consequences**: Some users will need additional tools for advanced scenarios, but the core system remains approachable and reliable for the majority use case. Future extensions can add capabilities without compromising the foundation.\n\n### Goal Achievement Metrics and Success Criteria\n\nTo ensure the system meets its stated goals, we establish measurable criteria for success across functional and performance dimensions.\n\n**Functional Success Metrics**\n\nPipeline completeness is measured by successful execution from raw data to deployed model without manual intervention. Success requires 95% of well-formed datasets to process without user intervention, with clear error messages guiding resolution of the remaining 5%.\n\nData ingestion success is measured by format support coverage and quality filtering effectiveness. The system should successfully process 90% of common dataset formats found in the wild, with quality metrics showing appropriate filtering of malformed or low-quality samples.\n\nMemory optimization effectiveness is measured by comparing actual memory usage against theoretical requirements. QLoRA should achieve within 10% of theoretical 4x memory reduction, and LoRA should reduce trainable parameters to less than 1% of total model parameters.\n\n**Performance Success Metrics**\n\nTraining speed targets are expressed as wall-clock time for standard benchmark tasks. Fine-tuning a 7B parameter model on 5,000 instruction-response pairs should complete within 4 hours on RTX 4080-class hardware with appropriate hyperparameters.\n\nMemory efficiency targets require peak VRAM usage to remain within hardware constraints with appropriate configuration. A 7B parameter model should fine-tune successfully on 16GB VRAM with QLoRA rank 16 and appropriate batch size settings.\n\nQuality preservation requires fine-tuned models to maintain base model capabilities while gaining task-specific performance. Perplexity on held-out general text should not increase by more than 5%, while task-specific metrics should show measurable improvement over the base model.\n\n**Reliability and Usability Metrics**\n\nSystem reliability is measured by failure recovery and error reporting quality. Training interruptions should resume correctly from checkpoints 100% of the time, and error messages should provide actionable guidance in 90% of failure scenarios.\n\nConfiguration ease is measured by successful setup time for new users. A practitioner familiar with Python and machine learning should be able to fine-tune their first model within 30 minutes of system setup, using provided example configurations and datasets.\n\nDocumentation completeness ensures all supported features have corresponding examples and common pitfalls are addressed with specific solutions rather than generic advice.\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Configuration Management | YAML files with Pydantic validation | Hydra with hierarchical configs |\n| Memory Monitoring | torch.cuda memory functions | Custom CUDA memory profiler |\n| Logging | Python logging with file output | Weights & Biases integration |\n| Data Loading | PyTorch DataLoader with custom datasets | HuggingFace datasets with streaming |\n| Model Loading | transformers.AutoModel with manual config | Custom model factory with architecture detection |\n| Evaluation | Simple perplexity calculation | Integration with lm-evaluation-harness |\n\n#### Configuration Schema Implementation\n\nThe system requires a hierarchical configuration structure that captures all tunable parameters while providing sensible defaults for common use cases.\n\n```python\nfrom dataclasses import dataclass\nfrom typing import Optional, List, Dict, Any\nfrom pathlib import Path\nimport yaml\n\n@dataclass\nclass QuantizationConfig:\n    \"\"\"Configuration for 4-bit model quantization using bitsandbytes.\"\"\"\n    load_in_4bit: bool = True\n    bnb_4bit_quant_type: str = \"nf4\"  # Use NF4_QUANTIZATION constant\n    bnb_4bit_compute_dtype: str = \"bfloat16\"  # Compute dtype for forward pass\n    bnb_4bit_use_double_quant: bool = True  # Double quantization for extra memory savings\n\n@dataclass\nclass LoRAConfig:\n    \"\"\"Configuration for Low-Rank Adaptation parameters.\"\"\"\n    r: int = 16  # DEFAULT_LORA_RANK - rank of adaptation matrices\n    alpha: int = 32  # DEFAULT_LORA_ALPHA - scaling parameter\n    dropout: float = 0.1  # Dropout rate for LoRA layers\n    target_modules: Optional[List[str]] = None  # Auto-detected if None\n    bias: str = \"none\"  # Bias update strategy: \"none\", \"all\", or \"lora_only\"\n    task_type: str = \"CAUSAL_LM\"  # Task type for PEFT library\n\n@dataclass\nclass TrainingConfig:\n    \"\"\"Complete training configuration combining model, quantization, and training parameters.\"\"\"\n    # Model and data paths\n    model_name_or_path: str = \"\"  # HuggingFace model name or local path\n    dataset_path: str = \"\"  # Path to training dataset\n    output_dir: str = \"./output\"  # Directory for checkpoints and logs\n    \n    # Component configurations\n    quantization: QuantizationConfig = None\n    lora: LoRAConfig = None\n    \n    # Training hyperparameters\n    learning_rate: float = 2e-4\n    num_train_epochs: int = 3\n    per_device_train_batch_size: int = 1\n    gradient_accumulation_steps: int = 8\n    warmup_steps: int = 100\n    max_length: int = 512\n    \n    # Evaluation and checkpointing\n    evaluation_strategy: str = \"steps\"\n    eval_steps: int = 500\n    save_steps: int = 500\n    logging_steps: int = 10\n    \n    def __post_init__(self):\n        \"\"\"Initialize nested configurations with defaults if not provided.\"\"\"\n        if self.quantization is None:\n            self.quantization = QuantizationConfig()\n        if self.lora is None:\n            self.lora = LoRAConfig()\n\ndef load_config_from_yaml(config_path: Path) -> TrainingConfig:\n    \"\"\"\n    Load and validate training configuration from YAML file.\n    \n    Provides detailed validation and helpful error messages for common\n    configuration mistakes.\n    \"\"\"\n    # TODO 1: Read YAML file and handle file not found errors\n    # TODO 2: Parse nested configuration sections (quantization, lora, training)\n    # TODO 3: Validate required fields (model_name_or_path, dataset_path)\n    # TODO 4: Apply defaults for optional fields using dataclass defaults\n    # TODO 5: Validate parameter ranges (learning_rate > 0, rank > 0, etc.)\n    # TODO 6: Return fully constructed TrainingConfig object\n    pass\n```\n\n#### Memory Monitoring Infrastructure\n\nThe memory monitoring system provides detailed tracking of GPU and system memory usage throughout the fine-tuning process, enabling optimization and debugging of memory-related issues.\n\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Optional\nimport torch\nimport psutil\nimport time\n\n@dataclass\nclass MemoryMonitor:\n    \"\"\"Tracks memory usage throughout training for optimization and debugging.\"\"\"\n    baseline_gpu_memory: Optional[int] = None  # GPU memory before model loading\n    baseline_system_memory: int = 0  # System memory at initialization\n    measurements: List[Dict] = field(default_factory=list)  # Time-series measurements\n    \n    def capture_baseline(self) -> None:\n        \"\"\"\n        Record initial memory state before model loading.\n        \n        This baseline enables calculation of memory overhead from each\n        component (quantization, LoRA injection, training loop).\n        \"\"\"\n        # TODO 1: Check if CUDA is available and get GPU memory info\n        # TODO 2: Record current GPU memory usage if available\n        # TODO 3: Record system memory usage using psutil\n        # TODO 4: Store baseline measurements with MEMORY_BASELINE_STAGE marker\n        pass\n    \n    def measure_current_usage(self, stage: str) -> Dict[str, float]:\n        \"\"\"\n        Measure current memory usage and return detailed statistics.\n        \n        Args:\n            stage: Descriptive name for current training stage\n            \n        Returns:\n            Dictionary with GPU memory, system memory, and delta from baseline\n        \"\"\"\n        # TODO 1: Get current GPU memory allocated and reserved\n        # TODO 2: Get current system memory usage (RSS and virtual)\n        # TODO 3: Calculate deltas from baseline measurements\n        # TODO 4: Create measurement record with timestamp and stage\n        # TODO 5: Append to measurements list for historical tracking\n        # TODO 6: Return current usage statistics\n        pass\n    \n    def get_peak_usage(self) -> Dict[str, float]:\n        \"\"\"\n        Return peak memory usage across all recorded measurements.\n        \n        Useful for determining minimum hardware requirements and\n        optimizing configuration parameters.\n        \"\"\"\n        # TODO 1: Iterate through all measurements to find peak values\n        # TODO 2: Track peak GPU memory, system memory, and deltas\n        # TODO 3: Return dictionary with peak usage statistics\n        pass\n\n# Memory monitoring constants\nMEMORY_BASELINE_STAGE = \"baseline\"  # Stage marker for initial measurement\nDEFAULT_MEMORY_LOG_INTERVAL = 100  # Steps between memory measurements\n```\n\n#### Pipeline Orchestration Structure\n\nThe main pipeline class coordinates all components and manages the end-to-end training workflow, providing a clean interface for users while handling complex component interactions internally.\n\n```python\nfrom typing import Optional, Dict, Any\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import get_peft_model, TaskType\n\nclass FineTuningPipeline:\n    \"\"\"\n    Main orchestrator for the end-to-end fine-tuning pipeline.\n    \n    Coordinates data preparation, model loading, LoRA setup, training,\n    and evaluation while providing comprehensive logging and error handling.\n    \"\"\"\n    \n    def __init__(self, config: TrainingConfig):\n        self.config = config\n        self.memory_monitor = MemoryMonitor()\n        self.tokenizer: Optional = None\n        self.model: Optional = None\n        self.peft_model: Optional = None\n        \n        # Initialize memory baseline before any model operations\n        self.memory_monitor.capture_baseline()\n    \n    def setup_model_and_tokenizer(self) -> None:\n        \"\"\"\n        Load quantized model and configure tokenizer for instruction tuning.\n        \n        Handles model loading with quantization configuration and tokenizer\n        setup with appropriate special tokens and padding configuration.\n        \"\"\"\n        # TODO 1: Load tokenizer with trust_remote_code and padding configuration\n        # TODO 2: Add special tokens if missing (pad_token, eos_token)\n        # TODO 3: Create BitsAndBytesConfig from quantization settings\n        # TODO 4: Load model with quantization config and device mapping\n        # TODO 5: Measure memory usage after model loading\n        # TODO 6: Resize token embeddings if tokenizer was modified\n        pass\n    \n    def setup_lora_adapters(self) -> None:\n        \"\"\"\n        Inject LoRA adapters into the frozen base model.\n        \n        Automatically detects target modules based on model architecture\n        and injects low-rank adaptation matrices.\n        \"\"\"\n        # TODO 1: Auto-detect target modules if not specified in config\n        # TODO 2: Create LoRA configuration with rank, alpha, and targets\n        # TODO 3: Wrap model with PEFT adapters using get_peft_model\n        # TODO 4: Print trainable parameter summary for verification\n        # TODO 5: Measure memory usage after adapter injection\n        pass\n    \n    def prepare_datasets(self) -> None:\n        \"\"\"\n        Load and preprocess training data with proper formatting.\n        \n        Handles data loading, chat template application, tokenization,\n        and train-validation splitting.\n        \"\"\"\n        # TODO 1: Load dataset from configured path and format\n        # TODO 2: Apply chat template and instruction formatting\n        # TODO 3: Tokenize with truncation and padding\n        # TODO 4: Split into training and validation sets\n        # TODO 5: Create DataLoader instances for training loop\n        pass\n    \n    def execute_training(self) -> str:\n        \"\"\"\n        Run the complete training loop with monitoring and checkpointing.\n        \n        Returns:\n            Path to the best checkpoint based on validation loss\n        \"\"\"\n        # TODO 1: Initialize trainer with model, datasets, and training arguments\n        # TODO 2: Set up logging and evaluation callbacks\n        # TODO 3: Configure checkpoint saving and early stopping\n        # TODO 4: Execute training with memory monitoring\n        # TODO 5: Return path to best checkpoint for evaluation\n        pass\n    \n    def evaluate_and_merge(self, checkpoint_path: str) -> Dict[str, Any]:\n        \"\"\"\n        Evaluate fine-tuned model and merge adapters for deployment.\n        \n        Args:\n            checkpoint_path: Path to the best training checkpoint\n            \n        Returns:\n            Dictionary containing evaluation metrics and export paths\n        \"\"\"\n        # TODO 1: Load checkpoint and run perplexity evaluation\n        # TODO 2: Run task-specific evaluation benchmarks\n        # TODO 3: Merge LoRA adapters into base model weights\n        # TODO 4: Export merged model in HuggingFace format\n        # TODO 5: Export in GGUF format if requested\n        # TODO 6: Return evaluation results and export paths\n        pass\n    \n    def run_complete_pipeline(self) -> Dict[str, Any]:\n        \"\"\"\n        Execute the complete fine-tuning pipeline from data to deployment.\n        \n        This is the main entry point for end-to-end training workflows.\n        Provides comprehensive error handling and progress reporting.\n        \n        Returns:\n            Dictionary containing training metrics, evaluation results,\n            and paths to exported models\n        \"\"\"\n        results = {}\n        \n        try:\n            # TODO 1: Setup model and tokenizer with quantization\n            # TODO 2: Inject LoRA adapters and verify parameter efficiency\n            # TODO 3: Prepare and validate training datasets\n            # TODO 4: Execute training loop with monitoring\n            # TODO 5: Evaluate final model and merge adapters\n            # TODO 6: Generate comprehensive results report\n            # TODO 7: Clean up GPU memory and temporary files\n            \n            return results\n            \n        except Exception as e:\n            # TODO: Implement comprehensive error handling with cleanup\n            # Include memory state, configuration dump, and recovery suggestions\n            raise\n```\n\n#### Constants and Configuration Defaults\n\n```python\n# Quantization constants\nNF4_QUANTIZATION = \"nf4\"  # 4-bit NormalFloat quantization type\nFP4_QUANTIZATION = \"fp4\"  # Alternative 4-bit quantization\nINT4_QUANTIZATION = \"int4\"  # Integer 4-bit quantization\n\n# LoRA defaults optimized for instruction tuning\nDEFAULT_LORA_RANK = 16  # Balanced rank for most use cases\nDEFAULT_LORA_ALPHA = 32  # Standard alpha scaling (2x rank)\nDEFAULT_LORA_DROPOUT = 0.1  # Conservative dropout rate\n\n# Memory monitoring stages\nMEMORY_BASELINE_STAGE = \"baseline\"\nMEMORY_MODEL_LOADED_STAGE = \"model_loaded\" \nMEMORY_LORA_INJECTED_STAGE = \"lora_injected\"\nMEMORY_TRAINING_START_STAGE = \"training_start\"\nMEMORY_TRAINING_PEAK_STAGE = \"training_peak\"\n\n# Training defaults\nDEFAULT_LEARNING_RATE = 2e-4  # Conservative learning rate for stability\nDEFAULT_WARMUP_RATIO = 0.1  # 10% of training steps for warmup\nDEFAULT_MAX_LENGTH = 512  # Token limit for memory efficiency\n```\n\n#### Milestone Checkpoints\n\nAfter implementing this Goals and Non-Goals section, verify the following:\n\n**Goal Specification Checkpoint:**\n1. Run configuration validation: `python -c \"from config import load_config_from_yaml; load_config_from_yaml('test_config.yaml')\"`\n2. Verify memory monitoring works: Initialize `MemoryMonitor` and call `capture_baseline()`\n3. Check scope boundaries: Attempt to load multi-GPU config and verify it's rejected with clear error\n4. Validate performance targets: Test memory estimation against stated hardware requirements\n\n**Expected Behavior:**\n- Configuration loading should provide detailed validation errors for missing required fields\n- Memory monitoring should report current GPU and system memory usage\n- Scope validation should clearly state what features are not supported\n- Performance estimation should align with stated memory reduction targets\n\n**Common Issues and Fixes:**\n- Configuration validation fails silently: Add explicit field validation with descriptive errors\n- Memory monitoring returns zero: Check CUDA availability and torch.cuda.memory_allocated()\n- Scope creep in implementation: Regularly review code against explicit non-goals list\n\n\n## High-Level Architecture\n\n> **Milestone(s):** All milestones - this section establishes the architectural foundation that enables all components to work together effectively throughout the fine-tuning pipeline\n\nThe LLM fine-tuning pipeline follows a modular architecture designed around the core constraint of memory efficiency while maintaining training effectiveness. Think of this system like a **precision manufacturing assembly line** where each station performs a specialized transformation on the raw materials (data and models) while carefully monitoring resource consumption at every step. Each component in our pipeline has a specific responsibility, much like how each station in a factory handles one aspect of production - some prepare materials, others perform the actual work, and the final stations verify quality and package the output.\n\nThe architecture addresses the fundamental **memory wall problem** that makes traditional fine-tuning impractical for billion-parameter models on consumer hardware. By decomposing the problem into discrete, well-coordinated components, we can apply different optimization strategies at each stage while maintaining clean interfaces and error propagation paths between subsystems.\n\n### Component Overview\n\nThe fine-tuning pipeline consists of five major subsystems that collaborate to transform raw training data and a base model into a fine-tuned, deployment-ready model. Each component owns specific aspects of the transformation process and maintains clear boundaries with its collaborators.\n\n![System Component Overview](./diagrams/system-overview.svg)\n\nThe **Dataset Preparation Component** serves as the data ingestion and standardization layer. This component transforms heterogeneous input data from various formats (JSON, JSONL, CSV, Parquet) into a standardized instruction-response format that the training loop can consume efficiently. Think of this component as a **language tutor preparing lesson materials** - it takes raw educational content and structures it into coherent learning exercises with proper formatting, length constraints, and quality standards. The component handles chat template application, ensuring that conversational data matches the specific prompt format expected by the target model family. It also manages tokenization with appropriate padding and truncation strategies, and splits the data into training and validation sets with configurable stratification options.\n\n| Responsibility | Description | Input | Output |\n|---------------|-------------|-------|---------|\n| Data Ingestion | Load training data from multiple file formats | Raw files (JSON, CSV, Parquet) | Validated records |\n| Format Standardization | Convert to instruction-response pairs | Raw conversational data | Structured training samples |\n| Chat Template Application | Apply model-specific prompt formatting | Instruction-response pairs | Templated prompts |\n| Tokenization | Convert text to token sequences with proper masks | Formatted text | Tokenized datasets |\n| Train-Val Splitting | Partition data with stratification options | Complete dataset | Training and validation splits |\n| Quality Filtering | Remove duplicates, short samples, overlength data | Raw training samples | Clean, filtered dataset |\n\nThe **LoRA Configuration Component** handles the setup of parameter-efficient fine-tuning adapters. This component automatically identifies target modules within the model architecture and injects low-rank matrices that will learn task-specific adaptations. The mental model here is a **skill overlay system** - imagine overlaying transparent sheets with new skills onto a master craftsperson's existing knowledge base, where each sheet contains incremental expertise that enhances specific capabilities without erasing the foundational knowledge. The component manages rank and alpha parameter selection, balancing adapter capacity with memory efficiency, and ensures that the vast majority of the base model parameters remain frozen during training.\n\n| Responsibility | Description | Input | Output |\n|---------------|-------------|-------|---------|\n| Target Module Detection | Identify attention and MLP layers for adaptation | Model architecture | List of target module names |\n| Rank-Alpha Selection | Choose low-rank dimensions and scaling factors | Task requirements, memory constraints | LoRA hyperparameters |\n| Adapter Initialization | Create and inject low-rank matrices | Target modules, LoRA config | Instrumented model |\n| Parameter Analysis | Count trainable vs frozen parameters | Adapted model | Efficiency metrics |\n| Memory Estimation | Predict VRAM usage with adapters | Model size, LoRA config | Memory requirements |\n\nThe **QLoRA Quantization Component** implements 4-bit quantization using NormalFloat precision to dramatically reduce memory consumption while preserving model quality. Think of this component as a **compression expert** who understands exactly which information can be stored in reduced precision without losing the essential characteristics that make the model effective. This component handles the complex orchestration of mixed-precision training where weights are stored in 4-bit format but computations happen in float16 or bfloat16 precision. It also manages double quantization, where even the quantization constants themselves are quantized for additional memory savings.\n\n| Responsibility | Description | Input | Output |\n|---------------|-------------|-------|---------|\n| 4-bit Model Loading | Load base model in NF4 quantized format | Model checkpoint path | Quantized model |\n| Quantization Configuration | Set up NF4, compute dtype, double quantization | Memory and quality targets | Quantization parameters |\n| Mixed-Precision Setup | Configure storage vs compute data types | Hardware capabilities | Training precision config |\n| Memory Benchmarking | Measure actual VRAM usage | Quantized model | Memory usage statistics |\n| Quality-Memory Analysis | Compare perplexity across quantization levels | Validation data | Quality degradation metrics |\n\nThe **Training Loop Component** orchestrates the actual fine-tuning process with sophisticated optimization strategies. This component acts like a **personal trainer** who carefully monitors progress, adjusts the training intensity, and knows when to push harder or ease back to avoid injury (catastrophic forgetting). It implements gradient accumulation to simulate large batch sizes across multiple micro-batches, manages learning rate scheduling with warmup and decay phases, and handles checkpoint saving with configurable strategies for model persistence and recovery.\n\n| Responsibility | Description | Input | Output |\n|---------------|-------------|-------|---------|\n| Training Orchestration | Coordinate forward/backward passes with accumulation | Prepared datasets, model | Trained adapters |\n| Learning Rate Scheduling | Manage warmup, decay, and optimization phases | Training progress | Dynamic learning rates |\n| Gradient Accumulation | Simulate large batches across micro-batches | Memory constraints | Effective batch training |\n| Checkpoint Management | Save model state and enable resumption | Training milestones | Persistent checkpoints |\n| Loss Monitoring | Track convergence and detect overfitting | Training metrics | Early stopping signals |\n| Memory Management | Monitor VRAM usage throughout training | Training state | Resource utilization alerts |\n\nThe **Evaluation and Merging Component** measures the effectiveness of fine-tuning and prepares the model for deployment. This component functions as an **exam proctor** who objectively measures learning progress against established benchmarks and then packages the knowledge for real-world application. It computes perplexity on validation data, runs task-specific evaluations to measure improvement over the base model, merges the learned LoRA adapter weights back into the base model for standalone inference, and exports the final model in deployment-optimized formats like GGUF for efficient inference engines.\n\n| Responsibility | Description | Input | Output |\n|---------------|-------------|-------|---------|\n| Perplexity Evaluation | Measure language modeling performance | Trained model, validation data | Perplexity scores |\n| Task-Specific Benchmarking | Run domain-relevant evaluation suites | Model checkpoints | Performance metrics |\n| Adapter Merging | Fuse LoRA weights into base model | Trained adapters, base model | Merged standalone model |\n| Model Export | Convert to inference-optimized formats | Merged model | GGUF/HF format files |\n| Quality Comparison | Compare fine-tuned vs base model performance | Evaluation results | Improvement analysis |\n| Deployment Validation | Verify merged model inference quality | Exported model files | Deployment readiness |\n\n> **Key Architectural Insight**: The separation of quantization, adaptation, and training concerns allows each component to optimize for its specific responsibilities while maintaining composability. The LoRA component focuses purely on parameter efficiency, the quantization component handles memory optimization, and the training component manages convergence - this separation of concerns prevents the complexity explosion that would occur if these responsibilities were mixed.\n\n### Data Flow and Dependencies\n\nThe components interact through a carefully orchestrated data flow that minimizes memory overhead while maintaining training stability. The flow follows a **pipeline pattern** where each stage produces artifacts consumed by subsequent stages, with explicit checkpointing and validation at component boundaries.\n\n![Training Flow Sequence](./diagrams/training-flow-sequence.svg)\n\nThe data flow begins with the **Dataset Preparation Component** ingesting raw training data and producing tokenized datasets with proper train-validation splits. This component has no dependencies on other pipeline components but requires access to the target model's tokenizer to ensure consistent token encoding. The tokenizer dependency creates a weak coupling to the model loading process, but this is resolved by loading the tokenizer independently during the data preparation phase.\n\n| Data Flow Stage | Component | Input Dependencies | Output Artifacts | Downstream Consumers |\n|----------------|-----------|-------------------|------------------|---------------------|\n| Data Ingestion | Dataset Preparation | Raw data files, model tokenizer | Tokenized train/val datasets | Training Loop |\n| Model Quantization | QLoRA Quantization | Base model checkpoint, hardware info | 4-bit quantized model | LoRA Configuration |\n| Adapter Setup | LoRA Configuration | Quantized model, efficiency targets | Adapter-instrumented model | Training Loop |\n| Fine-tuning | Training Loop | Prepared datasets, adapted model | Checkpoint series | Evaluation & Merging |\n| Deployment Prep | Evaluation & Merging | Best checkpoint, validation data | Merged model, metrics | External deployment |\n\nThe **QLoRA Quantization Component** loads the base model independently and produces a 4-bit quantized version that serves as the foundation for subsequent processing. This component has no dependencies on other pipeline components but must coordinate with the system's CUDA environment and bitsandbytes library. The quantized model becomes the input for the LoRA configuration process.\n\nThe **LoRA Configuration Component** receives the quantized model and injects low-rank adapters into the target modules. This component depends on the quantized model from the previous stage and produces an adapter-instrumented model ready for training. The component must coordinate with the quantization setup to ensure that adapter parameters are not quantized while maintaining the 4-bit base model weights.\n\nThe **Training Loop Component** orchestrates the convergence process using the prepared datasets and adapter-instrumented model. This component has dependencies on both the Dataset Preparation and LoRA Configuration outputs, making it a convergence point in the pipeline. The training loop produces a series of checkpoints, with the best checkpoint (determined by validation loss) becoming the input for evaluation and merging.\n\nThe **Evaluation and Merging Component** consumes the best training checkpoint along with the validation dataset to produce final performance metrics and a deployment-ready merged model. This component depends on outputs from both the Training Loop (for the checkpoint) and Dataset Preparation (for evaluation data).\n\n```\nPipeline Flow Dependencies:\n\nRaw Data + Base Model Checkpoint\n    ↓\nDataset Preparation ← tokenizer ← QLoRA Quantization\n    ↓                                ↓\nTokenized Datasets              4-bit Quantized Model\n    ↓                                ↓\n    ↓                        LoRA Configuration\n    ↓                                ↓\n    └─────→ Training Loop ←─── Adapter Model\n                ↓\n            Checkpoints\n                ↓\n         Evaluation & Merging ← Validation Data\n                ↓\n        Deployed Model + Metrics\n```\n\n**Inter-component communication** follows a message-passing pattern with well-defined interfaces and error propagation. Each component exposes configuration objects that capture all necessary parameters for its operation, enabling components to validate their inputs before processing begins.\n\n| Interface | Producer | Consumer | Data Structure | Validation Rules |\n|-----------|----------|----------|---------------|------------------|\n| Training Data | Dataset Preparation | Training Loop | `torch.utils.data.Dataset` | Non-empty, consistent tokenization |\n| Quantized Model | QLoRA Quantization | LoRA Configuration | `transformers.PreTrainedModel` | 4-bit weights, compatible device |\n| Adapted Model | LoRA Configuration | Training Loop | `peft.PeftModel` | Frozen base, trainable adapters |\n| Checkpoint Path | Training Loop | Evaluation & Merging | `str` (file path) | Exists, loadable state dict |\n| Evaluation Results | Evaluation & Merging | External | `Dict[str, float]` | Valid metrics, improvement scores |\n\n**State coordination** between components happens through shared configuration objects and explicit checkpointing. The `TrainingConfig` object serves as the central coordination mechanism, containing all the parameters needed by each component. This design ensures that components receive consistent configuration while maintaining loose coupling.\n\n| Configuration Object | Scope | Fields | Used By |\n|---------------------|-------|---------|---------|\n| `TrainingConfig` | Global | model_name_or_path, learning_rate, num_train_epochs | All components |\n| `QuantizationConfig` | QLoRA | load_in_4bit, bnb_4bit_quant_type, compute_dtype | QLoRA, LoRA |\n| `LoRAConfig` | Adapters | r, alpha, target_modules, dropout | LoRA, Training |\n| `DataConfig` | Dataset | train_file, validation_split, max_length | Dataset Preparation |\n\n**Error propagation** follows a fail-fast principle where each component validates its inputs and raises specific exceptions for different failure modes. Components catch and re-raise errors with additional context about the failure location and potential recovery strategies.\n\n> **Critical Design Decision**: Components communicate through filesystem artifacts (saved datasets, model checkpoints) rather than in-memory objects. This design trades some performance for robustness - if any component fails, the pipeline can resume from the last successful stage without recomputing earlier steps.\n\n### Recommended File Structure\n\nThe codebase organization follows a **domain-driven structure** where each major component lives in its own module with clear separation of concerns. The structure accommodates both the learning progression (implementing components incrementally) and production usage (clear module boundaries for testing and maintenance).\n\n```\nllm-fine-tuning-pipeline/\n├── README.md                           ← Quick start guide and examples\n├── requirements.txt                    ← Python dependencies\n├── pyproject.toml                      ← Project configuration and build settings\n├── config/                            ← Configuration templates and examples\n│   ├── example_training_config.yaml   ← Sample training configuration\n│   ├── model_configs/                 ← Model-specific configuration templates\n│   │   ├── llama2_7b.yaml\n│   │   ├── mistral_7b.yaml\n│   │   └── phi2_3b.yaml\n│   └── dataset_configs/               ← Dataset format examples\n│       ├── alpaca_format.json\n│       ├── conversation_format.json\n│       └── instruction_format.json\n├── src/                               ← Main source code\n│   ├── __init__.py\n│   ├── fine_tuning_pipeline.py        ← Main pipeline orchestrator class\n│   ├── config/                        ← Configuration management\n│   │   ├── __init__.py\n│   │   ├── training_config.py         ← TrainingConfig, LoRAConfig, QuantizationConfig\n│   │   ├── data_config.py             ← Dataset configuration objects\n│   │   └── config_loader.py           ← YAML/JSON config file loading\n│   ├── data_preparation/              ← Dataset Preparation Component\n│   │   ├── __init__.py\n│   │   ├── data_loader.py             ← Multi-format data ingestion\n│   │   ├── chat_template.py           ← Model-specific prompt formatting\n│   │   ├── tokenizer_pipeline.py      ← Tokenization with padding/truncation\n│   │   ├── data_splitter.py           ← Train-validation splitting\n│   │   ├── quality_filter.py          ← Data cleaning and validation\n│   │   └── dataset_builder.py         ← Coordinate data preparation stages\n│   ├── lora_config/                   ← LoRA Configuration Component\n│   │   ├── __init__.py\n│   │   ├── target_modules.py          ← Auto-detect adaptation targets\n│   │   ├── adapter_setup.py           ← LoRA injection and initialization\n│   │   ├── parameter_analysis.py      ← Count trainable parameters\n│   │   └── lora_manager.py            ← Coordinate LoRA setup stages\n│   ├── quantization/                  ← QLoRA Quantization Component\n│   │   ├── __init__.py\n│   │   ├── model_loader.py            ← 4-bit NF4 quantized model loading\n│   │   ├── quantization_setup.py      ← Configure quantization parameters\n│   │   ├── memory_monitor.py          ← VRAM usage tracking\n│   │   └── quantization_manager.py    ← Coordinate quantization stages\n│   ├── training/                      ← Training Loop Component\n│   │   ├── __init__.py\n│   │   ├── training_args.py           ← Training hyperparameter management\n│   │   ├── gradient_accumulation.py   ← Micro-batch coordination\n│   │   ├── lr_scheduler.py            ← Learning rate scheduling\n│   │   ├── checkpoint_manager.py      ← Model saving and resumption\n│   │   ├── loss_tracker.py            ← Training metrics and early stopping\n│   │   └── training_orchestrator.py   ← Main training loop coordination\n│   ├── evaluation/                    ← Evaluation and Merging Component\n│   │   ├── __init__.py\n│   │   ├── perplexity_calculator.py   ← Language modeling evaluation\n│   │   ├── task_evaluator.py          ← Domain-specific benchmarking\n│   │   ├── adapter_merger.py          ← LoRA weight merging\n│   │   ├── model_exporter.py          ← GGUF and HF format export\n│   │   └── evaluation_manager.py      ← Coordinate evaluation stages\n│   └── utils/                         ← Shared utilities\n│       ├── __init__.py\n│       ├── logging_setup.py           ← Structured logging configuration\n│       ├── device_utils.py            ← CUDA device management\n│       ├── file_utils.py              ← File I/O and path handling\n│       └── metric_utils.py            ← Common evaluation metrics\n├── scripts/                           ← Executable scripts and examples\n│   ├── run_fine_tuning.py             ← Main pipeline execution script\n│   ├── prepare_data_only.py           ← Run just data preparation\n│   ├── evaluate_model.py              ← Run evaluation on existing checkpoint\n│   ├── export_to_gguf.py              ← Convert model to GGUF format\n│   └── examples/                      ← Example usage scripts\n│       ├── fine_tune_llama2.py\n│       ├── fine_tune_code_model.py\n│       └── resume_training.py\n├── tests/                             ← Test suite\n│   ├── __init__.py\n│   ├── unit/                          ← Component unit tests\n│   │   ├── test_data_preparation.py\n│   │   ├── test_lora_config.py\n│   │   ├── test_quantization.py\n│   │   ├── test_training.py\n│   │   └── test_evaluation.py\n│   ├── integration/                   ← End-to-end integration tests\n│   │   ├── test_pipeline_flow.py\n│   │   ├── test_component_integration.py\n│   │   └── test_checkpoint_recovery.py\n│   ├── fixtures/                      ← Test data and mock models\n│   │   ├── sample_training_data.json\n│   │   ├── mock_model_config.json\n│   │   └── test_checkpoints/\n│   └── performance/                   ← Memory and speed benchmarks\n│       ├── test_memory_usage.py\n│       └── test_training_speed.py\n├── docs/                              ← Additional documentation\n│   ├── api_reference.md               ← Detailed API documentation\n│   ├── configuration_guide.md         ← Configuration options explained\n│   ├── troubleshooting.md             ← Common issues and solutions\n│   └── diagrams/                      ← Architecture diagrams\n└── experiments/                       ← Experimental code and research\n    ├── notebook_examples/             ← Jupyter notebooks for exploration\n    ├── alternative_approaches/        ← Alternative implementation attempts\n    └── benchmark_results/             ← Performance measurement results\n```\n\n**Module organization principles** guide the file structure decisions. Each component directory contains all the code needed for that component's responsibilities, minimizing cross-component imports and enabling independent testing. The structure supports **incremental implementation** where learners can implement components one at a time while maintaining a working system.\n\n| Directory | Purpose | Import Policy | Testing Strategy |\n|-----------|---------|---------------|------------------|\n| `config/` | Configuration classes and loading | No imports from other components | Unit tests with mock configs |\n| `data_preparation/` | Dataset processing pipeline | Import only from `config/` and `utils/` | Unit tests + integration with real data |\n| `lora_config/` | Adapter setup and management | Import from `config/`, `utils/`, `quantization/` | Unit tests + model architecture tests |\n| `quantization/` | 4-bit model loading and monitoring | Import from `config/`, `utils/` | Unit tests + memory benchmarking |\n| `training/` | Training loop and optimization | Import from all components | Integration tests + training convergence |\n| `evaluation/` | Model assessment and export | Import from all components | End-to-end tests + quality validation |\n| `utils/` | Shared infrastructure | No component imports | Utility function unit tests |\n\nThe **script organization** in the `scripts/` directory provides both learning examples and production usage patterns. The main `run_fine_tuning.py` script demonstrates the complete pipeline, while component-specific scripts enable testing and debugging individual stages.\n\n**Configuration management** centralizes all hyperparameters and settings in the `config/` directory with YAML files for different model families and use cases. This organization makes it easy to reproduce experiments and share working configurations between team members.\n\n> **Architectural Decision: Filesystem-Based Component Communication**\n> - **Context**: Components need to exchange large objects like datasets and model checkpoints, and the pipeline should support resumption after failures\n> - **Options Considered**: In-memory object passing, shared state objects, filesystem artifacts\n> - **Decision**: Use filesystem artifacts for inter-component communication\n> - **Rationale**: Enables pipeline resumption, simplifies debugging (intermediate results are inspectable), supports distributed processing, and reduces memory pressure\n> - **Consequences**: Slight performance overhead from serialization, but significant gains in robustness and debuggability\n\n### Implementation Guidance\n\nThe implementation follows a progressive complexity approach where each component can be built and tested independently before integration into the full pipeline.\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Configuration Management | YAML + dataclasses | Hydra configuration framework |\n| Dataset Loading | pandas + torch Dataset | HuggingFace datasets library |\n| Model Loading | transformers + bitsandbytes | Custom quantization with GPTQ |\n| Training Loop | HuggingFace Trainer | Custom PyTorch training loop |\n| Logging | Python logging + file output | Weights & Biases integration |\n| Memory Monitoring | torch.cuda.memory_stats | NVIDIA-ML-Py detailed profiling |\n\n**Core Infrastructure Starter Code:**\n\n```python\n# src/config/training_config.py\nfrom dataclasses import dataclass, field\nfrom typing import Optional, List, Dict, Any\nfrom pathlib import Path\nimport yaml\n\n@dataclass\nclass QuantizationConfig:\n    \"\"\"Configuration for 4-bit quantization settings.\"\"\"\n    load_in_4bit: bool = True\n    bnb_4bit_quant_type: str = \"nf4\"\n    bnb_4bit_compute_dtype: str = \"bfloat16\"\n    bnb_4bit_use_double_quant: bool = True\n    \n@dataclass\nclass LoRAConfig:\n    \"\"\"Configuration for LoRA adapter parameters.\"\"\"\n    r: int = 16  # Default rank for low-rank decomposition\n    alpha: int = 32  # Scaling parameter for adapter outputs  \n    dropout: float = 0.1\n    target_modules: Optional[List[str]] = None  # Auto-detect if None\n    bias: str = \"none\"  # Options: \"none\", \"all\", \"lora_only\"\n    task_type: str = \"CAUSAL_LM\"\n\n@dataclass \nclass TrainingConfig:\n    \"\"\"Master configuration object for the entire fine-tuning pipeline.\"\"\"\n    model_name_or_path: str\n    quantization: QuantizationConfig = field(default_factory=QuantizationConfig)\n    lora: LoRAConfig = field(default_factory=LoRAConfig)\n    learning_rate: float = 3e-4\n    num_train_epochs: int = 3\n    per_device_train_batch_size: int = 1\n    gradient_accumulation_steps: int = 8\n    warmup_ratio: float = 0.1\n    logging_steps: int = 10\n    save_steps: int = 100\n    eval_steps: int = 100\n    output_dir: str = \"./output\"\n    run_name: Optional[str] = None\n\ndef load_config_from_yaml(config_path: Path) -> TrainingConfig:\n    \"\"\"Load and validate configuration from YAML file.\"\"\"\n    with open(config_path, 'r') as f:\n        config_dict = yaml.safe_load(f)\n    \n    # Extract nested configuration sections\n    quantization_dict = config_dict.pop('quantization', {})\n    lora_dict = config_dict.pop('lora', {})\n    \n    # Create configuration objects\n    quantization_config = QuantizationConfig(**quantization_dict)\n    lora_config = LoRAConfig(**lora_dict)\n    \n    return TrainingConfig(\n        quantization=quantization_config,\n        lora=lora_config,\n        **config_dict\n    )\n```\n\n```python\n# src/utils/memory_monitor.py\nimport torch\nimport psutil\nfrom typing import Dict, List, Optional\nfrom dataclasses import dataclass, field\n\n@dataclass\nclass MemoryMonitor:\n    \"\"\"Tracks GPU and system memory usage throughout the pipeline.\"\"\"\n    baseline_gpu_memory: Optional[int] = None\n    baseline_system_memory: int = 0\n    measurements: List[Dict] = field(default_factory=list)\n    \n    def capture_baseline(self) -> None:\n        \"\"\"Record initial memory state before model loading.\"\"\"\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            self.baseline_gpu_memory = torch.cuda.memory_allocated()\n        self.baseline_system_memory = psutil.virtual_memory().used\n        \n    def measure_current_usage(self, stage: str) -> Dict[str, float]:\n        \"\"\"Measure current memory and return usage statistics.\"\"\"\n        measurement = {\n            \"stage\": stage,\n            \"timestamp\": time.time()\n        }\n        \n        if torch.cuda.is_available():\n            measurement.update({\n                \"gpu_allocated_mb\": torch.cuda.memory_allocated() / 1024**2,\n                \"gpu_reserved_mb\": torch.cuda.memory_reserved() / 1024**2,\n                \"gpu_free_mb\": (torch.cuda.get_device_properties(0).total_memory - \n                               torch.cuda.memory_reserved()) / 1024**2\n            })\n            \n        measurement[\"system_memory_mb\"] = psutil.virtual_memory().used / 1024**2\n        measurement[\"system_memory_percent\"] = psutil.virtual_memory().percent\n        \n        self.measurements.append(measurement)\n        return measurement\n        \n    def get_peak_usage(self) -> Dict[str, float]:\n        \"\"\"Return peak memory usage across all measurements.\"\"\"\n        if not self.measurements:\n            return {}\n            \n        peak_gpu = max(m.get(\"gpu_allocated_mb\", 0) for m in self.measurements)\n        peak_system = max(m.get(\"system_memory_mb\", 0) for m in self.measurements)\n        \n        return {\n            \"peak_gpu_mb\": peak_gpu,\n            \"peak_system_mb\": peak_system,\n            \"gpu_baseline_mb\": self.baseline_gpu_memory / 1024**2 if self.baseline_gpu_memory else 0,\n            \"system_baseline_mb\": self.baseline_system_memory / 1024**2\n        }\n```\n\n**Core Pipeline Orchestrator Skeleton:**\n\n```python\n# src/fine_tuning_pipeline.py\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Optional, Dict, Any\nimport logging\n\nfrom .config.training_config import TrainingConfig\nfrom .utils.memory_monitor import MemoryMonitor\n\n@dataclass\nclass FineTuningPipeline:\n    \"\"\"Main orchestrator for the end-to-end fine-tuning pipeline.\"\"\"\n    config: TrainingConfig\n    memory_monitor: MemoryMonitor = field(default_factory=MemoryMonitor)\n    tokenizer: Optional = None\n    model: Optional = None\n    \n    def setup_model_and_tokenizer(self) -> None:\n        \"\"\"Load quantized model and configure tokenizer.\"\"\"\n        # TODO 1: Initialize memory monitoring baseline\n        # TODO 2: Load tokenizer from config.model_name_or_path\n        # TODO 3: Apply chat template configuration if available\n        # TODO 4: Load model with quantization configuration\n        # TODO 5: Monitor memory usage after model loading\n        # Hint: Use AutoTokenizer.from_pretrained() and AutoModelForCausalLM.from_pretrained()\n        # Hint: Pass quantization config via BitsAndBytesConfig\n        pass\n        \n    def setup_lora_adapters(self) -> None:\n        \"\"\"Inject LoRA adapters into frozen base model.\"\"\"\n        # TODO 1: Auto-detect target modules if not specified in config\n        # TODO 2: Create PEFT configuration with LoRA parameters\n        # TODO 3: Wrap model with PEFT adapters using get_peft_model()\n        # TODO 4: Verify that base model parameters are frozen\n        # TODO 5: Count and log trainable vs total parameters\n        # TODO 6: Monitor memory usage after adapter injection\n        pass\n        \n    def prepare_datasets(self) -> None:\n        \"\"\"Load and preprocess training data.\"\"\"\n        # TODO 1: Load raw training data from configured sources\n        # TODO 2: Apply chat template formatting for instruction tuning\n        # TODO 3: Tokenize datasets with proper padding and truncation\n        # TODO 4: Split into training and validation sets\n        # TODO 5: Apply quality filtering and length constraints\n        # TODO 6: Cache processed datasets for resumption\n        pass\n        \n    def execute_training(self) -> str:\n        \"\"\"Run training loop and return best checkpoint path.\"\"\"\n        # TODO 1: Set up training arguments with learning rate schedule\n        # TODO 2: Initialize trainer with gradient accumulation configuration\n        # TODO 3: Set up logging and checkpoint saving callbacks\n        # TODO 4: Configure early stopping based on validation loss\n        # TODO 5: Start training loop with memory monitoring\n        # TODO 6: Track training metrics and loss convergence\n        # TODO 7: Return path to best checkpoint based on validation score\n        pass\n        \n    def evaluate_and_merge(self, checkpoint_path: str) -> Dict[str, Any]:\n        \"\"\"Evaluate model and merge adapters.\"\"\"\n        # TODO 1: Load best checkpoint and switch to evaluation mode\n        # TODO 2: Calculate perplexity on validation set\n        # TODO 3: Run task-specific evaluation benchmarks\n        # TODO 4: Merge LoRA adapter weights back into base model\n        # TODO 5: Verify merged model quality matches checkpoint\n        # TODO 6: Export merged model in HuggingFace and GGUF formats\n        # TODO 7: Return comprehensive evaluation metrics\n        pass\n        \n    def run_complete_pipeline(self) -> Dict[str, Any]:\n        \"\"\"Execute full pipeline from data to deployment.\"\"\"\n        try:\n            self.memory_monitor.capture_baseline()\n            \n            # Pipeline stages with progress tracking\n            logging.info(\"Setting up model and tokenizer...\")\n            self.setup_model_and_tokenizer()\n            \n            logging.info(\"Configuring LoRA adapters...\")\n            self.setup_lora_adapters()\n            \n            logging.info(\"Preparing datasets...\")\n            self.prepare_datasets()\n            \n            logging.info(\"Starting training...\")\n            best_checkpoint = self.execute_training()\n            \n            logging.info(\"Evaluating and merging model...\")\n            results = self.evaluate_and_merge(best_checkpoint)\n            \n            # Add memory usage summary to results\n            results[\"memory_usage\"] = self.memory_monitor.get_peak_usage()\n            \n            return results\n            \n        except Exception as e:\n            logging.error(f\"Pipeline failed: {e}\")\n            # Add error context and memory state\n            raise\n```\n\n**Milestone Checkpoint for Architecture Setup:**\nAfter implementing the basic pipeline structure:\n1. Run `python -m pytest tests/unit/test_config.py` - should pass configuration loading tests\n2. Execute `python scripts/run_fine_tuning.py --config config/example_training_config.yaml --dry-run` - should initialize components without training\n3. Check output directory contains properly structured component logs\n4. Verify memory monitoring captures baseline measurements before any model loading\n\n**Language-Specific Implementation Hints:**\n- Use `torch.cuda.is_available()` to detect GPU presence before quantization setup\n- Import `bitsandbytes` only when quantization is enabled to avoid CUDA errors on CPU-only systems\n- Use `transformers.AutoModel.from_pretrained()` with `device_map=\"auto\"` for automatic device placement\n- Enable gradient checkpointing with `model.gradient_checkpointing_enable()` for memory efficiency\n- Use `accelerate` library for automatic mixed precision and device placement in production\n\n\n## Data Model\n\n> **Milestone(s):** All milestones - these data structures form the backbone of the entire fine-tuning pipeline, enabling configuration management, training orchestration, and evaluation tracking across all components\n\nThe data model serves as the contract between all components in our fine-tuning pipeline, defining exactly how configuration parameters, training samples, and evaluation metrics flow through the system. Think of these data structures as the **architectural blueprints** that every component must understand - just as construction workers need detailed blueprints to coordinate their efforts on a building, our pipeline components need precise data schemas to coordinate the complex process of model fine-tuning.\n\n![Data Model and Configuration Schema](./diagrams/data-model-diagram.svg)\n\nThe data model design follows three core principles that make the system maintainable and extensible. First, **configuration immutability** ensures that once training begins, hyperparameters cannot accidentally change mid-process. Second, **type safety** prevents common errors like mixing up learning rates and rank parameters. Third, **progressive disclosure** means simple configurations use sensible defaults while advanced users can override every parameter.\n\n### Training Data Structures\n\nThe training data structures transform raw conversational data into the precise format required by modern instruction-tuned language models. Think of this as the **language tutor's lesson plan** - raw conversations must be converted into structured instruction-response pairs with proper formatting, special tokens, and attention masks that the model can learn from effectively.\n\nModern language models expect training data in a specific conversational format that includes system prompts, user instructions, and assistant responses. The challenge lies in standardizing this format across different data sources while preserving the conversational context that makes fine-tuning effective. Our data structures handle this transformation systematically.\n\n| Structure | Field | Type | Description |\n|-----------|-------|------|-------------|\n| `ConversationSample` | conversation_id | `str` | Unique identifier for tracking multi-turn conversations |\n| | turns | `List[ConversationTurn]` | Ordered sequence of conversation exchanges |\n| | metadata | `Dict[str, Any]` | Source information, quality scores, domain tags |\n| | total_tokens | `Optional[int]` | Token count after tokenization, None if not yet computed |\n| | is_valid | `bool` | Whether sample passes quality filters and length constraints |\n| `ConversationTurn` | role | `str` | Speaker role: \"system\", \"user\", \"assistant\" |\n| | content | `str` | The actual text content of this turn |\n| | turn_index | `int` | Position in conversation sequence, starting from 0 |\n| `InstructionSample` | instruction | `str` | The task description or question posed to the model |\n| | response | `str` | Expected model output or correct answer |\n| | system_prompt | `Optional[str]` | Context or constraints for the instruction |\n| | input_context | `Optional[str]` | Additional context data for the instruction |\n| | sample_id | `str` | Unique identifier for this instruction-response pair |\n| | source | `str` | Origin dataset or generation method |\n| | quality_score | `Optional[float]` | Automated quality assessment score 0-1 |\n| `TokenizedSample` | input_ids | `List[int]` | Token IDs for the complete formatted prompt |\n| | attention_mask | `List[int]` | Binary mask indicating which tokens to attend to |\n| | labels | `List[int]` | Target token IDs for loss computation, -100 for ignored tokens |\n| | prompt_length | `int` | Number of tokens in the instruction portion |\n| | response_length | `int` | Number of tokens in the response portion |\n| | total_length | `int` | Total sequence length including special tokens |\n\nThe conversation-to-instruction transformation follows a systematic process that preserves the natural flow of multi-turn dialogues while creating clear learning objectives. When processing multi-turn conversations, the system identifies natural instruction-response boundaries and creates separate training samples for each exchange, maintaining conversation history as context.\n\nThe tokenization process applies the model's specific chat template, which varies between model families. For example, Llama models use `<s>[INST] instruction [/INST] response </s>` format, while ChatML-formatted models use `<|im_start|>user\\ninstruction<|im_end|>\\n<|im_start|>assistant\\nresponse<|im_end|>`. Our tokenization component automatically detects the appropriate template based on the model configuration.\n\n> **Critical Design Insight**: The `labels` field uses -100 for instruction tokens to exclude them from loss computation. This ensures the model only learns to predict response tokens, not repeat instructions verbatim. This technique, called **causal language modeling with attention masks**, is essential for effective instruction tuning.\n\nQuality filtering operates at multiple levels to ensure training data meets minimum standards. The system removes samples with extremely short responses (less than 10 tokens), excessively long sequences that exceed the model's context window, and conversations with obvious formatting errors or corrupted encoding.\n\n> **Decision: Unified Conversation Format**\n> - **Context**: Different datasets use incompatible conversation formats (ChatML, Alpaca, ShareGPT), making data preparation complex\n> - **Options Considered**: \n>   1. Support each format natively with format-specific loaders\n>   2. Convert everything to a single intermediate format\n>   3. Use HuggingFace's ChatML standard throughout\n> - **Decision**: Convert all data to our unified `ConversationSample` format, then apply model-specific templates during tokenization\n> - **Rationale**: Single format simplifies data processing logic, enables consistent quality filtering, and separates data representation from model-specific formatting\n> - **Consequences**: Requires upfront conversion cost but eliminates format-specific bugs and enables uniform data augmentation\n\n| Approach | Pros | Cons |\n|----------|------|------|\n| Native format support | No conversion overhead, preserves original structure | Complex codebase, format-specific bugs, inconsistent quality control |\n| Unified intermediate format | Clean abstraction, consistent processing, easier testing | Conversion overhead, potential information loss |\n| HuggingFace ChatML standard | Industry standard, broad tool support | Less flexibility for custom formats, external dependency |\n\n### Configuration Objects\n\nConfiguration objects provide type-safe, validated containers for the numerous hyperparameters that control model quantization, LoRA adaptation, and training behavior. Think of these as the **master control panel** for the fine-tuning process - they centralize all the knobs and dials that determine how the training will proceed, with built-in validation to prevent common configuration mistakes.\n\nThe configuration system uses a hierarchical structure where high-level training configuration contains specialized sub-configurations for quantization and LoRA parameters. This design separates concerns while ensuring all components receive consistent parameter values throughout the training process.\n\n| Structure | Field | Type | Description |\n|-----------|-------|------|-------------|\n| `QuantizationConfig` | load_in_4bit | `bool` | Whether to quantize the base model to 4-bit precision |\n| | bnb_4bit_quant_type | `str` | Quantization algorithm: \"nf4\" or \"fp4\" |\n| | bnb_4bit_compute_dtype | `str` | Data type for computations: \"float16\" or \"bfloat16\" |\n| | bnb_4bit_use_double_quant | `bool` | Whether to quantize quantization constants |\n| | bnb_4bit_quant_storage | `str` | Storage format for quantized weights |\n| `LoRAConfig` | r | `int` | Rank of low-rank decomposition matrices |\n| | alpha | `int` | Scaling parameter for adapter weights |\n| | dropout | `float` | Dropout probability in adapter layers |\n| | target_modules | `Optional[List[str]]` | Layer names to inject adapters, None for auto-detection |\n| | bias | `str` | Bias handling: \"none\", \"all\", or \"lora_only\" |\n| | task_type | `str` | Task category: \"CAUSAL_LM\", \"SEQ_2_SEQ_LM\", etc. |\n| | lora_alpha_scaling | `bool` | Whether to apply alpha/r scaling to adapter outputs |\n| | init_lora_weights | `Union[bool, str]` | Weight initialization strategy for adapters |\n| `TrainingConfig` | model_name_or_path | `str` | HuggingFace model identifier or local path |\n| | quantization | `QuantizationConfig` | 4-bit quantization parameters |\n| | lora | `LoRAConfig` | Low-rank adaptation configuration |\n| | learning_rate | `float` | Base learning rate for optimizer |\n| | num_train_epochs | `int` | Number of complete passes through training data |\n| | per_device_train_batch_size | `int` | Training samples per GPU per forward pass |\n| | per_device_eval_batch_size | `int` | Evaluation samples per GPU per forward pass |\n| | gradient_accumulation_steps | `int` | Steps to accumulate before optimizer update |\n| | warmup_steps | `int` | Learning rate warmup duration |\n| | max_seq_length | `int` | Maximum sequence length in tokens |\n| | dataloader_num_workers | `int` | Parallel data loading processes |\n| | fp16 | `bool` | Enable 16-bit mixed precision training |\n| | bf16 | `bool` | Enable bfloat16 mixed precision training |\n| | dataloader_pin_memory | `bool` | Pin dataset tensors in CPU memory |\n| | gradient_checkpointing | `bool` | Trade compute for memory in backpropagation |\n| | logging_steps | `int` | Frequency of training metric logging |\n| | eval_steps | `int` | Frequency of validation evaluation |\n| | save_steps | `int` | Frequency of checkpoint saving |\n| | save_total_limit | `int` | Maximum number of checkpoints to retain |\n| | load_best_model_at_end | `bool` | Load best checkpoint after training completion |\n| | metric_for_best_model | `str` | Metric used for best model selection |\n| | greater_is_better | `bool` | Whether higher metric values indicate better performance |\n| | early_stopping_patience | `int` | Evaluations to wait before stopping if metric doesn't improve |\n| | early_stopping_threshold | `float` | Minimum improvement required to reset patience counter |\n\nThe configuration validation process occurs at object creation time, preventing invalid parameter combinations that would cause training failures hours later. For example, the system validates that `gradient_accumulation_steps * per_device_train_batch_size` creates a reasonable effective batch size, and that `learning_rate` is appropriate for the chosen `lora.r` and `lora.alpha` combination.\n\nQuantization configuration requires careful coordination between storage format and compute precision. The `bnb_4bit_compute_dtype` determines the precision used during forward and backward passes, while quantized weights remain in 4-bit format. The system validates that the chosen compute dtype is compatible with the available hardware and training precision settings.\n\n> **Decision: Hierarchical Configuration Structure**\n> - **Context**: Fine-tuning involves dozens of hyperparameters across quantization, LoRA, and training domains\n> - **Options Considered**:\n>   1. Flat configuration with all parameters in one object\n>   2. Hierarchical configuration with specialized sub-objects\n>   3. Multiple independent configuration files\n> - **Decision**: Hierarchical structure with `TrainingConfig` containing `QuantizationConfig` and `LoRAConfig`\n> - **Rationale**: Groups related parameters logically, enables component-specific validation, allows independent testing of configuration subsystems\n> - **Consequences**: Slightly more complex object construction but dramatically clearer parameter organization and better error messages\n\n| Approach | Pros | Cons |\n|----------|------|------|\n| Flat configuration | Simple structure, easy serialization | Parameter namespace pollution, unclear relationships |\n| Hierarchical configuration | Logical grouping, component isolation, clear validation | More complex construction, nested access patterns |\n| Multiple configuration files | Complete separation, independent versioning | File coordination complexity, inconsistency risks |\n\nLoRA configuration requires balancing adaptation capacity against memory efficiency. The rank `r` determines how many parameters the adapter can learn - higher ranks enable more complex adaptations but consume more memory and may lead to overfitting on small datasets. The alpha parameter `alpha` controls how much the adapter outputs influence the base model, with the effective scaling being `alpha / r`.\n\nDefault configuration values are chosen based on empirical results across diverse fine-tuning tasks. `DEFAULT_LORA_RANK = 16` provides good adaptation capacity for most instruction-following tasks, while `DEFAULT_LORA_ALPHA = 32` creates a 2x scaling that prevents adapter outputs from being overwhelmed by pre-trained weights.\n\n### Evaluation and Logging\n\nThe evaluation and logging data structures capture training progress, model performance, and system resource utilization throughout the fine-tuning process. Think of this as the **mission control dashboard** for training - these structures provide real-time visibility into whether the training is proceeding successfully and enable data-driven decisions about when to stop, adjust hyperparameters, or investigate problems.\n\n![Evaluation Workflow](./diagrams/evaluation-workflow.svg)\n\nComprehensive logging serves multiple critical functions: tracking convergence for early stopping decisions, diagnosing training instabilities, measuring resource utilization for cost optimization, and providing reproducible records for experiment comparison. The logging system balances detail with performance, avoiding expensive computations during training while capturing sufficient information for post-training analysis.\n\n| Structure | Field | Type | Description |\n|-----------|-------|------|-------------|\n| `TrainingMetrics` | step | `int` | Global training step number |\n| | epoch | `float` | Current epoch progress (e.g., 1.5 for halfway through epoch 2) |\n| | loss | `float` | Training loss for current batch |\n| | learning_rate | `float` | Current learning rate after scheduling |\n| | grad_norm | `Optional[float]` | Gradient norm before clipping, None if not computed |\n| | timestamp | `float` | Unix timestamp when metrics were recorded |\n| | gpu_memory_used | `Optional[float]` | GPU memory usage in GB |\n| | samples_per_second | `float` | Training throughput metric |\n| `EvaluationMetrics` | eval_step | `int` | Evaluation step number |\n| | eval_loss | `float` | Average loss on validation set |\n| | perplexity | `float` | Validation perplexity computed from eval_loss |\n| | bleu_score | `Optional[float]` | BLEU score for generation tasks |\n| | rouge_scores | `Optional[Dict[str, float]]` | ROUGE-1, ROUGE-2, ROUGE-L scores |\n| | exact_match | `Optional[float]` | Exact string match accuracy |\n| | eval_samples | `int` | Number of samples in evaluation set |\n| | eval_runtime | `float` | Time spent on this evaluation in seconds |\n| | eval_samples_per_second | `float` | Evaluation throughput |\n| | timestamp | `float` | Unix timestamp of evaluation completion |\n| `CheckpointMetadata` | checkpoint_path | `str` | File system path to saved checkpoint |\n| | step | `int` | Training step when checkpoint was created |\n| | epoch | `float` | Training epoch when checkpoint was created |\n| | eval_loss | `Optional[float]` | Validation loss at checkpoint time, None if not evaluated |\n| | is_best | `bool` | Whether this is the best checkpoint so far by the chosen metric |\n| | model_config | `Dict[str, Any]` | Serialized model and training configuration |\n| | adapter_config | `Dict[str, Any]` | LoRA adapter configuration and state |\n| | optimizer_state_size | `int` | Size of optimizer state in bytes |\n| | save_timestamp | `float` | Unix timestamp when checkpoint was saved |\n| `MemoryMetrics` | stage | `str` | Training stage: \"baseline\", \"model_loaded\", \"training\", \"evaluation\" |\n| | gpu_memory_allocated | `float` | GPU memory allocated by PyTorch in GB |\n| | gpu_memory_cached | `float` | GPU memory cached by PyTorch in GB |\n| | gpu_memory_reserved | `float` | Total GPU memory reserved by PyTorch in GB |\n| | system_memory_used | `float` | System RAM usage in GB |\n| | timestamp | `float` | Unix timestamp of memory measurement |\n| | details | `Optional[Dict[str, Any]]` | Additional stage-specific memory details |\n\nThe training metrics collection occurs at configurable intervals to balance monitoring granularity with training performance. High-frequency metrics like loss and learning rate are logged every few steps, while expensive metrics like gradient norms are computed less frequently. The system automatically adjusts logging frequency based on training speed to maintain roughly constant monitoring overhead.\n\nEvaluation metrics provide task-specific performance measurements that guide training decisions. Perplexity measures how well the model predicts validation text and correlates strongly with general language modeling capability. Task-specific metrics like BLEU and ROUGE are computed when the validation set includes reference outputs for comparison.\n\n> **Critical Design Insight**: Memory metrics collection uses PyTorch's built-in memory profiling rather than system-level tools because GPU memory allocation is more complex than simple usage monitoring. PyTorch's memory manager pools and caches memory, so system-level measurements can be misleading.\n\nThe checkpoint metadata system tracks not just model weights but the complete training context needed for reproducible resumption. This includes optimizer states, random number generator seeds, and data loader positions. The metadata enables intelligent checkpoint management, automatically cleaning up inferior checkpoints while preserving the best models.\n\n> **Decision: Structured Logging with Typed Metrics**\n> - **Context**: Training generates thousands of metric data points that need efficient storage and analysis\n> - **Options Considered**:\n>   1. Unstructured logging with print statements and manual parsing\n>   2. Structured logging with typed metric objects\n>   3. External monitoring systems like WandB with custom schemas\n> - **Decision**: Typed metric structures with optional external system integration\n> - **Rationale**: Type safety prevents metric recording bugs, structured format enables easy analysis, local storage ensures data availability regardless of external services\n> - **Consequences**: Requires more upfront design but provides reliable, analyzable training records and easier debugging\n\n| Approach | Pros | Cons |\n|----------|------|------|\n| Unstructured logging | Simple implementation, no dependencies | Difficult analysis, error-prone parsing, no validation |\n| Structured typed metrics | Type safety, easy analysis, reliable storage | More complex design, additional abstractions |\n| External monitoring only | Rich visualization, cloud storage, collaboration features | External dependency, data lock-in, network requirements |\n\nMemory monitoring operates continuously throughout training to detect memory leaks and optimization opportunities. The system establishes a baseline measurement before model loading, then tracks memory growth during each training phase. Unexpected memory growth patterns often indicate bugs in data loading, gradient accumulation, or checkpoint handling.\n\n### Common Pitfalls\n\n⚠️ **Pitfall: Configuration Parameter Type Mismatches**\nDevelopers frequently confuse integer and float parameters, especially for learning rates and LoRA parameters. For example, setting `learning_rate = 5` instead of `learning_rate = 5e-5` creates an absurdly high learning rate that causes immediate training collapse. Similarly, confusing LoRA rank and alpha parameters leads to either ineffective adaptation or unstable training. The solution is comprehensive configuration validation that checks parameter types, ranges, and relationships before training begins.\n\n⚠️ **Pitfall: Inconsistent Token Counting**\nToken length calculations often differ between data preparation and actual training due to tokenizer special tokens, chat template formatting, and padding strategies. This leads to samples that pass length filtering but cause out-of-memory errors during training, or samples that are unnecessarily truncated. The solution is to perform tokenization during data preparation using the exact same tokenizer configuration and special tokens that will be used during training.\n\n⚠️ **Pitfall: Missing Evaluation Baseline Comparisons**\nTraining metrics show improvement over time, but without baseline comparisons against the unmodified base model, it's impossible to determine whether fine-tuning actually improved task performance. The solution is to evaluate the base model on the validation set before training begins, storing these baseline metrics alongside training results for direct comparison.\n\n⚠️ **Pitfall: Inadequate Memory Monitoring Granularity**\nMonitoring only peak memory usage misses important memory allocation patterns that cause training failures. For example, gradient accumulation may work fine for several steps then suddenly cause out-of-memory errors when the optimizer state updates. The solution is to monitor memory at each phase transition (data loading, forward pass, backward pass, optimizer step) to identify exactly where memory problems occur.\n\n⚠️ **Pitfall: Configuration Serialization Incompatibilities**\nSaving configurations to YAML or JSON often fails when the configuration contains complex objects like tokenizers or custom data types. Additionally, loading configurations from files may not preserve exact object types, leading to subtle runtime errors. The solution is to implement proper serialization methods that handle all configuration object types and validate deserialized configurations match the original types and value ranges.\n\n### Implementation Guidance\n\nThe data model implementation focuses on creating robust, validated data structures that prevent common configuration errors and provide clear debugging information when problems occur. The implementation balances simplicity for basic use cases with extensibility for advanced configurations.\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Configuration Management | dataclasses with field validation | Pydantic models with advanced validation |\n| Serialization | JSON with custom encoders | YAML with schema validation |\n| Validation | Manual type checking | Marshmallow schemas with custom validators |\n| Logging Backend | Python logging with structured formatters | structlog with context preservation |\n\n**Recommended File Structure:**\n```\nsrc/fine_tuning_pipeline/\n  config/\n    __init__.py\n    training_config.py       ← TrainingConfig and sub-configurations\n    data_structures.py       ← Training data structures\n    validation.py           ← Configuration validation functions\n  metrics/\n    __init__.py\n    training_metrics.py     ← TrainingMetrics and logging\n    evaluation_metrics.py   ← EvaluationMetrics and benchmark results\n    memory_monitor.py       ← MemoryMetrics and monitoring utilities\n  utils/\n    serialization.py        ← Configuration save/load utilities\n    type_helpers.py         ← Type validation and conversion utilities\n```\n\n**Configuration Management Infrastructure (Complete):**\n\n```python\n\"\"\"\nComplete configuration management system with validation and serialization.\nThis provides the foundation for all pipeline components.\n\"\"\"\n\nfrom dataclasses import dataclass, field, asdict\nfrom typing import Optional, List, Dict, Any, Union\nfrom pathlib import Path\nimport json\nimport yaml\nfrom enum import Enum\n\nclass QuantizationType(Enum):\n    NF4 = \"nf4\"\n    FP4 = \"fp4\"\n\nclass ComputeDType(Enum):\n    FLOAT16 = \"float16\"\n    BFLOAT16 = \"bfloat16\"\n    FLOAT32 = \"float32\"\n\n@dataclass\nclass QuantizationConfig:\n    \"\"\"4-bit quantization configuration with validation.\"\"\"\n    load_in_4bit: bool = True\n    bnb_4bit_quant_type: str = \"nf4\"\n    bnb_4bit_compute_dtype: str = \"float16\"\n    bnb_4bit_use_double_quant: bool = True\n    bnb_4bit_quant_storage: str = \"uint8\"\n    \n    def __post_init__(self):\n        \"\"\"Validate quantization configuration after creation.\"\"\"\n        valid_quant_types = {\"nf4\", \"fp4\"}\n        if self.bnb_4bit_quant_type not in valid_quant_types:\n            raise ValueError(f\"Invalid quant_type: {self.bnb_4bit_quant_type}. Must be one of {valid_quant_types}\")\n        \n        valid_compute_dtypes = {\"float16\", \"bfloat16\", \"float32\"}\n        if self.bnb_4bit_compute_dtype not in valid_compute_dtypes:\n            raise ValueError(f\"Invalid compute_dtype: {self.bnb_4bit_compute_dtype}. Must be one of {valid_compute_dtypes}\")\n\n@dataclass  \nclass LoRAConfig:\n    \"\"\"LoRA adapter configuration with parameter validation.\"\"\"\n    r: int = 16\n    alpha: int = 32\n    dropout: float = 0.1\n    target_modules: Optional[List[str]] = None\n    bias: str = \"none\"\n    task_type: str = \"CAUSAL_LM\"\n    lora_alpha_scaling: bool = True\n    init_lora_weights: Union[bool, str] = True\n    \n    def __post_init__(self):\n        \"\"\"Validate LoRA configuration parameters.\"\"\"\n        if self.r <= 0 or self.r > 512:\n            raise ValueError(f\"LoRA rank must be between 1 and 512, got {self.r}\")\n        \n        if self.alpha <= 0:\n            raise ValueError(f\"LoRA alpha must be positive, got {self.alpha}\")\n            \n        if not 0.0 <= self.dropout <= 1.0:\n            raise ValueError(f\"Dropout must be between 0 and 1, got {self.dropout}\")\n            \n        valid_bias_options = {\"none\", \"all\", \"lora_only\"}\n        if self.bias not in valid_bias_options:\n            raise ValueError(f\"Invalid bias option: {self.bias}. Must be one of {valid_bias_options}\")\n    \n    @property\n    def effective_alpha(self) -> float:\n        \"\"\"Calculate effective alpha scaling factor.\"\"\"\n        return self.alpha / self.r if self.lora_alpha_scaling else self.alpha\n\ndef load_config_from_yaml(config_path: Path) -> \"TrainingConfig\":\n    \"\"\"Load and validate training configuration from YAML file.\"\"\"\n    with open(config_path, 'r') as f:\n        config_dict = yaml.safe_load(f)\n    \n    # Extract nested configurations\n    quantization_dict = config_dict.pop('quantization', {})\n    lora_dict = config_dict.pop('lora', {})\n    \n    # Create configuration objects\n    quantization_config = QuantizationConfig(**quantization_dict)\n    lora_config = LoRAConfig(**lora_dict)\n    \n    # Create main configuration\n    training_config = TrainingConfig(\n        quantization=quantization_config,\n        lora=lora_config,\n        **config_dict\n    )\n    \n    return training_config\n\ndef save_config_to_yaml(config: \"TrainingConfig\", config_path: Path) -> None:\n    \"\"\"Save training configuration to YAML file with proper structure.\"\"\"\n    config_dict = asdict(config)\n    with open(config_path, 'w') as f:\n        yaml.dump(config_dict, f, default_flow_style=False, indent=2)\n```\n\n**Core Training Configuration Structure (Skeleton):**\n\n```python\n@dataclass\nclass TrainingConfig:\n    \"\"\"Complete training configuration with validation and defaults.\"\"\"\n    \n    # Model configuration\n    model_name_or_path: str\n    quantization: QuantizationConfig = field(default_factory=QuantizationConfig)\n    lora: LoRAConfig = field(default_factory=LoRAConfig)\n    \n    # Training hyperparameters  \n    learning_rate: float = 5e-5\n    num_train_epochs: int = 3\n    per_device_train_batch_size: int = 1\n    per_device_eval_batch_size: int = 2\n    gradient_accumulation_steps: int = 4\n    warmup_steps: int = 100\n    max_seq_length: int = 2048\n    \n    # System configuration\n    dataloader_num_workers: int = 0\n    fp16: bool = False\n    bf16: bool = True\n    dataloader_pin_memory: bool = True\n    gradient_checkpointing: bool = True\n    \n    # Logging and evaluation\n    logging_steps: int = 10\n    eval_steps: int = 500\n    save_steps: int = 500\n    save_total_limit: int = 3\n    load_best_model_at_end: bool = True\n    metric_for_best_model: str = \"eval_loss\"\n    greater_is_better: bool = False\n    \n    # Early stopping\n    early_stopping_patience: int = 3\n    early_stopping_threshold: float = 0.001\n    \n    def __post_init__(self):\n        \"\"\"Validate training configuration after initialization.\"\"\"\n        # TODO 1: Validate learning rate is reasonable (1e-6 to 1e-2)\n        # TODO 2: Check that effective batch size is reasonable (grad_accum * batch_size >= 8)  \n        # TODO 3: Verify sequence length doesn't exceed model's max position embeddings\n        # TODO 4: Validate early stopping patience is positive\n        # TODO 5: Check that save_steps is multiple of eval_steps for consistent checkpointing\n        # Hint: Use self.quantization and self.lora to access sub-configurations\n        pass\n    \n    @property\n    def effective_batch_size(self) -> int:\n        \"\"\"Calculate effective training batch size accounting for gradient accumulation.\"\"\"\n        # TODO: Return per_device_train_batch_size * gradient_accumulation_steps\n        pass\n```\n\n**Memory Monitoring Infrastructure (Complete):**\n\n```python\n\"\"\"\nComplete memory monitoring system for tracking GPU and system memory usage.\nThis provides real-time visibility into memory consumption patterns.\n\"\"\"\n\nimport torch\nimport psutil\nimport time\nfrom dataclasses import dataclass\nfrom typing import Optional, List, Dict, Any\n\n@dataclass\nclass MemoryMetrics:\n    \"\"\"Memory usage metrics at a specific point in time.\"\"\"\n    stage: str\n    gpu_memory_allocated: float\n    gpu_memory_cached: float  \n    gpu_memory_reserved: float\n    system_memory_used: float\n    timestamp: float\n    details: Optional[Dict[str, Any]] = None\n\nclass MemoryMonitor:\n    \"\"\"Comprehensive memory monitoring for training pipeline.\"\"\"\n    \n    def __init__(self):\n        self.baseline_gpu_memory: Optional[int] = None\n        self.baseline_system_memory: int = 0\n        self.measurements: List[Dict] = []\n        \n        # Check if CUDA is available\n        self.cuda_available = torch.cuda.is_available()\n        if self.cuda_available:\n            torch.cuda.empty_cache()\n    \n    def capture_baseline(self) -> None:\n        \"\"\"Record initial memory state before model loading.\"\"\"\n        if self.cuda_available:\n            torch.cuda.empty_cache()\n            torch.cuda.synchronize()\n            self.baseline_gpu_memory = torch.cuda.memory_allocated()\n        \n        self.baseline_system_memory = psutil.virtual_memory().used\n        \n        baseline_metrics = self.measure_current_usage(\"baseline\")\n        self.measurements.append(baseline_metrics)\n    \n    def measure_current_usage(self, stage: str) -> Dict[str, float]:\n        \"\"\"Measure current memory usage and return statistics.\"\"\"\n        timestamp = time.time()\n        \n        # GPU memory measurement\n        gpu_allocated = 0.0\n        gpu_cached = 0.0  \n        gpu_reserved = 0.0\n        \n        if self.cuda_available:\n            torch.cuda.synchronize()  # Ensure all operations complete\n            gpu_allocated = torch.cuda.memory_allocated() / (1024**3)  # GB\n            gpu_cached = torch.cuda.memory_cached() / (1024**3)\n            gpu_reserved = torch.cuda.memory_reserved() / (1024**3)\n        \n        # System memory measurement\n        system_memory = psutil.virtual_memory().used / (1024**3)  # GB\n        \n        metrics = {\n            \"stage\": stage,\n            \"gpu_memory_allocated\": gpu_allocated,\n            \"gpu_memory_cached\": gpu_cached,\n            \"gpu_memory_reserved\": gpu_reserved, \n            \"system_memory_used\": system_memory,\n            \"timestamp\": timestamp\n        }\n        \n        self.measurements.append(metrics)\n        return metrics\n    \n    def get_peak_usage(self) -> Dict[str, float]:\n        \"\"\"Return peak memory usage across all measurements.\"\"\"\n        if not self.measurements:\n            return {\"error\": \"No measurements recorded\"}\n            \n        peak_gpu = max(m[\"gpu_memory_allocated\"] for m in self.measurements)\n        peak_system = max(m[\"system_memory_used\"] for m in self.measurements)\n        \n        return {\n            \"peak_gpu_memory_gb\": peak_gpu,\n            \"peak_system_memory_gb\": peak_system,\n            \"measurement_count\": len(self.measurements)\n        }\n```\n\n**Milestone Checkpoint:**\nAfter implementing the data model structures, verify the following behavior:\n\n1. **Configuration Validation**: Run `python -c \"from config import TrainingConfig; TrainingConfig(model_name_or_path='test')\"` - should create valid config with defaults\n2. **Memory Monitoring**: Create MemoryMonitor, call `capture_baseline()`, load a small model, call `measure_current_usage('model_loaded')` - should show memory increase\n3. **YAML Serialization**: Save config to YAML, load it back, verify all parameters match exactly\n4. **Invalid Configuration Handling**: Try creating LoRAConfig with negative rank - should raise ValueError with clear message\n\nExpected output: Configuration objects created successfully, memory measurements show realistic GPU/system usage, YAML round-trip preserves all values, validation catches parameter errors before training begins.\n\n\n## Dataset Preparation Component\n\n> **Milestone(s):** Milestone 1 - this section implements the data loading, validation, formatting, and preprocessing capabilities that transform raw training data into tokenized instruction-response pairs ready for fine-tuning\n\nThe dataset preparation component serves as the foundation of the entire fine-tuning pipeline, transforming heterogeneous raw data into carefully structured, tokenized samples that enable effective parameter-efficient fine-tuning. This component must handle the complexities of modern conversational AI training data while ensuring compatibility with the specific chat templates and tokenization requirements of different language models.\n\n### Mental Model: The Language Tutor\n\nThink of the dataset preparation component as **The Language Tutor** - an experienced teacher who takes raw educational materials from various sources and transforms them into structured lesson plans optimized for a specific student's learning style.\n\nJust as a skilled tutor would:\n- **Collect diverse materials** from textbooks, articles, and conversations, then standardize them into a consistent format\n- **Apply pedagogical templates** that match the student's preferred learning style (visual, auditory, kinesthetic)\n- **Break complex lessons** into appropriately-sized chunks that fit the student's attention span\n- **Create practice sets** by splitting materials into learning exercises and assessment questions\n- **Filter out distractions** by removing low-quality or irrelevant content that might confuse the learning process\n\nThe dataset preparation component performs analogous transformations:\n- **Ingests heterogeneous data** from JSON, JSONL, CSV, and Parquet sources into a unified schema\n- **Applies chat templates** that match the target model's expected conversation format\n- **Tokenizes and chunks** text into sequences that fit within the model's context length limits\n- **Creates train-validation splits** that enable learning assessment without data leakage\n- **Filters for quality** by removing duplicates, malformed samples, and content that exceeds token limits\n\nThis mental model emphasizes that dataset preparation is not merely a mechanical transformation, but a thoughtful curation process that directly impacts the quality and effectiveness of the fine-tuning process.\n\n![Data Preparation Pipeline](./diagrams/data-preparation-flow.svg)\n\n### Data Ingestion and Validation\n\nThe data ingestion subsystem provides the entry point for raw training data, supporting multiple file formats while ensuring data quality and consistency. This subsystem must handle the reality that training data often comes from diverse sources with varying schemas and quality levels.\n\nThe ingestion process begins with **format detection and loading**, where the system automatically identifies file types and applies appropriate parsing strategies. Different formats require different handling approaches due to their structural characteristics and common usage patterns in the machine learning community.\n\n| Format | Typical Use Case | Parsing Strategy | Memory Consideration |\n|--------|------------------|------------------|----------------------|\n| JSON | Small to medium datasets with complex nested structures | Load entire file into memory, parse as single object | Suitable for files under 1GB |\n| JSONL | Large datasets with simple record structure | Stream line-by-line parsing | Memory-efficient for multi-GB files |\n| CSV | Tabular data from databases or spreadsheets | Pandas or streaming CSV parser | Moderate memory usage with chunking |\n| Parquet | Large-scale datasets with schema enforcement | Columnar reading with PyArrow | Most memory-efficient for large datasets |\n\nThe **data validation layer** ensures that ingested records contain the required fields for instruction tuning. The validation process operates on a progressive strictness model, where certain fields are mandatory while others provide optional enhancement to the training process.\n\n| Field Category | Required Fields | Optional Fields | Validation Rules |\n|----------------|-----------------|-----------------|------------------|\n| Core Content | `instruction`, `response` | `input_context`, `system_prompt` | Non-empty strings, reasonable length limits |\n| Metadata | `sample_id` | `source`, `quality_score`, `conversation_id` | Unique IDs, valid score ranges |\n| Quality Indicators | None | `length_tokens`, `language`, `difficulty` | Positive integers, ISO language codes |\n\n**Quality filtering** removes samples that could negatively impact training effectiveness. The filtering pipeline applies multiple criteria to identify problematic content:\n\n1. **Duplicate detection** using content hashing to identify exact and near-duplicate instruction-response pairs\n2. **Length validation** ensuring samples fall within reasonable token count ranges for the target model\n3. **Content quality assessment** checking for obvious spam, garbled text, or inappropriate content\n4. **Language consistency** verifying that instruction and response use the same primary language\n5. **Completeness validation** ensuring responses are not truncated or obviously incomplete\n\nThe validation pipeline maintains detailed statistics about the filtering process, enabling users to understand how much data was removed and for what reasons.\n\n> **Key Design Insight**: The validation pipeline is designed to be conservative - it errs on the side of removing questionable samples rather than allowing low-quality data to pollute the training process. High-quality training data is far more valuable than large quantities of mediocre data.\n\n**Architecture Decision: Format-Agnostic Internal Representation**\n\n> **Decision: Standardize on InstructionSample for Internal Processing**\n> - **Context**: Raw data comes in many formats with different field names and structures, but downstream components need consistent interfaces\n> - **Options Considered**: \n>   1. Pass format-specific dictionaries and handle conversions in each component\n>   2. Create format-specific classes for each input type\n>   3. Standardize on a single `InstructionSample` structure with optional fields\n> - **Decision**: Use `InstructionSample` as the canonical internal representation\n> - **Rationale**: This provides type safety, clear documentation of expected fields, and simplifies downstream component interfaces while maintaining flexibility for optional fields\n> - **Consequences**: Requires upfront conversion cost but eliminates format-handling complexity from all downstream components\n\n| InstructionSample Field | Type | Purpose | Validation |\n|------------------------|------|---------|------------|\n| `instruction` | str | The task or question being asked | Non-empty, reasonable length |\n| `response` | str | The expected response or answer | Non-empty, reasonable length |\n| `system_prompt` | Optional[str] | Context or role definition | Optional but validated if present |\n| `input_context` | Optional[str] | Additional context for the instruction | Optional supporting information |\n| `sample_id` | str | Unique identifier for the sample | Must be unique within dataset |\n| `source` | str | Origin of the data for tracking and filtering | Helps with data provenance |\n| `quality_score` | Optional[float] | Automated or manual quality assessment | Range 0.0 to 1.0 if provided |\n\n### Chat Template Application\n\nThe chat template application subsystem transforms instruction-response pairs into the specific conversational format expected by the target language model. This transformation is critical because different model families use incompatible chat formats that affect both training convergence and inference quality.\n\nModern language models are trained with specific **chat templates** that define how multi-turn conversations are formatted with special tokens. These templates serve as the \"grammar\" for how the model expects to see conversational data structured.\n\n**Chat Template Concepts and Variations**\n\nDifferent model families implement distinct approaches to conversation formatting:\n\n| Model Family | Template Style | Special Tokens | Turn Separation |\n|--------------|----------------|----------------|-----------------|\n| Llama-2-Chat | XML-style tags | `<s>`, `</s>`, `[INST]`, `[/INST]` | Tagged instruction blocks |\n| ChatML (GPT-4) | Role-based | `<|im_start|>`, `<|im_end|>` | Role prefix with start/end tokens |\n| Alpaca | Instruction format | Standard tokens | Fixed template with placeholders |\n| Vicuna | Conversation style | Standard tokens | USER/ASSISTANT prefixes |\n\nThe template application process must handle several complexities:\n\n1. **System message integration** - Incorporating system prompts that define the model's role or behavior guidelines\n2. **Multi-turn conversation handling** - Managing instruction-response pairs that are part of longer conversations\n3. **Token efficiency** - Minimizing template overhead to maximize content within context limits\n4. **Special token placement** - Ensuring proper positioning of begin/end markers for training effectiveness\n\n**Template Application Algorithm**\n\nThe chat template application follows a systematic process to ensure consistency and correctness:\n\n1. **Identify the target model's template format** from the tokenizer configuration or explicit specification\n2. **Extract conversation components** including system prompt, instruction, and response from the `InstructionSample`\n3. **Construct the conversation structure** by organizing components into the model's expected role sequence\n4. **Apply template formatting** using the model-specific special tokens and delimiters\n5. **Validate template correctness** ensuring proper token pairing and expected sequence structure\n6. **Generate final formatted text** ready for tokenization with appropriate special token placement\n\n**Architecture Decision: Template Auto-Detection vs Explicit Configuration**\n\n> **Decision: Use Auto-Detection with Manual Override Capability**\n> - **Context**: Different models require different chat templates, and users may work with custom models or want to experiment with template variations\n> - **Options Considered**:\n>   1. Require explicit template specification for every model\n>   2. Auto-detect from tokenizer configuration only\n>   3. Auto-detect with manual override capability\n> - **Decision**: Auto-detect from the tokenizer's `chat_template` attribute with option to provide custom templates\n> - **Rationale**: Reduces configuration burden for standard models while maintaining flexibility for custom scenarios\n> - **Consequences**: Requires robust template detection logic and clear error messages when auto-detection fails\n\n### Tokenization and Length Handling\n\nThe tokenization subsystem converts formatted text into numerical token sequences that the model can process during training. This conversion must handle the complexities of subword tokenization while respecting length constraints and maintaining proper attention masking.\n\n**Tokenization Process and Considerations**\n\nModern language models use **subword tokenization** schemes like Byte-Pair Encoding (BPE) or SentencePiece that split text into smaller units than whole words. This approach enables the model to handle out-of-vocabulary words and improves computational efficiency.\n\nThe tokenization process for instruction tuning requires special handling to distinguish between input (instruction) and target (response) portions of the sequence:\n\n1. **Separate tokenization** of instruction and response components to track boundaries\n2. **Label masking** where instruction tokens receive -100 labels to exclude them from loss calculation\n3. **Attention mask generation** ensuring the model attends to all relevant tokens\n4. **Special token insertion** for begin-of-sequence, end-of-sequence, and padding tokens\n5. **Length validation** ensuring the complete sequence fits within model context limits\n\n| Tokenization Component | Purpose | Implementation Considerations |\n|------------------------|---------|------------------------------|\n| Input Tokenization | Convert instruction text to token IDs | Include chat template tokens, track boundary |\n| Response Tokenization | Convert response text to token IDs | Mark as training targets, add EOS token |\n| Label Generation | Create loss calculation targets | Set instruction tokens to -100, response tokens to their IDs |\n| Attention Masking | Define which tokens to attend to | Usually 1 for all non-padding tokens |\n| Length Management | Handle sequences exceeding context limit | Truncation strategies and boundary preservation |\n\n**Length Handling Strategies**\n\nWhen tokenized sequences exceed the model's context length, the system must apply truncation strategies that preserve training effectiveness:\n\n| Strategy | Application | Pros | Cons |\n|----------|-------------|------|------|\n| Truncate Response | Cut response at context limit | Preserves complete instruction | May lose important response content |\n| Truncate Instruction | Cut instruction, preserve response | Maintains complete response for learning | May lose essential context |\n| Proportional Truncation | Reduce both instruction and response | Balanced approach | Both components may lose information |\n| Sample Rejection | Remove samples exceeding limits | Maintains data quality | Reduces dataset size |\n\nThe length handling pipeline maintains statistics about truncation decisions to help users understand the impact on their dataset.\n\n**Architecture Decision: Token-Level vs Character-Level Length Limits**\n\n> **Decision: Use Token-Level Limits with Character-Level Pre-filtering**\n> - **Context**: Model context limits are defined in tokens, but token counting requires tokenization which is computationally expensive\n> - **Options Considered**:\n>   1. Use character-level approximations only\n>   2. Tokenize everything for exact token counts\n>   3. Character-level pre-filtering followed by exact token counting\n> - **Decision**: Apply character-level filtering (assuming ~4 chars per token) followed by exact tokenization for remaining samples\n> - **Rationale**: Eliminates obviously oversized samples efficiently while ensuring accuracy for borderline cases\n> - **Consequences**: Requires maintaining both character and token thresholds but significantly reduces tokenization overhead\n\n| TokenizedSample Field | Type | Purpose | Generation Method |\n|----------------------|------|---------|-------------------|\n| `input_ids` | List[int] | Token sequence for model input | Concatenated instruction + response tokens |\n| `attention_mask` | List[int] | Attention pattern definition | 1 for content tokens, 0 for padding |\n| `labels` | List[int] | Training targets for loss calculation | -100 for instruction, token IDs for response |\n| `prompt_length` | int | Number of tokens in instruction portion | Used for label masking and analysis |\n| `response_length` | int | Number of tokens in response portion | Training target length tracking |\n| `total_length` | int | Complete sequence length | Context limit validation |\n\n### Train-Validation Splitting\n\nThe train-validation splitting subsystem partitions the processed dataset into separate training and evaluation sets while avoiding data leakage and ensuring representative distributions. This split is crucial for monitoring training progress and preventing overfitting during fine-tuning.\n\n**Splitting Strategy Considerations**\n\nThe splitting process must balance several competing objectives:\n\n1. **Statistical representativeness** - Validation set should reflect the same distribution as training data\n2. **Temporal consistency** - For time-series data, validation should not contain future information\n3. **Source diversity** - Both sets should contain samples from similar data sources\n4. **Size optimization** - Validation set should be large enough for reliable evaluation but not waste training data\n\n**Standard Splitting Approaches**\n\n| Splitting Method | Use Case | Implementation | Pros | Cons |\n|------------------|----------|----------------|------|------|\n| Random Split | General purpose, mixed-source data | Shuffle and partition by percentage | Simple, unbiased | May not preserve temporal order |\n| Stratified Split | Maintaining category proportions | Split within each category/source | Preserves distributions | Requires meaningful stratification keys |\n| Temporal Split | Time-ordered data | Split at time boundary | Realistic evaluation | May introduce distribution shift |\n| Source-Aware Split | Multi-source datasets | Ensure sources in both sets | Balanced source representation | Complex implementation |\n\n**Stratification Implementation**\n\nFor instruction tuning datasets, stratification can be applied across multiple dimensions to ensure representative splits:\n\n| Stratification Key | Purpose | Implementation Method |\n|-------------------|---------|----------------------|\n| Data Source | Maintain source diversity | Group by `source` field, split proportionally |\n| Instruction Type | Preserve task variety | Classify instruction types, balance across splits |\n| Response Length | Maintain length distribution | Bin by token count, split within bins |\n| Quality Score | Preserve quality distribution | Bin by quality score, ensure balanced representation |\n\n**Architecture Decision: Fixed vs Dynamic Validation Set**\n\n> **Decision: Support Both Fixed and Dynamic Validation Sets**\n> - **Context**: Some users want consistent validation sets for reproducibility, others want fresh validation data for each experiment\n> - **Options Considered**:\n>   1. Always create fresh random splits\n>   2. Always use fixed, predetermined validation sets\n>   3. Support both modes with configuration option\n> - **Decision**: Default to seeded random splits for reproducibility, with option to provide pre-defined validation data\n> - **Rationale**: Balances reproducibility needs with flexibility for different experimental setups\n> - **Consequences**: Requires seed management and validation set consistency checks\n\nThe splitting process generates detailed statistics about the resulting partitions:\n\n| Split Statistic | Training Set | Validation Set | Purpose |\n|----------------|--------------|----------------|---------|\n| Sample Count | Total training samples | Total validation samples | Size verification |\n| Average Token Length | Mean tokens per sample | Mean tokens per sample | Length distribution check |\n| Source Distribution | Percentage from each source | Percentage from each source | Source balance verification |\n| Quality Score Distribution | Mean and std dev of scores | Mean and std dev of scores | Quality balance check |\n\n### Architecture Decision Records\n\nSeveral key decisions shape the dataset preparation component's architecture and behavior:\n\n**Architecture Decision: Streaming vs Batch Processing**\n\n> **Decision: Hybrid Approach with Streaming for Large Files**\n> - **Context**: Training datasets can range from thousands to millions of samples, requiring different memory management strategies\n> - **Options Considered**:\n>   1. Load entire dataset into memory for fast random access\n>   2. Stream all processing to minimize memory usage\n>   3. Hybrid approach based on dataset size and available memory\n> - **Decision**: Use memory-based processing for datasets under 100K samples, streaming for larger datasets\n> - **Rationale**: Small datasets benefit from fast random access during splitting and shuffling, while large datasets require streaming to avoid memory overflow\n> - **Consequences**: Requires implementing both code paths but optimizes for common use cases while scaling to large datasets\n\n**Architecture Decision: Error Handling Philosophy**\n\n> **Decision: Fail-Fast for Configuration Errors, Continue for Data Errors**\n> - **Context**: Dataset preparation involves both configuration validation and individual sample processing, each with different error recovery requirements\n> - **Options Considered**:\n>   1. Fail immediately on any error\n>   2. Continue processing and report all errors at the end\n>   3. Fail fast for configuration, continue for recoverable data errors\n> - **Decision**: Halt immediately for configuration problems (missing files, invalid formats), but continue processing individual samples that fail validation\n> - **Rationale**: Configuration errors indicate fundamental setup problems that require user intervention, while individual sample failures are often due to data quality issues that can be filtered out\n> - **Consequences**: Requires clear error categorization and robust logging to help users distinguish between critical and recoverable errors\n\n**Architecture Decision: Memory Optimization Strategy**\n\n> **Decision: Lazy Loading with Configurable Caching**\n> - **Context**: Tokenized samples consume significant memory, especially for long sequences, but re-tokenization is computationally expensive\n> - **Options Considered**:\n>   1. Pre-tokenize and cache all samples in memory\n>   2. Tokenize on-demand with no caching\n>   3. Configurable caching based on available memory and dataset size\n> - **Decision**: Implement lazy tokenization with LRU cache sized based on available system memory\n> - **Rationale**: Balances memory efficiency with computational performance, adapting to available hardware resources\n> - **Consequences**: Requires memory monitoring and cache management logic but provides optimal performance across different hardware configurations\n\n### Common Pitfalls\n\nThe dataset preparation component involves several areas where developers commonly encounter issues. Understanding these pitfalls and their solutions is essential for successful implementation.\n\n⚠️ **Pitfall: Inconsistent Chat Template Application**\n\nA frequent error occurs when developers apply chat templates inconsistently across samples or use templates that don't match the target model. This happens when the template detection logic fails or when mixing data from different model families.\n\n**Why it's wrong**: Inconsistent templates confuse the model's learned conversation patterns, leading to poor generation quality and training instability. The model expects specific token sequences to indicate role transitions and conversation structure.\n\n**How to fix**: Implement template validation that verifies the chosen template matches the model's tokenizer configuration. Add consistency checks that ensure all samples in a dataset use the same template format. Include explicit template specification options for custom models.\n\n⚠️ **Pitfall: Label Masking Errors**\n\nDevelopers often incorrectly set up the labels array, either forgetting to mask instruction tokens (-100) or including padding tokens in the loss calculation. This leads to the model learning to predict its own instructions rather than generating appropriate responses.\n\n**Why it's wrong**: When instruction tokens aren't masked, the model learns to memorize input patterns rather than learning the input-output mapping. This results in poor response generation and potential overfitting to instruction formats.\n\n**How to fix**: Implement careful label generation that tracks the boundary between instruction and response tokens. Set instruction tokens and padding tokens to -100, and only use response token IDs as training targets. Validate label arrays to ensure they contain the expected number of non-masked tokens.\n\n⚠️ **Pitfall: Context Length Boundary Violations**\n\nWhen sequences exceed the model's context length, naive truncation can cut tokens in the middle of important content or remove essential special tokens. This corrupts the training data and degrades model performance.\n\n**Why it's wrong**: Cutting sequences at arbitrary boundaries can remove critical response content or leave conversations in invalid states. Missing special tokens break the model's understanding of conversation structure.\n\n**How to fix**: Implement intelligent truncation that preserves special tokens and attempts to maintain complete thoughts. Use proportional truncation strategies that reduce both instruction and response content rather than arbitrary cutoff points. Consider rejecting samples that cannot be meaningfully truncated.\n\n⚠️ **Pitfall: Validation Set Data Leakage**\n\nCreating validation sets without considering data relationships can result in near-duplicate samples appearing in both training and validation sets. This leads to overly optimistic evaluation metrics that don't reflect real-world performance.\n\n**Why it's wrong**: Data leakage inflates validation scores, making it impossible to detect overfitting or compare model performance accurately. The model appears to perform better than it actually will on unseen data.\n\n**How to fix**: Implement deduplication that considers semantic similarity, not just exact matches. For conversation datasets, ensure that related conversations (same topic, same user session) are kept together in the same split. Use content hashing and similarity measures to detect near-duplicates before splitting.\n\n⚠️ **Pitfall: Tokenizer Mismatch**\n\nUsing a tokenizer that doesn't match the target model leads to incorrect vocabulary mapping and degraded performance. This often happens when developers use a convenient tokenizer for preprocessing but switch to a different model for training.\n\n**Why it's wrong**: Different tokenizers produce different token sequences for the same text. Training data tokenized with one tokenizer cannot be correctly interpreted by a model expecting a different tokenizer's vocabulary.\n\n**How to fix**: Always use the exact tokenizer associated with the target model. Load the tokenizer from the same model checkpoint or repository that will be used for training. Implement tokenizer validation checks that verify vocabulary compatibility.\n\n⚠️ **Pitfall: Memory Management Failures**\n\nAttempting to load large datasets entirely into memory without considering available system resources leads to out-of-memory crashes or excessive swap usage that severely impacts performance.\n\n**Why it's wrong**: Large datasets can consume tens of gigabytes of memory when fully loaded, especially after tokenization. This can crash the preprocessing pipeline or make the system unusable due to memory pressure.\n\n**How to fix**: Implement memory monitoring that tracks dataset size and available system memory. Use streaming processing for large datasets and implement checkpointing so that preprocessing progress isn't lost due to memory issues. Provide clear memory requirements in documentation.\n\n### Implementation Guidance\n\nThis subsection provides concrete implementation guidance for building the dataset preparation component, including technology recommendations, file structure, and starter code.\n\n**Technology Recommendations**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|------------------|\n| Data Loading | pandas for CSV/JSON + json module | Apache Arrow + PyArrow for Parquet |\n| Text Processing | Basic string operations + regex | spaCy or NLTK for advanced text analysis |\n| Tokenization | HuggingFace Tokenizers library | Custom BPE with tokenizers library |\n| Memory Management | Python lists and dictionaries | Memory-mapped files with NumPy |\n| Data Validation | Manual type checking + assertions | Pydantic models for schema validation |\n\n**Recommended File Structure**\n\n```\nllm-fine-tuning/\n  src/\n    data_preparation/\n      __init__.py                    ← component exports\n      data_loader.py                ← format detection and loading\n      instruction_formatter.py      ← InstructionSample conversion\n      chat_templates.py             ← template application logic\n      tokenizer_pipeline.py         ← tokenization and length handling\n      dataset_splitter.py           ← train-validation splitting\n      quality_filter.py             ← data validation and filtering\n      utils.py                      ← shared utilities\n    config/\n      data_config.py                ← configuration schemas\n  tests/\n    test_data_preparation/          ← component tests\n  example_data/\n    sample_instruction_data.jsonl   ← test data for development\n```\n\n**Core Data Structures (Complete Implementation)**\n\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional, List, Dict, Any, Union\nfrom pathlib import Path\nimport json\n\n@dataclass\nclass InstructionSample:\n    \"\"\"Standardized representation of an instruction-response pair.\"\"\"\n    instruction: str\n    response: str\n    system_prompt: Optional[str] = None\n    input_context: Optional[str] = None\n    sample_id: str = \"\"\n    source: str = \"unknown\"\n    quality_score: Optional[float] = None\n    \n    def __post_init__(self):\n        if not self.sample_id:\n            self.sample_id = hash(self.instruction + self.response)\n    \n    def to_dict(self) -> Dict[str, Any]:\n        return {\n            \"instruction\": self.instruction,\n            \"response\": self.response,\n            \"system_prompt\": self.system_prompt,\n            \"input_context\": self.input_context,\n            \"sample_id\": self.sample_id,\n            \"source\": self.source,\n            \"quality_score\": self.quality_score\n        }\n\n@dataclass\nclass ConversationTurn:\n    \"\"\"Single turn in a multi-turn conversation.\"\"\"\n    role: str  # \"user\", \"assistant\", \"system\"\n    content: str\n    turn_index: int\n    \n@dataclass\nclass ConversationSample:\n    \"\"\"Multi-turn conversation representation.\"\"\"\n    conversation_id: str\n    turns: List[ConversationTurn]\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    total_tokens: Optional[int] = None\n    is_valid: bool = True\n\n@dataclass\nclass TokenizedSample:\n    \"\"\"Tokenized and formatted sample ready for training.\"\"\"\n    input_ids: List[int]\n    attention_mask: List[int]\n    labels: List[int]\n    prompt_length: int\n    response_length: int\n    total_length: int\n    \n    def validate(self) -> bool:\n        \"\"\"Validate tokenized sample consistency.\"\"\"\n        expected_length = len(self.input_ids)\n        return (len(self.attention_mask) == expected_length and\n                len(self.labels) == expected_length and\n                self.total_length == expected_length)\n```\n\n**Data Loading Infrastructure (Complete Starter Code)**\n\n```python\nimport pandas as pd\nimport json\nfrom pathlib import Path\nfrom typing import Iterator, List, Dict, Any, Union\nimport pyarrow.parquet as pq\n\nclass DataLoader:\n    \"\"\"Handles loading data from multiple formats with format auto-detection.\"\"\"\n    \n    SUPPORTED_FORMATS = {'.json', '.jsonl', '.csv', '.parquet'}\n    \n    def __init__(self, chunk_size: int = 10000):\n        self.chunk_size = chunk_size\n        \n    def detect_format(self, file_path: Path) -> str:\n        \"\"\"Detect file format from extension.\"\"\"\n        suffix = file_path.suffix.lower()\n        if suffix not in self.SUPPORTED_FORMATS:\n            raise ValueError(f\"Unsupported format: {suffix}\")\n        return suffix\n    \n    def load_data(self, file_path: Path) -> Iterator[Dict[str, Any]]:\n        \"\"\"Load data with format-specific handling.\"\"\"\n        format_type = self.detect_format(file_path)\n        \n        if format_type == '.json':\n            yield from self._load_json(file_path)\n        elif format_type == '.jsonl':\n            yield from self._load_jsonl(file_path)\n        elif format_type == '.csv':\n            yield from self._load_csv(file_path)\n        elif format_type == '.parquet':\n            yield from self._load_parquet(file_path)\n    \n    def _load_json(self, file_path: Path) -> Iterator[Dict[str, Any]]:\n        \"\"\"Load JSON file (assumes array of objects).\"\"\"\n        with open(file_path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n            if isinstance(data, list):\n                yield from data\n            else:\n                yield data\n    \n    def _load_jsonl(self, file_path: Path) -> Iterator[Dict[str, Any]]:\n        \"\"\"Load JSONL file line by line.\"\"\"\n        with open(file_path, 'r', encoding='utf-8') as f:\n            for line_num, line in enumerate(f, 1):\n                line = line.strip()\n                if not line:\n                    continue\n                try:\n                    yield json.loads(line)\n                except json.JSONDecodeError as e:\n                    print(f\"Warning: Invalid JSON on line {line_num}: {e}\")\n    \n    def _load_csv(self, file_path: Path) -> Iterator[Dict[str, Any]]:\n        \"\"\"Load CSV file in chunks.\"\"\"\n        for chunk in pd.read_csv(file_path, chunksize=self.chunk_size):\n            for _, row in chunk.iterrows():\n                yield row.to_dict()\n    \n    def _load_parquet(self, file_path: Path) -> Iterator[Dict[str, Any]]:\n        \"\"\"Load Parquet file in batches.\"\"\"\n        parquet_file = pq.ParquetFile(file_path)\n        for batch in parquet_file.iter_batches(batch_size=self.chunk_size):\n            df = batch.to_pandas()\n            for _, row in df.iterrows():\n                yield row.to_dict()\n\nclass InstructionFormatter:\n    \"\"\"Converts raw data dictionaries to InstructionSample objects.\"\"\"\n    \n    # Common field mappings from different data formats\n    FIELD_MAPPINGS = {\n        'instruction': ['instruction', 'prompt', 'input', 'question'],\n        'response': ['response', 'output', 'answer', 'completion'],\n        'system_prompt': ['system', 'system_prompt', 'context'],\n        'input_context': ['input_context', 'context', 'background'],\n    }\n    \n    def __init__(self, field_mapping: Optional[Dict[str, str]] = None):\n        \"\"\"Initialize with optional custom field mapping.\"\"\"\n        self.custom_mapping = field_mapping or {}\n    \n    def format_sample(self, raw_data: Dict[str, Any]) -> InstructionSample:\n        \"\"\"Convert raw dictionary to InstructionSample.\"\"\"\n        # TODO 1: Extract instruction field using mapping priority\n        # TODO 2: Extract response field using mapping priority  \n        # TODO 3: Extract optional fields (system_prompt, input_context)\n        # TODO 4: Generate sample_id if not provided\n        # TODO 5: Set source field from metadata or filename\n        # TODO 6: Validate required fields are non-empty\n        # TODO 7: Return constructed InstructionSample object\n        pass\n    \n    def _extract_field(self, data: Dict[str, Any], target_field: str) -> Optional[str]:\n        \"\"\"Extract field using priority mapping.\"\"\"\n        # Check custom mapping first\n        if target_field in self.custom_mapping:\n            custom_key = self.custom_mapping[target_field]\n            if custom_key in data:\n                return str(data[custom_key])\n        \n        # Check standard mappings\n        possible_keys = self.FIELD_MAPPINGS.get(target_field, [])\n        for key in possible_keys:\n            if key in data and data[key] is not None:\n                return str(data[key])\n        \n        return None\n```\n\n**Quality Filtering Infrastructure (Complete Starter Code)**\n\n```python\nimport hashlib\nimport re\nfrom typing import Set, List, Tuple\nfrom dataclasses import dataclass\n\n@dataclass\nclass QualityFilterStats:\n    \"\"\"Statistics from quality filtering process.\"\"\"\n    total_samples: int = 0\n    duplicates_removed: int = 0\n    length_filtered: int = 0\n    quality_filtered: int = 0\n    final_count: int = 0\n\nclass QualityFilter:\n    \"\"\"Filters instruction samples for quality and consistency.\"\"\"\n    \n    def __init__(self, \n                 min_instruction_length: int = 10,\n                 max_instruction_length: int = 2048,\n                 min_response_length: int = 10,\n                 max_response_length: int = 2048,\n                 min_quality_score: float = 0.0):\n        self.min_instruction_length = min_instruction_length\n        self.max_instruction_length = max_instruction_length\n        self.min_response_length = min_response_length\n        self.max_response_length = max_response_length\n        self.min_quality_score = min_quality_score\n        self.seen_hashes: Set[str] = set()\n        self.stats = QualityFilterStats()\n    \n    def filter_dataset(self, samples: List[InstructionSample]) -> Tuple[List[InstructionSample], QualityFilterStats]:\n        \"\"\"Apply all quality filters to the dataset.\"\"\"\n        self.stats = QualityFilterStats()\n        self.stats.total_samples = len(samples)\n        \n        filtered_samples = []\n        \n        for sample in samples:\n            # TODO 1: Check for duplicates using content hash\n            # TODO 2: Validate instruction and response lengths\n            # TODO 3: Apply quality score filtering if available\n            # TODO 4: Check for obvious spam or corrupted content\n            # TODO 5: Validate language consistency between instruction/response\n            # TODO 6: Add to filtered list if all checks pass\n            pass\n        \n        self.stats.final_count = len(filtered_samples)\n        return filtered_samples, self.stats\n    \n    def _compute_content_hash(self, sample: InstructionSample) -> str:\n        \"\"\"Compute hash for duplicate detection.\"\"\"\n        content = f\"{sample.instruction.strip()}{sample.response.strip()}\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def _is_valid_length(self, sample: InstructionSample) -> bool:\n        \"\"\"Check if sample meets length requirements.\"\"\"\n        inst_len = len(sample.instruction.strip())\n        resp_len = len(sample.response.strip())\n        \n        return (self.min_instruction_length <= inst_len <= self.max_instruction_length and\n                self.min_response_length <= resp_len <= self.max_response_length)\n    \n    def _is_quality_content(self, sample: InstructionSample) -> bool:\n        \"\"\"Basic content quality checks.\"\"\"\n        # TODO: Implement spam detection, language detection, completeness checks\n        pass\n```\n\n**Core Logic Skeleton (for Student Implementation)**\n\n```python\nclass ChatTemplateApplicator:\n    \"\"\"Applies model-specific chat templates to instruction-response pairs.\"\"\"\n    \n    def __init__(self, tokenizer, custom_template: Optional[str] = None):\n        self.tokenizer = tokenizer\n        self.custom_template = custom_template\n        self._detected_template = None\n    \n    def apply_template(self, sample: InstructionSample) -> str:\n        \"\"\"Apply chat template to create formatted conversation.\"\"\"\n        # TODO 1: Detect or load the appropriate chat template\n        # TODO 2: Structure the conversation with roles (system, user, assistant)\n        # TODO 3: Apply the template formatting with special tokens\n        # TODO 4: Validate the formatted output has proper token structure\n        # TODO 5: Return the complete formatted string ready for tokenization\n        # Hint: Use tokenizer.apply_chat_template() if available\n        # Hint: Handle system prompts by adding them as first message\n        pass\n    \n    def _detect_template_format(self) -> str:\n        \"\"\"Auto-detect template format from tokenizer.\"\"\"\n        # TODO 1: Check if tokenizer has chat_template attribute\n        # TODO 2: Identify template type (ChatML, Llama, Alpaca, etc.)\n        # TODO 3: Return standardized template identifier\n        # TODO 4: Fall back to generic template if detection fails\n        pass\n\nclass TokenizerPipeline:\n    \"\"\"Handles tokenization with proper length management and label masking.\"\"\"\n    \n    def __init__(self, tokenizer, max_length: int = 2048):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.template_applicator = ChatTemplateApplicator(tokenizer)\n    \n    def tokenize_sample(self, sample: InstructionSample) -> TokenizedSample:\n        \"\"\"Convert InstructionSample to TokenizedSample with proper masking.\"\"\"\n        # TODO 1: Apply chat template to format the conversation\n        # TODO 2: Tokenize instruction and response portions separately\n        # TODO 3: Create attention mask (1 for content, 0 for padding)\n        # TODO 4: Create labels array (-100 for instruction, token_ids for response)\n        # TODO 5: Handle length truncation if sequence exceeds max_length\n        # TODO 6: Add special tokens (BOS, EOS) as needed\n        # TODO 7: Return TokenizedSample with all required fields\n        # Hint: Track the boundary between instruction and response tokens\n        # Hint: Use tokenizer.pad() method for consistent padding\n        pass\n    \n    def _create_labels_array(self, input_ids: List[int], prompt_length: int) -> List[int]:\n        \"\"\"Create labels array with proper masking.\"\"\"\n        # TODO 1: Set instruction tokens (0 to prompt_length) to -100\n        # TODO 2: Set response tokens (prompt_length onwards) to their token IDs\n        # TODO 3: Set padding tokens to -100\n        # TODO 4: Return complete labels array\n        pass\n\nclass DatasetSplitter:\n    \"\"\"Splits dataset into training and validation sets with stratification.\"\"\"\n    \n    def __init__(self, val_ratio: float = 0.1, random_seed: int = 42):\n        self.val_ratio = val_ratio\n        self.random_seed = random_seed\n    \n    def split_dataset(self, samples: List[InstructionSample], \n                     stratify_by: Optional[str] = None) -> Tuple[List[InstructionSample], List[InstructionSample]]:\n        \"\"\"Split samples into train and validation sets.\"\"\"\n        # TODO 1: Set random seed for reproducible splits\n        # TODO 2: If stratify_by is specified, group samples by that attribute\n        # TODO 3: Within each group, randomly assign samples to train/val\n        # TODO 4: Ensure validation ratio is approximately maintained\n        # TODO 5: Return tuple of (train_samples, val_samples)\n        # Hint: Use sklearn.model_selection.train_test_split if available\n        # Hint: For custom stratification, implement proportional sampling\n        pass\n```\n\n**Milestone Checkpoint**\n\nAfter implementing the dataset preparation component, verify the following behavior:\n\n1. **Data Loading Test**: \n   ```bash\n   python -m pytest tests/test_data_preparation/test_data_loader.py\n   ```\n   Expected: All format types (JSON, JSONL, CSV, Parquet) load correctly and produce InstructionSample objects.\n\n2. **Manual Validation**:\n   - Load a sample dataset and verify field extraction works correctly\n   - Apply chat templates and confirm special tokens are properly positioned  \n   - Check that tokenization produces correct label masking (-100 for instructions, token IDs for responses)\n   - Verify train-validation splits maintain approximately the specified ratio\n\n3. **Quality Check**:\n   - Process a dataset with known duplicates and confirm they're removed\n   - Test length filtering with samples that exceed context limits\n   - Verify that the pipeline handles malformed data gracefully without crashing\n\n**Signs of Problems**:\n- Memory usage growing unboundedly (indicates streaming isn't working)\n- Chat templates producing malformed output (check special token placement)\n- Label arrays containing unexpected values (verify masking logic)\n- Validation sets being too large or too small (check splitting math)\n\n\n## LoRA Configuration Component\n\n> **Milestone(s):** Milestone 2 - this section implements the LoRA adapter setup, target module identification, rank configuration, and parameter efficiency verification that enables memory-efficient fine-tuning\n\n### Mental Model: The Skill Overlay\n\nThink of LoRA as teaching a master craftsperson new specialized techniques without making them forget their core expertise. Imagine you have a master woodworker who already knows thousands of traditional techniques accumulated over years of experience. Rather than starting their education from scratch to learn modern power tool techniques, you create a **skill overlay** - a thin layer of new knowledge that builds on their existing foundation.\n\nThe skill overlay works by identifying specific decision points in their existing workflow where new techniques can be applied. For example, when the craftsperson reaches for a traditional hand saw (a \"target module\"), the overlay whispers: \"consider using the circular saw instead, but apply this specific adjustment to your grip and this modification to your cutting angle.\" The craftsperson's fundamental understanding of wood, joints, and structural principles remains untouched - only specific techniques get enhanced.\n\nLoRA operates on this same principle with neural networks. The base model retains all its learned language understanding, world knowledge, and reasoning patterns accumulated during pre-training on massive datasets. Instead of modifying these core capabilities (which would require retraining billions of parameters), LoRA creates small \"skill overlays\" - **low-rank adapter matrices** that learn task-specific modifications to apply at key decision points in the model's processing.\n\nWhen the model encounters a new instruction-following task, these adapters provide learned adjustments: \"when processing this type of instruction, adjust your attention patterns this way\" or \"for this response format, modify your output generation with these learned offsets.\" The fundamental language model remains frozen and intact, while the adapters provide targeted improvements for the specific fine-tuning objective.\n\nThis approach preserves the stability and generality of the pre-trained model while enabling efficient specialization. Just as the craftsperson can remove the skill overlay and return to traditional techniques when needed, LoRA adapters can be easily swapped, merged, or removed without affecting the underlying model capabilities.\n\n### Target Module Identification\n\nThe **target module identification** subsystem automatically discovers which layers in a transformer model are suitable for LoRA adaptation. This process involves analyzing the model's architecture to identify linear transformation layers where low-rank adapters can be effectively applied.\n\nModern transformer architectures contain numerous linear layers that serve different purposes in the information processing pipeline. The most effective targets for LoRA adaptation are typically the **attention projection matrices** (query, key, value, and output projections) and the **feed-forward network layers** within each transformer block. These layers perform high-dimensional linear transformations that benefit from the low-rank decomposition approach that LoRA provides.\n\nThe identification process begins by traversing the model's module hierarchy to catalog all linear layers and their roles. For attention mechanisms, this includes the `q_proj`, `k_proj`, `v_proj`, and `o_proj` layers that transform input embeddings into query, key, value, and output representations. For feed-forward networks, this encompasses the `up_proj`, `down_proj`, and `gate_proj` layers (in models with gated activations) that process information through the MLP blocks.\n\nDifferent model architectures use varying naming conventions for these components. Llama-style models typically use names like `self_attn.q_proj` and `mlp.up_proj`, while other architectures may use patterns like `attention.query` or `feed_forward.dense_h_to_4h`. The target module identifier maintains architecture-specific mapping tables that translate common layer patterns into standardized target specifications.\n\nThe selection strategy also considers the **computational and memory trade-offs** of adapting different layer types. Attention layers often provide the highest impact per parameter when adapted, as they control how the model focuses on different parts of the input sequence. Feed-forward layers offer additional capacity for learning task-specific transformations but consume more adapter parameters due to their larger dimensions.\n\n| Target Module Type | Typical Names | Impact on Task Performance | Memory Cost | Recommended Priority |\n|---------------------|---------------|---------------------------|-------------|---------------------|\n| Query Projection | `q_proj`, `query` | High - controls attention focus | Medium | High |\n| Key Projection | `k_proj`, `key` | High - affects attention patterns | Medium | High |\n| Value Projection | `v_proj`, `value` | High - determines information flow | Medium | High |\n| Output Projection | `o_proj`, `dense` | High - final attention transformation | Medium | High |\n| Feed-Forward Up | `up_proj`, `dense_h_to_4h` | Medium - expands representations | High | Medium |\n| Feed-Forward Down | `down_proj`, `dense_4h_to_h` | Medium - compresses representations | High | Medium |\n| Gate Projection | `gate_proj` | Medium - controls activation flow | High | Medium |\n\nThe automatic detection algorithm employs several heuristics to identify target modules reliably across different model architectures. It searches for linear layers with specific dimension patterns, analyzes the module naming hierarchy to identify attention and MLP components, and validates that identified targets have appropriate weight tensor shapes for LoRA decomposition.\n\n**Architecture Decision Record: Target Module Selection Strategy**\n\n> **Decision: Automatic Detection with Architecture-Specific Overrides**\n> \n> - **Context**: Different model families use varying naming conventions and architectural patterns for their linear layers. A hardcoded target list would break when applied to new architectures, but fully automatic detection might miss optimal adaptation points or include inappropriate layers.\n> \n> - **Options Considered**:\n>   1. **Hardcoded Target Lists**: Maintain fixed lists of target module names for each supported architecture\n>   2. **Fully Automatic Detection**: Use pattern matching and dimension analysis to detect all suitable linear layers\n>   3. **Hybrid Approach**: Automatic detection with architecture-specific override capabilities\n> \n> - **Decision**: Implement the hybrid approach with automatic detection as the default and architecture-specific configuration overrides\n> \n> - **Rationale**: This provides the best balance of flexibility and reliability. Automatic detection handles most cases correctly and adapts to new architectures, while overrides allow fine-tuning for specific model families where manual optimization improves results. The system remains maintainable as new architectures emerge.\n> \n> - **Consequences**: Requires implementing both the pattern-matching detection logic and the override configuration system. Increases initial complexity but dramatically improves long-term maintainability and compatibility across model families.\n\nThe detection algorithm also validates that identified target modules meet the requirements for LoRA adaptation. This includes verifying that modules are indeed linear transformations (not embeddings or normalization layers), confirming that they have trainable parameters that can be frozen, and ensuring that their dimensions are suitable for low-rank decomposition.\n\n### Rank and Alpha Parameter Selection\n\nThe **rank and alpha parameter selection** process determines the capacity and scaling behavior of LoRA adapters, directly impacting both the quality of fine-tuning and the efficiency gains achieved. These hyperparameters control the fundamental trade-off between adaptation capability and parameter efficiency that makes LoRA practical for large model fine-tuning.\n\nThe **rank parameter** (r) determines the dimensionality of the low-rank decomposition used in each adapter. LoRA replaces direct weight updates ΔW with the product of two smaller matrices: ΔW = BA, where B has dimensions (output_dim, r) and A has dimensions (r, input_dim). The rank value controls how much information the adapter can capture about the task-specific weight modifications needed.\n\nLower rank values create more constrained adapters that can only learn simple, low-dimensional modifications to the original weights. This provides strong regularization and memory efficiency but may limit the adapter's ability to capture complex task-specific patterns. Higher rank values allow adapters to learn more sophisticated transformations but consume more memory and may be prone to overfitting, especially with limited training data.\n\nThe **alpha parameter** (α) controls the scaling applied to the adapter outputs before they're added to the frozen base model weights. This scaling factor affects the relative influence of the adapted versus original model behavior. The effective learning rate for adapter parameters becomes proportional to α/r, meaning alpha acts as a learning rate multiplier specifically for the adaptation process.\n\n| Rank (r) | Memory per Adapter | Expressiveness | Overfitting Risk | Best Use Cases |\n|----------|-------------------|----------------|------------------|----------------|\n| 4-8 | Very Low | Limited | Very Low | Simple task adaptation, limited data |\n| 16-32 | Low | Moderate | Low | General instruction tuning, most tasks |\n| 64-128 | Moderate | High | Moderate | Complex reasoning, domain specialization |\n| 256+ | High | Very High | High | Research experimentation, abundant data |\n\n| Alpha (α) | Effective LR Scale | Adaptation Strength | Stability | Recommended Rank Range |\n|-----------|-------------------|--------------------:|-----------|------------------------|\n| 8-16 | Low | Subtle | High | 4-16 |\n| 32-64 | Medium | Balanced | Medium | 16-64 |\n| 128-256 | High | Strong | Lower | 64-128 |\n| 512+ | Very High | Aggressive | Low | Research only |\n\nThe selection process begins by analyzing the **task complexity and data characteristics**. Tasks requiring significant behavioral changes (like learning new response formats or domain-specific knowledge) typically benefit from higher rank values, while tasks focused on style adaptation or minor behavioral adjustments work well with lower ranks. The amount of training data also influences optimal rank selection - more data can support higher ranks without overfitting.\n\n**Model architecture characteristics** also inform rank selection. Larger models with more parameters per layer can typically benefit from higher-rank adapters, as they have more representational capacity to utilize. The original weight matrix dimensions provide upper bounds for useful rank values - using ranks approaching the smaller dimension of the weight matrix provides diminishing returns while consuming excessive memory.\n\nThe alpha parameter selection considers the desired **adaptation aggressiveness** and training stability requirements. Conservative alpha values (8-32) provide stable training with gradual adaptation, suitable for tasks where preserving most of the original model behavior is important. Higher alpha values (64-128) enable more aggressive adaptation for tasks requiring significant behavioral changes.\n\n**Architecture Decision Record: Rank-Alpha Scaling Relationship**\n\n> **Decision: Proportional Scaling with Task-Adaptive Defaults**\n> \n> - **Context**: The relationship between rank and alpha significantly affects training dynamics and final performance. Fixed alpha values don't account for different rank choices, while fixed ratios may not suit all task types.\n> \n> - **Options Considered**:\n>   1. **Fixed Alpha**: Use α=32 regardless of rank choice\n>   2. **Fixed Ratio**: Maintain constant α/r ratio (e.g., α=2r)\n>   3. **Task-Adaptive Scaling**: Adjust α/r ratio based on task characteristics and rank selection\n> \n> - **Decision**: Implement task-adaptive scaling with configurable α/r ratios and intelligent defaults\n> \n> - **Rationale**: Task-adaptive scaling provides the best results across different fine-tuning scenarios. Low-rank adapters benefit from higher α/r ratios to compensate for reduced capacity, while high-rank adapters work better with lower ratios to prevent instability.\n> \n> - **Consequences**: Requires implementing task analysis logic to recommend appropriate scaling factors. Adds complexity but significantly improves out-of-the-box performance across diverse fine-tuning tasks.\n\nThe system provides **automated rank and alpha recommendations** based on task analysis and resource constraints. This includes analyzing the training data to estimate task complexity, considering available GPU memory to determine maximum feasible rank, evaluating model architecture to suggest optimal target modules, and recommending alpha values that balance adaptation strength with training stability.\n\nFor users who prefer manual control, the configuration system supports explicit rank and alpha specification with validation to ensure parameters are within reasonable ranges. The system also provides guidance on the expected memory usage and training behavior for different parameter combinations.\n\n### Adapter Initialization and Injection\n\nThe **adapter initialization and injection** process creates the low-rank matrices and integrates them into the frozen base model to enable parameter-efficient fine-tuning. This process must carefully manage memory allocation, preserve the original model behavior during initialization, and ensure that gradients flow correctly through the adapter pathways during training.\n\nLoRA adapters consist of two matrices per target module: matrix **A** with dimensions (rank, input_features) and matrix **B** with dimensions (output_features, rank). During forward passes, the adapter computes the low-rank update as: `adapter_output = input @ A.T @ B.T`, which is then scaled by the alpha parameter and added to the frozen base layer output.\n\nThe **initialization strategy** for these matrices is crucial for training stability and convergence. Matrix A is typically initialized using small random values drawn from a normal or uniform distribution, providing the initial variability needed for gradient-based learning. Matrix B is initialized to zeros, ensuring that the adapter produces zero output initially and preserves the original model behavior at the start of training.\n\nThis zero-initialization approach means that the model begins fine-tuning with exactly the same behavior as the frozen base model. As training progresses, the adapter matrices learn non-zero values that gradually modify the model's responses toward the target task behavior. This provides a smooth transition from the pre-trained model capabilities to the fine-tuned specialization.\n\n| Matrix | Initialization | Dimensions | Purpose | Gradient Flow |\n|--------|----------------|------------|---------|---------------|\n| A | Random (std=0.01) | (rank, input_features) | Input projection | Receives gradients from B |\n| B | Zeros | (output_features, rank) | Output projection | Receives gradients from loss |\n| Scaling | Fixed (α/r) | Scalar | Controls adaptation strength | No gradients |\n\nThe **injection process** modifies the target modules to incorporate the adapter computation without affecting the original weight tensors. This typically involves wrapping the original linear layers with adapter-aware implementations that compute both the frozen base transformation and the trainable adapter transformation in parallel.\n\nThe adapter injection maintains careful separation between frozen and trainable parameters. The original model weights are marked as non-trainable and remain unchanged throughout the fine-tuning process. Only the adapter matrices A and B receive gradients and undergo optimization updates. This separation is essential for parameter efficiency and enables easy adapter removal or swapping after training.\n\n**Memory management** during injection requires attention to both GPU memory allocation and memory fragmentation. The system allocates adapter matrices on the same device as the base model weights to avoid expensive CPU-GPU transfers during forward and backward passes. Memory allocation is performed incrementally as each target module is processed, allowing for better memory utilization patterns.\n\nThe injection process also configures the **computation graph** to ensure proper gradient flow through the adapter pathways. This includes registering the adapter parameters with the optimizer, setting up the forward pass computation to combine base and adapter outputs, and ensuring that backward passes correctly accumulate gradients in the adapter matrices.\n\n> The key insight for adapter injection is maintaining **computational isolation** between the frozen base model and the trainable adapters while ensuring they compose correctly during forward passes. This isolation enables independent updates to adapter parameters without affecting base model weights and allows for easy adapter composition and removal.\n\n**Architecture Decision Record: Adapter Storage and Composition Strategy**\n\n> **Decision: Separate Adapter Modules with Runtime Composition**\n> \n> - **Context**: Adapters can be implemented as modifications to existing model layers or as separate modules that compose with frozen layers. The choice affects memory usage, computational efficiency, and adapter portability.\n> \n> - **Options Considered**:\n>   1. **In-Place Injection**: Modify existing linear layer implementations to include adapter computations directly\n>   2. **Wrapper Layers**: Create wrapper modules that contain both the frozen layer and its adapters\n>   3. **Separate Modules**: Store adapters as independent modules that compose with frozen layers at runtime\n> \n> - **Decision**: Implement separate adapter modules with runtime composition through the PEFT library integration\n> \n> - **Rationale**: Separate modules provide the cleanest separation of concerns, enable easy adapter swapping and merging, maintain compatibility with existing model architectures, and leverage well-tested PEFT implementations.\n> \n> - **Consequences**: Requires implementing adapter composition logic but provides maximum flexibility for adapter management and portability across different base models.\n\nThe adapter injection process includes **validation steps** to ensure correct integration with the target model. This involves verifying that adapter dimensions match the target module dimensions, confirming that only adapter parameters are marked as trainable, testing that forward passes produce expected output shapes, and validating that gradients flow correctly to adapter parameters during backward passes.\n\nFor models that will undergo quantization, the injection process coordinates with the quantization system to ensure adapters remain in full precision while base model weights are quantized. This requires careful device placement and dtype management to maintain training compatibility.\n\n### Trainable Parameter Analysis\n\nThe **trainable parameter analysis** subsystem provides comprehensive monitoring and validation of the parameter efficiency gains achieved through LoRA adaptation. This analysis is crucial for verifying that the fine-tuning process achieves its memory and computational efficiency goals while maintaining sufficient model capacity for effective learning.\n\nThe analysis begins by cataloging the **complete parameter inventory** of both the base model and the injected adapters. For the base model, this includes counting all weight tensors across transformer layers, embedding layers, and output projection layers. These parameters are marked as frozen and excluded from gradient computation and optimizer updates. For the adapter system, the analysis counts all A and B matrices across all target modules and calculates the total trainable parameters introduced by the LoRA configuration.\n\nThe **parameter efficiency metrics** provide quantitative measures of the memory and computational savings achieved. The primary metric is the trainable parameter ratio: the percentage of total model parameters that require gradient computation during training. Effective LoRA configurations typically achieve ratios below 1%, with many successful fine-tuning runs using only 0.1-0.5% of the original parameter count.\n\n| Parameter Category | Count | Percentage | Memory Usage | Gradient Computation |\n|--------------------|-------|------------|--------------|---------------------|\n| Base Model Weights | 7.2B | 99.2% | 14.4 GB (fp16) | Disabled |\n| LoRA Adapters | 58M | 0.8% | 116 MB (fp16) | Enabled |\n| Optimizer States | 58M | 0.8% | 232 MB (Adam) | Required |\n| Total Trainable | 58M | 0.8% | 348 MB | Active |\n\nThe analysis also examines the **rank utilization efficiency** across different target modules to identify potential optimization opportunities. Some target modules may be more critical for task adaptation than others, and the analysis can reveal whether rank allocation is appropriately balanced. Modules that consistently show low gradient magnitudes or minimal weight updates may be candidates for rank reduction or removal from the target list.\n\n**Memory footprint analysis** provides detailed breakdowns of GPU memory usage across different components of the fine-tuning system. This includes the quantized base model weights, the full-precision adapter parameters, the optimizer states for trainable parameters, and the activation memory required during forward and backward passes. Understanding these memory components helps optimize batch sizes and identify bottlenecks in the training configuration.\n\nThe analysis tracks **gradient statistics** for adapter parameters throughout training to monitor learning dynamics and identify potential issues. This includes measuring gradient magnitudes, monitoring gradient-to-parameter ratios, detecting gradient explosion or vanishing problems, and analyzing the distribution of updates across different adapter modules.\n\n**Effective rank analysis** examines whether the chosen rank values are being fully utilized by the learned adapters. Low effective rank (where the learned matrices have rank significantly below the configured rank) may indicate that smaller rank values could achieve similar performance with better efficiency. High effective rank utilization suggests that the adapters are making full use of their representational capacity.\n\n| Analysis Metric | Purpose | Target Range | Warning Indicators |\n|-----------------|---------|--------------|-------------------|\n| Trainable Ratio | Parameter efficiency | 0.1% - 2.0% | >5% suggests inefficiency |\n| Gradient Magnitude | Learning activity | 1e-5 - 1e-3 | <1e-6 suggests vanishing gradients |\n| Effective Rank | Capacity utilization | 70% - 95% of config rank | <50% suggests over-parameterization |\n| Memory Overhead | Resource usage | <20% of base model | >50% defeats efficiency purpose |\n\nThe analysis system provides **automated recommendations** for parameter optimization based on the observed training dynamics. This includes suggesting rank adjustments based on effective rank utilization, recommending alpha parameter tuning based on gradient statistics, identifying underutilized target modules that could be removed, and proposing memory optimizations based on usage patterns.\n\n**Convergence analysis** examines the relationship between adapter parameters and training loss to ensure that the parameter-efficient approach is not significantly compromising learning effectiveness. This includes comparing convergence rates with full fine-tuning baselines where available, analyzing the relationship between parameter count and final performance, and identifying the minimum parameter configuration that achieves acceptable task performance.\n\n> A critical insight from trainable parameter analysis is that **effective parameter efficiency depends on task-adapter alignment** - the degree to which the chosen target modules and rank configurations match the actual learning requirements of the fine-tuning task. Misaligned configurations may require more parameters to achieve the same performance, reducing the efficiency benefits.\n\n### Architecture Decision Records\n\nThis subsection documents the key architectural decisions made in designing the LoRA configuration component, providing context for the design choices and their implications for system behavior and performance.\n\n**Architecture Decision Record: Target Module Selection Granularity**\n\n> **Decision: Layer-Type Based Selection with Individual Override Capability**\n> \n> - **Context**: LoRA adapters can be applied at different granularities - all linear layers, specific layer types (attention vs MLP), or individual modules. The granularity affects both performance and memory usage, and different tasks may benefit from different targeting strategies.\n> \n> - **Options Considered**:\n>   1. **All Linear Layers**: Apply adapters to every linear transformation in the model\n>   2. **Layer-Type Grouping**: Apply to all attention layers, all MLP layers, or both\n>   3. **Individual Selection**: Allow specification of exact module names for adapter application\n>   4. **Hybrid Approach**: Layer-type defaults with individual override capability\n> \n> - **Decision**: Implement the hybrid approach with intelligent layer-type defaults and per-module override options\n> \n> - **Rationale**: Layer-type grouping provides good defaults for most use cases while individual selection enables fine-tuned optimization for specific tasks. The hybrid approach accommodates both novice users who want reasonable defaults and expert users who need precise control.\n> \n> - **Consequences**: Requires implementing both the automatic layer detection and the individual module specification systems. Increases configuration complexity but provides optimal flexibility for different use cases and expertise levels.\n\n**Architecture Decision Record: Adapter Parameter Initialization Strategy**\n\n> **Decision: Asymmetric Initialization with Configurable Distributions**\n> \n> - **Context**: LoRA adapter matrices need initialization that balances training stability with learning capability. Different initialization strategies affect convergence speed, final performance, and training stability.\n> \n> - **Options Considered**:\n>   1. **Symmetric Random**: Initialize both A and B matrices with small random values\n>   2. **Zero Initialization**: Initialize both matrices to zero (preserves base model exactly)\n>   3. **Asymmetric (A random, B zero)**: Initialize A randomly and B to zero\n>   4. **Orthogonal Initialization**: Use orthogonal matrices for improved gradient flow\n> \n> - **Decision**: Use asymmetric initialization (A random, B zero) as default with configurable alternatives\n> \n> - **Rationale**: Asymmetric initialization provides the best balance of training stability and learning capability. Starting with zero adapter output preserves base model behavior initially while random A matrix provides gradient diversity. This approach has proven most reliable across diverse fine-tuning tasks.\n> \n> - **Consequences**: Requires implementing multiple initialization strategies but provides optimal training dynamics for the majority of use cases while allowing experimentation with alternatives.\n\n**Architecture Decision Record: Rank Configuration Strategy**\n\n> **Decision: Task-Informed Automatic Defaults with Manual Override**\n> \n> - **Context**: Rank selection significantly impacts both performance and efficiency, but optimal ranks vary by task complexity, model size, and available training data. Users need guidance for rank selection while retaining control for specific requirements.\n> \n> - **Options Considered**:\n>   1. **Fixed Default Ranks**: Use the same rank (e.g., 16) for all configurations\n>   2. **Model-Size Scaling**: Scale rank proportionally to model parameter count\n>   3. **Task-Informed Defaults**: Recommend ranks based on task analysis with manual override\n>   4. **Adaptive Ranking**: Automatically adjust ranks during training based on performance\n> \n> - **Decision**: Implement task-informed defaults with comprehensive manual override capabilities\n> \n> - **Rationale**: Task-informed defaults provide good starting points for users while manual overrides enable optimization for specific use cases. Adaptive ranking adds complexity without clear benefits, while fixed defaults ignore important task variations.\n> \n> - **Consequences**: Requires implementing task analysis logic and rank recommendation algorithms. Increases system complexity but dramatically improves user experience and out-of-the-box performance.\n\n### Common Pitfalls\n\nThis subsection identifies frequent mistakes that developers encounter when implementing or configuring LoRA adapters, providing concrete guidance on recognition, prevention, and correction of these issues.\n\n⚠️ **Pitfall: Rank Selection Without Task Analysis**\n\nMany developers choose LoRA ranks based on memory constraints alone, without considering the learning requirements of their specific fine-tuning task. This often results in using very low ranks (r=4 or r=8) for complex tasks that require substantial behavioral changes, leading to underfitting and poor performance despite successful training completion.\n\nThe problem manifests as training loss that plateaus early at a suboptimal level, validation metrics that show minimal improvement over the base model, and generated outputs that fail to demonstrate the desired task-specific behaviors. The model appears to be learning (loss decreases initially) but cannot capture the full complexity of the target task due to insufficient adapter capacity.\n\nTo avoid this pitfall, analyze the task complexity before selecting ranks. Tasks requiring significant behavioral changes (like learning new response formats, domain-specific knowledge, or complex reasoning patterns) typically need ranks of 32-64 or higher. Simple style adaptations or minor behavioral adjustments can work with lower ranks (8-16). Consider the amount of training data available - more data can support higher ranks without overfitting.\n\n⚠️ **Pitfall: Mismatched Alpha-Rank Scaling**\n\nA common mistake is using default alpha values (often α=32) regardless of the chosen rank, which can lead to training instability or ineffective learning. The effective learning rate for adapters scales as α/r, so using fixed alpha with varying ranks creates inconsistent training dynamics.\n\nLow-rank configurations with high alpha values can cause training instability, gradient explosions, and catastrophic forgetting of base model capabilities. High-rank configurations with low alpha values result in extremely slow adaptation, requiring many more training steps to achieve convergence, and potentially getting stuck in poor local minima.\n\nImplement proportional alpha scaling as a starting point: use α=2r for most tasks, then adjust based on training behavior. For conservative adaptation (preserving base model behavior), use α=r. For aggressive adaptation (significant behavior changes), use α=4r. Monitor training loss curves and gradient magnitudes to adjust the scaling if needed.\n\n⚠️ **Pitfall: Incorrect Target Module Identification**\n\nDevelopers sometimes manually specify target modules without understanding the model architecture, leading to adapter injection into inappropriate layers such as embedding layers, normalization layers, or output heads. This wastes parameters on layers that don't benefit from adaptation and misses critical transformation layers where adaptation would be most effective.\n\nSymptoms include unexpectedly high parameter counts for adapters, poor training efficiency (high loss relative to parameter count), and training instability due to adapting normalization or embedding layers. The system may appear to be working but achieves suboptimal results compared to properly targeted configurations.\n\nUse automatic target module detection as the default, especially when working with new model architectures. When manually specifying targets, focus on attention projection layers (`q_proj`, `k_proj`, `v_proj`, `o_proj`) and feed-forward layers (`up_proj`, `down_proj`). Avoid targeting embedding layers, normalization layers (`layer_norm`, `rms_norm`), or final output projections unless specifically needed for the task.\n\n⚠️ **Pitfall: Forgetting to Freeze Base Model Parameters**\n\nA critical error is failing to properly freeze the base model parameters, which defeats the primary purpose of parameter-efficient fine-tuning. This can happen due to incorrect model loading, improper PEFT configuration, or accidentally enabling gradients for base model weights during training setup.\n\nThe failure manifests as unexpectedly high GPU memory usage (similar to full fine-tuning), dramatically slower training due to computing gradients for billions of parameters, and potential overfitting or catastrophic forgetting as the entire model adapts to the limited fine-tuning dataset. Memory usage may exceed available GPU capacity even with quantization.\n\nVerify that base model parameters have `requires_grad=False` after adapter injection. Use the trainable parameter analysis to confirm that less than 2% of total parameters are trainable. Monitor GPU memory usage during training startup - it should be dramatically lower than full fine-tuning. If memory usage is unexpectedly high, check the parameter freezing configuration and PEFT setup.\n\n⚠️ **Pitfall: Incompatible Quantization and Adapter Precision**\n\nWhen combining LoRA with quantization, developers sometimes place adapters and base model weights on different devices or use incompatible data types, leading to runtime errors, extremely slow training due to data transfers, or numerical precision issues that affect training stability.\n\nProblems include CUDA out-of-memory errors despite quantization, very slow forward passes due to CPU-GPU transfers, and training instability due to precision mismatches between 4-bit base weights and float16 adapter computations. Error messages often mention device placement or tensor type mismatches.\n\nEnsure that adapters are placed on the same device as the quantized base model (typically GPU). Configure adapters to use the same compute precision as the quantization setup (usually float16 or bfloat16). Use the memory monitoring tools to verify that both base model and adapters are allocated on the GPU. Test a small forward pass before starting full training to catch device placement issues early.\n\n⚠️ **Pitfall: Overestimating Adapter Memory Efficiency**\n\nUsers sometimes expect that LoRA adapters eliminate memory constraints entirely, leading to configurations with excessive ranks, too many target modules, or batch sizes that exceed available memory when combined with optimizer states and activation memory during training.\n\nThe issue appears as out-of-memory errors during training (not model loading), unexpectedly slow training due to memory pressure, and batch size requirements that are much smaller than expected. The problem is often not apparent until training begins and optimizer states are allocated.\n\nRemember that memory efficiency comes from frozen base model weights, but adapters, optimizer states, and activations still consume significant memory. Use the memory analysis tools to estimate total memory requirements including optimizer states (typically 2x adapter parameters for Adam). Start with conservative batch sizes and increase gradually. Monitor memory usage throughout training, not just during model loading.\n\n### Implementation Guidance\n\nThis subsection provides concrete implementation patterns, starter code, and practical guidance for building the LoRA configuration component using Python and the HuggingFace ecosystem.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| LoRA Implementation | HuggingFace PEFT library with default configs | Custom PEFT integration with fine-tuned hyperparameters |\n| Target Detection | Manual module lists by model family | Automatic detection with architecture introspection |\n| Parameter Analysis | Basic parameter counting with PyTorch | Comprehensive analysis with memory profiling |\n| Configuration Management | YAML configs with validation | Dynamic configs with task-informed defaults |\n| Memory Monitoring | PyTorch CUDA memory stats | Detailed profiling with nvtx and custom trackers |\n\n#### Recommended File Structure\n\nThe LoRA configuration component fits into the overall pipeline structure as follows:\n\n```\nllm-finetuning-pipeline/\n├── src/\n│   ├── components/\n│   │   ├── lora_config/                    ← This component\n│   │   │   ├── __init__.py\n│   │   │   ├── target_detector.py          ← Module identification logic\n│   │   │   ├── adapter_manager.py          ← Adapter initialization and injection\n│   │   │   ├── parameter_analyzer.py       ← Trainable parameter analysis\n│   │   │   ├── rank_selector.py            ← Rank and alpha recommendation\n│   │   │   └── config_validator.py         ← Configuration validation\n│   │   ├── data_prep/                      ← From previous milestone\n│   │   └── quantization/                   ← Next milestone\n│   ├── config/\n│   │   ├── lora_presets.yaml              ← Common LoRA configurations\n│   │   └── model_architectures.yaml       ← Architecture-specific settings\n│   └── utils/\n│       ├── memory_utils.py                ← Memory monitoring utilities\n│       └── model_utils.py                 ← Model introspection helpers\n├── tests/\n│   ├── test_lora_config/\n│   │   ├── test_target_detection.py\n│   │   ├── test_adapter_injection.py\n│   │   └── test_parameter_analysis.py\n│   └── integration/\n│       └── test_lora_pipeline.py\n└── examples/\n    ├── lora_configuration_examples.py\n    └── custom_target_selection.py\n```\n\n#### Core Configuration Infrastructure\n\n```python\n# src/components/lora_config/__init__.py\n\"\"\"\nLoRA Configuration Component\n\nProvides automatic target module detection, adapter initialization,\nand parameter efficiency analysis for memory-efficient fine-tuning.\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Optional, List, Union, Dict, Any\nfrom pathlib import Path\nimport torch\nimport yaml\n\n@dataclass\nclass LoRAConfig:\n    \"\"\"Configuration for LoRA adapters with validation and defaults.\"\"\"\n    r: int = 16  # Rank of the low-rank decomposition\n    alpha: int = 32  # Scaling parameter for adapter outputs\n    dropout: float = 0.1  # Dropout applied to adapter outputs\n    target_modules: Optional[List[str]] = None  # Auto-detect if None\n    bias: str = \"none\"  # Whether to adapt bias parameters\n    task_type: str = \"CAUSAL_LM\"  # Task type for PEFT\n    lora_alpha_scaling: bool = True  # Whether to apply alpha/r scaling\n    init_lora_weights: Union[bool, str] = True  # Initialization strategy\n    \n    def __post_init__(self):\n        \"\"\"Validate configuration parameters.\"\"\"\n        if self.r <= 0 or self.r > 512:\n            raise ValueError(f\"Rank must be between 1 and 512, got {self.r}\")\n        if self.alpha <= 0:\n            raise ValueError(f\"Alpha must be positive, got {self.alpha}\")\n        if not 0 <= self.dropout <= 1:\n            raise ValueError(f\"Dropout must be between 0 and 1, got {self.dropout}\")\n    \n    def effective_alpha(self) -> float:\n        \"\"\"Calculate the effective alpha scaling factor.\"\"\"\n        if self.lora_alpha_scaling:\n            return self.alpha / self.r\n        return self.alpha\n    \n    @classmethod\n    def from_task_complexity(cls, complexity: str, available_memory_gb: float) -> 'LoRAConfig':\n        \"\"\"Create LoRA config based on task complexity and memory constraints.\"\"\"\n        # TODO: Implement task complexity analysis\n        # TODO: Calculate maximum feasible rank from memory constraints\n        # TODO: Recommend alpha based on complexity and rank\n        # TODO: Set appropriate dropout based on expected overfitting risk\n        pass\n    \n    def to_peft_config(self) -> Dict[str, Any]:\n        \"\"\"Convert to PEFT library configuration format.\"\"\"\n        # TODO: Transform to PEFT LoraConfig dictionary\n        # TODO: Handle target_modules conversion\n        # TODO: Apply library-specific parameter mappings\n        pass\n\n@dataclass\nclass AdapterMetrics:\n    \"\"\"Metrics for analyzing adapter parameter efficiency.\"\"\"\n    total_base_params: int\n    total_adapter_params: int\n    trainable_ratio: float\n    memory_overhead_mb: float\n    target_modules_count: int\n    effective_rank_utilization: Dict[str, float]\n    gradient_statistics: Dict[str, Dict[str, float]]\n```\n\n#### Target Module Detection\n\n```python\n# src/components/lora_config/target_detector.py\n\"\"\"\nAutomatic target module detection for LoRA adapter injection.\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nfrom typing import List, Dict, Set, Optional\nimport re\nfrom transformers import PreTrainedModel\n\nclass TargetModuleDetector:\n    \"\"\"Detects suitable target modules for LoRA adaptation.\"\"\"\n    \n    # Architecture-specific module patterns\n    ATTENTION_PATTERNS = [\n        r'.*\\.(?:q_proj|query)$',\n        r'.*\\.(?:k_proj|key)$', \n        r'.*\\.(?:v_proj|value)$',\n        r'.*\\.(?:o_proj|dense|out_proj)$'\n    ]\n    \n    MLP_PATTERNS = [\n        r'.*\\.(?:up_proj|dense_h_to_4h)$',\n        r'.*\\.(?:down_proj|dense_4h_to_h)$',\n        r'.*\\.(?:gate_proj)$'\n    ]\n    \n    def __init__(self, model: PreTrainedModel):\n        self.model = model\n        self.architecture = self._detect_architecture()\n        \n    def _detect_architecture(self) -> str:\n        \"\"\"Detect the model architecture family.\"\"\"\n        # TODO: Analyze model class and config to identify architecture\n        # TODO: Return standardized architecture name (llama, gpt, etc.)\n        pass\n    \n    def detect_attention_modules(self) -> List[str]:\n        \"\"\"Find all attention projection modules.\"\"\"\n        attention_modules = []\n        \n        # TODO: Iterate through model named modules\n        # TODO: Apply attention patterns to find matches\n        # TODO: Validate that matched modules are Linear layers\n        # TODO: Filter out inappropriate modules (embeddings, norms)\n        # TODO: Return sorted list of module names\n        \n        return attention_modules\n    \n    def detect_mlp_modules(self) -> List[str]:\n        \"\"\"Find all MLP/feed-forward modules.\"\"\"\n        mlp_modules = []\n        \n        # TODO: Iterate through model named modules  \n        # TODO: Apply MLP patterns to find matches\n        # TODO: Validate module types and dimensions\n        # TODO: Handle architecture-specific naming variations\n        # TODO: Return sorted list of module names\n        \n        return mlp_modules\n    \n    def get_recommended_targets(self, \n                              include_attention: bool = True,\n                              include_mlp: bool = False,\n                              max_modules: Optional[int] = None) -> List[str]:\n        \"\"\"Get recommended target modules based on strategy.\"\"\"\n        targets = []\n        \n        if include_attention:\n            targets.extend(self.detect_attention_modules())\n        \n        if include_mlp:\n            targets.extend(self.detect_mlp_modules())\n        \n        # TODO: Sort by priority (attention > output > mlp)\n        # TODO: Limit to max_modules if specified\n        # TODO: Validate that targets are compatible with LoRA\n        \n        return targets\n    \n    def analyze_module_dimensions(self, module_names: List[str]) -> Dict[str, Dict[str, int]]:\n        \"\"\"Analyze dimensions of target modules for rank planning.\"\"\"\n        dimensions = {}\n        \n        for name in module_names:\n            module = dict(self.model.named_modules())[name]\n            if isinstance(module, nn.Linear):\n                dimensions[name] = {\n                    'in_features': module.in_features,\n                    'out_features': module.out_features,\n                    'max_useful_rank': min(module.in_features, module.out_features),\n                    'parameter_count': module.in_features * module.out_features\n                }\n        \n        return dimensions\n```\n\n#### Adapter Management and Injection\n\n```python\n# src/components/lora_config/adapter_manager.py\n\"\"\"\nLoRA adapter initialization, injection, and management.\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nfrom typing import Dict, List, Optional, Tuple\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom transformers import PreTrainedModel\n\nclass AdapterManager:\n    \"\"\"Manages LoRA adapter lifecycle and integration.\"\"\"\n    \n    def __init__(self, model: PreTrainedModel, lora_config: LoRAConfig):\n        self.base_model = model\n        self.lora_config = lora_config\n        self.peft_model = None\n        \n    def inject_adapters(self) -> PreTrainedModel:\n        \"\"\"Inject LoRA adapters into the base model.\"\"\"\n        # Convert our config to PEFT format\n        peft_config = self._create_peft_config()\n        \n        # TODO: Validate target modules exist in model\n        # TODO: Check module dimensions for rank compatibility\n        # TODO: Apply PEFT model wrapping with adapters\n        # TODO: Verify adapter injection success\n        # TODO: Return PEFT-wrapped model\n        \n        pass\n    \n    def _create_peft_config(self) -> LoraConfig:\n        \"\"\"Convert internal config to PEFT LoraConfig.\"\"\"\n        # TODO: Map LoRAConfig fields to PEFT LoraConfig\n        # TODO: Handle target_modules auto-detection if None\n        # TODO: Set task_type appropriately\n        # TODO: Configure initialization strategy\n        \n        pass\n    \n    def freeze_base_parameters(self):\n        \"\"\"Ensure base model parameters are frozen.\"\"\"\n        if self.peft_model is None:\n            raise ValueError(\"Must inject adapters before freezing parameters\")\n            \n        # TODO: Iterate through base model parameters\n        # TODO: Set requires_grad=False for non-adapter params\n        # TODO: Verify only adapter parameters remain trainable\n        # TODO: Log parameter freeze statistics\n        \n        pass\n    \n    def get_adapter_state_dict(self) -> Dict[str, torch.Tensor]:\n        \"\"\"Extract only the adapter parameters.\"\"\"\n        if self.peft_model is None:\n            raise ValueError(\"No adapters injected\")\n            \n        # TODO: Filter state_dict to only adapter parameters\n        # TODO: Remove base model weights from dictionary\n        # TODO: Return clean adapter-only state dict\n        \n        pass\n    \n    def merge_and_unload(self) -> PreTrainedModel:\n        \"\"\"Merge adapter weights into base model and return standalone model.\"\"\"\n        if self.peft_model is None:\n            raise ValueError(\"No adapters to merge\")\n            \n        # TODO: Use PEFT merge_and_unload functionality\n        # TODO: Validate merged model produces same outputs\n        # TODO: Return standalone model without adapter modules\n        \n        pass\n    \n    def estimate_memory_usage(self) -> Dict[str, float]:\n        \"\"\"Estimate memory usage for adapters in MB.\"\"\"\n        if self.lora_config.target_modules is None:\n            # Need to detect targets first\n            from .target_detector import TargetModuleDetector\n            detector = TargetModuleDetector(self.base_model)\n            target_modules = detector.get_recommended_targets()\n        else:\n            target_modules = self.lora_config.target_modules\n            \n        # TODO: Calculate adapter parameter count per module\n        # TODO: Account for rank, input/output dimensions\n        # TODO: Include optimizer state memory (2x for Adam)\n        # TODO: Return memory breakdown dictionary\n        \n        pass\n```\n\n#### Parameter Analysis and Monitoring\n\n```python\n# src/components/lora_config/parameter_analyzer.py\n\"\"\"\nAnalysis and monitoring of parameter efficiency for LoRA adapters.\n\"\"\"\n\nimport torch\nfrom typing import Dict, List, Optional, Tuple\nimport numpy as np\nfrom transformers import PreTrainedModel\n\nclass ParameterAnalyzer:\n    \"\"\"Analyzes parameter efficiency and training dynamics for LoRA.\"\"\"\n    \n    def __init__(self, model: PreTrainedModel):\n        self.model = model\n        self.base_param_count = None\n        self.adapter_param_count = None\n        \n    def analyze_parameter_distribution(self) -> Dict[str, Dict[str, int]]:\n        \"\"\"Analyze distribution of trainable vs frozen parameters.\"\"\"\n        analysis = {\n            'base_model': {'total': 0, 'trainable': 0, 'frozen': 0},\n            'adapters': {'total': 0, 'trainable': 0, 'frozen': 0},\n            'other': {'total': 0, 'trainable': 0, 'frozen': 0}\n        }\n        \n        # TODO: Iterate through all named parameters\n        # TODO: Classify parameters as base, adapter, or other\n        # TODO: Count trainable vs frozen in each category\n        # TODO: Calculate memory usage for each category\n        # TODO: Return comprehensive analysis dictionary\n        \n        return analysis\n    \n    def calculate_efficiency_metrics(self) -> Dict[str, float]:\n        \"\"\"Calculate parameter efficiency metrics.\"\"\"\n        param_analysis = self.analyze_parameter_distribution()\n        \n        # TODO: Calculate trainable parameter ratio\n        # TODO: Calculate memory efficiency ratio\n        # TODO: Calculate computational efficiency estimates\n        # TODO: Compare against full fine-tuning baseline\n        \n        return {}\n    \n    def analyze_gradient_statistics(self) -> Dict[str, Dict[str, float]]:\n        \"\"\"Analyze gradient statistics for adapter parameters.\"\"\"\n        if not any(p.requires_grad for p in self.model.parameters()):\n            raise ValueError(\"No trainable parameters found\")\n            \n        gradient_stats = {}\n        \n        # TODO: Collect gradients from adapter parameters only\n        # TODO: Calculate magnitude statistics (mean, std, max, min)\n        # TODO: Analyze gradient-to-parameter ratios\n        # TODO: Detect gradient vanishing/explosion patterns\n        # TODO: Group statistics by adapter module type\n        \n        return gradient_stats\n    \n    def estimate_effective_rank(self) -> Dict[str, float]:\n        \"\"\"Estimate the effective rank utilization of adapters.\"\"\"\n        effective_ranks = {}\n        \n        # TODO: Extract adapter A and B matrices\n        # TODO: Compute SVD to analyze rank utilization\n        # TODO: Calculate effective rank as ratio of significant singular values\n        # TODO: Compare against configured rank to identify under-utilization\n        \n        return effective_ranks\n    \n    def generate_efficiency_report(self) -> str:\n        \"\"\"Generate a comprehensive efficiency analysis report.\"\"\"\n        param_dist = self.analyze_parameter_distribution()\n        efficiency = self.calculate_efficiency_metrics()\n        \n        # TODO: Format parameter distribution statistics\n        # TODO: Include memory usage comparisons\n        # TODO: Add recommendations for optimization\n        # TODO: Generate human-readable report string\n        \n        return \"Parameter efficiency report placeholder\"\n```\n\n#### Rank Selection and Optimization\n\n```python\n# src/components/lora_config/rank_selector.py\n\"\"\"\nIntelligent rank and alpha parameter selection for LoRA configuration.\n\"\"\"\n\nfrom typing import Dict, Tuple, Optional\nimport torch\nfrom transformers import PreTrainedModel\n\nclass RankSelector:\n    \"\"\"Provides intelligent rank and alpha selection for LoRA configs.\"\"\"\n    \n    TASK_COMPLEXITY_RANKS = {\n        'simple_style': (4, 8),      # Style adaptation, formatting\n        'instruction_following': (16, 32),  # General instruction tuning\n        'domain_adaptation': (32, 64),      # Domain-specific knowledge\n        'complex_reasoning': (64, 128),     # Mathematical, logical reasoning\n        'code_generation': (32, 64),       # Programming tasks\n        'creative_writing': (16, 32),      # Creative text generation\n    }\n    \n    def __init__(self, model: PreTrainedModel, available_memory_gb: float):\n        self.model = model\n        self.available_memory = available_memory_gb\n        self.model_size = self._estimate_model_size()\n        \n    def _estimate_model_size(self) -> float:\n        \"\"\"Estimate model size in billions of parameters.\"\"\"\n        total_params = sum(p.numel() for p in self.model.parameters())\n        return total_params / 1e9\n    \n    def recommend_rank_for_task(self, task_type: str, dataset_size: int) -> Tuple[int, int]:\n        \"\"\"Recommend rank and alpha for a specific task.\"\"\"\n        # TODO: Look up base rank range for task type\n        # TODO: Adjust for dataset size (more data supports higher rank)\n        # TODO: Consider model size (larger models can utilize higher ranks)\n        # TODO: Apply memory constraints to limit maximum rank\n        # TODO: Return (recommended_rank, recommended_alpha) tuple\n        \n        pass\n    \n    def analyze_memory_constraints(self, rank: int, target_modules: List[str]) -> Dict[str, float]:\n        \"\"\"Analyze memory requirements for given rank and targets.\"\"\"\n        # TODO: Calculate adapter parameter count for each target module\n        # TODO: Estimate memory for adapter weights (fp16/fp32)\n        # TODO: Estimate optimizer state memory (Adam = 2x parameters)\n        # TODO: Include activation memory estimates for training\n        # TODO: Return memory breakdown and total requirements\n        \n        pass\n    \n    def find_optimal_rank(self, \n                         task_type: str,\n                         target_modules: List[str],\n                         max_memory_usage: float = 0.8) -> Tuple[int, int, Dict[str, float]]:\n        \"\"\"Find optimal rank within memory constraints.\"\"\"\n        # TODO: Start with task-based recommendation\n        # TODO: Binary search to find maximum feasible rank\n        # TODO: Validate memory usage under threshold\n        # TODO: Select appropriate alpha for chosen rank\n        # TODO: Return (rank, alpha, memory_analysis)\n        \n        pass\n    \n    def validate_configuration(self, rank: int, alpha: int, target_modules: List[str]) -> bool:\n        \"\"\"Validate that a rank/alpha configuration is feasible.\"\"\"\n        # TODO: Check rank against module dimensions\n        # TODO: Verify memory requirements are within limits\n        # TODO: Validate alpha/rank ratio for training stability\n        # TODO: Return True if configuration is valid\n        \n        pass\n```\n\n#### Milestone Checkpoint\n\nAfter implementing the LoRA configuration component, verify the following behavior:\n\n**Unit Test Validation:**\n```bash\ncd llm-finetuning-pipeline\npython -m pytest tests/test_lora_config/ -v\n```\n\n**Expected Output:**\n- All target module detection tests pass\n- Adapter injection creates correct parameter counts\n- Memory analysis shows <2% trainable parameters\n- Rank selection respects memory constraints\n\n**Manual Verification:**\n```python\n# Test with a small model (e.g., DistilBERT or small Llama)\nfrom src.components.lora_config import LoRAConfig, AdapterManager, TargetModuleDetector\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n\n# Test automatic target detection\ndetector = TargetModuleDetector(model)\ntargets = detector.get_recommended_targets()\nprint(f\"Found {len(targets)} target modules: {targets[:3]}...\")\n\n# Test adapter injection\nlora_config = LoRAConfig(r=16, alpha=32, target_modules=targets)\nadapter_manager = AdapterManager(model, lora_config)\npeft_model = adapter_manager.inject_adapters()\n\n# Verify parameter efficiency\nfrom src.components.lora_config import ParameterAnalyzer\nanalyzer = ParameterAnalyzer(peft_model)\nefficiency = analyzer.calculate_efficiency_metrics()\nprint(f\"Trainable parameter ratio: {efficiency['trainable_ratio']:.2%}\")\n\n# Should see <2% trainable parameters and successful adapter injection\n```\n\n**Signs of Success:**\n- Target detection finds appropriate attention/MLP modules\n- Adapter injection completes without errors\n- Parameter analysis shows dramatic efficiency gains\n- Memory usage is much lower than full fine-tuning\n- Model forward pass works correctly with adapters\n\n**Common Issues to Debug:**\n- Module name mismatches between architectures\n- Memory allocation failures during injection\n- Incorrect parameter freezing (too many trainable params)\n- PEFT library version compatibility issues\n\n\n## QLoRA Quantization Component\n\n> **Milestone(s):** Milestone 3 - this section implements the 4-bit quantization system using NormalFloat format, double quantization strategies, and mixed-precision training coordination that enables fine-tuning of large language models within consumer hardware memory constraints.\n\nThe QLoRA quantization component represents the most aggressive memory optimization strategy in our fine-tuning pipeline, enabling models that would normally require 80GB of VRAM to fit comfortably within 16GB consumer hardware. This component implements the cutting-edge quantization techniques introduced in the QLoRA paper, combining 4-bit NormalFloat quantization with double quantization and mixed-precision training to achieve unprecedented memory efficiency without catastrophic quality degradation.\n\n### Mental Model: The Compression Expert\n\nThink of the quantization component as a **compression expert** working in a high-end photography studio. The expert receives massive RAW image files from photographers (our 32-bit floating point model weights) and must compress them to fit on limited storage devices (GPU VRAM) while preserving the essential visual information needed for professional printing (model inference quality).\n\nThe compression expert uses three sophisticated techniques. First, they analyze the histogram of pixel values and discover that most photographs have a roughly normal distribution of brightness values - this insight leads them to use a **specialized compression format** (NormalFloat) that allocates bits more efficiently for typical photographic content rather than using a generic compression algorithm. Second, they apply **nested compression** (double quantization) by compressing even the compression metadata itself, squeezing out every possible byte. Finally, during editing workflows, they work with **high-quality intermediate files** (float16 computation) while keeping the compressed originals in storage (4-bit weights), only decompressing sections as needed for active editing.\n\nThis analogy captures the core insight of QLoRA: by understanding the statistical properties of neural network weights and carefully orchestrating when to use different precisions, we can achieve 75% memory reduction while maintaining the essential information needed for effective fine-tuning.\n\n### NormalFloat 4-bit Quantization\n\nThe foundation of our memory optimization strategy rests on **NormalFloat 4-bit quantization**, a technique that exploits the statistical properties of neural network weight distributions to achieve superior compression ratios compared to uniform quantization schemes. Traditional 4-bit quantization divides the weight range uniformly into 16 buckets, but this approach wastes precious bits on extreme values that rarely occur in practice while providing insufficient precision for the common values near zero where most neural network weights cluster.\n\nNormalFloat quantization solves this problem by pre-computing optimal quantization levels based on the assumption that neural network weights follow approximately normal distributions. The `NF4_QUANTIZATION` format allocates more quantization levels to values near zero and fewer levels to extreme values, matching the actual distribution of weights found in transformer models. This distributional awareness allows NF4 to represent the same weight tensor with higher effective precision using only 4 bits per parameter.\n\nThe quantization process begins with **statistical analysis** of the weight tensor to determine appropriate scaling factors and zero points. Our `QuantizationConfig` captures the essential parameters that control this process:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `load_in_4bit` | `bool` | Primary flag enabling 4-bit quantization during model loading |\n| `bnb_4bit_quant_type` | `str` | Quantization format, typically \"nf4\" for NormalFloat |\n| `bnb_4bit_compute_dtype` | `str` | Data type for forward pass computations (fp16/bf16) |\n| `bnb_4bit_use_double_quant` | `bool` | Enable quantization of quantization constants |\n| `bnb_4bit_quant_storage` | `str` | Storage format for quantized weights on disk |\n\nThe quantization component implements a **two-phase loading strategy** that first loads the full-precision model into system memory, applies the NF4 quantization transformation, and then moves the compressed weights to GPU memory. This approach ensures that quantization happens with full precision arithmetic, avoiding the quality degradation that would result from quantizing already-quantized weights.\n\nDuring the **quantization transformation**, each weight tensor undergoes the following process: the component calculates tensor-wise statistics to determine optimal scaling factors, applies the NF4 quantization mapping to convert 32-bit floats to 4-bit indices, stores the quantization metadata (scales and zero points) alongside the compressed indices, and registers dequantization callbacks that will be triggered during forward passes.\n\n> **Critical Design Insight**: The choice of NF4 over uniform quantization isn't just about better compression - it's about preserving the gradient flow characteristics that enable effective fine-tuning. Uniform quantization can create artificial \"quantization barriers\" that interfere with gradient-based optimization, while NF4's distribution-aware approach maintains smoother optimization landscapes.\n\nThe quantization component provides **quality monitoring** capabilities that track the approximation error introduced by 4-bit compression. The `MemoryMonitor` component measures both the memory savings achieved and the statistical properties of the quantization error, enabling developers to validate that their specific model and dataset combination maintains acceptable quality under 4-bit compression.\n\n### Double Quantization Strategy\n\nBuilding upon the NF4 foundation, **double quantization** represents the next level of memory optimization by applying quantization recursively to the quantization metadata itself. While NF4 quantization compresses the weight values to 4 bits, the scaling factors and zero points required for dequantization are typically stored in full 32-bit precision. For large models, these quantization constants can consume significant memory - double quantization addresses this by quantizing the constants themselves.\n\nThe double quantization process operates in **hierarchical layers**. The first quantization level compresses the original weight tensors using NF4, producing 4-bit indices and 32-bit quantization constants. The second quantization level then analyzes the distribution of these quantization constants across the model and applies another round of NF4 quantization to compress them further. This creates a nested structure where dequantization requires two levels of lookup and scaling operations.\n\nOur implementation enables double quantization through the `bnb_4bit_use_double_quant` flag in the `QuantizationConfig`. When enabled, the component performs additional analysis during model loading to identify patterns in the quantization constants and determine optimal secondary quantization parameters.\n\nThe **memory savings calculation** for double quantization follows a predictable pattern. For a model with `N` parameters, standard NF4 quantization requires `N/2` bytes for compressed weights plus approximately `N/2048` bytes for quantization constants (assuming block-wise quantization with 1024-element blocks). Double quantization further compresses the constants by roughly 75%, yielding total memory usage of approximately `N/2 + N/8192` bytes - a marginal but meaningful improvement for billion-parameter models.\n\nHowever, double quantization introduces **computational overhead** during the forward pass. Each dequantization operation now requires two sequential lookup and scaling steps, increasing the latency of memory-bound operations. Our implementation provides detailed profiling capabilities to measure this overhead and determine whether the memory savings justify the computational cost for specific deployment scenarios.\n\n| Quantization Level | Memory Usage | Dequant Overhead | Recommended For |\n|-------------------|--------------|------------------|-----------------|\n| None | 100% | 0% | Unlimited VRAM |\n| NF4 Only | ~25% | Low | Most use cases |\n| Double Quantized | ~23% | Medium | Extreme memory constraints |\n\nThe double quantization component implements **adaptive block sizing** that analyzes the distribution characteristics of quantization constants to determine optimal grouping strategies. Rather than using fixed block sizes, it can dynamically adjust the granularity of secondary quantization to minimize approximation error while maximizing memory savings.\n\n> **Decision: Double Quantization by Default**\n> - **Context**: Double quantization provides additional memory savings with modest computational overhead, but adds complexity to the dequantization path and debugging process.\n> - **Options Considered**: Always enable, never enable, or make it configurable with smart defaults\n> - **Decision**: Enable by default for 4-bit quantization but provide explicit configuration control\n> - **Rationale**: The memory savings (2-3% additional reduction) are meaningful for edge deployment scenarios, and the computational overhead is acceptable for fine-tuning workloads where memory efficiency is prioritized over raw inference speed\n> - **Consequences**: Slightly more complex error handling and debugging, but enables fine-tuning of larger models on the same hardware\n\n### Mixed-Precision Training Setup\n\nThe quantization component's most sophisticated capability lies in orchestrating **mixed-precision training** that seamlessly combines 4-bit weight storage with higher-precision computation. This approach addresses the fundamental challenge of quantized fine-tuning: while 4-bit weights save enormous amounts of memory, performing gradient computations in 4-bit precision would catastrophically degrade training stability and convergence quality.\n\nMixed-precision training operates on the principle of **precision specialization** - using the minimum precision necessary for each operation while maintaining overall training effectiveness. Weights remain stored in 4-bit NF4 format to minimize memory consumption, but forward pass computations dynamically dequantize weights to float16 or bfloat16 precision for stable numerical operations. Gradients are computed and accumulated in the higher precision format, then the LoRA adapter updates are applied without ever modifying the quantized base model weights.\n\nThe precision orchestration happens through **automatic dequantization hooks** registered during model loading. When a quantized linear layer receives input during the forward pass, it triggers the following sequence: dequantize the 4-bit weights to the configured compute dtype (fp16/bf16), perform the matrix multiplication using the higher-precision weights and activations, and discard the temporary dequantized weights to free memory immediately after the operation.\n\nOur `QuantizationConfig` provides precise control over the compute precision through the `bnb_4bit_compute_dtype` field:\n\n| Compute Dtype | Memory Usage | Numerical Stability | Hardware Support |\n|---------------|--------------|-------------------|------------------|\n| `float16` | Lower | Good | Universal |\n| `bfloat16` | Lower | Better | Modern GPUs only |\n| `float32` | Higher | Best | Universal |\n\nThe **gradient flow** in mixed-precision quantized training follows a carefully orchestrated path that ensures training stability while maintaining memory efficiency. During backpropagation, gradients flow through the higher-precision forward pass computations, accumulate in the optimizer state (typically float32), and apply updates only to the LoRA adapter parameters. The quantized base model weights remain frozen throughout training, eliminating the need to maintain optimizer states for the massive base model parameters.\n\nThis design creates a **memory usage profile** that scales primarily with the adapter size rather than the base model size. A 7B parameter model with 4-bit quantization and rank-16 LoRA adapters requires approximately 4.5GB for quantized weights, 0.1GB for adapter parameters and optimizer states, and 2-4GB for activation storage during training - fitting comfortably within 8GB VRAM while maintaining training effectiveness comparable to full-precision fine-tuning.\n\nThe component implements **automatic mixed-precision configuration** that selects optimal compute dtypes based on hardware capabilities and training requirements. Modern Ampere and newer GPUs benefit from bfloat16 computation due to better numerical stability and hardware acceleration, while older hardware defaults to float16 to ensure compatibility.\n\n> **Critical Implementation Detail**: The timing of dequantization operations is crucial for memory efficiency. Our implementation uses a \"just-in-time\" dequantization strategy that decompresses weights immediately before use and discards the decompressed values immediately after, rather than maintaining persistent caches that would defeat the memory savings of quantization.\n\n### Memory Usage Monitoring\n\nThe quantization component integrates comprehensive **memory usage monitoring** through the `MemoryMonitor` system that tracks GPU and system memory consumption across all stages of model loading, quantization, and training. This monitoring capability serves both operational and analytical purposes - providing real-time feedback during training and generating detailed reports for optimization and debugging.\n\nMemory monitoring operates through **strategic measurement points** that capture memory usage at critical transitions in the quantization pipeline. The `MemoryMonitor` automatically captures baseline measurements before model loading, tracks memory allocation during quantization transformation, monitors peak usage during training, and analyzes memory efficiency metrics throughout the fine-tuning process.\n\nThe monitoring system maintains detailed memory metrics through structured data collection:\n\n| Metric Category | Measurements | Purpose |\n|-----------------|-------------|---------|\n| GPU Allocated | PyTorch tensor memory | Active model and gradient storage |\n| GPU Cached | CUDA memory pool | Available for immediate allocation |\n| GPU Reserved | Total CUDA allocation | Maximum memory claimed from system |\n| System Memory | Process RSS/VMS | Host memory for data loading and processing |\n| Quantization Savings | Before/after comparison | Compression effectiveness validation |\n\nThe `MemoryMonitor.measure_current_usage()` method provides **real-time memory profiling** that can be called at any point during training to capture current memory state. This enables detection of memory leaks, validation of quantization effectiveness, and identification of memory bottlenecks that could impact training stability.\n\nThe monitoring component implements **memory efficiency analysis** that calculates and reports key optimization metrics. The quantization compression ratio compares pre and post-quantization memory usage to validate expected savings. Parameter efficiency metrics show the ratio of trainable adapter parameters to total model parameters. Peak memory tracking identifies the maximum memory usage across the entire training run for capacity planning.\n\nMemory monitoring data feeds into **automated optimization recommendations** that suggest configuration adjustments based on observed usage patterns. If peak memory usage approaches hardware limits, the system recommends reducing batch size or gradient accumulation steps. If quantization savings are lower than expected, it suggests investigating model architecture compatibility with 4-bit compression.\n\nThe component provides **memory usage visualization** through integration with monitoring frameworks like Weights & Biases. Memory consumption graphs show the impact of each optimization technique, enabling practitioners to understand the memory budget allocation between base model storage, adapter parameters, optimizer states, and activation caching.\n\n| Memory Category | Typical 7B Model (4-bit) | Percentage | Optimization Opportunity |\n|-----------------|---------------------------|------------|-------------------------|\n| Quantized Weights | 4.5GB | 60% | Already optimized |\n| Activations | 2.0GB | 27% | Gradient checkpointing |\n| Adapter Params | 0.1GB | 1% | Rank tuning |\n| Optimizer State | 0.2GB | 3% | 8-bit optimizers |\n| System Overhead | 0.7GB | 9% | OS/driver optimization |\n\n> **Monitoring Best Practice**: Memory measurements should be taken at consistent points in the training loop (e.g., after optimizer steps) to ensure comparable readings. GPU memory allocation can be highly dynamic during forward/backward passes, making timing-sensitive measurements unreliable for optimization decisions.\n\n### Architecture Decision Records\n\nThe quantization component embodies several critical architectural decisions that balance memory efficiency, training stability, and implementation complexity. These decisions were made based on extensive experimentation with large language models and analysis of real-world deployment constraints.\n\n> **Decision: NormalFloat Over Uniform Quantization**\n> - **Context**: 4-bit quantization requires choosing between uniform quantization (equal spacing) and distribution-aware quantization schemes like NormalFloat that allocate precision based on weight distributions\n> - **Options Considered**: Uniform INT4, NormalFloat (NF4), and dynamic range quantization\n> - **Decision**: Implement NormalFloat as the primary quantization format\n> - **Rationale**: Neural network weights exhibit approximately normal distributions, making NF4's distribution-aware bit allocation 2-3x more effective at preserving model quality compared to uniform quantization for the same bit budget\n> - **Consequences**: Higher implementation complexity but significantly better quality preservation, enabling fine-tuning of larger models without catastrophic performance degradation\n\n| Option | Quality Loss | Implementation | Memory Usage | Hardware Support |\n|--------|--------------|----------------|--------------|------------------|\n| Uniform INT4 | High | Simple | 4 bits/param | Universal |\n| NormalFloat | Low | Complex | 4 bits/param | bitsandbytes only |\n| Dynamic Range | Medium | Medium | 4-8 bits/param | Limited |\n\n> **Decision: Just-in-Time Dequantization Strategy**\n> - **Context**: Quantized weights must be dequantized for computation, but keeping decompressed weights in memory defeats the purpose of quantization\n> - **Options Considered**: Persistent dequantized cache, just-in-time dequantization, and hybrid caching strategies\n> - **Decision**: Implement just-in-time dequantization with immediate disposal of decompressed weights\n> - **Rationale**: Maintains maximum memory efficiency by keeping weights in 4-bit format except during actual computation, despite the computational overhead of repeated dequantization\n> - **Consequences**: Slight performance impact from repeated dequantization operations, but enables training of larger models by maintaining consistent memory footprint\n\n> **Decision: Mixed-Precision Training with Configurable Compute Dtype**\n> - **Context**: Training stability requires higher precision than 4-bit for gradient computation, but full float32 reduces memory benefits\n> - **Options Considered**: Fixed float16 computation, fixed bfloat16 computation, or configurable compute precision\n> - **Decision**: Implement configurable compute dtype with intelligent hardware-based defaults\n> - **Rationale**: Different hardware generations have varying capabilities for mixed-precision computation, and some models benefit more from bfloat16's extended range versus float16's precision\n> - **Consequences**: More complex configuration surface but optimal performance across diverse hardware environments\n\n> **Decision: Automatic Double Quantization with Override**\n> - **Context**: Double quantization provides additional memory savings but adds computational overhead and implementation complexity\n> - **Options Considered**: Never use double quantization, always enable it, or make it configurable with smart defaults\n> - **Decision**: Enable double quantization by default but provide explicit configuration override\n> - **Rationale**: The 2-3% additional memory savings are meaningful for edge deployment scenarios, and the computational overhead is acceptable for fine-tuning workloads that prioritize memory efficiency\n> - **Consequences**: Slight additional complexity in error handling and debugging, but enables training of larger models on severely memory-constrained hardware\n\n### Common Pitfalls\n\nThe quantization component introduces several subtle failure modes that can cause memory issues, training instability, or compatibility problems. Understanding these pitfalls is essential for successful deployment of quantized fine-tuning systems.\n\n⚠️ **Pitfall: CUDA Version Incompatibility with bitsandbytes**\n\nThe most common quantization failure occurs when the installed CUDA version doesn't match the bitsandbytes library's compiled CUDA version. This manifests as cryptic import errors or runtime crashes during model loading, often with unhelpful error messages about missing CUDA functions.\n\nThe root cause lies in bitsandbytes' use of custom CUDA kernels for quantization operations. These kernels are compiled against specific CUDA versions and cannot load if the runtime CUDA version differs significantly. The issue is particularly common in environments where CUDA was upgraded after installing bitsandbytes via pip.\n\nTo diagnose this issue, check the CUDA version used to compile bitsandbytes using `python -c \"import bitsandbytes; print(bitsandbytes.cuda_setup.main())\"` and compare it against the system CUDA version via `nvidia-smi`. If they differ, reinstall bitsandbytes from source or use conda to ensure version compatibility.\n\n⚠️ **Pitfall: Optimizer State Memory Explosion**\n\nDespite quantizing model weights to 4-bit, training can still exhaust GPU memory due to **optimizer state memory explosion**. Standard optimizers like Adam maintain momentum and variance estimates in full float32 precision for every trainable parameter, which can consume more memory than the quantized weights themselves.\n\nThis pitfall is particularly insidious because it manifests after training begins successfully, causing out-of-memory errors during the first optimizer step rather than during model loading. The memory explosion occurs because optimizer states are allocated only when gradients are first computed, creating a delayed failure mode.\n\nThe solution involves using memory-efficient optimizers like 8-bit Adam (`adamw_bnb_8bit`) that quantize optimizer states, or ensuring that only LoRA adapter parameters receive gradients while base model parameters remain frozen. Our implementation automatically freezes base model parameters during quantized training to prevent this issue.\n\n⚠️ **Pitfall: Mixing Quantized and Unquantized Model Components**\n\nLoading a pre-quantized model checkpoint and attempting to apply additional quantization leads to **double quantization errors** that can corrupt weights and cause training failure. This occurs when saved checkpoints already contain 4-bit quantized weights but the loading process attempts to apply quantization again.\n\nThe issue manifests as models that load successfully but produce nonsensical outputs or fail to converge during training. Weight histograms will show unnatural distributions with discrete jumps rather than smooth curves, indicating corrupted quantization.\n\nPrevention requires careful checkpoint metadata management that records the quantization state of saved models. Our implementation stores quantization configuration alongside model weights and validates compatibility during loading to prevent double quantization scenarios.\n\n⚠️ **Pitfall: Insufficient Compute Dtype Precision**\n\nSetting `bnb_4bit_compute_dtype` to `float16` on older GPUs or with models that have large dynamic ranges can cause **numerical instability** manifesting as loss spikes, gradient explosions, or training divergence. This occurs because float16's limited range (±65,504) and precision cannot handle the full range of values encountered during forward passes of large models.\n\nThe failure mode typically appears as intermittent loss spikes rather than consistent poor performance, making it difficult to diagnose. Gradient norms may show periodic extreme values indicating overflow conditions in the float16 computation path.\n\nThe solution involves upgrading to `bfloat16` if hardware supports it, or falling back to `float32` compute dtype for maximum stability. Our implementation provides automatic dtype selection based on hardware capabilities, but manual override may be necessary for problematic models.\n\n⚠️ **Pitfall: Inadequate Memory Budget for Quantization Overhead**\n\nQuantization introduces **temporary memory overhead** during model loading that can cause out-of-memory errors even when the final quantized model would fit in available memory. This occurs because the quantization process briefly requires both the original float32 weights and the 4-bit compressed weights in memory simultaneously.\n\nThe memory spike happens during the quantization transformation phase and can be 2-3x larger than the final memory footprint. Systems that precisely allocate memory based on expected quantized model size may fail during this transitional phase.\n\nPrevention requires reserving additional memory headroom during quantization (typically 20-30% beyond the expected final memory usage) and implementing gradual quantization strategies that process the model in chunks rather than loading everything simultaneously.\n\n![Memory Optimization Stack](./diagrams/memory-optimization-layers.svg)\n\n### Implementation Guidance\n\nThis section provides concrete implementation patterns for building the QLoRA quantization component, focusing on the bitsandbytes integration, memory monitoring infrastructure, and mixed-precision training coordination.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Quantization Backend | bitsandbytes with defaults | Custom CUDA kernels with hardware-specific optimization |\n| Memory Monitoring | PyTorch memory stats | NVIDIA ML-Python integration with detailed profiling |\n| Configuration Management | Python dataclasses | Hydra/OmegaConf for hierarchical configuration |\n| Compute Dtype Selection | Fixed float16 | Hardware-aware automatic dtype selection |\n\n#### Recommended File Structure\n\n```\nproject-root/\n  src/fine_tuning/\n    quantization/\n      __init__.py                 ← component exports\n      config.py                   ← QuantizationConfig and related types\n      memory_monitor.py           ← MemoryMonitor implementation\n      quantized_model_loader.py   ← model loading with quantization\n      mixed_precision_trainer.py  ← training coordination\n      utils.py                    ← quantization utilities and helpers\n    config/\n      quantization_presets.yaml   ← pre-configured quantization settings\n  tests/\n    test_quantization_config.py   ← configuration validation tests\n    test_memory_monitoring.py     ← memory tracking functionality tests\n    test_quantized_loading.py     ← model loading integration tests\n```\n\n#### Quantization Configuration Infrastructure\n\n```python\nfrom dataclasses import dataclass\nfrom typing import Optional, Dict, Any\nimport torch\nimport yaml\nfrom pathlib import Path\n\n@dataclass\nclass QuantizationConfig:\n    \"\"\"Configuration for 4-bit quantization with bitsandbytes integration.\"\"\"\n    \n    load_in_4bit: bool = True\n    bnb_4bit_quant_type: str = \"nf4\"  # NormalFloat quantization\n    bnb_4bit_compute_dtype: str = \"float16\"  # Computation precision\n    bnb_4bit_use_double_quant: bool = True  # Quantize quantization constants\n    bnb_4bit_quant_storage: str = \"uint8\"  # Storage format for quantized weights\n    \n    def to_bitsandbytes_config(self) -> Dict[str, Any]:\n        \"\"\"Convert to bitsandbytes BitsAndBytesConfig format.\"\"\"\n        # TODO 1: Import BitsAndBytesConfig from bitsandbytes\n        # TODO 2: Map compute_dtype string to torch dtype\n        # TODO 3: Return configured BitsAndBytesConfig object\n        # Hint: torch.float16, torch.bfloat16 are the typical compute dtypes\n        pass\n    \n    def estimate_memory_reduction(self, base_model_params: int) -> Dict[str, float]:\n        \"\"\"Estimate memory savings from quantization.\"\"\"\n        # TODO 1: Calculate original model memory (params * 4 bytes for fp32)\n        # TODO 2: Calculate quantized memory (params * 0.5 bytes for 4-bit)\n        # TODO 3: Account for quantization constant overhead\n        # TODO 4: Return dict with original_mb, quantized_mb, reduction_ratio\n        pass\n    \n    @classmethod\n    def from_model_size(cls, model_size_gb: float, available_memory_gb: float) -> 'QuantizationConfig':\n        \"\"\"Create optimized config based on model size and available memory.\"\"\"\n        # TODO 1: Determine if quantization is necessary based on memory ratio\n        # TODO 2: Enable double quantization for very large models\n        # TODO 3: Select compute dtype based on available memory headroom\n        # TODO 4: Return configured QuantizationConfig instance\n        pass\n\ndef detect_optimal_compute_dtype() -> str:\n    \"\"\"Detect the best compute dtype for current hardware.\"\"\"\n    # TODO 1: Check CUDA capability version\n    # TODO 2: Test bfloat16 support with torch.cuda.is_bf16_supported()\n    # TODO 3: Return \"bfloat16\" for modern GPUs, \"float16\" for older ones\n    # Hint: Ampere (8.0) and newer support efficient bfloat16\n    pass\n\ndef validate_quantization_compatibility(model_name: str) -> bool:\n    \"\"\"Validate that model architecture supports bitsandbytes quantization.\"\"\"\n    # TODO 1: Load model config from HuggingFace\n    # TODO 2: Check architecture type (llama, mistral, etc.)\n    # TODO 3: Verify bitsandbytes supports this architecture\n    # TODO 4: Return compatibility boolean\n    pass\n```\n\n#### Memory Monitoring System\n\n```python\nimport torch\nimport psutil\nimport time\nfrom typing import Dict, List, Optional\nfrom dataclasses import dataclass, field\n\n@dataclass\nclass MemoryMetrics:\n    \"\"\"Memory usage snapshot at a specific point in time.\"\"\"\n    stage: str\n    gpu_memory_allocated: float  # MB\n    gpu_memory_cached: float     # MB \n    gpu_memory_reserved: float   # MB\n    system_memory_used: float    # MB\n    timestamp: float\n    details: Optional[Dict[str, Any]] = None\n\n@dataclass\nclass MemoryMonitor:\n    \"\"\"Tracks GPU and system memory usage throughout quantization and training.\"\"\"\n    \n    baseline_gpu_memory: Optional[int] = None\n    baseline_system_memory: int = 0\n    measurements: List[Dict] = field(default_factory=list)\n    \n    def capture_baseline(self) -> None:\n        \"\"\"Record initial memory state before model loading.\"\"\"\n        # TODO 1: Clear GPU memory cache with torch.cuda.empty_cache()\n        # TODO 2: Record GPU memory allocated/cached/reserved\n        # TODO 3: Record system memory usage with psutil.Process().memory_info()\n        # TODO 4: Store baseline values in instance variables\n        pass\n    \n    def measure_current_usage(self, stage: str) -> Dict[str, float]:\n        \"\"\"Measure current memory usage and return statistics.\"\"\"\n        # TODO 1: Get current GPU memory stats from torch.cuda\n        # TODO 2: Convert bytes to MB for readability\n        # TODO 3: Get current system memory usage\n        # TODO 4: Calculate deltas from baseline if available\n        # TODO 5: Store measurement in self.measurements list\n        # TODO 6: Return current usage dict\n        pass\n    \n    def get_peak_usage(self) -> Dict[str, float]:\n        \"\"\"Return peak memory usage across all measurements.\"\"\"\n        # TODO 1: Iterate through all measurements in self.measurements\n        # TODO 2: Find maximum values for each memory category\n        # TODO 3: Return dict with peak values and when they occurred\n        pass\n    \n    def estimate_quantization_savings(self, before_stage: str, after_stage: str) -> Dict[str, float]:\n        \"\"\"Calculate memory savings achieved by quantization.\"\"\"\n        # TODO 1: Find measurements matching before_stage and after_stage\n        # TODO 2: Calculate absolute and relative memory reductions\n        # TODO 3: Return savings statistics\n        pass\n    \n    def generate_memory_report(self) -> str:\n        \"\"\"Generate human-readable memory usage report.\"\"\"\n        # TODO 1: Format baseline and peak usage statistics\n        # TODO 2: Calculate quantization effectiveness metrics\n        # TODO 3: Include recommendations for memory optimization\n        # TODO 4: Return formatted report string\n        pass\n\ndef setup_memory_monitoring() -> MemoryMonitor:\n    \"\"\"Initialize memory monitoring with baseline measurements.\"\"\"\n    monitor = MemoryMonitor()\n    monitor.capture_baseline()\n    return monitor\n```\n\n#### Quantized Model Loading\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom typing import Tuple, Optional\nimport gc\n\nclass QuantizedModelLoader:\n    \"\"\"Handles loading and quantization of large language models.\"\"\"\n    \n    def __init__(self, config: QuantizationConfig, memory_monitor: MemoryMonitor):\n        self.config = config\n        self.memory_monitor = memory_monitor\n        \n    def load_model_and_tokenizer(self, model_name: str) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:\n        \"\"\"Load model with 4-bit quantization and compatible tokenizer.\"\"\"\n        # TODO 1: Create BitsAndBytesConfig from self.config\n        # TODO 2: Measure memory before loading\n        # TODO 3: Load tokenizer first (minimal memory impact)\n        # TODO 4: Load model with quantization config\n        # TODO 5: Measure memory after loading\n        # TODO 6: Force garbage collection to clean up loading overhead\n        # TODO 7: Return model and tokenizer tuple\n        # Hint: Use device_map=\"auto\" for automatic GPU placement\n        pass\n    \n    def verify_quantization_quality(self, model: AutoModelForCausalLM) -> Dict[str, Any]:\n        \"\"\"Verify that quantization didn't catastrophically degrade model quality.\"\"\"\n        # TODO 1: Check that model parameters are actually quantized\n        # TODO 2: Run a simple forward pass with dummy input\n        # TODO 3: Verify output tensors have reasonable values (not NaN/Inf)\n        # TODO 4: Return quality validation results\n        pass\n    \n    def analyze_quantized_layers(self, model: AutoModelForCausalLM) -> Dict[str, Any]:\n        \"\"\"Analyze which layers were quantized and their memory usage.\"\"\"\n        # TODO 1: Iterate through model.named_modules()\n        # TODO 2: Identify quantized linear layers\n        # TODO 3: Calculate memory usage per layer type\n        # TODO 4: Return analysis dictionary\n        pass\n\ndef setup_quantized_model(model_name: str, config: QuantizationConfig) -> Tuple[AutoModelForCausalLM, AutoTokenizer, MemoryMonitor]:\n    \"\"\"Complete setup for quantized model with monitoring.\"\"\"\n    monitor = setup_memory_monitoring()\n    loader = QuantizedModelLoader(config, monitor)\n    \n    # Load and quantize the model\n    model, tokenizer = loader.load_model_and_tokenizer(model_name)\n    \n    # Verify quantization was successful\n    quality_report = loader.verify_quantization_quality(model)\n    \n    return model, tokenizer, monitor\n```\n\n#### Mixed-Precision Training Coordinator\n\n```python\nfrom torch.cuda.amp import autocast, GradScaler\nfrom typing import Any, Dict\nimport torch.nn as nn\n\nclass MixedPrecisionTrainer:\n    \"\"\"Coordinates mixed-precision training with quantized models.\"\"\"\n    \n    def __init__(self, config: QuantizationConfig):\n        self.config = config\n        self.compute_dtype = self._resolve_compute_dtype()\n        self.use_amp = self.compute_dtype != torch.float32\n        self.scaler = GradScaler() if self.use_amp else None\n        \n    def _resolve_compute_dtype(self) -> torch.dtype:\n        \"\"\"Convert string compute dtype to torch dtype.\"\"\"\n        # TODO 1: Create mapping from config strings to torch dtypes\n        # TODO 2: Handle \"auto\" selection based on hardware capabilities\n        # TODO 3: Validate dtype is supported on current device\n        # TODO 4: Return resolved torch dtype\n        pass\n    \n    def forward_pass_context(self):\n        \"\"\"Return appropriate context manager for forward pass.\"\"\"\n        # TODO 1: Return autocast context if using mixed precision\n        # TODO 2: Return nullcontext if using full precision\n        # Hint: autocast('cuda', dtype=self.compute_dtype) for mixed precision\n        pass\n    \n    def backward_and_step(self, loss: torch.Tensor, optimizer: Any, model: nn.Module) -> Dict[str, float]:\n        \"\"\"Execute backward pass and optimizer step with proper scaling.\"\"\"\n        # TODO 1: Scale loss if using gradient scaler\n        # TODO 2: Compute gradients with scaled loss\n        # TODO 3: Unscale gradients before optimizer step\n        # TODO 4: Check for gradient overflow and skip step if necessary\n        # TODO 5: Update gradient scaler\n        # TODO 6: Return step statistics (loss, grad_norm, etc.)\n        pass\n    \n    def configure_optimizer_for_quantized_training(self, model: nn.Module) -> Any:\n        \"\"\"Configure memory-efficient optimizer for quantized model training.\"\"\"\n        # TODO 1: Filter for only trainable parameters (LoRA adapters)\n        # TODO 2: Create 8-bit AdamW optimizer if available\n        # TODO 3: Set appropriate learning rate and weight decay\n        # TODO 4: Return configured optimizer\n        pass\n\ndef create_training_coordinator(quantization_config: QuantizationConfig, model: nn.Module) -> MixedPrecisionTrainer:\n    \"\"\"Create and configure mixed-precision training coordinator.\"\"\"\n    trainer = MixedPrecisionTrainer(quantization_config)\n    \n    # Verify compute dtype compatibility with model\n    test_input = torch.randn(1, 10, dtype=trainer.compute_dtype, device=next(model.parameters()).device)\n    \n    return trainer\n```\n\n#### Milestone Checkpoints\n\nAfter implementing the QLoRA quantization component, verify the following behavior:\n\n**Memory Usage Validation:**\n- Run `python -c \"from src.fine_tuning.quantization import setup_quantized_model; model, tokenizer, monitor = setup_quantized_model('microsoft/DialoGPT-medium', QuantizationConfig()); print(monitor.generate_memory_report())\"`\n- Expected: Memory usage should be approximately 25% of the original model size\n- Troubleshooting: If memory usage is higher than expected, check that `load_in_4bit=True` and verify bitsandbytes installation\n\n**Quantization Quality Check:**\n- Load a quantized model and generate text with the same prompt as the original model\n- Expected: Output quality should be comparable with minimal degradation\n- Warning signs: Repetitive text, nonsensical outputs, or generation failures indicate quantization issues\n\n**Mixed-Precision Training Stability:**\n- Start a short training run (10 steps) and monitor loss values and gradient norms\n- Expected: Loss should decrease and gradients should remain finite\n- Red flags: Loss spikes to infinity, gradient norms exceeding 10^6, or immediate divergence\n\n**Memory Monitoring Accuracy:**\n- Compare reported memory usage with `nvidia-smi` output during model loading\n- Expected: Memory measurements should match within 10% of system measurements\n- Issues: Large discrepancies indicate measurement timing problems or cache clearing issues\n\n\n## Training Loop Component\n\n> **Milestone(s):** Milestone 4 - this section implements the training orchestration system that coordinates the fine-tuning process with gradient accumulation, learning rate scheduling, checkpoint management, loss tracking, and early stopping mechanisms.\n\n![Training Loop State Machine](./diagrams/training-state-machine.svg)\n\n### Mental Model: The Personal Trainer\n\nThink of the training loop as a **dedicated personal trainer** working with an athlete to master new skills. The trainer doesn't just tell the athlete to \"get better\" - they create a structured program with carefully planned progressions, monitor performance metrics, adjust intensity based on progress, and know when to push harder or ease up to prevent burnout.\n\nJust as a personal trainer breaks down complex movements into manageable sets and reps, the training loop breaks down the massive task of learning from thousands of examples into digestible micro-batches. The trainer tracks performance over time, celebrates improvements, and intervenes when progress stalls. They save snapshots of the athlete's best performances and can roll back to previous training states if an injury or setback occurs.\n\nThe training loop embodies this same nurturing but systematic approach. It feeds the model small batches of examples repeatedly, accumulates gradients like a trainer accumulates evidence of improvement, adjusts the learning intensity through scheduling, and maintains detailed records of progress. When the model shows signs of overtraining (like an athlete showing fatigue), the training loop implements early stopping to preserve peak performance.\n\nThis mental model helps us understand why each component of the training loop matters: gradient accumulation simulates the effect of larger training sessions, learning rate scheduling prevents the model from making overly aggressive changes that could disrupt previous learning, checkpointing preserves the best versions of model knowledge, and loss tracking provides the quantitative feedback needed to make informed training decisions.\n\n### Gradient Accumulation Strategy\n\n**Gradient accumulation** serves as the cornerstone technique for simulating large effective batch sizes when GPU memory constraints prevent loading large batches directly. In parameter-efficient fine-tuning, this becomes even more critical because we're working with resource-constrained environments where even quantized models push memory limits.\n\nThe fundamental insight behind gradient accumulation is that gradient computation is linear - we can split a large batch into smaller micro-batches, compute gradients for each micro-batch separately, accumulate (sum) these gradients, and then perform a single optimizer step. This mathematically equivalent to processing the entire large batch at once, but with dramatically reduced peak memory usage.\n\nConsider a scenario where we want an effective batch size of 32 samples, but our GPU can only fit 4 samples at a time. The gradient accumulation strategy divides this into 8 micro-batches of 4 samples each. The training loop processes each micro-batch sequentially, accumulating gradients without clearing them between batches, and only performs the optimizer step and gradient clearing after processing all 8 micro-batches.\n\nThe implementation requires careful coordination of several PyTorch mechanisms. During each micro-batch forward pass, we must scale the loss by the number of accumulation steps to ensure the final accumulated gradient has the correct magnitude. We also need to manage the model's training state to prevent automatic gradient clearing between micro-batches.\n\nHere's how the gradient accumulation process unfolds step by step:\n\n1. **Initialize accumulation counters** and set the model to training mode while ensuring gradients start cleared from any previous iteration\n2. **Enter the micro-batch loop** where each iteration processes one micro-batch worth of samples from the current training batch\n3. **Perform forward pass** on the micro-batch, computing loss values and scaling them by the inverse of the total accumulation steps\n4. **Execute backward pass** using the scaled loss, which computes gradients and adds them to any existing accumulated gradients in parameter tensors\n5. **Check memory usage** and optionally clear intermediate activations to prevent memory buildup during long accumulation sequences\n6. **Continue accumulation loop** until all micro-batches for the current effective batch have been processed\n7. **Apply gradient clipping** to the accumulated gradients to prevent training instability from gradient explosion\n8. **Perform optimizer step** using the accumulated gradients to update model parameters\n9. **Clear accumulated gradients** and reset counters in preparation for the next effective batch\n10. **Update learning rate scheduler** based on the completion of one effective training step\n\nThe gradient accumulation strategy interacts closely with mixed-precision training in QLoRA setups. When using 4-bit quantized weights with float16 computation, gradient accumulation must handle the precision transitions carefully. The accumulated gradients themselves are typically stored in float32 for numerical stability, even when forward pass computations use lower precision.\n\nMemory management during gradient accumulation requires particular attention. While the strategy reduces peak memory usage compared to large batches, it still accumulates gradients in GPU memory throughout the accumulation process. In extreme memory-constrained scenarios, we may need to implement gradient checkpointing alongside accumulation to reduce activation memory usage during the backward passes.\n\n> **Key Design Insight**: Gradient accumulation transforms memory constraints from a hard limit into a time-space tradeoff. We exchange longer training time for reduced memory usage, making large-scale fine-tuning feasible on consumer hardware.\n\nThe strategy also affects learning dynamics subtly but importantly. Gradient accumulation introduces slightly different noise characteristics compared to true large batch training because the accumulated gradients represent an exact sum rather than a stochastic sample. This generally leads to more stable training but can occasionally reduce the beneficial regularization effect of gradient noise.\n\n**Architecture Decision Record: Gradient Accumulation Implementation**\n\n> **Decision: Implement gradient accumulation at the training loop level rather than using framework-provided utilities**\n> - **Context**: Multiple frameworks offer gradient accumulation features, but we need fine-grained control over memory management and compatibility with QLoRA quantization\n> - **Options Considered**: \n>   - Use Transformers library's gradient accumulation in `TrainingArguments`\n>   - Implement custom gradient accumulation in our training loop\n>   - Use PyTorch's native gradient accumulation utilities\n> - **Decision**: Implement custom gradient accumulation with explicit control over each step\n> - **Rationale**: Custom implementation allows precise memory management during quantized training, better integration with our checkpoint system, and explicit handling of precision transitions in mixed-precision QLoRA training\n> - **Consequences**: Requires more implementation effort but provides necessary control over memory usage patterns and ensures compatibility with our quantization and adapter strategies\n\n| Gradient Accumulation Option | Pros | Cons | Memory Control | QLoRA Compatibility |\n|---|---|---|---|---|\n| Transformers TrainingArguments | Minimal code, battle-tested | Limited memory control, potential quantization conflicts | Low | Medium |\n| Custom loop implementation | Full control, optimized for our use case | More implementation complexity, potential bugs | High | High |\n| PyTorch native utilities | Good balance, framework support | Less control than custom, may not handle our edge cases | Medium | Medium |\n\n### Learning Rate Scheduling\n\n**Learning rate scheduling** serves as the sophisticated mechanism for controlling how aggressively the model updates its parameters throughout the training process. In the context of parameter-efficient fine-tuning with LoRA adapters, learning rate scheduling becomes even more nuanced because we're adapting a pre-trained model rather than training from scratch.\n\nThe fundamental challenge in fine-tuning lies in balancing knowledge retention with new knowledge acquisition. Too high a learning rate causes **catastrophic forgetting** where the model loses its pre-trained capabilities while learning the new task. Too low a learning rate results in insufficient adaptation, where the model fails to learn the new task effectively. Learning rate scheduling navigates this delicate balance by starting with careful warm-up phases and implementing strategic decay patterns.\n\n**Warmup scheduling** addresses the initialization instability problem that commonly occurs at the beginning of fine-tuning. When LoRA adapters are first initialized, they contain random weights that produce large, potentially destructive gradient updates. The warmup phase gradually increases the learning rate from near-zero to the target value over a specified number of steps, allowing the adapter weights to stabilize before full-strength training begins.\n\nThe warmup process typically follows a linear schedule where the learning rate increases proportionally with the step number until reaching the peak learning rate. During this phase, the model makes small, cautious updates that prevent early training instability while allowing the LoRA adapters to find reasonable initial directions for adaptation.\n\n**Decay scheduling** manages the later phases of training when the model approaches convergence. As training progresses and loss improvements become smaller, maintaining high learning rates can cause the model to oscillate around optimal parameter values rather than settling into them. Decay schedules systematically reduce the learning rate to enable fine-grained convergence.\n\n**Cosine decay scheduling** has emerged as particularly effective for fine-tuning applications. This schedule follows a cosine curve that starts at the peak learning rate after warmup and smoothly decreases to a minimum value (often 10% of the peak rate) by the end of training. The cosine schedule provides faster initial decay when large improvements are still possible, followed by more gradual decay as the model approaches convergence.\n\nThe mathematical formulation for cosine decay with warmup involves several phases:\n\n1. **Warmup phase** (steps 0 to warmup_steps): learning rate increases linearly from 0 to peak_learning_rate\n2. **Cosine decay phase** (steps warmup_steps to total_steps): learning rate follows cosine decay from peak_learning_rate to min_learning_rate\n3. **Optional constant phase**: some schedules maintain the minimum learning rate for final stabilization\n\n**Linear decay scheduling** provides an alternative approach where the learning rate decreases linearly after the warmup phase. While simpler to implement and understand, linear decay can sometimes lead to more abrupt transitions between high and low learning rate regimes, potentially causing training instability.\n\n**Constant scheduling with warmup** maintains the peak learning rate throughout most of training, only applying warmup at the beginning. This approach works well for tasks where the optimal learning rate remains consistent throughout training, but it requires careful tuning to avoid late-training instability.\n\nThe interaction between learning rate scheduling and LoRA's alpha parameter requires special consideration. The effective learning rate for LoRA adapters is scaled by the ratio alpha/rank, meaning that the scheduled learning rate gets further modified by the LoRA configuration. This interaction must be accounted for when selecting appropriate peak learning rates and warmup schedules.\n\n**Architecture Decision Record: Learning Rate Scheduling Strategy**\n\n> **Decision: Implement cosine decay with linear warmup as the primary scheduling strategy**\n> - **Context**: Fine-tuning requires careful balance between adaptation and knowledge retention, with different optimal learning rates at different training phases\n> - **Options Considered**:\n>   - Linear warmup with cosine decay\n>   - Linear warmup with linear decay\n>   - Exponential warmup with polynomial decay\n> - **Decision**: Linear warmup with cosine decay, configurable warmup ratio and minimum learning rate\n> - **Rationale**: Cosine decay provides smooth convergence characteristics that work well with pre-trained models, linear warmup is stable and predictable, and the combination has strong empirical results in fine-tuning literature\n> - **Consequences**: Requires tuning of warmup ratio and minimum learning rate, but provides robust convergence characteristics across different tasks and model sizes\n\n| Learning Rate Schedule | Convergence Quality | Stability | Tuning Complexity | Memory Retention |\n|---|---|---|---|---|\n| Linear warmup + Cosine decay | High | High | Medium | High |\n| Linear warmup + Linear decay | Medium | High | Low | Medium |\n| Exponential warmup + Polynomial decay | Medium | Medium | High | Medium |\n| Constant with warmup | Low | Medium | Low | Low |\n\nThe learning rate scheduling system must integrate closely with the gradient accumulation strategy. Since gradient accumulation changes the effective frequency of parameter updates, the scheduler must track \"effective steps\" (completed accumulation cycles) rather than individual forward passes. This ensures that learning rate changes correspond to actual parameter updates rather than individual micro-batch processes.\n\n### Checkpoint and State Management\n\n**Checkpoint and state management** forms the reliability backbone of the fine-tuning pipeline, ensuring that training progress is preserved against hardware failures, preemption events, and experimental iterations. In parameter-efficient fine-tuning contexts, checkpoint management becomes more sophisticated because we must handle both base model state and adapter-specific configurations.\n\nThe checkpoint system must preserve multiple categories of training state. **Model state** includes the LoRA adapter weights (the base model weights remain frozen and unchanged). **Optimizer state** contains momentum buffers, variance estimates, and other stateful information needed for optimizers like AdamW to resume training effectively. **Scheduler state** tracks the current step count, learning rate value, and any internal scheduler parameters. **Training metadata** captures the current epoch, step count, best validation loss, and configuration parameters.\n\n**Checkpoint timing strategies** determine when to save training state. **Step-interval checkpointing** saves state every N training steps, providing regular snapshots of training progress. This strategy ensures minimal progress loss during failures but can generate large numbers of checkpoint files during long training runs. **Epoch-based checkpointing** saves state at the end of each training epoch, aligning checkpoints with natural training boundaries but potentially losing more progress during failures.\n\n**Evaluation-triggered checkpointing** saves state after each validation evaluation, particularly when validation metrics improve. This strategy focuses checkpoint storage on the most promising model states but may miss intermediate improvements or create irregular checkpoint intervals.\n\nThe **best model tracking** mechanism maintains a separate record of the checkpoint with the lowest validation loss (or highest task-specific metric). This enables easy retrieval of the optimal model state even if training continues past the optimal point or experiences later degradation.\n\n**Storage optimization** becomes critical when dealing with large models and frequent checkpointing. **Incremental checkpointing** saves only the changes since the last checkpoint, dramatically reducing storage requirements for LoRA adapters where only a small fraction of parameters change between checkpoints. **Compression techniques** can reduce checkpoint file sizes by 50-80% with minimal impact on loading times.\n\n**Checkpoint cleanup policies** manage disk space by automatically removing older checkpoints. **Keep-last-N policies** maintain only the most recent N checkpoints, ensuring bounded storage usage. **Keep-best-plus-recent policies** preserve the best checkpoint plus the last few recent checkpoints, balancing optimization needs with failure recovery.\n\nThe checkpoint loading and resumption process requires careful state reconstruction. When resuming training, the system must reload the model state, optimizer state, scheduler state, and training metadata in the correct order. **State validation** checks ensure that the loaded checkpoint matches the current training configuration, preventing subtle errors from configuration changes between training sessions.\n\n**Architecture Decision Record: Checkpoint Storage Format**\n\n> **Decision: Use separate files for model weights, optimizer state, and metadata with atomic write operations**\n> - **Context**: Checkpoint corruption during writes can destroy training progress, and different checkpoint components have different update frequencies and storage requirements\n> - **Options Considered**:\n>   - Single monolithic checkpoint file\n>   - Separate files for different state components\n>   - Database-backed checkpoint storage\n> - **Decision**: Separate files with atomic writes and validation checksums\n> - **Rationale**: Separate files enable partial checkpoint loading, reduce corruption risk through atomic operations, and allow different compression strategies for different components. Validation checksums detect corruption early.\n> - **Consequences**: Slightly more complex checkpoint management code, but dramatically improved reliability and flexibility for partial loading scenarios\n\n| Checkpoint Strategy | Reliability | Storage Efficiency | Loading Speed | Implementation Complexity |\n|---|---|---|---|---|\n| Monolithic files | Medium | Low | Fast | Low |\n| Separate component files | High | High | Medium | Medium |\n| Database storage | High | Medium | Slow | High |\n| Compressed monolithic | Medium | High | Medium | Medium |\n\n**Checkpoint validation** ensures that saved checkpoints are complete and uncorrupted. The validation process includes checksum verification of file contents, consistency checks between model architecture and saved weights, and verification that optimizer state dimensions match the current model configuration.\n\n**Distributed checkpoint handling** becomes relevant when scaling to multi-GPU scenarios. Each process must coordinate checkpoint timing to ensure consistent global state, and checkpoint files must account for distributed optimizer states and model sharding.\n\n### Loss Tracking and Early Stopping\n\n**Loss tracking and early stopping** provide the monitoring and intervention mechanisms that prevent overfitting and enable efficient resource utilization during fine-tuning. These systems work together to continuously assess training progress and make intelligent decisions about when to continue, modify, or terminate training.\n\n**Comprehensive loss tracking** monitors multiple categories of metrics throughout training. **Training loss** measures how well the model fits the training data at each step, providing immediate feedback on learning progress. **Validation loss** evaluates model performance on held-out data, serving as the primary indicator of generalization quality and overfitting detection. **Learning rate tracking** records the current learning rate value to correlate training behavior with schedule changes. **Gradient norm tracking** monitors the magnitude of gradient updates to detect training instability or convergence.\n\nThe loss tracking system implements **smoothing and statistical analysis** to distinguish meaningful trends from random fluctuations. **Exponential moving averages** smooth noisy training loss curves to reveal underlying trends. **Validation loss analysis** looks for patterns like consistent improvement, stagnation, or deterioration over multiple evaluation cycles.\n\n**Memory-efficient logging** becomes important during long training runs that may generate thousands of metric data points. **Hierarchical sampling** logs every step initially but reduces frequency for older data points. **Statistical summaries** replace individual data points with summary statistics (mean, min, max, variance) over time windows.\n\n**Early stopping mechanisms** implement sophisticated policies for training termination based on validation performance. **Patience-based stopping** allows validation loss to stop improving for a configurable number of evaluation cycles before terminating training. This approach accounts for natural training fluctuations while preventing excessive training on non-improving models.\n\n**Improvement threshold stopping** requires validation loss improvements to exceed a minimum threshold to be considered significant. This prevents early stopping due to tiny, potentially meaningless improvements and focuses on substantial learning progress.\n\n**Combined patience and threshold stopping** requires both significant improvement magnitude and recent improvement timing. This robust approach combines the benefits of both individual strategies while reducing false positive termination.\n\nThe early stopping system must handle **validation evaluation scheduling** carefully. **Fixed interval evaluation** runs validation every N training steps, providing regular performance snapshots but potentially missing rapid changes. **Adaptive evaluation** increases evaluation frequency when validation loss shows interesting patterns (improvement or degradation) and reduces frequency during stable periods.\n\n**Plateau detection algorithms** identify when training has reached a local optimum or learning has stagnated. **Statistical plateau detection** uses statistical tests to determine when validation loss distributions have become statistically indistinguishable across recent evaluation windows. **Slope-based plateau detection** measures the slope of validation loss curves and triggers when slopes approach zero for extended periods.\n\n**Architecture Decision Record: Early Stopping Configuration**\n\n> **Decision: Implement patience-based early stopping with configurable improvement thresholds and validation scheduling**\n> - **Context**: Fine-tuning runs can be expensive and may overfit if allowed to continue indefinitely, but premature stopping can prevent full adaptation\n> - **Options Considered**:\n>   - Simple patience-based stopping\n>   - Threshold-based stopping only\n>   - Combined patience and threshold with adaptive evaluation\n> - **Decision**: Combined approach with configurable patience, improvement threshold, and evaluation interval\n> - **Rationale**: Combined approach provides robustness against both premature stopping and excessive training. Configurable parameters allow adaptation to different task characteristics and resource constraints.\n> - **Consequences**: More configuration complexity but better training efficiency and overfitting prevention across diverse tasks\n\n| Early Stopping Strategy | False Positive Rate | Resource Efficiency | Configuration Complexity | Overfitting Prevention |\n|---|---|---|---|---|\n| Patience only | Medium | Medium | Low | Medium |\n| Threshold only | High | Low | Low | Low |\n| Combined patience + threshold | Low | High | Medium | High |\n| Adaptive evaluation + combined | Very Low | Very High | High | Very High |\n\n**Loss divergence detection** identifies training instability before it becomes catastrophic. **Gradient explosion detection** monitors gradient norms for sudden increases that indicate numerical instability. **Loss explosion detection** watches for rapid increases in training loss that suggest learning rate problems or batch corruption.\n\n**Recovery mechanisms** can sometimes salvage training runs that show signs of instability. **Learning rate reduction** automatically decreases the learning rate when loss divergence is detected. **Checkpoint rollback** reverts to a previous stable checkpoint and resumes training with modified hyperparameters.\n\n### Architecture Decision Records\n\nThis section consolidates the key architectural decisions that shape the training loop's design and behavior, providing the rationale and trade-offs for each major choice.\n\n**Decision: Training Step Definition and Progress Tracking**\n\n> **Decision: Define training steps as completed gradient accumulation cycles rather than individual forward passes**\n> - **Context**: Gradient accumulation creates a distinction between micro-batch forward passes and actual parameter updates, affecting how we measure training progress\n> - **Options Considered**:\n>   - Count individual forward passes as steps\n>   - Count completed accumulation cycles as steps\n>   - Maintain separate counters for both metrics\n> - **Decision**: Use completed accumulation cycles as the primary step metric with secondary micro-batch tracking\n> - **Rationale**: Accumulation cycles correspond to actual parameter updates and learning progress. Scheduler updates and checkpoint intervals should align with meaningful learning steps rather than arbitrary micro-batch boundaries.\n> - **Consequences**: Requires careful coordination between accumulation logic and progress tracking, but provides more meaningful progress metrics and better alignment with learning rate scheduling\n\n**Decision: Memory Management During Training**\n\n> **Decision: Implement proactive memory monitoring with automatic garbage collection triggers**\n> - **Context**: QLoRA training pushes GPU memory limits, and memory leaks or accumulation can cause out-of-memory failures mid-training\n> - **Options Considered**:\n>   - Reactive memory management (handle OOM when it occurs)\n>   - Proactive monitoring with periodic cleanup\n>   - Continuous memory optimization with automatic adjustment\n> - **Decision**: Proactive monitoring with configurable memory thresholds and automatic cleanup triggers\n> - **Rationale**: Proactive approach prevents training failures rather than recovering from them. Configurable thresholds allow adaptation to different hardware configurations. Automatic cleanup reduces manual intervention requirements.\n> - **Consequences**: Adds monitoring overhead but significantly improves training reliability on memory-constrained systems\n\n**Decision: Checkpoint Metadata and Versioning**\n\n> **Decision: Include comprehensive metadata and configuration snapshots in all checkpoints**\n> - **Context**: Training experiments often involve iterative hyperparameter adjustments, and checkpoint compatibility across configuration changes affects reproducibility\n> - **Options Considered**:\n>   - Minimal checkpoints with only model weights\n>   - Checkpoints with basic metadata (step, loss)\n>   - Comprehensive checkpoints with full configuration snapshots\n> - **Decision**: Comprehensive metadata including configuration hashes, dependency versions, and training environment information\n> - **Rationale**: Comprehensive metadata enables reproducible research, supports checkpoint compatibility validation, and facilitates experiment tracking. The storage overhead is minimal compared to model weights.\n> - **Consequences**: Larger checkpoint files and more complex checkpoint loading logic, but dramatically improved experiment reproducibility and debugging capabilities\n\n| Architecture Decision Area | Complexity Impact | Performance Impact | Maintenance Impact | Chosen Approach |\n|---|---|---|---|---|\n| Step definition strategy | Medium | Low | Low | Accumulation cycles |\n| Memory management approach | High | Medium | Medium | Proactive monitoring |\n| Checkpoint metadata strategy | Medium | Low | Low | Comprehensive metadata |\n| Early stopping configuration | High | High | Medium | Combined patience + threshold |\n\n### Common Pitfalls\n\nThis section identifies the most frequent mistakes that developers encounter when implementing training loop components, providing concrete descriptions of each pitfall and specific guidance for avoiding or correcting them.\n\n⚠️ **Pitfall: Learning Rate Scale Confusion with Gradient Accumulation**\n\nMany developers forget to adjust their learning rate when implementing gradient accumulation, leading to either under-training or training instability. When you accumulate gradients across N micro-batches, the effective batch size increases by a factor of N, but the gradient magnitude also increases by the same factor. If you don't adjust the learning rate accordingly, the model receives N times stronger updates than intended.\n\nThe specific mistake is using the same learning rate that worked for batch size 4 when implementing gradient accumulation that creates an effective batch size of 32. The accumulated gradients are 8 times larger, causing the model to make overly aggressive parameter updates that can lead to catastrophic forgetting or numerical instability.\n\nTo fix this, either scale your learning rate down by the accumulation factor (divide by N) or scale your loss by the accumulation factor during the forward pass. The loss scaling approach is generally preferred because it maintains the same numerical range for gradients and works better with mixed-precision training.\n\n⚠️ **Pitfall: Optimizer State Corruption During Checkpoint Resume**\n\nA subtle but devastating error occurs when the optimizer state becomes misaligned with model parameters during checkpoint resumption. This happens when developers save model state and optimizer state separately, then reload them in the wrong order or with different model configurations. The optimizer maintains momentum and variance estimates for each parameter, and misalignment causes convergence problems that are difficult to diagnose.\n\nThe symptom appears as training that resumes successfully but shows degraded performance, erratic loss curves, or failure to converge from the resumed checkpoint. The model architecture and weights load correctly, but the optimizer's internal state references the wrong parameters or contains stale statistics.\n\nTo prevent this, always save and load model and optimizer states as a coordinated unit. Validate that the number of trainable parameters matches between save and load time. Include optimizer state validation checks that compare parameter shapes and counts. When in doubt, restart optimizer state from scratch rather than risking misaligned momentum buffers.\n\n⚠️ **Pitfall: Early Stopping on Training Loss Instead of Validation Loss**\n\nNovice developers sometimes implement early stopping based on training loss rather than validation loss, completely defeating the purpose of early stopping. Training loss typically decreases monotonically throughout training (especially with gradient accumulation smoothing), so early stopping on training loss either never triggers or triggers prematurely due to temporary loss spikes.\n\nThe fundamental error is misunderstanding that early stopping should prevent overfitting, which requires monitoring generalization performance via validation loss. Training loss measures memorization of the training set, not the model's ability to generalize to new data.\n\nAlways implement early stopping based on validation loss or other held-out evaluation metrics. Set up regular validation evaluation during training (every few epochs or every N steps). Track validation loss trends over multiple evaluation cycles to distinguish random fluctuation from genuine performance degradation.\n\n⚠️ **Pitfall: Gradient Accumulation Without Proper Loss Scaling**\n\nDevelopers often implement gradient accumulation by simply not clearing gradients between micro-batches, but forget to scale the loss values appropriately. This results in accumulated gradients that are N times larger than intended, where N is the number of accumulation steps. The oversized gradients cause training instability, poor convergence, or complete training failure.\n\nThe mistake manifests as training that appears to work initially but shows unstable loss curves, poor final performance, or catastrophic forgetting of pre-trained capabilities. The gradient norms become abnormally large, and learning rate schedules that normally work become completely inappropriate.\n\nTo fix this, divide your loss by the number of accumulation steps during the forward pass, before calling backward(). This ensures that the accumulated gradients have the same magnitude as if you had processed a single large batch. Always verify gradient norms remain in reasonable ranges after implementing accumulation.\n\n⚠️ **Pitfall: Memory Leaks from Retained Computation Graphs**\n\nIn PyTorch, computation graphs can accumulate during training loops if intermediate tensors retain references to gradient computation. This is particularly problematic with gradient accumulation, where multiple forward passes occur before graph cleanup. The leaked computation graphs consume GPU memory progressively until out-of-memory errors occur.\n\nThe symptom is training that starts successfully but shows steadily increasing memory usage even when batch sizes and model sizes remain constant. Memory usage grows linearly with training steps rather than remaining stable after initial allocation.\n\nTo prevent this, ensure that loss values are converted to Python scalars (using .item()) before logging or storing them. Avoid keeping references to intermediate tensors outside the training loop. Use del statements to explicitly remove large temporary tensors. Enable gradient checkpointing to reduce activation memory usage during backward passes.\n\n⚠️ **Pitfall: Checkpoint Corruption from Concurrent Writes**\n\nFile system races can corrupt checkpoints when training processes write checkpoint data concurrently or when external processes (monitoring tools, backup systems) access checkpoint files during writes. This leads to partially written or corrupted checkpoints that fail to load during recovery attempts.\n\nThe corruption typically manifests as load errors with cryptic messages about truncated files, mismatched tensor shapes, or deserialization failures. These errors only appear when attempting to resume training, making them particularly frustrating to debug.\n\nTo prevent corruption, always use atomic write operations for checkpoints. Write to temporary files first, then rename them to the final checkpoint name after successful completion. Implement file locking or write coordination if multiple processes might access checkpoint directories. Add checksum validation to detect corrupted checkpoints early.\n\n⚠️ **Pitfall: Validation Set Data Leakage Through Preprocessing**\n\nSubtle data leakage occurs when preprocessing steps (normalization statistics, vocabulary building, or feature scaling) use information from both training and validation sets. In the context of LLM fine-tuning, this often happens with tokenizer training or chat template standardization that incorporates validation data.\n\nThe leakage compromises the validation set's ability to provide unbiased performance estimates, leading to overly optimistic early stopping decisions and poor generalization to truly unseen data. The training appears to converge well, but deployed models show performance degradation.\n\nTo avoid leakage, ensure all preprocessing decisions use only training data. Compute normalization statistics, vocabulary, and templates from training samples only, then apply these fixed parameters to validation data. Maintain strict separation between training and validation data throughout the entire pipeline.\n\n| Pitfall Category | Detection Difficulty | Impact Severity | Prevention Complexity | Most Common Cause |\n|---|---|---|---|---|\n| Learning rate scaling | Medium | High | Low | Forgetting accumulation effects |\n| Optimizer state corruption | High | Very High | Medium | Checkpoint loading order |\n| Wrong loss for early stopping | Low | Medium | Low | Conceptual misunderstanding |\n| Loss scaling in accumulation | Medium | High | Low | Incomplete accumulation implementation |\n| Memory leaks | Medium | High | Medium | Computation graph retention |\n| Checkpoint corruption | High | Very High | Medium | Concurrent file access |\n| Validation data leakage | Very High | High | High | Preprocessing on combined data |\n\n### Implementation Guidance\n\nThis implementation guidance provides the concrete technical foundation for building the training loop component, including complete starter code for infrastructure components and detailed skeleton code for core training logic.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|---|---|---|\n| Training Framework | HuggingFace Transformers Trainer | Custom PyTorch training loop with Accelerate |\n| Learning Rate Scheduling | transformers.get_cosine_schedule_with_warmup | Custom schedulers with complex warmup strategies |\n| Checkpoint Management | torch.save/torch.load with simple intervals | Advanced checkpoint management with metadata and compression |\n| Logging and Monitoring | Python logging + simple file outputs | Weights & Biases or TensorBoard integration |\n| Memory Monitoring | Basic torch.cuda.memory_summary() | nvidia-ml-py for detailed GPU monitoring |\n| Early Stopping | Simple validation loss patience | Multi-metric early stopping with statistical tests |\n\n#### Recommended File Structure\n\n```\ntraining/\n├── __init__.py\n├── loop.py                    ← TrainingLoop main class\n├── schedulers.py             ← Learning rate scheduling utilities\n├── checkpoints.py            ← Checkpoint management and metadata\n├── metrics.py                ← Loss tracking and early stopping\n├── memory.py                 ← Memory monitoring utilities\n├── configs/\n│   ├── __init__.py\n│   └── training_config.py    ← TrainingConfig and related classes\n└── tests/\n    ├── test_loop.py\n    ├── test_schedulers.py\n    └── test_checkpoints.py\n```\n\n#### Infrastructure Starter Code\n\n**Memory Monitoring Utilities (training/memory.py)**\n\n```python\nimport torch\nimport psutil\nimport time\nfrom typing import Dict, List, Optional, Any\nfrom dataclasses import dataclass, field\n\n@dataclass\nclass MemoryMetrics:\n    stage: str\n    gpu_memory_allocated: float\n    gpu_memory_cached: float\n    gpu_memory_reserved: float\n    system_memory_used: float\n    timestamp: float\n    details: Optional[Dict[str, Any]] = None\n\n@dataclass\nclass MemoryMonitor:\n    baseline_gpu_memory: Optional[int] = None\n    baseline_system_memory: int = 0\n    measurements: List[Dict] = field(default_factory=list)\n    \n    def capture_baseline(self) -> None:\n        \"\"\"Record initial memory state before model loading.\"\"\"\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            torch.cuda.synchronize()\n            self.baseline_gpu_memory = torch.cuda.memory_allocated()\n        \n        self.baseline_system_memory = psutil.virtual_memory().used\n        \n        # Take initial measurement\n        baseline_metrics = self.measure_current_usage(\"baseline\")\n        self.measurements.append(baseline_metrics.__dict__)\n    \n    def measure_current_usage(self, stage: str) -> MemoryMetrics:\n        \"\"\"Measure current memory usage and return detailed statistics.\"\"\"\n        timestamp = time.time()\n        \n        # GPU memory measurement\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n            gpu_allocated = torch.cuda.memory_allocated() / 1024**3  # Convert to GB\n            gpu_cached = torch.cuda.memory_reserved() / 1024**3\n            gpu_reserved = torch.cuda.max_memory_reserved() / 1024**3\n            \n            # Detailed GPU memory info\n            memory_summary = torch.cuda.memory_summary()\n            details = {\n                \"memory_summary\": memory_summary,\n                \"device_count\": torch.cuda.device_count(),\n                \"current_device\": torch.cuda.current_device()\n            }\n        else:\n            gpu_allocated = gpu_cached = gpu_reserved = 0.0\n            details = {\"gpu_available\": False}\n        \n        # System memory measurement\n        system_memory = psutil.virtual_memory()\n        system_used_gb = system_memory.used / 1024**3\n        \n        details.update({\n            \"system_memory_percent\": system_memory.percent,\n            \"system_memory_available_gb\": system_memory.available / 1024**3\n        })\n        \n        metrics = MemoryMetrics(\n            stage=stage,\n            gpu_memory_allocated=gpu_allocated,\n            gpu_memory_cached=gpu_cached,\n            gpu_memory_reserved=gpu_reserved,\n            system_memory_used=system_used_gb,\n            timestamp=timestamp,\n            details=details\n        )\n        \n        # Store measurement\n        self.measurements.append(metrics.__dict__)\n        \n        return metrics\n    \n    def get_peak_usage(self) -> Dict[str, float]:\n        \"\"\"Return peak memory usage across all measurements.\"\"\"\n        if not self.measurements:\n            return {}\n        \n        peak_gpu = max(m[\"gpu_memory_allocated\"] for m in self.measurements)\n        peak_system = max(m[\"system_memory_used\"] for m in self.measurements)\n        \n        return {\n            \"peak_gpu_memory_gb\": peak_gpu,\n            \"peak_system_memory_gb\": peak_system,\n            \"measurements_count\": len(self.measurements)\n        }\n    \n    def cleanup_old_measurements(self, keep_last: int = 100):\n        \"\"\"Remove old measurements to prevent unbounded memory growth.\"\"\"\n        if len(self.measurements) > keep_last:\n            self.measurements = self.measurements[-keep_last:]\n```\n\n**Checkpoint Management Infrastructure (training/checkpoints.py)**\n\n```python\nimport json\nimport torch\nimport hashlib\nimport time\nimport shutil\nfrom pathlib import Path\nfrom typing import Dict, Any, Optional, List\nfrom dataclasses import dataclass, asdict\n\n@dataclass\nclass CheckpointMetadata:\n    checkpoint_path: str\n    step: int\n    epoch: float\n    eval_loss: Optional[float]\n    is_best: bool\n    model_config: Dict[str, Any]\n    adapter_config: Dict[str, Any]\n    optimizer_state_size: int\n    save_timestamp: float\n    \n    def to_dict(self) -> Dict[str, Any]:\n        return asdict(self)\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> 'CheckpointMetadata':\n        return cls(**data)\n\nclass CheckpointManager:\n    def __init__(self, checkpoint_dir: Path, keep_last: int = 3, keep_best: bool = True):\n        self.checkpoint_dir = Path(checkpoint_dir)\n        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.keep_last = keep_last\n        self.keep_best = keep_best\n        self.checkpoints: List[CheckpointMetadata] = []\n        self.best_checkpoint: Optional[CheckpointMetadata] = None\n        \n        # Load existing checkpoint metadata\n        self._load_checkpoint_registry()\n    \n    def save_checkpoint(\n        self,\n        model_state: Dict[str, torch.Tensor],\n        optimizer_state: Dict[str, Any],\n        scheduler_state: Dict[str, Any],\n        step: int,\n        epoch: float,\n        eval_loss: Optional[float] = None,\n        model_config: Optional[Dict[str, Any]] = None,\n        adapter_config: Optional[Dict[str, Any]] = None\n    ) -> CheckpointMetadata:\n        \"\"\"Save a complete checkpoint with atomic write operations.\"\"\"\n        \n        timestamp = time.time()\n        checkpoint_name = f\"checkpoint-step-{step}\"\n        checkpoint_path = self.checkpoint_dir / checkpoint_name\n        \n        # Create checkpoint directory\n        checkpoint_path.mkdir(exist_ok=True)\n        \n        # Determine if this is the best checkpoint\n        is_best = self._is_best_checkpoint(eval_loss)\n        \n        # Prepare checkpoint data\n        checkpoint_data = {\n            \"model_state\": model_state,\n            \"optimizer_state\": optimizer_state,\n            \"scheduler_state\": scheduler_state,\n            \"step\": step,\n            \"epoch\": epoch,\n            \"eval_loss\": eval_loss,\n            \"timestamp\": timestamp\n        }\n        \n        # Use atomic write operations\n        temp_path = checkpoint_path / f\".tmp_{timestamp}\"\n        try:\n            # Save main checkpoint file\n            torch.save(checkpoint_data, temp_path / \"training_state.pt\")\n            \n            # Save metadata\n            metadata = CheckpointMetadata(\n                checkpoint_path=str(checkpoint_path),\n                step=step,\n                epoch=epoch,\n                eval_loss=eval_loss,\n                is_best=is_best,\n                model_config=model_config or {},\n                adapter_config=adapter_config or {},\n                optimizer_state_size=self._estimate_state_size(optimizer_state),\n                save_timestamp=timestamp\n            )\n            \n            with open(temp_path / \"metadata.json\", 'w') as f:\n                json.dump(metadata.to_dict(), f, indent=2)\n            \n            # Atomic move from temp to final location\n            for temp_file in temp_path.iterdir():\n                final_file = checkpoint_path / temp_file.name\n                shutil.move(str(temp_file), str(final_file))\n            \n            temp_path.rmdir()\n            \n        except Exception as e:\n            # Cleanup on failure\n            if temp_path.exists():\n                shutil.rmtree(temp_path)\n            raise RuntimeError(f\"Checkpoint save failed: {e}\")\n        \n        # Update registry\n        self.checkpoints.append(metadata)\n        if is_best:\n            self.best_checkpoint = metadata\n        \n        # Cleanup old checkpoints\n        self._cleanup_old_checkpoints()\n        \n        # Save updated registry\n        self._save_checkpoint_registry()\n        \n        return metadata\n    \n    def load_checkpoint(self, checkpoint_path: Optional[Path] = None) -> Dict[str, Any]:\n        \"\"\"Load a checkpoint with validation.\"\"\"\n        \n        if checkpoint_path is None:\n            if not self.checkpoints:\n                raise ValueError(\"No checkpoints available to load\")\n            checkpoint_path = Path(self.checkpoints[-1].checkpoint_path)\n        \n        # Load and validate metadata\n        metadata_path = checkpoint_path / \"metadata.json\"\n        if not metadata_path.exists():\n            raise ValueError(f\"Checkpoint metadata not found: {metadata_path}\")\n        \n        with open(metadata_path, 'r') as f:\n            metadata_dict = json.load(f)\n        \n        metadata = CheckpointMetadata.from_dict(metadata_dict)\n        \n        # Load training state\n        state_path = checkpoint_path / \"training_state.pt\"\n        if not state_path.exists():\n            raise ValueError(f\"Checkpoint state not found: {state_path}\")\n        \n        checkpoint_data = torch.load(state_path, map_location='cpu')\n        checkpoint_data[\"metadata\"] = metadata\n        \n        return checkpoint_data\n    \n    def get_best_checkpoint_path(self) -> Optional[Path]:\n        \"\"\"Get path to the best checkpoint based on evaluation loss.\"\"\"\n        if self.best_checkpoint is None:\n            return None\n        return Path(self.best_checkpoint.checkpoint_path)\n    \n    def _is_best_checkpoint(self, eval_loss: Optional[float]) -> bool:\n        \"\"\"Determine if this checkpoint is the best so far.\"\"\"\n        if eval_loss is None:\n            return False\n        \n        if self.best_checkpoint is None:\n            return True\n        \n        if self.best_checkpoint.eval_loss is None:\n            return True\n        \n        return eval_loss < self.best_checkpoint.eval_loss\n    \n    def _estimate_state_size(self, state: Dict[str, Any]) -> int:\n        \"\"\"Estimate the size of optimizer state in bytes.\"\"\"\n        try:\n            # Quick estimation based on tensor sizes\n            total_elements = 0\n            for value in state.values():\n                if isinstance(value, torch.Tensor):\n                    total_elements += value.numel()\n                elif isinstance(value, dict):\n                    # Recursive estimation for nested dictionaries\n                    for nested_value in value.values():\n                        if isinstance(nested_value, torch.Tensor):\n                            total_elements += nested_value.numel()\n            \n            # Assume float32 (4 bytes per element) for rough estimation\n            return total_elements * 4\n        except Exception:\n            return 0\n    \n    def _cleanup_old_checkpoints(self):\n        \"\"\"Remove old checkpoints according to retention policy.\"\"\"\n        if len(self.checkpoints) <= self.keep_last:\n            return\n        \n        # Sort by step number\n        sorted_checkpoints = sorted(self.checkpoints, key=lambda x: x.step)\n        \n        # Determine which checkpoints to remove\n        checkpoints_to_remove = sorted_checkpoints[:-self.keep_last]\n        \n        for checkpoint in checkpoints_to_remove:\n            # Don't remove the best checkpoint if keep_best is True\n            if self.keep_best and checkpoint.is_best:\n                continue\n            \n            # Remove checkpoint directory\n            checkpoint_path = Path(checkpoint.checkpoint_path)\n            if checkpoint_path.exists():\n                shutil.rmtree(checkpoint_path)\n            \n            # Remove from registry\n            self.checkpoints.remove(checkpoint)\n    \n    def _save_checkpoint_registry(self):\n        \"\"\"Save the checkpoint registry to disk.\"\"\"\n        registry_path = self.checkpoint_dir / \"checkpoint_registry.json\"\n        registry_data = {\n            \"checkpoints\": [cp.to_dict() for cp in self.checkpoints],\n            \"best_checkpoint\": self.best_checkpoint.to_dict() if self.best_checkpoint else None\n        }\n        \n        with open(registry_path, 'w') as f:\n            json.dump(registry_data, f, indent=2)\n    \n    def _load_checkpoint_registry(self):\n        \"\"\"Load existing checkpoint registry from disk.\"\"\"\n        registry_path = self.checkpoint_dir / \"checkpoint_registry.json\"\n        if not registry_path.exists():\n            return\n        \n        try:\n            with open(registry_path, 'r') as f:\n                registry_data = json.load(f)\n            \n            self.checkpoints = [\n                CheckpointMetadata.from_dict(cp_data) \n                for cp_data in registry_data.get(\"checkpoints\", [])\n            ]\n            \n            best_data = registry_data.get(\"best_checkpoint\")\n            if best_data:\n                self.best_checkpoint = CheckpointMetadata.from_dict(best_data)\n        \n        except Exception as e:\n            print(f\"Warning: Could not load checkpoint registry: {e}\")\n            self.checkpoints = []\n            self.best_checkpoint = None\n```\n\n#### Core Training Loop Skeleton\n\n**Main Training Loop Class (training/loop.py)**\n\n```python\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom transformers import PreTrainedModel, PreTrainedTokenizer\nfrom typing import Dict, Any, Optional, Tuple\nimport time\nimport logging\n\nfrom .memory import MemoryMonitor, MemoryMetrics\nfrom .checkpoints import CheckpointManager, CheckpointMetadata\nfrom .metrics import MetricsTracker, TrainingMetrics, EvaluationMetrics\nfrom .schedulers import CosineScheduleWithWarmup\n\n@dataclass\nclass TrainingMetrics:\n    step: int\n    epoch: float\n    loss: float\n    learning_rate: float\n    grad_norm: Optional[float]\n    timestamp: float\n    gpu_memory_used: Optional[float]\n    samples_per_second: float\n\nclass TrainingLoop:\n    def __init__(\n        self,\n        model: PreTrainedModel,\n        tokenizer: PreTrainedTokenizer,\n        train_dataloader: DataLoader,\n        eval_dataloader: DataLoader,\n        optimizer: torch.optim.Optimizer,\n        config: 'TrainingConfig'\n    ):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.train_dataloader = train_dataloader\n        self.eval_dataloader = eval_dataloader\n        self.optimizer = optimizer\n        self.config = config\n        \n        # Initialize components\n        self.memory_monitor = MemoryMonitor()\n        self.checkpoint_manager = CheckpointManager(\n            checkpoint_dir=Path(config.output_dir) / \"checkpoints\",\n            keep_last=config.save_total_limit,\n            keep_best=True\n        )\n        self.metrics_tracker = MetricsTracker()\n        \n        # Initialize scheduler\n        total_steps = self.calculate_total_training_steps()\n        warmup_steps = int(total_steps * config.warmup_ratio)\n        \n        self.scheduler = CosineScheduleWithWarmup(\n            optimizer=optimizer,\n            num_warmup_steps=warmup_steps,\n            num_training_steps=total_steps,\n            min_lr_ratio=config.min_learning_rate_ratio\n        )\n        \n        # Training state\n        self.global_step = 0\n        self.current_epoch = 0\n        self.best_eval_loss = float('inf')\n        self.epochs_without_improvement = 0\n        \n        # Setup logging\n        self.logger = logging.getLogger(__name__)\n    \n    def train(self) -> str:\n        \"\"\"Execute the complete training loop and return path to best checkpoint.\"\"\"\n        \n        self.logger.info(\"Starting training loop\")\n        self.memory_monitor.capture_baseline()\n        \n        try:\n            # TODO 1: Set model to training mode and prepare for gradient accumulation\n            # TODO 2: Initialize training metrics tracking and logging\n            # TODO 3: Enter main epoch loop for specified number of training epochs\n            # TODO 4: For each epoch, iterate through training batches with progress tracking\n            # TODO 5: Implement gradient accumulation loop for each effective batch\n            # TODO 6: Perform forward pass on micro-batch and scale loss appropriately\n            # TODO 7: Execute backward pass and accumulate gradients without clearing\n            # TODO 8: After accumulation cycle, clip gradients and perform optimizer step\n            # TODO 9: Update learning rate scheduler and clear accumulated gradients\n            # TODO 10: Log training metrics and monitor memory usage periodically\n            # TODO 11: Run evaluation at configured intervals and update best metrics\n            # TODO 12: Save checkpoints based on evaluation results and step intervals\n            # TODO 13: Check early stopping conditions based on validation performance\n            # TODO 14: Handle training completion and return best checkpoint path\n            \n            return self._execute_training_loop()\n            \n        except Exception as e:\n            self.logger.error(f\"Training failed: {e}\")\n            # TODO: Implement proper error recovery and cleanup\n            raise\n    \n    def _execute_training_loop(self) -> str:\n        \"\"\"Core training loop implementation.\"\"\"\n        \n        # Training preparation\n        self.model.train()\n        \n        # Main training loop\n        for epoch in range(int(self.config.num_train_epochs)):\n            self.current_epoch = epoch\n            epoch_start_time = time.time()\n            \n            # Epoch training loop\n            epoch_loss = self._train_epoch()\n            \n            # Evaluation\n            if self._should_evaluate(epoch):\n                eval_metrics = self._evaluate()\n                \n                # Update best metrics\n                if eval_metrics.eval_loss < self.best_eval_loss:\n                    self.best_eval_loss = eval_metrics.eval_loss\n                    self.epochs_without_improvement = 0\n                else:\n                    self.epochs_without_improvement += 1\n                \n                # Save checkpoint\n                checkpoint = self._save_checkpoint(eval_metrics.eval_loss)\n                \n                # Check early stopping\n                if self._should_stop_early():\n                    self.logger.info(f\"Early stopping triggered after {epoch + 1} epochs\")\n                    break\n            \n            epoch_time = time.time() - epoch_start_time\n            self.logger.info(f\"Epoch {epoch + 1} completed in {epoch_time:.2f}s, loss: {epoch_loss:.6f}\")\n        \n        # Return path to best checkpoint\n        best_checkpoint_path = self.checkpoint_manager.get_best_checkpoint_path()\n        if best_checkpoint_path:\n            return str(best_checkpoint_path)\n        else:\n            raise RuntimeError(\"No valid checkpoints were saved during training\")\n    \n    def _train_epoch(self) -> float:\n        \"\"\"Train for one complete epoch with gradient accumulation.\"\"\"\n        \n        epoch_loss = 0.0\n        num_batches = 0\n        \n        for batch_idx, batch in enumerate(self.train_dataloader):\n            batch_start_time = time.time()\n            \n            # TODO 1: Move batch to appropriate device (GPU/CPU)\n            # TODO 2: Initialize gradient accumulation for this effective batch\n            # TODO 3: Split batch into micro-batches based on accumulation steps\n            # TODO 4: Process each micro-batch in accumulation loop\n            # TODO 5: Forward pass: compute loss and scale by accumulation steps\n            # TODO 6: Backward pass: accumulate gradients without clearing\n            # TODO 7: After all micro-batches, clip gradients if configured\n            # TODO 8: Perform optimizer step and scheduler update\n            # TODO 9: Clear accumulated gradients for next batch\n            # TODO 10: Log metrics and update progress tracking\n            \n            step_loss = self._train_step(batch)\n            epoch_loss += step_loss\n            num_batches += 1\n            \n            # Memory monitoring\n            if self.global_step % self.config.logging_steps == 0:\n                memory_metrics = self.memory_monitor.measure_current_usage(f\"step_{self.global_step}\")\n                \n                # Log training metrics\n                training_metrics = TrainingMetrics(\n                    step=self.global_step,\n                    epoch=self.current_epoch + (batch_idx / len(self.train_dataloader)),\n                    loss=step_loss,\n                    learning_rate=self.scheduler.get_last_lr()[0],\n                    grad_norm=None,  # Will be populated by gradient clipping\n                    timestamp=time.time(),\n                    gpu_memory_used=memory_metrics.gpu_memory_allocated,\n                    samples_per_second=self.config.per_device_train_batch_size / (time.time() - batch_start_time)\n                )\n                \n                self.metrics_tracker.log_training_metrics(training_metrics)\n        \n        return epoch_loss / max(num_batches, 1)\n    \n    def _train_step(self, batch: Dict[str, torch.Tensor]) -> float:\n        \"\"\"Execute one training step with gradient accumulation.\"\"\"\n        \n        # TODO 1: Move batch tensors to model device\n        # TODO 2: Initialize accumulation variables and clear any existing gradients\n        # TODO 3: Calculate number of micro-batches based on accumulation steps\n        # TODO 4: Split current batch into micro-batches for accumulation\n        # TODO 5: For each micro-batch, perform forward pass and compute loss\n        # TODO 6: Scale loss by number of accumulation steps to maintain gradient magnitude\n        # TODO 7: Perform backward pass to accumulate gradients\n        # TODO 8: After all micro-batches, apply gradient clipping if configured\n        # TODO 9: Perform optimizer step to update model parameters\n        # TODO 10: Update learning rate scheduler and clear gradients\n        # TODO 11: Increment global step counter and return average loss\n        \n        total_loss = 0.0\n        effective_batch_size = self.calculate_effective_batch_size()\n        accumulation_steps = self.config.gradient_accumulation_steps\n        \n        # Clear gradients from previous step\n        self.optimizer.zero_grad()\n        \n        # Gradient accumulation loop\n        for acc_step in range(accumulation_steps):\n            # Extract micro-batch\n            micro_batch = self._extract_micro_batch(batch, acc_step, accumulation_steps)\n            \n            # Forward pass\n            outputs = self.model(**micro_batch)\n            loss = outputs.loss\n            \n            # Scale loss for accumulation\n            scaled_loss = loss / accumulation_steps\n            total_loss += loss.item()\n            \n            # Backward pass\n            scaled_loss.backward()\n        \n        # Gradient clipping\n        grad_norm = None\n        if self.config.max_grad_norm > 0:\n            grad_norm = torch.nn.utils.clip_grad_norm_(\n                self.model.parameters(), \n                self.config.max_grad_norm\n            ).item()\n        \n        # Optimizer step\n        self.optimizer.step()\n        self.scheduler.step()\n        \n        # Update global step\n        self.global_step += 1\n        \n        return total_loss / accumulation_steps\n    \n    def _evaluate(self) -> EvaluationMetrics:\n        \"\"\"Run evaluation on validation dataset.\"\"\"\n        \n        # TODO 1: Set model to evaluation mode and disable gradient computation\n        # TODO 2: Initialize evaluation metrics tracking (loss, perplexity, etc.)\n        # TODO 3: Iterate through evaluation dataset without gradient accumulation\n        # TODO 4: For each batch, perform forward pass and collect predictions\n        # TODO 5: Compute loss values and accumulate for final averages\n        # TODO 6: Calculate task-specific metrics (perplexity, accuracy, etc.)\n        # TODO 7: Aggregate all metrics and compute evaluation statistics\n        # TODO 8: Restore model to training mode before returning\n        # TODO 9: Log evaluation results and update metrics tracking\n        # TODO 10: Return comprehensive evaluation metrics object\n        \n        self.model.eval()\n        total_eval_loss = 0.0\n        total_samples = 0\n        \n        eval_start_time = time.time()\n        \n        with torch.no_grad():\n            for batch in self.eval_dataloader:\n                # Move batch to device\n                batch = {k: v.to(self.model.device) for k, v in batch.items()}\n                \n                # Forward pass\n                outputs = self.model(**batch)\n                loss = outputs.loss\n                \n                total_eval_loss += loss.item()\n                total_samples += batch['input_ids'].size(0)\n        \n        # Calculate final metrics\n        avg_eval_loss = total_eval_loss / len(self.eval_dataloader)\n        perplexity = torch.exp(torch.tensor(avg_eval_loss)).item()\n        eval_runtime = time.time() - eval_start_time\n        \n        eval_metrics = EvaluationMetrics(\n            eval_step=self.global_step,\n            eval_loss=avg_eval_loss,\n            perplexity=perplexity,\n            bleu_score=None,  # TODO: Implement task-specific metrics\n            rouge_scores=None,\n            exact_match=None,\n            eval_samples=total_samples,\n            eval_runtime=eval_runtime,\n            eval_samples_per_second=total_samples / eval_runtime,\n            timestamp=time.time()\n        )\n        \n        # Restore training mode\n        self.model.train()\n        \n        return eval_metrics\n    \n    def calculate_effective_batch_size(self) -> int:\n        \"\"\"Calculate the effective batch size accounting for gradient accumulation.\"\"\"\n        return self.config.per_device_train_batch_size * self.config.gradient_accumulation_steps\n    \n    def calculate_total_training_steps(self) -> int:\n        \"\"\"Calculate total number of training steps for scheduler setup.\"\"\"\n        steps_per_epoch = len(self.train_dataloader) // self.config.gradient_accumulation_steps\n        return steps_per_epoch * self.config.num_train_epochs\n    \n    def _should_evaluate(self, epoch: int) -> bool:\n        \"\"\"Determine if evaluation should be performed at current point.\"\"\"\n        if self.config.evaluation_strategy == \"epoch\":\n            return True\n        elif self.config.evaluation_strategy == \"steps\":\n            return self.global_step % self.config.eval_steps == 0\n        return False\n    \n    def _should_stop_early(self) -> bool:\n        \"\"\"Check if early stopping conditions are met.\"\"\"\n        if not self.config.early_stopping_patience:\n            return False\n        \n        return self.epochs_without_improvement >= self.config.early_stopping_patience\n    \n    def _save_checkpoint(self, eval_loss: float) -> CheckpointMetadata:\n        \"\"\"Save training checkpoint with current state.\"\"\"\n        \n        # TODO 1: Extract model state dict (adapter weights only for LoRA)\n        # TODO 2: Extract optimizer state dict with all momentum/variance buffers\n        # TODO 3: Extract scheduler state dict for proper training resumption\n        # TODO 4: Prepare model and adapter configuration for checkpoint metadata\n        # TODO 5: Call checkpoint manager to save state with atomic operations\n        # TODO 6: Update checkpoint registry and cleanup old checkpoints if needed\n        # TODO 7: Return checkpoint metadata for further processing\n        \n        # Get model state (only adapter weights for LoRA)\n        if hasattr(self.model, 'get_adapter_state_dict'):\n            model_state = self.model.get_adapter_state_dict()\n        else:\n            model_state = self.model.state_dict()\n        \n        # Save checkpoint\n        checkpoint = self.checkpoint_manager.save_checkpoint(\n            model_state=model_state,\n            optimizer_state=self.optimizer.state_dict(),\n            scheduler_state=self.scheduler.state_dict(),\n            step=self.global_step,\n            epoch=self.current_epoch,\n            eval_loss=eval_loss,\n            model_config=self.model.config.to_dict() if hasattr(self.model.config, 'to_dict') else {},\n            adapter_config=getattr(self.config, 'lora', {})\n        )\n        \n        return checkpoint\n```\n\n#### Milestone Checkpoint\n\nAfter implementing the training loop component, verify the following behavior:\n\n**Basic Functionality Test:**\n```bash\npython -m pytest training/tests/test_loop.py::test_training_loop_initialization\npython -m pytest training/tests/test_loop.py::test_gradient_accumulation\n```\n\n**Expected Output:**\n- Training loop initializes without errors\n- Gradient accumulation produces mathematically equivalent results to larger batches\n- Memory monitoring captures baseline and training memory usage\n- Checkpoint manager saves and loads training state correctly\n\n**Manual Verification Steps:**\n1. **Start a short training run** (1 epoch, small dataset) and verify that:\n   - Training loss decreases over steps\n   - Learning rate follows the expected schedule (warmup then decay)\n   - GPU memory usage stabilizes after initial allocation\n   - Checkpoints are saved at the expected intervals\n\n2. **Test checkpoint resumption** by:\n   - Stopping training mid-epoch\n   - Resuming from the latest checkpoint\n   - Verifying that training continues from the correct step and loss value\n\n3. **Verify gradient accumulation** by:\n   - Training with batch_size=4, accumulation_steps=1\n   - Training with batch_size=2, accumulation_steps=2\n   - Confirming that final model weights are nearly identical\n\n**Warning Signs:**\n- Memory usage increasing throughout training (indicates memory leaks)\n- Loss values becoming NaN or infinity (gradient explosion)\n- Training loss not decreasing after several steps (learning rate too low)\n- Checkpoint loading errors (state dict mismatches)\n\n\n## Evaluation and Merging Component\n\n> **Milestone(s):** Milestone 5 - this section implements the evaluation system that measures fine-tuned model performance against baseline metrics, executes LoRA adapter merging for deployment, and exports models to inference-optimized formats\n\nAfter successfully training a model with LoRA adapters, the evaluation and merging component serves as the final checkpoint that validates whether the fine-tuning process achieved its intended objectives. This component measures model performance against both language modeling benchmarks and task-specific metrics, then seamlessly transforms the trained adapters into deployable model formats. The evaluation process provides quantitative evidence of learning progress while the merging functionality consolidates the distributed adapter weights back into a unified model suitable for production inference.\n\n### Mental Model: The Exam Proctor\n\nThink of the evaluation and merging component as a meticulous exam proctor overseeing a comprehensive graduation assessment. Just as a proctor administers standardized tests to measure student learning objectively, this component runs systematic evaluations to quantify how much the model has learned during fine-tuning. The proctor doesn't just check if answers are correct—they measure improvement over baseline performance, ensure the assessment covers relevant skills, and verify that learning transferred appropriately to new scenarios.\n\nWhen the examination concludes successfully, the proctor certifies the student's readiness for real-world application. Similarly, once evaluation confirms the fine-tuned model meets quality thresholds, the merging process consolidates the learned adaptations into a unified credential—the merged model—that can operate independently in production environments. The proctor's final responsibility involves preparing official transcripts in various formats that different institutions can accept, just as model export creates deployment-ready formats optimized for different inference engines.\n\nThe evaluation proctor maintains strict standards throughout the assessment process. They verify that test conditions match real-world scenarios, prevent data leakage between training and evaluation sets, and ensure consistent measurement protocols across different runs. This objectivity prevents the common pitfall of models that appear impressive on training data but fail to generalize to practical applications.\n\n### Perplexity and Language Modeling Metrics\n\nPerplexity serves as the fundamental language modeling metric that measures how well the fine-tuned model predicts validation text sequences. Conceptually, perplexity quantifies the model's \"surprise\" when encountering validation tokens—lower perplexity indicates the model assigns higher probability mass to the actual next tokens, suggesting better language understanding. The mathematical relationship between cross-entropy loss and perplexity provides a interpretable measure where perplexity of 1.0 represents perfect prediction while higher values indicate increasing uncertainty.\n\nThe evaluation system computes perplexity by running inference on the held-out validation set using the same tokenization and attention masking strategies employed during training. This consistency ensures that perplexity measurements reflect genuine model improvements rather than preprocessing artifacts. The system calculates perplexity separately for prompt tokens and response tokens to distinguish between the model's comprehension of instructions versus its generation capabilities.\n\nBeyond basic perplexity calculation, the system tracks perplexity trends across different validation data categories. For instruction-tuning datasets, this means measuring performance separately on different instruction types, response lengths, and domain categories. These granular measurements reveal whether fine-tuning improved performance uniformly or created specialized strengths in particular areas.\n\n| Perplexity Metric | Calculation Method | Interpretation Range | Quality Threshold |\n|------------------|-------------------|---------------------|-------------------|\n| **Overall Perplexity** | exp(mean cross-entropy loss) | 1.0 (perfect) to ∞ (random) | <20 for good instruction following |\n| **Prompt Perplexity** | exp(loss on instruction tokens only) | 1.0 to ∞ | Should remain stable vs base model |\n| **Response Perplexity** | exp(loss on response tokens only) | 1.0 to ∞ | Primary target for improvement |\n| **Length-Conditional Perplexity** | exp(loss grouped by response length) | 1.0 to ∞ | Consistent across length buckets |\n| **Domain-Specific Perplexity** | exp(loss grouped by instruction category) | 1.0 to ∞ | Balanced improvement across domains |\n\nThe system implements perplexity calculation through careful attention mask handling that excludes padding tokens from loss computation while properly accounting for causal attention patterns. Token-level loss values are accumulated across the validation set using numerically stable log-sum-exp operations to prevent floating-point overflow when processing long sequences.\n\n> **Design Insight**: Perplexity alone can be misleading because a model might achieve low perplexity by memorizing training patterns without developing genuine understanding. The evaluation system therefore complements perplexity with task-specific metrics that measure functional capabilities rather than just statistical fit.\n\nPerplexity comparison against the base model baseline establishes whether fine-tuning provided meaningful improvements. The system loads both the fine-tuned model and original base model to run identical evaluation protocols, generating side-by-side perplexity measurements that quantify the learning delta. Significant perplexity reduction on validation data while maintaining reasonable performance on held-out general language modeling benchmarks indicates successful adaptation without catastrophic forgetting.\n\nThe evaluation system also implements perplexity-based early stopping validation by tracking validation perplexity trends throughout training. When validation perplexity begins increasing while training loss continues decreasing, this signals overfitting that requires training termination or checkpoint rollback to the best-performing state.\n\n### Task-Specific Evaluation\n\nWhile perplexity measures general language modeling capability, task-specific evaluation assesses whether the fine-tuned model actually performs better on the intended downstream applications. This evaluation dimension runs domain-relevant benchmarks that test functional capabilities like instruction following, factual accuracy, reasoning consistency, and response appropriateness. Unlike perplexity's focus on token prediction probability, task-specific metrics evaluate the semantic quality and practical utility of generated responses.\n\nThe evaluation system supports multiple task-specific measurement protocols depending on the fine-tuning objective. For instruction-tuning applications, this includes exact match scoring for factual questions, BLEU and ROUGE metrics for text generation quality, and semantic similarity measurements using embedding-based comparisons. Each metric captures different aspects of model performance that collectively provide comprehensive quality assessment.\n\n| Task-Specific Metric | Measurement Method | Score Range | Typical Improvement Target |\n|---------------------|-------------------|-------------|---------------------------|\n| **Exact Match Accuracy** | Percentage of responses matching reference exactly | 0-100% | +10-20% over base model |\n| **BLEU Score** | N-gram overlap with reference responses | 0-100 | +5-15 points over baseline |\n| **ROUGE-L F1** | Longest common subsequence with references | 0-1.0 | +0.05-0.15 over baseline |\n| **Semantic Similarity** | Cosine similarity of response embeddings | -1.0-1.0 | +0.1-0.3 over base model |\n| **Instruction Following Rate** | Percentage following instruction format/constraints | 0-100% | +20-40% over base model |\n| **Factual Consistency** | Percentage of factually accurate statements | 0-100% | Maintain base model level |\n\nThe benchmark evaluation process involves generating responses from both the fine-tuned model and base model using identical prompts and generation parameters. This controlled comparison isolates the impact of fine-tuning by removing confounding variables like different sampling strategies or temperature settings. The system generates multiple responses per prompt when evaluating stochastic metrics, then reports both mean performance and confidence intervals.\n\nFor instruction-following evaluation, the system implements specialized scoring functions that assess whether generated responses adhere to explicit constraints in the instruction prompts. This includes checking output format requirements, length constraints, style specifications, and content guidelines. Instruction-following metrics often provide more actionable feedback than perplexity because they directly measure the behaviors that fine-tuning aimed to improve.\n\nThe evaluation system integrates with external benchmark suites like HellaSwag, MMLU, or domain-specific evaluation datasets relevant to the fine-tuning task. This integration involves formatting the fine-tuned model's responses according to each benchmark's expected input/output schema and computing official benchmark scores for comparison with published baselines.\n\n> **Design Principle**: Task-specific evaluation must use held-out data that was never seen during training or validation to provide genuine generalization assessment. The system enforces strict data isolation by checking for overlap between training samples and evaluation prompts.\n\nBeyond aggregate scores, the evaluation system performs detailed error analysis by categorizing failure modes and identifying systematic weaknesses in the fine-tuned model. This analysis examines which types of instructions the model handles well versus poorly, whether errors stem from factual knowledge gaps or reasoning failures, and how response quality varies across different domains or complexity levels.\n\nThe comparative evaluation framework generates comprehensive reports showing before-and-after performance across all measured dimensions. These reports include statistical significance testing to determine whether observed improvements are meaningful rather than random variation. The system flags cases where fine-tuning improved some metrics while degrading others, highlighting potential trade-offs in model capabilities.\n\n### LoRA Adapter Merging\n\nOnce evaluation confirms that fine-tuning achieved satisfactory performance improvements, the adapter merging process consolidates the learned LoRA parameters back into the base model weights. This merging operation transforms the distributed parameter structure—where base model weights remain frozen and adaptation occurs through separate low-rank matrices—into a unified parameter set that contains all learned adaptations directly in the model weights.\n\nThe mathematical foundation of LoRA adapter merging involves reconstructing the full-rank weight updates from the low-rank decomposition, then adding these updates to the corresponding base model parameters. For each target module with LoRA adapters, the system computes the full adaptation matrix by multiplying the down-projection matrix A with the up-projection matrix B, scales this result by the alpha parameter, then adds it to the original frozen weights.\n\n| Merging Operation | Mathematical Formula | Memory Requirement | Computational Cost |\n|------------------|---------------------|-------------------|-------------------|\n| **Adapter Weight Reconstruction** | ΔW = α * B @ A | rank × (input_dim + output_dim) | O(rank × input_dim × output_dim) |\n| **Base Weight Update** | W_new = W_base + ΔW | full parameter size | O(input_dim × output_dim) |\n| **Bias Integration** | b_new = b_base + Δb (if enabled) | output_dim | O(output_dim) |\n| **Layer Normalization Merge** | ln_new = ln_base + Δln (if targeted) | hidden_dim | O(hidden_dim) |\n\nThe merging process requires careful attention to parameter dtypes and device placement to avoid numerical precision loss or memory overflow. The system performs merging operations in float32 precision even when the base model uses lower precision formats, then converts the merged weights back to the target precision after integration. This approach prevents accumulation of quantization errors during the weight combination process.\n\nAdapter merging operates module-by-module to manage memory usage efficiently. Rather than loading all parameters simultaneously, the system iterates through target modules, loads the relevant LoRA matrices and base weights, performs the merging computation, stores the updated weights, then releases intermediate tensors before proceeding to the next module. This streaming approach enables merging of models that exceed available GPU memory.\n\nThe system implements verification procedures that confirm merged model behavior matches the original LoRA-adapted model within numerical precision tolerances. This verification involves running identical forward passes through both the unmerged LoRA model and merged model, comparing the output activations layer-by-layer to detect any discrepancies that might indicate merging errors.\n\n> **Critical Implementation Detail**: The merging process must account for different LoRA configurations across target modules. Some modules might use different ranks, alpha scaling factors, or dropout rates, requiring per-module parameter handling rather than uniform merging operations.\n\nAfter successful merging, the system performs cleanup operations that remove all LoRA-specific parameters, metadata, and configuration artifacts from the model state. The resulting merged model contains only standard transformer parameters and can be used with any inference framework that supports the base model architecture, eliminating dependencies on PEFT or LoRA-specific libraries.\n\nThe merging component also supports partial merging operations where only a subset of adapters are integrated into the base model. This capability enables experimentation with different adapter combinations or selective integration of adapters that showed positive evaluation results while excluding those that degraded performance on certain metrics.\n\n### Model Export and Deployment\n\nThe model export functionality transforms the merged model into deployment-ready formats optimized for different inference environments and hardware platforms. This transformation process involves converting between different serialization formats, applying additional quantization for inference efficiency, and generating auxiliary files required for model loading and tokenization in deployment systems.\n\nThe primary export target is the HuggingFace format that maintains full compatibility with the transformers library and associated inference tools. This export involves saving the merged model weights using the `save_pretrained` method, preserving the model configuration, tokenizer settings, and generation parameters in the standard directory structure expected by HuggingFace pipelines.\n\nFor deployment scenarios requiring maximum inference efficiency, the system supports export to GGUF format compatible with llama.cpp and similar optimized inference engines. GGUF export involves additional quantization steps that convert float16 or float32 weights to 4-bit or 8-bit integer formats using quantization schemes optimized for fast CPU or mobile inference.\n\n| Export Format | Target Runtime | Quantization Options | Inference Speed | Model Size Reduction |\n|---------------|----------------|---------------------|-----------------|---------------------|\n| **HuggingFace** | transformers, vLLM, text-generation-inference | fp16, bf16, int8 | Baseline | 50% (fp16) |\n| **GGUF Q4_0** | llama.cpp, ollama | 4-bit symmetric | 2-3x faster | 75% reduction |\n| **GGUF Q8_0** | llama.cpp CPU inference | 8-bit symmetric | 1.5-2x faster | 50% reduction |\n| **ONNX** | ONNXRuntime, server deployments | fp16, int8 | Variable | 50-75% reduction |\n| **TensorRT** | NVIDIA inference servers | fp16, int8, int4 | 3-5x faster | 50-85% reduction |\n\nThe GGUF export process requires integration with external conversion utilities that handle the complex format transformation and quantization procedures. The system manages this integration by preparing the merged model in the expected input format, invoking the conversion tools with appropriate parameters, then validating the resulting GGUF file through test inference runs.\n\nQuality validation during export ensures that format conversion and additional quantization do not degrade model performance beyond acceptable thresholds. The system runs comparative evaluations between the original merged model and each exported format, measuring perplexity and task-specific metrics to quantify quality preservation across different export targets.\n\nThe export system generates deployment packages that include not only the converted model weights but also all auxiliary files required for inference deployment. This includes tokenizer configurations, generation parameter presets, model metadata, and example usage scripts tailored to the target deployment environment.\n\n> **Deployment Consideration**: Different export formats involve trade-offs between inference speed, memory usage, and model quality. The system provides guidance on format selection based on deployment constraints and performance requirements.\n\nFor production deployment scenarios, the export process includes security scanning that checks for potential vulnerabilities in the model weights or metadata. This scanning identifies suspicious patterns that might indicate backdoors, data leakage, or other security concerns that require attention before deployment.\n\nThe system maintains export provenance by generating detailed metadata that tracks the complete lineage from original base model through fine-tuning to final export. This provenance information includes training configuration, evaluation results, merging parameters, and export settings, enabling reproducible deployments and audit trails for model governance.\n\n### Architecture Decision Records\n\nThe evaluation and merging component involves several critical design decisions that significantly impact both the accuracy of performance measurement and the efficiency of model deployment. These decisions balance evaluation completeness against computational cost while ensuring that merged models maintain the quality improvements achieved during fine-tuning.\n\n> **Decision: Evaluation Frequency During Training**\n> - **Context**: Evaluation can be performed after every epoch, at fixed step intervals, or only at the end of training, with trade-offs between training speed and monitoring granularity\n> - **Options Considered**: \n>   1. Every epoch evaluation for complete monitoring\n>   2. Fixed step interval evaluation (e.g., every 500 steps)\n>   3. End-of-training evaluation only\n> - **Decision**: Configurable evaluation frequency with default every 0.25 epochs for instruction tuning tasks\n> - **Rationale**: Instruction tuning often shows rapid initial improvement followed by slower convergence, requiring frequent early monitoring to detect overfitting while allowing longer intervals during stable training phases\n> - **Consequences**: Enables early stopping and optimal checkpoint selection at the cost of 10-15% training time overhead for evaluation runs\n\n| Option | Training Speed Impact | Overfitting Detection | Early Stopping Effectiveness | Resource Usage |\n|--------|----------------------|----------------------|------------------------------|----------------|\n| **Every Epoch** | -20% slower | Excellent | Optimal | High GPU/memory |\n| **Every 0.25 Epochs** | -10% slower | Good | Effective | Moderate |\n| **End Only** | No impact | None | Not available | Minimal |\n\n> **Decision: Perplexity Calculation Methodology**\n> - **Context**: Perplexity can be calculated across all tokens, excluding instruction tokens, or with different weighting schemes for prompts versus responses\n> - **Options Considered**:\n>   1. Standard perplexity across all non-padding tokens\n>   2. Response-only perplexity excluding instruction tokens\n>   3. Weighted perplexity with higher emphasis on response quality\n> - **Decision**: Separate calculation of prompt perplexity, response perplexity, and overall perplexity with primary optimization target on response perplexity\n> - **Rationale**: Instruction tuning should maintain instruction comprehension (stable prompt perplexity) while improving response generation (decreasing response perplexity), requiring separate measurement\n> - **Consequences**: Provides granular insight into model improvements at the cost of additional computation and storage for multiple perplexity metrics\n\n> **Decision: Adapter Merging Memory Strategy**\n> - **Context**: Merging adapters can be performed in-memory for speed or with checkpoint streaming for memory efficiency, particularly important for large models\n> - **Options Considered**:\n>   1. Full in-memory merging for maximum speed\n>   2. Checkpoint-based streaming merging for memory efficiency  \n>   3. Hybrid approach with in-memory merging for smaller modules\n> - **Decision**: Module-wise streaming merging with configurable batch size for simultaneous module processing\n> - **Rationale**: Large language models often exceed GPU memory even without gradients and optimizer states, requiring streaming approaches that balance memory usage with reasonable merging speed\n> - **Consequences**: Enables merging of arbitrarily large models at the cost of increased merging time and implementation complexity\n\n| Strategy | Memory Usage | Merging Speed | Model Size Limit | Implementation Complexity |\n|----------|--------------|---------------|------------------|---------------------------|\n| **Full In-Memory** | Very High | Fastest | GPU memory limit | Simple |\n| **Module Streaming** | Low | Moderate | Unlimited | Moderate |\n| **Hybrid Batching** | Configurable | Fast | Flexible | Complex |\n\n> **Decision: Export Format Priority and Quality Validation**\n> - **Context**: Different deployment environments require different export formats, each with specific quantization and optimization trade-offs\n> - **Options Considered**:\n>   1. HuggingFace format only for maximum compatibility\n>   2. GGUF format priority for inference efficiency\n>   3. Multiple format support with quality validation\n> - **Decision**: Multi-format export with HuggingFace as primary and GGUF as secondary, including automated quality validation for each format\n> - **Rationale**: Production deployments require flexibility between research compatibility (HuggingFace) and inference efficiency (GGUF), while quality validation prevents deployment of degraded models\n> - **Consequences**: Comprehensive deployment support at the cost of longer export times and increased storage requirements for multiple format versions\n\n### Common Pitfalls\n\nThe evaluation and merging component presents several common failure modes that can invalidate performance measurements or corrupt model deployments. Understanding these pitfalls helps developers implement robust evaluation procedures and avoid deployment disasters.\n\n⚠️ **Pitfall: Evaluation Data Leakage**\nMany developers accidentally include training samples in evaluation datasets or use evaluation data that shares significant overlap with training content. This contamination leads to artificially inflated performance metrics that don't reflect genuine generalization capability. The symptom appears as unrealistically high evaluation scores that don't match real-world performance after deployment. To prevent this issue, implement strict data provenance tracking that verifies evaluation samples were never seen during training, and use content-based deduplication to identify near-duplicates that might bias evaluation results.\n\n⚠️ **Pitfall: Inconsistent Evaluation Protocols**\nComparing base model and fine-tuned model performance using different generation parameters, tokenization settings, or prompt formats produces meaningless evaluation results. This often happens when developers use different inference scripts or model loading procedures for baseline versus fine-tuned evaluation. The fix requires implementing unified evaluation pipelines that apply identical generation settings, prompt templates, and post-processing steps to both models being compared.\n\n⚠️ **Pitfall: Numerical Precision Loss During Merging**\nAdapter merging operations can introduce numerical errors when performed in reduced precision or with incorrect dtype handling. This problem manifests as subtle performance degradation in the merged model compared to the unmerged LoRA version, particularly noticeable in tasks requiring precise numerical reasoning. The solution involves performing all merging computations in float32 precision regardless of storage format, then converting to target precision only after weight integration is complete.\n\n⚠️ **Pitfall: Missing Validation of Merged Model Equivalence**\nDevelopers often assume that merged models behave identically to their unmerged LoRA counterparts without verification, leading to deployment of models with unexpected behavior changes. This issue can result from incorrect merging implementations, precision issues, or configuration mismatches. The prevention strategy requires implementing automated verification that runs identical prompts through both unmerged and merged models, comparing outputs for numerical equivalence within expected tolerance bounds.\n\n⚠️ **Pitfall: Export Format Compatibility Assumptions**\nConverting models between formats (HuggingFace to GGUF, for example) without validating compatibility can produce models that fail to load or generate incorrect outputs in the target inference environment. This often occurs because different formats have varying support for model architectures, attention mechanisms, or tokenization schemes. The solution involves implementing comprehensive compatibility testing that loads exported models in their target runtime environments and validates inference behavior before deployment approval.\n\n⚠️ **Pitfall: Inadequate Task-Specific Benchmark Coverage**\nFocusing evaluation exclusively on perplexity or a narrow set of task-specific metrics can miss important capability degradations or improvements in areas not covered by the evaluation suite. This limitation becomes apparent when deployed models perform poorly on user tasks that weren't represented in the evaluation benchmarks. The mitigation approach involves implementing comprehensive evaluation suites that cover diverse instruction types, reasoning patterns, and domain knowledge areas relevant to the intended deployment scenarios.\n\n⚠️ **Pitfall: Ignoring Base Model Comparison Context**\nEvaluating fine-tuned models in isolation without comparing against base model performance makes it impossible to distinguish actual improvements from measurement noise or evaluation artifacts. This problem leads to overconfidence in fine-tuning effectiveness when apparent improvements might be within normal variance bounds. The correction requires implementing side-by-side evaluation protocols that measure both models using identical procedures and report improvement deltas with statistical significance assessments.\n\n### Implementation Guidance\n\nThe evaluation and merging component requires careful integration of model loading, metric computation, adapter manipulation, and format conversion capabilities. The implementation balances evaluation completeness with computational efficiency while ensuring reliable model deployment preparation.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| **Perplexity Calculation** | Manual cross-entropy computation with PyTorch | HuggingFace evaluate library with custom metrics |\n| **Task-Specific Metrics** | Basic BLEU/ROUGE with nltk/rouge-score | Comprehensive evaluation with evaluate + custom benchmarks |\n| **Adapter Merging** | Manual weight combination with PyTorch operations | PEFT library merge_and_unload with verification |\n| **Model Export** | HuggingFace save_pretrained for basic export | llama.cpp integration + multiple format support |\n| **Benchmark Integration** | Custom evaluation scripts with manual scoring | Integration with lm-evaluation-harness framework |\n\n#### Recommended File Structure\n\nThe evaluation and merging component integrates with the overall pipeline structure while maintaining clear separation of evaluation, merging, and export responsibilities:\n\n```\nsrc/\n  evaluation/\n    __init__.py                    ← evaluation component exports\n    metrics/\n      __init__.py\n      perplexity.py               ← perplexity calculation utilities\n      task_specific.py            ← BLEU, ROUGE, exact match metrics\n      benchmark_runner.py         ← external benchmark integration\n    evaluator.py                  ← main evaluation orchestration\n    comparator.py                 ← base vs fine-tuned comparison\n  merging/\n    __init__.py\n    adapter_merger.py             ← LoRA weight merging operations\n    model_validator.py            ← merged model verification\n    export_manager.py             ← multi-format model export\n  utils/\n    model_loader.py               ← unified model loading utilities\n    format_converter.py           ← format conversion helpers\ntests/\n  test_evaluation.py              ← evaluation pipeline tests\n  test_merging.py                 ← adapter merging tests\n```\n\n#### Core Evaluation Infrastructure\n\n```python\n\"\"\"\nEvaluation infrastructure for measuring fine-tuned model performance.\nProvides perplexity calculation, task-specific metrics, and baseline comparison.\n\"\"\"\n\nimport torch\nimport torch.nn.functional as F\nfrom typing import Dict, List, Optional, Tuple, Any\nfrom dataclasses import dataclass\nimport numpy as np\nfrom transformers import PreTrainedModel, PreTrainedTokenizer\nimport logging\n\nfrom ..data_model import EvaluationMetrics, InstructionSample, TokenizedSample\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass PerplexityResult:\n    \"\"\"Results from perplexity calculation with detailed breakdowns.\"\"\"\n    overall_perplexity: float\n    prompt_perplexity: float  \n    response_perplexity: float\n    token_count: int\n    prompt_token_count: int\n    response_token_count: int\n    loss_values: List[float]\n\nclass PerplexityCalculator:\n    \"\"\"\n    Calculates perplexity metrics for language model evaluation.\n    Supports separate calculation for prompt and response tokens.\n    \"\"\"\n    \n    def __init__(self, model: PreTrainedModel, tokenizer: PreTrainedTokenizer):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.model.eval()\n    \n    def calculate_perplexity(self, samples: List[TokenizedSample]) -> PerplexityResult:\n        \"\"\"\n        Calculate perplexity across a dataset of tokenized samples.\n        \n        Args:\n            samples: List of tokenized instruction-response pairs\n            \n        Returns:\n            PerplexityResult with overall and component perplexities\n        \"\"\"\n        # TODO 1: Initialize loss accumulators for overall, prompt, and response\n        # TODO 2: Initialize token counters for each component\n        # TODO 3: Set model to evaluation mode and disable gradients\n        # TODO 4: Iterate through samples in batches for memory efficiency\n        # TODO 5: For each batch, compute forward pass and extract logits\n        # TODO 6: Calculate cross-entropy loss for prompt tokens (labels = input_ids)\n        # TODO 7: Calculate cross-entropy loss for response tokens only  \n        # TODO 8: Accumulate losses and token counts with proper masking\n        # TODO 9: Convert accumulated losses to perplexity using exp()\n        # TODO 10: Return PerplexityResult with all computed metrics\n        pass\n    \n    def _compute_batch_loss(self, batch: Dict[str, torch.Tensor]) -> Tuple[float, float, int, int]:\n        \"\"\"\n        Compute loss for a single batch with prompt/response separation.\n        \n        Returns:\n            Tuple of (prompt_loss, response_loss, prompt_tokens, response_tokens)\n        \"\"\"\n        # TODO 1: Move batch tensors to model device\n        # TODO 2: Run forward pass to get logits\n        # TODO 3: Shift logits and labels for causal language modeling\n        # TODO 4: Create masks for prompt vs response tokens\n        # TODO 5: Calculate cross-entropy loss for each component\n        # TODO 6: Return losses and token counts\n        pass\n\nclass TaskSpecificEvaluator:\n    \"\"\"\n    Evaluates model performance on task-specific metrics like BLEU, ROUGE, exact match.\n    Compares fine-tuned model against baseline model performance.\n    \"\"\"\n    \n    def __init__(self, model: PreTrainedModel, tokenizer: PreTrainedTokenizer,\n                 baseline_model: Optional[PreTrainedModel] = None):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.baseline_model = baseline_model\n        \n    def evaluate_instruction_following(self, samples: List[InstructionSample], \n                                     max_new_tokens: int = 512) -> Dict[str, float]:\n        \"\"\"\n        Evaluate instruction following capability using multiple metrics.\n        \n        Args:\n            samples: Evaluation samples with reference responses\n            max_new_tokens: Maximum tokens to generate per response\n            \n        Returns:\n            Dictionary of metric name to score mappings\n        \"\"\"\n        # TODO 1: Generate responses from fine-tuned model for all samples\n        # TODO 2: Generate baseline responses if baseline model available\n        # TODO 3: Calculate exact match accuracy against references\n        # TODO 4: Calculate BLEU scores using n-gram overlap\n        # TODO 5: Calculate ROUGE-L scores using longest common subsequence\n        # TODO 6: Calculate semantic similarity using embedding comparisons\n        # TODO 7: Assess instruction format compliance and constraint following\n        # TODO 8: Compile results into comprehensive metrics dictionary\n        pass\n    \n    def _generate_response(self, instruction: str, model: PreTrainedModel) -> str:\n        \"\"\"Generate a single response using specified model and generation parameters.\"\"\"\n        # TODO 1: Apply chat template to format instruction\n        # TODO 2: Tokenize formatted prompt\n        # TODO 3: Generate response with temperature=0.7, top_p=0.9\n        # TODO 4: Decode generated tokens and extract response portion\n        # TODO 5: Return cleaned response text\n        pass\n\nclass ModelComparator:\n    \"\"\"\n    Compares fine-tuned model performance against baseline model.\n    Provides statistical significance testing and improvement quantification.\n    \"\"\"\n    \n    def __init__(self, fine_tuned_model: PreTrainedModel, \n                 baseline_model: PreTrainedModel, tokenizer: PreTrainedTokenizer):\n        self.fine_tuned_model = fine_tuned_model\n        self.baseline_model = baseline_model\n        self.tokenizer = tokenizer\n        \n    def run_comparative_evaluation(self, eval_samples: List[InstructionSample]) -> Dict[str, Any]:\n        \"\"\"\n        Run comprehensive comparison between fine-tuned and baseline models.\n        \n        Returns:\n            Detailed comparison results with improvement measurements\n        \"\"\"\n        # TODO 1: Calculate perplexity for both models using identical samples\n        # TODO 2: Run task-specific evaluation for both models\n        # TODO 3: Generate side-by-side response comparisons\n        # TODO 4: Calculate improvement deltas for each metric\n        # TODO 5: Perform statistical significance testing\n        # TODO 6: Generate detailed comparison report\n        pass\n```\n\n#### LoRA Adapter Merging Infrastructure\n\n```python\n\"\"\"\nLoRA adapter merging functionality for consolidating fine-tuned parameters.\nHandles weight combination, verification, and cleanup operations.\n\"\"\"\n\nimport torch\nfrom typing import Dict, Optional, List\nfrom transformers import PreTrainedModel\nfrom peft import PeftModel, get_peft_model_state_dict\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass AdapterMerger:\n    \"\"\"\n    Handles merging of LoRA adapters back into base model weights.\n    Provides verification and cleanup functionality.\n    \"\"\"\n    \n    def __init__(self, peft_model: PeftModel):\n        self.peft_model = peft_model\n        self.base_model = peft_model.get_base_model()\n        \n    def merge_adapters(self, verification: bool = True) -> PreTrainedModel:\n        \"\"\"\n        Merge LoRA adapter weights into base model parameters.\n        \n        Args:\n            verification: Whether to verify merged model equivalence\n            \n        Returns:\n            Standalone model with merged weights\n        \"\"\"\n        # TODO 1: Extract adapter state dict from PEFT model\n        # TODO 2: Identify all target modules with LoRA adapters\n        # TODO 3: For each target module, compute full-rank weight update\n        # TODO 4: Add computed updates to base model parameters\n        # TODO 5: Handle different adapter configurations per module\n        # TODO 6: Clean up PEFT-specific parameters and metadata\n        # TODO 7: If verification enabled, run equivalence testing\n        # TODO 8: Return standalone merged model\n        pass\n    \n    def _merge_module_weights(self, module_name: str, base_weight: torch.Tensor,\n                            lora_A: torch.Tensor, lora_B: torch.Tensor, \n                            alpha: float, scaling: float) -> torch.Tensor:\n        \"\"\"\n        Merge LoRA matrices into base weight for a single module.\n        \n        Returns:\n            Updated weight tensor with LoRA adaptations integrated\n        \"\"\"\n        # TODO 1: Ensure all tensors are in float32 for numerical stability\n        # TODO 2: Compute full-rank update: delta_W = alpha * (B @ A) * scaling\n        # TODO 3: Add delta_W to base_weight\n        # TODO 4: Convert result back to original dtype if needed\n        # TODO 5: Return merged weight tensor\n        pass\n    \n    def verify_merger_equivalence(self, merged_model: PreTrainedModel, \n                                test_inputs: List[torch.Tensor]) -> bool:\n        \"\"\"\n        Verify that merged model produces equivalent outputs to original PEFT model.\n        \n        Args:\n            merged_model: Model with merged adapters\n            test_inputs: List of test input tensors for verification\n            \n        Returns:\n            True if outputs are numerically equivalent within tolerance\n        \"\"\"\n        # TODO 1: Set both models to evaluation mode\n        # TODO 2: Disable gradients for inference\n        # TODO 3: Run identical forward passes through both models\n        # TODO 4: Compare output tensors with numerical tolerance (1e-5)\n        # TODO 5: Check intermediate activations if full verification needed\n        # TODO 6: Return True if all outputs match within tolerance\n        pass\n\nclass ModelExporter:\n    \"\"\"\n    Exports merged models to various deployment formats.\n    Supports HuggingFace, GGUF, and other inference-optimized formats.\n    \"\"\"\n    \n    def __init__(self, model: PreTrainedModel, tokenizer: PreTrainedTokenizer):\n        self.model = model\n        self.tokenizer = tokenizer\n        \n    def export_huggingface(self, output_path: str, push_to_hub: bool = False) -> str:\n        \"\"\"\n        Export model in HuggingFace format for transformers compatibility.\n        \n        Args:\n            output_path: Local directory path for model export\n            push_to_hub: Whether to upload to HuggingFace Hub\n            \n        Returns:\n            Path to exported model directory\n        \"\"\"\n        # TODO 1: Create output directory structure\n        # TODO 2: Save model weights using save_pretrained\n        # TODO 3: Save tokenizer configuration and vocabulary\n        # TODO 4: Generate model card with training details\n        # TODO 5: Save generation configuration and parameters\n        # TODO 6: If push_to_hub enabled, upload to repository\n        # TODO 7: Return path to exported model\n        pass\n    \n    def export_gguf(self, output_path: str, quantization: str = \"q4_0\") -> str:\n        \"\"\"\n        Export model in GGUF format for llama.cpp inference.\n        \n        Args:\n            output_path: Path for GGUF model file\n            quantization: Quantization type (q4_0, q8_0, f16)\n            \n        Returns:\n            Path to exported GGUF file\n        \"\"\"\n        # TODO 1: Prepare model in format expected by conversion script\n        # TODO 2: Save temporary HuggingFace format for conversion\n        # TODO 3: Invoke llama.cpp conversion utility with quantization\n        # TODO 4: Validate GGUF file can be loaded by llama.cpp\n        # TODO 5: Clean up temporary files\n        # TODO 6: Return path to GGUF model file\n        pass\n    \n    def validate_export_quality(self, exported_model_path: str, \n                               export_format: str) -> Dict[str, float]:\n        \"\"\"\n        Validate that exported model maintains acceptable quality.\n        \n        Returns:\n            Quality metrics comparing exported vs original model\n        \"\"\"\n        # TODO 1: Load exported model in appropriate format\n        # TODO 2: Prepare test dataset for quality comparison\n        # TODO 3: Run inference with both original and exported models\n        # TODO 4: Calculate perplexity and task-specific metrics\n        # TODO 5: Compare quality metrics and flag significant degradation\n        # TODO 6: Return comprehensive quality comparison results\n        pass\n```\n\n#### Milestone Checkpoint\n\nAfter implementing the evaluation and merging component, verify the following behaviors to ensure correct functionality:\n\n**Evaluation Validation:**\n1. Run `python -m evaluation.evaluator --model-path ./checkpoints/best --eval-data ./data/validation.jsonl` to execute comprehensive evaluation\n2. Verify that perplexity calculations produce reasonable values (typically 5-50 for instruction-tuned models)\n3. Check that task-specific metrics show improvement over baseline model performance\n4. Confirm evaluation reports include both aggregate scores and detailed per-category breakdowns\n\n**Adapter Merging Verification:**\n1. Execute `python -m merging.adapter_merger --checkpoint ./checkpoints/best --output ./merged_model` to merge adapters\n2. Verify that merged model directory contains standard HuggingFace format files (pytorch_model.bin, config.json, tokenizer files)\n3. Load merged model and run test inference to confirm generation capability\n4. Compare inference outputs between merged and unmerged models to verify equivalence\n\n**Export Quality Testing:**\n1. Run `python -m merging.export_manager --model-path ./merged_model --formats huggingface,gguf` to export multiple formats\n2. Load exported HuggingFace model and verify it generates coherent responses\n3. Test GGUF model with llama.cpp to confirm compatibility and inference speed\n4. Compare perplexity scores across original, merged, and exported models to validate quality preservation\n\n**Expected Output Patterns:**\n- Evaluation reports should show 10-30% improvement in task-specific metrics compared to base model\n- Merged model should produce identical outputs to unmerged LoRA model within 1e-5 numerical tolerance\n- GGUF export should reduce model size by 60-75% while maintaining within 5% of original perplexity\n- Export validation should confirm successful loading in target inference environments\n\n\n## Interactions and Data Flow\n\n> **Milestone(s):** All milestones - this section describes how the components work together throughout the complete fine-tuning pipeline, from initial data loading through final model deployment\n\nThe fine-tuning pipeline orchestrates a complex sequence of operations where each component transforms data and coordinates state with downstream components. Think of this as **The Assembly Line Conductor** - a factory floor manager who ensures that raw materials (training data) flow smoothly through specialized workstations (components), with each station receiving exactly what it needs when it needs it, while maintaining quality control and handling disruptions gracefully. The conductor doesn't just pass materials along; they coordinate timing, manage shared resources, handle bottlenecks, and ensure that the final product (fine-tuned model) meets all specifications.\n\nThe pipeline's architecture follows a **producer-consumer pattern with state coordination**, where each component produces outputs that become inputs for downstream components, while sharing configuration and state information through well-defined interfaces. Unlike a simple linear pipeline, this system requires bidirectional communication for error propagation, progress reporting, and dynamic resource management. The `FineTuningPipeline` class serves as the central coordinator, maintaining shared state and orchestrating component interactions while ensuring data consistency and error recovery.\n\n### End-to-End Pipeline Flow\n\nThe complete pipeline execution follows a carefully orchestrated sequence that transforms raw training data into a deployment-ready fine-tuned model. This flow involves five major phases, each with multiple internal steps and coordination points.\n\n#### Phase 1: Configuration and Initialization\n\nThe pipeline begins with configuration loading and system initialization, establishing the foundation for all subsequent operations. The `FineTuningPipeline` coordinates this startup sequence to ensure all components receive consistent configuration and system resources are properly allocated.\n\n| Step | Operation | Component | Input | Output | State Changes |\n|------|-----------|-----------|--------|--------|---------------|\n| 1.1 | Load configuration | Pipeline | YAML config file | `TrainingConfig` object | Configuration validated and stored |\n| 1.2 | Initialize memory monitor | `MemoryMonitor` | System specs | Baseline memory metrics | Baseline memory state captured |\n| 1.3 | Validate hardware compatibility | Pipeline | GPU specs, memory | Compatibility report | Hardware constraints established |\n| 1.4 | Initialize logging system | Pipeline | Log configuration | Logger instances | Logging infrastructure active |\n| 1.5 | Create output directories | Pipeline | Output paths | Directory structure | File system prepared |\n\nThe initialization phase establishes critical invariants that subsequent phases depend on. The memory monitor captures baseline measurements before any model loading occurs, enabling accurate tracking of memory usage throughout the pipeline. Hardware validation ensures that the quantization configuration and model size are compatible with available GPU memory, preventing out-of-memory failures during training.\n\n> **Critical Insight**: The initialization phase must establish all shared state and resource constraints before any component begins processing. Attempting to modify system-level configuration after model loading can lead to inconsistent behavior and resource conflicts.\n\n#### Phase 2: Model and Data Preparation\n\nThis phase loads the base model with quantization and prepares the training dataset, requiring careful coordination between memory management and data processing. The order of operations is critical - quantization must occur during model loading, and LoRA adapters must be injected before the training data loader is created.\n\n| Step | Operation | Component | Input | Output | Dependencies |\n|------|-----------|-----------|--------|--------|--------------|\n| 2.1 | Load quantized base model | QLoRA Component | Model name, quant config | Quantized model | Memory baseline established |\n| 2.2 | Initialize tokenizer | Pipeline | Model name, special tokens | Configured tokenizer | Model loaded successfully |\n| 2.3 | Detect target modules | LoRA Component | Model architecture | Target module list | Model architecture analyzed |\n| 2.4 | Configure LoRA adapters | LoRA Component | Rank, alpha, targets | `LoRAConfig` object | Target modules identified |\n| 2.5 | Inject adapters | LoRA Component | Base model, LoRA config | PEFT model with adapters | LoRA configuration validated |\n| 2.6 | Load and validate training data | Dataset Component | Data file paths | Raw data samples | File system accessible |\n| 2.7 | Apply data formatting | Dataset Component | Raw samples, tokenizer | `InstructionSample` objects | Data structure validated |\n| 2.8 | Create train/val split | Dataset Component | Formatted samples | Split datasets | Data quality metrics computed |\n| 2.9 | Initialize data loaders | Pipeline | Split datasets, batch size | PyTorch DataLoaders | Memory available for batching |\n\nThe model preparation sequence is particularly sensitive to memory management. The quantized model loading triggers the largest single memory allocation, and the memory monitor tracks this carefully. If quantization fails due to insufficient memory, the pipeline can attempt fallback configurations or provide detailed error diagnostics.\n\n**Data preparation coordination** requires the tokenizer from the model loading step, creating a dependency chain that must be respected. The chat template application depends on the specific model's tokenizer configuration, and the train/validation split must occur after quality filtering to ensure representative sampling.\n\n#### Phase 3: Training Orchestration\n\nThe training phase represents the most complex component interaction, with the training loop coordinating between data loading, forward/backward passes, gradient accumulation, evaluation, and checkpointing. The `TrainingOrchestrator` manages this coordination while monitoring for failure conditions.\n\n| Training Step | Operations | Components Involved | Data Flow | State Updates |\n|---------------|------------|---------------------|-----------|---------------|\n| Training Step Setup | Load batch, move to GPU, prepare inputs | DataLoader, Training Loop | Raw batch → GPU tensors | Step counter incremented |\n| Forward Pass | Model inference, loss calculation | PEFT Model, Loss Function | Input tensors → loss scalar | Gradients computed |\n| Backward Pass | Gradient computation, accumulation | Optimizer, PEFT Model | Loss → parameter gradients | Gradient buffers updated |\n| Optimizer Step | Parameter updates, scheduler step | Optimizer, LR Scheduler | Gradients → parameter updates | Learning rate updated |\n| Evaluation Trigger | Check evaluation schedule | Training Loop | Step count → evaluation decision | Evaluation flag set |\n| Checkpoint Save | Model state serialization | Checkpoint Manager | Model state → checkpoint file | Best model tracking updated |\n\nThe training orchestration must handle **gradient accumulation** across multiple micro-batches, which requires careful state management to ensure that gradients accumulate correctly before the optimizer step. The effective batch size calculation impacts learning rate scaling and convergence behavior.\n\n**Evaluation coordination** occurs at scheduled intervals, requiring the training loop to pause, switch the model to evaluation mode, run validation samples through the evaluation component, and then resume training. This state transition must preserve the training state exactly.\n\n> **Design Decision**: **Synchronous vs Asynchronous Evaluation**\n> - **Context**: Evaluation can be time-consuming and might benefit from background processing\n> - **Options Considered**: \n>   - Synchronous evaluation that blocks training progress\n>   - Asynchronous evaluation using separate processes\n>   - Cached evaluation using model fingerprinting\n> - **Decision**: Synchronous evaluation with configurable frequency\n> - **Rationale**: Model state consistency is critical for accurate evaluation, and the complexity of state synchronization across processes outweighs performance benefits for the target use case\n> - **Consequences**: Training pauses during evaluation, but evaluation results are guaranteed to reflect the exact model state at that training step\n\n#### Phase 4: Evaluation and Quality Assessment\n\nThe evaluation phase performs comprehensive model assessment using multiple metrics and comparison strategies. This phase coordinates between perplexity calculation, task-specific evaluation, and baseline comparison to provide a complete picture of fine-tuning effectiveness.\n\n| Evaluation Type | Component | Input | Output | Dependencies |\n|-----------------|-----------|-------|---------|-------------|\n| Perplexity Calculation | `PerplexityCalculator` | Validation samples, model | `PerplexityResult` | Model in eval mode |\n| Task-Specific Metrics | `TaskSpecificEvaluator` | Test samples, model | Accuracy, BLEU, ROUGE scores | Generation working properly |\n| Baseline Comparison | `ModelComparator` | Fine-tuned model, base model | Improvement metrics | Both models loaded |\n| Memory Impact Analysis | `MemoryMonitor` | Current usage, baseline | Memory efficiency report | Memory tracking active |\n| Adapter Analysis | `ParameterAnalyzer` | PEFT model | Efficiency metrics | LoRA adapters injected |\n\nThe evaluation coordination requires loading both the fine-tuned model and the baseline model simultaneously for direct comparison, which creates additional memory pressure. The pipeline must monitor available memory and potentially implement evaluation strategies that avoid loading both models simultaneously if memory is constrained.\n\n**Quality thresholds** guide decision-making about whether to continue training, stop early, or adjust hyperparameters. The evaluation component provides recommendations based on convergence patterns and performance relative to baseline metrics.\n\n#### Phase 5: Model Merging and Export\n\nThe final phase merges LoRA adapters back into the base model and exports the result in deployment-ready formats. This phase requires careful verification to ensure that the merged model maintains equivalent behavior to the adapter-based model.\n\n| Export Step | Operation | Component | Input | Output | Verification |\n|-------------|-----------|-----------|-------|--------|-------------|\n| Adapter Merging | Combine LoRA weights with base weights | `AdapterMerger` | PEFT model | Merged model | Numerical equivalence check |\n| Merged Model Validation | Test output equivalence | `ModelComparator` | Original + merged models | Validation report | Output comparison passed |\n| HuggingFace Export | Save in HF format | `ModelExporter` | Merged model, tokenizer | HF model files | Format validation |\n| GGUF Export | Convert to GGUF format | `ModelExporter` | Merged model | GGUF file | llama.cpp compatibility |\n| Quality Verification | Test exported model quality | `ModelExporter` | Exported model | Quality metrics | Performance threshold met |\n| Deployment Package | Bundle model and metadata | Pipeline | All export artifacts | Deployment package | Complete package validation |\n\n**Merger verification** is critical because floating-point arithmetic during weight combination can introduce numerical differences. The verification process runs identical inputs through both the adapter-based and merged models, comparing outputs within acceptable tolerance bounds.\n\n### Inter-Component Communication\n\nComponents communicate through well-defined interfaces that specify data formats, method signatures, and state management protocols. This communication follows both synchronous method calls for direct data transformation and asynchronous signaling for progress reporting and error propagation.\n\n#### Primary Data Interfaces\n\nThe pipeline defines standardized data structures that flow between components, ensuring type safety and consistent data representation throughout the system.\n\n| Interface | Producer Component | Consumer Component | Data Structure | Purpose |\n|-----------|-------------------|-------------------|----------------|---------|\n| Raw Data → Formatted Data | Dataset Preparation | Training Loop | `List[InstructionSample]` | Standardized instruction-response pairs |\n| Formatted Data → Tokenized Data | Dataset Preparation | Training Loop | `List[TokenizedSample]` | GPU-ready tensor data |\n| Model Configuration → Quantized Model | QLoRA Quantization | LoRA Configuration | `PreTrainedModel` + `QuantizationConfig` | Memory-optimized base model |\n| LoRA Configuration → Adapted Model | LoRA Configuration | Training Loop | `PeftModel` | Model with trainable adapters |\n| Training State → Checkpoints | Training Loop | Evaluation Component | `CheckpointMetadata` | Serialized model state |\n| Evaluation Results → Decisions | Evaluation Component | Training Loop | `EvaluationMetrics` | Performance assessment |\n| Final Model → Export Formats | Evaluation Component | Model Export | `PreTrainedModel` | Deployment-ready model |\n\n**Data transformation protocols** ensure that each component receives data in the expected format. For example, the dataset preparation component guarantees that all `InstructionSample` objects include required fields and pass quality validation before being passed to the tokenization stage.\n\n#### Configuration Propagation\n\nConfiguration information flows from the central `TrainingConfig` object to individual components, with each component extracting relevant parameters and validating compatibility with its requirements.\n\n| Component | Configuration Extract | Validation Requirements | Error Handling |\n|-----------|----------------------|------------------------|----------------|\n| QLoRA Quantization | `quantization: QuantizationConfig` | GPU compatibility, model size limits | Fallback to FP16, detailed error reporting |\n| LoRA Configuration | `lora: LoRAConfig` | Target modules exist, rank feasibility | Auto-detect targets, adjust rank recommendations |\n| Training Loop | Learning rate, batch size, epochs | Memory constraints, convergence parameters | Gradient accumulation adjustment, LR scaling |\n| Dataset Preparation | Data paths, quality filters | File accessibility, format compatibility | Format auto-detection, quality threshold adjustment |\n| Evaluation | Metrics configuration, baseline model | Evaluation data availability | Skip unavailable metrics, baseline-free evaluation |\n\n**Configuration validation** occurs at component initialization, allowing early detection of incompatible settings before expensive operations like model loading begin. Components provide detailed error messages that include suggested fixes or alternative configurations.\n\n#### Progress and Error Signaling\n\nComponents communicate status updates and error conditions through a combination of return values, exception propagation, and event signaling.\n\n| Signal Type | Components Involved | Information Carried | Handler Actions |\n|-------------|-------------------|-------------------|----------------|\n| Progress Updates | All components → Pipeline coordinator | Step counts, completion percentages | Update progress bars, log milestones |\n| Memory Warnings | Memory Monitor → All components | Current usage, threshold breaches | Trigger garbage collection, reduce batch sizes |\n| Training Metrics | Training Loop → Logging/Monitoring | Loss values, learning rates, throughput | Log to WandB/TensorBoard, trigger early stopping |\n| Quality Alerts | Evaluation Component → Training Loop | Performance degradation, convergence issues | Adjust learning rate, trigger checkpointing |\n| Error Conditions | Any component → Pipeline coordinator | Exception details, recovery suggestions | Attempt recovery, clean shutdown, detailed reporting |\n\n**Error propagation** follows a structured approach where components catch and wrap exceptions with context-specific information before re-raising them. This allows the pipeline coordinator to make informed decisions about recovery strategies.\n\n#### Method Call Patterns\n\nComponent interactions follow consistent patterns that simplify testing and maintenance while ensuring predictable behavior.\n\n| Pattern | Example Methods | Purpose | Error Handling |\n|---------|-----------------|---------|----------------|\n| Configuration Validation | `validate_configuration()` | Pre-flight checks | Return detailed validation errors |\n| Resource Initialization | `setup_model_and_tokenizer()` | One-time setup operations | Clean up partial state on failure |\n| Data Transformation | `tokenize_sample()`, `apply_template()` | Convert data between formats | Validate inputs, provide fallbacks |\n| State Queries | `get_peak_usage()`, `calculate_efficiency_metrics()` | Non-mutating information retrieval | Never fail, return best available data |\n| State Mutations | `inject_adapters()`, `merge_adapters()` | Modify component state | Atomic operations, rollback on failure |\n| Batch Processing | `filter_dataset()`, `split_dataset()` | Process collections efficiently | Progress reporting, partial results on interruption |\n\n> **Architecture Decision**: **Synchronous vs Asynchronous Method Calls**\n> - **Context**: Component operations vary widely in execution time, from millisecond tokenization to minute-scale model loading\n> - **Options Considered**:\n>   - Pure synchronous calls with blocking behavior\n>   - Async/await patterns for long-running operations  \n>   - Thread-based background processing\n> - **Decision**: Synchronous calls with progress callbacks for long-running operations\n> - **Rationale**: Maintains simple control flow and error handling while providing responsiveness through progress reporting. GPU operations are inherently synchronous, and the added complexity of async coordination outweighs benefits.\n> - **Consequences**: UI responsiveness depends on progress callback frequency, but error handling and state management remain straightforward.\n\n### State Coordination and Dependencies\n\nThe pipeline maintains both shared global state and component-specific local state, with carefully managed dependencies to ensure consistency and enable recovery from failures.\n\n#### Global State Management\n\nThe `FineTuningPipeline` maintains global state that multiple components need to access or coordinate around. This state is managed through a centralized coordinator to prevent inconsistencies and race conditions.\n\n| State Category | Data Structure | Owner | Readers | Writers | Synchronization |\n|----------------|----------------|--------|---------|---------|-----------------|\n| Configuration | `TrainingConfig` | Pipeline | All components | Pipeline only | Immutable after initialization |\n| Memory Metrics | `MemoryMonitor` | Memory Monitor | All components | Memory Monitor only | Thread-safe updates |\n| Training Progress | `TrainingMetrics` | Training Loop | Evaluation, Logging | Training Loop only | Atomic metric updates |\n| Model Checkpoints | `CheckpointMetadata` | Checkpoint Manager | Training, Evaluation | Checkpoint Manager only | File-system locking |\n| Evaluation Results | `EvaluationMetrics` | Evaluation Component | Training Loop, Logging | Evaluation Component only | Event-driven updates |\n\n**State access patterns** follow the principle of single-writer, multiple-reader to avoid consistency issues. Components that need to coordinate state changes do so through the pipeline coordinator rather than direct communication.\n\n#### Component Dependency Graph\n\nComponents have both initialization dependencies (what must be set up before this component can initialize) and runtime dependencies (what must be available during operation).\n\n| Component | Initialization Dependencies | Runtime Dependencies | Failure Impact |\n|-----------|----------------------------|---------------------|----------------|\n| QLoRA Quantization | Memory baseline, GPU availability | CUDA drivers, sufficient VRAM | Cannot load model - pipeline abort |\n| LoRA Configuration | Quantized model loaded | Model architecture stable | Cannot create adapters - pipeline abort |\n| Dataset Preparation | File system access, tokenizer | None after initialization | Cannot create training data - pipeline abort |\n| Training Loop | PEFT model, training data, optimizer | GPU memory, data loader | Cannot train - pipeline abort |\n| Evaluation | Trained model, validation data | Model in eval mode, generation working | Cannot evaluate - continue with limited metrics |\n\n**Dependency resolution** occurs during pipeline initialization, with each component declaring its dependencies and the pipeline coordinator ensuring proper ordering. If dependencies cannot be satisfied, the pipeline provides detailed error messages explaining what's missing and potential solutions.\n\n#### State Persistence and Recovery\n\nThe pipeline implements comprehensive checkpointing that captures both component state and coordination state, enabling recovery from various failure modes.\n\n| Checkpoint Type | Frequency | Contents | Recovery Scenario |\n|-----------------|-----------|----------|-------------------|\n| Full Model Checkpoint | Every N steps | Model weights, optimizer state, LoRA adapters | Training interruption, system restart |\n| Configuration Snapshot | At initialization | Complete training configuration, validation | Configuration validation, reproducibility |\n| Progress Checkpoint | Every step | Training metrics, evaluation results | Progress monitoring, performance analysis |\n| Component State | On state changes | Component-specific persistent state | Component failure, partial recovery |\n| Dependency Metadata | At setup completion | Component initialization order, versions | Debugging initialization failures |\n\n**Recovery coordination** follows a hierarchical approach where the pipeline coordinator determines what level of recovery is possible based on available checkpoint data and current system state.\n\n#### Memory Coordination\n\nMemory management requires coordination across all components because GPU memory is a shared, limited resource that can cause system-wide failures if not managed carefully.\n\n| Memory Pool | Manager | Allocation Strategy | Overflow Handling |\n|-------------|---------|--------------------| ------------------|\n| Base Model Weights | QLoRA Quantization | Single large allocation at startup | Fallback quantization levels, detailed OOM reporting |\n| LoRA Adapters | LoRA Configuration | Small allocations per target module | Reduce target modules, warn about capacity limits |\n| Training Batches | Training Loop | Dynamic allocation per batch | Reduce batch size, increase gradient accumulation |\n| Optimizer States | Training Loop | Parallel to model parameters | Use memory-efficient optimizers, offload to CPU |\n| Evaluation Buffers | Evaluation Component | Temporary allocation during eval | Skip memory-intensive metrics, batch evaluation |\n\n**Memory pressure detection** uses the `MemoryMonitor` to track allocation patterns and predict when out-of-memory conditions might occur. Components receive memory pressure signals and can adjust their resource usage before failures occur.\n\n> **Critical Design Principle**: **Graceful Degradation Under Memory Pressure**\n> \n> The pipeline is designed to maintain functionality even when memory constraints prevent optimal operation. Components implement fallback strategies that trade performance or features for memory efficiency, allowing the pipeline to complete training even on resource-constrained hardware.\n\n#### Inter-Component Error Recovery\n\nWhen component failures occur, the recovery strategy depends on the failure type, the point in the pipeline where it occurred, and what state can be preserved.\n\n| Failure Type | Detection Method | Recovery Strategy | State Preservation |\n|--------------|------------------|-------------------|-------------------|\n| Data Loading Error | Exception during file I/O | Skip corrupted samples, continue with valid data | Maintain data quality metrics |\n| Model Loading OOM | CUDA out-of-memory exception | Retry with more aggressive quantization | Preserve configuration for retry |\n| Training Step Failure | Loss becomes NaN or infinite | Restore last checkpoint, reduce learning rate | Roll back to last stable state |\n| Evaluation Failure | Generation timeout or error | Skip current evaluation, continue training | Maintain training progress |\n| Export Failure | File system or format error | Retry with different format, manual intervention | Preserve merged model in memory |\n\n**Error context propagation** ensures that when failures occur, sufficient information is available to diagnose the root cause and determine appropriate recovery actions. Each component adds context-specific information to exceptions before propagating them to the coordinator.\n\n### Common Pitfalls in Pipeline Orchestration\n\n⚠️ **Pitfall: Configuration Mutation After Initialization**\n\nMany developers attempt to modify configuration objects after components have been initialized, leading to inconsistent behavior where some components operate with old configuration while others use new settings. This is particularly problematic with memory-related settings that affect resource allocation.\n\nThe problem occurs because components often cache configuration-derived values during initialization. For example, the QLoRA quantization component calculates memory allocation sizes based on the quantization configuration, and changing the configuration afterward doesn't trigger recalculation.\n\n**Solution**: Make configuration objects immutable after the initialization phase completes. If configuration changes are needed, restart the pipeline with new configuration rather than attempting in-place modification.\n\n⚠️ **Pitfall: Ignoring Component Initialization Order**\n\nAttempting to initialize components in the wrong order leads to subtle bugs where dependencies are not properly established. For example, injecting LoRA adapters before the base model is fully loaded can result in adapter injection failures or incorrect target module detection.\n\nThis happens because developers often focus on individual component implementation without understanding the dependency graph. The base model architecture must be fully analyzed before LoRA target modules can be identified, and the tokenizer configuration must be stable before chat templates can be applied.\n\n**Solution**: Implement explicit dependency declarations for each component and use topological sorting to determine initialization order automatically. Validate that all dependencies are satisfied before allowing component initialization to proceed.\n\n⚠️ **Pitfall: Memory Pressure Cascade Failures**\n\nWhen one component encounters memory pressure and fails, it often triggers a cascade of failures in other components that depend on it. For example, if batch size needs to be reduced due to memory constraints, this affects gradient accumulation calculations, learning rate scaling, and convergence behavior.\n\nThe cascade occurs because components make assumptions about resource availability based on initial configuration. When these assumptions are violated, components don't have fallback strategies and simply fail rather than adapting.\n\n**Solution**: Implement memory pressure signaling throughout the pipeline, allowing components to adjust their resource usage proactively. Design components with fallback strategies that maintain functionality even when optimal resource allocation is not available.\n\n⚠️ **Pitfall: State Synchronization Race Conditions**\n\nWhen multiple components need to coordinate state changes, race conditions can occur if synchronization is not properly managed. This is particularly common between the training loop and evaluation component, where evaluation state must be synchronized with training progress.\n\nThe problem manifests as evaluation results that don't correspond to the reported training step, or checkpoint metadata that doesn't match the actual model state. This makes debugging and reproducibility extremely difficult.\n\n**Solution**: Use a centralized state coordinator that manages all cross-component state changes through atomic operations. Implement state versioning so that components can verify they're operating on consistent state snapshots.\n\n⚠️ **Pitfall: Incomplete Error Recovery State Cleanup**\n\nWhen recovery from component failures is attempted, incomplete cleanup of partial state can leave the pipeline in an inconsistent condition. For example, if LoRA adapter injection fails partway through, some modules might have adapters while others don't, leading to training instability.\n\nThis occurs because error handling code often focuses on the immediate failure without considering all the state changes that occurred before the failure point. Partial state can persist and cause problems in subsequent operations.\n\n**Solution**: Implement transactional state changes where possible, with automatic rollback on failure. For complex operations that can't be made atomic, maintain explicit cleanup procedures that restore all affected components to a known good state.\n\n### Implementation Guidance\n\nThis section provides practical implementation patterns and starter code for orchestrating the fine-tuning pipeline components.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Pipeline Orchestration | Class-based coordinator with method chaining | State machine with event-driven transitions |\n| Configuration Management | YAML files with Pydantic validation | Hydra with composition and overrides |\n| Progress Reporting | Simple print statements with progress bars | Rich console with structured logging |\n| Error Handling | Try-catch blocks with context managers | Custom exception hierarchy with recovery strategies |\n| State Persistence | Pickle-based checkpoints | HuggingFace Accelerate state management |\n| Memory Monitoring | PyTorch CUDA memory functions | nvidia-ml-py with detailed GPU metrics |\n\n#### Recommended File Structure\n\n```\nllm_fine_tuning_pipeline/\n├── core/\n│   ├── pipeline.py              ← FineTuningPipeline coordinator\n│   ├── config.py                ← Configuration data structures  \n│   ├── state_manager.py         ← Global state coordination\n│   └── memory_monitor.py        ← Memory usage tracking\n├── components/\n│   ├── dataset_preparation.py   ← Data loading and preprocessing\n│   ├── lora_configuration.py    ← LoRA adapter management\n│   ├── quantization.py          ← QLoRA quantization handling\n│   ├── training_loop.py         ← Training orchestration\n│   └── evaluation.py            ← Model evaluation and export\n├── utils/\n│   ├── checkpoint_manager.py    ← Checkpoint saving and loading\n│   ├── error_recovery.py        ← Error handling utilities\n│   └── progress_tracking.py     ← Progress reporting helpers\n├── tests/\n│   ├── integration/             ← End-to-end pipeline tests\n│   └── component/               ← Individual component tests\n└── examples/\n    ├── basic_fine_tuning.py     ← Simple pipeline usage\n    └── advanced_config.yaml     ← Complex configuration example\n```\n\n#### Pipeline Coordinator Infrastructure\n\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Any, Optional, List\nfrom pathlib import Path\nimport logging\nimport torch\nfrom contextlib import contextmanager\n\nfrom .config import TrainingConfig\nfrom .memory_monitor import MemoryMonitor\nfrom .state_manager import StateManager\nfrom components.dataset_preparation import DatasetPreparation\nfrom components.lora_configuration import LoRAConfiguration\nfrom components.quantization import QLoRAQuantization\nfrom components.training_loop import TrainingLoop\nfrom components.evaluation import EvaluationComponent\n\nclass FineTuningPipeline:\n    \"\"\"\n    Central coordinator for the LLM fine-tuning pipeline.\n    \n    Orchestrates component initialization, data flow, and state management\n    throughout the complete fine-tuning process from raw data to deployed model.\n    \"\"\"\n    \n    def __init__(self, config: TrainingConfig):\n        self.config = config\n        self.memory_monitor = MemoryMonitor()\n        self.state_manager = StateManager()\n        self.logger = self._setup_logging()\n        \n        # Component instances - initialized during setup\n        self.dataset_prep: Optional[DatasetPreparation] = None\n        self.quantization: Optional[QLoRAQuantization] = None\n        self.lora_config: Optional[LoRAConfiguration] = None\n        self.training_loop: Optional[TrainingLoop] = None\n        self.evaluation: Optional[EvaluationComponent] = None\n        \n        # Shared state\n        self.tokenizer = None\n        self.model = None\n        self.training_data = None\n        self.validation_data = None\n    \n    @contextmanager\n    def pipeline_context(self):\n        \"\"\"Context manager for pipeline execution with proper cleanup.\"\"\"\n        try:\n            self.memory_monitor.capture_baseline()\n            self.state_manager.initialize()\n            self.logger.info(\"Pipeline context initialized\")\n            yield self\n        except Exception as e:\n            self.logger.error(f\"Pipeline execution failed: {e}\")\n            self._cleanup_on_failure()\n            raise\n        finally:\n            self._final_cleanup()\n            self.logger.info(\"Pipeline context cleaned up\")\n    \n    def _setup_logging(self) -> logging.Logger:\n        \"\"\"Initialize structured logging for the pipeline.\"\"\"\n        # TODO: Configure logging based on config.logging settings\n        # TODO: Set up file handlers, console handlers, and formatters\n        # TODO: Configure log levels and filtering\n        # TODO: Initialize WandB or TensorBoard integration if specified\n        pass\n    \n    def _cleanup_on_failure(self):\n        \"\"\"Clean up resources when pipeline fails.\"\"\"\n        # TODO: Clear GPU memory and cached tensors\n        # TODO: Close file handles and temporary resources\n        # TODO: Save partial state for debugging and recovery\n        # TODO: Send failure notifications if configured\n        pass\n    \n    def _final_cleanup(self):\n        \"\"\"Final cleanup regardless of success or failure.\"\"\"\n        # TODO: Clear all GPU memory allocations\n        # TODO: Close logging handlers and flush buffers\n        # TODO: Generate final resource usage report\n        # TODO: Clean up temporary files and directories\n        pass\n\ndef load_config_from_yaml(config_path: Path) -> TrainingConfig:\n    \"\"\"Load and validate pipeline configuration from YAML file.\"\"\"\n    # TODO: Parse YAML file with proper error handling\n    # TODO: Validate all required fields are present\n    # TODO: Check for configuration consistency (e.g., memory limits vs model size)\n    # TODO: Apply default values for optional fields\n    # TODO: Return fully validated TrainingConfig object\n    pass\n```\n\n#### State Manager Implementation\n\n```python\nimport threading\nfrom typing import Dict, Any, Optional, Callable\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom datetime import datetime\n\nclass PipelineState(Enum):\n    UNINITIALIZED = \"uninitialized\"\n    INITIALIZING = \"initializing\" \n    DATA_PREPARATION = \"data_preparation\"\n    MODEL_LOADING = \"model_loading\"\n    ADAPTER_CONFIGURATION = \"adapter_configuration\"\n    TRAINING = \"training\"\n    EVALUATION = \"evaluation\"\n    EXPORT = \"export\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n\n@dataclass\nclass StateSnapshot:\n    \"\"\"Immutable snapshot of pipeline state at a point in time.\"\"\"\n    state: PipelineState\n    timestamp: datetime\n    metrics: Dict[str, Any] = field(default_factory=dict)\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    \nclass StateManager:\n    \"\"\"Thread-safe manager for global pipeline state coordination.\"\"\"\n    \n    def __init__(self):\n        self._lock = threading.RLock()\n        self._current_state = PipelineState.UNINITIALIZED\n        self._state_history: List[StateSnapshot] = []\n        self._state_callbacks: Dict[PipelineState, List[Callable]] = {}\n        self._shared_data: Dict[str, Any] = {}\n    \n    def initialize(self):\n        \"\"\"Initialize the state manager for a new pipeline run.\"\"\"\n        with self._lock:\n            # TODO: Clear any previous state and initialize fresh\n            # TODO: Set up state transition validation rules\n            # TODO: Initialize shared data storage with thread safety\n            # TODO: Register default state change callbacks\n            pass\n    \n    def transition_to(self, new_state: PipelineState, \n                     metrics: Optional[Dict[str, Any]] = None,\n                     metadata: Optional[Dict[str, Any]] = None):\n        \"\"\"Thread-safe state transition with validation and callbacks.\"\"\"\n        with self._lock:\n            # TODO: Validate that transition from current_state to new_state is allowed\n            # TODO: Create state snapshot with timestamp and provided data\n            # TODO: Update current state and add snapshot to history\n            # TODO: Execute registered callbacks for the new state\n            # TODO: Log state transition with appropriate detail level\n            pass\n    \n    def get_current_state(self) -> StateSnapshot:\n        \"\"\"Get immutable snapshot of current pipeline state.\"\"\"\n        with self._lock:\n            # TODO: Create and return StateSnapshot of current state\n            # TODO: Include copy of current metrics and metadata\n            # TODO: Ensure returned snapshot is fully immutable\n            pass\n    \n    def register_state_callback(self, state: PipelineState, callback: Callable):\n        \"\"\"Register callback to execute when entering specified state.\"\"\"\n        # TODO: Add callback to the appropriate state's callback list\n        # TODO: Validate callback signature is compatible\n        # TODO: Handle duplicate registrations appropriately\n        pass\n    \n    def set_shared_data(self, key: str, value: Any):\n        \"\"\"Thread-safe storage of shared data across components.\"\"\"\n        with self._lock:\n            # TODO: Store value in shared data dictionary\n            # TODO: Validate key format and value serializability\n            # TODO: Log data updates for debugging purposes\n            pass\n    \n    def get_shared_data(self, key: str, default: Any = None) -> Any:\n        \"\"\"Thread-safe retrieval of shared data.\"\"\"\n        with self._lock:\n            # TODO: Return value from shared data or default if key missing\n            # TODO: Consider implementing key expiration if needed\n            # TODO: Log data access for debugging purposes\n            pass\n```\n\n#### Core Pipeline Flow Implementation\n\n```python\nclass FineTuningPipeline:\n    # ... (previous methods) ...\n    \n    def run_complete_pipeline(self) -> Dict[str, Any]:\n        \"\"\"Execute the complete fine-tuning pipeline from start to finish.\"\"\"\n        with self.pipeline_context():\n            try:\n                # Phase 1: Initialize components and validate configuration\n                self.state_manager.transition_to(PipelineState.INITIALIZING)\n                # TODO: Call setup_model_and_tokenizer() and handle failures\n                # TODO: Call setup_lora_adapters() with dependency checking\n                # TODO: Validate memory constraints are satisfied\n                \n                # Phase 2: Prepare training data\n                self.state_manager.transition_to(PipelineState.DATA_PREPARATION)\n                # TODO: Call prepare_datasets() with progress tracking\n                # TODO: Validate data quality and format compliance\n                # TODO: Log dataset statistics and splits\n                \n                # Phase 3: Execute training loop\n                self.state_manager.transition_to(PipelineState.TRAINING)\n                # TODO: Call execute_training() with checkpoint management\n                # TODO: Monitor training progress and handle early stopping\n                # TODO: Save best checkpoint path for evaluation\n                \n                # Phase 4: Evaluate and export model\n                self.state_manager.transition_to(PipelineState.EVALUATION)\n                # TODO: Call evaluate_and_merge() with quality validation\n                # TODO: Generate comprehensive evaluation report\n                # TODO: Export model in requested formats\n                \n                self.state_manager.transition_to(PipelineState.COMPLETED)\n                return self._generate_completion_report()\n                \n            except Exception as e:\n                self.state_manager.transition_to(PipelineState.FAILED, \n                                                metadata={\"error\": str(e)})\n                self.logger.error(f\"Pipeline failed: {e}\", exc_info=True)\n                raise\n    \n    def setup_model_and_tokenizer(self) -> None:\n        \"\"\"Initialize quantized model and tokenizer with memory monitoring.\"\"\"\n        self.state_manager.transition_to(PipelineState.MODEL_LOADING)\n        \n        # TODO: Initialize QLoRAQuantization component with config.quantization\n        # TODO: Load base model with quantization and monitor memory usage\n        # TODO: Initialize tokenizer and configure special tokens for instruction tuning\n        # TODO: Store model and tokenizer in shared state for component access\n        # TODO: Validate model loaded successfully and is in expected precision\n        # TODO: Log model statistics including parameter count and memory usage\n        pass\n    \n    def setup_lora_adapters(self) -> None:\n        \"\"\"Configure and inject LoRA adapters into the quantized base model.\"\"\"\n        if self.model is None:\n            raise ValueError(\"Base model must be loaded before LoRA adapter setup\")\n        \n        # TODO: Initialize LoRAConfiguration component with config.lora\n        # TODO: Detect target modules based on model architecture\n        # TODO: Create and inject LoRA adapters with specified rank and alpha\n        # TODO: Freeze base model parameters and verify only adapters are trainable\n        # TODO: Calculate and log parameter efficiency metrics\n        # TODO: Store adapted model in shared state for training loop access\n        pass\n    \n    def prepare_datasets(self) -> None:\n        \"\"\"Load, format, and split training data for the fine-tuning process.\"\"\"\n        if self.tokenizer is None:\n            raise ValueError(\"Tokenizer must be initialized before dataset preparation\")\n        \n        # TODO: Initialize DatasetPreparation component with data file paths\n        # TODO: Load raw data and apply quality filtering with statistics tracking\n        # TODO: Format samples using instruction-response templates and chat formatting\n        # TODO: Tokenize formatted samples with proper attention masks and padding\n        # TODO: Split into training and validation sets with stratification if requested\n        # TODO: Create PyTorch DataLoaders with appropriate batching and shuffling\n        # TODO: Store training and validation data in shared state\n        # TODO: Log comprehensive dataset statistics and quality metrics\n        pass\n    \n    def execute_training(self) -> str:\n        \"\"\"Run the training loop with progress monitoring and checkpoint management.\"\"\"\n        if not all([self.model, self.training_data, self.validation_data]):\n            raise ValueError(\"Model and datasets must be prepared before training\")\n        \n        # TODO: Initialize TrainingLoop component with model, data, and training config\n        # TODO: Set up optimizer, learning rate scheduler, and gradient accumulation\n        # TODO: Configure checkpoint saving frequency and early stopping criteria\n        # TODO: Execute training loop with progress reporting and metric logging\n        # TODO: Monitor for convergence, overfitting, or training instability\n        # TODO: Handle training interruption gracefully with state preservation\n        # TODO: Return path to best checkpoint based on validation performance\n        pass\n    \n    def evaluate_and_merge(self, checkpoint_path: str) -> Dict[str, Any]:\n        \"\"\"Evaluate trained model performance and merge adapters for deployment.\"\"\"\n        # TODO: Initialize EvaluationComponent with trained model and baseline\n        # TODO: Calculate perplexity on validation set with detailed breakdown\n        # TODO: Run task-specific evaluation metrics if configured\n        # TODO: Compare performance against baseline model quantitatively\n        # TODO: Merge LoRA adapters into base model with verification\n        # TODO: Export merged model in HuggingFace and GGUF formats as requested\n        # TODO: Validate exported models maintain acceptable quality\n        # TODO: Return comprehensive evaluation results and export metadata\n        pass\n    \n    def _generate_completion_report(self) -> Dict[str, Any]:\n        \"\"\"Generate comprehensive report of pipeline execution and results.\"\"\"\n        # TODO: Collect metrics from all pipeline phases\n        # TODO: Calculate resource usage statistics and efficiency metrics\n        # TODO: Generate model performance summary with baseline comparisons\n        # TODO: Include export information and deployment recommendations\n        # TODO: Format report for human readability and programmatic access\n        pass\n```\n\n#### Milestone Checkpoints\n\n**After Dataset Preparation (Milestone 1):**\n- Run: `python -m llm_fine_tuning_pipeline.examples.basic_fine_tuning --data-only`\n- Expected: Dataset statistics logged, train/validation splits created, tokenized samples ready\n- Verify: Check that `InstructionSample` objects have all required fields and `TokenizedSample` objects have proper attention masks\n\n**After LoRA Configuration (Milestone 2):**\n- Run: `python -m llm_fine_tuning_pipeline.examples.basic_fine_tuning --setup-only`\n- Expected: LoRA adapters injected, parameter efficiency report generated, memory usage logged\n- Verify: Confirm that only 0.1-1% of parameters are trainable and target modules are correctly identified\n\n**After Training Completion (Milestone 4):**\n- Run: Complete training pipeline and check final metrics\n- Expected: Training loss converged, validation metrics improved over baseline, checkpoints saved\n- Verify: Best checkpoint can be loaded successfully and generates coherent responses\n\n**After Evaluation and Export (Milestone 5):**\n- Run: Full pipeline with export to both HuggingFace and GGUF formats\n- Expected: Merged model shows equivalent performance to adapter version, exports load correctly\n- Verify: Use llama.cpp to load GGUF model and confirm it generates reasonable outputs\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | Diagnosis Method | Fix |\n|---------|--------------|------------------|-----|\n| Pipeline hangs during initialization | Circular component dependencies | Check dependency graph in state manager | Reorder component initialization sequence |\n| Memory usage grows unexpectedly | Memory leak in component interactions | Use memory profiler during each phase | Add explicit cleanup in component destructors |\n| Training metrics don't match between runs | Non-deterministic component initialization | Check random seed propagation | Set seeds in each component initialization |\n| Evaluation results inconsistent | Model state not synchronized between training and eval | Verify model.eval() calls and shared state | Use state manager to coordinate mode switches |\n| Export format validation fails | Precision loss during adapter merging | Compare outputs before/after merging | Increase numerical tolerance or use higher precision |\n\n\n## Error Handling and Edge Cases\n\n> **Milestone(s):** All milestones - robust error handling and recovery strategies are essential throughout the entire fine-tuning pipeline, from data preparation through model export\n\nWhen fine-tuning large language models on resource-constrained hardware, failures are not exceptional cases but expected realities. The parameter-efficient fine-tuning pipeline must gracefully handle hardware limitations, training instabilities, data quality issues, and infrastructure failures while preserving training progress and enabling recovery. This section establishes comprehensive error handling strategies that maintain system reliability even under adverse conditions.\n\n### Mental Model: The Safety Net System\n\nThink of error handling in the fine-tuning pipeline like a comprehensive safety net system in a high-wire performance. Just as acrobats use multiple layers of protection—primary safety nets, backup harnesses, spotters, and emergency protocols—our pipeline employs layered defensive strategies. The primary nets catch common failures like memory exhaustion before they cause crashes. Backup systems preserve training state when hardware fails. Recovery protocols restore operation from the last known good state. This multi-layered approach ensures that even when individual components fail, the entire performance doesn't come crashing down, and the show can continue from where it left off.\n\nThe key insight is that in machine learning pipelines, graceful degradation is often more valuable than perfect operation. A system that can detect memory pressure and automatically reduce batch size is more useful than one that crashes with an out-of-memory error. A training loop that saves frequent checkpoints and recovers seamlessly from hardware failures enables longer, more reliable fine-tuning runs than one that requires manual intervention after every GPU driver crash.\n\n### Hardware and Memory Failures\n\nHardware failures represent the most common and challenging class of errors in GPU-based fine-tuning. Memory exhaustion, GPU driver crashes, thermal throttling, and sudden power losses can interrupt training at any moment. The pipeline must detect these conditions early, implement preventive measures, and recover gracefully when prevention fails.\n\n#### Memory Pressure Detection and Management\n\nMemory management forms the foundation of reliable fine-tuning on consumer hardware. The system continuously monitors GPU and system memory usage, implements adaptive batch sizing, and triggers emergency cleanup when memory pressure approaches critical thresholds.\n\n| Component | Responsibility | Detection Method | Recovery Action |\n|-----------|---------------|------------------|-----------------|\n| `MemoryMonitor` | Track baseline and current usage | CUDA memory queries every training step | Log warnings when usage exceeds 85% of capacity |\n| `AdaptiveBatchManager` | Adjust effective batch size | Monitor allocation failures and OOM exceptions | Reduce gradient accumulation steps by half |\n| `EmergencyGC` | Force garbage collection | Detect fragmented memory pools | Clear CUDA cache and trigger Python GC |\n| `GradientClipping` | Prevent memory spikes | Monitor gradient norm statistics | Clip gradients more aggressively during pressure |\n\nThe memory monitoring system establishes baseline usage during model loading and tracks deviations throughout training. When memory usage exceeds 85% of available GPU memory, the system triggers graduated response protocols. First, it reduces gradient accumulation steps to decrease peak memory usage. If pressure continues, it enables more aggressive gradient clipping to reduce optimizer state size. As a last resort, it forces garbage collection and CUDA cache clearing between training steps.\n\n> **Critical Design Principle:** Memory management must be proactive rather than reactive. By the time an out-of-memory error occurs, the training state may already be corrupted. Early detection and gradual adaptation prevent catastrophic failures while maintaining training progress.\n\n**Memory Failure Recovery Strategies:**\n\n1. **Graduated Response Protocol**: The system implements a series of escalating interventions as memory pressure increases. At 85% usage, it logs warnings and reduces batch size. At 90% usage, it forces garbage collection between steps. At 95% usage, it triggers emergency checkpoint saving and prepares for graceful shutdown.\n\n2. **Dynamic Batch Size Adaptation**: When memory allocation fails during batch processing, the system automatically reduces the effective batch size by decreasing gradient accumulation steps. This maintains the same learning dynamics while reducing peak memory requirements.\n\n3. **Checkpoint-Based Recovery**: If an out-of-memory error forces training termination, the system automatically restores from the most recent checkpoint and resumes with reduced memory settings. This enables continuous training despite memory constraints.\n\n4. **Memory Fragmentation Management**: Long-running training jobs can suffer from GPU memory fragmentation. The system periodically clears CUDA cache and defragments memory pools to maintain efficient allocation patterns.\n\n| Memory Pressure Level | Detection Threshold | Automatic Actions | Manual Interventions |\n|----------------------|-------------------|------------------|---------------------|\n| Normal | < 80% GPU memory | Standard operation | None required |\n| Elevated | 80-85% GPU memory | Log warnings, monitor closely | Consider reducing model size |\n| High | 85-90% GPU memory | Reduce gradient accumulation by 25% | Enable gradient checkpointing |\n| Critical | 90-95% GPU memory | Force GC, reduce batch size by 50% | Emergency checkpoint save |\n| Emergency | > 95% GPU memory | Immediate checkpoint and graceful shutdown | Restart with lower settings |\n\n#### GPU Driver and Hardware Recovery\n\nGPU driver crashes and hardware failures require different recovery strategies than memory management issues. These failures often result in complete loss of GPU state, requiring model reloading and training resumption from disk-based checkpoints.\n\nThe system implements comprehensive hardware failure detection through multiple monitoring channels. CUDA runtime errors indicate driver instability or hardware malfunctions. Thermal monitoring prevents overheating-related failures. Power monitoring detects unstable power delivery that can corrupt GPU state.\n\n**Hardware Failure Types and Recovery:**\n\n| Failure Type | Detection Method | Recovery Strategy | Prevention Measures |\n|--------------|------------------|------------------|-------------------|\n| Driver Crash | CUDA runtime exceptions | Restart training process, reload from checkpoint | Regular driver updates, stability testing |\n| Thermal Throttling | GPU temperature monitoring | Reduce batch size, increase cooling wait times | Better case ventilation, undervolting |\n| Power Instability | Sudden disconnection events | Resume from last checkpoint with lower power target | Stable PSU, power monitoring |\n| Memory Corruption | Checksum validation failures | Reload model from disk, skip corrupted batch | ECC memory when available, lower memory speeds |\n| Hardware Degradation | Increased error rates over time | Automatic fallback to CPU processing for affected operations | Regular hardware diagnostics |\n\nThe hardware recovery system maintains redundant state information to enable rapid restoration. Model weights, optimizer states, and training metadata are stored both in GPU memory and on persistent storage. When hardware failures occur, the system can quickly restore the complete training state without manual intervention.\n\n> **Decision: Automatic vs Manual Recovery**\n> - **Context**: Hardware failures can be transient (driver glitches) or persistent (failing hardware), requiring different recovery approaches\n> - **Options Considered**: Always manual recovery, always automatic recovery, hybrid approach with failure pattern analysis\n> - **Decision**: Implement automatic recovery for transient failures with manual override for persistent issues\n> - **Rationale**: Automatic recovery handles common transient issues without user intervention, while persistent failure detection prevents infinite retry loops\n> - **Consequences**: Faster recovery from common issues, but requires sophisticated failure classification logic\n\n#### Power Management and Thermal Protection\n\nPower and thermal management prevent hardware damage while maintaining training stability. The system monitors GPU power consumption, temperature, and fan speeds to detect potential hardware stress before it causes failures.\n\nWhen thermal limits approach, the system automatically reduces computational load by decreasing batch sizes or introducing cooling delays between training steps. This prevents thermal throttling events that can cause unpredictable performance degradation or hardware damage.\n\nPower management involves monitoring both instantaneous power consumption and sustained power delivery. Sudden power spikes can indicate unstable power supplies or inadequate cooling. The system can automatically reduce power targets or enable power limiting to maintain stable operation.\n\n### Training Instability\n\nTraining instability manifests through gradient explosions, loss spikes, NaN propagation, and convergence failures. These issues can corrupt model weights, waste computational resources, and prevent successful fine-tuning completion. The pipeline implements comprehensive stability monitoring and automatic correction mechanisms.\n\n#### Gradient Explosion Detection and Recovery\n\nGradient explosions represent one of the most common training instabilities in language model fine-tuning. Large gradients can corrupt model weights, cause optimizer state overflow, and propagate NaN values throughout the network. The system continuously monitors gradient norms and implements multi-stage intervention protocols.\n\n| Gradient Norm Range | Classification | Automatic Actions | Recovery Strategy |\n|-------------------|---------------|------------------|------------------|\n| < 1.0 | Normal | Continue training | None required |\n| 1.0 - 5.0 | Elevated | Log warnings, monitor closely | Consider gradient clipping adjustment |\n| 5.0 - 20.0 | High | Apply gradient clipping, reduce learning rate temporarily | Skip current batch if clipping insufficient |\n| 20.0 - 100.0 | Critical | Emergency gradient clipping, learning rate reduction | Revert to previous checkpoint if instability persists |\n| > 100.0 | Explosion | Skip batch, reload from recent checkpoint | Investigate data quality and hyperparameter settings |\n\nThe gradient monitoring system calculates both global gradient norms and per-layer gradient statistics to identify localized instabilities. When gradients exceed normal ranges, the system can apply targeted interventions rather than global corrections.\n\n**Gradient Explosion Recovery Protocol:**\n\n1. **Real-time Monitoring**: The system calculates gradient norms after each backward pass and maintains moving averages to establish stability baselines. Sudden spikes beyond three standard deviations trigger intervention protocols.\n\n2. **Adaptive Clipping**: When gradient norms exceed established thresholds, the system automatically adjusts clipping parameters to maintain stability while preserving training dynamics. This prevents the need for manual hyperparameter tuning.\n\n3. **Learning Rate Adjustment**: Persistent gradient instability triggers temporary learning rate reduction to improve training stability. The system gradually restores the original learning rate as stability returns.\n\n4. **Batch Skipping**: When gradient explosions cannot be controlled through clipping, the system skips the problematic batch and continues with the next training sample. This prevents single bad examples from corrupting the entire training run.\n\n5. **Checkpoint Restoration**: If gradient explosions persist despite interventions, the system automatically reverts to the most recent stable checkpoint and resumes training with adjusted hyperparameters.\n\n#### Loss Spike Management\n\nLoss spikes indicate sudden degradation in model performance, often caused by bad training examples, learning rate issues, or optimizer state corruption. The system monitors both absolute loss values and relative changes to detect performance degradation.\n\nWhen loss spikes occur, the system first determines whether the spike represents a temporary fluctuation or sustained degradation. Temporary spikes may be handled through batch skipping or learning rate adjustment. Sustained degradation typically requires checkpoint restoration.\n\n**Loss Monitoring Strategy:**\n\n| Loss Change Pattern | Classification | Likely Cause | Recovery Action |\n|-------------------|---------------|--------------|-----------------|\n| Single step spike, then recovery | Transient | Bad training example | Continue monitoring, log warning |\n| Multiple consecutive increases | Trend degradation | Learning rate too high | Reduce learning rate by 50% |\n| Sudden jump with no recovery | Performance collapse | Optimizer state corruption | Restore from checkpoint |\n| Gradual increase over epochs | Overfitting onset | Insufficient regularization | Enable early stopping |\n| Oscillating high/low values | Training instability | Batch size or accumulation issues | Adjust gradient accumulation |\n\nThe loss monitoring system maintains both short-term and long-term loss statistics to distinguish between normal training fluctuations and genuine instability. Exponential moving averages provide stable baselines for detecting significant deviations.\n\n#### NaN and Infinity Propagation\n\nNaN (Not a Number) and infinity values can propagate through neural networks, corrupting gradients and model weights. Once NaN values enter the computation graph, they typically spread rapidly, making recovery difficult without checkpoint restoration.\n\nThe system implements comprehensive NaN detection at multiple computation stages. Forward pass monitoring catches NaN values in activations before they corrupt gradients. Gradient monitoring detects NaN values in parameter updates before they modify model weights. Loss monitoring identifies NaN values in objective function computation.\n\n**NaN Recovery Protocol:**\n\n1. **Early Detection**: The system checks for NaN values in activations, gradients, and loss computations at each training step. Early detection prevents propagation throughout the network.\n\n2. **Immediate Isolation**: When NaN values are detected, the system immediately stops gradient computation and parameter updates for the current batch to prevent corruption.\n\n3. **Source Identification**: The system analyzes which operations or data examples caused the NaN values to help prevent recurrence.\n\n4. **State Restoration**: If NaN values have already corrupted model parameters, the system restores from the most recent checkpoint before NaN propagation began.\n\n5. **Prevention Measures**: After NaN recovery, the system implements additional safeguards such as stricter gradient clipping, learning rate reduction, or problematic data filtering.\n\n> **Architecture Decision: NaN Handling Strategy**\n> - **Context**: NaN values can emerge from various sources (overflow, underflow, invalid operations) and spread rapidly through networks\n> - **Options Considered**: Ignore and continue training, immediate checkpoint restoration, gradual intervention escalation\n> - **Decision**: Implement immediate detection with graduated intervention based on NaN scope and source\n> - **Rationale**: Early detection prevents widespread corruption, while graduated intervention avoids unnecessary training interruption for recoverable issues\n> - **Consequences**: Requires comprehensive monitoring overhead but prevents catastrophic training failures\n\n### Data Quality Issues\n\nData quality problems can manifest during training rather than preprocessing, requiring runtime detection and handling. Corrupted files, encoding issues, malformed JSON, and inconsistent formatting can interrupt training or degrade model performance. The pipeline implements robust data validation and fallback mechanisms.\n\n#### Runtime Data Validation\n\nThe data loading system performs comprehensive validation beyond initial preprocessing checks. Runtime validation catches corruption that occurs after preprocessing, such as file system errors, network interruptions during streaming, or memory corruption affecting data structures.\n\n| Validation Level | Check Type | Detection Method | Recovery Action |\n|-----------------|------------|------------------|-----------------|\n| File Integrity | Checksum verification | Compare stored and computed hashes | Reload from backup or skip corrupted files |\n| Format Consistency | Schema validation | JSON/JSONL structure verification | Attempt format correction or skip invalid samples |\n| Content Quality | Text analysis | Length, character encoding, language detection | Filter problematic samples, apply corrections |\n| Token Validity | Tokenization verification | Check for unknown tokens, length limits | Re-tokenize with updated tokenizer, truncate oversized samples |\n| Label Accuracy | Response validation | Check instruction-response consistency | Skip samples with mismatched or corrupted labels |\n\nThe validation system maintains statistics on data quality issues to identify systemic problems. High corruption rates may indicate storage issues, network problems, or preprocessing errors that require immediate attention.\n\n**Data Corruption Recovery:**\n\n1. **Redundant Storage**: Critical training data is stored with redundant copies and checksums to enable corruption detection and recovery. The system automatically switches to backup copies when corruption is detected.\n\n2. **Graceful Degradation**: When data corruption affects small portions of the training set, the system continues training with the remaining valid data rather than terminating the entire process.\n\n3. **Real-time Filtering**: The data loader implements real-time quality filtering to remove corrupted or malformed samples during training without interrupting the process.\n\n4. **Automatic Reprocessing**: When systematic data corruption is detected, the system can automatically trigger reprocessing of affected data sources with updated validation rules.\n\n#### Streaming Data Handling\n\nFor large datasets that exceed memory capacity, streaming data loading introduces additional failure modes. Network interruptions, storage failures, and concurrent access issues can disrupt data flow during training.\n\nThe streaming system implements robust error handling with automatic retry mechanisms, local caching, and fallback data sources. When streaming failures occur, the system can temporarily switch to cached data or alternative sources while attempting to restore the primary stream.\n\n**Streaming Failure Recovery:**\n\n| Failure Type | Detection Method | Immediate Response | Long-term Resolution |\n|--------------|------------------|-------------------|---------------------|\n| Network Timeout | Connection monitoring | Retry with exponential backoff | Switch to local cache |\n| Storage Unavailable | File system errors | Attempt alternative paths | Enable redundant storage |\n| Concurrent Access | File locking conflicts | Queue requests, implement retry logic | Coordinate access patterns |\n| Bandwidth Saturation | Transfer rate monitoring | Reduce concurrent streams | Implement traffic shaping |\n| Data Corruption | Integrity checking | Skip corrupted chunks, request retransmission | Verify storage subsystem health |\n\n#### Format Inconsistency Handling\n\nTraining datasets often contain samples with inconsistent formatting, especially when combining multiple data sources. The system must handle variations in chat templates, instruction formatting, and response structures without losing valuable training data.\n\nThe format handling system implements automatic format detection and normalization. When inconsistent formats are detected, the system attempts automatic conversion to the standard format. If conversion fails, samples are quarantined for manual review rather than corrupting the training process.\n\n### Checkpoint and State Recovery\n\nCheckpoint corruption, incomplete saves, and state inconsistencies can prevent training resumption after interruptions. The pipeline implements robust checkpointing with verification, redundancy, and automatic repair mechanisms.\n\n#### Checkpoint Integrity Management\n\nThe checkpointing system ensures data integrity through multiple verification layers. Checksums verify file integrity during save and load operations. Metadata validation confirms checkpoint consistency with training configuration. State verification ensures that restored models produce expected outputs.\n\n| Checkpoint Component | Integrity Check | Verification Method | Recovery Strategy |\n|---------------------|-----------------|-------------------|------------------|\n| Model Weights | SHA-256 checksum | Compare saved and computed hashes | Restore from backup checkpoint |\n| Optimizer State | Size and structure validation | Verify tensor dimensions and types | Reinitialize optimizer state if corrupted |\n| Training Metadata | JSON schema validation | Check required fields and data types | Reconstruct from training logs |\n| Random State | Sequence verification | Test random number generation | Reseed with deterministic values |\n| Configuration | Hash comparison | Verify against original config | Restore from configuration backup |\n\nThe integrity management system maintains multiple checkpoint generations to provide recovery options when the most recent checkpoint is corrupted. Automatic backup rotation ensures that recent good checkpoints are always available.\n\n**Checkpoint Corruption Recovery:**\n\n1. **Multi-level Verification**: The system performs integrity checks at multiple levels—file system, binary data, and semantic correctness—to detect different types of corruption.\n\n2. **Automatic Repair**: When minor corruption is detected, the system attempts automatic repair using redundant information or reconstructing missing components from available data.\n\n3. **Graceful Fallback**: If the most recent checkpoint is corrupted, the system automatically falls back to the previous verified checkpoint and resumes training from that point.\n\n4. **Progressive Verification**: During checkpoint loading, the system performs progressive verification to identify exactly which components are corrupted and preserve as much valid state as possible.\n\n#### State Synchronization\n\nTraining state includes not only model weights but also optimizer states, learning rate schedules, random number generators, and data loader positions. Inconsistencies between these components can cause subtle training issues or prevent proper resumption.\n\nThe state synchronization system ensures that all training components remain synchronized across checkpoint save and restore operations. When inconsistencies are detected, the system can either repair the inconsistencies or reinitialize affected components.\n\n**State Component Dependencies:**\n\n| Component | Dependencies | Synchronization Requirements | Recovery Priority |\n|-----------|--------------|----------------------------|------------------|\n| Model Weights | None | Must match training configuration | Highest - core model state |\n| Optimizer State | Model architecture, training step | Must align with current model parameters | High - affects convergence |\n| Learning Rate Schedule | Training step, epoch count | Must reflect actual training progress | Medium - can be reconstructed |\n| Random State | Training step, epoch | Should ensure reproducible behavior | Low - affects reproducibility only |\n| Data Loader Position | Dataset configuration, shuffle state | Must prevent data duplication or skipping | Medium - affects training distribution |\n\n#### Distributed Training State Recovery\n\nWhen training involves multiple GPUs or nodes, state recovery becomes more complex. The system must handle partial failures where some processes succeed while others fail, maintain consistency across distributed components, and coordinate recovery across all participating nodes.\n\nThe distributed recovery system implements coordinator-based recovery protocols. A designated coordinator node manages the recovery process, verifying state consistency across all participants and coordinating synchronized resumption.\n\n**Distributed Recovery Protocol:**\n\n1. **Failure Detection**: Each training node monitors the health of other participants and reports failures to the coordinator. The coordinator maintains a global view of system health.\n\n2. **State Collection**: When recovery is triggered, the coordinator collects checkpoint information from all surviving nodes and determines the most recent consistent state across the entire system.\n\n3. **Consistency Verification**: The coordinator verifies that all nodes have compatible checkpoint states and identifies any inconsistencies that must be resolved before resumption.\n\n4. **Synchronized Restart**: The coordinator orchestrates synchronized restart across all nodes, ensuring that training resumes from the same global state on all participants.\n\n5. **Health Monitoring**: After resumption, the coordinator continues monitoring node health and checkpoint consistency to detect any ongoing issues.\n\n### Common Pitfalls\n\n⚠️ **Pitfall: Insufficient Memory Headroom**\nMany implementations monitor memory usage but don't account for temporary allocations during gradient computation and optimizer updates. This causes unexpected out-of-memory errors even when steady-state memory usage appears safe. Always maintain at least 15% memory headroom and monitor peak allocation patterns, not just average usage.\n\n⚠️ **Pitfall: Inconsistent Error Recovery States**\nWhen recovering from failures, different system components may restore to different temporal states, causing subtle inconsistencies. For example, the model may restore from step 1000 while the learning rate schedule restores from step 1050. Always verify temporal consistency across all training components during recovery.\n\n⚠️ **Pitfall: Gradient Clipping After Explosion Detection**\nChecking for gradient explosions after applying gradient clipping is ineffective because clipping masks the underlying instability. Monitor raw gradient norms before clipping and adjust clipping parameters dynamically based on explosion frequency.\n\n⚠️ **Pitfall: Ignoring NaN in Validation Metrics**\nNaN values in validation metrics often indicate training instability, but they're frequently ignored because validation continues to run. Monitor validation metrics for NaN values and treat them as serious indicators of training problems requiring intervention.\n\n⚠️ **Pitfall: Checkpoint Frequency vs Recovery Time Trade-offs**\nSaving checkpoints too frequently can significantly slow training, while saving too infrequently increases recovery time after failures. Implement adaptive checkpoint frequency based on training stability and progress rate rather than fixed intervals.\n\n⚠️ **Pitfall: Hardware-Specific Recovery Assumptions**\nRecovery strategies often assume specific hardware behaviors (e.g., CUDA error types, memory allocation patterns) that don't generalize across different GPU models or drivers. Implement hardware-agnostic error detection with hardware-specific recovery adaptations.\n\n⚠️ **Pitfall: Data Corruption Propagation Through Caches**\nCorrupted data can persist in various caching layers (tokenizer caches, data loader buffers, model attention caches) even after the original corruption is detected. Implement comprehensive cache invalidation when data corruption is detected.\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Memory Monitoring | `torch.cuda.memory_allocated()` with periodic logging | Custom CUDA memory profiler with real-time alerts |\n| Error Logging | Python `logging` module with file handlers | Structured logging with ELK stack integration |\n| Checkpoint Storage | Local filesystem with file locking | Distributed storage with consistency guarantees |\n| Health Monitoring | Simple heartbeat signals | Comprehensive metrics collection with Prometheus |\n| Recovery Coordination | File-based coordination | Distributed consensus with etcd or Consul |\n\n#### Recommended File Structure\n\n```\nsrc/\n  pipeline/\n    error_handling/\n      __init__.py\n      memory_monitor.py          ← Memory pressure detection and adaptive management\n      training_stability.py     ← Gradient explosion and loss spike handling  \n      data_validation.py        ← Runtime data quality checking and recovery\n      checkpoint_manager.py     ← Checkpoint integrity and state recovery\n      hardware_monitor.py       ← GPU health monitoring and failure detection\n      recovery_coordinator.py   ← Orchestrates recovery across components\n    utils/\n      error_types.py            ← Custom exception classes for different failure modes\n      monitoring_utils.py       ← Shared monitoring and alerting utilities\n      state_utils.py           ← State synchronization and verification helpers\n```\n\n#### Infrastructure Starter Code\n\nHere's a complete memory monitoring implementation that tracks GPU memory usage and implements graduated response protocols:\n\n```python\nimport torch\nimport psutil\nimport logging\nfrom typing import Dict, List, Optional, Any\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nimport threading\nimport time\n\n@dataclass\nclass MemoryMetrics:\n    \"\"\"Memory usage measurements at a specific point in time\"\"\"\n    stage: str\n    gpu_memory_allocated: float\n    gpu_memory_cached: float  \n    gpu_memory_reserved: float\n    system_memory_used: float\n    timestamp: float\n    details: Optional[Dict[str, Any]] = None\n\n@dataclass\nclass MemoryMonitor:\n    \"\"\"Monitors GPU and system memory usage with adaptive management\"\"\"\n    baseline_gpu_memory: Optional[int] = None\n    baseline_system_memory: int = 0\n    measurements: List[MemoryMetrics] = field(default_factory=list)\n    _lock: threading.Lock = field(default_factory=threading.Lock)\n    _alert_thresholds: Dict[str, float] = field(default_factory=lambda: {\n        'warning': 0.8, 'high': 0.85, 'critical': 0.9, 'emergency': 0.95\n    })\n    \n    def capture_baseline(self) -> None:\n        \"\"\"Record initial memory state before model loading\"\"\"\n        with self._lock:\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()  # Clear any existing allocations\n                self.baseline_gpu_memory = torch.cuda.memory_allocated()\n            \n            process = psutil.Process()\n            self.baseline_system_memory = process.memory_info().rss\n            \n            logging.info(f\"Memory baseline captured - GPU: {self.baseline_gpu_memory / 1e9:.2f}GB, \"\n                        f\"System: {self.baseline_system_memory / 1e9:.2f}GB\")\n    \n    def measure_current_usage(self, stage: str) -> Dict[str, float]:\n        \"\"\"Measure current memory usage and return statistics\"\"\"\n        with self._lock:\n            timestamp = time.time()\n            \n            if torch.cuda.is_available():\n                gpu_allocated = torch.cuda.memory_allocated()\n                gpu_cached = torch.cuda.memory_cached()\n                gpu_reserved = torch.cuda.memory_reserved()\n            else:\n                gpu_allocated = gpu_cached = gpu_reserved = 0.0\n            \n            process = psutil.Process()\n            system_used = process.memory_info().rss\n            \n            metrics = MemoryMetrics(\n                stage=stage,\n                gpu_memory_allocated=gpu_allocated,\n                gpu_memory_cached=gpu_cached,\n                gpu_memory_reserved=gpu_reserved,\n                system_memory_used=system_used,\n                timestamp=timestamp,\n                details={\n                    'gpu_utilization': gpu_allocated / torch.cuda.max_memory_allocated() if torch.cuda.is_available() else 0.0,\n                    'system_utilization': system_used / psutil.virtual_memory().total\n                }\n            )\n            \n            self.measurements.append(metrics)\n            \n            # Check for memory pressure and trigger alerts\n            self._check_memory_pressure(metrics)\n            \n            return {\n                'gpu_allocated_gb': gpu_allocated / 1e9,\n                'gpu_cached_gb': gpu_cached / 1e9,\n                'system_used_gb': system_used / 1e9,\n                'gpu_utilization': metrics.details['gpu_utilization'],\n                'system_utilization': metrics.details['system_utilization']\n            }\n    \n    def get_peak_usage(self) -> Dict[str, float]:\n        \"\"\"Return peak memory usage across all measurements\"\"\"\n        if not self.measurements:\n            return {}\n        \n        peak_gpu = max(m.gpu_memory_allocated for m in self.measurements)\n        peak_system = max(m.system_memory_used for m in self.measurements)\n        \n        return {\n            'peak_gpu_gb': peak_gpu / 1e9,\n            'peak_system_gb': peak_system / 1e9,\n            'peak_gpu_utilization': peak_gpu / torch.cuda.max_memory_allocated() if torch.cuda.is_available() else 0.0\n        }\n    \n    def _check_memory_pressure(self, metrics: MemoryMetrics) -> None:\n        \"\"\"Check for memory pressure and trigger appropriate responses\"\"\"\n        if not torch.cuda.is_available():\n            return\n        \n        gpu_utilization = metrics.details['gpu_utilization']\n        \n        if gpu_utilization >= self._alert_thresholds['emergency']:\n            logging.error(f\"EMERGENCY: GPU memory usage at {gpu_utilization:.1%} - immediate action required\")\n            # Trigger emergency protocols\n        elif gpu_utilization >= self._alert_thresholds['critical']:\n            logging.error(f\"CRITICAL: GPU memory usage at {gpu_utilization:.1%} - reducing batch size\")\n            # Trigger batch size reduction\n        elif gpu_utilization >= self._alert_thresholds['high']:\n            logging.warning(f\"HIGH: GPU memory usage at {gpu_utilization:.1%} - monitoring closely\")\n            # Increase monitoring frequency\n        elif gpu_utilization >= self._alert_thresholds['warning']:\n            logging.info(f\"WARNING: GPU memory usage at {gpu_utilization:.1%}\")\n```\n\nHere's a comprehensive checkpoint integrity manager:\n\n```python\nimport torch\nimport json\nimport hashlib\nimport shutil\nfrom pathlib import Path\nfrom typing import Dict, Any, Optional, List\nfrom dataclasses import dataclass\nimport logging\n\n@dataclass\nclass CheckpointMetadata:\n    \"\"\"Metadata about a saved checkpoint\"\"\"\n    checkpoint_path: str\n    step: int\n    epoch: float\n    eval_loss: Optional[float]\n    is_best: bool\n    model_config: Dict[str, Any]\n    adapter_config: Dict[str, Any]\n    optimizer_state_size: int\n    save_timestamp: float\n\nclass CheckpointManager:\n    \"\"\"Manages checkpoint saving, loading, and integrity verification\"\"\"\n    \n    def __init__(self, checkpoint_dir: str, max_checkpoints: int = 5):\n        self.checkpoint_dir = Path(checkpoint_dir)\n        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n        self.max_checkpoints = max_checkpoints\n        \n    def save_checkpoint(self, model, optimizer, step: int, epoch: float, \n                       eval_loss: Optional[float] = None, is_best: bool = False) -> CheckpointMetadata:\n        \"\"\"Save checkpoint with integrity verification\"\"\"\n        # TODO 1: Create checkpoint filename with timestamp and step number\n        # TODO 2: Save model state dict with error handling for disk space\n        # TODO 3: Save optimizer state dict separately for memory efficiency\n        # TODO 4: Generate and save integrity checksums for all files\n        # TODO 5: Create checkpoint metadata JSON with all relevant information\n        # TODO 6: Verify saved files can be loaded correctly before considering save complete\n        # TODO 7: Clean up old checkpoints if maximum count exceeded\n        # TODO 8: Return CheckpointMetadata object with save information\n        pass\n    \n    def load_checkpoint(self, checkpoint_path: str) -> Dict[str, Any]:\n        \"\"\"Load checkpoint with integrity verification\"\"\"\n        # TODO 1: Verify checkpoint directory exists and contains required files\n        # TODO 2: Load and validate checkpoint metadata JSON\n        # TODO 3: Verify file integrity using saved checksums\n        # TODO 4: Load model state dict with error handling for corrupted weights\n        # TODO 5: Load optimizer state dict with compatibility checking\n        # TODO 6: Validate loaded state dimensions match expected model architecture\n        # TODO 7: Return complete checkpoint state or raise appropriate exceptions\n        pass\n    \n    def verify_checkpoint_integrity(self, checkpoint_path: str) -> bool:\n        \"\"\"Verify checkpoint files are not corrupted\"\"\"\n        # TODO 1: Check that all required checkpoint files exist\n        # TODO 2: Validate JSON metadata structure and required fields\n        # TODO 3: Compute and compare file checksums against saved values\n        # TODO 4: Attempt to load state dicts without applying to verify format\n        # TODO 5: Return True if all integrity checks pass, False otherwise\n        pass\n    \n    def find_latest_valid_checkpoint(self) -> Optional[str]:\n        \"\"\"Find the most recent checkpoint that passes integrity checks\"\"\"\n        # TODO 1: List all checkpoint directories sorted by creation time\n        # TODO 2: Check each checkpoint starting from most recent\n        # TODO 3: Return path to first checkpoint that passes integrity verification\n        # TODO 4: Return None if no valid checkpoints found\n        pass\n```\n\n#### Core Logic Skeleton Code\n\nHere's the training stability monitor that detects and handles gradient explosions and loss spikes:\n\n```python\nimport torch\nimport numpy as np\nfrom typing import Dict, List, Optional, Tuple\nfrom collections import deque\nimport logging\n\nclass TrainingStabilityMonitor:\n    \"\"\"Monitors training for instabilities and implements recovery strategies\"\"\"\n    \n    def __init__(self, gradient_clip_norm: float = 1.0, loss_spike_threshold: float = 2.0):\n        self.gradient_clip_norm = gradient_clip_norm\n        self.loss_spike_threshold = loss_spike_threshold\n        self.gradient_history = deque(maxlen=100)\n        self.loss_history = deque(maxlen=100) \n        self.nan_count = 0\n        self.explosion_count = 0\n        \n    def check_gradient_stability(self, model: torch.nn.Module) -> Dict[str, Any]:\n        \"\"\"Monitor gradient norms and detect explosions\"\"\"\n        # TODO 1: Calculate total gradient norm across all model parameters\n        # TODO 2: Calculate per-layer gradient norms for localized explosion detection\n        # TODO 3: Compare current gradient norm to historical statistics\n        # TODO 4: Detect gradient explosions using threshold and statistical analysis\n        # TODO 5: Check for NaN or infinite values in gradients\n        # TODO 6: Update gradient history statistics for future comparisons\n        # TODO 7: Return stability report with recommendations for intervention\n        # Hint: Use torch.nn.utils.clip_grad_norm_ for gradient norm calculation\n        pass\n    \n    def handle_gradient_explosion(self, model: torch.nn.Module, optimizer: torch.optim.Optimizer) -> bool:\n        \"\"\"Handle detected gradient explosion with graduated intervention\"\"\"\n        # TODO 1: Log gradient explosion event with severity assessment\n        # TODO 2: Apply emergency gradient clipping with reduced threshold\n        # TODO 3: Optionally skip optimizer step if explosion is severe\n        # TODO 4: Adjust learning rate temporarily to improve stability  \n        # TODO 5: Clear problematic gradients and reset gradient accumulation\n        # TODO 6: Update explosion count and consider checkpoint restoration\n        # TODO 7: Return True if training can continue, False if recovery needed\n        pass\n    \n    def check_loss_stability(self, current_loss: float) -> Dict[str, Any]:\n        \"\"\"Monitor loss values for spikes and degradation\"\"\"\n        # TODO 1: Add current loss to historical tracking\n        # TODO 2: Calculate loss statistics (mean, std, percentiles) over recent history\n        # TODO 3: Detect loss spikes using threshold and statistical analysis\n        # TODO 4: Identify loss degradation trends using moving averages\n        # TODO 5: Check for NaN or infinite loss values\n        # TODO 6: Assess whether loss spike indicates temporary or sustained issues\n        # TODO 7: Return stability assessment with recommended recovery actions\n        pass\n    \n    def detect_nan_propagation(self, model: torch.nn.Module, loss: torch.Tensor) -> bool:\n        \"\"\"Detect NaN values in model parameters, gradients, or loss\"\"\"\n        # TODO 1: Check loss tensor for NaN or infinite values\n        # TODO 2: Scan model parameters for NaN or infinite weights\n        # TODO 3: Check gradients for NaN or infinite values if they exist\n        # TODO 4: Identify which layers or operations introduced NaN values\n        # TODO 5: Log detailed information about NaN source and scope\n        # TODO 6: Update NaN occurrence statistics\n        # TODO 7: Return True if NaN detected, False otherwise\n        pass\n        \n    def should_restore_checkpoint(self) -> bool:\n        \"\"\"Determine if training instability requires checkpoint restoration\"\"\"\n        # TODO 1: Assess frequency and severity of recent gradient explosions\n        # TODO 2: Evaluate sustained loss degradation patterns\n        # TODO 3: Check NaN occurrence frequency and propagation scope\n        # TODO 4: Consider cumulative instability indicators over training history\n        # TODO 5: Return True if checkpoint restoration recommended, False otherwise\n        pass\n```\n\n#### Language-Specific Hints\n\n**Memory Management in PyTorch:**\n- Use `torch.cuda.empty_cache()` to clear GPU memory cache, but note this only releases cached memory, not actively allocated memory\n- Monitor both `torch.cuda.memory_allocated()` (actively used) and `torch.cuda.memory_cached()` (available for reuse)\n- Use `torch.cuda.memory_summary()` for detailed memory debugging information\n- Enable `PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512` to reduce memory fragmentation\n\n**Error Handling Best Practices:**\n- Catch specific CUDA exceptions like `torch.cuda.OutOfMemoryError` rather than generic exceptions\n- Use context managers for resource cleanup: `with torch.cuda.device(gpu_id):`\n- Implement exponential backoff for transient failures: `time.sleep(min(2**attempt, 60))`\n- Log stack traces with `logging.exception()` to preserve error context\n\n**Checkpoint Safety:**\n- Use atomic writes by saving to temporary files then renaming: `torch.save(state, temp_path); os.rename(temp_path, final_path)`\n- Verify disk space before saving: `shutil.disk_usage(checkpoint_dir).free > estimated_size`\n- Use `pickle_protocol=4` for better compatibility across Python versions\n- Save checksums separately: `hashlib.sha256(checkpoint_data).hexdigest()`\n\n#### Milestone Checkpoint\n\nAfter implementing error handling components, verify the following behavior:\n\n**Memory Monitoring Verification:**\n```bash\n# Run training with memory monitoring enabled\npython train.py --enable-memory-monitoring --log-level DEBUG\n\n# Expected output should show:\n# - Baseline memory measurements before model loading\n# - Regular memory usage reports during training\n# - Warnings when memory usage exceeds thresholds\n# - Automatic batch size reduction under memory pressure\n```\n\n**Error Recovery Testing:**\n```bash\n# Test checkpoint recovery\npython test_recovery.py --simulate-failure --checkpoint-path ./checkpoints/\n\n# Expected behavior:\n# - Training resumes from last valid checkpoint\n# - Model produces consistent outputs before and after recovery\n# - Optimizer state restored correctly with matching learning rates\n```\n\n**Training Stability Validation:**\n```bash\n# Monitor training stability with problematic data\npython train.py --unstable-data --enable-stability-monitoring\n\n# Expected automatic handling:\n# - Gradient explosions detected and handled with clipping adjustments\n# - Loss spikes logged with appropriate recovery actions\n# - NaN values caught before propagating through the network\n```\n\n**Signs of Proper Implementation:**\n- Training continues smoothly despite memory pressure and hardware hiccups\n- Recovery from failures happens automatically without manual intervention\n- Detailed logs provide clear information about what went wrong and how it was handled\n- Checkpoint integrity verification prevents loading of corrupted states\n\n**Common Issues and Fixes:**\n- If memory monitoring doesn't trigger: Check that thresholds are set appropriately for your hardware\n- If checkpoint recovery fails: Verify file permissions and disk space availability  \n- If gradient explosions aren't caught: Ensure monitoring happens before gradient clipping\n- If NaN detection is too slow: Implement sampling-based checks rather than full parameter scans\n\n\n## Testing Strategy\n\n> **Milestone(s):** All milestones - comprehensive testing validates the correctness, reliability, and performance of each component and their interactions throughout the fine-tuning pipeline\n\nTesting a parameter-efficient fine-tuning pipeline presents unique challenges that traditional software testing approaches don't fully address. Unlike conventional applications where we can easily verify outputs against expected values, machine learning systems involve probabilistic behaviors, hardware-dependent performance characteristics, and complex interactions between numerical computations that can lead to subtle but critical failures.\n\n### Mental Model: The Quality Assurance Laboratory\n\nThink of our testing strategy as operating a multi-layered quality assurance laboratory for a precision manufacturing process. Just as a manufacturing QA lab has different stations - incoming material inspection, component testing, assembly verification, final product evaluation, and performance benchmarking - our testing approach operates at multiple levels of granularity and concern.\n\nThe **unit testing station** examines individual components in isolation, like testing whether a single gear meshes properly or a sensor reads accurately. The **integration testing station** verifies that multiple components work together correctly, similar to testing whether the gear assembly rotates smoothly under load. The **end-to-end testing station** validates the complete manufacturing pipeline, ensuring that raw materials transform into finished products meeting all specifications. Finally, the **performance testing station** measures whether the entire system operates within acceptable speed, resource, and quality parameters under various operating conditions.\n\nWhat makes ML system testing particularly challenging is that we're not just testing deterministic mechanical processes - we're testing systems that involve probabilistic computations, hardware-dependent optimizations, and emergent behaviors that only become apparent when components interact under realistic workloads.\n\n## Unit Testing Strategy\n\nUnit testing in the context of LLM fine-tuning focuses on validating individual components in complete isolation from the complexities of model loading, GPU memory management, and distributed computation. Each component should be testable with predictable inputs and verifiable outputs, allowing us to catch logic errors, configuration problems, and boundary condition failures before they manifest in expensive training runs.\n\n### Core Component Testing Framework\n\nThe foundation of our unit testing approach rests on the principle that every component should be testable with mock dependencies and synthetic data that capture the essential characteristics of real training scenarios without requiring actual model weights or GPU resources. This enables fast, deterministic testing that can run in continuous integration environments.\n\nOur testing framework establishes several categories of unit tests, each targeting different aspects of component behavior:\n\n| Test Category | Focus Area | Example Components | Key Validation Points |\n|---------------|------------|-------------------|----------------------|\n| Data Structure Validation | Configuration objects and data formats | `LoRAConfig`, `QuantizationConfig`, `InstructionSample` | Field validation, type checking, constraint enforcement |\n| Transformation Logic | Data processing and formatting functions | `apply_template`, `tokenize_sample`, `format_sample` | Input-output correctness, edge case handling, error propagation |\n| Analysis and Calculation | Computational functions without model dependencies | `analyze_parameter_distribution`, `calculate_efficiency_metrics` | Mathematical correctness, numerical precision, boundary conditions |\n| Configuration Generation | Dynamic configuration creation | `from_task_complexity`, `recommend_rank_for_task` | Logic correctness, constraint satisfaction, consistency |\n\n### Data Structure and Configuration Testing\n\nConfiguration objects form the backbone of the entire pipeline, and errors in configuration validation can lead to training failures that only manifest after expensive model loading and initialization. Our unit tests for configuration objects validate both positive and negative cases comprehensively.\n\nThe `LoRAConfig` testing suite verifies that rank and alpha parameters satisfy mathematical constraints, target modules are properly validated against known patterns, and edge cases like zero rank or negative alpha are handled gracefully. We test configuration serialization and deserialization to ensure that saved configurations can be reliably restored, and we verify that the `effective_alpha` calculation produces mathematically correct scaling factors under various parameter combinations.\n\nFor `QuantizationConfig` objects, tests verify that quantization type strings map to valid bitsandbytes configurations, compute dtype selections are compatible with the specified quantization format, and memory estimation functions produce reasonable predictions for various model sizes. The testing includes validation of the `to_bitsandbytes_config` method to ensure that our internal configuration representation correctly translates to the format expected by the underlying quantization library.\n\nData structure testing for `InstructionSample` and `ConversationSample` objects focuses on validation logic, ensuring that required fields are present, conversation turns maintain proper role sequencing, and quality scores fall within expected ranges. These tests use synthetic conversation data that captures the diversity of real training scenarios while remaining deterministic and fast to execute.\n\n### Transformation and Processing Logic Testing\n\nThe data transformation pipeline contains numerous functions that convert between different representations of training data. These transformations must be tested exhaustively because errors in tokenization, template application, or format conversion can silently corrupt training data, leading to poor model performance that's difficult to diagnose.\n\nTemplate application testing uses a comprehensive suite of instruction-response pairs that exercise different conversation patterns, special character handling, and edge cases like empty responses or extremely long instructions. The test suite includes validation of chat template consistency, ensuring that the same logical conversation structure produces identical formatted output across multiple invocations.\n\nTokenization testing validates the `tokenize_sample` function against known tokenizer behaviors, ensuring that attention masks are correctly generated, special tokens are properly inserted, and length constraints are enforced consistently. These tests use pre-computed expected outputs for specific tokenizer configurations, allowing us to detect regressions in tokenization logic or changes in underlying tokenizer behavior.\n\nThe data loading and format conversion functions are tested with synthetic datasets that include various edge cases: malformed JSON records, missing required fields, inconsistent field types, and boundary conditions like empty datasets or extremely large individual samples. These tests validate both the happy path behavior and the error handling characteristics of the data ingestion pipeline.\n\n### Analysis and Calculation Testing\n\nMathematical and analytical functions within the pipeline must be tested for numerical correctness, handling of edge cases, and consistency across different input scales. These functions often involve floating-point computations that can be sensitive to numerical precision issues or input scaling problems.\n\nParameter analysis functions like `analyze_parameter_distribution` are tested with synthetic model structures that have known parameter counts and distributions. Test cases include models with different architectures, unusual parameter distributions, and edge cases like models with no trainable parameters or extremely sparse parameter structures.\n\nMemory estimation functions are validated against empirically measured memory usage patterns, with test cases covering different model sizes, quantization configurations, and LoRA parameter combinations. While these tests can't perfectly predict actual memory usage due to hardware-dependent factors, they validate the mathematical correctness of the estimation algorithms and ensure consistency across different configuration scenarios.\n\nEfficiency calculation functions are tested for mathematical correctness using synthetic parameter distributions with known expected outcomes. These tests validate that efficiency ratios are calculated correctly, parameter counts are aggregated properly, and edge cases like division by zero or negative parameter counts are handled gracefully.\n\n### Mock Infrastructure and Test Utilities\n\nSupporting the unit test suite requires a comprehensive mocking infrastructure that can simulate the interfaces and behaviors of external dependencies without requiring actual GPU resources or network access. This infrastructure enables fast, deterministic testing while maintaining high fidelity to actual component interactions.\n\nThe mock tokenizer implementation provides deterministic tokenization behavior for test scenarios, with configurable vocabulary sizes, special token handling, and length constraints. This allows tokenization tests to focus on the logic of our tokenization pipeline rather than the specific behavior of different tokenizer implementations.\n\nMock model objects simulate the interface characteristics of actual transformer models, providing parameter iteration, module inspection, and basic forward pass capabilities without requiring actual model weights. These mock objects enable testing of model analysis functions, target module detection, and parameter counting logic.\n\nThe test data generation utilities create synthetic datasets with controllable characteristics: specific token length distributions, conversation turn patterns, quality score ranges, and error injection capabilities. This synthetic data enables comprehensive testing of data processing pipelines while maintaining reproducibility and fast execution times.\n\n## Integration and End-to-End Testing\n\nIntegration testing validates the correctness of component interactions and data flow between different parts of the fine-tuning pipeline. Unlike unit tests that isolate individual functions, integration tests verify that components work together correctly, handle shared state appropriately, and propagate errors and configuration changes properly through the system.\n\n### Component Integration Testing\n\nThe first level of integration testing focuses on pairs and small groups of components, verifying that their interfaces align correctly and that data transformations preserve essential properties across component boundaries. These tests use larger-scale test data than unit tests but still avoid loading full-scale models or executing actual training loops.\n\nData preparation and tokenization integration testing verifies the complete flow from raw data files through chat template application to final tokenized datasets. These tests use realistic data samples that exercise the full range of supported input formats and conversation patterns. The tests validate that data quality statistics are computed correctly, that train-validation splits maintain proper distributions, and that tokenization parameters are applied consistently across the entire dataset.\n\nLoRA configuration and model analysis integration testing verifies that target module detection works correctly with actual model architectures, that rank and alpha parameter recommendations are consistent with memory constraints, and that the adapter injection process preserves model structure. These tests use smaller pre-trained models that can be loaded quickly while still exercising the full adapter configuration pipeline.\n\nQuantization and memory monitoring integration testing validates that memory usage predictions align with actual memory consumption, that quantization configurations are applied correctly, and that the combination of quantization and LoRA adapters produces expected memory efficiency gains. These tests require GPU resources but use smaller models to minimize execution time while maintaining test coverage.\n\n### Pipeline State Management Testing\n\nThe fine-tuning pipeline maintains complex shared state across components, including configuration objects, intermediate data products, and training progress information. Integration tests validate that this state management works correctly under various execution scenarios and failure modes.\n\nState coordination testing verifies that the `StateManager` correctly handles component transitions, maintains consistency of shared data, and provides proper isolation between concurrent operations. These tests exercise various execution patterns: normal forward progress, component restart scenarios, and error recovery situations.\n\nConfiguration propagation testing ensures that changes to configuration objects are properly reflected across all dependent components. When a user modifies LoRA rank parameters, for example, the tests verify that memory estimates are updated, target module configurations are recalculated, and training loop parameters are adjusted accordingly.\n\nCheckpoint and recovery testing validates that training state can be saved and restored correctly, including model weights, optimizer states, LoRA adapter parameters, and training progress metadata. These tests exercise various checkpoint timing scenarios and verify that restored training runs continue exactly where they left off.\n\n### Data Flow Validation Testing\n\nEnd-to-end data flow testing traces training samples through the complete pipeline transformation sequence, from raw input data to final model outputs, ensuring that data integrity is preserved and that transformations are applied correctly at each stage.\n\nThe complete data flow test suite loads realistic training datasets, applies the full data preparation pipeline, configures LoRA adapters and quantization, and executes abbreviated training runs with thorough validation at each step. These tests verify that tokenized data maintains proper alignment with original instruction-response pairs, that gradient computations are applied only to adapter parameters, and that evaluation metrics reflect expected model behavior changes.\n\nCross-component data consistency testing validates that shared data structures remain consistent as they pass between components. For example, when `TokenizedSample` objects are created by the data preparation component, the integration tests verify that the tokenization parameters used are identical to those expected by the training loop, and that attention masks are compatible with the model architecture configuration.\n\nError propagation testing validates that errors occurring in any component are properly caught, contextualized, and propagated through the system with sufficient information for debugging. These tests inject various types of failures - configuration errors, data corruption, resource exhaustion - and verify that the resulting error messages provide actionable diagnostic information.\n\n### Resource Management Integration Testing\n\nThe fine-tuning pipeline must coordinate access to limited GPU memory and compute resources across multiple components. Integration testing validates that this resource management works correctly under various load scenarios and resource constraints.\n\nMemory coordination testing verifies that memory usage predictions align with actual consumption, that components properly release GPU memory when finished, and that the combination of quantization and LoRA adapters achieves expected efficiency gains. These tests exercise various model sizes and configuration combinations to validate resource management across different scales.\n\nCompute resource sharing testing validates that components properly coordinate access to GPU compute resources, that operations are properly queued and executed in sequence when necessary, and that resource contention doesn't lead to deadlocks or performance degradation.\n\nHardware failure simulation testing verifies that the pipeline handles various hardware-related failures gracefully: GPU out-of-memory conditions, CUDA driver issues, thermal throttling events, and temporary network connectivity problems when accessing model repositories.\n\n## Milestone Validation Checkpoints\n\nEach milestone in the fine-tuning pipeline development process requires specific validation checkpoints that verify not only the correctness of individual components but also their readiness for integration with subsequent milestone deliverables. These checkpoints serve as quality gates that prevent architectural problems from propagating to later stages of development.\n\n### Milestone 1: Dataset Preparation Validation\n\nThe dataset preparation milestone validation focuses on verifying that training data is correctly loaded, formatted, and prepared for consumption by the training pipeline. This checkpoint must validate both the technical correctness of the data transformations and the statistical properties of the resulting datasets.\n\nData loading validation verifies that all supported input formats (JSON, JSONL, CSV, Parquet) are correctly parsed and converted to standardized `InstructionSample` objects. The validation process tests various file sizes, encoding formats, and data quality scenarios to ensure robust data ingestion. Expected behavior includes proper handling of malformed records, consistent field mapping across formats, and accurate preservation of metadata through the loading process.\n\nChat template application validation ensures that instruction-response pairs are correctly formatted according to the target model's expected conversation format. The checkpoint validates that special tokens are inserted correctly, that multi-turn conversations maintain proper role sequencing, and that template application is deterministic and reversible. Tests include comparison of template outputs against known-good examples and validation of template consistency across different conversation lengths and complexity levels.\n\nTokenization validation verifies that text is correctly converted to token sequences with proper attention masks, padding, and truncation handling. The checkpoint includes validation of token length distributions, verification that tokenization parameters are applied consistently, and testing of boundary conditions like empty inputs or extremely long sequences. Expected outputs include correctly formatted `TokenizedSample` objects with token counts that match statistical expectations for the input data.\n\nTrain-validation split validation ensures that data partitioning maintains proper distribution characteristics and prevents data leakage between training and validation sets. The checkpoint verifies that stratification parameters are applied correctly, that split ratios match configuration specifications, and that the resulting datasets have appropriate statistical properties for effective training and evaluation.\n\n### Milestone 2: LoRA Configuration Validation\n\nLoRA configuration validation focuses on verifying that adapter setup produces the expected parameter efficiency gains while maintaining sufficient model capacity for effective fine-tuning. This checkpoint validates both the mathematical correctness of the LoRA implementation and the practical effectiveness of the configuration choices.\n\nTarget module detection validation verifies that the automatic identification of suitable layers for LoRA adapter injection works correctly across different model architectures. The checkpoint tests detection logic against various transformer architectures, validates that detected modules have appropriate dimensions for low-rank decomposition, and ensures that module selection follows established best practices for parameter-efficient fine-tuning.\n\nRank and alpha parameter validation ensures that the selected hyperparameters produce reasonable trade-offs between model capacity and parameter efficiency. The checkpoint includes validation of the mathematical relationship between rank, alpha, and effective learning rate scaling, verification that parameter count reductions meet efficiency targets, and testing of rank selection logic under various memory constraint scenarios.\n\nAdapter injection validation verifies that LoRA matrices are correctly inserted into the target model layers and that the resulting model maintains expected forward pass behavior. The checkpoint includes comparison of model outputs before and after adapter injection (which should be identical with untrained adapters), validation of parameter freezing for base model weights, and verification that gradient computation is limited to adapter parameters only.\n\nParameter efficiency validation confirms that the configured LoRA setup achieves the expected parameter count reduction while maintaining sufficient capacity for the target fine-tuning task. The checkpoint measures actual trainable parameter counts, validates efficiency ratios against targets, and verifies that memory usage predictions align with practical measurements.\n\n### Milestone 3: QLoRA Quantization Validation\n\nQuantization validation focuses on verifying that 4-bit quantization achieves significant memory reduction while maintaining acceptable model quality and training stability. This checkpoint requires actual GPU testing to validate memory usage and numerical precision characteristics.\n\nNormalFloat quantization validation verifies that 4-bit quantization is correctly applied to model weights and that the resulting quantized model maintains reasonable output quality compared to the full-precision baseline. The checkpoint includes comparison of model outputs at various quantization configurations, validation of quantization parameter settings, and measurement of actual memory usage reduction achieved.\n\nMixed-precision training validation ensures that the combination of 4-bit weight storage with float16/bfloat16 computation produces stable training dynamics without significant accuracy degradation. The checkpoint includes validation of forward pass correctness, gradient computation stability, and compatibility between quantization settings and training loop configuration.\n\nMemory efficiency validation measures actual GPU memory usage with different quantization configurations and validates that usage aligns with theoretical predictions. The checkpoint includes testing with various model sizes, validation of memory usage under different batch sizes and sequence lengths, and verification that quantization achieves targeted memory reduction goals.\n\nDouble quantization validation verifies that the optional double quantization feature provides additional memory savings without compromising training stability or model quality. The checkpoint includes measurement of incremental memory savings, validation of numerical precision preservation, and testing of compatibility with various quantization configurations.\n\n### Milestone 4: Training Loop Validation\n\nTraining loop validation focuses on verifying that the fine-tuning process produces expected learning dynamics, maintains training stability, and generates reliable checkpoints for model recovery and evaluation. This checkpoint requires extended testing with realistic training scenarios.\n\nGradient accumulation validation ensures that simulated larger batch sizes through gradient accumulation produce training dynamics equivalent to actual large batch training. The checkpoint includes comparison of loss curves with different accumulation strategies, validation of learning rate scaling relationships, and verification that effective batch size calculations are mathematically correct.\n\nLearning rate scheduling validation verifies that warmup, decay, and other scheduling strategies produce expected learning dynamics and convergence characteristics. The checkpoint includes testing of various scheduling configurations, validation of learning rate transitions, and comparison of convergence behavior against established fine-tuning best practices.\n\nCheckpoint management validation ensures that training state is correctly saved and restored, enabling reliable training interruption and resumption. The checkpoint includes testing of various checkpoint timing scenarios, validation of state restoration accuracy, and verification that resumed training continues with identical dynamics to uninterrupted training.\n\nLoss tracking and early stopping validation verifies that training progress monitoring works correctly and that early stopping criteria prevent overfitting while allowing sufficient learning. The checkpoint includes validation of loss calculation accuracy, testing of early stopping trigger conditions, and verification that stopped training produces better validation performance than continued training.\n\n### Milestone 5: Evaluation and Merging Validation\n\nThe final milestone validation focuses on verifying that fine-tuned models demonstrate measurable improvement over baseline models and that adapter merging produces deployable models with maintained performance characteristics.\n\nPerplexity evaluation validation ensures that the fine-tuned model shows improved language modeling performance on validation data compared to the base model. The checkpoint includes validation of perplexity calculation accuracy, comparison of prompt versus response perplexity improvements, and verification that perplexity improvements correlate with task-specific performance gains.\n\nTask-specific evaluation validation verifies that the fine-tuned model demonstrates improved performance on domain-relevant benchmarks and instruction-following tasks. The checkpoint includes comparison against baseline model performance, validation of evaluation metric calculations, and verification that performance improvements are statistically significant and practically meaningful.\n\nAdapter merging validation ensures that combining LoRA adapter weights with base model parameters produces a standalone model with equivalent performance to the adapter-enabled version. The checkpoint includes numerical precision validation of weight merging operations, equivalence testing of model outputs before and after merging, and verification that merged models maintain expected performance characteristics.\n\nModel export validation verifies that exported models in various formats (HuggingFace, GGUF) load correctly in target inference environments and maintain acceptable performance characteristics. The checkpoint includes compatibility testing with target inference frameworks, validation of format conversion accuracy, and verification that exported models produce outputs consistent with the original fine-tuned model.\n\n## Performance and Memory Testing\n\nPerformance and memory testing for LLM fine-tuning systems requires specialized approaches that account for the unique characteristics of parameter-efficient training, quantization effects, and GPU memory management. Unlike traditional application performance testing, these tests must validate not just speed and throughput, but also memory efficiency, numerical precision preservation, and training stability under resource constraints.\n\n### Memory Efficiency Validation Framework\n\nThe foundation of performance testing rests on comprehensive memory monitoring that tracks GPU memory usage patterns throughout all phases of the fine-tuning pipeline. Our `MemoryMonitor` system captures baseline measurements, tracks peak usage during training, and validates that memory consumption aligns with theoretical predictions based on model size, quantization settings, and LoRA configuration parameters.\n\nMemory efficiency testing follows a structured progression through different scales and configurations. Initial tests use smaller models (1-3B parameters) to validate that memory monitoring infrastructure works correctly and that basic efficiency calculations are accurate. These tests establish baseline memory consumption patterns for different quantization and adapter configurations without requiring extensive GPU resources.\n\nProgressive scaling tests validate memory efficiency across model sizes, starting with small models and scaling up to larger configurations as memory efficiency is validated. Each scale tests the relationship between theoretical memory predictions and actual usage, identifies memory usage patterns that don't scale linearly, and validates that quantization and adapter configurations maintain their efficiency characteristics across different model sizes.\n\n| Model Size Category | Parameter Range | Memory Test Focus | Expected Efficiency Gain |\n|-------------------|-----------------|-------------------|-------------------------|\n| Small Scale | 1-3B parameters | Baseline validation, infrastructure testing | 4x reduction with 4-bit quantization |\n| Medium Scale | 7-13B parameters | Scaling validation, configuration optimization | 4x quantization + 99% LoRA parameter reduction |\n| Large Scale | 30B+ parameters | Practical deployment validation, resource limits | Combined efficiency enabling single-GPU training |\n\n### Training Performance Benchmarking\n\nTraining performance testing validates that parameter-efficient fine-tuning achieves acceptable training speeds while maintaining the memory efficiency gains that make large model fine-tuning practical. These tests measure tokens per second, effective throughput with gradient accumulation, and training stability under different batch size and sequence length configurations.\n\nThroughput benchmarking compares training speed across different efficiency configurations, measuring the trade-offs between memory savings and computational overhead. Pure full-precision training serves as a baseline, with comparisons against 4-bit quantized training, LoRA-only training, and combined QLoRA configurations. These benchmarks account for the overhead of quantization/dequantization operations and the computational characteristics of low-rank matrix operations.\n\nGradient accumulation performance testing validates that simulated larger batch sizes don't introduce disproportionate overhead compared to actual large batch training. These tests measure the relationship between accumulation steps and training throughput, identify optimal accumulation strategies for different hardware configurations, and validate that gradient accumulation maintains training stability characteristics equivalent to large batch training.\n\nMemory pressure testing validates training performance under various GPU memory utilization levels, ensuring that the system maintains stable performance even when operating close to memory limits. These tests deliberately stress memory usage through larger batch sizes, longer sequence lengths, and concurrent operations to validate that memory management remains stable under realistic high-utilization scenarios.\n\n### Quantization Impact Assessment\n\nQuantization introduces trade-offs between memory efficiency and numerical precision that must be carefully validated to ensure that memory savings don't compromise training effectiveness or model quality. Our testing framework measures these trade-offs across different quantization configurations and model architectures.\n\nNumerical precision testing compares model outputs and training dynamics between full-precision and quantized configurations. These tests measure the magnitude of differences in forward pass outputs, gradient computations, and parameter updates to quantify the precision impact of different quantization strategies. The testing includes validation that precision differences remain within acceptable bounds for effective fine-tuning.\n\nTraining stability assessment validates that quantized training maintains stable learning dynamics without gradient explosions, loss spikes, or convergence failures. These tests run extended training sequences with various quantization configurations, monitoring gradient norms, loss trajectory smoothness, and parameter update characteristics to ensure that quantization doesn't introduce training instabilities.\n\nQuality preservation testing measures the impact of quantization on final model performance, validating that memory efficiency gains don't come at the cost of significant quality degradation. These tests compare fine-tuned model performance between full-precision and quantized training runs, measuring perplexity differences, task-specific performance impacts, and the relationship between quantization aggressiveness and quality preservation.\n\n### Hardware Compatibility and Scalability Testing\n\nThe fine-tuning pipeline must work reliably across different GPU hardware configurations, CUDA versions, and system environments. Performance testing validates not only optimal-case performance but also compatibility and graceful degradation across different hardware scenarios.\n\nGPU architecture testing validates performance characteristics across different GPU generations and memory configurations. Tests cover various NVIDIA architectures (Ampere, Ada Lovelace, Hopper) to validate that quantization implementations work correctly and that performance characteristics scale appropriately with different compute capabilities and memory bandwidths.\n\nCUDA compatibility testing ensures that the quantization and training infrastructure works correctly across different CUDA toolkit versions and driver configurations. These tests validate that bitsandbytes integration remains stable, that quantization operations produce consistent results, and that performance characteristics are maintained across different CUDA environments.\n\nMulti-GPU scaling testing validates that the pipeline can effectively utilize multiple GPU configurations when available, while gracefully degrading to single-GPU operation when necessary. These tests measure scaling efficiency for data parallel training, validate memory usage distribution across multiple GPUs, and ensure that multi-GPU configurations maintain training stability and convergence characteristics.\n\nSystem resource testing validates performance under various system memory, storage, and CPU configurations, ensuring that the pipeline performs acceptably even on systems with limited resources beyond GPU capabilities. These tests identify bottlenecks in data loading, checkpoint saving, and evaluation phases that might not be apparent during pure training performance testing.\n\n### Benchmark Comparison Framework\n\nTo provide meaningful performance context, our testing framework includes comparisons against established fine-tuning approaches and publicly available benchmarks. These comparisons validate that our parameter-efficient approach provides practical advantages over alternative methods while maintaining competitive training effectiveness.\n\nBaseline comparison testing measures training performance and memory usage against full fine-tuning approaches, quantifying the practical benefits of parameter-efficient methods. These tests provide concrete measurements of memory reduction, training speed differences, and final model quality comparisons to validate that efficiency gains justify any trade-offs in training characteristics.\n\nIndustry benchmark alignment validates performance against published results for similar fine-tuning approaches, ensuring that our implementation achieves performance characteristics comparable to research implementations and commercial fine-tuning services. These comparisons provide confidence that the implementation represents state-of-the-art parameter-efficient fine-tuning practices.\n\nRegression testing maintains performance baselines across development iterations, ensuring that code changes don't inadvertently degrade performance characteristics or introduce memory usage regressions. These tests establish automated performance monitoring that alerts to significant changes in training throughput, memory efficiency, or model quality outcomes.\n\n### Implementation Guidance\n\nOur testing infrastructure is built around comprehensive monitoring, synthetic data generation, and automated validation that provides high confidence in system reliability while minimizing the computational cost of test execution.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Unit Testing | pytest with custom fixtures | pytest + hypothesis for property-based testing |\n| Memory Monitoring | torch.cuda.memory_* functions | Custom CUDA memory profiler with timeline visualization |\n| Performance Benchmarking | Simple timing with time.time() | torch.profiler with detailed kernel analysis |\n| Mock Infrastructure | unittest.mock with manual setup | pytest-mock with automatic dependency injection |\n| Test Data Generation | Static JSON files with test cases | Synthetic data generators with configurable distributions |\n| Validation Framework | Manual assertion checking | Automated validation with statistical significance testing |\n\n#### Recommended File Structure\n\n```\ntests/\n├── unit/\n│   ├── test_data_preparation.py      # Dataset loading, formatting, tokenization\n│   ├── test_lora_config.py          # LoRA configuration and parameter analysis\n│   ├── test_quantization.py         # Quantization configuration and memory estimation\n│   ├── test_training_loop.py        # Training orchestration and checkpoint management\n│   └── test_evaluation.py           # Evaluation metrics and model merging\n├── integration/\n│   ├── test_component_interactions.py   # Component interface validation\n│   ├── test_data_flow.py               # End-to-end data transformation\n│   ├── test_state_management.py        # Pipeline state coordination\n│   └── test_resource_management.py     # Memory and GPU resource handling\n├── performance/\n│   ├── test_memory_efficiency.py       # Memory usage and optimization validation\n│   ├── test_training_performance.py    # Training speed and throughput benchmarking\n│   ├── test_quantization_impact.py     # Quantization trade-off analysis\n│   └── test_hardware_compatibility.py  # Cross-platform and cross-hardware validation\n├── fixtures/\n│   ├── synthetic_datasets.py           # Test data generation utilities\n│   ├── mock_models.py                  # Lightweight model mocks for testing\n│   ├── memory_monitoring.py            # Memory usage tracking infrastructure\n│   └── performance_benchmarks.py       # Performance measurement utilities\n└── milestone_validation/\n    ├── test_milestone_1_data_prep.py   # Dataset preparation milestone validation\n    ├── test_milestone_2_lora.py        # LoRA configuration milestone validation\n    ├── test_milestone_3_quantization.py # Quantization milestone validation\n    ├── test_milestone_4_training.py     # Training loop milestone validation\n    └── test_milestone_5_evaluation.py   # Evaluation and merging milestone validation\n```\n\n#### Core Testing Infrastructure\n\nThe testing infrastructure provides complete, working utilities for memory monitoring, performance benchmarking, and test data generation that can be used immediately across all test categories.\n\n**Memory Monitoring Infrastructure (Complete Implementation)**\n\n```python\n# tests/fixtures/memory_monitoring.py\nimport torch\nimport psutil\nimport time\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Optional, Any\nfrom contextlib import contextmanager\n\n@dataclass\nclass MemoryMetrics:\n    stage: str\n    gpu_memory_allocated: float\n    gpu_memory_cached: float  \n    gpu_memory_reserved: float\n    system_memory_used: float\n    timestamp: float\n    details: Optional[Dict[str, Any]] = None\n\nclass MemoryMonitor:\n    def __init__(self):\n        self.baseline_gpu_memory: Optional[int] = None\n        self.baseline_system_memory: int = 0\n        self.measurements: List[MemoryMetrics] = []\n        \n    def capture_baseline(self) -> None:\n        \"\"\"Record initial memory state before model loading\"\"\"\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            self.baseline_gpu_memory = torch.cuda.memory_allocated()\n        self.baseline_system_memory = psutil.virtual_memory().used\n        \n    def measure_current_usage(self, stage: str) -> Dict[str, float]:\n        \"\"\"Measure current memory and return usage statistics\"\"\"\n        timestamp = time.time()\n        \n        if torch.cuda.is_available():\n            gpu_allocated = torch.cuda.memory_allocated()\n            gpu_cached = torch.cuda.memory_cached()\n            gpu_reserved = torch.cuda.memory_reserved()\n        else:\n            gpu_allocated = gpu_cached = gpu_reserved = 0.0\n            \n        system_used = psutil.virtual_memory().used\n        \n        metrics = MemoryMetrics(\n            stage=stage,\n            gpu_memory_allocated=gpu_allocated / 1024**3,  # Convert to GB\n            gpu_memory_cached=gpu_cached / 1024**3,\n            gpu_memory_reserved=gpu_reserved / 1024**3,\n            system_memory_used=system_used / 1024**3,\n            timestamp=timestamp\n        )\n        \n        self.measurements.append(metrics)\n        \n        return {\n            'gpu_allocated_gb': metrics.gpu_memory_allocated,\n            'gpu_cached_gb': metrics.gpu_memory_cached,\n            'gpu_reserved_gb': metrics.gpu_memory_reserved,\n            'system_used_gb': metrics.system_memory_used,\n            'gpu_peak_gb': max(m.gpu_memory_allocated for m in self.measurements)\n        }\n        \n    def get_peak_usage(self) -> Dict[str, float]:\n        \"\"\"Return peak memory usage across all measurements\"\"\"\n        if not self.measurements:\n            return {}\n            \n        return {\n            'peak_gpu_allocated': max(m.gpu_memory_allocated for m in self.measurements),\n            'peak_gpu_cached': max(m.gpu_memory_cached for m in self.measurements),\n            'peak_gpu_reserved': max(m.gpu_memory_reserved for m in self.measurements),\n            'peak_system_used': max(m.system_memory_used for m in self.measurements)\n        }\n    \n    @contextmanager\n    def monitor_stage(self, stage_name: str):\n        \"\"\"Context manager for monitoring memory during a specific stage\"\"\"\n        self.measure_current_usage(f\"{stage_name}_start\")\n        try:\n            yield self\n        finally:\n            self.measure_current_usage(f\"{stage_name}_end\")\n```\n\n**Performance Benchmarking Infrastructure (Complete Implementation)**\n\n```python\n# tests/fixtures/performance_benchmarks.py\nimport time\nimport torch\nimport statistics\nfrom typing import List, Dict, Any, Callable\nfrom dataclasses import dataclass\nfrom contextlib import contextmanager\n\n@dataclass\nclass PerformanceBenchmark:\n    operation_name: str\n    execution_times: List[float]\n    throughput_metrics: Dict[str, float]\n    resource_usage: Dict[str, float]\n    \n    @property\n    def mean_time(self) -> float:\n        return statistics.mean(self.execution_times)\n    \n    @property\n    def std_time(self) -> float:\n        return statistics.stdev(self.execution_times) if len(self.execution_times) > 1 else 0.0\n    \n    @property \n    def median_time(self) -> float:\n        return statistics.median(self.execution_times)\n\nclass PerformanceProfiler:\n    def __init__(self):\n        self.benchmarks: Dict[str, PerformanceBenchmark] = {}\n        \n    @contextmanager\n    def profile_operation(self, operation_name: str, throughput_unit: str = \"items/sec\"):\n        \"\"\"Profile a single operation with timing and resource tracking\"\"\"\n        # Clear GPU cache for consistent measurements\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n            torch.cuda.empty_cache()\n            \n        start_time = time.perf_counter()\n        start_gpu_mem = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n        \n        try:\n            yield\n        finally:\n            if torch.cuda.is_available():\n                torch.cuda.synchronize()\n            \n            end_time = time.perf_counter()\n            end_gpu_mem = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n            \n            execution_time = end_time - start_time\n            gpu_memory_delta = (end_gpu_mem - start_gpu_mem) / 1024**2  # MB\n            \n            if operation_name not in self.benchmarks:\n                self.benchmarks[operation_name] = PerformanceBenchmark(\n                    operation_name=operation_name,\n                    execution_times=[],\n                    throughput_metrics={},\n                    resource_usage={}\n                )\n                \n            self.benchmarks[operation_name].execution_times.append(execution_time)\n            self.benchmarks[operation_name].resource_usage['gpu_memory_delta_mb'] = gpu_memory_delta\n    \n    def benchmark_repeated_operation(self, operation_name: str, operation_func: Callable, \n                                   num_iterations: int = 5, **kwargs) -> PerformanceBenchmark:\n        \"\"\"Benchmark an operation multiple times for statistical reliability\"\"\"\n        for _ in range(num_iterations):\n            with self.profile_operation(operation_name):\n                operation_func(**kwargs)\n                \n        return self.benchmarks[operation_name]\n    \n    def compare_operations(self, operation_names: List[str]) -> Dict[str, Dict[str, float]]:\n        \"\"\"Compare performance metrics across multiple operations\"\"\"\n        comparison = {}\n        for name in operation_names:\n            if name in self.benchmarks:\n                benchmark = self.benchmarks[name]\n                comparison[name] = {\n                    'mean_time': benchmark.mean_time,\n                    'median_time': benchmark.median_time,\n                    'std_time': benchmark.std_time,\n                    'relative_speed': 1.0  # Will be calculated relative to baseline\n                }\n                \n        # Calculate relative speeds (baseline = first operation)\n        if operation_names and operation_names[0] in comparison:\n            baseline_time = comparison[operation_names[0]]['mean_time']\n            for name in operation_names:\n                if name in comparison:\n                    comparison[name]['relative_speed'] = baseline_time / comparison[name]['mean_time']\n                    \n        return comparison\n```\n\n**Synthetic Test Data Generation (Complete Implementation)**\n\n```python\n# tests/fixtures/synthetic_datasets.py\nimport json\nimport random\nfrom typing import List, Dict, Any, Iterator\nfrom dataclasses import dataclass, asdict\n\n@dataclass\nclass SyntheticConversation:\n    instruction: str\n    response: str\n    system_prompt: str = \"You are a helpful assistant.\"\n    quality_score: float = 1.0\n    source: str = \"synthetic\"\n    \ndef generate_instruction_response_pairs(num_samples: int = 100, \n                                       min_instruction_tokens: int = 5,\n                                       max_instruction_tokens: int = 50,\n                                       min_response_tokens: int = 10,\n                                       max_response_tokens: int = 200) -> List[Dict[str, Any]]:\n    \"\"\"Generate synthetic instruction-response pairs with controllable characteristics\"\"\"\n    \n    # Template instruction patterns\n    instruction_templates = [\n        \"Explain the concept of {topic} in simple terms.\",\n        \"Write a {length} summary of {topic}.\",\n        \"List {count} key points about {topic}.\",\n        \"Compare and contrast {topic1} and {topic2}.\",\n        \"Provide step-by-step instructions for {topic}.\"\n    ]\n    \n    # Sample topics and parameters\n    topics = [\"machine learning\", \"data structures\", \"algorithms\", \"software engineering\", \n              \"artificial intelligence\", \"databases\", \"web development\", \"cybersecurity\"]\n    lengths = [\"brief\", \"detailed\", \"comprehensive\"]\n    counts = [\"3\", \"5\", \"7\", \"10\"]\n    \n    conversations = []\n    \n    for i in range(num_samples):\n        # Generate instruction\n        template = random.choice(instruction_templates)\n        if \"{topic1}\" in template and \"{topic2}\" in template:\n            topic1, topic2 = random.sample(topics, 2)\n            instruction = template.format(topic1=topic1, topic2=topic2)\n        elif \"{count}\" in template:\n            instruction = template.format(topic=random.choice(topics), count=random.choice(counts))\n        elif \"{length}\" in template:\n            instruction = template.format(topic=random.choice(topics), length=random.choice(lengths))\n        else:\n            instruction = template.format(topic=random.choice(topics))\n            \n        # Generate response (simplified - in practice would use more sophisticated generation)\n        response_length = random.randint(min_response_tokens, max_response_tokens)\n        response_words = [\"This\", \"is\", \"a\", \"synthetic\", \"response\", \"about\", \"the\", \"topic\"] * (response_length // 8 + 1)\n        response = \" \".join(response_words[:response_length]) + \".\"\n        \n        conversations.append({\n            \"instruction\": instruction,\n            \"response\": response,\n            \"system_prompt\": \"You are a helpful assistant.\",\n            \"sample_id\": f\"synthetic_{i:06d}\",\n            \"source\": \"synthetic_generator\",\n            \"quality_score\": random.uniform(0.7, 1.0)\n        })\n        \n    return conversations\n\ndef generate_conversation_dataset(output_path: str, num_samples: int = 1000):\n    \"\"\"Generate a complete synthetic dataset and save to file\"\"\"\n    conversations = generate_instruction_response_pairs(num_samples)\n    \n    with open(output_path, 'w') as f:\n        for conv in conversations:\n            json.dump(conv, f)\n            f.write('\\n')\n            \n    return output_path\n```\n\n#### Unit Test Implementation Examples\n\n**Data Preparation Unit Tests (Core Implementation)**\n\n```python\n# tests/unit/test_data_preparation.py\nimport pytest\nimport tempfile\nimport json\nfrom pathlib import Path\nfrom unittest.mock import Mock, patch\n\n# TODO 1: Import the data preparation components from your implementation\n# from src.data_preparation import DataLoader, InstructionFormatter, TokenizationPipeline\n\ndef test_data_loader_handles_multiple_formats():\n    \"\"\"Test that DataLoader correctly loads and validates data from different file formats\"\"\"\n    # TODO 2: Create temporary test files in different formats (JSON, JSONL, CSV)\n    # TODO 3: Initialize DataLoader and test loading from each format\n    # TODO 4: Validate that all formats produce identical InstructionSample objects\n    # TODO 5: Test error handling for malformed files and unsupported formats\n    pass\n\ndef test_chat_template_application():\n    \"\"\"Test that chat templates are applied correctly for different conversation patterns\"\"\"\n    # TODO 6: Create mock tokenizer with known chat template\n    # TODO 7: Test template application on single-turn conversations\n    # TODO 8: Test template application on multi-turn conversations  \n    # TODO 9: Validate that special tokens are inserted correctly\n    # TODO 10: Test template consistency across multiple applications\n    pass\n\ndef test_tokenization_with_length_constraints():\n    \"\"\"Test that tokenization handles length constraints and attention masks correctly\"\"\"\n    # TODO 11: Create mock tokenizer with predictable behavior\n    # TODO 12: Test tokenization of samples within length limits\n    # TODO 13: Test truncation behavior for samples exceeding limits\n    # TODO 14: Validate attention mask generation for different input lengths\n    # TODO 15: Test padding behavior for batch processing\n    pass\n\ndef test_train_validation_split_stratification():\n    \"\"\"Test that dataset splitting maintains proper distribution characteristics\"\"\"  \n    # TODO 16: Generate synthetic dataset with known distribution characteristics\n    # TODO 17: Apply train-validation split with stratification parameters\n    # TODO 18: Validate that split ratios match configuration\n    # TODO 19: Test that stratification preserves important distribution properties\n    # TODO 20: Verify no data leakage between training and validation sets\n    pass\n```\n\n**LoRA Configuration Unit Tests (Core Implementation)**\n\n```python\n# tests/unit/test_lora_config.py\nimport pytest\nimport torch\nfrom unittest.mock import Mock, MagicMock\n\n# TODO 1: Import LoRA configuration components from your implementation  \n# from src.lora_config import LoRAConfig, TargetModuleDetector, RankSelector\n\ndef test_lora_config_validation():\n    \"\"\"Test that LoRA configuration validates parameters correctly\"\"\"\n    # TODO 2: Test valid LoRA configurations are accepted\n    # TODO 3: Test invalid rank values (negative, zero) are rejected\n    # TODO 4: Test invalid alpha values are handled appropriately\n    # TODO 5: Test target module list validation\n    # TODO 6: Test effective_alpha calculation accuracy\n    pass\n\ndef test_target_module_detection():\n    \"\"\"Test automatic detection of suitable modules for LoRA adapter injection\"\"\"\n    # TODO 7: Create mock transformer model with known architecture\n    # TODO 8: Test detection of attention modules (Q, K, V, O projections)\n    # TODO 9: Test detection of MLP/feed-forward modules  \n    # TODO 10: Validate detected modules have appropriate dimensions\n    # TODO 11: Test architecture-specific detection patterns\n    pass\n\ndef test_rank_selection_logic():\n    \"\"\"Test that rank selection considers task complexity and memory constraints\"\"\"\n    # TODO 12: Test rank selection for different task complexity levels\n    # TODO 13: Test memory constraint handling in rank selection\n    # TODO 14: Validate that recommended ranks produce expected parameter counts\n    # TODO 15: Test edge cases like extremely limited memory or very simple tasks\n    pass\n\ndef test_parameter_efficiency_calculations():\n    \"\"\"Test that parameter efficiency metrics are calculated correctly\"\"\"\n    # TODO 16: Create mock model with known parameter distribution\n    # TODO 17: Apply LoRA configuration and measure parameter changes\n    # TODO 18: Validate trainable parameter ratio calculations\n    # TODO 19: Test memory usage estimation accuracy\n    # TODO 20: Verify efficiency metrics match theoretical expectations\n    pass\n```\n\n#### Integration Test Implementation Examples\n\n**Component Interaction Integration Tests**\n\n```python\n# tests/integration/test_component_interactions.py\nimport pytest\nimport torch\nfrom pathlib import Path\n\ndef test_data_preparation_to_training_integration():\n    \"\"\"Test complete flow from data loading to training-ready datasets\"\"\"\n    # TODO 1: Load realistic test dataset using data preparation components\n    # TODO 2: Apply full data preparation pipeline (loading, formatting, tokenization)\n    # TODO 3: Validate that output format matches training loop expectations\n    # TODO 4: Test that tokenization parameters are consistent across pipeline\n    # TODO 5: Verify that data quality statistics are computed correctly\n    pass\n\ndef test_lora_quantization_compatibility():\n    \"\"\"Test that LoRA adapters work correctly with quantized models\"\"\"\n    # TODO 6: Load small model with 4-bit quantization\n    # TODO 7: Apply LoRA adapter configuration to quantized model\n    # TODO 8: Validate that adapter injection preserves quantization\n    # TODO 9: Test forward pass correctness with combined LoRA + quantization\n    # TODO 10: Verify that gradient computation works correctly\n    pass\n\ndef test_training_checkpoint_integration():\n    \"\"\"Test that checkpointing works correctly with LoRA adapters and quantization\"\"\"\n    # TODO 11: Set up training configuration with LoRA and quantization\n    # TODO 12: Execute short training run and save checkpoint\n    # TODO 13: Load checkpoint and verify model state restoration\n    # TODO 14: Continue training from checkpoint and verify consistency\n    # TODO 15: Test checkpoint compatibility across different configurations\n    pass\n```\n\n#### Milestone Validation Checkpoints\n\n**Milestone Validation Implementation**\n\n```python\n# tests/milestone_validation/test_milestone_1_data_prep.py\nimport pytest\nfrom pathlib import Path\n\ndef test_milestone_1_complete_data_preparation():\n    \"\"\"Complete validation checkpoint for Milestone 1: Dataset Preparation\"\"\"\n    \n    # Expected behavior after Milestone 1:\n    # - All supported data formats load correctly\n    # - Chat templates are applied consistently  \n    # - Tokenization produces proper attention masks\n    # - Train/validation splits maintain distribution properties\n    \n    # TODO 1: Load test dataset from multiple formats\n    # TODO 2: Validate data loading statistics match expectations\n    # TODO 3: Apply chat template and verify output format\n    # TODO 4: Execute tokenization pipeline and validate token distributions\n    # TODO 5: Perform train-validation split and verify no data leakage\n    \n    # Success criteria:\n    assert True  # Replace with actual validation logic\n    # - Data loading completes without errors\n    # - Template application is deterministic and reversible\n    # - Tokenization produces expected token count distributions\n    # - Split datasets have appropriate size ratios and statistical properties\n```\n\n#### Performance Test Implementation Examples\n\n**Memory Efficiency Performance Tests**\n\n```python\n# tests/performance/test_memory_efficiency.py\nimport pytest\nimport torch\nfrom tests.fixtures.memory_monitoring import MemoryMonitor\n\ndef test_quantization_memory_reduction():\n    \"\"\"Test that 4-bit quantization achieves expected memory savings\"\"\"\n    monitor = MemoryMonitor()\n    monitor.capture_baseline()\n    \n    # TODO 1: Load model in full precision and measure memory usage\n    # TODO 2: Load same model with 4-bit quantization\n    # TODO 3: Compare memory usage and validate reduction ratio\n    # TODO 4: Test memory usage with different model sizes\n    # TODO 5: Validate that memory predictions align with actual usage\n    \n    # Expected: ~75% memory reduction with 4-bit quantization\n    pass\n\ndef test_lora_parameter_efficiency():\n    \"\"\"Test that LoRA adapters achieve expected parameter count reduction\"\"\"\n    # TODO 6: Configure model with LoRA adapters\n    # TODO 7: Count trainable parameters vs total parameters\n    # TODO 8: Validate that trainable ratio is less than 1%\n    # TODO 9: Test efficiency across different rank configurations\n    # TODO 10: Verify memory overhead of adapters is minimal\n    pass\n\n@pytest.mark.slow\ndef test_combined_efficiency_large_model():\n    \"\"\"Test combined LoRA + quantization efficiency on large models\"\"\"\n    # TODO 11: Load large model (7B+ parameters) with combined optimizations\n    # TODO 12: Measure actual memory usage vs theoretical predictions\n    # TODO 13: Validate that combination enables single-GPU training\n    # TODO 14: Test training stability with combined optimizations\n    pass\n```\n\nThis comprehensive testing strategy ensures that the LLM fine-tuning pipeline is thoroughly validated at every level, from individual component correctness to full-scale performance characteristics, providing confidence in the system's reliability and effectiveness across diverse training scenarios.\n\n### Implementation Guidance\n\nThe testing infrastructure requires a carefully orchestrated approach that balances thorough validation with practical execution time and resource requirements. The implementation focuses on providing complete, working test utilities that can be immediately integrated into the development workflow.\n\n**Technology Recommendations**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Unit Testing Framework | pytest with basic fixtures | pytest + hypothesis for property-based testing |\n| Memory Profiling | torch.cuda.memory_* APIs | NVIDIA NSight Systems with custom profiling hooks |\n| Performance Benchmarking | Manual timing with time.perf_counter() | torch.profiler with kernel-level analysis and visualization |\n| Mock Infrastructure | unittest.mock with manual setup | pytest-mock with automatic dependency injection and fixture management |\n| Test Data Management | Static JSON files with hardcoded test cases | Synthetic data generators with parameterizable distributions |\n| CI/CD Integration | Manual test execution | GitHub Actions with GPU runners and automated performance regression detection |\n\n**Recommended File Structure**\n\n```\ntests/\n├── conftest.py                          # Shared pytest fixtures and configuration\n├── unit/\n│   ├── data_preparation/\n│   │   ├── test_data_loading.py         # Data format parsing and validation\n│   │   ├── test_chat_templates.py       # Template application and conversation formatting  \n│   │   ├── test_tokenization.py         # Token encoding and attention mask generation\n│   │   └── test_data_splitting.py       # Train-validation partitioning and stratification\n│   ├── lora_configuration/\n│   │   ├── test_config_validation.py    # LoRA parameter validation and constraint checking\n│   │   ├── test_target_detection.py     # Automatic module identification for adapter injection\n│   │   ├── test_rank_selection.py       # Rank and alpha parameter optimization\n│   │   └── test_efficiency_analysis.py  # Parameter count and memory usage calculations\n│   ├── quantization/\n│   │   ├── test_nf4_config.py          # NormalFloat quantization configuration\n│   │   ├── test_memory_estimation.py    # Memory usage prediction and validation\n│   │   └── test_precision_impact.py     # Numerical precision preservation testing\n│   ├── training_loop/\n│   │   ├── test_gradient_accumulation.py # Batch size simulation and scaling\n│   │   ├── test_lr_scheduling.py        # Learning rate warmup and decay strategies\n│   │   ├── test_checkpointing.py        # Training state persistence and recovery\n│   │   └── test_early_stopping.py       # Convergence monitoring and training termination\n│   └── evaluation/\n│       ├── test_perplexity_calc.py      # Language modeling performance measurement\n│       ├── test_task_evaluation.py      # Domain-specific benchmark assessment\n│       ├── test_adapter_merging.py      # LoRA weight combination and standalone model creation\n│       └── test_model_export.py         # Format conversion and deployment preparation\n├── integration/\n│   ├── test_data_to_training_flow.py    # Complete data preparation to training pipeline\n│   ├── test_lora_quantization_compat.py # LoRA and quantization interaction validation\n│   ├── test_checkpoint_recovery.py      # Training interruption and resumption scenarios\n│   └── test_evaluation_pipeline.py      # Training completion to model deployment flow\n├── performance/\n│   ├── test_memory_benchmarks.py        # GPU and system memory usage profiling\n│   ├── test_training_throughput.py      # Tokens per second and batch processing speed\n│   ├── test_quantization_overhead.py    # Performance impact of 4-bit operations\n│   └── test_scaling_characteristics.py  # Performance across different model sizes\n├── milestone_validation/\n│   ├── validate_milestone_1.py          # Dataset preparation milestone checkpoint\n│   ├── validate_milestone_2.py          # LoRA configuration milestone checkpoint  \n│   ├── validate_milestone_3.py          # Quantization setup milestone checkpoint\n│   ├── validate_milestone_4.py          # Training loop milestone checkpoint\n│   └── validate_milestone_5.py          # Evaluation and merging milestone checkpoint\n└── fixtures/\n    ├── synthetic_data_generator.py       # Configurable test dataset creation\n    ├── mock_model_factory.py            # Lightweight transformer model mocks\n    ├── memory_profiling_utils.py        # GPU memory tracking and analysis\n    └── performance_measurement.py        # Benchmarking and timing utilities\n```\n\n**Complete Memory Monitoring Infrastructure**\n\n```python\n# tests/fixtures/memory_profiling_utils.py\nimport torch\nimport psutil\nimport threading\nimport time\nfrom typing import Dict, List, Optional, Callable\nfrom dataclasses import dataclass\nfrom contextlib import contextmanager\n\n@dataclass  \nclass MemorySnapshot:\n    timestamp: float\n    stage: str\n    gpu_allocated_gb: float\n    gpu_cached_gb: float\n    gpu_reserved_gb: float\n    system_memory_gb: float\n    process_memory_gb: float\n\nclass ContinuousMemoryMonitor:\n    \"\"\"Continuous memory monitoring with background thread collection\"\"\"\n    \n    def __init__(self, sampling_interval: float = 0.1):\n        self.sampling_interval = sampling_interval\n        self.snapshots: List[MemorySnapshot] = []\n        self.monitoring = False\n        self.monitor_thread: Optional[threading.Thread] = None\n        self.current_stage = \"idle\"\n        \n    def start_monitoring(self):\n        \"\"\"Start background memory monitoring thread\"\"\"\n        self.monitoring = True\n        self.monitor_thread = threading.Thread(target=self._monitoring_loop, daemon=True)\n        self.monitor_thread.start()\n        \n    def stop_monitoring(self):\n        \"\"\"Stop background monitoring and return collected snapshots\"\"\"\n        self.monitoring = False\n        if self.monitor_thread:\n            self.monitor_thread.join(timeout=1.0)\n        return self.snapshots.copy()\n        \n    def set_stage(self, stage_name: str):\n        \"\"\"Update current monitoring stage label\"\"\"\n        self.current_stage = stage_name\n        \n    def _monitoring_loop(self):\n        \"\"\"Background thread loop for continuous memory sampling\"\"\"\n        while self.monitoring:\n            try:\n                snapshot = self._capture_snapshot()\n                self.snapshots.append(snapshot)\n            except Exception as e:\n                print(f\"Memory monitoring error: {e}\")\n            time.sleep(self.sampling_interval)\n            \n    def _capture_snapshot(self) -> MemorySnapshot:\n        \"\"\"Capture single memory usage snapshot\"\"\"\n        timestamp = time.time()\n        \n        if torch.cuda.is_available():\n            gpu_allocated = torch.cuda.memory_allocated() / (1024**3)\n            gpu_cached = torch.cuda.memory_cached() / (1024**3)  \n            gpu_reserved = torch.cuda.memory_reserved() / (1024**3)\n        else:\n            gpu_allocated = gpu_cached = gpu_reserved = 0.0\n            \n        system_memory = psutil.virtual_memory()\n        process_memory = psutil.Process().memory_info()\n        \n        return MemorySnapshot(\n            timestamp=timestamp,\n            stage=self.current_stage,\n            gpu_allocated_gb=gpu_allocated,\n            gpu_cached_gb=gpu_cached,\n            gpu_reserved_gb=gpu_reserved,\n            system_memory_gb=system_memory.used / (1024**3),\n            process_memory_gb=process_memory.rss / (1024**3)\n        )\n        \n    def get_peak_usage_by_stage(self) -> Dict[str, Dict[str, float]]:\n        \"\"\"Calculate peak memory usage grouped by monitoring stage\"\"\"\n        stage_peaks = {}\n        for snapshot in self.snapshots:\n            if snapshot.stage not in stage_peaks:\n                stage_peaks[snapshot.stage] = {\n                    'peak_gpu_allocated': 0.0,\n                    'peak_gpu_cached': 0.0,\n                    'peak_system_memory': 0.0\n                }\n            \n            peaks = stage_peaks[snapshot.stage]\n            peaks['peak_gpu_allocated'] = max(peaks['peak_gpu_allocated'], snapshot.gpu_allocated_gb)\n            peaks['peak_gpu_cached'] = max(peaks['peak_gpu_cached'], snapshot.gpu_cached_gb)\n            peaks['peak_system_memory'] = max(peaks['peak_system_memory'], snapshot.system_memory_gb)\n            \n        return stage_peaks\n\n@contextmanager\ndef profile_memory_usage(stage_name: str, continuous: bool = False):\n    \"\"\"Context manager for memory usage profiling during specific operations\"\"\"\n    if continuous:\n        monitor = ContinuousMemoryMonitor(sampling_interval=0.05)\n        monitor.set_stage(stage_name)\n        monitor.start_monitoring()\n        \n        try:\n            yield monitor\n        finally:\n            snapshots = monitor.stop_monitoring()\n            \n    else:\n        # Single point-in-time measurements\n        class SimpleMonitor:\n            def __init__(self):\n                self.start_usage = self._measure_current()\n                self.end_usage = None\n                \n            def _measure_current(self):\n                return {\n                    'gpu_allocated': torch.cuda.memory_allocated() / (1024**3) if torch.cuda.is_available() else 0,\n                    'system_memory': psutil.virtual_memory().used / (1024**3)\n                }\n                \n            def get_delta(self):\n                self.end_usage = self._measure_current()\n                return {\n                    'gpu_delta_gb': self.end_usage['gpu_allocated'] - self.start_usage['gpu_allocated'],\n                    'system_delta_gb': self.end_usage['system_memory'] - self.start_usage['system_memory']\n                }\n        \n        monitor = SimpleMonitor()\n        try:\n            yield monitor\n        finally:\n            pass\n```\n\n**Complete Performance Benchmarking Infrastructure**\n\n```python\n# tests/fixtures/performance_measurement.py\nimport time\nimport torch\nimport statistics\nimport json\nfrom typing import Dict, List, Any, Callable, Optional\nfrom dataclasses import dataclass, asdict\nfrom contextlib import contextmanager\nfrom pathlib import Path\n\n@dataclass\nclass BenchmarkResult:\n    operation_name: str\n    mean_duration: float\n    std_duration: float\n    min_duration: float\n    max_duration: float\n    iterations: int\n    throughput_items_per_sec: Optional[float] = None\n    memory_delta_mb: Optional[float] = None\n    gpu_utilization: Optional[float] = None\n\nclass PerformanceBenchmark:\n    \"\"\"Comprehensive performance benchmarking with statistical analysis\"\"\"\n    \n    def __init__(self, warmup_iterations: int = 2, measurement_iterations: int = 10):\n        self.warmup_iterations = warmup_iterations\n        self.measurement_iterations = measurement_iterations\n        self.results: Dict[str, BenchmarkResult] = {}\n        \n    def benchmark_function(self, \n                         function: Callable,\n                         function_name: str,\n                         *args,\n                         items_processed: Optional[int] = None,\n                         **kwargs) -> BenchmarkResult:\n        \"\"\"Benchmark a function with statistical analysis of execution times\"\"\"\n        \n        # Warmup iterations to stabilize performance\n        for _ in range(self.warmup_iterations):\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n                torch.cuda.synchronize()\n            function(*args, **kwargs)\n            \n        # Measurement iterations\n        execution_times = []\n        memory_deltas = []\n        \n        for _ in range(self.measurement_iterations):\n            # Pre-measurement cleanup\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n                torch.cuda.synchronize()\n                start_memory = torch.cuda.memory_allocated()\n            else:\n                start_memory = 0\n                \n            # Time the operation\n            start_time = time.perf_counter()\n            function(*args, **kwargs)\n            \n            if torch.cuda.is_available():\n                torch.cuda.synchronize()\n            end_time = time.perf_counter()\n            \n            # Memory measurement\n            if torch.cuda.is_available():\n                end_memory = torch.cuda.memory_allocated()\n                memory_delta = (end_memory - start_memory) / (1024**2)  # MB\n                memory_deltas.append(memory_delta)\n            \n            execution_times.append(end_time - start_time)\n            \n        # Statistical analysis\n        mean_time = statistics.mean(execution_times)\n        std_time = statistics.stdev(execution_times) if len(execution_times) > 1 else 0.0\n        min_time = min(execution_times) \n        max_time = max(execution_times)\n        \n        # Calculate throughput if items processed is provided\n        throughput = items_processed / mean_time if items_processed else None\n        \n        # Average memory delta\n        avg_memory_delta = statistics.mean(memory_deltas) if memory_deltas else None\n        \n        result = BenchmarkResult(\n            operation_name=function_name,\n            mean_duration=mean_time,\n            std_duration=std_time,\n            min_duration=min_time,\n            max_duration=max_time,\n            iterations=self.measurement_iterations,\n            throughput_items_per_sec=throughput,\n            memory_delta_mb=avg_memory_delta\n        )\n        \n        self.results[function_name] = result\n        return result\n    \n    @contextmanager \n    def benchmark_context(self, operation_name: str, items_processed: Optional[int] = None):\n        \"\"\"Context manager for benchmarking code blocks\"\"\"\n        execution_times = []\n        memory_deltas = []\n        \n        for iteration in range(self.warmup_iterations + self.measurement_iterations):\n            # Preparation\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n                torch.cuda.synchronize()\n                start_memory = torch.cuda.memory_allocated()\n            else:\n                start_memory = 0\n                \n            # Timing\n            start_time = time.perf_counter()\n            yield iteration  # Allow caller to execute their code\n            \n            if torch.cuda.is_available():\n                torch.cuda.synchronize()\n            end_time = time.perf_counter()\n            \n            # Skip warmup iterations for statistics\n            if iteration >= self.warmup_iterations:\n                execution_times.append(end_time - start_time)\n                \n                if torch.cuda.is_available():\n                    end_memory = torch.cuda.memory_allocated()\n                    memory_deltas.append((end_memory - start_memory) / (1024**2))\n        \n        # Calculate statistics and store result\n        if execution_times:\n            mean_time = statistics.mean(execution_times)\n            result = BenchmarkResult(\n                operation_name=operation_name,\n                mean_duration=mean_time,\n                std_duration=statistics.stdev(execution_times) if len(execution_times) > 1 else 0.0,\n                min_duration=min(execution_times),\n                max_duration=max(execution_times),\n                iterations=len(execution_times),\n                throughput_items_per_sec=items_processed / mean_time if items_processed else None,\n                memory_delta_mb=statistics.mean(memory_deltas) if memory_deltas else None\n            )\n            self.results[operation_name] = result\n    \n    def compare_benchmarks(self, baseline_name: str) -> Dict[str, float]:\n        \"\"\"Compare all benchmarks relative to a baseline operation\"\"\"\n        if baseline_name not in self.results:\n            raise ValueError(f\"Baseline '{baseline_name}' not found in results\")\n            \n        baseline_time = self.results[baseline_name].mean_duration\n        comparisons = {}\n        \n        for name, result in self.results.items():\n            comparisons[name] = baseline_time / result.mean_duration  # Relative speedup\n            \n        return comparisons\n    \n    def export_results(self, output_path: Path):\n        \"\"\"Export benchmark results to JSON for analysis and reporting\"\"\"\n        serializable_results = {name: asdict(result) for name, result in self.results.items()}\n        \n        with open(output_path, 'w') as f:\n            json.dump({\n                'benchmark_config': {\n                    'warmup_iterations': self.warmup_iterations,\n                    'measurement_iterations': self.measurement_iterations\n                },\n                'results': serializable_results,\n                'summary': {\n                    'total_operations': len(self.results),\n                    'fastest_operation': min(self.results.keys(), key=lambda x: self.results[x].mean_duration),\n                    'slowest_operation': max(self.results.keys(), key=lambda x: self.results[x].mean_duration)\n                }\n            }, f, indent=2)\n```\n\n**Milestone Validation Checkpoint Implementation**\n\n```python\n# tests/milestone_validation/validate_milestone_1.py\nimport pytest\nimport tempfile\nimport json\nfrom pathlib import Path\nfrom typing import List, Dict\n\ndef test_milestone_1_dataset_preparation_complete():\n    \"\"\"Complete validation checkpoint for Milestone 1: Dataset Preparation\n    \n    This test validates that all dataset preparation capabilities are working\n    correctly and ready for integration with subsequent pipeline components.\n    \"\"\"\n    \n    # TODO 1: Initialize data preparation components\n    # data_loader = DataLoader()\n    # formatter = InstructionFormatter() \n    # tokenizer_pipeline = TokenizationPipeline()\n    # splitter = DatasetSplitter()\n    \n    # Test data loading from multiple formats\n    with tempfile.TemporaryDirectory() as temp_dir:\n        temp_path = Path(temp_dir)\n        \n        # TODO 2: Create test datasets in different formats\n        # json_data = create_test_json_dataset(temp_path / \"test.json\", num_samples=100)\n        # jsonl_data = create_test_jsonl_dataset(temp_path / \"test.jsonl\", num_samples=100) \n        # csv_data = create_test_csv_dataset(temp_path / \"test.csv\", num_samples=100)\n        \n        # TODO 3: Load data from each format and validate consistency\n        # json_samples = list(data_loader.load_data(temp_path / \"test.json\"))\n        # jsonl_samples = list(data_loader.load_data(temp_path / \"test.jsonl\"))\n        # csv_samples = list(data_loader.load_data(temp_path / \"test.csv\"))\n        \n        # Validation checkpoints for data loading\n        # assert len(json_samples) == 100, \"JSON loading should produce 100 samples\"\n        # assert len(jsonl_samples) == 100, \"JSONL loading should produce 100 samples\"  \n        # assert len(csv_samples) == 100, \"CSV loading should produce 100 samples\"\n        \n        # TODO 4: Test chat template application\n        # for sample in json_samples[:10]:  # Test subset for performance\n        #     formatted_text = formatter.apply_template(sample)\n        #     assert isinstance(formatted_text, str),\n\n```\n\n\n## Debugging Guide\n\n> **Milestone(s):** All milestones - debugging skills and systematic troubleshooting approaches are essential throughout the entire fine-tuning pipeline, from data preparation through model export\n\nThe debugging process for LLM fine-tuning can feel like being a detective investigating a crime scene where the evidence keeps changing. Unlike traditional software debugging where stack traces point directly to the problem, fine-tuning issues often manifest as subtle performance degradations, mysterious memory exhaustion, or training instability that emerges hours into a long-running process. The challenge lies in distinguishing between normal training variance and genuine problems that require intervention.\n\nThink of debugging a fine-tuning pipeline like diagnosing a complex mechanical system where multiple components interact in non-obvious ways. A memory leak in the data loader might not surface until the third epoch. A misconfigured chat template might produce syntactically valid but semantically meaningless training data. A LoRA rank that's too low might cause the model to converge to a local minimum that looks reasonable on training metrics but fails catastrophically on evaluation.\n\nThe key insight for effective debugging is that LLM fine-tuning failures rarely have single root causes. Instead, they emerge from the interaction of configuration choices, data quality issues, hardware constraints, and implementation bugs. This section provides systematic approaches to isolate these interacting factors and identify the true root cause.\n\n> **Critical Principle: Establish Baselines First**\n>\n> Before debugging any fine-tuning issue, establish known-good baselines for memory usage, loss curves, and model outputs. Many apparent \"bugs\" are actually normal behavior that wasn't properly characterized during initial setup.\n\n### Symptom-Cause-Fix Tables\n\nThe following tables provide quick reference guides for the most common fine-tuning issues. Each entry follows the pattern of observable symptom, underlying technical cause, and specific remediation steps. These tables are organized by the pipeline stage where symptoms typically first appear.\n\n#### Data Preparation Issues\n\n| Symptom | Root Cause | Diagnostic Steps | Fix |\n|---------|------------|------------------|-----|\n| DataLoader runs out of memory during iteration | Chat template expansion creates unexpectedly long sequences | Check `TokenizedSample.total_length` distribution, examine template-applied samples | Implement dynamic batching based on sequence length, reduce `max_length` in tokenization |\n| Training loss starts abnormally high (>10.0) | Chat templates not properly applied, model seeing raw instruction text | Manually inspect first few batches with `tokenizer.decode()`, verify special tokens | Fix `apply_template()` implementation, ensure EOS tokens between turns |\n| Validation loss significantly higher than training loss from epoch 1 | Train-validation split has distribution mismatch | Compare instruction types, lengths, and domains between splits | Re-split with stratification by instruction category or length bins |\n| Tokenizer produces unexpected token counts | Model tokenizer doesn't match base model, subword vocabulary mismatch | Compare `tokenizer.vocab_size` with model config, test tokenization of known phrases | Load correct tokenizer for base model, verify tokenizer and model compatibility |\n| Some samples have `labels` filled with -100 | Prompt-response separation failed, entire sequence marked as prompt | Check `prompt_length` vs `total_length` ratio, examine tokenization boundaries | Fix prompt-response splitting in `tokenize_sample()`, ensure response has positive labels |\n| Training data appears corrupted with random characters | Encoding mismatch during file loading, UTF-8 vs ASCII issues | Check file encoding with `chardet`, examine raw file bytes | Specify correct encoding in `load_data()`, handle encoding errors gracefully |\n\n#### LoRA Configuration Issues\n\n| Symptom | Root Cause | Diagnostic Steps | Fix |\n|---------|------------|------------------|-----|\n| Training loss doesn't decrease after multiple epochs | LoRA rank too low for task complexity, insufficient adapter capacity | Check `effective_rank()` utilization, compare against task complexity metrics | Increase rank from 16 to 32-64, adjust alpha proportionally |\n| GPU memory usage higher than expected despite LoRA | Base model parameters not frozen, full gradients computed | Verify `requires_grad=False` for base parameters, check `AdapterMetrics.trainable_ratio` | Ensure `freeze_base_parameters()` called after adapter injection |\n| Adapter parameters not updating (gradients zero) | Target modules incorrectly identified, LoRA not injected properly | Inspect `target_modules` list, verify modules exist in model architecture | Use `detect_attention_modules()` and `detect_mlp_modules()` for auto-detection |\n| Training becomes unstable with gradient explosions | LoRA alpha too high relative to rank, effective learning rate too large | Monitor `grad_norm` in training logs, calculate `effective_alpha()` | Reduce alpha or increase rank to maintain alpha/rank ratio around 2:1 |\n| Model outputs degrade compared to base model | Adapter weights initialized improperly, breaking pre-trained representations | Compare base model vs adapted model outputs on same prompts before training | Use proper weight initialization in `init_lora_weights`, start with smaller alpha |\n| Adapter injection fails with shape mismatch errors | Target module dimensions don't support chosen rank | Check `analyze_module_dimensions()` output, verify rank < module width | Reduce rank or exclude problematic modules from `target_modules` |\n\n#### Quantization Issues\n\n| Symptom | Root Cause | Diagnostic Steps | Fix |\n|---------|------------|------------------|-----|\n| Model loading fails with CUDA out of memory | Quantization not applied, model loading in full precision | Check `load_in_4bit=True` in config, verify bitsandbytes installation | Ensure `QuantizationConfig.load_in_4bit=True`, install compatible bitsandbytes version |\n| Training slower than expected despite quantization | Compute dtype mismatch, dequantization overhead in forward pass | Monitor GPU utilization, check `bnb_4bit_compute_dtype` setting | Set compute dtype to `torch.bfloat16`, enable Flash Attention if available |\n| Numerical instability with loss spikes | Mixed precision conflicts with 4-bit quantization | Check loss curves for sudden jumps, monitor gradient norms | Use `torch.bfloat16` instead of `float16`, enable gradient clipping |\n| Model outputs significantly degraded after quantization | Quantization parameters inappropriate for model architecture | Compare base model outputs before/after quantization on test prompts | Try FP4 instead of NF4, disable double quantization, use higher compute dtype |\n| Bitsandbytes import errors or CUDA kernel failures | CUDA version incompatibility with bitsandbytes build | Check CUDA version with `nvcc --version`, verify bitsandbytes CUDA support | Install bitsandbytes version matching CUDA installation |\n| Memory usage not reduced as expected | Double quantization not enabled, optimizer states still in full precision | Check actual memory with `torch.cuda.memory_allocated()`, verify config flags | Enable `bnb_4bit_use_double_quant=True`, use paged optimizers for large models |\n\n#### Training Loop Issues\n\n| Symptom | Root Cause | Diagnostic Steps | Fix |\n|---------|------------|------------------|-----|\n| Training loss oscillates wildly | Learning rate too high for adapter configuration | Plot loss curves, check `learning_rate` vs `effective_alpha()` | Reduce learning rate by 2-4x, implement warmup scheduling |\n| Training stops with NaN loss | Gradient explosion, numerical overflow in mixed precision | Check for NaN in model parameters and gradients | Enable gradient clipping, reduce learning rate, check for corrupted data samples |\n| GPU utilization low despite batch size | Gradient accumulation misconfigured, effective batch size too small | Check `effective_batch_size()` vs hardware capability | Increase gradient accumulation steps, optimize data loading pipeline |\n| Training extremely slow | Data loading bottleneck, synchronous preprocessing | Profile training step timing, check data loader queue utilization | Enable `dataloader_num_workers`, implement async data preprocessing |\n| Checkpoints corrupted or unloadable | Interrupt during checkpoint writing, insufficient disk space | Check disk space, verify checkpoint file integrity | Implement atomic checkpoint writing, verify disk space before saving |\n| Memory usage grows throughout training | Memory leak in data preprocessing, accumulated gradients | Monitor memory usage over time, check for unreleased tensors | Clear optimizer state periodically, fix data loader memory leaks |\n\n#### Evaluation and Export Issues\n\n| Symptom | Root Cause | Diagnostic Steps | Fix |\n|---------|------------|------------------|-----|\n| Perplexity calculation returns NaN or infinity | Division by zero in loss computation, empty validation batches | Check validation dataset size, examine loss computation implementation | Ensure non-empty validation set, add numerical stability checks in perplexity calculation |\n| Fine-tuned model worse than base model | Overfitting, catastrophic forgetting, poor hyperparameter choices | Compare training vs validation metrics, test on diverse prompts | Reduce learning rate, implement early stopping, increase regularization |\n| Adapter merging produces incorrect outputs | Weight merging logic error, scaling factor miscalculation | Test merged vs unmerged models on identical inputs | Verify merging implementation, check alpha scaling in merge operation |\n| GGUF export fails with format errors | Model architecture not supported, tokenizer incompatibility | Check model architecture compatibility, verify tokenizer config | Use compatible model architecture, export tokenizer separately if needed |\n| Exported model significantly slower than expected | Quantization not preserved in export, inefficient format conversion | Profile inference speed, check exported model precision | Preserve quantization in export, use appropriate GGUF quantization settings |\n| Quality degradation after export | Format conversion introduces precision loss | Compare outputs before/after export on test set | Use higher precision export, implement export quality validation |\n\n### Debugging Tools and Techniques\n\nEffective debugging of LLM fine-tuning requires a combination of system-level monitoring, model-specific analysis tools, and careful experimental design. The challenge is distinguishing between hardware issues, implementation bugs, and fundamental algorithmic problems. Think of this toolkit as a medical diagnostic suite - different tools reveal different aspects of the system's health.\n\n#### GPU Profiling and Memory Analysis\n\nUnderstanding GPU behavior during fine-tuning is critical because many issues manifest as performance problems rather than explicit errors. The GPU operates as a complex pipeline where memory allocation, kernel execution, and data transfer can all become bottlenecks.\n\n| Tool | Purpose | Usage Pattern | Key Metrics |\n|------|---------|---------------|-------------|\n| `nvidia-smi` | Real-time GPU monitoring | Run in separate terminal with `-l 1` for continuous updates | GPU utilization %, memory usage MB, temperature, power draw |\n| `torch.profiler` | PyTorch kernel profiling | Wrap training steps, export Chrome trace format | Kernel execution time, memory allocations, CUDA API calls |\n| `py-spy` | Python CPU profiling | Sample running training process | Function call frequency, CPU hotspots, GIL contention |\n| `torch.cuda.memory` | CUDA memory debugging | Call at checkpoints during training | Allocated vs cached memory, memory fragmentation |\n| `wandb.log` | Training metrics streaming | Log every training step | Loss curves, learning rate, gradient norms, hardware metrics |\n\nThe `MemoryMonitor` class provides systematic memory tracking throughout the pipeline. It establishes baseline measurements before model loading and tracks usage at each stage:\n\n| Stage | Memory Measurement | Typical Patterns | Warning Signs |\n|-------|-------------------|------------------|---------------|\n| Baseline | System memory before any model loading | 1-4 GB system RAM usage | High baseline indicates memory leaks from previous runs |\n| Model Loading | GPU memory after base model + quantization | 40-60% of available VRAM for 7B models | >80% usage indicates insufficient VRAM for training |\n| Adapter Injection | Additional memory for LoRA matrices | 1-5% increase over base model | Large increase suggests incorrect target module selection |\n| Training Start | Memory after first forward pass | Additional 20-40% for optimizer states | OOM on first step indicates batch size too large |\n| Training Steady | Memory during sustained training | Stable usage with minor fluctuations | Continuous growth indicates memory leak |\n\n> **Performance Insight: The CUDA Context Trap**\n>\n> PyTorch creates a CUDA context that reserves GPU memory even before model loading. This \"invisible\" memory usage can cause OOM errors that appear to be model-related but are actually context overhead. Always measure baseline GPU memory before any PyTorch operations.\n\n#### Training Visualization and Analysis\n\nTraining fine-tuning models requires monitoring multiple interconnected metrics simultaneously. Unlike traditional machine learning where accuracy provides clear feedback, LLM fine-tuning involves balancing multiple objectives that can conflict.\n\n| Visualization | Purpose | Update Frequency | Interpretation Guidelines |\n|---------------|---------|------------------|---------------------------|\n| Loss curves (train/val) | Monitor convergence and overfitting | Every training step | Smooth downward trend expected, validate gap indicates overfitting |\n| Learning rate schedule | Verify optimizer behavior | Every optimizer step | Should follow configured schedule, sudden changes indicate bugs |\n| Gradient norms | Detect training instability | Every backward pass | Values >1.0 may indicate instability, sudden spikes suggest gradient explosion |\n| Memory usage over time | Identify memory leaks | Every epoch | Should be stable after initial allocation, growth indicates leaks |\n| Token throughput | Measure training efficiency | Every logging interval | Consistent throughput expected, drops indicate bottlenecks |\n| Perplexity validation | Assess model quality | Every evaluation | Should decrease over training, sudden increases suggest overfitting |\n\nThe `TrainingMetrics` and `EvaluationMetrics` structures provide standardized data collection for these visualizations. Key patterns to watch for:\n\n**Healthy Training Patterns:**\n- Training loss decreases smoothly with minor fluctuations\n- Validation loss follows training loss with small gap (1-2x training loss)\n- Gradient norms remain stable between 0.1-1.0\n- Memory usage stable after initial allocation\n- Perplexity decreases consistently during evaluation\n\n**Warning Signs:**\n- Loss curves show sudden spikes or plateaus\n- Validation loss diverges significantly from training loss\n- Gradient norms show sudden spikes >10.0\n- Memory usage grows continuously throughout training\n- Perplexity increases or oscillates wildly\n\n#### Model Behavior Analysis\n\nUnderstanding how fine-tuning changes model behavior requires systematic comparison between base and adapted models. This analysis helps distinguish between successful learning and problematic changes like catastrophic forgetting.\n\n| Analysis Type | Methodology | Success Indicators | Failure Indicators |\n|---------------|-------------|-------------------|-------------------|\n| Output quality comparison | Generate responses to fixed test prompts | Improved relevance, maintained fluency | Degraded language quality, nonsensical outputs |\n| Instruction following | Test adherence to specific formatting requests | Better following of instructions | Ignoring instructions, reverting to base behavior |\n| Domain knowledge retention | Query general knowledge outside training domain | Maintains base model capabilities | Loss of general knowledge, catastrophic forgetting |\n| Response consistency | Multiple generations for same prompt | Consistent high-quality responses | High variance, occasional nonsensical outputs |\n| Length and formatting | Analyze response characteristics | Appropriate length, proper formatting | Extremely short/long responses, format violations |\n\nThe `ModelComparator` class enables systematic before-and-after analysis:\n\n| Comparison Method | Base Model Response | Fine-tuned Response | Evaluation Criteria |\n|-------------------|-------------------|-------------------|-------------------|\n| Factual QA | Accurate but generic | Accurate with domain-specific details | Accuracy maintained, specificity improved |\n| Creative writing | Coherent but bland | Coherent with target style | Coherence maintained, style successfully adapted |\n| Code generation | Syntactically correct | Syntactically correct with conventions | Correctness maintained, conventions learned |\n| Instruction following | Partial compliance | Full compliance with formatting | Instruction adherence significantly improved |\n\n#### Systematic Debugging Methodology\n\nWhen facing a fine-tuning issue, follow this systematic approach to isolate the root cause:\n\n**Phase 1: Information Gathering (5-10 minutes)**\n1. Record exact error message, loss values, and training metrics at failure point\n2. Capture hardware state (GPU memory, temperature, utilization)\n3. Document recent changes to configuration, data, or code\n4. Check system logs for hardware errors or resource limits\n5. Verify all dependencies and versions match known-working configurations\n\n**Phase 2: Minimal Reproduction (15-30 minutes)**\n1. Create smallest possible dataset that reproduces the issue (10-100 samples)\n2. Test with minimal configuration (smallest model, lowest rank, shortest sequences)\n3. Run base model without adapters to isolate LoRA-specific issues\n4. Test data loading and tokenization independently from training\n5. Verify issue persists across different random seeds\n\n**Phase 3: Component Isolation (20-40 minutes)**\n1. Test data preparation component independently with sample data\n2. Verify LoRA configuration produces expected parameter counts\n3. Test quantization setup with base model inference\n4. Run single training step to isolate training loop issues\n5. Test evaluation metrics with known-good checkpoints\n\n**Phase 4: Root Cause Analysis (Variable duration)**\n1. Compare configurations against known-working baselines\n2. Analyze data distribution differences between working and failing cases\n3. Profile memory usage and computation patterns\n4. Test incremental configuration changes to isolate problematic settings\n5. Validate assumptions about model architecture and data formats\n\n### Effective Logging Strategies\n\nLogging for LLM fine-tuning requires balancing information richness with performance impact. Unlike traditional applications where detailed logging has minimal overhead, fine-tuning can process thousands of samples per second where excessive logging becomes a bottleneck.\n\nThink of logging strategy like flight data recording - capture enough information to diagnose any failure, but don't let the recording system interfere with the flight. The key is logging the right information at the right frequency to enable post-hoc analysis without impacting training performance.\n\n#### Structured Logging Schema\n\nAll logging should follow a structured format that enables automated analysis and alerting. The logging schema supports both human debugging and automated monitoring systems:\n\n| Log Level | Purpose | Frequency | Example Content |\n|-----------|---------|-----------|-----------------|\n| ERROR | System failures requiring immediate attention | Only on failures | \"CUDA out of memory during forward pass: allocated 15.2GB, requested 2.1GB additional\" |\n| WARNING | Concerning patterns that don't stop execution | Per epoch or significant events | \"Validation loss increased for 3 consecutive evaluations, early stopping in 2 more\" |\n| INFO | Major pipeline milestones and configuration | Component boundaries | \"Loaded model with 4-bit quantization: 6.7B params reduced to 3.8GB VRAM\" |\n| DEBUG | Detailed information for troubleshooting | Configurable intervals | \"Batch 150: loss=2.341, lr=3.2e-5, grad_norm=0.87, memory=12.3GB\" |\n\n#### Training Progress Logging\n\nTraining progress requires logging multiple interconnected metrics that reveal both immediate training health and longer-term trends:\n\n| Metric Category | Logging Frequency | Key Fields | Alerting Thresholds |\n|-----------------|-------------------|------------|-------------------|\n| Loss and convergence | Every training step | `step`, `epoch`, `loss`, `learning_rate`, `grad_norm` | Loss >10.0, grad_norm >5.0, NaN values |\n| Performance metrics | Every logging interval | `samples_per_second`, `tokens_per_second`, `gpu_utilization` | <50% target throughput, <70% GPU utilization |\n| Memory tracking | Every epoch | `gpu_memory_used`, `gpu_memory_cached`, `system_memory_used` | >95% GPU memory, continuous growth |\n| Model quality | Every evaluation | `eval_loss`, `perplexity`, `task_metrics` | Perplexity increase >20%, eval_loss divergence |\n\nThe `TrainingMetrics` structure captures training state at each step:\n\n```\nstep: 1247\nepoch: 2.34\nloss: 2.183\nlearning_rate: 2.8e-5\ngrad_norm: 0.92\ntimestamp: 1701234567.89\ngpu_memory_used: 11.2\nsamples_per_second: 3.4\n```\n\n#### Component-Specific Logging\n\nEach pipeline component requires specialized logging to capture component-specific issues and state:\n\n**Data Preparation Logging:**\n- Sample validation results and filtering statistics\n- Tokenization length distributions and truncation rates\n- Chat template application success/failure rates\n- Train-validation split characteristics and balance\n\n| Event | Log Level | Message Template | Fields |\n|-------|-----------|------------------|--------|\n| Dataset loaded | INFO | \"Loaded {total_samples} samples from {source_file}\" | `total_samples`, `source_file`, `format` |\n| Quality filtering | INFO | \"Filtered {filtered_count}/{total_samples}: {duplicate_pct}% duplicates, {length_pct}% length\" | `QualityFilterStats` fields |\n| Tokenization complete | INFO | \"Tokenized dataset: avg_length={avg_len}, max_length={max_len}, truncated={trunc_pct}%\" | Length statistics |\n| Template errors | WARNING | \"Chat template failed for {failed_count} samples: {error_summary}\" | Error counts and types |\n\n**LoRA Configuration Logging:**\n- Target module detection results and parameter counts\n- Rank and alpha configuration with efficiency metrics\n- Adapter injection success and parameter freezing verification\n\n| Event | Log Level | Message Template | Fields |\n|-------|-----------|------------------|--------|\n| Target detection | INFO | \"Detected {attention_count} attention, {mlp_count} MLP modules for adaptation\" | Module counts by type |\n| Adapter injection | INFO | \"Injected rank-{rank} adapters: {trainable_params}/{total_params} ({ratio:.2%}) trainable\" | `AdapterMetrics` summary |\n| Parameter analysis | DEBUG | \"Adapter efficiency: {memory_mb}MB overhead, {effective_rank:.1f} avg effective rank\" | Efficiency metrics |\n\n**Quantization Logging:**\n- Memory reduction achieved and quantization parameters\n- Compatibility verification and performance impact\n- Quantization quality assessment\n\n| Event | Log Level | Message Template | Fields |\n|-------|-----------|------------------|--------|\n| Quantization setup | INFO | \"Applied {quant_type} quantization: {original_gb}GB -> {quantized_gb}GB ({reduction:.1%} reduction)\" | Memory statistics |\n| Quality check | INFO | \"Quantization quality: {perplexity_increase:.2%} perplexity increase on validation set\" | Quality metrics |\n\n#### Error Context Logging\n\nWhen errors occur, comprehensive context logging enables effective debugging. Error logs should capture not just the immediate failure, but the system state leading to the failure:\n\n| Error Category | Context Information | Example Fields |\n|----------------|-------------------|----------------|\n| Memory errors | GPU/system memory state, recent allocations, model size | `available_memory`, `requested_memory`, `largest_free_block` |\n| Training instability | Recent loss values, gradient norms, learning rate | `loss_history`, `grad_norm_history`, `lr_schedule_position` |\n| Data errors | Sample information, tokenization state, validation results | `sample_id`, `sequence_length`, `validation_errors` |\n| Configuration errors | Full configuration dump, compatibility checks | `full_config`, `version_info`, `hardware_capabilities` |\n\n**Error Log Format Example:**\n```\nERROR [2024-01-15 14:32:17] Training step failed\nContext:\n  step: 1247\n  epoch: 2.34\n  recent_losses: [2.18, 2.21, 2.19, 2.17]\n  grad_norm: 15.7\n  gpu_memory: 15.2GB / 16.0GB\n  last_lr: 2.8e-5\n  sample_id: \"conversation_4821\"\n  sequence_length: 2048\nError: CUDA out of memory during backward pass\nStack trace: [full stack trace]\nRecovery action: Reducing batch size and restarting from last checkpoint\n```\n\n#### Performance and Bottleneck Logging\n\nPerformance logging helps identify bottlenecks and optimization opportunities. This logging focuses on throughput, resource utilization, and timing analysis:\n\n| Performance Area | Metrics | Logging Frequency | Analysis Purpose |\n|------------------|---------|-------------------|------------------|\n| Data loading | Samples/second, queue utilization, preprocessing time | Every 100 steps | Identify data pipeline bottlenecks |\n| Model computation | Tokens/second, GPU utilization, kernel timing | Every logging interval | Optimize computational efficiency |\n| Memory management | Allocation patterns, fragmentation, garbage collection | Every epoch | Prevent memory issues |\n| I/O operations | Checkpoint save time, log write latency | Every significant I/O | Optimize storage performance |\n\n#### Logging Configuration and Management\n\nLogging configuration should be environment-aware and performance-conscious. Development environments need detailed debugging information, while production training runs prioritize performance with selective detailed logging:\n\n| Environment | Log Level | File Output | Structured Format | Performance Impact |\n|-------------|-----------|-------------|------------------|-------------------|\n| Development | DEBUG | Local files + console | JSON for analysis tools | High detail, moderate performance cost |\n| Training | INFO | Remote logging + local backup | Structured with metadata | Balanced detail and performance |\n| Production | WARNING | Centralized logging system | Compact binary format | Minimal performance impact |\n\n**Adaptive Logging Strategy:**\n- Start with detailed logging during initial epochs to catch early issues\n- Reduce logging frequency after stable training is established\n- Increase logging detail when issues are detected (circuit breaker pattern)\n- Implement log sampling for high-frequency events to control volume\n\n> **Logging Performance Principle: Log Smart, Not Hard**\n>\n> The cost of logging a single training step can equal the cost of processing several samples. Design logging to provide maximum debugging value with minimal performance impact by using sampling, buffering, and adaptive detail levels.\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Memory Profiling | `torch.cuda.memory_stats()` + manual tracking | `torch.profiler` with Chrome trace export |\n| Training Visualization | WandB free tier with basic charts | Custom TensorBoard + Grafana dashboard |\n| Log Management | Python `logging` module to local files | ELK stack (Elasticsearch + Logstash + Kibana) |\n| Performance Monitoring | `nvidia-smi` + manual observation | Prometheus + GPU exporters + alerting |\n| Error Tracking | Print statements + manual analysis | Sentry for error aggregation and alerting |\n\n#### Recommended File Structure\n\n```\nfine_tuning_pipeline/\n├── debugging/\n│   ├── __init__.py\n│   ├── memory_monitor.py          # MemoryMonitor and GPU tracking\n│   ├── training_monitor.py        # TrainingStabilityMonitor and loss tracking\n│   ├── performance_profiler.py    # Performance analysis and bottleneck detection\n│   ├── error_handler.py           # Structured error logging and recovery\n│   └── diagnostic_tools.py        # Debugging utilities and analysis functions\n├── logging_config/\n│   ├── development.yaml           # Detailed logging for development\n│   ├── training.yaml              # Balanced logging for training runs\n│   └── production.yaml            # Minimal logging for production\n├── monitoring/\n│   ├── dashboards/                # Grafana/TensorBoard dashboard configs\n│   ├── alerts/                    # Alerting rules and thresholds\n│   └── metrics_collectors.py      # Custom metrics collection\n└── tools/\n    ├── log_analyzer.py            # Automated log analysis and issue detection\n    ├── memory_analyzer.py         # Memory usage analysis and optimization\n    └── performance_benchmark.py   # Performance baseline establishment\n```\n\n#### Core Debugging Infrastructure\n\n**Memory Monitoring System (Complete Implementation):**\n\n```python\nimport torch\nimport psutil\nimport time\nfrom typing import Dict, List, Optional, Any\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\n\n@dataclass\nclass MemoryMetrics:\n    stage: str\n    gpu_memory_allocated: float\n    gpu_memory_cached: float\n    gpu_memory_reserved: float\n    system_memory_used: float\n    timestamp: float\n    details: Optional[Dict[str, Any]] = None\n\nclass MemoryMonitor:\n    def __init__(self):\n        self.baseline_gpu_memory: Optional[int] = None\n        self.baseline_system_memory: int = 0\n        self.measurements: List[Dict] = []\n        self.peak_gpu_usage: float = 0.0\n        self.peak_system_usage: float = 0.0\n        \n    def capture_baseline(self) -> None:\n        \"\"\"Record initial memory state before model loading.\"\"\"\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            self.baseline_gpu_memory = torch.cuda.memory_allocated()\n        else:\n            self.baseline_gpu_memory = 0\n        \n        self.baseline_system_memory = psutil.virtual_memory().used\n        self.measurements.clear()\n        \n    def measure_current_usage(self, stage: str) -> Dict[str, float]:\n        \"\"\"Measure current memory and return usage statistics.\"\"\"\n        if torch.cuda.is_available():\n            gpu_allocated = torch.cuda.memory_allocated()\n            gpu_cached = torch.cuda.memory_cached()\n            gpu_reserved = torch.cuda.memory_reserved()\n        else:\n            gpu_allocated = gpu_cached = gpu_reserved = 0.0\n        \n        system_memory = psutil.virtual_memory().used\n        \n        # Convert to MB for readability\n        gpu_allocated_mb = gpu_allocated / (1024 ** 2)\n        gpu_cached_mb = gpu_cached / (1024 ** 2)\n        gpu_reserved_mb = gpu_reserved / (1024 ** 2)\n        system_memory_mb = system_memory / (1024 ** 2)\n        \n        metrics = MemoryMetrics(\n            stage=stage,\n            gpu_memory_allocated=gpu_allocated_mb,\n            gpu_memory_cached=gpu_cached_mb,\n            gpu_memory_reserved=gpu_reserved_mb,\n            system_memory_used=system_memory_mb,\n            timestamp=time.time(),\n            details={\n                \"baseline_gpu_delta\": gpu_allocated_mb - (self.baseline_gpu_memory or 0) / (1024 ** 2),\n                \"baseline_system_delta\": system_memory_mb - self.baseline_system_memory / (1024 ** 2),\n                \"gpu_utilization_pct\": gpu_allocated / gpu_reserved if gpu_reserved > 0 else 0.0\n            }\n        )\n        \n        self.measurements.append(metrics.__dict__)\n        self.peak_gpu_usage = max(self.peak_gpu_usage, gpu_allocated_mb)\n        self.peak_system_usage = max(self.peak_system_usage, system_memory_mb)\n        \n        return {\n            \"stage\": stage,\n            \"gpu_allocated_mb\": gpu_allocated_mb,\n            \"gpu_cached_mb\": gpu_cached_mb,\n            \"system_memory_mb\": system_memory_mb,\n            \"baseline_delta_mb\": metrics.details[\"baseline_gpu_delta\"]\n        }\n        \n    def get_peak_usage(self) -> Dict[str, float]:\n        \"\"\"Return peak memory usage across all measurements.\"\"\"\n        return {\n            \"peak_gpu_mb\": self.peak_gpu_usage,\n            \"peak_system_mb\": self.peak_system_usage,\n            \"baseline_gpu_mb\": (self.baseline_gpu_memory or 0) / (1024 ** 2),\n            \"baseline_system_mb\": self.baseline_system_memory / (1024 ** 2),\n            \"total_measurements\": len(self.measurements)\n        }\n```\n\n**Training Stability Monitor (Complete Implementation):**\n\n```python\nfrom collections import deque\nimport torch\nimport math\nfrom typing import Dict, Any, Optional, Deque\n\nclass TrainingStabilityMonitor:\n    def __init__(self, gradient_clip_norm: float = 1.0, loss_spike_threshold: float = 2.0):\n        self.gradient_clip_norm = gradient_clip_norm\n        self.loss_spike_threshold = loss_spike_threshold\n        self.gradient_history: Deque[float] = deque(maxlen=100)\n        self.loss_history: Deque[float] = deque(maxlen=50)\n        self.instability_count = 0\n        \n    def check_gradient_stability(self, model) -> Dict[str, Any]:\n        \"\"\"Monitor gradient norms and detect explosions.\"\"\"\n        total_norm = 0.0\n        param_count = 0\n        \n        for param in model.parameters():\n            if param.grad is not None:\n                param_norm = param.grad.data.norm(2)\n                total_norm += param_norm.item() ** 2\n                param_count += 1\n                \n        if param_count == 0:\n            return {\"status\": \"no_gradients\", \"total_norm\": 0.0}\n            \n        total_norm = total_norm ** (1. / 2)\n        self.gradient_history.append(total_norm)\n        \n        # Calculate recent trend\n        recent_grads = list(self.gradient_history)[-10:]\n        avg_recent = sum(recent_grads) / len(recent_grads) if recent_grads else 0.0\n        \n        # Detection logic\n        is_explosion = total_norm > 10.0 or total_norm > avg_recent * 5.0\n        is_vanishing = total_norm < 1e-6\n        is_unstable = len(recent_grads) >= 5 and max(recent_grads) > min(recent_grads) * 10.0\n        \n        if is_explosion:\n            self.instability_count += 1\n            \n        return {\n            \"status\": \"explosion\" if is_explosion else \"vanishing\" if is_vanishing else \"unstable\" if is_unstable else \"stable\",\n            \"total_norm\": total_norm,\n            \"avg_recent_norm\": avg_recent,\n            \"instability_count\": self.instability_count,\n            \"should_clip\": total_norm > self.gradient_clip_norm,\n            \"clip_factor\": min(1.0, self.gradient_clip_norm / total_norm) if total_norm > self.gradient_clip_norm else 1.0\n        }\n        \n    def handle_gradient_explosion(self, model, optimizer) -> bool:\n        \"\"\"Handle detected gradient explosion with graduated intervention.\"\"\"\n        stability = self.check_gradient_stability(model)\n        \n        if stability[\"status\"] == \"explosion\":\n            # Graduated response based on severity\n            if stability[\"total_norm\"] > 100.0:\n                # Severe explosion - zero gradients and reduce learning rate\n                for param in model.parameters():\n                    if param.grad is not None:\n                        param.grad.zero_()\n                        \n                # Reduce learning rate by half\n                for param_group in optimizer.param_groups:\n                    param_group['lr'] *= 0.5\n                    \n                return True  # Indicate intervention taken\n                \n            elif stability[\"total_norm\"] > 10.0:\n                # Moderate explosion - clip gradients aggressively\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                return True\n                \n        return False  # No intervention needed\n        \n    def check_loss_stability(self, current_loss: float) -> Dict[str, Any]:\n        \"\"\"Monitor loss values for spikes and degradation.\"\"\"\n        if math.isnan(current_loss) or math.isinf(current_loss):\n            return {\n                \"status\": \"invalid\",\n                \"loss\": current_loss,\n                \"requires_intervention\": True\n            }\n            \n        self.loss_history.append(current_loss)\n        \n        if len(self.loss_history) < 5:\n            return {\"status\": \"insufficient_data\", \"loss\": current_loss}\n            \n        recent_losses = list(self.loss_history)[-10:]\n        avg_recent = sum(recent_losses) / len(recent_losses)\n        \n        # Check for sudden spikes\n        is_spike = current_loss > avg_recent * self.loss_spike_threshold\n        \n        # Check for consistent increase (potential divergence)\n        if len(recent_losses) >= 5:\n            increasing_trend = all(recent_losses[i] <= recent_losses[i+1] for i in range(-5, -1))\n        else:\n            increasing_trend = False\n            \n        return {\n            \"status\": \"spike\" if is_spike else \"diverging\" if increasing_trend else \"stable\",\n            \"loss\": current_loss,\n            \"avg_recent\": avg_recent,\n            \"requires_intervention\": is_spike or increasing_trend,\n            \"spike_factor\": current_loss / avg_recent if avg_recent > 0 else 1.0\n        }\n        \n    def detect_nan_propagation(self, model, loss) -> bool:\n        \"\"\"Detect NaN values in model parameters, gradients, or loss.\"\"\"\n        # Check loss\n        if math.isnan(loss) or math.isinf(loss):\n            return True\n            \n        # Check model parameters and gradients\n        for param in model.parameters():\n            if torch.isnan(param).any() or torch.isinf(param).any():\n                return True\n            if param.grad is not None and (torch.isnan(param.grad).any() or torch.isinf(param.grad).any()):\n                return True\n                \n        return False\n        \n    def should_restore_checkpoint(self) -> bool:\n        \"\"\"Determine if training instability requires checkpoint restoration.\"\"\"\n        return self.instability_count >= 3 or len([l for l in self.loss_history if math.isnan(l) or math.isinf(l)]) > 0\n```\n\n#### Debugging Tool Skeletons\n\n**Performance Profiler (Implementation Structure):**\n\n```python\nimport time\nimport torch.profiler\nfrom contextlib import contextmanager\nfrom typing import Dict, Any, Optional\n\nclass PerformanceProfiler:\n    def __init__(self, output_dir: str = \"./profiles\"):\n        self.output_dir = output_dir\n        self.current_profile: Optional[torch.profiler.profile] = None\n        self.timing_stack = []\n        \n    @contextmanager\n    def profile_training_step(self, step: int):\n        \"\"\"Profile a complete training step with detailed breakdown.\"\"\"\n        # TODO 1: Initialize torch.profiler with GPU and CPU tracking enabled\n        # TODO 2: Start profiling context manager\n        # TODO 3: Yield control to training step execution\n        # TODO 4: Stop profiling and export trace to Chrome format\n        # TODO 5: Extract key timing metrics and return summary\n        # Hint: Use torch.profiler.profile(activities=[CPU, CUDA], record_shapes=True)\n        pass\n        \n    @contextmanager  \n    def time_operation(self, operation_name: str):\n        \"\"\"Time a specific operation and track in performance metrics.\"\"\"\n        # TODO 1: Record start time and push operation onto timing stack\n        # TODO 2: Yield control to operation execution\n        # TODO 3: Calculate elapsed time and pop operation from stack\n        # TODO 4: Store timing data with operation context\n        # TODO 5: Check for performance regressions against baselines\n        pass\n        \n    def analyze_bottlenecks(self, profile_data: Dict) -> Dict[str, Any]:\n        \"\"\"Analyze profiling data to identify performance bottlenecks.\"\"\"\n        # TODO 1: Extract GPU kernel execution times from profile\n        # TODO 2: Identify top time-consuming operations\n        # TODO 3: Calculate GPU utilization and memory bandwidth\n        # TODO 4: Detect common bottleneck patterns (data loading, computation, memory)\n        # TODO 5: Generate recommendations for performance optimization\n        pass\n```\n\n**Error Analysis System (Implementation Structure):**\n\n```python\nimport traceback\nimport sys\nfrom typing import Dict, Any, List, Optional\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nclass ErrorCategory(Enum):\n    MEMORY_ERROR = \"memory\"\n    TRAINING_INSTABILITY = \"training\"\n    DATA_ERROR = \"data\"\n    CONFIGURATION_ERROR = \"config\"\n    HARDWARE_ERROR = \"hardware\"\n\n@dataclass\nclass ErrorContext:\n    category: ErrorCategory\n    message: str\n    stack_trace: str\n    system_state: Dict[str, Any]\n    recovery_actions: List[str]\n    timestamp: float\n\nclass ErrorAnalyzer:\n    def __init__(self):\n        self.error_patterns = self._load_error_patterns()\n        self.recovery_strategies = self._load_recovery_strategies()\n        \n    def analyze_exception(self, exception: Exception, context: Dict[str, Any]) -> ErrorContext:\n        \"\"\"Analyze exception and provide structured error context.\"\"\"\n        # TODO 1: Classify exception type into error categories\n        # TODO 2: Extract relevant system state (memory, GPU, training metrics)\n        # TODO 3: Match error pattern against known issues database\n        # TODO 4: Generate specific recovery recommendations\n        # TODO 5: Create structured error context for logging\n        pass\n        \n    def suggest_recovery_actions(self, error_category: ErrorCategory, context: Dict) -> List[str]:\n        \"\"\"Suggest specific recovery actions based on error analysis.\"\"\"\n        # TODO 1: Look up category-specific recovery strategies\n        # TODO 2: Filter strategies based on current system state\n        # TODO 3: Prioritize actions by likelihood of success\n        # TODO 4: Include specific parameter adjustments or config changes\n        # TODO 5: Return ordered list of recovery actions to attempt\n        pass\n        \n    def _load_error_patterns(self) -> Dict:\n        \"\"\"Load database of known error patterns and classifications.\"\"\"\n        return {\n            \"cuda.*out of memory\": ErrorCategory.MEMORY_ERROR,\n            \"gradient.*explosion\": ErrorCategory.TRAINING_INSTABILITY,\n            \"tokenizer.*encode.*failed\": ErrorCategory.DATA_ERROR,\n            # TODO: Expand with comprehensive error pattern database\n        }\n```\n\n#### Milestone Validation Checkpoints\n\n**After Dataset Preparation (Milestone 1):**\n- Run `python -m debugging.memory_monitor` to establish baseline memory usage\n- Verify data loading completes without memory growth over multiple epochs\n- Check tokenization produces expected sequence lengths and no truncation warnings\n- Confirm chat template application succeeds on 100% of samples\n\n**After LoRA Configuration (Milestone 2):**\n- Validate adapter injection with `python -m debugging.diagnostic_tools --check-adapters`\n- Confirm trainable parameter ratio <1% with `ParameterAnalyzer.analyze_parameter_distribution()`\n- Test gradient flow through adapters with dummy backward pass\n- Verify base model parameters remain frozen during adapter training\n\n**After Training Implementation (Milestone 4):**\n- Monitor training stability for first 100 steps with `TrainingStabilityMonitor`\n- Confirm loss decreases smoothly without spikes or plateaus\n- Validate checkpoint saving/loading maintains training state consistency\n- Test early stopping triggers correctly on validation loss plateau\n\n**After Complete Pipeline (Milestone 5):**\n- Run end-to-end memory profiling to identify peak usage and potential leaks\n- Validate exported model produces identical outputs to merged checkpoint\n- Confirm performance benchmarks meet throughput requirements\n- Test recovery from various failure modes (OOM, gradient explosion, corruption)\n\n\n## Future Extensions\n\n> **Milestone(s):** All milestones - this section outlines the evolution path for the fine-tuning pipeline, describing scalability enhancements, alternative parameter-efficient techniques, and advanced evaluation methods that build upon the foundational system\n\nThe current fine-tuning pipeline provides a solid foundation for parameter-efficient model adaptation, but several exciting extensions can enhance its capabilities, scalability, and evaluation depth. These future enhancements fall into three primary categories: scalability improvements that enable training larger models on multiple GPUs, alternative adapter methods that offer different trade-offs between efficiency and performance, and advanced evaluation techniques that provide deeper insights into model capabilities and safety.\n\n### Mental Model: The Workshop Evolution\n\nThink of the current fine-tuning pipeline as a skilled craftsperson's workshop equipped with essential tools for creating custom adaptations. The future extensions represent the natural evolution of this workshop: expanding to multiple workstations for larger projects (multi-GPU training), acquiring specialized tools for different crafting techniques (alternative adapter methods), and developing sophisticated quality assessment methods (advanced evaluation). Just as a master craftsperson continuously expands their capabilities while maintaining their core expertise, these extensions build upon the existing pipeline's strengths while opening new possibilities for scale and precision.\n\nThe extensions maintain backward compatibility with the existing system while providing optional enhanced capabilities. Users can choose to adopt specific extensions based on their requirements, whether that's scaling to larger models, experimenting with cutting-edge adapter techniques, or implementing comprehensive safety evaluations.\n\n### Scalability Extensions\n\nMulti-GPU training and distributed fine-tuning represent the most significant scalability enhancements for the pipeline. These extensions enable training larger models that cannot fit on a single GPU and dramatically reduce training time through parallelization. The scalability extensions maintain the parameter-efficiency principles while distributing computation across multiple devices.\n\n#### Distributed Training Architecture\n\nThe distributed training extension adds a coordination layer above the existing training components. This coordination layer manages multiple GPU workers, each running a copy of the training loop with synchronized gradient updates. The architecture supports both data parallelism, where different workers process different batches of the same dataset, and model parallelism, where different parts of the model reside on different GPUs.\n\n**Data parallelism** represents the more straightforward extension, where each GPU worker loads the full model and processes different subsets of training data. Workers periodically synchronize their gradients using an all-reduce operation that combines gradients across all workers before applying optimizer updates. This approach scales well for models that fit on individual GPUs but benefit from larger effective batch sizes.\n\n**Model parallelism** becomes necessary when individual model layers exceed single GPU memory capacity. The pipeline extension partitions the model across multiple GPUs, with different layers residing on different devices. Activations flow between GPUs as they pass through the network, requiring careful scheduling to minimize communication overhead and maintain high GPU utilization.\n\n| Parallelism Strategy | Memory Scaling | Communication Overhead | Implementation Complexity | Best Use Case |\n|---------------------|----------------|------------------------|-------------------------|---------------|\n| Data Parallelism | Linear per worker | Gradient sync only | Low | Models that fit on single GPU |\n| Pipeline Parallelism | Linear with layers | Activation passing | Medium | Very large models with sequential structure |\n| Tensor Parallelism | Linear with model width | High-frequency sync | High | Massive transformer layers |\n| Hybrid Approaches | Best of multiple | Balanced | Very High | Largest possible models |\n\n> **Decision: Gradient Synchronization Strategy**\n> - **Context**: Distributed training requires coordinating gradient updates across multiple workers to maintain training convergence\n> - **Options Considered**: Synchronous all-reduce, asynchronous parameter servers, elastic averaging\n> - **Decision**: Implement synchronous all-reduce with gradient compression\n> - **Rationale**: Synchronous training provides deterministic convergence behavior critical for reproducible fine-tuning results, while gradient compression reduces communication bandwidth requirements\n> - **Consequences**: Enables linear scaling with worker count while maintaining training stability, but requires workers to synchronize at each step\n\n#### Memory-Efficient Distributed Quantization\n\nCombining QLoRA quantization with distributed training requires careful coordination to maintain memory efficiency gains. The extension implements **gradient synchronization in higher precision** while maintaining 4-bit model weights on each worker. This approach prevents quantization errors from accumulating across gradient synchronization rounds while preserving the memory benefits of quantized storage.\n\nThe distributed quantization system also supports **heterogeneous GPU configurations** where workers have different memory capacities. Workers with larger memory can process larger micro-batches or maintain larger gradient accumulation buffers, while the coordination layer balances workload distribution to prevent bottlenecks from slower workers.\n\n**Quantization-aware load balancing** distributes model components across workers based on their quantized memory footprint rather than parameter count. Since different layer types compress differently under 4-bit quantization, this dynamic allocation ensures more even memory utilization across workers.\n\n#### Multi-GPU LoRA Coordination\n\nDistributing LoRA adapters across multiple GPUs requires sophisticated coordination to maintain the low-rank structure while enabling parallel computation. The extension implements **adapter sharding strategies** that split individual LoRA matrices across workers or distribute entire adapter modules to different GPUs based on the target module layout.\n\n**Cross-GPU adapter communication** becomes critical when adapter matrices span multiple devices. The system implements efficient communication patterns that minimize adapter weight transfers during forward and backward passes. For adapters that require cross-GPU computation, the system batches communication operations to reduce latency overhead.\n\nThe distributed LoRA system also supports **dynamic adapter scaling** where the effective rank can be increased by adding more workers, each contributing additional low-rank components to the adaptation. This approach enables scaling adapter capacity with available hardware resources.\n\n### Alternative Adapter Methods\n\nWhile LoRA provides an excellent balance of efficiency and performance, several emerging adapter techniques offer different trade-offs that may be beneficial for specific use cases. These alternative methods can be integrated into the existing pipeline architecture as additional options alongside LoRA.\n\n#### AdaLoRA: Adaptive Low-Rank Adaptation\n\n**AdaLoRA** extends basic LoRA by dynamically adjusting the rank allocation across different modules during training. Instead of using a fixed rank for all adapter modules, AdaLoRA starts with a higher initial rank and progressively prunes less important singular value directions based on gradient information and importance scores.\n\nThe adaptive rank allocation algorithm monitors the **singular value importance** of each adapter matrix throughout training. Modules that show high gradient activity in their singular value directions retain higher ranks, while modules with low activity have their ranks reduced through progressive pruning. This dynamic allocation results in more efficient parameter utilization compared to fixed-rank LoRA.\n\n**Rank budget management** becomes a key component of the AdaLoRA extension. Users specify a total parameter budget rather than per-module ranks, and the system automatically allocates this budget across modules based on their learning dynamics. This approach often achieves better performance than LoRA with the same total parameter count.\n\n| Adapter Method | Rank Strategy | Parameter Efficiency | Training Complexity | Best Use Cases |\n|----------------|---------------|---------------------|-------------------|----------------|\n| LoRA | Fixed rank per module | High with proper tuning | Low | General fine-tuning, proven baselines |\n| AdaLoRA | Dynamic rank allocation | Higher than LoRA | Medium | Tasks requiring varied module adaptation |\n| DoRA | Weight decomposition | Competitive with LoRA | Medium | Tasks needing magnitude and direction control |\n| QLoRA | Fixed rank + quantization | Highest memory efficiency | Low | Memory-constrained environments |\n| LoRA+ | Different learning rates | Similar to LoRA | Low | Fine-grained optimization control |\n\n#### DoRA: Weight-Decomposed Low-Rank Adaptation\n\n**DoRA (Weight-Decomposed Low-Rank Adaptation)** decomposes weight updates into magnitude and directional components, applying low-rank adaptation specifically to the directional component while allowing full-rank updates to the magnitude. This decomposition often provides better performance than standard LoRA because magnitude and direction updates have different optimization dynamics.\n\nThe DoRA extension modifies the adapter injection process to **separate magnitude and direction updates**. For each target module weight matrix W, DoRA decomposes updates into magnitude scaling factors and directional changes. The low-rank adaptation applies only to the directional component, while magnitude updates use learned scaling parameters.\n\n**Magnitude-direction optimization** requires different learning rate schedules for the two components. The magnitude parameters typically benefit from lower learning rates since they control the overall scale of activations, while directional parameters can use higher learning rates since they operate in the normalized space.\n\n#### LoRA+ and Multi-Learning-Rate Extensions\n\n**LoRA+** introduces different learning rates for the A and B matrices in LoRA decomposition, based on the observation that these matrices have different roles in the adaptation process. The A matrix (which receives the input) and B matrix (which produces the output) benefit from different optimization dynamics.\n\nThe multi-learning-rate extension allows fine-grained control over adaptation speed across different components. **Matrix-specific learning rates** can be tuned based on the singular value spectrum of the target modules, with matrices corresponding to larger singular values receiving lower learning rates to prevent over-adaptation.\n\n**Layer-wise learning rate adaptation** extends this concept to automatically adjust learning rates based on the depth of the target modules. Deeper layers, which often require more delicate adaptation to avoid catastrophic forgetting, receive progressively lower learning rates.\n\n> **Decision: Adapter Method Plugin Architecture**\n> - **Context**: Supporting multiple adapter methods requires extensible architecture without complicating the core pipeline\n> - **Options Considered**: Monolithic implementation with all methods, plugin-based architecture, separate pipelines per method\n> - **Decision**: Implement plugin-based adapter architecture with common interfaces\n> - **Rationale**: Plugin architecture enables easy addition of new methods while maintaining code clarity and allowing users to include only needed adapters\n> - **Consequences**: Enables rapid experimentation with new techniques while keeping the core pipeline focused, but requires careful interface design for compatibility\n\n### Advanced Evaluation\n\nStandard perplexity and task-specific metrics provide important baseline assessments, but advanced evaluation techniques offer deeper insights into model capabilities, safety characteristics, and real-world performance. These advanced evaluation methods help practitioners understand not just whether fine-tuning improved performance, but how and why the improvements occurred.\n\n#### Human Evaluation Integration\n\n**Human evaluation systems** provide ground-truth assessment of model outputs that automated metrics cannot capture. The advanced evaluation extension integrates with human evaluation platforms to collect structured feedback on model responses across multiple dimensions including accuracy, helpfulness, harmfulness, and coherence.\n\nThe human evaluation component implements **comparative assessment protocols** where human raters compare outputs from the fine-tuned model against baseline models and reference responses. This comparative approach reduces absolute rating bias and provides more reliable relative performance measurements.\n\n**Inter-rater reliability tracking** ensures evaluation quality by monitoring agreement between human evaluators. The system implements multiple rater assignment for critical evaluations and provides calibration exercises to maintain consistent evaluation standards across different raters.\n\n| Evaluation Method | Cost | Scalability | Reliability | Coverage | Best Use Case |\n|-------------------|------|-------------|-------------|----------|---------------|\n| Human Evaluation | High | Low | High with calibration | Deep but narrow | Critical safety assessment |\n| Automated Benchmarks | Low | High | Medium | Broad but shallow | Rapid iteration feedback |\n| Model-Based Evaluation | Medium | High | Variable | Moderate | Scalable quality assessment |\n| Adversarial Testing | Medium | Medium | High | Targeted | Safety and robustness validation |\n| Real-World Deployment | Variable | Variable | Highest | Complete | Final validation |\n\n#### Safety and Alignment Benchmarks\n\n**Safety evaluation suites** assess model behavior across multiple risk dimensions including harmful content generation, bias amplification, and prompt injection vulnerabilities. The safety evaluation component integrates established benchmarks like TruthfulQA for truthfulness assessment and custom evaluation sets for domain-specific safety concerns.\n\n**Alignment measurement** evaluates how well the fine-tuned model follows intended behaviors and avoids unintended behaviors. This includes measuring instruction following accuracy, refusal behavior for inappropriate requests, and consistency of responses across semantically similar prompts.\n\nThe safety evaluation system implements **adversarial testing protocols** that attempt to elicit problematic behaviors through sophisticated prompting techniques. These tests help identify failure modes that might not appear in standard evaluation but could emerge in real-world deployment scenarios.\n\n#### Multi-Dimensional Performance Analysis\n\n**Capability decomposition** breaks down overall model performance into specific skill areas such as reasoning, knowledge recall, language understanding, and generation quality. This decomposition helps practitioners understand which aspects of model performance improved through fine-tuning and which may have degraded.\n\n**Performance stability analysis** measures how consistently the model performs across different prompting strategies, input formats, and context lengths. Models that show high variance in performance may require additional training or different adapter configurations.\n\nThe multi-dimensional analysis includes **forgetting assessment** that measures how fine-tuning affected performance on capabilities not directly targeted by the training data. This assessment helps identify catastrophic forgetting and guides decisions about continual learning strategies.\n\n#### Model-Based Evaluation Systems\n\n**LLM-as-judge evaluation** uses larger, more capable models to assess the quality of fine-tuned model outputs. This approach scales better than human evaluation while providing more nuanced assessment than simple automated metrics. The system implements multiple judge models to reduce bias from any single evaluator.\n\n**Constitutional AI evaluation** applies structured principles to assess whether model outputs align with specified behavioral guidelines. This evaluation method can be customized for different use cases and provides interpretable scores across multiple alignment dimensions.\n\nThe model-based evaluation system includes **chain-of-thought evaluation** where judge models provide detailed reasoning for their assessments. This reasoning helps practitioners understand specific strengths and weaknesses in model outputs and guides targeted improvements.\n\n#### Specialized Domain Evaluations\n\nFor domain-specific fine-tuning, the advanced evaluation system supports **custom benchmark integration** that allows practitioners to evaluate models on proprietary or specialized datasets relevant to their specific use cases. The system provides tools for creating evaluation datasets, defining custom metrics, and comparing performance against domain-specific baselines.\n\n**Expert evaluation protocols** facilitate assessment by subject matter experts in specialized domains. The system provides interfaces for expert evaluators to assess technical accuracy, domain appropriateness, and practical utility of model outputs in their specific fields.\n\n> **Decision: Evaluation Framework Design**\n> - **Context**: Advanced evaluation requires balancing comprehensiveness with practicality while maintaining reproducible results\n> - **Options Considered**: Monolithic evaluation suite, modular evaluation components, third-party integration only\n> - **Decision**: Implement modular evaluation framework with standardized interfaces and optional third-party integrations\n> - **Rationale**: Modular design enables users to select relevant evaluation methods while maintaining result comparability across different evaluation components\n> - **Consequences**: Provides flexibility for different evaluation needs while ensuring consistent methodology, but requires careful interface design for cross-method compatibility\n\n### Common Pitfalls in Extensions\n\n⚠️ **Pitfall: Distributed Training Without Proper Synchronization**\nImplementing distributed training without careful attention to gradient synchronization can lead to divergent training dynamics where different workers learn contradictory adaptations. This occurs when gradient updates are applied inconsistently across workers or when communication delays cause workers to update based on stale gradient information. To avoid this, implement synchronous gradient aggregation with verification that all workers receive identical gradient updates before proceeding to the next training step.\n\n⚠️ **Pitfall: Memory Optimization Assumptions in Multi-GPU Setup**\nAssuming that memory optimizations scale linearly across multiple GPUs often leads to unexpected out-of-memory errors. Memory usage patterns differ between single-GPU and multi-GPU training due to additional communication buffers, gradient synchronization overhead, and NCCL library memory requirements. Always benchmark actual memory usage in the target multi-GPU configuration rather than extrapolating from single-GPU measurements.\n\n⚠️ **Pitfall: Alternative Adapter Method Hyperparameter Transfer**\nDirectly transferring hyperparameters optimized for LoRA to alternative adapter methods like AdaLoRA or DoRA typically yields suboptimal results because different methods have different parameter sensitivity profiles. Each adapter method requires its own hyperparameter optimization process. Budget additional time for hyperparameter tuning when experimenting with alternative adapter techniques.\n\n⚠️ **Pitfall: Evaluation Method Selection Bias**\nChoosing evaluation methods that favor the specific type of fine-tuning performed can create misleading performance assessments. For example, evaluating instruction-tuned models only on instruction-following tasks while ignoring general language modeling capabilities can mask degradation in other areas. Implement comprehensive evaluation suites that assess multiple capability dimensions, including areas not directly targeted by fine-tuning.\n\n⚠️ **Pitfall: Human Evaluation Without Proper Calibration**\nConducting human evaluations without proper rater calibration and inter-rater reliability checks can produce inconsistent and unreliable results. Human evaluators often have different standards and may drift in their evaluation criteria over time. Implement regular calibration exercises, measure inter-rater agreement, and provide clear evaluation guidelines with concrete examples to maintain evaluation quality.\n\n### Implementation Guidance\n\nThe future extensions build upon the existing pipeline architecture through well-defined extension points and plugin interfaces. This implementation approach ensures backward compatibility while enabling selective adoption of advanced capabilities based on specific requirements.\n\n#### Technology Recommendations\n\n| Extension Category | Simple Option | Advanced Option |\n|-------------------|---------------|-----------------|\n| Distributed Training | PyTorch DistributedDataParallel + NCCL | DeepSpeed ZeRO + Gradient Compression |\n| Alternative Adapters | Custom LoRA variants | PEFT library extension framework |\n| Advanced Evaluation | HuggingFace Evaluate + custom metrics | Comprehensive evaluation frameworks (HELM, Eleuther) |\n| Human Evaluation | Simple rating interface + CSV export | Integration with Scale AI or similar platforms |\n| Safety Benchmarks | Basic bias detection + manual review | Comprehensive safety suites (HELM Safety, TruthfulQA) |\n\n#### Recommended Extension Architecture\n\n```\nextensions/\n├── distributed/\n│   ├── __init__.py\n│   ├── multi_gpu_trainer.py      # DistributedDataParallel integration\n│   ├── gradient_sync.py          # Synchronization protocols\n│   └── memory_coordinator.py     # Cross-GPU memory management\n├── adapters/\n│   ├── __init__.py\n│   ├── ada_lora.py               # AdaLoRA implementation\n│   ├── dora.py                   # DoRA implementation\n│   ├── lora_plus.py              # LoRA+ implementation\n│   └── adapter_factory.py        # Plugin registration system\n├── evaluation/\n│   ├── __init__.py\n│   ├── human_eval.py             # Human evaluation coordination\n│   ├── safety_benchmarks.py     # Safety evaluation suite\n│   ├── model_judge.py            # LLM-as-judge evaluation\n│   └── domain_specific.py        # Custom domain evaluations\n└── interfaces/\n    ├── __init__.py\n    ├── adapter_interface.py      # Common adapter API\n    ├── evaluator_interface.py    # Common evaluation API\n    └── trainer_interface.py      # Extended trainer API\n```\n\n#### Distributed Training Infrastructure\n\n```python\n\"\"\"\nMulti-GPU training extension that maintains compatibility with existing pipeline.\nSupports both data parallelism and pipeline parallelism strategies.\n\"\"\"\n\nimport torch\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel\nfrom typing import Dict, List, Optional, Any\nimport logging\n\nclass DistributedTrainingConfig:\n    \"\"\"Configuration for distributed training setup.\"\"\"\n    \n    def __init__(self, \n                 world_size: int,\n                 local_rank: int,\n                 backend: str = \"nccl\",\n                 gradient_compression: bool = False,\n                 sync_frequency: int = 1):\n        self.world_size = world_size\n        self.local_rank = local_rank\n        self.backend = backend\n        self.gradient_compression = gradient_compression\n        self.sync_frequency = sync_frequency\n\nclass MultiGPUTrainer:\n    \"\"\"Extended trainer supporting distributed training across multiple GPUs.\"\"\"\n    \n    def __init__(self, base_trainer, distributed_config: DistributedTrainingConfig):\n        self.base_trainer = base_trainer\n        self.distributed_config = distributed_config\n        self.process_group = None\n        \n    def initialize_distributed(self) -> None:\n        \"\"\"Initialize distributed training environment.\"\"\"\n        # TODO: Initialize distributed process group using torch.distributed\n        # TODO: Set device based on local_rank\n        # TODO: Wrap model with DistributedDataParallel\n        # TODO: Configure gradient synchronization settings\n        # TODO: Set up memory monitoring for distributed context\n        pass\n        \n    def synchronize_gradients(self, model: torch.nn.Module) -> Dict[str, float]:\n        \"\"\"Synchronize gradients across all workers with optional compression.\"\"\"\n        # TODO: Collect gradients from all model parameters\n        # TODO: Apply gradient compression if enabled\n        # TODO: Perform all-reduce operation across workers\n        # TODO: Apply synchronized gradients back to model\n        # TODO: Return synchronization statistics (timing, compression ratio)\n        pass\n        \n    def distributed_evaluation(self, eval_dataset) -> Dict[str, Any]:\n        \"\"\"Run evaluation across multiple GPUs and aggregate results.\"\"\"\n        # TODO: Distribute evaluation dataset across workers\n        # TODO: Run evaluation on local subset\n        # TODO: Gather evaluation results from all workers\n        # TODO: Aggregate metrics and compute final scores\n        # TODO: Return comprehensive evaluation results\n        pass\n```\n\n#### Alternative Adapter Plugin System\n\n```python\n\"\"\"\nPlugin architecture for alternative adapter methods.\nEnables easy integration of new parameter-efficient techniques.\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, List, Optional, Any\nimport torch\nfrom torch import nn\n\nclass AdapterInterface(ABC):\n    \"\"\"Common interface for all adapter methods.\"\"\"\n    \n    @abstractmethod\n    def inject_adapters(self, model: nn.Module, config: Dict[str, Any]) -> nn.Module:\n        \"\"\"Inject adapters into the target model.\"\"\"\n        pass\n        \n    @abstractmethod\n    def get_trainable_parameters(self) -> List[nn.Parameter]:\n        \"\"\"Return list of trainable adapter parameters.\"\"\"\n        pass\n        \n    @abstractmethod\n    def merge_adapters(self, model: nn.Module) -> nn.Module:\n        \"\"\"Merge adapter weights into base model.\"\"\"\n        pass\n        \n    @abstractmethod\n    def save_adapters(self, path: str) -> None:\n        \"\"\"Save adapter weights to disk.\"\"\"\n        pass\n\nclass AdaLoRAAdapter(AdapterInterface):\n    \"\"\"AdaLoRA implementation with dynamic rank allocation.\"\"\"\n    \n    def __init__(self, initial_rank: int, target_rank: int, rank_allocation_steps: int):\n        self.initial_rank = initial_rank\n        self.target_rank = target_rank\n        self.rank_allocation_steps = rank_allocation_steps\n        self.current_step = 0\n        self.adapter_modules = {}\n        \n    def inject_adapters(self, model: nn.Module, config: Dict[str, Any]) -> nn.Module:\n        \"\"\"Inject AdaLoRA adapters with dynamic rank capability.\"\"\"\n        # TODO: Identify target modules for adaptation\n        # TODO: Create high-rank initial adapters\n        # TODO: Initialize importance tracking for each adapter\n        # TODO: Set up pruning schedule for rank reduction\n        # TODO: Register forward hooks for importance computation\n        pass\n        \n    def update_ranks(self, importance_scores: Dict[str, torch.Tensor]) -> Dict[str, int]:\n        \"\"\"Update adapter ranks based on importance scores.\"\"\"\n        # TODO: Compute importance scores for each adapter module\n        # TODO: Determine new rank allocation based on importance\n        # TODO: Prune less important singular value directions\n        # TODO: Redistribute rank budget across modules\n        # TODO: Return updated rank allocation\n        pass\n        \nclass DoRAAdapter(AdapterInterface):\n    \"\"\"DoRA implementation with magnitude-direction decomposition.\"\"\"\n    \n    def __init__(self, rank: int, magnitude_lr_scale: float = 0.1):\n        self.rank = rank\n        self.magnitude_lr_scale = magnitude_lr_scale\n        self.magnitude_params = {}\n        self.direction_params = {}\n        \n    def inject_adapters(self, model: nn.Module, config: Dict[str, Any]) -> nn.Module:\n        \"\"\"Inject DoRA adapters with magnitude-direction separation.\"\"\"\n        # TODO: Identify target modules for adaptation\n        # TODO: Create magnitude scaling parameters\n        # TODO: Create low-rank direction adaptation matrices\n        # TODO: Set up different learning rates for magnitude vs direction\n        # TODO: Implement forward pass with decomposed updates\n        pass\n\nclass AdapterFactory:\n    \"\"\"Factory for creating and registering adapter implementations.\"\"\"\n    \n    _adapters: Dict[str, type] = {\n        \"lora\": None,  # Existing LoRA implementation\n        \"adalora\": AdaLoRAAdapter,\n        \"dora\": DoRAAdapter,\n    }\n    \n    @classmethod\n    def create_adapter(cls, adapter_type: str, **kwargs) -> AdapterInterface:\n        \"\"\"Create adapter instance of specified type.\"\"\"\n        # TODO: Validate adapter type is registered\n        # TODO: Create adapter instance with provided configuration\n        # TODO: Return configured adapter interface\n        pass\n        \n    @classmethod\n    def register_adapter(cls, name: str, adapter_class: type) -> None:\n        \"\"\"Register new adapter implementation.\"\"\"\n        # TODO: Validate adapter class implements AdapterInterface\n        # TODO: Add adapter to registry\n        # TODO: Log successful registration\n        pass\n```\n\n#### Advanced Evaluation Framework\n\n```python\n\"\"\"\nComprehensive evaluation framework supporting multiple evaluation methods.\nIntegrates automated metrics, human evaluation, and safety assessments.\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, List, Any, Optional, Callable\nimport torch\nfrom transformers import PreTrainedModel, PreTrainedTokenizer\n\nclass EvaluationInterface(ABC):\n    \"\"\"Common interface for all evaluation methods.\"\"\"\n    \n    @abstractmethod\n    def evaluate(self, model: PreTrainedModel, dataset: List[Dict]) -> Dict[str, Any]:\n        \"\"\"Run evaluation and return comprehensive metrics.\"\"\"\n        pass\n        \n    @abstractmethod\n    def get_evaluation_name(self) -> str:\n        \"\"\"Return human-readable name for this evaluation.\"\"\"\n        pass\n\nclass SafetyEvaluator(EvaluationInterface):\n    \"\"\"Comprehensive safety evaluation including bias, toxicity, and truthfulness.\"\"\"\n    \n    def __init__(self, safety_datasets: Dict[str, Any]):\n        self.safety_datasets = safety_datasets\n        self.bias_detector = None\n        self.toxicity_classifier = None\n        \n    def evaluate(self, model: PreTrainedModel, dataset: List[Dict]) -> Dict[str, Any]:\n        \"\"\"Run comprehensive safety evaluation.\"\"\"\n        # TODO: Test for various forms of bias (gender, racial, religious)\n        # TODO: Evaluate toxicity and harmfulness of generated content\n        # TODO: Assess truthfulness and factual accuracy\n        # TODO: Test prompt injection and jailbreak resistance\n        # TODO: Measure consistency of safety behaviors\n        # TODO: Return detailed safety metrics with specific failure examples\n        pass\n        \n    def evaluate_bias(self, model: PreTrainedModel, prompts: List[str]) -> Dict[str, float]:\n        \"\"\"Evaluate model bias across multiple protected characteristics.\"\"\"\n        # TODO: Generate responses for bias evaluation prompts\n        # TODO: Analyze responses for biased language and assumptions\n        # TODO: Compute bias scores across different demographic groups\n        # TODO: Return bias metrics with confidence intervals\n        pass\n\nclass HumanEvaluator(EvaluationInterface):\n    \"\"\"Human evaluation coordinator with inter-rater reliability tracking.\"\"\"\n    \n    def __init__(self, evaluation_platform: str, num_raters: int = 3):\n        self.evaluation_platform = evaluation_platform\n        self.num_raters = num_raters\n        self.calibration_results = {}\n        \n    def evaluate(self, model: PreTrainedModel, dataset: List[Dict]) -> Dict[str, Any]:\n        \"\"\"Coordinate human evaluation with quality controls.\"\"\"\n        # TODO: Generate model responses for evaluation dataset\n        # TODO: Create evaluation tasks for human raters\n        # TODO: Distribute tasks to calibrated raters\n        # TODO: Collect and validate rater responses\n        # TODO: Compute inter-rater agreement scores\n        # TODO: Return human evaluation metrics with reliability measures\n        pass\n        \n    def calibrate_raters(self, calibration_set: List[Dict]) -> Dict[str, float]:\n        \"\"\"Calibrate human raters on known examples.\"\"\"\n        # TODO: Present calibration examples to raters\n        # TODO: Compare rater responses to ground truth\n        # TODO: Provide feedback and additional training if needed\n        # TODO: Measure and track calibration accuracy\n        # TODO: Return calibration scores for each rater\n        pass\n\nclass ModelJudgeEvaluator(EvaluationInterface):\n    \"\"\"LLM-as-judge evaluation with multiple judge models for reliability.\"\"\"\n    \n    def __init__(self, judge_models: List[str], evaluation_criteria: Dict[str, str]):\n        self.judge_models = judge_models\n        self.evaluation_criteria = evaluation_criteria\n        self.loaded_judges = {}\n        \n    def evaluate(self, model: PreTrainedModel, dataset: List[Dict]) -> Dict[str, Any]:\n        \"\"\"Evaluate using multiple LLM judges for comprehensive assessment.\"\"\"\n        # TODO: Load and prepare judge models\n        # TODO: Generate responses from target model\n        # TODO: Create evaluation prompts for each criterion\n        # TODO: Collect judgments from all judge models\n        # TODO: Aggregate judgments and compute agreement scores\n        # TODO: Return judge-based metrics with confidence measures\n        pass\n        \n    def single_judge_evaluation(self, judge_model: PreTrainedModel, \n                              response: str, criteria: str) -> Dict[str, Any]:\n        \"\"\"Get detailed evaluation from single judge model.\"\"\"\n        # TODO: Format evaluation prompt with response and criteria\n        # TODO: Generate judge response with reasoning\n        # TODO: Parse numerical scores and qualitative feedback\n        # TODO: Return structured evaluation results\n        pass\n\nclass ComprehensiveEvaluationSuite:\n    \"\"\"Orchestrates multiple evaluation methods for complete assessment.\"\"\"\n    \n    def __init__(self):\n        self.evaluators: List[EvaluationInterface] = []\n        self.evaluation_results = {}\n        \n    def add_evaluator(self, evaluator: EvaluationInterface) -> None:\n        \"\"\"Add evaluation method to the suite.\"\"\"\n        # TODO: Validate evaluator implements required interface\n        # TODO: Add to evaluator list\n        # TODO: Log successful addition\n        pass\n        \n    def run_comprehensive_evaluation(self, model: PreTrainedModel, \n                                   eval_datasets: Dict[str, List[Dict]]) -> Dict[str, Any]:\n        \"\"\"Run all registered evaluations and aggregate results.\"\"\"\n        # TODO: Run each evaluator on appropriate datasets\n        # TODO: Collect and organize all evaluation results\n        # TODO: Compute meta-metrics across evaluation methods\n        # TODO: Generate comprehensive evaluation report\n        # TODO: Return complete evaluation results with summary\n        pass\n```\n\n#### Milestone Checkpoints for Extensions\n\n**Distributed Training Milestone:**\n- Command: `python -m torch.distributed.launch --nprocs_per_node=2 train_distributed.py --config distributed_config.yaml`\n- Expected: Training progress logged from multiple processes with synchronized loss curves\n- Verification: Check that model checkpoints are identical across all workers\n- Memory: Monitor total GPU memory usage scales appropriately with worker count\n\n**Alternative Adapter Milestone:**\n- Command: `python test_adapters.py --adapter_type adalora --dataset test_set.jsonl`\n- Expected: AdaLoRA adapters successfully inject and train with dynamic rank adjustment\n- Verification: Confirm adapter rank decreases over training steps based on importance scores\n- Performance: Validate AdaLoRA achieves competitive or better performance than standard LoRA\n\n**Advanced Evaluation Milestone:**\n- Command: `python comprehensive_eval.py --model fine_tuned_model --eval_suite complete`\n- Expected: Multiple evaluation methods run successfully and generate detailed reports\n- Verification: Safety evaluation identifies potential issues, human evaluation shows improved ratings\n- Output: Comprehensive evaluation report comparing fine-tuned model against baseline across all metrics\n\n\n## Glossary\n\n> **Milestone(s):** All milestones - this comprehensive glossary provides essential definitions and explanations for technical terms, acronyms, and domain-specific concepts used throughout the entire fine-tuning pipeline\n\nThis glossary serves as the definitive reference for understanding the specialized vocabulary, technical concepts, and methodological approaches that form the foundation of the LLM fine-tuning pipeline. Each entry provides not only a definition but also contextual information about how the concept applies within our system architecture.\n\n### Core Fine-tuning Concepts\n\n**Parameter-efficient fine-tuning** refers to methods that adapt large language models using far fewer trainable parameters than traditional full fine-tuning approaches. Instead of updating all billions of parameters in a model, these techniques introduce small adapter components that learn task-specific modifications while keeping the base model frozen. This dramatically reduces memory requirements, training time, and computational costs while often achieving comparable performance to full fine-tuning.\n\n**Low-rank adaptation** is the mathematical foundation underlying LoRA techniques, where weight updates are decomposed into the product of two smaller matrices. Rather than learning a full-rank weight matrix W of dimensions d×k, the system learns matrices A (d×r) and B (r×k) where r << min(d,k). The rank parameter r controls the capacity and expressiveness of the adaptation - higher ranks can capture more complex patterns but require more memory and parameters.\n\n**Catastrophic forgetting** describes the phenomenon where neural networks lose previously learned knowledge when trained on new tasks. In the context of LLM fine-tuning, this manifests as degraded performance on general language tasks after specializing on domain-specific data. Parameter-efficient methods like LoRA help mitigate this by preserving the original model weights while learning additive modifications.\n\n**Instruction tuning** represents a fine-tuning paradigm that teaches language models to follow human instructions and generate appropriate responses. Unlike traditional language modeling that predicts the next token in arbitrary text, instruction tuning uses structured datasets containing instruction-response pairs, system prompts, and conversational contexts to develop models that can understand and execute diverse user requests.\n\n### Memory and Quantization\n\n**Memory wall problem** describes the fundamental constraint where GPU memory requirements for training large language models exceed available hardware capacity. Modern LLMs with billions of parameters require hundreds of gigabytes of memory for full fine-tuning when accounting for model weights, optimizer states, gradients, and activation tensors. This creates an insurmountable barrier for most practitioners using consumer or mid-range professional hardware.\n\n**Quantization** reduces the numerical precision of model weights and activations to decrease memory consumption. Instead of storing parameters as 32-bit floating-point numbers, quantization represents them using fewer bits (typically 8, 4, or even 2 bits) with minimal impact on model performance. The challenge lies in maintaining numerical stability and avoiding significant quality degradation during the quantization process.\n\n**NormalFloat quantization** (NF4) is a specialized 4-bit quantization format optimized for neural network weight distributions. Unlike uniform quantization that divides the value range into equal intervals, NF4 uses quantization levels that follow a normal distribution, better matching the actual distribution of neural network parameters. This alignment results in lower quantization error and better preserved model quality.\n\n**Double quantization** applies quantization recursively to the quantization constants themselves, achieving additional memory savings. The first level quantizes model weights from float32 to 4-bit representations with scaling factors. The second level then quantizes these scaling factors, typically reducing them from float32 to 8-bit integers, providing incremental but meaningful memory reduction.\n\n**Mixed-precision training** balances memory efficiency with numerical stability by using different precision formats for storage versus computation. Weights are stored in low precision (4-bit) for memory efficiency, but during forward and backward passes, computations are performed in higher precision (float16 or bfloat16) to maintain numerical accuracy and gradient stability.\n\n### LoRA Architecture and Configuration\n\n**Target modules** specify which layers in the neural network receive LoRA adapter injections. Common targets include attention projection matrices (query, key, value, and output projections) and feed-forward network layers. The choice of target modules significantly impacts both the effectiveness of adaptation and the memory overhead, as each targeted layer requires its own set of low-rank matrices.\n\n![LoRA Adapter Architecture](./diagrams/lora-architecture.svg)\n\n**Rank** represents the dimensionality of the low-rank decomposition and serves as the primary hyperparameter controlling adapter capacity. Higher ranks (64-128) can capture more complex adaptations but require proportionally more parameters and memory. Lower ranks (8-32) are more memory-efficient but may underfitfor complex domain adaptations. The optimal rank depends on task complexity, dataset size, and available computational resources.\n\n**Alpha parameter** functions as a scaling factor that controls the magnitude of adapter contributions to the final model outputs. The effective learning rate for LoRA adapters equals the base learning rate multiplied by alpha divided by rank. This scaling mechanism allows fine-grained control over how aggressively the adapters modify the base model behavior during training.\n\n**Adapter injection** describes the process of inserting LoRA matrices into the computational graph of the frozen base model. During forward passes, the original linear transformation Wx is augmented with the low-rank term α(BAx)/r, where B and A are the learned adapter matrices. This injection preserves the base model's weights while enabling task-specific modifications through the adapter pathway.\n\n**Effective rank** measures the actual dimensionality utilized by learned LoRA adapters, which may be lower than the configured rank parameter. Singular value decomposition of the learned BA product reveals how many dimensions contribute significantly to the adaptation. Low effective rank suggests the chosen rank is oversized, while effective rank approaching the configured rank indicates full utilization.\n\n**Trainable parameter ratio** quantifies the efficiency of parameter-efficient fine-tuning by comparing the number of trainable adapter parameters to the total model parameter count. Typical LoRA configurations achieve ratios below 1%, meaning less than one percent of the model's parameters receive gradient updates during training.\n\n### Training Dynamics and Optimization\n\n**Gradient accumulation** simulates larger effective batch sizes by accumulating gradients across multiple micro-batches before performing optimizer updates. This technique enables training with batch sizes that exceed GPU memory capacity, as only one micro-batch's activations need to reside in memory at a time. The effective batch size equals micro-batch size multiplied by accumulation steps.\n\n**Learning rate scheduling** systematically adjusts the learning rate throughout training to optimize convergence and final performance. Common schedules include linear warmup followed by cosine decay, where the learning rate gradually increases from zero to a target value, then smoothly decreases following a cosine curve. Proper scheduling prevents training instability while maximizing final model quality.\n\n**Warmup scheduling** gradually increases the learning rate from zero to the target value over the initial training steps. This prevents optimization instability that can occur when large learning rates are applied to randomly initialized parameters or when fine-tuning pre-trained models. Warmup duration typically ranges from 3-10% of total training steps.\n\n**Cosine decay** reduces the learning rate following a cosine curve from the peak value to near zero over the training duration. This schedule provides aggressive learning rate reduction in later training phases, encouraging convergence to sharp minima that often generalize better than the wider minima found with constant learning rates.\n\n**Early stopping** monitors validation performance throughout training and terminates the process when improvement ceases. The mechanism tracks the best validation loss and stops training when the loss fails to improve for a configurable number of evaluation epochs. This prevents overfitting and reduces computational waste on unproductive training.\n\n**Checkpoint management** systematically saves training state at regular intervals to enable recovery from failures and selection of the best-performing model. Checkpoints include model weights, optimizer states, learning rate scheduler state, and training metadata. The system typically maintains multiple checkpoints while automatically removing older saves to manage disk space.\n\n### Data Handling and Tokenization\n\n**Chat template** defines the model-specific format for structuring conversational data with special tokens that delineate different parts of the conversation. Each model family (Llama, ChatML, Alpaca) uses distinct templates with specific tokens for system prompts, user instructions, assistant responses, and conversation boundaries. Proper template application is crucial for effective instruction tuning.\n\n**Subword tokenization** encodes text by splitting it into smaller meaningful units that balance vocabulary efficiency with semantic preservation. Modern tokenizers like SentencePiece and Byte-Pair Encoding create vocabularies of 30,000-50,000 subword units that can represent any text while avoiding the sparsity issues of word-level tokenization and the excessive sequence length of character-level approaches.\n\n**Causal language modeling** predicts the next token in a sequence using only preceding context, enforcing left-to-right information flow through attention masks. During training, the model learns to predict each position in the target sequence given all previous positions, enabling autoregressive generation during inference.\n\n**Data leakage** occurs when validation or test data contains information that also appears in training data, leading to overoptimistic performance estimates. In instruction tuning contexts, leakage can be subtle, such as paraphrased instructions or responses that convey identical information despite different wording. Proper data partitioning and deduplication are essential to prevent leakage.\n\n### Evaluation and Quality Assessment\n\n**Perplexity** measures model uncertainty when predicting validation tokens, calculated as the exponential of the average cross-entropy loss. Lower perplexity indicates better language modeling capability, as the model assigns higher probability to the actual next tokens in the validation data. Perplexity serves as a fundamental metric for assessing language model quality.\n\n**Task-specific evaluation** assesses functional capabilities like instruction following, reasoning, or domain expertise rather than just statistical language modeling performance. These evaluations use benchmarks relevant to the fine-tuning objectives, such as question-answering accuracy, instruction compliance rates, or domain-specific knowledge assessments.\n\n**Baseline comparison** evaluates fine-tuned model performance against the original base model to quantify the improvements achieved through adaptation. Side-by-side comparisons on identical prompts reveal whether fine-tuning enhanced the desired capabilities without causing degradation in general language abilities.\n\n**Adapter merging** combines learned LoRA weights with base model parameters to create a single standalone model. The merging process computes W' = W + α(BA)/r for each adapted layer, where W represents original weights and BA represents the learned adaptation. Merged models eliminate the need for separate adapter management during inference.\n\n**Export format** refers to deployment-ready model serializations optimized for specific inference environments. Popular formats include HuggingFace's safetensors for general-purpose deployment and GGUF for quantized inference in environments like llama.cpp. Each format involves different trade-offs between file size, loading speed, and inference performance.\n\n### System Architecture and Pipeline Management\n\n**Pipeline orchestration** coordinates the sequential execution of data preparation, model loading, adapter configuration, training, and evaluation components. The orchestrator manages dependencies between stages, handles error propagation, and maintains consistent state throughout the multi-stage process.\n\n**State coordination** manages shared state and configuration across pipeline components, ensuring consistent behavior and proper initialization ordering. The state management system tracks pipeline progress, stores intermediate results, and enables components to access shared configuration and metrics.\n\n**Memory pressure** describes situations where GPU memory usage approaches hardware limits, requiring careful resource management to prevent out-of-memory failures. The system monitors memory consumption throughout training and implements strategies like gradient checkpointing and dynamic batch sizing to operate within available memory constraints.\n\n**Component isolation** maintains independence between pipeline components while enabling necessary coordination and communication. Each component has well-defined interfaces, manages its own internal state, and communicates through structured APIs rather than direct state sharing.\n\n### Error Handling and System Reliability\n\n**Gradient explosion** occurs when gradient magnitudes become extremely large during backpropagation, causing training instability and potential numerical overflow. Detection involves monitoring gradient norms across training steps, while mitigation uses gradient clipping to limit maximum gradient magnitudes to stable ranges.\n\n**Training instability** encompasses various pathological behaviors during fine-tuning, including loss spikes, gradient explosions, and NaN propagation. Instability often results from inappropriate learning rates, numerical precision issues, or corrupted training data. Recovery strategies include checkpoint restoration, learning rate reduction, and gradient scaling adjustments.\n\n**NaN propagation** describes the spread of Not-a-Number values through neural network computations, typically originating from numerical overflow, underflow, or invalid operations like division by zero. NaN values contaminate subsequent computations and require immediate detection and mitigation to prevent training failure.\n\n**Checkpoint corruption** involves data integrity issues in saved training states, potentially caused by hardware failures, interrupted write operations, or filesystem errors. The system implements integrity verification through checksums and maintains multiple checkpoint versions to ensure recovery capability.\n\n### Advanced Techniques and Extensions\n\n**Distributed training** scales fine-tuning across multiple GPUs or nodes to accelerate training and handle larger models. Common parallelization strategies include data parallelism (replicating the model across devices), model parallelism (distributing layers across devices), and pipeline parallelism (sequential processing across devices).\n\n**Gradient synchronization** coordinates gradient updates across distributed workers, ensuring consistent model updates despite parallel computation. Synchronization strategies range from synchronous all-reduce operations to asynchronous parameter servers, each with different trade-offs between consistency and training speed.\n\n**Adaptive rank allocation** dynamically adjusts LoRA ranks based on the measured importance of different adaptation components. This technique can reduce memory usage by allocating higher ranks to more critical adaptations while using lower ranks for less important modifications.\n\n**Human evaluation calibration** trains human raters on standardized examples to ensure consistent and reliable assessment of model outputs. Calibration processes measure inter-rater reliability and establish scoring rubrics that minimize subjective variation in human judgments.\n\n**Safety benchmarking** systematically tests fine-tuned models for harmful or biased behaviors across various demographic groups and sensitive topics. Safety evaluation helps identify potential misuse risks and ensures responsible deployment of fine-tuned models in production environments.\n\n### Implementation Guidance\n\nThe terminology and concepts defined in this glossary form the foundation for implementing an effective LLM fine-tuning pipeline. Understanding these definitions enables clear communication about system requirements, design decisions, and technical trade-offs throughout the development process.\n\n#### Key Concept Categories\n\n| Category | Core Concepts | Implementation Priority |\n|----------|---------------|------------------------|\n| Memory Optimization | Quantization, Mixed-precision, Parameter-efficient fine-tuning | Critical - enables training on limited hardware |\n| Adaptation Techniques | LoRA, Rank, Alpha parameter, Target modules | High - core functionality of the system |\n| Training Dynamics | Gradient accumulation, Learning rate scheduling, Early stopping | High - ensures stable and effective training |\n| Data Handling | Chat templates, Tokenization, Data leakage prevention | Medium - essential for data quality |\n| Evaluation | Perplexity, Task-specific metrics, Baseline comparison | Medium - validates training effectiveness |\n| System Reliability | Error handling, Checkpoint management, State coordination | Low - important for production deployment |\n\n#### Terminology Usage Guidelines\n\nWhen implementing the fine-tuning pipeline, maintain consistency in terminology usage across code comments, documentation, and user interfaces. Use the precise terms defined in this glossary rather than informal synonyms or abbreviations. This consistency improves code maintainability and reduces confusion among team members.\n\nFor error messages and user-facing output, prefer descriptive terminology that helps users understand the system state and required actions. For example, report \"gradient explosion detected\" rather than generic \"training instability,\" and specify \"adapter merging in progress\" rather than ambiguous \"model processing.\"\n\n#### Technical Communication Standards\n\nDocumentation and code comments should reference these glossary terms to establish precise meaning and avoid ambiguity. When introducing new concepts not covered in this glossary, provide clear definitions and consider updating the glossary to maintain completeness.\n\nTeam communications about system architecture, performance optimization, and debugging should leverage this shared vocabulary to ensure accurate understanding of technical issues and proposed solutions.\n"}