{"html":"<h1 id=\"lock-free-data-structures-design-document\">Lock-free Data Structures: Design Document</h1>\n<h2 id=\"overview\">Overview</h2>\n<p>This system implements concurrent data structures using atomic operations and compare-and-swap (CAS) primitives instead of traditional locks. The key architectural challenge is achieving thread-safe operations while maintaining performance and avoiding deadlocks, race conditions, and memory reclamation issues inherent in lock-free programming.</p>\n<blockquote>\n<p>This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.</p>\n</blockquote>\n<h2 id=\"context-and-problem-statement\">Context and Problem Statement</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (foundational concepts underlying the entire project)</p>\n</blockquote>\n<p>The world of concurrent programming presents a fundamental challenge: how do we coordinate multiple threads accessing shared data without sacrificing performance or correctness? Traditional approaches rely heavily on locks—mutexes, semaphores, and other blocking synchronization primitives that ensure mutual exclusion. While conceptually straightforward, these mechanisms introduce significant bottlenecks in high-performance systems where dozens or hundreds of threads compete for shared resources.</p>\n<p>Lock-free programming represents a paradigm shift in concurrent system design. Instead of using locks to serialize access to shared data, lock-free algorithms rely on atomic operations and clever coordination protocols to allow multiple threads to make progress simultaneously. This approach promises higher throughput, better scalability, and elimination of common concurrency hazards like deadlocks and priority inversion. However, these benefits come at the cost of significantly increased algorithmic complexity and subtle correctness challenges that can confound even experienced developers.</p>\n<h3 id=\"mental-model-the-crowded-kitchen-analogy\">Mental Model: The Crowded Kitchen Analogy</h3>\n<p>To understand the fundamental difference between lock-based and lock-free approaches, imagine a busy restaurant kitchen during the dinner rush. The kitchen has shared resources: cutting boards, knives, stoves, and prep stations that multiple chefs need to use simultaneously.</p>\n<p><strong>The Lock-Based Kitchen</strong> operates like a traditional workspace with strict protocols. Each shared resource has a single key, and chefs must obtain the key before using any equipment. When Chef Alice needs the main cutting board, she takes its key, preventing anyone else from using it until she&#39;s completely finished and returns the key. If Chef Bob needs both the cutting board and the meat grinder simultaneously, he must acquire both keys in a predetermined order to avoid deadlocks. When Chef Carol arrives and needs the cutting board, she stands idle, waiting for Alice to finish—even if Alice is taking a smoke break while holding the key.</p>\n<p>This system ensures safety (no two chefs accidentally interfere with each other&#39;s work), but creates obvious inefficiencies. Chefs spend significant time waiting for keys, productivity suffers when key-holders are interrupted or delayed, and the entire kitchen can grind to a halt if someone forgets to return a key or if two chefs try to acquire keys in different orders.</p>\n<p><strong>The Lock-Free Kitchen</strong> operates on a radically different principle: optimistic coordination without exclusive ownership. Instead of keys, chefs use a &quot;try-and-verify&quot; approach. When Chef Alice needs to prep vegetables, she approaches the cutting board, checks if it&#39;s available, and begins working. If another chef starts using the same space, they negotiate in real-time—perhaps Alice moves to a different section of the board, or switches to a different cutting board entirely. The key insight is that chefs don&#39;t block each other; they adapt and continue working.</p>\n<p>In this system, Chef Bob doesn&#39;t wait for permission to start his mise en place. He begins work optimistically, knowing he might need to adjust his approach if conflicts arise. When Chef Carol needs workspace, she doesn&#39;t stand idle—she finds an alternative approach or waits for just the brief moment when she can safely make her contribution. The kitchen maintains higher throughput because chefs spend their time cooking rather than managing keys.</p>\n<p>However, the lock-free kitchen requires more sophisticated chefs. They must constantly verify their work hasn&#39;t been disrupted by others, have backup plans when conflicts arise, and coordinate through subtle communication protocols. A mistake in coordination can lead to ruined dishes or wasted ingredients—the equivalent of data corruption in concurrent systems.</p>\n<p><strong>The Memory Reclamation Challenge</strong> extends our analogy to kitchen cleanup. In the lock-based kitchen, cleanup is straightforward: when a chef finishes with equipment and returns the key, the dishwasher can immediately clean it because no one else could be using it. In the lock-free kitchen, the situation is more complex. Even after Chef Alice finishes with a cutting board and walks away, Chef Bob might still be using vegetables she prepared on that board. The dishwasher can&#39;t immediately sanitize the board without potentially disrupting Bob&#39;s work. Instead, the kitchen needs a more sophisticated protocol to determine when resources are truly safe to clean—this is the essence of the memory reclamation problem in lock-free systems.</p>\n<h3 id=\"limitations-of-lock-based-concurrency\">Limitations of Lock-Based Concurrency</h3>\n<p>Traditional mutex-based synchronization suffers from several fundamental limitations that become increasingly problematic as system scale and performance requirements grow. Understanding these limitations provides the motivation for exploring lock-free alternatives, despite their increased complexity.</p>\n<p><strong>Performance Bottlenecks and Scalability Issues</strong></p>\n<p>Locks introduce serialization points that fundamentally limit parallelism. When multiple threads contend for the same mutex, only one can make progress while others block, waiting in the kernel&#39;s scheduler queue. This blocking behavior creates several performance pathologies. First, <strong>context switching overhead</strong> becomes significant under contention. When a thread blocks on a mutex, the operating system must save its complete execution context, select another thread to run, and restore that thread&#39;s context—a process that can take thousands of CPU cycles. Under heavy contention, threads may spend more time context switching than doing useful work.</p>\n<p>Second, <strong>cache coherency traffic</strong> increases dramatically with lock contention. Modern CPUs maintain cache coherence through protocols like MESI, where cache lines bounce between cores as different threads acquire and release locks. A heavily contended mutex can cause its cache line to ping-pong between CPU cores, generating expensive memory bus traffic and cache misses. This effect is particularly pronounced on NUMA systems where cross-socket memory access latencies are significantly higher than local access.</p>\n<p>Third, <strong>convoy effects</strong> emerge when a lock-holding thread is preempted or interrupted. If the operating system preempts a thread while it holds a critical mutex, all other threads requiring that mutex must wait until the preempted thread is rescheduled and releases the lock. This can cause cascading delays where dozens of threads become synchronized to the scheduling quantum of a single unfortunate thread.</p>\n<p><strong>Deadlock and Livelock Vulnerabilities</strong></p>\n<p>Lock-based systems are inherently susceptible to deadlock conditions where two or more threads wait indefinitely for each other to release resources. Consider a hash table implementation that uses per-bucket locks for fine-grained concurrency. If Thread A acquires the lock for bucket 5 and then needs bucket 12, while Thread B simultaneously acquires bucket 12&#39;s lock and needs bucket 5, both threads will wait forever. Preventing deadlocks requires careful lock ordering protocols, but these become increasingly complex as the number of locks grows.</p>\n<p><strong>Lock ordering discipline</strong> becomes a significant maintenance burden in large codebases. Developers must remember to always acquire locks in a predetermined global order, even when the natural algorithm flow suggests a different sequence. This constraint often forces awkward code structures and makes otherwise simple operations complex. Worse, deadlock bugs are notoriously difficult to reproduce and debug because they depend on precise timing conditions that may not manifest during testing.</p>\n<p><strong>Livelock</strong> presents a related problem where threads avoid deadlock by backing off and retrying, but end up in a situation where they continuously interfere with each other without making progress. This is particularly common in systems that use timeouts and retry logic to handle deadlock detection.</p>\n<p><strong>Priority Inversion and Fairness Issues</strong></p>\n<p>Priority inversion occurs when a high-priority thread is blocked waiting for a lock held by a low-priority thread, effectively running at the low-priority thread&#39;s scheduling level. This problem becomes severe when a medium-priority thread preempts the low-priority lock holder, preventing it from releasing the lock and allowing the high-priority thread to proceed. While solutions like priority inheritance exist, they add complexity to the kernel scheduler and don&#39;t eliminate the fundamental issue.</p>\n<p><strong>Lock fairness</strong> presents another challenge. Most mutex implementations don&#39;t guarantee fair access under contention—a thread might repeatedly lose the race to acquire a lock while other threads succeed. This can lead to starvation scenarios where some threads make no progress for extended periods. Implementing fair locks typically requires more complex algorithms that further hurt performance.</p>\n<p><strong>Composability and Modularity Problems</strong></p>\n<p>Lock-based designs suffer from poor composability. When combining multiple lock-protected data structures, the resulting system often requires careful analysis to ensure deadlock freedom and reasonable performance. Simple operations that should compose naturally become complex when lock ordering requirements are considered.</p>\n<p>For example, consider implementing a transfer operation between two concurrent bank accounts, each protected by its own mutex. The operation requires acquiring both account locks, but the natural implementation <code>transfer(from_account, to_account, amount)</code> creates a potential deadlock if another thread simultaneously calls <code>transfer(to_account, from_account, other_amount)</code>. Solving this requires imposing an artificial ordering (perhaps by account number) that has nothing to do with the business logic.</p>\n<p><strong>Testing and Debugging Challenges</strong></p>\n<p>Concurrent bugs in lock-based systems are notoriously difficult to reproduce and debug. Race conditions often manifest only under specific timing conditions that are hard to recreate in development environments. Traditional debugging tools like debuggers and print statements can alter timing enough to hide bugs—the infamous &quot;Heisenbug&quot; effect.</p>\n<p><strong>Lock contention profiling</strong> requires sophisticated tools to understand where threads spend time waiting, and the results often vary significantly between different hardware configurations, load patterns, and even compiler optimizations. This makes performance optimization a challenging iterative process.</p>\n<blockquote>\n<p><strong>Key Insight</strong>: The fundamental problem with locks is that they solve the coordination problem by eliminating parallelism. Every critical section protected by a mutex becomes a sequential bottleneck, and the cumulative effect of these bottlenecks limits system scalability regardless of how many CPU cores are available.</p>\n</blockquote>\n<h3 id=\"comparison-of-concurrency-approaches\">Comparison of Concurrency Approaches</h3>\n<p>Understanding the trade-offs between different concurrency paradigms is essential for making informed architectural decisions. Each approach represents a different point in the design space, trading off implementation complexity, performance characteristics, and correctness guarantees.</p>\n<blockquote>\n<p><strong>Decision: Concurrency Model Selection Framework</strong></p>\n<ul>\n<li><strong>Context</strong>: Modern systems must handle increasing concurrency demands while maintaining correctness and performance. Different concurrency models offer varying trade-offs between implementation complexity, performance characteristics, and correctness guarantees.</li>\n<li><strong>Options Considered</strong>: Lock-based mutual exclusion, lock-free algorithms with atomic operations, wait-free algorithms with universal constructions, and hybrid approaches combining multiple techniques</li>\n<li><strong>Decision</strong>: Implement a progression from lock-based through lock-free to demonstrate the trade-offs, focusing primarily on lock-free algorithms as the sweet spot for practical high-performance systems</li>\n<li><strong>Rationale</strong>: Lock-free algorithms provide significant performance benefits over locks while remaining implementable without the theoretical complexity of wait-free constructions. This allows developers to understand both the benefits and costs of moving beyond traditional locking.</li>\n<li><strong>Consequences</strong>: Developers will understand when to apply each technique and can make informed decisions based on their specific performance, complexity, and correctness requirements.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>Coordination Method</th>\n<th>Progress Guarantee</th>\n<th>Implementation Complexity</th>\n<th>Performance Under Contention</th>\n<th>Failure Modes</th>\n<th>Best Use Cases</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Blocking (Locks)</strong></td>\n<td>Mutual exclusion with mutexes/semaphores</td>\n<td>Blocking - threads may wait indefinitely</td>\n<td>Low - straightforward critical sections</td>\n<td>Poor - serialization bottlenecks</td>\n<td>Deadlock, priority inversion, convoy effects</td>\n<td>Simple data structures, low contention scenarios</td>\n</tr>\n<tr>\n<td><strong>Lock-Free</strong></td>\n<td>Atomic operations with CAS retry loops</td>\n<td>Non-blocking - at least one thread makes progress</td>\n<td>High - complex retry logic and memory reclamation</td>\n<td>Good - multiple threads can progress simultaneously</td>\n<td>ABA problems, memory reclamation complexity, livelock potential</td>\n<td>High-performance data structures, real-time systems</td>\n</tr>\n<tr>\n<td><strong>Wait-Free</strong></td>\n<td>Universal constructions or specialized algorithms</td>\n<td>Strongest - every thread makes progress within bounded steps</td>\n<td>Very High - often requires helping mechanisms</td>\n<td>Excellent - no thread can be delayed by others</td>\n<td>Implementation complexity, memory overhead from helping</td>\n<td>Ultra-low latency systems, hard real-time applications</td>\n</tr>\n<tr>\n<td><strong>Hybrid</strong></td>\n<td>Combines techniques based on contention levels</td>\n<td>Variable - adapts based on runtime conditions</td>\n<td>Medium - managing multiple coordination mechanisms</td>\n<td>Variable - can adapt to different load patterns</td>\n<td>Complexity of mode transitions, tuning parameters</td>\n<td>Systems with varying load patterns, need for graceful degradation</td>\n</tr>\n</tbody></table>\n<p><strong>Lock-Based Mutual Exclusion Details</strong></p>\n<p>Traditional lock-based approaches use operating system primitives to enforce mutual exclusion around critical sections. Threads acquire locks before accessing shared data and release them afterward, ensuring that only one thread can modify the data at any given time.</p>\n<p>The <strong>primary advantage</strong> of lock-based approaches lies in their conceptual simplicity. Critical sections are clearly delineated, making it easier to reason about correctness. Most developers have experience with lock-based programming, reducing the learning curve for new team members. Additionally, locks compose reasonably well with existing APIs and frameworks that expect blocking semantics.</p>\n<p><strong>Performance characteristics</strong> vary significantly based on contention levels. Under low contention, well-implemented locks can be quite fast, with modern adaptive mutexes using efficient user-space spinning before falling back to kernel-mediated blocking. However, performance degrades rapidly as contention increases, with the worst-case scenario being complete serialization where threads effectively execute single-file through critical sections.</p>\n<p><strong>Memory ordering</strong> is generally handled automatically by lock implementations, which typically include full memory barriers on acquire and release operations. This simplifies reasoning about memory consistency but may be over-conservative for performance-critical applications that could benefit from relaxed ordering semantics.</p>\n<p><strong>Lock-Free Non-Blocking Algorithms</strong></p>\n<p>Lock-free algorithms eliminate blocking by using atomic operations, particularly compare-and-swap (CAS), to coordinate between threads. These algorithms guarantee that at least one thread will make progress within a bounded number of steps, even if individual threads might be delayed by contention.</p>\n<p>The <strong>core technique</strong> involves optimistic execution followed by validation. Threads read shared data, compute updates locally, and then attempt to atomically install their changes using CAS operations. If the CAS fails (indicating another thread modified the data concurrently), the operation is retried with updated values.</p>\n<p><strong>Memory reclamation</strong> becomes significantly more complex in lock-free systems. Since threads don&#39;t block, a thread might be accessing a data structure node even after another thread has logically removed it. Safe memory reclamation schemes like hazard pointers, epochs, or reference counting are necessary to prevent use-after-free errors.</p>\n<p><strong>ABA problems</strong> represent a subtle correctness issue unique to lock-free programming. If a thread reads a pointer value A, gets preempted, and then observes the same pointer value A again, it might incorrectly assume nothing has changed—even though the memory might have been freed and reallocated to the same address in the interim. Solutions typically involve tagged pointers or version counters.</p>\n<p><strong>Performance benefits</strong> are substantial under contention. Multiple threads can make progress simultaneously, cache lines experience less ping-ponging (since threads retry locally rather than blocking), and there&#39;s no context switching overhead from blocking operations. However, under low contention, lock-free algorithms may actually perform slightly worse than locks due to the overhead of atomic operations and retry logic.</p>\n<p><strong>Wait-Free Universal Constructions</strong></p>\n<p>Wait-free algorithms provide the strongest progress guarantee: every thread is guaranteed to complete any operation within a bounded number of its own steps, regardless of the behavior of other threads. This eliminates the possibility of indefinite delays that could affect real-time guarantees.</p>\n<p><strong>Implementation approaches</strong> typically fall into two categories. Universal constructions use consensus objects (like compare-and-swap) to implement arbitrary data structures by having threads propose operations and use consensus to determine which operations are applied in what order. Specialized wait-free algorithms are designed from scratch for specific data structures, often using helping mechanisms where threads assist each other to ensure progress.</p>\n<p><strong>Memory and computational overhead</strong> is typically higher than lock-free approaches. Universal constructions often require per-thread operation records and complex helping protocols. The constant factors in wait-free algorithms can be significantly higher than their lock-free counterparts, making them impractical for many applications despite their theoretical advantages.</p>\n<p><strong>Practical applicability</strong> is limited by implementation complexity and performance overhead. Wait-free algorithms are primarily justified in hard real-time systems where bounded execution time is more important than average-case performance, or in systems where even temporary delays could have catastrophic consequences.</p>\n<p><strong>Hybrid Adaptive Approaches</strong></p>\n<p>Modern high-performance systems increasingly use hybrid approaches that adapt their coordination strategy based on runtime conditions. These systems might start with optimistic lock-free protocols under low contention and fall back to more structured approaches when contention increases.</p>\n<p><strong>Adaptive mutexes</strong> represent one successful hybrid approach, spinning in user space for short periods before falling back to kernel-mediated blocking. This provides the low latency of spinning when locks are held briefly while avoiding the CPU waste of indefinite spinning.</p>\n<p><strong>Contention management</strong> becomes a key design challenge in hybrid systems. The system must detect contention levels, decide when to transition between coordination modes, and manage the complexity of supporting multiple protocols simultaneously. Poor tuning of these parameters can lead to pathological behavior where the system thrashes between different modes.</p>\n<p><strong>Examples</strong> include Java&#39;s concurrent collections, which use techniques like lock striping (reducing contention by using multiple locks) combined with lock-free operations for read-heavy workloads. Database systems often use hybrid approaches, starting with optimistic concurrency control and falling back to locking when conflicts are detected.</p>\n<blockquote>\n<p><strong>Common Pitfalls in Concurrency Model Selection</strong></p>\n<p>⚠️ <strong>Pitfall: Premature Lock-Free Optimization</strong> - Teams often attempt lock-free implementations without first establishing that lock contention is actually a bottleneck. Lock-free algorithms are significantly more complex to implement correctly, and their benefits only manifest under high contention scenarios.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Memory Reclamation Complexity</strong> - Many developers underestimate the complexity of safe memory management in lock-free systems. Memory reclamation schemes like hazard pointers can add significant complexity and overhead that may negate the performance benefits of lock-free algorithms.</p>\n<p>⚠️ <strong>Pitfall: Incorrect Progress Guarantee Assumptions</strong> - Lock-free doesn&#39;t mean faster—it means non-blocking. Under low contention, well-tuned locks often outperform lock-free algorithms. The choice should be based on system requirements for progress guarantees rather than pure performance considerations.</p>\n<p>⚠️ <strong>Pitfall: Underestimating Testing Complexity</strong> - Concurrent correctness bugs in lock-free systems are extremely difficult to reproduce and debug. Teams often underestimate the testing infrastructure and expertise required to validate lock-free implementations correctly.</p>\n</blockquote>\n<p>The selection of a concurrency approach should be driven by specific system requirements rather than theoretical elegance. Lock-based approaches remain appropriate for many scenarios, particularly when contention is low or when development team expertise with concurrent programming is limited. Lock-free approaches provide compelling benefits in high-contention scenarios where performance and scalability are critical, but require significant expertise and testing investment. Wait-free approaches should be reserved for systems with hard real-time requirements where the complexity and overhead can be justified by the stronger progress guarantees.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides practical guidance for understanding and implementing the concepts discussed above, serving as a foundation for the lock-free data structures that follow in subsequent sections.</p>\n<p><strong>Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Atomic Operations</td>\n<td><code>threading.Lock</code> with explicit synchronization</td>\n<td><code>ctypes</code> with C atomic operations or <code>multiprocessing.Value</code></td>\n</tr>\n<tr>\n<td>Memory Ordering</td>\n<td>Python GIL provides implicit ordering guarantees</td>\n<td>Explicit memory barriers using <code>threading.Barrier</code> or C extensions</td>\n</tr>\n<tr>\n<td>Performance Testing</td>\n<td><code>time.time()</code> measurements with basic threading</td>\n<td><code>perf_counter()</code> with statistical analysis and contention metrics</td>\n</tr>\n<tr>\n<td>Concurrency Testing</td>\n<td>Sequential consistency checking with simple asserts</td>\n<td>Property-based testing with <code>hypothesis</code> and race condition detection</td>\n</tr>\n<tr>\n<td>Debugging Tools</td>\n<td><code>print()</code> statements with thread IDs and timestamps</td>\n<td><code>faulthandler</code>, <code>py-spy</code> profiling, and custom logging with memory barriers</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended Module Structure</strong></p>\n<p>The lock-free data structures project should be organized to clearly separate concerns and build complexity incrementally:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>lock_free_structures/\n├── __init__.py                    # Package initialization and public API\n├── atomic/\n│   ├── __init__.py               # Atomic operations foundation\n│   ├── primitives.py             # AtomicReference, AtomicInteger, CAS operations\n│   ├── memory_ordering.py        # Memory ordering semantics and barriers\n│   └── aba_protection.py         # Tagged pointers and version counters\n├── structures/\n│   ├── __init__.py               # Data structure implementations\n│   ├── stack.py                  # Treiber stack implementation\n│   ├── queue.py                  # Michael-Scott queue implementation\n│   └── hashmap.py                # Split-ordered hash map implementation\n├── memory/\n│   ├── __init__.py               # Memory reclamation schemes\n│   ├── hazard_pointers.py        # Hazard pointer implementation\n│   ├── epoch_based.py            # Epoch-based reclamation (optional)\n│   └── reference_counting.py     # Atomic reference counting (optional)\n├── testing/\n│   ├── __init__.py               # Testing utilities and frameworks\n│   ├── linearizability.py       # Linearizability checking tools\n│   ├── stress_testing.py         # High-contention stress test framework\n│   └── correctness_checks.py     # Property verification utilities\n└── examples/\n    ├── benchmarks.py             # Performance comparison demos\n    ├── correctness_demo.py       # Correctness property demonstrations\n    └── pitfall_examples.py       # Common mistake demonstrations</code></pre></div>\n\n<p><strong>Atomic Operations Infrastructure</strong></p>\n<p>Since Python&#39;s threading model includes the Global Interpreter Lock (GIL), true lock-free programming requires careful consideration of how to achieve atomic operations. The following infrastructure provides a foundation for the lock-free algorithms that follow:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Atomic operations foundation for lock-free data structures.</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">This module provides atomic primitives that form the building blocks</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">for lock-free algorithms. Due to Python's GIL, true atomicity requires</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">careful implementation using either ctypes for C-level atomics or</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">careful synchronization protocols.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ctypes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TypeVar, Generic, Optional, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">T </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TypeVar(</span><span style=\"color:#9ECBFF\">'T'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MemoryOrdering</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Memory ordering constraints for atomic operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RELAXED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"relaxed\"</span><span style=\"color:#6A737D\">      # No ordering constraints</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ACQUIRE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"acquire\"</span><span style=\"color:#6A737D\">      # Acquire semantics for loads</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RELEASE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"release\"</span><span style=\"color:#6A737D\">      # Release semantics for stores</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ACQ_REL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"acq_rel\"</span><span style=\"color:#6A737D\">     # Both acquire and release</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SEQ_CST</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"seq_cst\"</span><span style=\"color:#6A737D\">     # Sequential consistency</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TaggedPointer</span><span style=\"color:#E1E4E8\">(Generic[T]):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Tagged pointer to prevent ABA problems.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pointer: Optional[T]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tag: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __hash__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> hash</span><span style=\"color:#E1E4E8\">((</span><span style=\"color:#79B8FF\">id</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.pointer), </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.tag))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __eq__</span><span style=\"color:#E1E4E8\">(self, other):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(other, TaggedPointer):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.pointer </span><span style=\"color:#F97583\">is</span><span style=\"color:#E1E4E8\"> other.pointer </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.tag </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> other.tag</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AtomicReference</span><span style=\"color:#E1E4E8\">(Generic[T]):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Thread-safe atomic reference with compare-and-swap support.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This implementation uses Python's threading primitives to simulate</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    atomic operations. In a production system, you would typically use</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    C extensions or specialized libraries for true lock-free atomics.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, initial_value: Optional[T] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize the internal value storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create a lock for simulating atomicity (remove in real lock-free impl)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Initialize any debugging/monitoring counters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> load</span><span style=\"color:#E1E4E8\">(self, ordering: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> MemoryOrdering.</span><span style=\"color:#79B8FF\">SEQ_CST</span><span style=\"color:#E1E4E8\">) -> Optional[T]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Atomically load the current value.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            ordering: Memory ordering constraint for this load</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Current value of the atomic reference</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Apply appropriate memory ordering semantics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Atomically read the current value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return the loaded value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> store</span><span style=\"color:#E1E4E8\">(self, new_value: Optional[T], ordering: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> MemoryOrdering.</span><span style=\"color:#79B8FF\">SEQ_CST</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Atomically store a new value.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            new_value: Value to store</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            ordering: Memory ordering constraint for this store</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Apply appropriate memory ordering semantics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Atomically update the stored value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Ensure visibility to other threads</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> compare_and_swap</span><span style=\"color:#E1E4E8\">(self, expected: Optional[T], new_value: Optional[T]) -> Tuple[</span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, Optional[T]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Atomically compare current value with expected and swap if equal.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        This is the fundamental primitive for lock-free algorithms.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            expected: Expected current value</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            new_value: New value to store if comparison succeeds</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Tuple of (success, observed_value)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            - success: True if swap occurred, False otherwise</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            - observed_value: The value that was actually in memory</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Atomically load the current value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Compare current value with expected (use 'is' for object identity)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If equal, store new_value and return (True, expected)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If not equal, return (False, current_value)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Ensure entire operation is atomic</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AtomicInteger</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Thread-safe atomic integer with arithmetic operations.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Provides atomic increment, decrement, and fetch-and-add operations</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    commonly needed in lock-free algorithms.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, initial_value: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize integer storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create synchronization primitives</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> load</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Atomically load current integer value.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement atomic load</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> store</span><span style=\"color:#E1E4E8\">(self, value: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Atomically store new integer value.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement atomic store</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> fetch_and_add</span><span style=\"color:#E1E4E8\">(self, increment: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Atomically add increment to current value and return previous value.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        This is a fundamental building block for counters and sequence numbers.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Use CAS loop to implement fetch-and-add</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Load current value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compute new value (current + increment)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Attempt CAS with current and new value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Retry if CAS fails, return old value if succeeds</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> increment</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Atomically increment and return new value.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.fetch_and_add(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> decrement</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Atomically decrement and return new value.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.fetch_and_add(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span></span></code></pre></div>\n\n<p><strong>ABA Problem Demonstration</strong></p>\n<p>Understanding the ABA problem is crucial for implementing correct lock-free algorithms:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Demonstration of the ABA problem and its solutions.</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">The ABA problem occurs when a thread reads a value A, gets preempted,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">and later observes the same value A, incorrectly concluding nothing</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">has changed - even though the memory might have been modified and</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">restored in the interim.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Node</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Simple linked list node for demonstrating ABA.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, data: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, next_node: Optional[</span><span style=\"color:#9ECBFF\">'Node'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> data</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.next </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> next_node</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ProblematicStack</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Stack implementation vulnerable to ABA problem.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.top </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> AtomicReference[Optional[Node]](</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> push</span><span style=\"color:#E1E4E8\">(self, data: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Push operation - not vulnerable to ABA.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create new node with data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Use CAS loop to update top pointer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> pop</span><span style=\"color:#E1E4E8\">(self) -> Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Pop operation - vulnerable to ABA problem.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load current top</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If empty, return None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Read next pointer (DANGEROUS: node might be freed)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Attempt CAS to swing top to next</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return data if successful, retry if failed</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> demonstrate_aba_problem</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Demonstrates how ABA can cause corruption in naive lock-free code.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This function sets up a scenario where:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    1. Thread 1 reads stack top (A)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    2. Thread 2 pops A and B, then pushes A back</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    3. Thread 1 sees A again and incorrectly thinks nothing changed</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    4. Thread 1's CAS succeeds but creates corruption</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create stack with nodes A -> B -> C</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Start thread 1 that begins pop operation, then sleeps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Start thread 2 that does: pop A, pop B, push A</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Let thread 1 continue - its CAS will succeed incorrectly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Demonstrate the resulting corruption</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SafeTaggedStack</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Stack implementation using tagged pointers to prevent ABA.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.top </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> AtomicReference[TaggedPointer[Optional[Node]]](</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TaggedPointer(</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> push</span><span style=\"color:#E1E4E8\">(self, data: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"ABA-safe push using tagged pointers.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create new node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: CAS loop with tag increment</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> pop</span><span style=\"color:#E1E4E8\">(self) -> Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"ABA-safe pop using tagged pointers.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load current tagged top</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: CAS with incremented tag</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Concurrency Testing Framework</strong></p>\n<p>Testing lock-free code requires specialized approaches to detect race conditions and verify correctness properties:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Testing framework for concurrent correctness verification.</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Provides utilities for stress testing, linearizability checking,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">and property verification in concurrent data structures.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> random</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Callable, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> collections </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> defaultdict</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Operation</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents a single operation in a concurrent execution.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    thread_id: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    operation: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    arguments: List[Any]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    return_value: Any</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start_time: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    end_time: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> LinearizabilityChecker</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Checks if a concurrent execution is linearizable.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    A concurrent execution is linearizable if there exists a sequential</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    execution of the same operations that:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    1. Produces the same results</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    2. Respects the real-time order of non-overlapping operations</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.operations </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Lock()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> record_operation</span><span style=\"color:#E1E4E8\">(self, op: Operation):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record an operation for later linearizability analysis.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Thread-safely add operation to history</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> check_linearizability</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Verify that recorded operations are linearizable.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        This is a simplified checker - full linearizability checking</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        is NP-complete in general.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Sort operations by start time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Try to find valid linearization points</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Verify sequential specification is satisfied</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return True if linearizable, False otherwise</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> stress_test_data_structure</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    data_structure: Any,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    operations: List[Callable],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_threads: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    operations_per_thread: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    duration_seconds: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Stress test a data structure with concurrent operations.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        data_structure: The data structure to test</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        operations: List of operation functions to call</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        num_threads: Number of concurrent threads</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        operations_per_thread: Operations each thread should perform</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        duration_seconds: Maximum test duration</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Dictionary with test results and statistics</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create worker threads that randomly call operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Record timing and correctness metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Run for specified duration or operation count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Collect and return comprehensive statistics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint: Foundation Verification</strong></p>\n<p>After implementing the atomic operations foundation, verify correct behavior with these tests:</p>\n<ol>\n<li><p><strong>Atomic Reference Correctness</strong>: Run <code>python -m pytest testing/test_atomic_reference.py -v</code>. Expected output should show all CAS operations succeeding with correct return values and no lost updates.</p>\n</li>\n<li><p><strong>ABA Problem Demonstration</strong>: Run <code>python examples/aba_demo.py</code>. You should see output showing:</p>\n<ul>\n<li>Problematic stack exhibiting corruption under specific timing</li>\n<li>Tagged pointer stack maintaining correctness under same conditions</li>\n<li>Clear explanation of why the naive approach fails</li>\n</ul>\n</li>\n<li><p><strong>Concurrent Counter Test</strong>: Run a test with 10 threads each incrementing an atomic counter 1000 times. Final value should always be exactly 10,000 with no lost updates.</p>\n</li>\n<li><p><strong>Memory Ordering Verification</strong>: Use threading barriers to verify acquire/release semantics prevent reordering of dependent operations.</p>\n</li>\n</ol>\n<p><strong>Performance Baseline Measurements</strong></p>\n<p>Before implementing lock-free data structures, establish performance baselines:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Measure lock-based vs atomic operation overhead</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> benchmarks/atomic_vs_locks.py</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected output:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Lock acquire/release: 50ns average</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Atomic CAS success: 20ns average  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Atomic CAS failure: 15ns average</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Contended lock: 2000ns average (context switch)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Contended CAS: 100ns average (retry loop)</span></span></code></pre></div>\n\n<p><strong>Common Implementation Pitfalls</strong></p>\n<p>⚠️ <strong>Pitfall: GIL Dependency</strong> - Python&#39;s GIL provides some atomicity guarantees that don&#39;t exist in other languages. Code that works in Python might have race conditions when ported to Go or Rust.</p>\n<p>⚠️ <strong>Pitfall: Object Identity vs Value Equality</strong> - Use <code>is</code> for comparing object references in CAS operations, not <code>==</code>. Value equality can give false positives when different objects have the same content.</p>\n<p>⚠️ <strong>Pitfall: Missing Memory Barriers</strong> - Even with atomic operations, you may need explicit memory barriers to prevent reordering of adjacent non-atomic operations.</p>\n<p>⚠️ <strong>Pitfall: Infinite CAS Loops</strong> - Always include backoff or retry limits in CAS loops to prevent livelock under extreme contention.</p>\n<p>The foundation established in this section provides the building blocks for implementing the lock-free data structures in subsequent milestones. Understanding these concepts deeply is essential before proceeding to the more complex algorithms that follow.</p>\n<h2 id=\"goals-and-non-goals\">Goals and Non-Goals</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (this section defines the scope and boundaries for the entire lock-free data structure library)</p>\n</blockquote>\n<p>The success of any complex software project hinges on clearly defining what it will and will not accomplish. Lock-free programming is particularly susceptible to scope creep because the theoretical possibilities are vast, yet the practical implementation challenges are enormous. This section establishes concrete, measurable goals that will guide our implementation decisions and help us recognize when we have succeeded.</p>\n<h3 id=\"mental-model-the-precision-tool-workshop\">Mental Model: The Precision Tool Workshop</h3>\n<p>Think of our lock-free data structure library as building a precision tool workshop rather than a general-purpose hardware store. A precision tool workshop focuses on creating a small number of extremely high-quality, specialized tools that perform specific tasks better than any generic alternative. Each tool is crafted with meticulous attention to detail, tested under extreme conditions, and designed to work flawlessly when used correctly.</p>\n<p>In contrast, a hardware store carries thousands of items of varying quality, trying to serve every possible need. Our library follows the precision workshop philosophy: we will build a small number of lock-free data structures exceptionally well, with bulletproof correctness guarantees and outstanding performance characteristics, rather than attempting to implement every conceivable concurrent data structure with mediocre quality.</p>\n<p>This focused approach allows us to deeply understand the subtle challenges of lock-free programming, develop robust solutions to memory reclamation and correctness verification, and create reference implementations that others can learn from and build upon. Like a master craftsman who perfects their core techniques before expanding to new domains, we will master the fundamental patterns of lock-free programming through careful implementation of stack, queue, and hash map structures.</p>\n<h2 id=\"functional-goals\">Functional Goals</h2>\n<p>Our functional goals define the core correctness and safety properties that every component in our library must satisfy. These are non-negotiable requirements that distinguish a production-ready lock-free library from academic prototypes or hobby projects.</p>\n<h3 id=\"thread-safe-operations-without-data-races\">Thread-Safe Operations Without Data Races</h3>\n<p>Every operation exposed by our data structures must be thread-safe by design, meaning multiple threads can invoke operations concurrently without causing data corruption, undefined behavior, or inconsistent state. This goes beyond simply avoiding crashes—we must ensure that the data structure maintains its structural invariants even under arbitrary thread interleavings and timing variations.</p>\n<p>Thread safety in our context means that all shared memory accesses use appropriate atomic operations with correct memory ordering constraints. Non-atomic operations on shared variables are forbidden, as they create data races that lead to undefined behavior according to the language memory model. We will use the compare-and-swap primitive as our primary synchronization mechanism, combined with atomic loads and stores using carefully chosen memory ordering semantics.</p>\n<p>Our thread safety guarantee extends to structural integrity: linked list pointers will never become dangling, reference counts will never become negative, and internal consistency invariants (such as a queue&#39;s head always being reachable from its tail) will be preserved across all concurrent operations. This requires careful attention to the order of pointer updates and the use of helping mechanisms to ensure that partially completed operations do not leave the data structure in an inconsistent state.</p>\n<blockquote>\n<p><strong>Decision: Complete Operation Atomicity</strong></p>\n<ul>\n<li><strong>Context</strong>: Operations like queue enqueue involve multiple pointer updates that cannot all be performed atomically in a single instruction</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Accept temporary inconsistent states visible to other threads</li>\n<li>Use multi-word compare-and-swap hardware instructions where available</li>\n<li>Design algorithms with single atomic update points that transition between consistent states</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Design algorithms with single atomic linearization points</li>\n<li><strong>Rationale</strong>: Multi-word CAS has limited hardware support and complex fallback requirements. Single-point atomicity provides the strongest correctness guarantees and simplifies reasoning about concurrent behavior.</li>\n<li><strong>Consequences</strong>: May require more complex algorithms (like Michael-Scott queue) but provides clear linearizability and easier verification</li>\n</ul>\n</blockquote>\n<h3 id=\"fifo-ordering-preservation\">FIFO Ordering Preservation</h3>\n<p>Our queue implementation must provide strict first-in-first-out ordering semantics, meaning that elements dequeued from the queue appear in exactly the same order they were enqueued, regardless of the timing and concurrency of operations. This property must hold even when multiple threads are performing concurrent enqueue and dequeue operations with arbitrary interleavings.</p>\n<p>FIFO ordering is more challenging to maintain in lock-free algorithms than in lock-based approaches because we cannot use a global mutex to serialize all operations. Instead, we must use techniques like the Michael-Scott algorithm that carefully coordinates separate head and tail pointer updates to preserve ordering while allowing concurrent access to different ends of the queue.</p>\n<p>The ordering guarantee applies at the logical level of linearizability: if one enqueue operation completes before another begins (in real time), then the first element will be dequeued before the second. For overlapping operations, their effective ordering is determined by their linearization points—the specific atomic operations that make them appear to take effect instantaneously.</p>\n<h3 id=\"linearizability-as-the-correctness-standard\">Linearizability as the Correctness Standard</h3>\n<p>All our data structures will satisfy linearizability, which is the standard correctness condition for concurrent objects. Linearizability means that every operation appears to take effect atomically at some point between its invocation and response, and the results of all operations are consistent with some sequential execution that respects the real-time ordering of non-overlapping operations.</p>\n<table>\n<thead>\n<tr>\n<th>Correctness Property</th>\n<th>Definition</th>\n<th>Our Implementation Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Sequential Consistency</td>\n<td>Operations appear in program order per thread</td>\n<td>Maintained through memory ordering constraints</td>\n</tr>\n<tr>\n<td>Linearizability</td>\n<td>Operations have atomic linearization points</td>\n<td>Identified and documented for each data structure operation</td>\n</tr>\n<tr>\n<td>Lock-freedom</td>\n<td>At least one thread makes progress</td>\n<td>Ensured through helping mechanisms and bounded retry loops</td>\n</tr>\n<tr>\n<td>Memory Safety</td>\n<td>No use-after-free or dangling pointers</td>\n<td>Achieved through hazard pointer memory reclamation</td>\n</tr>\n</tbody></table>\n<p>For our stack implementation, the linearization point of a successful push operation is the successful compare-and-swap that updates the top pointer. For a successful pop operation, it is the successful compare-and-swap that advances the top pointer to the next node. These points are clearly identifiable in the algorithm and provide a formal basis for proving correctness.</p>\n<p>The linearizability guarantee allows our data structures to be composed with other concurrent operations while maintaining predictable semantics. Applications can reason about our data structures as if they were atomic, sequential objects, while still benefiting from the performance advantages of lock-free implementation.</p>\n<h3 id=\"safe-memory-reclamation\">Safe Memory Reclamation</h3>\n<p>Memory management in lock-free data structures presents a fundamental challenge: we cannot immediately free memory when removing a node from a data structure because other threads may still be accessing that node. Traditional garbage collection solves this problem but introduces pause times and memory overhead that conflict with our performance goals.</p>\n<p>Our solution implements hazard pointers, a memory reclamation scheme that allows threads to announce which nodes they are currently accessing, preventing those nodes from being freed by other threads. This approach provides memory safety guarantees equivalent to garbage collection while maintaining the real-time characteristics of lock-free algorithms.</p>\n<p>The hazard pointer system operates in several phases:</p>\n<ol>\n<li><strong>Protection</strong>: Before accessing a shared node, a thread sets a hazard pointer to that node</li>\n<li><strong>Validation</strong>: After setting the hazard pointer, the thread re-reads the shared pointer to ensure it still points to the protected node</li>\n<li><strong>Safe Access</strong>: While the hazard pointer is set, no other thread will free the protected node</li>\n<li><strong>Release</strong>: After finishing access, the thread clears its hazard pointer</li>\n<li><strong>Retirement</strong>: When removing nodes from data structures, threads place them on a retirement list rather than immediately freeing them</li>\n<li><strong>Scanning</strong>: Periodically, threads scan all active hazard pointers and free retired nodes that are not protected</li>\n</ol>\n<p>This approach guarantees that freed memory is never accessed, preventing crashes and data corruption that would otherwise occur in naive lock-free implementations.</p>\n<table>\n<thead>\n<tr>\n<th>Memory Safety Requirement</th>\n<th>Implementation Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>No use-after-free</td>\n<td>Hazard pointer protection before access</td>\n</tr>\n<tr>\n<td>No double-free</td>\n<td>Retirement list prevents duplicate freeing</td>\n</tr>\n<tr>\n<td>No memory leaks</td>\n<td>Periodic scanning and batch reclamation</td>\n</tr>\n<tr>\n<td>Bounded memory usage</td>\n<td>Threshold-based reclamation triggers</td>\n</tr>\n</tbody></table>\n<h2 id=\"performance-goals\">Performance Goals</h2>\n<p>Performance goals define the quantitative characteristics that distinguish our lock-free implementation from simpler alternatives. These goals drive architectural decisions and provide measurable criteria for evaluating our success.</p>\n<h3 id=\"high-throughput-under-contention\">High Throughput Under Contention</h3>\n<p>Our data structures must maintain high operation throughput even when many threads are concurrently accessing the same data structure. This requires minimizing the serialization points where threads must compete for exclusive access to shared state.</p>\n<p>Traditional mutex-based data structures suffer from fundamental throughput limitations because the critical section becomes a bottleneck—only one thread can make progress at a time, regardless of how many cores are available. Our lock-free approach eliminates this bottleneck by allowing multiple threads to make progress simultaneously, even when they are modifying the same data structure.</p>\n<p>We will measure throughput as operations per second under controlled contention scenarios:</p>\n<table>\n<thead>\n<tr>\n<th>Scenario</th>\n<th>Thread Count</th>\n<th>Target Throughput</th>\n<th>Comparison Baseline</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Low Contention</td>\n<td>2-4 threads</td>\n<td>10M ops/sec</td>\n<td>Mutex-protected equivalent</td>\n</tr>\n<tr>\n<td>Medium Contention</td>\n<td>8-16 threads</td>\n<td>5M ops/sec</td>\n<td>2x better than mutex baseline</td>\n</tr>\n<tr>\n<td>High Contention</td>\n<td>32+ threads</td>\n<td>1M ops/sec</td>\n<td>3x better than mutex baseline</td>\n</tr>\n<tr>\n<td>Mixed Operations</td>\n<td>Variable</td>\n<td>Proportional to operation mix</td>\n<td>Separate read/write measurements</td>\n</tr>\n</tbody></table>\n<p>The key insight is that lock-free algorithms can achieve increasing absolute throughput as more threads are added, up to the point where hardware resources (memory bandwidth, cache coherence traffic) become the limiting factor. Mutex-based approaches typically show decreasing throughput as contention increases due to context switching overhead and lock acquisition delays.</p>\n<h3 id=\"scalability-across-cpu-cores\">Scalability Across CPU Cores</h3>\n<p>Our implementation must demonstrate near-linear scalability across CPU cores for embarrassingly parallel workloads. When threads are operating on different parts of the data structure or when the workload naturally distributes across multiple independent operations, performance should improve proportionally with the number of available cores.</p>\n<p>Scalability challenges in lock-free programming often arise from false sharing (multiple threads modifying different data that happens to reside in the same cache line) and cache coherence traffic (the overhead of keeping atomic variables synchronized across cores). Our design will minimize these effects through careful memory layout and algorithmic choices.</p>\n<p>For our hash map implementation, scalability means that threads operating on different hash buckets should rarely interfere with each other. The split-ordered list design achieves this by distributing operations across bucket chains and minimizing shared state between buckets.</p>\n<blockquote>\n<p><strong>Decision: NUMA-Aware Memory Allocation Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: On multi-socket systems, memory access costs vary dramatically depending on which NUMA node allocated the memory</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Use system default allocation (simple but may cause remote memory access)</li>\n<li>Implement thread-local memory pools with NUMA binding</li>\n<li>Use NUMA-aware allocation hints for shared data structures</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Start with system default allocation, add NUMA awareness as an optimization</li>\n<li><strong>Rationale</strong>: NUMA effects vary significantly across hardware platforms, and premature optimization could complicate the core algorithm implementation without clear benefits</li>\n<li><strong>Consequences</strong>: May leave performance on the table for large multi-socket systems, but ensures portability and implementation focus on correctness first</li>\n</ul>\n</blockquote>\n<h3 id=\"minimal-latency-for-individual-operations\">Minimal Latency for Individual Operations</h3>\n<p>While throughput measures aggregate performance across many operations, latency measures the time required for individual operations to complete. Low latency is critical for real-time applications and interactive systems where response time directly affects user experience.</p>\n<p>Lock-free algorithms provide latency advantages by eliminating the unpredictable delays associated with lock acquisition. When a thread holds a mutex, other threads must wait for an unbounded time that depends on scheduling decisions, page faults, and other factors outside the algorithm&#39;s control. Lock-free operations complete in a time bounded only by the algorithm&#39;s retry behavior and memory access latencies.</p>\n<p>Our latency goals focus on worst-case behavior rather than average case:</p>\n<table>\n<thead>\n<tr>\n<th>Latency Metric</th>\n<th>Target Value</th>\n<th>Measurement Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>99th Percentile</td>\n<td>&lt;100 microseconds</td>\n<td>High-frequency timing under contention</td>\n</tr>\n<tr>\n<td>99.9th Percentile</td>\n<td>&lt;1 millisecond</td>\n<td>Includes memory allocation delays</td>\n</tr>\n<tr>\n<td>Maximum Observed</td>\n<td>&lt;10 milliseconds</td>\n<td>Excludes OS scheduling anomalies</td>\n</tr>\n<tr>\n<td>Variance</td>\n<td>Low coefficient of variation</td>\n<td>Consistent performance across runs</td>\n</tr>\n</tbody></table>\n<p>The bounded latency property of lock-free algorithms makes them suitable for soft real-time applications where predictable response times are more important than peak throughput. However, we must be careful to avoid livelock situations where threads interfere with each other&#39;s progress, leading to excessive retry loops that inflate latency.</p>\n<h3 id=\"deterministic-performance-characteristics\">Deterministic Performance Characteristics</h3>\n<p>Our algorithms must exhibit predictable performance behavior that does not depend on lucky or unlucky timing of thread interleavings. This determinism allows application developers to reason about system behavior and make reliable capacity planning decisions.</p>\n<p>Deterministic performance means avoiding algorithms with worst-case exponential backoff or unbounded retry loops that could theoretically run forever. While our compare-and-swap loops may retry multiple times under heavy contention, the number of retries should be bounded by reasonable constants related to the number of concurrent threads, not by arbitrary timing factors.</p>\n<p>We will validate deterministic behavior through stress testing that measures performance variance across many runs with identical workloads. High variance would indicate that our algorithms are sensitive to timing-dependent effects that could cause unpredictable performance in production systems.</p>\n<h2 id=\"explicit-non-goals\">Explicit Non-Goals</h2>\n<p>Clearly defining what our library will NOT provide is as important as defining what it will provide. These non-goals help maintain focus and prevent scope creep that could compromise the quality of our core functionality.</p>\n<h3 id=\"blocking-operations-and-wait-free-guarantees\">Blocking Operations and Wait-Free Guarantees</h3>\n<p>Our library will provide lock-free algorithms but NOT wait-free algorithms. The distinction is crucial: lock-free guarantees that at least one thread makes progress at any point in time, while wait-free guarantees that every thread makes progress within a bounded number of steps.</p>\n<table>\n<thead>\n<tr>\n<th>Progress Guarantee</th>\n<th>Definition</th>\n<th>Implementation Complexity</th>\n<th>Performance Trade-offs</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Lock-free</td>\n<td>At least one thread progresses</td>\n<td>Moderate (CAS retry loops)</td>\n<td>High throughput possible</td>\n</tr>\n<tr>\n<td>Wait-free</td>\n<td>Every thread progresses</td>\n<td>High (universal constructions)</td>\n<td>Lower throughput, higher latency</td>\n</tr>\n<tr>\n<td>Obstruction-free</td>\n<td>Threads progress when running alone</td>\n<td>Low (simple CAS)</td>\n<td>Poor contention handling</td>\n</tr>\n</tbody></table>\n<p>Wait-free algorithms require significantly more complex implementations, often using universal constructions that simulate stronger synchronization primitives. These constructions typically have higher constant factors and lower peak throughput than simpler lock-free algorithms, making them unsuitable for our performance goals.</p>\n<p>Our lock-free approach may allow some threads to be delayed by interference from other threads, but it provides much better average-case performance while still avoiding the deadlock and priority inversion problems of lock-based approaches. Applications that require strict wait-free guarantees should use specialized libraries designed specifically for that purpose.</p>\n<h3 id=\"automatic-garbage-collection-integration\">Automatic Garbage Collection Integration</h3>\n<p>We will NOT integrate with language-specific garbage collection systems or rely on GC for memory management. Instead, we implement explicit memory reclamation through hazard pointers, giving applications direct control over memory management behavior and timing.</p>\n<p>Garbage collection integration would provide implementation simplicity but conflicts with our performance goals in several ways:</p>\n<ul>\n<li><strong>Unpredictable pause times</strong>: GC pauses would violate our latency guarantees</li>\n<li><strong>Memory overhead</strong>: GC systems typically use 2-4x more memory than explicit management</li>\n<li><strong>Language dependence</strong>: GC integration would make the library non-portable across languages</li>\n<li><strong>Performance unpredictability</strong>: GC pressure could cause sudden performance degradation</li>\n</ul>\n<blockquote>\n<p><strong>Decision: Manual Memory Management with Hazard Pointers</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to safely reclaim memory in lock-free algorithms without relying on garbage collection</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Rely on language GC (simple but performance unpredictable)</li>\n<li>Reference counting with atomic operations (ABA problems and cycle issues)</li>\n<li>Hazard pointers for safe manual reclamation</li>\n<li>RCU (Read-Copy-Update) mechanisms</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement hazard pointers for memory reclamation</li>\n<li><strong>Rationale</strong>: Hazard pointers provide deterministic reclamation timing, bounded memory usage, and portability across languages and runtime systems</li>\n<li><strong>Consequences</strong>: More complex implementation but predictable performance and broad applicability</li>\n</ul>\n</blockquote>\n<h3 id=\"complex-data-structures-beyond-fundamentals\">Complex Data Structures Beyond Fundamentals</h3>\n<p>Our scope is limited to fundamental data structures: stack, queue, and hash map. We will NOT implement complex structures like:</p>\n<ul>\n<li><strong>Trees</strong> (B-trees, red-black trees, AVL trees): Complex balancing operations are difficult to make lock-free</li>\n<li><strong>Graphs</strong> (adjacency lists, adjacency matrices): Require complex traversal algorithms and memory management</li>\n<li><strong>Sets and Maps with ordering requirements</strong>: Sorted data structures have complex linearizability requirements</li>\n<li><strong>Multi-dimensional structures</strong>: Spatial data structures, R-trees, quad-trees</li>\n<li><strong>Specialized structures</strong>: Bloom filters, skip lists, tries</li>\n</ul>\n<p>The rationale for this limitation is that complex data structures would require substantially more development time while providing diminishing educational value. The patterns learned from implementing stack, queue, and hash map are sufficient to understand the fundamental challenges of lock-free programming, and these structures serve as building blocks for more complex algorithms.</p>\n<p>Applications requiring complex lock-free data structures should compose our fundamental structures or use specialized libraries designed for specific use cases. Our goal is to provide high-quality, well-understood implementations that serve as both production components and educational examples.</p>\n<h3 id=\"dynamic-memory-pool-management\">Dynamic Memory Pool Management</h3>\n<p>We will NOT implement sophisticated memory pool management, custom allocators, or memory optimization features. Memory allocation will use standard system allocators (malloc/free or language equivalents) rather than implementing pool-based or region-based allocation strategies.</p>\n<p>Custom memory management would add significant complexity without directly advancing the core learning goals of lock-free algorithm implementation. While memory allocation patterns can significantly affect performance, optimizing allocation is a separate concern from understanding compare-and-swap algorithms and memory reclamation.</p>\n<table>\n<thead>\n<tr>\n<th>Memory Management Feature</th>\n<th>Status</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Custom allocators</td>\n<td>Excluded</td>\n<td>Complexity doesn&#39;t advance core learning goals</td>\n</tr>\n<tr>\n<td>Memory pools</td>\n<td>Excluded</td>\n<td>Can be added as separate optimization layer</td>\n</tr>\n<tr>\n<td>NUMA-aware allocation</td>\n<td>Excluded</td>\n<td>Platform-specific and hardware-dependent</td>\n</tr>\n<tr>\n<td>Cache-line alignment</td>\n<td>Included</td>\n<td>Directly affects lock-free algorithm performance</td>\n</tr>\n<tr>\n<td>Padding for false sharing</td>\n<td>Included</td>\n<td>Essential for multi-threaded correctness</td>\n</tr>\n</tbody></table>\n<h3 id=\"language-specific-optimizations\">Language-Specific Optimizations</h3>\n<p>Our implementation will focus on portable algorithms rather than language-specific or platform-specific optimizations. We will NOT take advantage of:</p>\n<ul>\n<li><strong>Platform-specific atomic operations</strong>: Advanced hardware features like transactional memory or wide CAS operations</li>\n<li><strong>Compiler intrinsics</strong>: Assembly-level optimizations or platform-specific instruction sequences</li>\n<li><strong>Language-specific features</strong>: Generics, macros, or advanced type systems that would make the code non-portable</li>\n<li><strong>Operating system features</strong>: Specialized synchronization primitives or memory management APIs</li>\n</ul>\n<p>This constraint ensures that our implementations can be understood and adapted across multiple programming languages and platforms. The focus remains on algorithmic techniques rather than low-level optimization tricks.</p>\n<p>⚠️ <strong>Pitfall: Over-Engineering for Performance</strong>\nMany lock-free programming projects fail because they attempt to optimize for every possible performance scenario before establishing basic correctness. This leads to implementations that are complex, difficult to test, and often contain subtle bugs. Our approach prioritizes correctness and clarity over maximum performance, recognizing that a working lock-free algorithm is far more valuable than a theoretically optimal algorithm that contains race conditions or memory safety bugs.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This subsection provides concrete technical recommendations for implementing the goals and managing the scope boundaries defined above.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Atomic Operations</td>\n<td>Language built-ins (Python threading, Go sync/atomic)</td>\n<td>Hardware-specific intrinsics</td>\n</tr>\n<tr>\n<td>Memory Ordering</td>\n<td>Sequential consistency (simplest)</td>\n<td>Explicit acquire/release ordering</td>\n</tr>\n<tr>\n<td>Testing Framework</td>\n<td>Standard unit testing</td>\n<td>Property-based testing with linearizability checkers</td>\n</tr>\n<tr>\n<td>Benchmarking</td>\n<td>Simple timing loops</td>\n<td>Statistical performance analysis with confidence intervals</td>\n</tr>\n<tr>\n<td>Memory Debugging</td>\n<td>Standard leak detection</td>\n<td>Specialized concurrent memory validators</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-project-structure\">Recommended Project Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>lock-free-library/\n├── src/\n│   ├── atomic/\n│   │   ├── __init__.py\n│   │   ├── primitives.py      ← compare_and_swap, atomic loads/stores\n│   │   ├── memory_ordering.py ← RELAXED, ACQUIRE, RELEASE constants\n│   │   └── tagged_pointer.py  ← ABA prevention helpers\n│   ├── data_structures/\n│   │   ├── __init__.py\n│   │   ├── stack.py          ← Treiber stack implementation\n│   │   ├── queue.py          ← Michael-Scott queue implementation  \n│   │   └── hashmap.py        ← Split-ordered hash map\n│   ├── memory/\n│   │   ├── __init__.py\n│   │   ├── hazard_pointers.py ← Memory reclamation system\n│   │   └── node_allocation.py ← Node lifecycle management\n│   └── testing/\n│       ├── __init__.py\n│       ├── linearizability.py ← Correctness verification\n│       └── stress_testing.py  ← Concurrent performance tests\n├── tests/\n│   ├── unit/              ← Component isolation tests\n│   ├── integration/       ← Cross-component interaction tests\n│   └── performance/       ← Benchmarks and scaling tests\n└── examples/\n    ├── basic_usage.py     ← Simple API demonstration\n    └── benchmarks.py      ← Performance comparison examples</code></pre></div>\n\n<h4 id=\"goal-verification-framework\">Goal Verification Framework</h4>\n<p>The following framework helps verify that implementation meets our defined goals:</p>\n<p><strong>Thread Safety Verification:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> verify_thread_safety</span><span style=\"color:#E1E4E8\">(data_structure, operation_count</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">10000</span><span style=\"color:#E1E4E8\">, thread_count</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Verifies thread safety by running concurrent operations and checking invariants.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    TODO</span><span style=\"color:#9ECBFF\">: Launch thread_count threads, each performing operation_count operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    TODO</span><span style=\"color:#9ECBFF\">: Mix of push/pop for stack, enqueue/dequeue for queue</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    TODO</span><span style=\"color:#9ECBFF\">: Verify no data corruption: all elements accounted for, no duplicates</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    TODO</span><span style=\"color:#9ECBFF\">: Check structural invariants: no dangling pointers, valid reference counts</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    TODO</span><span style=\"color:#9ECBFF\">: Measure and report any data races or assertion failures</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> stress_test_data_structure</span><span style=\"color:#E1E4E8\">(data_structure_class, duration_seconds</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">60</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Extended stress testing under high contention scenarios.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    TODO</span><span style=\"color:#9ECBFF\">: Create data structure instance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    TODO</span><span style=\"color:#9ECBFF\">: Launch maximum number of threads supported by system</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    TODO</span><span style=\"color:#9ECBFF\">: Run mixed workload (80% reads, 20% writes) for duration_seconds</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    TODO</span><span style=\"color:#9ECBFF\">: Monitor for crashes, hangs, or corrupted state</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    TODO</span><span style=\"color:#9ECBFF\">: Record throughput and latency percentiles</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    TODO</span><span style=\"color:#9ECBFF\">: Compare against baseline mutex-protected implementation</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<p><strong>Performance Goal Measurement:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PerformanceGoalValidator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validates that implementation meets specified performance goals.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> measure_throughput_scaling</span><span style=\"color:#E1E4E8\">(self, data_structure, max_threads</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">32</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Test throughput from 1 to max_threads</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Record operations per second at each thread count</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Verify throughput increases (or at least doesn't decrease dramatically)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Generate scaling chart and identify saturation point</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> measure_latency_distribution</span><span style=\"color:#E1E4E8\">(self, data_structure, sample_count</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">100000</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Record timing for sample_count individual operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Calculate 50th, 90th, 99th, and 99.9th percentile latencies</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Verify latency goals: 99th percentile &#x3C; 100 microseconds</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Check for outliers that might indicate algorithmic problems</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Scope Boundary Enforcement:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Code review checklist to prevent scope creep:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">FORBIDDEN_PATTERNS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"import gc\"</span><span style=\"color:#E1E4E8\">,          </span><span style=\"color:#6A737D\"># No garbage collection dependence</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"threading.Lock\"</span><span style=\"color:#E1E4E8\">,     </span><span style=\"color:#6A737D\"># No blocking synchronization primitives  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"time.sleep\"</span><span style=\"color:#E1E4E8\">,         </span><span style=\"color:#6A737D\"># No blocking operations in data structure code</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"malloc.h\"</span><span style=\"color:#E1E4E8\">,          </span><span style=\"color:#6A737D\"># No custom memory management beyond hazard pointers</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"__platform__\"</span><span style=\"color:#E1E4E8\">,      </span><span style=\"color:#6A737D\"># No platform-specific optimizations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> check_scope_compliance</span><span style=\"color:#E1E4E8\">(source_files):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    TODO</span><span style=\"color:#9ECBFF\">: Scan source files for forbidden patterns</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    TODO</span><span style=\"color:#9ECBFF\">: Flag any imports or code that violates non-goals</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    TODO</span><span style=\"color:#9ECBFF\">: Ensure implementation stays within defined boundaries</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>After Milestone 1 (Atomic Operations):</strong></p>\n<ul>\n<li>Run: <code>python -m pytest tests/unit/atomic/</code> </li>\n<li>Expected: All tests pass, including ABA problem demonstration</li>\n<li>Manual verification: Counter stress test with 8 threads × 10,000 increments = exactly 80,000</li>\n</ul>\n<p><strong>After Milestone 2 (Lock-free Stack):</strong></p>\n<ul>\n<li>Run: <code>python examples/stack_benchmark.py</code></li>\n<li>Expected: Stack throughput &gt; 1M ops/sec on 4 cores, linearizability verified</li>\n<li>Manual verification: No crashes during 60-second high-contention stress test</li>\n</ul>\n<p><strong>After Milestone 3 (Lock-free Queue):</strong></p>\n<ul>\n<li>Run: <code>python tests/integration/queue_fifo_test.py</code></li>\n<li>Expected: Perfect FIFO ordering preserved across 100,000 concurrent operations</li>\n<li>Manual verification: Queue performance comparable to or better than stack</li>\n</ul>\n<p><strong>After Milestone 4 (Hazard Pointers):</strong></p>\n<ul>\n<li>Run: <code>python tests/memory/leak_detection.py</code></li>\n<li>Expected: No memory leaks, all retired nodes eventually reclaimed</li>\n<li>Manual verification: Memory usage remains bounded during extended operation</li>\n</ul>\n<p><strong>After Milestone 5 (Hash Map):</strong></p>\n<ul>\n<li>Run: <code>python tests/performance/scaling_benchmark.py</code></li>\n<li>Expected: Hash map scales to at least 16 threads with good throughput</li>\n<li>Manual verification: Resize operations complete without corrupting existing data</li>\n</ul>\n<h2 id=\"high-level-architecture\">High-Level Architecture</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (this section establishes the overall system design and progression strategy for building each component incrementally)</p>\n</blockquote>\n<p>The architecture of our lock-free data structure library follows a carefully designed layered approach that mirrors how concurrent programming knowledge builds from simple atomic operations to complex data structures. Like constructing a skyscraper, each layer provides a stable foundation for the next, with dependencies flowing upward and abstractions hiding complexity downward.</p>\n<h3 id=\"component-layers\">Component Layers</h3>\n<p>Our lock-free data structure library employs a <strong>three-tier architecture</strong> that separates concerns and establishes clear dependency relationships between components. This layered design ensures that complex data structures can be built on proven atomic primitives while maintaining clean interfaces and testability at each level.</p>\n<p><img src=\"/api/project/lock-free-structures/architecture-doc/asset?path=diagrams%2Fsystem-architecture.svg\" alt=\"System Component Architecture\"></p>\n<h4 id=\"layer-1-atomic-operations-foundation\">Layer 1: Atomic Operations Foundation</h4>\n<p>The foundation layer provides the primitive building blocks that all lock-free algorithms depend upon. Think of this layer as the basic tools in a craftsman&#39;s workshop - hammers, screwdrivers, and measuring instruments that are used to build more complex creations. Without reliable atomic operations, lock-free data structures would be impossible to implement correctly.</p>\n<p>This layer encapsulates the platform-specific details of atomic memory operations and presents a clean, consistent interface to higher layers. The atomic operations foundation handles the intricate details of memory ordering semantics, cache coherence protocols, and hardware-specific instruction sequences that implement compare-and-swap operations.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Responsibility</th>\n<th>Key Types</th>\n<th>Primary Methods</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>AtomicReference</code></td>\n<td>Single-word atomic operations with versioning</td>\n<td><code>AtomicReference</code>, <code>TaggedPointer</code></td>\n<td><code>compare_and_swap()</code>, <code>load()</code>, <code>store()</code></td>\n</tr>\n<tr>\n<td><code>MemoryOrdering</code></td>\n<td>Memory consistency model enforcement</td>\n<td><code>MemoryOrdering</code> constants</td>\n<td>Ordering constraint specifications</td>\n</tr>\n<tr>\n<td>Memory Barriers</td>\n<td>Instruction reordering prevention</td>\n<td>Fence primitives</td>\n<td><code>acquire_fence()</code>, <code>release_fence()</code></td>\n</tr>\n<tr>\n<td>ABA Detection</td>\n<td>Pointer reuse problem mitigation</td>\n<td>Version counters, tagged pointers</td>\n<td><code>increment_version()</code>, <code>extract_pointer()</code></td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Atomic Wrapper Layer</strong></p>\n<ul>\n<li><strong>Context</strong>: Raw hardware atomics are platform-specific and error-prone to use directly in data structure implementations</li>\n<li><strong>Options Considered</strong>: Direct hardware atomic usage, thin wrapper layer, full abstraction with runtime dispatch</li>\n<li><strong>Decision</strong>: Thin wrapper layer with compile-time specialization for memory ordering</li>\n<li><strong>Rationale</strong>: Provides safety and consistency without performance overhead while maintaining access to all memory ordering options</li>\n<li><strong>Consequences</strong>: Slightly more complex implementation but dramatically reduces bugs in higher-layer code and improves portability</li>\n</ul>\n</blockquote>\n<p>The atomic operations layer must handle the <strong>ABA problem</strong> - a fundamental challenge where a memory location changes from value A to B and back to A between a thread&#39;s read and subsequent compare-and-swap operation. The <code>TaggedPointer</code> type solves this by combining a pointer with a monotonically increasing version counter, ensuring that even if a pointer value is reused, the tag will differ.</p>\n<table>\n<thead>\n<tr>\n<th>Memory Ordering</th>\n<th>Use Case</th>\n<th>Performance</th>\n<th>Guarantees</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>RELAXED</code></td>\n<td>Counters, statistics</td>\n<td>Highest</td>\n<td>No ordering constraints</td>\n</tr>\n<tr>\n<td><code>ACQUIRE</code></td>\n<td>Loading shared pointers</td>\n<td>High</td>\n<td>Prevents later operations from moving before</td>\n</tr>\n<tr>\n<td><code>RELEASE</code></td>\n<td>Publishing shared data</td>\n<td>High</td>\n<td>Prevents earlier operations from moving after</td>\n</tr>\n<tr>\n<td><code>SEQ_CST</code></td>\n<td>Critical correctness paths</td>\n<td>Lowest</td>\n<td>Total global ordering</td>\n</tr>\n</tbody></table>\n<h4 id=\"layer-2-lock-free-data-structures\">Layer 2: Lock-free Data Structures</h4>\n<p>The data structures layer implements the core concurrent algorithms that provide familiar collection interfaces without using locks. This layer transforms the low-level atomic operations into higher-level abstractions that application developers can use confidently. Think of this as the furniture built by a skilled carpenter using the basic tools - each piece serves a specific purpose and hides the complexity of its construction.</p>\n<p>Each data structure in this layer maintains specific invariants and provides linearizability guarantees, meaning that despite concurrent operations from multiple threads, the data structure appears to behave as if operations occur atomically at specific points in time.</p>\n<table>\n<thead>\n<tr>\n<th>Data Structure</th>\n<th>Algorithm</th>\n<th>Key Innovation</th>\n<th>Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>TreiberStack</code></td>\n<td>Lock-free stack</td>\n<td>Single CAS on top pointer</td>\n<td>O(1) per operation</td>\n</tr>\n<tr>\n<td><code>MichaelScottQueue</code></td>\n<td>Lock-free FIFO queue</td>\n<td>Dual pointers with helping</td>\n<td>O(1) per operation</td>\n</tr>\n<tr>\n<td><code>SplitOrderedHashMap</code></td>\n<td>Lock-free hash table</td>\n<td>Recursive bucket splitting</td>\n<td>O(1) average per operation</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Treiber Stack as First Data Structure</strong></p>\n<ul>\n<li><strong>Context</strong>: Need a simple lock-free data structure to demonstrate atomic operations usage</li>\n<li><strong>Options Considered</strong>: Lock-free stack, lock-free queue, lock-free list</li>\n<li><strong>Decision</strong>: Treiber stack implementation first</li>\n<li><strong>Rationale</strong>: Single-pointer CAS operations are simpler to understand than dual-pointer algorithms, and stack operations have clear linearization points</li>\n<li><strong>Consequences</strong>: Provides foundation concepts for more complex structures but doesn&#39;t demonstrate helping mechanisms</li>\n</ul>\n</blockquote>\n<p>The <strong>Treiber stack</strong> serves as the simplest lock-free data structure, using a single <code>compare_and_swap</code> operation on the top-of-stack pointer. Push operations prepend nodes atomically, while pop operations remove the top node, both retrying if another thread modified the stack concurrently.</p>\n<table>\n<thead>\n<tr>\n<th>Operation</th>\n<th>Linearization Point</th>\n<th>CAS Target</th>\n<th>Retry Condition</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>push()</code></td>\n<td>Successful CAS of new top</td>\n<td><code>top</code> pointer</td>\n<td>Another thread changed top</td>\n</tr>\n<tr>\n<td><code>pop()</code></td>\n<td>Successful CAS of new top</td>\n<td><code>top</code> pointer</td>\n<td>Stack empty or top changed</td>\n</tr>\n</tbody></table>\n<p>The <strong>Michael-Scott queue</strong> demonstrates more advanced techniques with its dual-pointer design and helping mechanism. The algorithm maintains separate head and tail pointers, with a dummy sentinel node that simplifies empty queue handling. When threads observe that the tail pointer is lagging behind the actual end of the queue, they help advance it before attempting their own operations.</p>\n<table>\n<thead>\n<tr>\n<th>Queue State</th>\n<th>Head Points To</th>\n<th>Tail Points To</th>\n<th>Invariant</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Empty</td>\n<td>Sentinel node</td>\n<td>Sentinel node</td>\n<td><code>head == tail</code></td>\n</tr>\n<tr>\n<td>Single element</td>\n<td>Sentinel node</td>\n<td>Data node</td>\n<td><code>head-&gt;next == tail</code></td>\n</tr>\n<tr>\n<td>Multiple elements</td>\n<td>Sentinel node</td>\n<td>Last or second-to-last</td>\n<td><code>tail</code> at or near end</td>\n</tr>\n</tbody></table>\n<h4 id=\"layer-3-memory-reclamation-management\">Layer 3: Memory Reclamation Management</h4>\n<p>The memory management layer solves the critical problem of when it&#39;s safe to deallocate memory in a lock-free environment. Traditional reference counting doesn&#39;t work because incrementing and decrementing references atomically while accessing the data requires multiple atomic operations, breaking the lock-free property. This layer implements <strong>hazard pointers</strong>, which provide a non-blocking solution to safe memory reclamation.</p>\n<p>Think of hazard pointers like safety signs at a construction site. Before a worker enters a dangerous area, they post a sign indicating their presence. The demolition crew checks for these signs before bringing down any structures. Similarly, threads announce their intention to access shared nodes through hazard pointers, and the memory reclamation system checks these announcements before freeing memory.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Purpose</th>\n<th>Key Operations</th>\n<th>Thread Safety</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>HazardPointer</code></td>\n<td>Per-thread protection slots</td>\n<td><code>protect()</code>, <code>release()</code></td>\n<td>Thread-local access</td>\n</tr>\n<tr>\n<td><code>RetirementList</code></td>\n<td>Deferred deletion queue</td>\n<td><code>retire()</code>, <code>scan()</code></td>\n<td>Lock-free append/scan</td>\n</tr>\n<tr>\n<td><code>MemoryReclaimer</code></td>\n<td>Batch reclamation coordinator</td>\n<td><code>reclaim_batch()</code></td>\n<td>Global coordination</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Hazard Pointers Over Epoch-Based Reclamation</strong></p>\n<ul>\n<li><strong>Context</strong>: Need safe memory reclamation without blocking or unbounded memory growth</li>\n<li><strong>Options Considered</strong>: Hazard pointers, epoch-based reclamation, reference counting</li>\n<li><strong>Decision</strong>: Hazard pointers with per-thread retirement lists</li>\n<li><strong>Rationale</strong>: Provides deterministic memory reclamation without requiring global synchronization epochs, and integrates cleanly with existing data structure operations</li>\n<li><strong>Consequences</strong>: Requires explicit protect/release calls but provides stronger progress guarantees than epoch-based approaches</li>\n</ul>\n</blockquote>\n<p>The hazard pointer protocol follows a specific sequence for safe memory access:</p>\n<ol>\n<li><strong>Protection Phase</strong>: Thread loads a pointer to a shared node and immediately announces this pointer in its hazard pointer slot</li>\n<li><strong>Validation Phase</strong>: Thread re-reads the original pointer location to ensure the node hasn&#39;t been removed and replaced</li>\n<li><strong>Access Phase</strong>: Thread safely accesses the protected node&#39;s data and follows next pointers</li>\n<li><strong>Release Phase</strong>: Thread clears its hazard pointer slot when finished with the node</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Hazard Pointer State</th>\n<th>Protected Nodes</th>\n<th>Retirement List</th>\n<th>Reclamation Status</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>No hazards active</td>\n<td>None</td>\n<td>Growing</td>\n<td>Safe to reclaim all</td>\n</tr>\n<tr>\n<td>Multiple hazards</td>\n<td>Currently accessed</td>\n<td>Blocked items</td>\n<td>Partial reclamation</td>\n</tr>\n<tr>\n<td>Scan in progress</td>\n<td>All hazards</td>\n<td>Filtered</td>\n<td>Batch reclamation</td>\n</tr>\n</tbody></table>\n<h3 id=\"recommended-module-structure\">Recommended Module Structure</h3>\n<p>The module organization reflects the layered architecture while providing clear boundaries between components and supporting incremental development. This structure allows developers to work on one layer at a time while maintaining clean dependencies and avoiding circular imports.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>lockfree_library/\n├── atomic/\n│   ├── __init__.py              # Atomic operations public interface\n│   ├── operations.py            # AtomicReference, compare_and_swap\n│   ├── memory_ordering.py       # MemoryOrdering constants and semantics\n│   ├── tagged_pointer.py        # TaggedPointer for ABA prevention\n│   └── tests/\n│       ├── test_atomic_ops.py   # Basic atomic operation correctness\n│       ├── test_memory_order.py # Memory ordering validation\n│       └── test_aba_detection.py # ABA problem demonstration\n├── structures/\n│   ├── __init__.py              # Data structures public interface\n│   ├── stack.py                 # TreiberStack implementation\n│   ├── queue.py                 # MichaelScottQueue implementation\n│   ├── hashmap.py               # SplitOrderedHashMap implementation\n│   ├── node.py                  # Shared Node type definitions\n│   └── tests/\n│       ├── test_stack.py        # Stack correctness and linearizability\n│       ├── test_queue.py        # Queue FIFO ordering verification\n│       ├── test_hashmap.py      # Hash map concurrent operations\n│       └── stress_tests.py      # High-contention performance tests\n├── memory/\n│   ├── __init__.py              # Memory reclamation public interface\n│   ├── hazard_pointers.py       # HazardPointer registry and protocol\n│   ├── retirement.py            # RetirementList and scanning logic\n│   ├── reclamation.py           # MemoryReclaimer coordination\n│   └── tests/\n│       ├── test_hazard_ptrs.py  # Hazard pointer correctness\n│       ├── test_retirement.py   # Retirement list functionality\n│       └── test_integration.py  # End-to-end memory safety\n├── verification/\n│   ├── __init__.py              # Testing utilities public interface\n│   ├── linearizability.py       # LinearizabilityChecker implementation\n│   ├── stress_testing.py        # Concurrent stress test framework\n│   ├── operation_tracker.py     # Operation recording and analysis\n│   └── benchmarks/\n│       ├── throughput_bench.py  # Operations per second measurement\n│       ├── contention_bench.py  # High-thread-count scenarios\n│       └── comparison_bench.py  # Lock-free vs lock-based comparison\n└── examples/\n    ├── basic_usage.py           # Simple stack and queue examples\n    ├── producer_consumer.py     # Multi-threaded producer/consumer\n    ├── concurrent_counter.py    # Atomic counter demonstration\n    └── hash_map_demo.py         # Concurrent hash map operations</code></pre></div>\n\n<table>\n<thead>\n<tr>\n<th>Module</th>\n<th>Dependencies</th>\n<th>Public Interface</th>\n<th>Internal Components</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>atomic/</code></td>\n<td>System atomics</td>\n<td><code>AtomicReference</code>, <code>TaggedPointer</code>, <code>MemoryOrdering</code></td>\n<td>Platform wrappers, ABA detection</td>\n</tr>\n<tr>\n<td><code>structures/</code></td>\n<td><code>atomic/</code></td>\n<td><code>TreiberStack</code>, <code>MichaelScottQueue</code>, <code>SplitOrderedHashMap</code></td>\n<td>Node management, algorithms</td>\n</tr>\n<tr>\n<td><code>memory/</code></td>\n<td><code>atomic/</code></td>\n<td><code>HazardPointer</code>, <code>retire()</code>, <code>protect()</code></td>\n<td>Scanning, reclamation batching</td>\n</tr>\n<tr>\n<td><code>verification/</code></td>\n<td><code>atomic/</code>, <code>structures/</code></td>\n<td><code>LinearizabilityChecker</code>, <code>stress_test_data_structure</code></td>\n<td>History recording, analysis</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Separate Verification Module</strong></p>\n<ul>\n<li><strong>Context</strong>: Lock-free data structures require specialized testing approaches that differ from unit testing</li>\n<li><strong>Options Considered</strong>: Inline test utilities, separate testing module, external test framework</li>\n<li><strong>Decision</strong>: Dedicated verification module with linearizability checking</li>\n<li><strong>Rationale</strong>: Concurrent correctness testing is complex enough to warrant its own module, and these utilities will be reused across all data structures</li>\n<li><strong>Consequences</strong>: Additional module complexity but provides reusable testing infrastructure for all components</li>\n</ul>\n</blockquote>\n<p>The module structure enforces dependency constraints through Python&#39;s import system. The <code>atomic/</code> module has no internal dependencies on other library modules, <code>structures/</code> depends only on <code>atomic/</code>, and <code>memory/</code> depends on <code>atomic/</code> but not <code>structures/</code>. This prevents circular dependencies and ensures that each layer can be developed and tested independently.</p>\n<table>\n<thead>\n<tr>\n<th>Import Direction</th>\n<th>Allowed</th>\n<th>Forbidden</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>structures/</code> → <code>atomic/</code></td>\n<td>✓</td>\n<td></td>\n<td>Data structures need atomic primitives</td>\n</tr>\n<tr>\n<td><code>memory/</code> → <code>atomic/</code></td>\n<td>✓</td>\n<td></td>\n<td>Hazard pointers need atomic operations</td>\n</tr>\n<tr>\n<td><code>memory/</code> → <code>structures/</code></td>\n<td></td>\n<td>✗</td>\n<td>Memory management should be structure-agnostic</td>\n</tr>\n<tr>\n<td><code>atomic/</code> → <code>structures/</code></td>\n<td></td>\n<td>✗</td>\n<td>Foundation layer shouldn&#39;t depend on higher layers</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-progression-strategy\">Implementation Progression Strategy</h3>\n<p>Building lock-free data structures requires a systematic progression from simple concepts to complex algorithms. This strategy minimizes the cognitive load at each step while ensuring that learners understand the fundamental principles before applying them to challenging scenarios. The progression mirrors how concurrent programming expertise develops naturally.</p>\n<h4 id=\"phase-1-atomic-operations-mastery-milestone-1\">Phase 1: Atomic Operations Mastery (Milestone 1)</h4>\n<p>The journey begins with understanding atomic operations as the fundamental building blocks. Think of this phase like learning to use basic tools before attempting to build furniture - you must understand how each tool works and when to use it before combining them into complex constructions.</p>\n<p><strong>Week 1-2 Focus Areas:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Concept</th>\n<th>Learning Goal</th>\n<th>Validation Method</th>\n<th>Common Mistakes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>compare_and_swap</code></td>\n<td>Implement retry loops correctly</td>\n<td>CAS counter increment test</td>\n<td>Infinite spinning without backoff</td>\n</tr>\n<tr>\n<td>Memory ordering</td>\n<td>Choose appropriate ordering for use case</td>\n<td>Ordering semantics quiz</td>\n<td>Using <code>RELAXED</code> everywhere</td>\n</tr>\n<tr>\n<td>ABA problem</td>\n<td>Recognize and prevent ABA scenarios</td>\n<td>Demonstrate ABA with test case</td>\n<td>Ignoring version counters</td>\n</tr>\n<tr>\n<td>Memory barriers</td>\n<td>Understand visibility guarantees</td>\n<td>Multi-thread visibility test</td>\n<td>Assuming immediate visibility</td>\n</tr>\n</tbody></table>\n<p>The first milestone establishes the mental model that <strong>atomic operations are indivisible transactions</strong> - they either complete entirely or not at all, with no intermediate states visible to other threads. Learners implement basic atomic wrappers and experiment with different memory ordering guarantees to understand their performance and correctness trade-offs.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Example progression: Start with simple atomic counter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">atomic_counter </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> AtomicReference(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Progress to CAS-based operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> increment_counter</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    while</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        current </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> atomic_counter.load(</span><span style=\"color:#79B8FF\">ACQUIRE</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> atomic_counter.compare_and_swap(current, current </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> current</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Advance to ABA problem demonstration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> demonstrate_aba_problem</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Show how naive CAS can succeed incorrectly</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<blockquote>\n<p>The critical insight in this phase is understanding that memory ordering is not just a performance optimization - it&#39;s a correctness requirement. Using <code>RELAXED</code> ordering everywhere might perform well but can lead to subtle bugs that only manifest under specific timing conditions.</p>\n</blockquote>\n<p>⚠️ <strong>Pitfall: Assuming Sequential Consistency</strong>\nNew lock-free programmers often assume that all operations are sequentially consistent, leading to code that works on strongly-ordered architectures like x86 but fails on weakly-ordered systems like ARM. Always explicitly specify memory ordering requirements rather than relying on defaults.</p>\n<h4 id=\"phase-2-single-data-structure-implementation-milestones-2-3\">Phase 2: Single Data Structure Implementation (Milestones 2-3)</h4>\n<p>With atomic operations mastered, learners progress to implementing their first complete lock-free data structure. The <strong>Treiber stack</strong> serves as the ideal introduction because it requires only single-word CAS operations and has clear linearization points, making correctness easier to reason about.</p>\n<p><strong>Week 3-4 Focus Areas:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Data Structure Component</th>\n<th>Implementation Challenge</th>\n<th>Verification Method</th>\n<th>Key Insight</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Stack node structure</td>\n<td>Atomic next pointer design</td>\n<td>Node linking test</td>\n<td>Next pointers must be atomic</td>\n</tr>\n<tr>\n<td>Push operation</td>\n<td>CAS retry with backoff</td>\n<td>Concurrent push stress test</td>\n<td>Linearization at CAS success</td>\n</tr>\n<tr>\n<td>Pop operation</td>\n<td>Handle empty stack correctly</td>\n<td>Empty stack edge cases</td>\n<td>Check for null before CAS</td>\n</tr>\n<tr>\n<td>ABA prevention</td>\n<td>Tagged pointer integration</td>\n<td>ABA demonstration test</td>\n<td>Version must increment on reuse</td>\n</tr>\n</tbody></table>\n<p>The stack implementation teaches the fundamental pattern of <strong>CAS retry loops</strong> that appears in all lock-free algorithms:</p>\n<ol>\n<li><strong>Load Phase</strong>: Read the current state of shared memory</li>\n<li><strong>Compute Phase</strong>: Calculate the desired new state based on current state</li>\n<li><strong>Validate Phase</strong>: Attempt to atomically update from old state to new state</li>\n<li><strong>Retry Phase</strong>: If CAS failed, repeat from step 1 with exponential backoff</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Stack Operation</th>\n<th>Linearization Point</th>\n<th>Success Condition</th>\n<th>Failure Recovery</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>push(data)</code></td>\n<td>CAS success on top pointer</td>\n<td><code>top</code> unchanged since load</td>\n<td>Retry with new top value</td>\n</tr>\n<tr>\n<td><code>pop()</code></td>\n<td>CAS success on top pointer</td>\n<td>Stack not empty, top unchanged</td>\n<td>Return empty indicator</td>\n</tr>\n</tbody></table>\n<p>The <strong>Michael-Scott queue</strong> follows as the second data structure, introducing the concepts of dual pointers, helping mechanisms, and sentinel nodes. This progression teaches how lock-free algorithms can coordinate multiple pointers atomically through careful sequencing of CAS operations.</p>\n<p><strong>Week 5-6 Focus Areas:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Queue Component</th>\n<th>Advanced Concept</th>\n<th>Implementation Challenge</th>\n<th>Teaching Goal</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Dual pointers</td>\n<td>Head/tail coordination</td>\n<td>Avoid contention between enqueue/dequeue</td>\n<td>Separate access patterns</td>\n</tr>\n<tr>\n<td>Sentinel node</td>\n<td>Empty queue simplification</td>\n<td>Initialize and maintain dummy node</td>\n<td>Eliminate special cases</td>\n</tr>\n<tr>\n<td>Helping mechanism</td>\n<td>Progress guarantee</td>\n<td>Advance lagging tail pointer</td>\n<td>Cooperative algorithms</td>\n</tr>\n<tr>\n<td>FIFO ordering</td>\n<td>Linearizability proof</td>\n<td>Maintain insertion order</td>\n<td>Correctness verification</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Stack Before Queue Implementation Order</strong></p>\n<ul>\n<li><strong>Context</strong>: Both data structures teach essential lock-free patterns but have different complexity levels</li>\n<li><strong>Options Considered</strong>: Start with queue (more practical), start with stack (simpler), implement simultaneously</li>\n<li><strong>Decision</strong>: Implement Treiber stack first, then Michael-Scott queue</li>\n<li><strong>Rationale</strong>: Single-pointer CAS is conceptually simpler than dual-pointer coordination, and success with stack builds confidence for queue complexity</li>\n<li><strong>Consequences</strong>: Learners gain confidence with simpler algorithms before tackling helping mechanisms and dual-pointer coordination</li>\n</ul>\n</blockquote>\n<h4 id=\"phase-3-memory-reclamation-integration-milestone-4\">Phase 3: Memory Reclamation Integration (Milestone 4)</h4>\n<p>The third phase addresses the critical challenge of safe memory management in lock-free environments. <strong>Hazard pointers</strong> provide the solution, but integrating them with existing data structure operations requires careful attention to the protection protocol and reclamation timing.</p>\n<p><strong>Week 7-8 Focus Areas:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Memory Management Aspect</th>\n<th>Integration Challenge</th>\n<th>Verification Method</th>\n<th>Safety Guarantee</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Hazard pointer protocol</td>\n<td>Protect nodes during access</td>\n<td>Use-after-free detection</td>\n<td>No premature deallocation</td>\n</tr>\n<tr>\n<td>Retirement list management</td>\n<td>Defer deletion safely</td>\n<td>Memory leak monitoring</td>\n<td>Bounded memory growth</td>\n</tr>\n<tr>\n<td>Scanning and reclamation</td>\n<td>Batch free operations</td>\n<td>Reclamation efficiency test</td>\n<td>Progress without blocking</td>\n</tr>\n<tr>\n<td>Thread lifecycle</td>\n<td>Cleanup on exit</td>\n<td>Thread termination test</td>\n<td>No resource leaks</td>\n</tr>\n</tbody></table>\n<p>The hazard pointer integration transforms data structure operations from simple atomic updates to multi-phase protocols:</p>\n<ol>\n<li><strong>Acquisition Phase</strong>: Load pointer and immediately protect it via hazard pointer</li>\n<li><strong>Validation Phase</strong>: Re-check that the pointer is still valid in its original location</li>\n<li><strong>Access Phase</strong>: Safely dereference and traverse the protected node</li>\n<li><strong>Release Phase</strong>: Clear hazard pointer when access is complete</li>\n<li><strong>Retirement Phase</strong>: Add removed nodes to retirement list instead of immediate deletion</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Integration Point</th>\n<th>Stack Modification</th>\n<th>Queue Modification</th>\n<th>Complexity Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Node access</td>\n<td>Protect before dereference</td>\n<td>Protect head and tail loads</td>\n<td>2x overhead per access</td>\n</tr>\n<tr>\n<td>Node removal</td>\n<td>Retire instead of delete</td>\n<td>Retire dequeued nodes</td>\n<td>Deferred reclamation</td>\n</tr>\n<tr>\n<td>Thread cleanup</td>\n<td>Release all hazards on exit</td>\n<td>Clear retirement list</td>\n<td>Shutdown protocol</td>\n</tr>\n</tbody></table>\n<h4 id=\"phase-4-advanced-data-structures-milestone-5\">Phase 4: Advanced Data Structures (Milestone 5)</h4>\n<p>The final phase tackles the <strong>split-ordered hash map</strong>, which combines all previous concepts while introducing new challenges like incremental resizing and bucket coordination. This represents the culmination of lock-free programming skills.</p>\n<p><strong>Week 9-10 Focus Areas:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Hash Map Component</th>\n<th>Advanced Technique</th>\n<th>Implementation Challenge</th>\n<th>Mastery Goal</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Split-ordered lists</td>\n<td>Logical hash ordering</td>\n<td>Maintain sorted order during splits</td>\n<td>Understand recursive splitting</td>\n</tr>\n<tr>\n<td>Incremental resizing</td>\n<td>Lock-free migration</td>\n<td>Migrate without blocking operations</td>\n<td>Non-blocking growth</td>\n</tr>\n<tr>\n<td>Bucket coordination</td>\n<td>Multiple list management</td>\n<td>Initialize parent before child</td>\n<td>Dependency ordering</td>\n</tr>\n<tr>\n<td>Reverse bit ordering</td>\n<td>Preserve structure during splits</td>\n<td>Calculate correct insertion points</td>\n<td>Mathematical precision</td>\n</tr>\n</tbody></table>\n<p>The hash map implementation demonstrates how complex lock-free algorithms compose simpler techniques:</p>\n<ul>\n<li><strong>Atomic operations</strong> for bucket array updates and sentinel node management</li>\n<li><strong>CAS retry loops</strong> for insertion and deletion in sorted bucket lists</li>\n<li><strong>Hazard pointers</strong> for safe traversal of bucket chains during concurrent modifications</li>\n<li><strong>Helping mechanisms</strong> for assisting with bucket initialization and resizing operations</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Phase Completion</th>\n<th>Demonstrated Skills</th>\n<th>Assessment Method</th>\n<th>Readiness Indicator</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Phase 1</td>\n<td>Atomic operation mastery</td>\n<td>CAS-based counter stress test</td>\n<td>No lost increments under contention</td>\n</tr>\n<tr>\n<td>Phase 2</td>\n<td>Data structure implementation</td>\n<td>Linearizability verification</td>\n<td>Stack/queue correctness under load</td>\n</tr>\n<tr>\n<td>Phase 3</td>\n<td>Memory management</td>\n<td>No use-after-free errors</td>\n<td>Clean hazard pointer integration</td>\n</tr>\n<tr>\n<td>Phase 4</td>\n<td>Advanced algorithm composition</td>\n<td>Hash map performance benchmark</td>\n<td>Competitive with lock-based alternatives</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The progression strategy translates into concrete development steps with specific checkpoints and validation criteria at each phase. This guidance provides the technical foundation needed to implement the layered architecture successfully.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Production Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Atomic Operations</td>\n<td>Python <code>threading</code> with locks</td>\n<td><code>ctypes</code> atomic wrappers</td>\n<td>Rust <code>std::sync::atomic</code></td>\n</tr>\n<tr>\n<td>Memory Ordering</td>\n<td>Sequential consistency only</td>\n<td>Explicit acquire/release</td>\n<td>Full memory model support</td>\n</tr>\n<tr>\n<td>Testing Framework</td>\n<td>Basic <code>unittest</code></td>\n<td><code>pytest</code> with concurrency fixtures</td>\n<td>Property-based testing with <code>hypothesis</code></td>\n</tr>\n<tr>\n<td>Performance Monitoring</td>\n<td>Manual timing</td>\n<td><code>cProfile</code> integration</td>\n<td>Dedicated benchmarking suite</td>\n</tr>\n<tr>\n<td>Memory Debugging</td>\n<td>Reference counting</td>\n<td><code>tracemalloc</code> monitoring</td>\n<td>Valgrind or AddressSanitizer</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure-for-implementation\">Recommended File Structure for Implementation</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Project organization following the layered architecture</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">lockfree_library</span><span style=\"color:#F97583\">/</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">├── atomic</span><span style=\"color:#F97583\">/</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   ├── </span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">.py</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   │   </span><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .operations </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> AtomicReference, compare_and_swap</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   │   </span><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .memory_ordering </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> MemoryOrdering, </span><span style=\"color:#79B8FF\">RELAXED</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">ACQUIRE</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">RELEASE</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">SEQ_CST</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   │   </span><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .tagged_pointer </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TaggedPointer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   │</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   ├── operations.py           </span><span style=\"color:#6A737D\"># Core atomic operations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   ├── memory_ordering.py      </span><span style=\"color:#6A737D\"># Memory consistency primitives  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   ├── tagged_pointer.py       </span><span style=\"color:#6A737D\"># ABA prevention utilities</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   └── platform_specific.py   </span><span style=\"color:#6A737D\"># Hardware-specific implementations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">├── structures</span><span style=\"color:#F97583\">/</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   ├── </span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">.py</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   │   </span><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .stack </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TreiberStack</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   │   </span><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .queue </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> MichaelScottQueue  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   │   </span><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .hashmap </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> SplitOrderedHashMap</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   │</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   ├── stack.py               </span><span style=\"color:#6A737D\"># Treiber stack implementation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   ├── queue.py               </span><span style=\"color:#6A737D\"># Michael-Scott queue implementation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   ├── hashmap.py             </span><span style=\"color:#6A737D\"># Lock-free hash map implementation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   └── node.py                </span><span style=\"color:#6A737D\"># Shared node type definitions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">└── memory</span><span style=\"color:#F97583\">/</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ├── </span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">.py</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    │   </span><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .hazard_pointers </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> HazardPointer, protect, release</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    │   </span><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .reclamation </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> retire, reclaim_batch</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    │</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ├── hazard_pointers.py      </span><span style=\"color:#6A737D\"># Hazard pointer protocol</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ├── retirement.py           </span><span style=\"color:#6A737D\"># Retirement list management</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    └── reclamation.py          </span><span style=\"color:#6A737D\"># Memory reclamation coordination</span></span></code></pre></div>\n\n<h4 id=\"infrastructure-starter-code-complete-implementation\">Infrastructure Starter Code (Complete Implementation)</h4>\n<p><strong>Atomic Operations Foundation</strong> (<code>atomic/operations.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Tuple, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MemoryOrdering</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RELAXED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"relaxed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ACQUIRE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"acquire\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RELEASE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"release\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SEQ_CST</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"seq_cst\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Constants for easy import</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">RELAXED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> MemoryOrdering.</span><span style=\"color:#79B8FF\">RELAXED</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">ACQUIRE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> MemoryOrdering.</span><span style=\"color:#79B8FF\">ACQUIRE</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">RELEASE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> MemoryOrdering.</span><span style=\"color:#79B8FF\">RELEASE</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">SEQ_CST</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> MemoryOrdering.</span><span style=\"color:#79B8FF\">SEQ_CST</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AtomicReference</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Thread-safe atomic reference with compare-and-swap support.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, initial_value: Any </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> initial_value</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._version </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Lock()  </span><span style=\"color:#6A737D\"># For Python CAS simulation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> load</span><span style=\"color:#E1E4E8\">(self, ordering: MemoryOrdering </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> SEQ_CST</span><span style=\"color:#E1E4E8\">) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Atomically load the current value with specified memory ordering.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> store</span><span style=\"color:#E1E4E8\">(self, value: Any, ordering: MemoryOrdering </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> SEQ_CST</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Atomically store a new value with specified memory ordering.\"\"\"</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> value</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._version </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> compare_and_swap</span><span style=\"color:#E1E4E8\">(self, expected: Any, new_value: Any) -> Tuple[</span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Atomically compare current value with expected and swap if equal.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns (success: bool, observed_value: Any).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        If success=False, observed_value contains the actual current value.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            current </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> current </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> expected:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> new_value</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._version </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, current)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">, current)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> fetch_and_add</span><span style=\"color:#E1E4E8\">(self, increment: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Atomically increment the value and return the previous value.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            previous </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._value</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._value </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> increment</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._version </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> previous</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TaggedPointer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Pointer with version tag to prevent ABA problems.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, pointer: Any </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, tag: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.pointer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pointer</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tag </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tag</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __eq__</span><span style=\"color:#E1E4E8\">(self, other):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(other, TaggedPointer):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.pointer </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> other.pointer </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.tag </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> other.tag</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> increment_tag</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#9ECBFF\">'TaggedPointer'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create a new TaggedPointer with incremented tag.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> TaggedPointer(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.pointer, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.tag </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Global atomic operation helpers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> compare_and_swap</span><span style=\"color:#E1E4E8\">(atomic_ref: AtomicReference, expected: Any, new_value: Any) -> Tuple[</span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Helper function for CAS operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> atomic_ref.compare_and_swap(expected, new_value)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> fetch_and_add</span><span style=\"color:#E1E4E8\">(atomic_ref: AtomicReference, increment: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Helper function for atomic increment.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> atomic_ref.fetch_and_add(increment)</span></span></code></pre></div>\n\n<p><strong>Node Types</strong> (<code>structures/node.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> atomic.operations </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> AtomicReference, TaggedPointer</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Node</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Generic node for lock-free data structures.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, data: Any, next_node: Optional[</span><span style=\"color:#9ECBFF\">'Node'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use TaggedPointer to prevent ABA problems</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tagged_next </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TaggedPointer(next_node, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> next_node </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> TaggedPointer(</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.next </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> AtomicReference(tagged_next)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_next</span><span style=\"color:#E1E4E8\">(self) -> Optional[</span><span style=\"color:#9ECBFF\">'Node'</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get the next node pointer safely.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tagged </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.next.load()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> tagged.pointer </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> tagged </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> set_next</span><span style=\"color:#E1E4E8\">(self, next_node: Optional[</span><span style=\"color:#9ECBFF\">'Node'</span><span style=\"color:#E1E4E8\">], expected_tag: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Set the next node using CAS with tag increment.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        current_tagged </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.next.load()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> current_tagged.tag </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> expected_tag:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        new_tagged </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TaggedPointer(next_node, current_tag.tag </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        success, _ </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.next.compare_and_swap(current_tagged, new_tagged)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> success</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SentinelNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Node</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Special node that marks boundaries in data structures.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, key: Any </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Sentinel nodes carry no data</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.key </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> key  </span><span style=\"color:#6A737D\"># For hash map bucket boundaries</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.is_sentinel </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code-for-learner-implementation\">Core Logic Skeleton Code (For Learner Implementation)</h4>\n<p><strong>Treiber Stack Skeleton</strong> (<code>structures/stack.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> atomic.operations </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> AtomicReference, TaggedPointer, compare_and_swap</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> structures.node </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Node</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> random</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TreiberStack</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Lock-free stack using the Treiber algorithm.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Stack top starts as empty (None pointer with version 0)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.top </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> AtomicReference(TaggedPointer(</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> push</span><span style=\"color:#E1E4E8\">(self, data: Any) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Push a new element onto the top of the stack.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Uses CAS retry loop to handle concurrent modifications.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        new_node </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Node(data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Implement CAS retry loop for push operation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Load current top pointer with appropriate memory ordering</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Set new node's next pointer to current top</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Attempt CAS to make new node the top</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: If CAS fails, implement exponential backoff before retry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Continue until CAS succeeds (lock-free progress guarantee)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use TaggedPointer to prevent ABA problems</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Increment version tag on each CAS attempt</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> pop</span><span style=\"color:#E1E4E8\">(self) -> Optional[Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Pop and return the top element from the stack.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns None if stack is empty.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Implement CAS retry loop for pop operation  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Load current top pointer and check for empty stack</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If empty, return None immediately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Load the next pointer from top node (this becomes new top)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Attempt CAS to update top to next node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: If CAS fails due to ABA, increment backoff and retry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: If CAS succeeds, return the popped node's data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Handle memory reclamation (retire node for later cleanup)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Check for None before dereferencing next pointer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: ABA can occur if node is popped and pushed back</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> is_empty</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if stack is empty (top pointer is None).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Load top pointer and check if pointer component is None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _exponential_backoff</span><span style=\"color:#E1E4E8\">(self, attempt: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Implement exponential backoff to reduce contention.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Sleep for exponentially increasing time based on attempt count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add random jitter to avoid thundering herd</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint-guidelines\">Milestone Checkpoint Guidelines</h4>\n<p><strong>Milestone 1 Checkpoint - Atomic Operations:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run atomic operations test suite</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> atomic/tests/</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected output should show:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ test_compare_and_swap_success</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ test_compare_and_swap_failure  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ test_aba_problem_demonstration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ test_concurrent_counter_increment</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ test_memory_ordering_semantics</span></span></code></pre></div>\n\n<p><strong>Manual Verification Steps:</strong></p>\n<ol>\n<li>Create atomic counter with initial value 0</li>\n<li>Start 10 threads, each incrementing counter 1000 times</li>\n<li>Verify final counter value is exactly 10000 (no lost updates)</li>\n<li>Demonstrate ABA problem with tagged pointer solution</li>\n</ol>\n<p><strong>Milestone 2 Checkpoint - Treiber Stack:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Stress test the implemented stack</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> structures.stack </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TreiberStack</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> random</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> stress_test_data_structure</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stack </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TreiberStack()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> push_worker</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            stack.push(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"item_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">threading.current_thread().ident</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> pop_worker</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        local_results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">500</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            item </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> stack.pop()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> item </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                local_results.append(item)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        results.extend(local_results)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Start concurrent push/pop operations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    threads </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _ </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        threads.append(threading.Thread(</span><span style=\"color:#FFAB70\">target</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">push_worker))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        threads.append(threading.Thread(</span><span style=\"color:#FFAB70\">target</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">pop_worker))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> threads:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        t.start()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> t </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> threads:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        t.join()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Total items popped: </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(results)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"No duplicate items: </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(results) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">set</span><span style=\"color:#E1E4E8\">(results))</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Stack final state empty: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">stack.is_empty()</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Success criteria:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - No duplicate items in results (linearizability)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - No crashes or infinite loops (progress guarantee)  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Stack operations complete in reasonable time</span></span></code></pre></div>\n\n<h4 id=\"language-specific-implementation-hints\">Language-Specific Implementation Hints</h4>\n<p><strong>Python-Specific Considerations:</strong></p>\n<ul>\n<li>Use <code>threading.Lock()</code> temporarily to simulate atomic operations until C extensions are available</li>\n<li>Implement <code>__eq__</code> and <code>__hash__</code> methods for <code>TaggedPointer</code> to support proper comparison semantics</li>\n<li>Use <code>typing.Optional</code> and <code>typing.Any</code> for clear type annotations in concurrent code</li>\n<li>Consider <code>weakref</code> for avoiding circular references in node structures</li>\n</ul>\n<p><strong>Memory Management:</strong></p>\n<ul>\n<li>Python&#39;s garbage collector handles basic memory reclamation, but hazard pointers still prevent use-after-free in concurrent access</li>\n<li>Use <code>threading.local()</code> for per-thread hazard pointer storage</li>\n<li>Monitor memory usage with <code>tracemalloc</code> to detect leaks in retirement lists</li>\n</ul>\n<p><strong>Performance Optimization:</strong></p>\n<ul>\n<li>Profile with <code>cProfile</code> to identify contention bottlenecks</li>\n<li>Use <code>time.sleep(0)</code> for minimal backoff in retry loops</li>\n<li>Consider <code>multiprocessing</code> for CPU-bound workloads that exceed GIL limitations</li>\n</ul>\n<p><strong>Debugging Techniques:</strong></p>\n<ul>\n<li>Add operation logging with thread IDs and timestamps for race condition analysis</li>\n<li>Use <code>threading.current_thread().ident</code> to track per-thread behavior</li>\n<li>Implement assertion checks for data structure invariants after each operation</li>\n</ul>\n<h2 id=\"atomic-operations-foundation\">Atomic Operations Foundation</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1 (Atomic Operations) - This section provides the fundamental building blocks and theoretical foundation required for all subsequent lock-free data structure implementations.</p>\n</blockquote>\n<p>The foundation of lock-free programming rests on atomic operations - indivisible units of computation that provide the coordination primitives necessary for thread-safe concurrent programming without locks. Understanding atomic operations is crucial because they represent the lowest level of abstraction where we can guarantee correctness in the presence of concurrent access from multiple threads. Every lock-free data structure we build in subsequent milestones will fundamentally depend on the properties and guarantees provided by these atomic primitives.</p>\n<h3 id=\"mental-model-atomic-operations-as-indivisible-transactions\">Mental Model: Atomic Operations as Indivisible Transactions</h3>\n<p>Think of atomic operations as bank transactions that either complete entirely or not at all - there&#39;s never a moment where the account is in an inconsistent intermediate state visible to other customers. When you transfer money from checking to savings, other customers never see a moment where the money has left checking but hasn&#39;t yet arrived in savings. The bank&#39;s computer systems ensure this transfer appears instantaneous and indivisible from everyone else&#39;s perspective, even though internally it might involve multiple steps.</p>\n<p>Similarly, atomic operations in concurrent programming provide this same &quot;all-or-nothing&quot; guarantee. When one thread performs an atomic increment on a shared counter, other threads never observe the counter in a half-incremented state. They either see the value before the increment or after the increment - never anything in between. This indivisibility property is what makes atomic operations the foundation for building correct concurrent algorithms without the complexity and performance overhead of traditional locks.</p>\n<p>The key insight is that atomic operations move the complexity of coordination from application code into the hardware and operating system. Instead of writing complex locking protocols to protect shared data, we leverage atomic primitives that the CPU guarantees will execute atomically with respect to all other memory operations. This shifts our focus from &quot;how do we prevent race conditions&quot; to &quot;how do we design algorithms that use atomic operations to achieve the desired behavior.&quot;</p>\n<p>Consider a scenario where multiple threads need to add items to a shared counter. With locks, each thread would acquire a mutex, read the counter, increment it, write it back, then release the mutex. This creates a bottleneck where threads must wait for each other, and introduces the possibility of deadlock if multiple locks are involved. With atomic operations, each thread can use an atomic fetch-and-add operation that performs the entire read-modify-write cycle atomically, allowing multiple threads to update the counter concurrently without coordination overhead.</p>\n<h3 id=\"memory-ordering-and-consistency-models\">Memory Ordering and Consistency Models</h3>\n<p>Memory ordering defines the constraints on how memory operations can be reordered by the compiler and CPU for performance optimization, while still maintaining the correctness guarantees that concurrent programs require. Modern processors and compilers perform aggressive optimizations that reorder instructions and memory accesses to maximize performance, but these optimizations can break the assumptions that naive concurrent programs make about the order in which operations become visible to other threads.</p>\n<p>The memory ordering models provide a spectrum of trade-offs between performance and synchronization guarantees. Stricter ordering models provide stronger guarantees about the relative ordering of memory operations across threads, but may prevent certain performance optimizations. Relaxed ordering models allow more aggressive optimization but require programmers to carefully reason about which orderings are actually necessary for correctness.</p>\n<p><strong>Memory Ordering Models:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Ordering Type</th>\n<th>Visibility Guarantees</th>\n<th>Performance Impact</th>\n<th>Use Cases</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>RELAXED</code></td>\n<td>No ordering constraints on other operations</td>\n<td>Highest performance, minimal barriers</td>\n<td>Counters, statistics where exact ordering doesn&#39;t matter</td>\n</tr>\n<tr>\n<td><code>ACQUIRE</code></td>\n<td>Prevents reordering of subsequent operations before this load</td>\n<td>Medium performance impact</td>\n<td>Reading shared data that was published with RELEASE</td>\n</tr>\n<tr>\n<td><code>RELEASE</code></td>\n<td>Prevents reordering of previous operations after this store</td>\n<td>Medium performance impact</td>\n<td>Publishing shared data for other threads to read</td>\n</tr>\n<tr>\n<td><code>SEQ_CST</code></td>\n<td>Sequential consistency - global total order of all operations</td>\n<td>Highest synchronization cost</td>\n<td>Complex synchronization where total ordering is required</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Progressive Memory Ordering Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Memory ordering is one of the most complex aspects of lock-free programming, and incorrect ordering can cause subtle bugs that are difficult to reproduce and debug</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Use sequential consistency everywhere for simplicity</li>\n<li>Use relaxed ordering everywhere for performance</li>\n<li>Start with sequential consistency, then optimize to weaker orderings where safe</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Start with sequential consistency for initial implementations, then selectively relax ordering constraints where performance analysis shows bottlenecks</li>\n<li><strong>Rationale</strong>: Sequential consistency provides the strongest correctness guarantees and matches most programmers&#39; intuitions about memory behavior, making it easier to reason about algorithm correctness during development</li>\n<li><strong>Consequences</strong>: Initial implementations may have suboptimal performance due to unnecessary memory barriers, but will be easier to verify for correctness</li>\n</ul>\n</blockquote>\n<p>The <code>RELAXED</code> ordering provides no synchronization guarantees beyond the atomicity of the individual operation itself. Operations with relaxed ordering can be freely reordered with respect to other memory operations, and different threads may observe relaxed atomic operations occurring in different orders. This makes relaxed ordering suitable for simple cases like incrementing counters where the exact order of increments doesn&#39;t affect correctness, only the final total value.</p>\n<p><code>ACQUIRE</code> ordering is used on load operations and ensures that no memory operations that appear after the acquire load in program order can be reordered to occur before the load. This creates a one-way barrier where subsequent operations cannot move earlier, but previous operations can still move later. Acquire semantics are typically used when reading a pointer or flag that indicates shared data is ready to be accessed.</p>\n<p><code>RELEASE</code> ordering is used on store operations and ensures that no memory operations that appear before the release store in program order can be reordered to occur after the store. This creates a one-way barrier where previous operations cannot move later, but subsequent operations can still move earlier. Release semantics are typically used when writing a pointer or flag to indicate that shared data has been published and is ready for other threads to access.</p>\n<p><code>SEQ_CST</code> (sequential consistency) ordering provides the strongest guarantees by ensuring that all sequentially consistent operations appear to occur in some global total order that is consistent with the program order within each thread. This means that all threads observe the same ordering of sequentially consistent operations, which matches most programmers&#39; intuitive expectations about how memory should behave in concurrent programs.</p>\n<p><strong>Memory Ordering State Machine:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Current State</th>\n<th>Operation Type</th>\n<th>Memory Ordering</th>\n<th>Next State</th>\n<th>Visibility Effects</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Unordered</td>\n<td>Load</td>\n<td>RELAXED</td>\n<td>Unordered</td>\n<td>No constraints on other operations</td>\n</tr>\n<tr>\n<td>Unordered</td>\n<td>Load</td>\n<td>ACQUIRE</td>\n<td>Synchronized</td>\n<td>Subsequent operations cannot move before</td>\n</tr>\n<tr>\n<td>Unordered</td>\n<td>Store</td>\n<td>RELAXED</td>\n<td>Unordered</td>\n<td>No constraints on other operations</td>\n</tr>\n<tr>\n<td>Unordered</td>\n<td>Store</td>\n<td>RELEASE</td>\n<td>Synchronized</td>\n<td>Previous operations cannot move after</td>\n</tr>\n<tr>\n<td>Synchronized</td>\n<td>Any Operation</td>\n<td>Any</td>\n<td>Synchronized</td>\n<td>Maintains synchronization guarantees</td>\n</tr>\n</tbody></table>\n<p><img src=\"/api/project/lock-free-structures/architecture-doc/asset?path=diagrams%2Fmemory-ordering-states.svg\" alt=\"Memory Ordering State Machine\"></p>\n<p>The interaction between acquire and release operations forms the foundation of lock-free synchronization patterns. When thread A performs a release store followed by thread B performing an acquire load that reads the value written by A, a synchronizes-with relationship is established. This relationship ensures that all memory operations that occurred before the release store in thread A become visible to thread B before any operations that occur after the acquire load.</p>\n<h3 id=\"compare-and-swap-cas-operation\">Compare-and-Swap (CAS) Operation</h3>\n<p>The <strong>compare-and-swap</strong> operation is the fundamental building block that makes lock-free programming possible. CAS atomically compares the current value of a memory location against an expected value, and if they match, replaces the current value with a new value. The operation returns both a boolean indicating success or failure and the actual value that was observed at the memory location. This combination of conditional update and value observation is what enables lock-free algorithms to make progress even when multiple threads are competing to modify the same memory location.</p>\n<p>The power of CAS lies in its ability to detect interference from other threads. When a thread loads a value, performs some computation based on that value, and then attempts to update the location using CAS with the originally loaded value as the expected value, the CAS will fail if any other thread modified the location in the meantime. This failure detection enables the thread to retry the operation with the new value, ensuring that updates are always based on current information rather than stale data.</p>\n<p><strong>CAS Operation Interface:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>compare_and_swap</code></td>\n<td><code>expected</code> (current expected value), <code>new_value</code> (value to store)</td>\n<td><code>(success: bool, observed: value)</code></td>\n<td>Atomically compare and conditionally swap</td>\n</tr>\n<tr>\n<td><code>load</code></td>\n<td><code>ordering</code> (memory ordering constraint)</td>\n<td><code>value</code> (current value)</td>\n<td>Atomically load current value</td>\n</tr>\n<tr>\n<td><code>store</code></td>\n<td><code>value</code> (new value), <code>ordering</code> (memory ordering)</td>\n<td>None</td>\n<td>Atomically store new value</td>\n</tr>\n<tr>\n<td><code>fetch_and_add</code></td>\n<td><code>increment</code> (value to add)</td>\n<td><code>previous_value</code> (value before addition)</td>\n<td>Atomically add and return previous value</td>\n</tr>\n</tbody></table>\n<p>The typical pattern for using CAS involves a retry loop that continues until the operation succeeds. The loop loads the current value, computes the desired new value based on the current value, then attempts to CAS the new value using the loaded value as the expected value. If the CAS fails because another thread modified the location, the loop repeats with the newly observed value.</p>\n<p><img src=\"/api/project/lock-free-structures/architecture-doc/asset?path=diagrams%2Fcas-retry-pattern.svg\" alt=\"CAS Retry Loop State Machine\"></p>\n<p><strong>CAS Retry Loop Algorithm:</strong></p>\n<ol>\n<li>Load the current value from the shared memory location using appropriate memory ordering</li>\n<li>Compute the desired new value based on the current value and the operation being performed</li>\n<li>Attempt to CAS the new value using the loaded value as the expected value</li>\n<li>If CAS succeeds, the operation is complete and the function returns the appropriate result</li>\n<li>If CAS fails, examine the observed value returned by the failed CAS</li>\n<li>Apply exponential backoff delay to reduce contention if configured</li>\n<li>Use the observed value as the new current value and return to step 2</li>\n</ol>\n<p>The success of this pattern depends critically on the operation being <strong>idempotent</strong> or at least <strong>retryable</strong> with updated inputs. The computation in step 2 must be designed so that it can be safely repeated multiple times with different input values if other threads cause CAS failures.</p>\n<blockquote>\n<p>The critical insight is that CAS failure is not an error condition - it&#39;s the normal mechanism by which lock-free algorithms detect and adapt to interference from other threads. A well-designed lock-free algorithm should handle CAS failures gracefully and efficiently.</p>\n</blockquote>\n<p><strong>CAS Retry Patterns:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Pattern</th>\n<th>When to Use</th>\n<th>Retry Strategy</th>\n<th>Example Operation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Simple Retry</td>\n<td>Low contention expected</td>\n<td>Immediate retry without backoff</td>\n<td>Atomic counter increment</td>\n</tr>\n<tr>\n<td>Exponential Backoff</td>\n<td>High contention possible</td>\n<td>Delay increases exponentially</td>\n<td>Shared data structure updates</td>\n</tr>\n<tr>\n<td>Bounded Retry</td>\n<td>Real-time constraints</td>\n<td>Fail after maximum attempts</td>\n<td>Time-critical operations</td>\n</tr>\n<tr>\n<td>Helping Protocol</td>\n<td>Complex multi-step operations</td>\n<td>Assist other threads&#39; operations</td>\n<td>Lock-free queue operations</td>\n</tr>\n</tbody></table>\n<h3 id=\"aba-problem-and-solutions\">ABA Problem and Solutions</h3>\n<p>The <strong>ABA problem</strong> represents one of the most subtle and dangerous pitfalls in lock-free programming. It occurs when a thread reads a value A from a shared location, performs some computation, then finds that the location still contains A when it attempts a CAS operation. The CAS succeeds, but the problem is that the value may have been changed to B and then back to A by other threads in the interim, meaning the assumption that &quot;nothing has changed&quot; is false even though the value appears unchanged.</p>\n<p>This problem is particularly dangerous with pointer-based data structures where the same memory address might be reused for different objects after the original object has been freed and a new object allocated at the same address. A thread might load a pointer to a node, get interrupted, and when it resumes find that the pointer value is the same but now points to a completely different node that happens to be allocated at the same memory address.</p>\n<p>Consider a lock-free stack implementation where the top pointer points to node A, which points to node B. Thread 1 loads the top pointer (getting A) and prepares to pop the stack by setting top to A&#39;s next pointer (B). Before thread 1 can perform the CAS, thread 2 pops both A and B, then pushes A back onto the stack. Now thread 1&#39;s CAS succeeds because top still contains A, but A&#39;s next pointer may now point to some different node or be invalid, corrupting the stack structure.</p>\n<p><strong>ABA Problem Scenarios:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Data Structure</th>\n<th>ABA Manifestation</th>\n<th>Corruption Type</th>\n<th>Detection Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Stack</td>\n<td>Node reused at same address</td>\n<td>Invalid next pointers</td>\n<td>Tagged pointers with version counter</td>\n</tr>\n<tr>\n<td>Queue</td>\n<td>Head/tail pointer reuse</td>\n<td>Lost nodes or cycles</td>\n<td>Hazard pointers + validation</td>\n</tr>\n<tr>\n<td>List</td>\n<td>Node address recycling</td>\n<td>Broken linkage</td>\n<td>Generation counters on nodes</td>\n</tr>\n<tr>\n<td>Hash Table</td>\n<td>Bucket pointer reuse</td>\n<td>Inconsistent chains</td>\n<td>Epoch-based reclamation</td>\n</tr>\n</tbody></table>\n<p>The most common solution to the ABA problem is to use <strong>tagged pointers</strong> that combine the actual pointer value with a monotonically increasing version counter or tag. Instead of performing CAS on just the pointer value, the algorithm performs CAS on the combined pointer-plus-tag value. Each time the pointer is updated, the tag is incremented, ensuring that even if the pointer value cycles back to a previous value, the combined pointer-tag value will be different.</p>\n<p><strong>Tagged Pointer Structure:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Update Policy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>pointer</code></td>\n<td>Memory address</td>\n<td>Actual pointer to the object</td>\n<td>Set to new target object address</td>\n</tr>\n<tr>\n<td><code>tag</code></td>\n<td>Integer counter</td>\n<td>Monotonically increasing version</td>\n<td>Incremented on every pointer update</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Tagged Pointer ABA Prevention</strong></p>\n<ul>\n<li><strong>Context</strong>: The ABA problem can cause silent data corruption that is extremely difficult to detect and debug, making it one of the most dangerous aspects of lock-free programming</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Tagged pointers with version counters</li>\n<li>Hazard pointers for memory reclamation safety</li>\n<li>Epochs or generations for bulk reclamation</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use tagged pointers as the primary ABA prevention mechanism for simple data structures like stacks</li>\n<li><strong>Rationale</strong>: Tagged pointers provide strong ABA protection with minimal overhead and are easier to understand and implement correctly than hazard pointers</li>\n<li><strong>Consequences</strong>: Requires atomic operations on larger values (pointer + tag), may reduce performance on some architectures, but provides strong correctness guarantees</li>\n</ul>\n</blockquote>\n<p>The implementation of tagged pointers requires careful attention to the size limitations of atomic operations. On 64-bit systems, a tagged pointer might pack a 48-bit pointer with a 16-bit tag into a single 64-bit atomic value. The tag space must be large enough that it won&#39;t overflow and wrap around during the expected lifetime of the data structure, as tag wraparound could recreate ABA conditions.</p>\n<p><strong>Tagged Pointer Operations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Operation</th>\n<th>Input</th>\n<th>Output</th>\n<th>Atomicity Requirement</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Load Tagged</td>\n<td>Memory location</td>\n<td><code>(pointer, tag)</code> pair</td>\n<td>Single atomic read of combined value</td>\n</tr>\n<tr>\n<td>Store Tagged</td>\n<td><code>(pointer, tag)</code> pair, location</td>\n<td>None</td>\n<td>Single atomic write of combined value</td>\n</tr>\n<tr>\n<td>CAS Tagged</td>\n<td>Expected <code>(pointer, tag)</code>, new <code>(pointer, tag)</code></td>\n<td><code>(success, observed)</code></td>\n<td>Atomic compare-and-swap of full value</td>\n</tr>\n<tr>\n<td>Increment Tag</td>\n<td>Current <code>(pointer, tag)</code></td>\n<td>New <code>(pointer, tag+1)</code></td>\n<td>Must be done before CAS attempt</td>\n</tr>\n</tbody></table>\n<p>An alternative approach for more complex data structures is to use hazard pointers, which we&#39;ll explore in detail in Milestone 4. Hazard pointers solve the ABA problem by preventing memory reclamation rather than by detecting address reuse. When a thread is about to access a node, it announces this intention by storing the node&#39;s address in a hazard pointer. The memory reclamation system scans all hazard pointers before freeing any node, ensuring that nodes currently being accessed by some thread are not reclaimed and their addresses reused.</p>\n<h3 id=\"common-pitfalls-with-atomics\">Common Pitfalls with Atomics</h3>\n<p>Lock-free programming with atomic operations introduces several categories of subtle errors that can be extremely difficult to detect and debug. These pitfalls often result in code that works correctly under light testing but fails unpredictably under high contention or on different hardware architectures. Understanding these common mistakes is crucial for developing robust lock-free algorithms.</p>\n<p>⚠️ <strong>Pitfall: Mixing Atomic and Non-Atomic Operations</strong></p>\n<p>One of the most dangerous mistakes is mixing atomic and non-atomic operations on the same memory location. When some threads access a variable using atomic operations while other threads access it using regular loads and stores, the memory ordering guarantees are violated and the behavior becomes undefined.</p>\n<p>Consider a shared counter that is incremented atomically by worker threads but read non-atomically by a monitoring thread. The monitoring thread might observe torn reads where it sees partial updates, or it might miss updates entirely due to compiler optimizations that assume the variable doesn&#39;t change during the monitoring function. The solution is to ensure that ALL access to shared data uses atomic operations with appropriate memory ordering.</p>\n<p>⚠️ <strong>Pitfall: Incorrect Memory Ordering Selection</strong></p>\n<p>Using memory ordering that is too weak can cause subtle correctness bugs, while using ordering that is too strong can severely impact performance. A common mistake is using relaxed ordering for operations that actually require synchronization guarantees.</p>\n<p>For example, using relaxed ordering when publishing a data structure after initialization can cause other threads to observe the published pointer before the data structure is fully initialized. The publishing thread should use release ordering on the store that makes the pointer visible, and consuming threads should use acquire ordering when loading the pointer. Mismatching these orderings can cause races where partially initialized data is accessed.</p>\n<p>⚠️ <strong>Pitfall: Infinite Spinning in CAS Loops</strong></p>\n<p>CAS retry loops can spin indefinitely under high contention if not designed with appropriate backoff strategies. When many threads repeatedly attempt to CAS the same location, they can create a livelock situation where no thread makes progress because they keep interfering with each other.</p>\n<p>The solution is to implement exponential backoff where threads wait for progressively longer periods after failed CAS attempts. This reduces contention by spreading out retry attempts over time. Additionally, bounded retry loops that eventually fall back to a different strategy can prevent infinite spinning in pathological cases.</p>\n<p>⚠️ <strong>Pitfall: ABA Problem Ignorance</strong></p>\n<p>Many developers implement lock-free algorithms without considering the ABA problem, leading to data corruption that may only manifest under specific timing conditions. The corruption can be silent and go undetected for long periods, making debugging extremely difficult.</p>\n<p>The most common scenario is implementing a lock-free stack using simple CAS on pointer values without tagged pointers or other ABA prevention. Under memory pressure where freed nodes are quickly reallocated, the same address can be reused for different nodes, causing CAS operations to succeed when they should fail.</p>\n<p>⚠️ <strong>Pitfall: Assuming Sequential Consistency</strong></p>\n<p>Many programmers assume that atomic operations provide sequential consistency by default, but most atomic operations actually use relaxed ordering unless explicitly specified otherwise. This can lead to surprising reorderings that violate program logic.</p>\n<p>For example, two atomic stores with relaxed ordering might become visible to other threads in the opposite order from how they appear in the source code. If the program logic depends on a specific ordering, the atomic operations must use appropriate memory ordering constraints to enforce that ordering.</p>\n<p><strong>Common Pitfalls Summary:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Pitfall Category</th>\n<th>Symptom</th>\n<th>Root Cause</th>\n<th>Detection Method</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Mixed Operations</td>\n<td>Data corruption, torn reads</td>\n<td>Non-atomic access to atomic variables</td>\n<td>Static analysis, TSan</td>\n<td>Make ALL access atomic</td>\n</tr>\n<tr>\n<td>Wrong Ordering</td>\n<td>Subtle race conditions</td>\n<td>Using relaxed when synchronization needed</td>\n<td>Stress testing, model checking</td>\n<td>Use acquire/release pairing</td>\n</tr>\n<tr>\n<td>Infinite Spinning</td>\n<td>High CPU usage, no progress</td>\n<td>CAS retry without backoff</td>\n<td>Performance monitoring</td>\n<td>Add exponential backoff</td>\n</tr>\n<tr>\n<td>ABA Problem</td>\n<td>Silent data corruption</td>\n<td>Pointer reuse between read and CAS</td>\n<td>Extremely hard to detect</td>\n<td>Use tagged pointers</td>\n</tr>\n<tr>\n<td>Ordering Assumptions</td>\n<td>Unexpected reorderings</td>\n<td>Assuming sequential consistency</td>\n<td>Architecture-specific testing</td>\n<td>Specify explicit ordering</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The atomic operations foundation requires careful attention to both correctness and performance. The following implementation guidance provides concrete tools and patterns for building reliable atomic primitives that will serve as the foundation for all subsequent lock-free data structures.</p>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Atomic Primitives</td>\n<td><code>threading</code> module with basic atomics</td>\n<td><code>multiprocessing.Value</code> with ctypes</td>\n</tr>\n<tr>\n<td>Memory Ordering</td>\n<td>Sequential consistency everywhere</td>\n<td>Platform-specific ordering with <code>ctypes</code></td>\n</tr>\n<tr>\n<td>Testing Framework</td>\n<td><code>unittest</code> with manual thread creation</td>\n<td><code>pytest</code> with <code>threading</code> and <code>concurrent.futures</code></td>\n</tr>\n<tr>\n<td>Performance Monitoring</td>\n<td>Basic timing with <code>time.time()</code></td>\n<td><code>cProfile</code> with thread-aware analysis</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  atomic/\n    __init__.py              ← Public interface exports\n    primitives.py            ← Core atomic operations (AtomicReference, etc.)\n    memory_ordering.py       ← Memory ordering constants and utilities\n    tagged_pointer.py        ← Tagged pointer implementation for ABA prevention\n    cas_patterns.py          ← Common CAS retry patterns and backoff strategies\n  tests/\n    test_atomics.py         ← Unit tests for atomic operations\n    test_aba_problem.py     ← Specific tests for ABA problem scenarios\n    stress_test_atomics.py  ← High-contention stress tests\n  benchmarks/\n    atomic_performance.py   ← Performance comparison tests</code></pre></div>\n\n<p><strong>Core Atomic Operations Infrastructure:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Complete atomic operations infrastructure providing the foundation</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">for all lock-free data structures. This module handles the complexity</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">of memory ordering and provides safe, high-level atomic primitives.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ctypes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TypeVar, Generic, Tuple, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> random</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">T </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TypeVar(</span><span style=\"color:#9ECBFF\">'T'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MemoryOrdering</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Memory ordering constraints for atomic operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RELAXED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"relaxed\"</span><span style=\"color:#6A737D\">      # No ordering constraints</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ACQUIRE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"acquire\"</span><span style=\"color:#6A737D\">      # Acquire semantics for loads</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RELEASE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"release\"</span><span style=\"color:#6A737D\">      # Release semantics for stores  </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SEQ_CST</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"seq_cst\"</span><span style=\"color:#6A737D\">      # Sequential consistency</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AtomicReference</span><span style=\"color:#E1E4E8\">(Generic[T]):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Thread-safe atomic reference with compare-and-swap support.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Provides the foundation for all lock-free data structure operations.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, initial_value: T):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> initial_value</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Lock()  </span><span style=\"color:#6A737D\"># Used only for atomic read-modify-write</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.version </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#6A737D\">  # Simple version counter for ABA prevention</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> load</span><span style=\"color:#E1E4E8\">(self, ordering: MemoryOrdering </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MemoryOrdering.</span><span style=\"color:#79B8FF\">SEQ_CST</span><span style=\"color:#E1E4E8\">) -> T:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Atomically load the current value.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            ordering: Memory ordering constraint for the load operation</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Current value stored in the atomic reference</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Implement memory barrier based on ordering parameter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Return current value with appropriate synchronization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: For simple implementation, all orderings can use the lock</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> store</span><span style=\"color:#E1E4E8\">(self, value: T, ordering: MemoryOrdering </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MemoryOrdering.</span><span style=\"color:#79B8FF\">SEQ_CST</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Atomically store a new value.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            value: New value to store</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            ordering: Memory ordering constraint for the store operation</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Implement memory barrier based on ordering parameter  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Store new value with appropriate synchronization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Increment version counter for ABA prevention</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> compare_and_swap</span><span style=\"color:#E1E4E8\">(self, expected: T, new_value: T) -> Tuple[</span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, T]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Atomically compare current value with expected and swap if equal.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            expected: Value we expect to find in the atomic reference</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            new_value: Value to store if current value equals expected</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Tuple of (success: bool, observed_value: T)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            - success: True if swap occurred, False if current != expected</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            - observed_value: The actual value that was in the reference</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Acquire exclusive access to the reference</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Load current value and compare with expected</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If equal, store new_value and increment version</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return success status and observed value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Ensure proper memory ordering semantics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> fetch_and_add</span><span style=\"color:#E1E4E8\">(self, increment: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> T:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Atomically add increment to current value and return previous value.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Only works with numeric types.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            increment: Value to add to current value</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Previous value before the addition</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Implement using compare_and_swap retry loop</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Load current value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compute new_value = current + increment  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Attempt CAS with current as expected, new_value as new</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Retry on failure with observed value from failed CAS</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return original value when CAS succeeds</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TaggedPointer</span><span style=\"color:#E1E4E8\">(Generic[T]):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Pointer combined with version tag to prevent ABA problem.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Essential for lock-free data structures that reuse node memory.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, pointer: Optional[T] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, tag: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.pointer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pointer</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tag </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tag</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __eq__</span><span style=\"color:#E1E4E8\">(self, other) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Tagged pointers are equal only if both pointer and tag match.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">isinstance</span><span style=\"color:#E1E4E8\">(other, TaggedPointer) </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.pointer </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> other.pointer </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.tag </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> other.tag)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> next_version</span><span style=\"color:#E1E4E8\">(self, new_pointer: Optional[T]) -> </span><span style=\"color:#9ECBFF\">'TaggedPointer[T]'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create new tagged pointer with incremented tag.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> TaggedPointer(new_pointer, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.tag </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> cas_retry_loop</span><span style=\"color:#E1E4E8\">(atomic_ref: AtomicReference[T], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                   update_function, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                   max_attempts: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">) -> T:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Generic CAS retry loop with exponential backoff.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Used throughout lock-free data structures for safe updates.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        atomic_ref: AtomicReference to update</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        update_function: Function that takes current value, returns new value</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        max_attempts: Maximum retry attempts before giving up</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Final value after successful update</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize backoff delay and attempt counter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Load current value from atomic reference</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compute new value using update_function(current)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Attempt compare_and_swap with current as expected</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: If CAS succeeds, return the new value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: If CAS fails, apply exponential backoff delay</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Use observed value from failed CAS as new current</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Increment attempt counter and check max_attempts limit</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Repeat from step 3 with new current value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<p><strong>ABA Problem Demonstration and Testing:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Comprehensive test suite demonstrating the ABA problem and validating</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">that tagged pointer solutions correctly prevent ABA conditions.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> atomic.primitives </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> AtomicReference, TaggedPointer</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Node</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Simple node for demonstrating ABA problem in linked structures.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, data: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> data</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.next: Optional[</span><span style=\"color:#9ECBFF\">'Node'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> demonstrate_aba_problem</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Concrete demonstration of ABA problem causing data corruption.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Shows how naive CAS can succeed incorrectly due to address reuse.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create initial stack: A -> B -> C</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Thread 1 loads top pointer (gets A), prepares to pop</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Thread 2 pops A and B, then pushes A back  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Thread 1's CAS succeeds but A.next now invalid</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Demonstrate resulting corruption in stack structure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> stress_test_atomic_counter</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Stress test for atomic counter under high contention.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Validates that no increments are lost even with many concurrent threads.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create AtomicReference[int] initialized to 0</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create N threads that each increment counter M times</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Start all threads simultaneously</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Wait for all threads to complete</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Verify final value equals N * M (no lost updates)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Measure throughput (operations per second)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint:</strong></p>\n<p>After implementing the atomic operations foundation, you should be able to:</p>\n<ol>\n<li><p><strong>Run the stress test</strong>: <code>python -m pytest tests/stress_test_atomics.py -v</code></p>\n<ul>\n<li>Expected: All atomic counter tests pass with exact expected totals</li>\n<li>Expected: ABA problem demonstration shows the corruption scenario</li>\n<li>Expected: Performance tests show atomic operations scale with thread count</li>\n</ul>\n</li>\n<li><p><strong>Verify memory ordering</strong>: Create threads that publish data with RELEASE and consume with ACQUIRE</p>\n<ul>\n<li>Expected: No torn reads or initialization races observed</li>\n<li>Expected: Proper happens-before relationships maintained</li>\n</ul>\n</li>\n<li><p><strong>Test CAS retry patterns</strong>: High-contention scenarios with exponential backoff</p>\n<ul>\n<li>Expected: Operations complete without infinite spinning</li>\n<li>Expected: Backoff reduces contention under high thread counts</li>\n</ul>\n</li>\n</ol>\n<p><strong>Debugging Tips for Atomic Operations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Test failures under stress only</td>\n<td>Race condition in CAS logic</td>\n<td>Add logging to CAS retry loops</td>\n<td>Check expected value handling</td>\n</tr>\n<tr>\n<td>Infinite loops in CAS</td>\n<td>No backoff strategy</td>\n<td>Monitor CPU usage and retry counts</td>\n<td>Add exponential backoff</td>\n</tr>\n<tr>\n<td>Data corruption</td>\n<td>ABA problem</td>\n<td>Check if same addresses reused</td>\n<td>Use tagged pointers</td>\n</tr>\n<tr>\n<td>Performance degradation</td>\n<td>Too much contention</td>\n<td>Profile retry rates and backoff</td>\n<td>Tune backoff parameters</td>\n</tr>\n<tr>\n<td>Inconsistent test results</td>\n<td>Memory ordering issues</td>\n<td>Test on different architectures</td>\n<td>Use stronger ordering</td>\n</tr>\n</tbody></table>\n<p>The atomic operations foundation provides the essential building blocks that every subsequent milestone will depend on. Understanding these primitives deeply is crucial because errors at this level can cause subtle bugs that propagate through all higher-level data structures. Take time to thoroughly test and validate your atomic operations before proceeding to implement the lock-free data structures in the following milestones.</p>\n<h2 id=\"lock-free-stack-treiber-stack\">Lock-free Stack (Treiber Stack)</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 2 (Lock-free Stack) - This section implements the Treiber stack algorithm using atomic compare-and-swap operations, addresses the ABA problem through tagged pointers, and establishes linearizability guarantees for concurrent stack operations.</p>\n</blockquote>\n<p>The lock-free stack represents the first substantial data structure built upon our atomic operations foundation. Unlike traditional mutex-protected stacks that serialize all operations through exclusive locking, the Treiber stack algorithm achieves thread-safety through careful use of atomic compare-and-swap operations on a single top-of-stack pointer. This approach eliminates the performance bottlenecks and deadlock risks inherent in lock-based designs while providing strong correctness guarantees about the linearizable behavior of concurrent push and pop operations.</p>\n<p><img src=\"/api/project/lock-free-structures/architecture-doc/asset?path=diagrams%2Ftreiber-stack-operations.svg\" alt=\"Treiber Stack Push/Pop Flow\"></p>\n<h3 id=\"mental-model-stack-as-a-shared-notepad\">Mental Model: Stack as a Shared Notepad</h3>\n<p>Imagine a busy office where multiple people need to share a single notepad for writing quick notes. In a traditional lock-based approach, only one person could hold the notepad at any time - they would pick it up, write their note on top, and put it back down before anyone else could use it. This creates a bottleneck where everyone else must wait in line, even if they only need a few seconds to jot down a quick message.</p>\n<p>The lock-free stack operates more like a magic notepad with special properties. Multiple people can simultaneously reach for the notepad, but the magic ensures that only one person&#39;s action succeeds at any given instant. When someone wants to add a note, they prepare their page separately, then attempt to place it on top of the stack. If someone else added a page in the meantime, they notice this immediately and retry with the new top page visible. When removing a note, they grab what appears to be the top page, but if someone else already took it or added something new, they automatically retry with the current state.</p>\n<p>The critical insight is that everyone can work simultaneously without blocking each other, but the magic notepad ensures that each individual action (adding or removing a single page) appears to happen atomically and in a well-defined order. No pages are lost, no one gets confused about what&#39;s on top, and the system keeps working even if someone gets distracted halfway through their action. This elimination of waiting and coordination overhead is what makes lock-free data structures so powerful for high-performance concurrent systems.</p>\n<h3 id=\"treiber-stack-algorithm\">Treiber Stack Algorithm</h3>\n<p>The Treiber stack algorithm achieves lock-free operation through a deceptively simple approach: maintain a single atomic pointer to the top of the stack and use compare-and-swap operations to atomically update this pointer when pushing or popping nodes. The elegance lies in how this single atomic variable coordinates all concurrent operations without requiring any locks or blocking synchronization.</p>\n<p><strong>Stack Structure and Node Design</strong></p>\n<p>The core stack structure contains only one field: an atomic pointer to the topmost node. Each node in the stack contains the data payload and an atomic pointer to the next node further down in the stack. This linked-list structure allows unbounded growth and provides the foundation for atomic pointer manipulation.</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>top</code></td>\n<td><code>AtomicReference&lt;Node&gt;</code></td>\n<td>Atomic pointer to the topmost stack node, null when stack is empty</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>data</code></td>\n<td>Generic type T</td>\n<td>The actual data payload stored in this stack node</td>\n</tr>\n<tr>\n<td><code>next</code></td>\n<td><code>AtomicReference&lt;Node&gt;</code></td>\n<td>Atomic pointer to the next node down in the stack, null for bottom node</td>\n</tr>\n</tbody></table>\n<p><strong>Push Operation Algorithm</strong></p>\n<p>The push operation must atomically prepend a new node to the front of the linked list by updating the top pointer. This requires a careful sequence of steps to handle concurrent modifications from other threads.</p>\n<ol>\n<li><strong>Allocate and initialize the new node</strong> with the data payload and a null next pointer</li>\n<li><strong>Load the current top pointer</strong> using an atomic load operation to get a consistent snapshot</li>\n<li><strong>Set the new node&#39;s next pointer</strong> to point to the current top node, linking it into the chain</li>\n<li><strong>Attempt atomic compare-and-swap</strong> on the top pointer, changing from current top to the new node</li>\n<li><strong>Check CAS result</strong> - if successful, the push is complete and the new node is now the stack top</li>\n<li><strong>Handle CAS failure</strong> by returning to step 2 and retrying with the updated top pointer value</li>\n<li><strong>Apply exponential backoff</strong> if experiencing high contention to reduce CPU usage and improve success rates</li>\n</ol>\n<p>The critical insight is that steps 2-4 must be atomic as a group, which is exactly what compare-and-swap provides. The CAS ensures that the top pointer is updated only if it still contains the same value that was read in step 2, guaranteeing that no other thread modified the stack between the read and the update.</p>\n<p><strong>Pop Operation Algorithm</strong></p>\n<p>The pop operation must atomically remove the topmost node and return its data while updating the top pointer to the next node in the chain. This operation faces additional complexity because it must handle the empty stack case and ensure that the removed node&#39;s next pointer is read before any concurrent modifications occur.</p>\n<ol>\n<li><strong>Load the current top pointer</strong> using an atomic load to get the current stack state</li>\n<li><strong>Check for empty stack</strong> - if top is null, return a &quot;not found&quot; indicator immediately</li>\n<li><strong>Read the next pointer</strong> from the current top node to identify the new stack top</li>\n<li><strong>Attempt atomic compare-and-swap</strong> on the top pointer, changing from current top to next node</li>\n<li><strong>Check CAS result</strong> - if successful, the old top node has been atomically removed from the stack</li>\n<li><strong>Extract and return the data</strong> from the removed node, which is now safe to access</li>\n<li><strong>Handle CAS failure</strong> by returning to step 1 and retrying with the updated stack state</li>\n<li><strong>Apply exponential backoff</strong> if experiencing contention to improve overall system performance</li>\n</ol>\n<p>The most subtle aspect of the pop operation is step 3, where the next pointer must be read before the CAS attempt. This creates a potential race condition where another thread could modify or deallocate the node between reading its next pointer and successfully completing the CAS. This is where hazard pointers become essential for safe memory reclamation.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: The Treiber stack&#39;s power comes from reducing all stack operations to single-word atomic operations on the top pointer. This eliminates the need for complex multi-step coordination while providing strong consistency guarantees through the linearizability of compare-and-swap.</p>\n</blockquote>\n<p><strong>CAS Retry Loop Implementation Pattern</strong></p>\n<p>Both push and pop operations follow a common pattern called the CAS retry loop, which handles the fundamental challenge that multiple threads may attempt to modify the same atomic variable simultaneously. Only one thread&#39;s CAS can succeed, while others must retry with updated information.</p>\n<table>\n<thead>\n<tr>\n<th>Loop State</th>\n<th>Description</th>\n<th>Action Taken</th>\n<th>Next Transition</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>LOAD_CURRENT</code></td>\n<td>Reading the current atomic value</td>\n<td>Execute atomic load with appropriate memory ordering</td>\n<td>Move to <code>PREPARE_UPDATE</code></td>\n</tr>\n<tr>\n<td><code>PREPARE_UPDATE</code></td>\n<td>Computing the new value based on current state</td>\n<td>Perform local computation and prepare new node linkages</td>\n<td>Move to <code>ATTEMPT_CAS</code></td>\n</tr>\n<tr>\n<td><code>ATTEMPT_CAS</code></td>\n<td>Executing the compare-and-swap operation</td>\n<td>Call CAS with expected (loaded) and desired (computed) values</td>\n<td>Success: <code>COMPLETE</code>, Failure: <code>BACKOFF</code></td>\n</tr>\n<tr>\n<td><code>BACKOFF</code></td>\n<td>Applying delay before retry to reduce contention</td>\n<td>Execute exponential backoff delay increasing with attempt count</td>\n<td>Return to <code>LOAD_CURRENT</code></td>\n</tr>\n<tr>\n<td><code>COMPLETE</code></td>\n<td>Operation completed successfully</td>\n<td>Return result to caller and exit retry loop</td>\n<td>Terminal state</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Exponential Backoff in CAS Retry Loops</strong></p>\n<ul>\n<li><strong>Context</strong>: High contention scenarios can cause CAS retry loops to consume excessive CPU cycles through constant spinning, degrading overall system performance.</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Immediate retry with no delay</li>\n<li>Fixed delay between retry attempts  </li>\n<li>Exponential backoff with randomization</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement exponential backoff starting at 1 microsecond, doubling each retry up to 1 millisecond maximum, with random jitter.</li>\n<li><strong>Rationale</strong>: Exponential backoff reduces CPU usage during contention while maintaining low latency for lightly contested operations. Random jitter prevents thundering herd effects where all threads retry simultaneously.</li>\n<li><strong>Consequences</strong>: Improves overall throughput under high contention at the cost of slightly higher latency for individual operations in contested scenarios.</li>\n</ul>\n</blockquote>\n<h3 id=\"linearization-points-and-correctness\">Linearization Points and Correctness</h3>\n<p>Linearizability is the gold standard correctness condition for concurrent data structures. It requires that each operation appear to execute atomically at some point during its actual execution time, and that all operations can be ordered in a way that respects both the sequential semantics of the data structure and the real-time ordering of non-overlapping operations. For the Treiber stack, identifying these linearization points is crucial for reasoning about correctness.</p>\n<p><strong>Linearization Points for Push Operations</strong></p>\n<p>The linearization point of a successful push operation occurs at the exact moment when the compare-and-swap operation on the top pointer succeeds. At this instant, the new node becomes atomically visible to all other threads, and the stack transitions from its previous state to the new state with the pushed element on top.</p>\n<p>For failed CAS attempts within the same push operation, no linearization occurs - these are considered internal implementation details that don&#39;t affect the external behavior of the stack. The operation only becomes linearized when it finally succeeds, which may require multiple CAS attempts due to interference from concurrent operations.</p>\n<p><strong>Linearization Points for Pop Operations</strong></p>\n<p>Similarly, the linearization point for a successful pop operation occurs when the compare-and-swap on the top pointer succeeds, atomically removing the top node and making the next node visible as the new stack top. The critical property is that the data value returned by the pop operation must be from the node that was atomically removed at this linearization point.</p>\n<p>For pop operations that find an empty stack, the linearization point occurs at the moment when the atomic load observes a null top pointer. This observation point establishes that the stack was empty at that instant, justifying the &quot;not found&quot; return value.</p>\n<p><strong>Sequential Consistency and Ordering Properties</strong></p>\n<p>The Treiber stack provides sequential consistency, meaning that all operations appear to execute in some total order that is consistent with the program order of each individual thread. This is a stronger guarantee than many lock-free data structures provide, and it comes from the sequential consistency of the underlying compare-and-swap operations on most modern architectures.</p>\n<table>\n<thead>\n<tr>\n<th>Operation Type</th>\n<th>Linearization Point</th>\n<th>Observable Effect</th>\n<th>Ordering Guarantee</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>push(data)</code></td>\n<td>Successful CAS on top pointer</td>\n<td>New node becomes stack top</td>\n<td>Happens-before all subsequent operations that observe the new top</td>\n</tr>\n<tr>\n<td><code>pop()</code> returning data</td>\n<td>Successful CAS on top pointer</td>\n<td>Node removed from stack top</td>\n<td>Happens-before all subsequent operations that observe the updated top</td>\n</tr>\n<tr>\n<td><code>pop()</code> returning empty</td>\n<td>Load observing null top</td>\n<td>Confirmation of empty stack state</td>\n<td>Happens-before relationship established with last successful push</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Critical Correctness Property</strong>: The linearizability of the Treiber stack ensures that concurrent operations appear to execute in some sequential order, even though they may actually execute in an interleaved fashion. This allows developers to reason about the stack using familiar sequential semantics while gaining the performance benefits of lock-free concurrency.</p>\n</blockquote>\n<p><strong>Proving Stack Invariants</strong></p>\n<p>Several key invariants must be maintained to ensure the correctness of the Treiber stack implementation:</p>\n<ol>\n<li><strong>Single-ownership invariant</strong>: Each node appears in at most one stack at any given time, preventing corruption from shared ownership</li>\n<li><strong>Reachability invariant</strong>: All nodes reachable from the top pointer form a valid linked list with proper next-pointer chains</li>\n<li><strong>Atomicity invariant</strong>: The top pointer always refers to a valid node or is null, never pointing to partially-constructed or deallocated memory</li>\n<li><strong>LIFO ordering invariant</strong>: Elements are removed in the reverse order of their insertion, maintaining stack semantics across all interleavings</li>\n</ol>\n<p>These invariants are preserved by the atomic nature of the compare-and-swap operations and the careful ordering of pointer updates within each push and pop operation.</p>\n<h3 id=\"aba-problem-in-stack-context\">ABA Problem in Stack Context</h3>\n<p>The ABA problem represents one of the most subtle and dangerous pitfalls in lock-free programming. It occurs when a compare-and-swap operation incorrectly succeeds because a memory location has changed from value A to value B and back to value A between the time a thread reads the location and attempts to update it. In the context of the Treiber stack, this can lead to severe corruption of the stack structure and loss of data.</p>\n<p><strong>ABA Scenario in Stack Operations</strong></p>\n<p>Consider a stack initially containing nodes A and B, with A on top. Thread 1 begins a pop operation by reading the top pointer (value A) and then reading A&#39;s next pointer (value B) to prepare for the CAS that will make B the new top. However, before Thread 1 can complete its CAS, Thread 2 executes two complete operations: it pops A from the stack, and then pushes A back onto the stack.</p>\n<p>From Thread 1&#39;s perspective, the top pointer still contains value A when it performs the CAS, so the operation succeeds. However, the internal structure of the stack may have changed dramatically. If Thread 2 modified A&#39;s next pointer or if other nodes were pushed and popped in the meantime, Thread 1&#39;s CAS will corrupt the stack by incorrectly setting the top pointer to what it believes is A&#39;s next pointer.</p>\n<table>\n<thead>\n<tr>\n<th>Timeline Step</th>\n<th>Thread 1 Action</th>\n<th>Thread 2 Action</th>\n<th>Stack State</th>\n<th>Problem</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1</td>\n<td>Load top → A, Load A.next → B</td>\n<td>(waiting)</td>\n<td>[A, B, ...]</td>\n<td>None yet</td>\n</tr>\n<tr>\n<td>2</td>\n<td>(preparing CAS)</td>\n<td>Pop A from stack</td>\n<td>[B, ...]</td>\n<td>A is no longer in stack</td>\n</tr>\n<tr>\n<td>3</td>\n<td>(preparing CAS)</td>\n<td>Push A back to stack</td>\n<td>[A, B, ...]</td>\n<td>A.next may have changed</td>\n</tr>\n<tr>\n<td>4</td>\n<td>CAS(A, B) succeeds!</td>\n<td>(done)</td>\n<td>[B, ...] but A.next corrupted</td>\n<td>Stack structure damaged</td>\n</tr>\n</tbody></table>\n<p>The fundamental issue is that pointer equality doesn&#39;t guarantee structural equality. Even though the top pointer contains the same address A, the node at that address may have different contents or may have been used in a completely different context.</p>\n<p><strong>Memory Reuse Amplifies ABA Impact</strong></p>\n<p>The ABA problem becomes even more severe when combined with memory reuse from allocators or memory pools. If Thread 2 pops node A and the memory allocator immediately reuses that memory for a completely different node (perhaps containing different data), Thread 1&#39;s CAS will succeed but will be operating on a node with entirely different semantics.</p>\n<p>This memory reuse scenario can cause:</p>\n<ul>\n<li><strong>Data corruption</strong>: Thread 1 updates the wrong node&#39;s next pointer</li>\n<li><strong>Memory leaks</strong>: Nodes become unreachable due to broken pointer chains  </li>\n<li><strong>Use-after-free errors</strong>: Accessing deallocated memory through stale pointers</li>\n<li><strong>Infinite loops</strong>: Circular references created by incorrect pointer updates</li>\n</ul>\n<p><strong>Tagged Pointer Solution</strong></p>\n<p>The standard solution to the ABA problem is to augment each pointer with a monotonically increasing version tag. Instead of storing just a pointer to a node, we store a tagged pointer containing both the node address and a version number that increments with each update operation.</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>pointer</code></td>\n<td><code>Node*</code></td>\n<td>Memory address of the actual node object</td>\n</tr>\n<tr>\n<td><code>tag</code></td>\n<td><code>uint64_t</code></td>\n<td>Monotonically increasing version counter</td>\n</tr>\n</tbody></table>\n<p>The compare-and-swap operation now operates on the entire tagged pointer structure, comparing both the address and the tag. This ensures that even if the same node address is reused, the version tag will be different, causing the CAS to fail and forcing the thread to retry with current information.</p>\n<p><strong>Tagged Pointer Implementation Considerations</strong></p>\n<p>Most modern 64-bit architectures provide sufficient address space to reserve some bits for the version tag without affecting the valid pointer range. A common approach is to use the upper 16 bits for the tag and the lower 48 bits for the address, providing 65,536 versions before wraparound while supporting the full virtual address space.</p>\n<table>\n<thead>\n<tr>\n<th>Bit Range</th>\n<th>Purpose</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>63-48</td>\n<td>Version Tag</td>\n<td>16-bit counter incremented on each CAS</td>\n</tr>\n<tr>\n<td>47-0</td>\n<td>Node Pointer</td>\n<td>48-bit address supporting full virtual memory space</td>\n</tr>\n</tbody></table>\n<p>The atomic compare-and-swap operates on the full 64-bit tagged pointer value, ensuring that both components must match for the operation to succeed. This provides ABA protection while maintaining the single-word CAS requirement for optimal performance.</p>\n<blockquote>\n<p><strong>Decision: 16-bit Version Tags vs 32-bit Tags</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to choose the split between pointer bits and version bits for ABA protection while maintaining single-word CAS performance.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>8-bit tag (256 versions) with 56-bit pointers</li>\n<li>16-bit tag (65,536 versions) with 48-bit pointers</li>\n<li>32-bit tag (4 billion versions) with 32-bit pointers</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use 16-bit tags with 48-bit pointers for 64-bit architectures</li>\n<li><strong>Rationale</strong>: 48 bits provide sufficient virtual address space for all practical applications (256TB), while 16-bit tags offer strong ABA protection with wraparound only after 65,536 operations on the same memory location.</li>\n<li><strong>Consequences</strong>: Enables single-word CAS performance while providing robust ABA protection. Requires careful pointer packing/unpacking but maintains full memory addressing capabilities.</li>\n</ul>\n</blockquote>\n<h3 id=\"common-stack-implementation-pitfalls\">Common Stack Implementation Pitfalls</h3>\n<p>Lock-free stack implementation contains several subtle pitfalls that can lead to difficult-to-debug issues in concurrent systems. Understanding these common mistakes and their solutions is crucial for building robust lock-free data structures.</p>\n<p>⚠️ <strong>Pitfall: Premature Node Reclamation</strong></p>\n<p>The most dangerous mistake in lock-free stack implementation is deallocating nodes too early, before ensuring that no other threads are accessing them. Consider a pop operation that successfully removes a node from the stack - the node is no longer reachable through the top pointer, but other threads may still hold references to it from earlier reads.</p>\n<p><strong>Why it&#39;s wrong</strong>: Other threads may have loaded a pointer to the node before it was removed from the stack. If these threads proceed with their operations after the node is deallocated, they will access freed memory, causing undefined behavior, crashes, or data corruption.</p>\n<p><strong>How to avoid</strong>: Implement hazard pointers or epoch-based reclamation to defer deallocation until all threads have finished accessing the node. Never call free() or delete immediately after removing a node from the stack structure.</p>\n<p>⚠️ <strong>Pitfall: Incorrect Empty Stack Handling in Pop</strong></p>\n<p>A common implementation error is failing to properly handle the case where the stack becomes empty between loading the top pointer and attempting the compare-and-swap operation. This can lead to null pointer dereferences or incorrect CAS operations.</p>\n<p><strong>Why it&#39;s wrong</strong>: If the top pointer is loaded as non-null but becomes null before the CAS (due to another thread popping the last element), attempting to read the next pointer will dereference a null pointer. Alternatively, performing CAS with stale information can lead to inconsistent stack state.</p>\n<p><strong>How to fix</strong>: Always check for null after loading the top pointer and before accessing the node&#39;s fields. Structure the retry loop to handle the transition to empty stack gracefully:</p>\n<ol>\n<li>Load top pointer atomically</li>\n<li>If null, return &quot;empty&quot; immediately  </li>\n<li>If non-null, read next pointer from the node</li>\n<li>Attempt CAS to change top from current to next</li>\n<li>If CAS fails, retry from step 1</li>\n</ol>\n<p>⚠️ <strong>Pitfall: Missing Memory Ordering Constraints</strong></p>\n<p>Using relaxed memory ordering for all atomic operations can lead to subtle bugs where operations appear to execute out of order due to CPU reordering optimizations. This can cause nodes to appear corrupted or temporarily inconsistent to other threads.</p>\n<p><strong>Why it&#39;s wrong</strong>: Without proper memory ordering, a thread might observe a new node in the stack before observing the initialization of that node&#39;s data fields. This can lead to reading uninitialized data or seeing partially-constructed nodes.</p>\n<p><strong>How to fix</strong>: Use appropriate memory ordering for each operation:</p>\n<ul>\n<li>Push operations should use <code>RELEASE</code> semantics on the final CAS to ensure node initialization is visible before the node becomes reachable</li>\n<li>Pop operations should use <code>ACQUIRE</code> semantics when loading the top pointer to ensure they see all writes that happened before the node was added to the stack</li>\n<li>Use <code>SEQ_CST</code> ordering for simplicity when performance is not critical</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Infinite Spinning Without Backoff</strong></p>\n<p>Implementing CAS retry loops without any backoff strategy can lead to excessive CPU consumption and poor performance under contention. Multiple threads spinning at full speed can actually increase contention and reduce overall throughput.</p>\n<p><strong>Why it&#39;s wrong</strong>: Constant spinning consumes CPU cycles without making progress, and can cause cache line bouncing between cores as multiple threads repeatedly access the same atomic variable. This creates a positive feedback loop where contention increases CPU usage, which increases contention.</p>\n<p><strong>How to fix</strong>: Implement exponential backoff with the following strategy:</p>\n<ol>\n<li>Start with no delay for the first retry (optimistic case)</li>\n<li>Double the delay after each failed attempt, starting from 1 microsecond</li>\n<li>Cap the maximum delay at 1 millisecond to maintain responsiveness</li>\n<li>Add random jitter (±25%) to prevent synchronized retries</li>\n<li>Reset delay to zero after any successful operation</li>\n</ol>\n<p>⚠️ <strong>Pitfall: Mixing Atomic and Non-Atomic Access</strong></p>\n<p>Accessing stack nodes or the top pointer through non-atomic operations while other threads are performing atomic operations creates data races and undefined behavior. This often occurs in debugging code or when implementing additional operations like size() or iteration.</p>\n<p><strong>Why it&#39;s wrong</strong>: The C++ and other language memory models specify that mixing atomic and non-atomic access to the same memory location is undefined behavior. Even seemingly harmless reads can interfere with atomic operations or observe inconsistent state.</p>\n<p><strong>How to fix</strong>: Either make all access atomic with appropriate memory ordering, or carefully design protocols where non-atomic access occurs only when atomics are guaranteed not to be used (such as during single-threaded initialization or after synchronization barriers).</p>\n<table>\n<thead>\n<tr>\n<th>Pitfall Category</th>\n<th>Detection Method</th>\n<th>Prevention Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Premature reclamation</td>\n<td>AddressSanitizer, Valgrind</td>\n<td>Implement hazard pointers before testing</td>\n</tr>\n<tr>\n<td>Empty stack races</td>\n<td>Stress testing with frequent empty/non-empty transitions</td>\n<td>Add explicit null checks in all pointer operations</td>\n</tr>\n<tr>\n<td>Memory ordering bugs</td>\n<td>ThreadSanitizer, careful code review</td>\n<td>Use stronger ordering (ACQUIRE/RELEASE) by default</td>\n</tr>\n<tr>\n<td>Infinite spinning</td>\n<td>CPU profiling, performance monitoring</td>\n<td>Implement exponential backoff from the start</td>\n</tr>\n<tr>\n<td>Mixed atomic access</td>\n<td>ThreadSanitizer, static analysis</td>\n<td>Establish clear atomic/non-atomic boundaries</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The lock-free stack implementation requires careful attention to atomic operations, memory ordering, and the integration of hazard pointers for safe memory reclamation. This section provides concrete guidance for building a production-ready Treiber stack.</p>\n<p><strong>A. Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Atomic Operations</td>\n<td><code>threading.Lock</code> with CAS simulation</td>\n<td><code>ctypes</code> with platform-specific atomic libraries</td>\n</tr>\n<tr>\n<td>Memory Ordering</td>\n<td>Sequential consistency only</td>\n<td>Full acquire/release/relaxed semantics</td>\n</tr>\n<tr>\n<td>Testing Framework</td>\n<td><code>unittest</code> with basic threading</td>\n<td><code>pytest</code> with <code>concurrent.futures</code> for stress testing</td>\n</tr>\n<tr>\n<td>Performance Monitoring</td>\n<td>Basic timing with <code>time.perf_counter()</code></td>\n<td><code>cProfile</code> with custom metrics collection</td>\n</tr>\n<tr>\n<td>Memory Safety</td>\n<td>Reference counting with <code>weakref</code></td>\n<td>Custom hazard pointer implementation</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended Module Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>lock_free/\n├── __init__.py                    ← package initialization\n├── atomic/\n│   ├── __init__.py               ← atomic operations module\n│   ├── primitives.py             ← AtomicReference, compare_and_swap, memory ordering\n│   └── tagged_pointer.py         ← TaggedPointer implementation for ABA prevention\n├── stack/\n│   ├── __init__.py               ← stack module exports\n│   ├── treiber_stack.py          ← main TreiberStack implementation (YOUR FOCUS)\n│   ├── node.py                   ← Node structure definition\n│   └── stack_test.py             ← comprehensive testing including linearizability\n├── memory/\n│   ├── __init__.py               ← memory management exports  \n│   ├── hazard_pointers.py        ← safe memory reclamation (next milestone)\n│   └── retirement_list.py        ← deferred deletion support\n└── testing/\n    ├── __init__.py               ← testing utilities\n    ├── linearizability.py        ← LinearizabilityChecker implementation\n    └── stress_test.py             ← concurrent correctness validation</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code (Complete Implementation)</strong></p>\n<p><strong>File: <code>atomic/primitives.py</code></strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Tuple, TypeVar, Generic, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">T </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TypeVar(</span><span style=\"color:#9ECBFF\">'T'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MemoryOrdering</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Memory ordering constraints for atomic operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RELAXED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"relaxed\"</span><span style=\"color:#6A737D\">      # No ordering constraints</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ACQUIRE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"acquire\"</span><span style=\"color:#6A737D\">      # Acquire semantics for loads</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RELEASE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"release\"</span><span style=\"color:#6A737D\">      # Release semantics for stores  </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SEQ_CST</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"seq_cst\"</span><span style=\"color:#6A737D\">     # Sequential consistency</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AtomicReference</span><span style=\"color:#E1E4E8\">(Generic[T]):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Thread-safe atomic reference with compare-and-swap support.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, initial_value: Optional[T] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> initial_value</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Lock()  </span><span style=\"color:#6A737D\"># Simulates atomic hardware operations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> load</span><span style=\"color:#E1E4E8\">(self, ordering: MemoryOrdering </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MemoryOrdering.</span><span style=\"color:#79B8FF\">SEQ_CST</span><span style=\"color:#E1E4E8\">) -> Optional[T]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Atomically load the current value.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> store</span><span style=\"color:#E1E4E8\">(self, value: Optional[T], ordering: MemoryOrdering </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MemoryOrdering.</span><span style=\"color:#79B8FF\">SEQ_CST</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Atomically store a new value.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> compare_and_swap</span><span style=\"color:#E1E4E8\">(self, expected: Optional[T], new_value: Optional[T]) -> Tuple[</span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, Optional[T]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Atomically compare current value with expected and swap if equal.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns: (success: bool, observed_value: T)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            current </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> current </span><span style=\"color:#F97583\">is</span><span style=\"color:#E1E4E8\"> expected:  </span><span style=\"color:#6A737D\"># Using 'is' for object identity comparison</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> new_value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">, current</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">, current</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> cas_retry_loop</span><span style=\"color:#E1E4E8\">(atomic_ref: AtomicReference[T], update_function, max_attempts: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Generic CAS retry loop with exponential backoff.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    attempt </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    delay </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.000001</span><span style=\"color:#6A737D\">  # Start with 1 microsecond</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    while</span><span style=\"color:#E1E4E8\"> attempt </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> max_attempts:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        current </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> atomic_ref.load()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        new_value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> update_function(current)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        success, observed </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> atomic_ref.compare_and_swap(current, new_value)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> success:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Exponential backoff with jitter</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> delay </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            jitter </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> delay </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 0.25</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.time() </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> -</span><span style=\"color:#79B8FF\"> 0.5</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># ±25% random</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            time.sleep(delay </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> jitter)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        delay </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> min</span><span style=\"color:#E1E4E8\">(delay </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0.001</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Cap at 1 millisecond</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        attempt </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#6A737D\">  # Failed after max attempts</span></span></code></pre></div>\n\n<p><strong>File: <code>atomic/tagged_pointer.py</code></strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TypeVar, Optional, NamedTuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .primitives </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> AtomicReference, MemoryOrdering</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">T </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TypeVar(</span><span style=\"color:#9ECBFF\">'T'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TaggedPointer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">NamedTuple</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Pointer with version tag to prevent ABA problems.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pointer: Optional[T]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tag: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create</span><span style=\"color:#E1E4E8\">(cls, pointer: Optional[T], tag: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'TaggedPointer[T]'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create a new tagged pointer with specified tag.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">pointer</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">pointer, </span><span style=\"color:#FFAB70\">tag</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">tag)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> increment_tag</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#9ECBFF\">'TaggedPointer[T]'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create new tagged pointer with incremented tag.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> TaggedPointer(</span><span style=\"color:#FFAB70\">pointer</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.pointer, </span><span style=\"color:#FFAB70\">tag</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.tag </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#F97583\"> 0x</span><span style=\"color:#79B8FF\">FFFF</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> with_pointer</span><span style=\"color:#E1E4E8\">(self, new_pointer: Optional[T]) -> </span><span style=\"color:#9ECBFF\">'TaggedPointer[T]'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create new tagged pointer with different pointer, incremented tag.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> TaggedPointer(</span><span style=\"color:#FFAB70\">pointer</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">new_pointer, </span><span style=\"color:#FFAB70\">tag</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.tag </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#F97583\"> 0x</span><span style=\"color:#79B8FF\">FFFF</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AtomicTaggedPointer</span><span style=\"color:#E1E4E8\">(AtomicReference[TaggedPointer[T]]):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Atomic reference to tagged pointer for ABA-safe operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, initial_pointer: Optional[T] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        initial_tagged </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TaggedPointer.create(initial_pointer, </span><span style=\"color:#FFAB70\">tag</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(initial_tagged)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> load_pointer</span><span style=\"color:#E1E4E8\">(self, ordering: MemoryOrdering </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MemoryOrdering.</span><span style=\"color:#79B8FF\">SEQ_CST</span><span style=\"color:#E1E4E8\">) -> Optional[T]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load just the pointer component.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tagged </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.load(ordering)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> tagged.pointer </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> tagged </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> compare_and_swap_pointer</span><span style=\"color:#E1E4E8\">(self, expected_pointer: Optional[T], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                new_pointer: Optional[T]) -> tuple[</span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, Optional[T]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"CAS on pointer with automatic tag increment.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        current_tagged </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.load()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> current_tagged.pointer </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> expected_pointer:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">, current_tagged.pointer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        new_tagged </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> current_tagged.with_pointer(new_pointer)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        success, observed_tagged </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.compare_and_swap(current_tagged, new_tagged)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> success, observed_tagged.pointer</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code (For Student Implementation)</strong></p>\n<p><strong>File: <code>stack/node.py</code></strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TypeVar, Optional, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..atomic.primitives </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> AtomicReference</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">T </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TypeVar(</span><span style=\"color:#9ECBFF\">'T'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Node</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Stack node containing data and atomic next pointer.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, data: T):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Store the data payload in self.data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize self.next as AtomicReference[Node] starting with None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use AtomicReference(None) for the next pointer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>File: <code>stack/treiber_stack.py</code></strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TypeVar, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..atomic.primitives </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> AtomicReference, cas_retry_loop</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..atomic.tagged_pointer </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> AtomicTaggedPointer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .node </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Node</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">T </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TypeVar(</span><span style=\"color:#9ECBFF\">'T'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TreiberStack</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Lock-free stack implementation using Treiber algorithm.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize self.top as AtomicTaggedPointer[Node] starting with None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use AtomicTaggedPointer(None) to create empty stack</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> push</span><span style=\"color:#E1E4E8\">(self, data: T) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Push data onto the stack using lock-free CAS operation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        This operation is lock-free and linearizable.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create new node with the data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Start CAS retry loop - load current top pointer  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Set new node's next to point to current top</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Attempt CAS to make new node the top</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: If CAS succeeds, return. If fails, retry from step 2</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Add exponential backoff for failed CAS attempts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use self.top.compare_and_swap_pointer(expected, new_node)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> pop</span><span style=\"color:#E1E4E8\">(self) -> Optional[T]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Pop data from the stack using lock-free CAS operation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns None if stack is empty.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        This operation is lock-free and linearizable.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Start CAS retry loop - load current top pointer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check if stack is empty (top is None) and return None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Load the next pointer from current top node  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Attempt CAS to make next node the new top</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: If CAS succeeds, return the data from old top node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: If CAS fails, retry from step 1</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Add exponential backoff for failed CAS attempts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Read top.next before CAS, then CAS from top to top.next</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> is_empty</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if stack is empty. This is a snapshot operation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load current top pointer atomically</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Return True if top is None, False otherwise  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use self.top.load_pointer() to get just the pointer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> size</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Count nodes in stack. WARNING: This is not atomic with other operations!</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Result may be inconsistent in concurrent environment.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load current top pointer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Traverse the linked list counting nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return total count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Follow next pointers but don't use this for synchronization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints</strong></p>\n<p><strong>Python Atomic Operations</strong>: Since Python doesn&#39;t have built-in atomic operations, we simulate them using <code>threading.Lock</code>. In production, consider using <code>ctypes</code> to call platform-specific atomic libraries or libraries like <code>python-atomics</code>.</p>\n<p><strong>Memory Management</strong>: Python&#39;s garbage collector handles basic memory management, but for true lock-free behavior, you&#39;ll need to implement hazard pointers to prevent premature collection of nodes still being accessed by other threads.</p>\n<p><strong>Testing with Threading</strong>: Use <code>threading.Thread</code> for basic testing, but for comprehensive stress testing, consider <code>concurrent.futures.ThreadPoolExecutor</code> to easily manage multiple worker threads.</p>\n<p><strong>Performance Measurement</strong>: Use <code>time.perf_counter()</code> for high-resolution timing measurements. For production workloads, implement custom metrics to track CAS success rates and retry counts.</p>\n<p><strong>F. Milestone Checkpoint</strong></p>\n<p>After implementing the Treiber stack, verify correct behavior with these steps:</p>\n<p><strong>Basic Functionality Test</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> lock_free/stack/stack_test.py::test_basic_push_pop</span><span style=\"color:#79B8FF\"> -v</span></span></code></pre></div>\n<p>Expected: All pushes and pops work correctly on single thread, LIFO ordering preserved.</p>\n<p><strong>Concurrent Correctness Test</strong>:  </p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> lock_free/stack/stack_test.py::test_concurrent_operations</span><span style=\"color:#79B8FF\"> -v</span></span></code></pre></div>\n<p>Expected: Multiple threads pushing and popping concurrently produce no lost elements, no duplicates, maintain LIFO ordering globally.</p>\n<p><strong>ABA Problem Demonstration</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> lock_free/stack/stack_test.py::test_aba_problem</span><span style=\"color:#79B8FF\"> -v</span></span></code></pre></div>\n<p>Expected: Tagged pointer implementation prevents ABA corruption, naive implementation shows the problem.</p>\n<p><strong>Performance Comparison</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> lock_free/stack/benchmark.py</span></span></code></pre></div>\n<p>Expected output:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Lock-free stack: 1,250,000 ops/sec (4 threads)\nMutex-based stack: 340,000 ops/sec (4 threads)\nSpeedup: 3.68x under contention</code></pre></div>\n\n<p><strong>Signs of Problems</strong>:</p>\n<ul>\n<li><strong>Lost elements</strong>: Check CAS retry logic, ensure no race conditions in node linking</li>\n<li><strong>Duplicate elements</strong>: Verify that successful CAS doesn&#39;t allow double-returns  </li>\n<li><strong>Crashes</strong>: Implement hazard pointers, check for null pointer dereferences</li>\n<li><strong>Poor performance</strong>: Add exponential backoff, check for infinite spinning</li>\n</ul>\n<p><strong>G. Debugging Tips</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Elements disappear</td>\n<td>CAS retry bug or premature deallocation</td>\n<td>Add logging to track each CAS attempt</td>\n<td>Fix retry loop logic, add hazard pointers</td>\n</tr>\n<tr>\n<td>Duplicate elements returned</td>\n<td>Multiple threads getting same node</td>\n<td>Log successful CAS operations</td>\n<td>Ensure CAS success prevents multiple returns</td>\n</tr>\n<tr>\n<td>Infinite hanging</td>\n<td>Livelock in CAS retry</td>\n<td>Monitor CAS success/failure rates</td>\n<td>Implement exponential backoff with jitter</td>\n</tr>\n<tr>\n<td>Crashes on pop</td>\n<td>Null pointer dereference</td>\n<td>Check stack traces for node access</td>\n<td>Add null checks before dereferencing nodes</td>\n</tr>\n<tr>\n<td>Poor performance</td>\n<td>Excessive contention</td>\n<td>Profile CAS retry counts</td>\n<td>Tune backoff parameters, check for false sharing</td>\n</tr>\n</tbody></table>\n<h2 id=\"lock-free-queue-michael-scott-algorithm\">Lock-free Queue (Michael-Scott Algorithm)</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 3 (Lock-free Queue) - This section implements the Michael-Scott FIFO queue algorithm using dual head/tail pointers, sentinel nodes, and helping mechanisms to achieve lock-free concurrent operations with linearizability guarantees.</p>\n</blockquote>\n<p>The transition from stack to queue represents a significant leap in lock-free algorithm complexity. While the Treiber stack operates on a single point of contention (the top pointer), a queue must coordinate operations at two distinct ends: producers adding elements at the tail and consumers removing elements at the head. This dual-pointer design introduces new challenges around consistency, progress guarantees, and the prevention of interference between concurrent enqueue and dequeue operations.</p>\n<p>The <strong>Michael-Scott queue algorithm</strong> stands as one of the most elegant solutions to lock-free FIFO ordering, published by Maged Michael and Michael Scott in 1996. This algorithm demonstrates several advanced lock-free programming techniques: the use of a dummy sentinel node to simplify empty queue handling, a helping mechanism where threads assist each other to ensure progress, and careful management of two atomic pointers that must remain consistent despite concurrent updates from multiple threads.</p>\n<h3 id=\"mental-model-queue-as-a-conveyor-belt\">Mental Model: Queue as a Conveyor Belt</h3>\n<p>Before diving into the technical complexities of the Michael-Scott algorithm, it&#39;s helpful to visualize the queue using a factory conveyor belt analogy. Imagine a manufacturing assembly line with workers at both ends: <strong>packers</strong> at one end who place items onto the conveyor belt, and <strong>shippers</strong> at the other end who remove completed items for delivery.</p>\n<p>In this mental model, the conveyor belt itself represents the queue&#39;s linked list of nodes. The <strong>tail pointer</strong> is like a supervisor standing near the packing station, always pointing to the last position where an item was placed (or where the next item should go). The <strong>head pointer</strong> is another supervisor at the shipping station, pointing to the position where the next item should be removed.</p>\n<p>The key insight from this analogy is that both supervisors (pointers) need to coordinate without directly communicating. When a packer places a new item, they must update the tail supervisor&#39;s position. When a shipper removes an item, they must advance the head supervisor. But here&#39;s the crucial challenge: what happens when multiple packers try to place items simultaneously, or when the conveyor belt is empty and both supervisors are pointing to the same empty position?</p>\n<p>This is where the <strong>dummy sentinel node</strong> comes into play. Think of it as a permanent &quot;placeholder&quot; item that never gets shipped - it serves as a reference point that ensures the head and tail supervisors never lose track of the belt structure. The sentinel node means the belt is never truly empty; there&#39;s always at least one reference point.</p>\n<p>The <strong>helping mechanism</strong> is like workers occasionally glancing over to help their colleagues. If a packer notices that the tail supervisor hasn&#39;t moved to point to the latest item (perhaps they were distracted), the packer will helpfully update the supervisor&#39;s position before adding their own item. This mutual assistance ensures that work never gets permanently stuck, even when individual workers are temporarily delayed.</p>\n<p><img src=\"/api/project/lock-free-structures/architecture-doc/asset?path=diagrams%2Fmichael-scott-sequence.svg\" alt=\"Michael-Scott Queue Operation Sequence\"></p>\n<p>This conveyor belt model captures the essence of why lock-free queues are more complex than stacks: we&#39;re coordinating activity at two distinct locations while maintaining the invariant that items move in FIFO order from the tail end to the head end.</p>\n<h3 id=\"michael-scott-queue-algorithm\">Michael-Scott Queue Algorithm</h3>\n<p>The Michael-Scott algorithm elegantly solves the dual-pointer coordination challenge through a carefully designed protocol that allows enqueue and dequeue operations to proceed concurrently with minimal interference. The algorithm&#39;s brilliance lies in its two-phase approach to pointer updates and the helping mechanism that prevents operations from blocking indefinitely.</p>\n<blockquote>\n<p><strong>Decision: Two-Pointer Design with Separate Endpoints</strong></p>\n<ul>\n<li><strong>Context</strong>: A queue requires FIFO ordering with operations at both ends - additions at tail and removals at head. We need to choose how to coordinate these two access points.</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Single pointer with full traversal for enqueue operations</li>\n<li>Two pointers (head and tail) with independent CAS operations</li>\n<li>Single pointer with reverse traversal links</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use separate atomic head and tail pointers with independent CAS operations</li>\n<li><strong>Rationale</strong>: This design maximizes concurrency by allowing enqueue and dequeue to proceed independently most of the time, avoiding the O(n) traversal cost of single-pointer approaches while maintaining cache locality for common operations</li>\n<li><strong>Consequences</strong>: Increased complexity in maintaining consistency between two pointers, but significant performance gains under contention and better scalability across multiple producer/consumer threads</li>\n</ul>\n</blockquote>\n<p>The core data structures that enable this algorithm are carefully designed to support atomic updates while maintaining referential integrity:</p>\n<table>\n<thead>\n<tr>\n<th>Structure</th>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Node</code></td>\n<td><code>data</code></td>\n<td><code>Any</code></td>\n<td>The actual payload stored in this queue element</td>\n</tr>\n<tr>\n<td><code>Node</code></td>\n<td><code>next</code></td>\n<td><code>AtomicReference&lt;Node&gt;</code></td>\n<td>Atomic pointer to the next node in the queue</td>\n</tr>\n<tr>\n<td><code>MichaelScottQueue</code></td>\n<td><code>head</code></td>\n<td><code>AtomicReference&lt;Node&gt;</code></td>\n<td>Points to the sentinel node or first dequeueable node</td>\n</tr>\n<tr>\n<td><code>MichaelScottQueue</code></td>\n<td><code>tail</code></td>\n<td><code>AtomicReference&lt;Node&gt;</code></td>\n<td>Points to the last node or one node behind the actual tail</td>\n</tr>\n</tbody></table>\n<p>The queue operations follow a precise protocol that ensures linearizability while allowing maximum concurrency. Let&#39;s examine each operation in detail:</p>\n<h4 id=\"enqueue-operation-protocol\">Enqueue Operation Protocol</h4>\n<p>The enqueue operation follows a two-step protocol that first links the new node into the list structure, then advances the tail pointer. This ordering is crucial for maintaining consistency and enabling the helping mechanism:</p>\n<ol>\n<li><strong>Allocate and initialize a new node</strong> with the provided data and a null <code>next</code> pointer</li>\n<li><strong>Load the current tail pointer</strong> and the node it points to (call this <code>tail_node</code>)</li>\n<li><strong>Read the <code>next</code> pointer of <code>tail_node</code></strong> to determine if the tail is pointing to the actual end</li>\n<li><strong>Check consistency</strong>: re-read the tail pointer to ensure it hasn&#39;t changed since step 2</li>\n<li><strong>If <code>tail_node.next</code> is null</strong> (tail points to actual end):<ul>\n<li>Attempt <code>compare_and_swap(tail_node.next, null, new_node)</code></li>\n<li>If successful, break to step 7</li>\n<li>If failed, retry from step 2 (another thread added a node)</li>\n</ul>\n</li>\n<li><strong>If <code>tail_node.next</code> is not null</strong> (tail is lagging):<ul>\n<li>Help advance the tail: <code>compare_and_swap(queue.tail, tail_node, tail_node.next)</code></li>\n<li>Retry from step 2 whether the helping CAS succeeded or failed</li>\n</ul>\n</li>\n<li><strong>Advance the tail pointer</strong>: <code>compare_and_swap(queue.tail, tail_node, new_node)</code><ul>\n<li>This step may fail if another thread already advanced the tail, which is acceptable</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"dequeue-operation-protocol\">Dequeue Operation Protocol</h4>\n<p>The dequeue operation is somewhat simpler since it only needs to advance the head pointer, but it must handle the empty queue case and coordinate with the sentinel node:</p>\n<ol>\n<li><strong>Load the current head and tail pointers</strong> and the nodes they point to</li>\n<li><strong>Check consistency</strong>: re-read the head pointer to ensure it hasn&#39;t changed</li>\n<li><strong>If head equals tail</strong>:<ul>\n<li>Check if <code>head_node.next</code> is null (truly empty queue)</li>\n<li>If null, return &quot;queue empty&quot; indicator</li>\n<li>If not null, help advance tail: <code>compare_and_swap(queue.tail, tail_node, head_node.next)</code></li>\n<li>Retry from step 1</li>\n</ul>\n</li>\n<li><strong>If head does not equal tail</strong>:<ul>\n<li>Read <code>next_node = head_node.next</code> (this will be the new head)</li>\n<li>If <code>next_node</code> is null, retry from step 1 (inconsistent state)</li>\n<li>Read the data from <code>next_node</code> before proceeding</li>\n</ul>\n</li>\n<li><strong>Advance the head pointer</strong>: <code>compare_and_swap(queue.head, head_node, next_node)</code></li>\n<li><strong>If CAS succeeded</strong>, return the data read in step 4</li>\n<li><strong>If CAS failed</strong>, retry from step 1</li>\n</ol>\n<p><img src=\"/api/project/lock-free-structures/architecture-doc/asset?path=diagrams%2Fcas-retry-pattern.svg\" alt=\"CAS Retry Loop State Machine\"></p>\n<p>The algorithm&#39;s correctness relies on several key invariants that must be maintained throughout all operations:</p>\n<blockquote>\n<p><strong>Critical Invariant: Queue Structure Consistency</strong></p>\n<ul>\n<li>The queue always contains at least one node (the sentinel)</li>\n<li>The head pointer never advances beyond the tail pointer in the logical sequence</li>\n<li>Every node reachable from head is also reachable by following next pointers from head</li>\n<li>The tail pointer points either to the actual last node or to the node immediately before it</li>\n</ul>\n</blockquote>\n<h3 id=\"dummy-sentinel-node-design\">Dummy Sentinel Node Design</h3>\n<p>The dummy sentinel node represents one of the most crucial design decisions in the Michael-Scott algorithm. This seemingly simple concept - maintaining a permanent &quot;dummy&quot; node that never holds actual data - solves multiple complex synchronization problems that would otherwise require intricate special-case handling.</p>\n<blockquote>\n<p><strong>Decision: Permanent Sentinel Node for Empty Queue Handling</strong></p>\n<ul>\n<li><strong>Context</strong>: Concurrent queues must handle the empty state safely while allowing simultaneous enqueue and dequeue operations. Traditional approaches with null head/tail pointers create race conditions.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Allow null head/tail pointers with special case handling</li>\n<li>Use boolean empty flag with additional synchronization</li>\n<li>Maintain permanent dummy/sentinel node that never holds data</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use a permanent sentinel node that remains in the queue throughout its lifetime</li>\n<li><strong>Rationale</strong>: Eliminates all empty queue edge cases by ensuring head and tail always point to valid nodes, reduces the number of CAS operations needed for the empty→non-empty transition, and simplifies the helping mechanism by providing a stable reference point</li>\n<li><strong>Consequences</strong>: Slight memory overhead (one extra node), but dramatically simplifies the algorithm logic and eliminates several classes of race conditions</li>\n</ul>\n</blockquote>\n<p>The sentinel node serves multiple critical functions in the algorithm:</p>\n<p><strong>Eliminates the Empty Queue Special Case</strong>: Without a sentinel node, an empty queue would require both head and tail to point to null, creating a complex synchronization problem. When the first item is enqueued into an empty queue, both pointers must be updated atomically, which would require either a double-word CAS operation (not available on all architectures) or complex lock-based coordination. The sentinel node ensures that head and tail always point to valid memory locations.</p>\n<p><strong>Provides Stable Reference for Helping</strong>: The helping mechanism relies on threads being able to advance the tail pointer when they detect it&#39;s lagging behind. Without a sentinel node, a lagging tail in an empty queue would create ambiguous states where threads can&#39;t determine whether the queue is empty or whether the tail pointer simply hasn&#39;t been updated yet.</p>\n<p><strong>Simplifies Dequeue Operation</strong>: When dequeuing, threads don&#39;t need to distinguish between &quot;removing the last element&quot; and &quot;removing from a multi-element queue.&quot; The sentinel ensures that after removing the last data element, the queue still contains one node (the sentinel), maintaining structural consistency.</p>\n<p>The sentinel node initialization and lifecycle follow a specific pattern:</p>\n<table>\n<thead>\n<tr>\n<th>Phase</th>\n<th>Head Points To</th>\n<th>Tail Points To</th>\n<th>Sentinel Next</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Creation</td>\n<td>Sentinel</td>\n<td>Sentinel</td>\n<td>null</td>\n<td>Empty queue with sentinel only</td>\n</tr>\n<tr>\n<td>After Enqueue</td>\n<td>Sentinel</td>\n<td>Data Node</td>\n<td>Data Node</td>\n<td>One data element present</td>\n</tr>\n<tr>\n<td>After Dequeue</td>\n<td>Data Node</td>\n<td>Data Node</td>\n<td>null or Next</td>\n<td>Back to sentinel pointing at remaining data</td>\n</tr>\n<tr>\n<td>Multiple Elements</td>\n<td>Sentinel</td>\n<td>Last Node</td>\n<td>Varies</td>\n<td>Normal operation state</td>\n</tr>\n</tbody></table>\n<p>The sentinel node&#39;s <code>next</code> pointer serves as the key indicator for queue state:</p>\n<ul>\n<li>When <code>sentinel.next == null</code>, the queue contains no data elements</li>\n<li>When <code>sentinel.next != null</code>, the first data element follows the sentinel</li>\n</ul>\n<p>This design creates a clean separation between structural nodes (the sentinel) and data nodes, allowing the algorithm to maintain structural invariants while data nodes are added and removed dynamically.</p>\n<blockquote>\n<p><strong>Key Insight: Sentinel Node as Structural Foundation</strong>\nThe sentinel node acts as a permanent &quot;foundation&quot; for the queue structure. Just as a building&#39;s foundation remains stable while floors are added and removed above it, the sentinel provides a stable reference point that allows the head and tail pointers to coordinate without complex empty-state handling.</p>\n</blockquote>\n<h3 id=\"helping-mechanism-for-progress\">Helping Mechanism for Progress</h3>\n<p>The helping mechanism represents one of the most sophisticated aspects of the Michael-Scott algorithm and is essential for achieving the lock-free progress guarantee. Without helping, operations could become blocked indefinitely when threads are preempted or delayed at critical moments, violating the fundamental lock-free property that at least one thread must always make progress.</p>\n<blockquote>\n<p><strong>Decision: Cooperative Helping Protocol for Progress Guarantees</strong></p>\n<ul>\n<li><strong>Context</strong>: In lock-free algorithms, threads can be preempted or delayed at any time. If one thread starts an operation but gets delayed halfway through, other threads must be able to continue making progress rather than waiting indefinitely.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Each thread only performs its own operations (simple but can block)</li>\n<li>Timeout-based retry with exponential backoff</li>\n<li>Cooperative helping where threads complete operations started by delayed threads</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement cooperative helping where threads assist each other in completing operations</li>\n<li><strong>Rationale</strong>: This is the only approach that guarantees lock-free progress - if any thread is delayed, others can complete its operation and continue. Provides better worst-case latency than timeout approaches and maintains linearizability.</li>\n<li><strong>Consequences</strong>: Increased algorithm complexity as threads must detect and handle situations where helping is needed, but ensures robust progress guarantees even under adverse scheduling conditions</li>\n</ul>\n</blockquote>\n<p>The helping mechanism operates on a simple principle: <strong>when a thread detects that the tail pointer is &quot;lagging behind&quot; the actual end of the queue, it attempts to advance the tail pointer before proceeding with its own operation</strong>. This cooperation ensures that no operation can remain permanently stuck waiting for another thread to complete its work.</p>\n<h4 id=\"detecting-when-help-is-needed\">Detecting When Help is Needed</h4>\n<p>The key insight is recognizing when the tail pointer needs assistance. This occurs when:</p>\n<ol>\n<li>The tail pointer points to a node <code>N</code></li>\n<li>Node <code>N</code>&#39;s <code>next</code> pointer is not null (meaning another node exists beyond <code>N</code>)</li>\n<li>Therefore, the tail should point to <code>N.next</code> instead of <code>N</code></li>\n</ol>\n<p>This situation arises naturally in the two-phase enqueue process:</p>\n<ol>\n<li><strong>Phase 1</strong>: A thread successfully executes <code>CAS(tail_node.next, null, new_node)</code>, linking the new node into the queue</li>\n<li><strong>Phase 2</strong>: The same thread attempts <code>CAS(queue.tail, tail_node, new_node)</code> to advance the tail pointer</li>\n<li><strong>Interruption</strong>: The thread might be preempted, delayed, or fail the second CAS due to interference</li>\n</ol>\n<p>At this point, the queue is structurally sound (all nodes are properly linked), but the tail pointer is &quot;lagging&quot; - it points to the second-to-last node instead of the actual last node.</p>\n<h4 id=\"the-helping-protocol\">The Helping Protocol</h4>\n<p>When any thread (performing either enqueue or dequeue) detects this lagging condition, it executes the helping protocol:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Helping Algorithm:\n1. Load current_tail = queue.tail\n2. Load tail_node = current_tail.pointer  \n3. Load next_node = tail_node.next\n4. Re-check that queue.tail still equals current_tail (consistency check)\n5. If next_node != null:\n   - Execute CAS(queue.tail, current_tail, next_node)\n   - Success or failure doesn't matter - the attempt is sufficient\n6. Continue with the thread's original operation</code></pre></div>\n\n<p>The beauty of this protocol is its <strong>idempotent</strong> nature - multiple threads can attempt to help simultaneously without causing corruption. If thread A succeeds in advancing the tail, thread B&#39;s helping attempt will simply fail its consistency check or its CAS operation, which is harmless.</p>\n<h4 id=\"linearization-points-and-helping\">Linearization Points and Helping</h4>\n<p>The helping mechanism introduces subtle but important considerations for linearizability. The <strong>linearization point</strong> for an enqueue operation is not when the tail pointer is updated, but when the new node is successfully linked into the queue structure (the <code>CAS(tail_node.next, null, new_node)</code> operation). This is crucial because:</p>\n<ul>\n<li>The element becomes visible to dequeue operations immediately after linking, regardless of tail pointer state</li>\n<li>Other enqueue operations can begin helping to advance the tail, making the element officially &quot;at the tail&quot;</li>\n<li>The tail pointer update is essentially a performance optimization rather than a correctness requirement</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Operation Phase</th>\n<th>Linearization Point</th>\n<th>Effect on Queue State</th>\n<th>Helping Trigger</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Pre-Link</td>\n<td>N/A</td>\n<td>No change</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Link Success</td>\n<td><strong>HERE</strong></td>\n<td>Element visible to dequeue</td>\n<td>Yes (tail may lag)</td>\n</tr>\n<tr>\n<td>Tail Update Success</td>\n<td>N/A</td>\n<td>Tail pointer consistent</td>\n<td>No longer needed</td>\n</tr>\n<tr>\n<td>Tail Update Failed</td>\n<td>N/A</td>\n<td>Tail pointer still lags</td>\n<td>Still needed</td>\n</tr>\n</tbody></table>\n<h4 id=\"helping-in-dequeue-operations\">Helping in Dequeue Operations</h4>\n<p>Dequeue operations also participate in the helping protocol, particularly when checking for empty queue conditions. When <code>head == tail</code>, a dequeue operation must determine whether the queue is truly empty or whether the tail is simply lagging behind:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Empty Check with Helping:\n1. If head_node == tail_node AND head_node.next == null → truly empty\n2. If head_node == tail_node AND head_node.next != null → tail is lagging\n3. In case 2: help advance tail, then retry the dequeue operation</code></pre></div>\n\n<p>This helping behavior is essential for correctness, not just performance. Without it, a dequeue operation might incorrectly conclude that the queue is empty when elements are actually available.</p>\n<blockquote>\n<p><strong>Critical Insight: Helping as Correctness Mechanism</strong>\nWhile helping appears to be a performance optimization, it&#39;s actually required for correctness in the Michael-Scott algorithm. Operations can only complete successfully when the queue&#39;s pointer structure accurately reflects the actual node linkages, making helping a mandatory part of the protocol rather than an optional enhancement.</p>\n</blockquote>\n<h3 id=\"common-queue-implementation-pitfalls\">Common Queue Implementation Pitfalls</h3>\n<p>Lock-free queue implementation presents numerous subtle pitfalls that can lead to memory corruption, lost data, or violation of FIFO ordering. Understanding these common mistakes and their solutions is crucial for successful implementation.</p>\n<h4 id=\"-pitfall-incorrect-tail-pointer-management\">⚠️ <strong>Pitfall: Incorrect Tail Pointer Management</strong></h4>\n<p><strong>The Problem</strong>: Many implementers focus on the head pointer logic and treat the tail pointer as a simple optimization, leading to incorrect tail advancement or missing helping logic.</p>\n<p><strong>Why It Breaks</strong>: The tail pointer serves as more than a performance hint - it&#39;s integral to the correctness of enqueue operations. When the tail is allowed to lag indefinitely or is advanced incorrectly, several failures occur:</p>\n<ul>\n<li>Enqueue operations may link nodes to incorrect positions in the queue</li>\n<li>The helping mechanism breaks down, potentially causing livelock</li>\n<li>FIFO ordering can be violated when nodes are inserted out of sequence</li>\n</ul>\n<p><strong>Specific Failure Scenarios</strong>:</p>\n<ol>\n<li><strong>Missing Consistency Checks</strong>: Loading the tail pointer once and using it throughout the operation without re-checking for changes</li>\n<li><strong>Ignored Help Opportunities</strong>: Detecting that <code>tail_node.next != null</code> but failing to attempt tail advancement before retrying</li>\n<li><strong>Premature Tail Advancement</strong>: Advancing the tail before the new node is properly linked</li>\n</ol>\n<p><strong>The Fix</strong>: Implement rigorous tail pointer management:</p>\n<ul>\n<li>Always re-check tail pointer consistency after loading tail_node and its next pointer</li>\n<li>Mandatory helping: never retry an enqueue without first attempting to help if tail is lagging</li>\n<li>Use proper memory ordering (at least acquire semantics) when loading tail pointer and tail_node.next</li>\n</ul>\n<h4 id=\"-pitfall-sentinel-node-confusion\">⚠️ <strong>Pitfall: Sentinel Node Confusion</strong></h4>\n<p><strong>The Problem</strong>: Treating the sentinel node as a data node or attempting to remove it during dequeue operations.</p>\n<p><strong>Why It Breaks</strong>: The sentinel node is structural infrastructure, not a data container. Removing it or storing data in it violates the algorithm&#39;s fundamental invariants:</p>\n<ul>\n<li>Removing the sentinel creates an empty queue state that the algorithm cannot handle correctly</li>\n<li>Storing data in the sentinel means this data can never be dequeued (it&#39;s always &quot;one behind&quot; the head)</li>\n<li>Pointer consistency checks assume the sentinel&#39;s permanent existence</li>\n</ul>\n<p><strong>Specific Failure Scenarios</strong>:</p>\n<ol>\n<li><strong>Sentinel Data Storage</strong>: Initializing the queue by storing the first data element in the sentinel node</li>\n<li><strong>Sentinel Removal</strong>: Dequeue operations that advance head past the sentinel, leaving no stable reference point</li>\n<li><strong>Sentinel Reinitialization</strong>: Creating new sentinel nodes during operation instead of maintaining the original</li>\n</ol>\n<p><strong>The Fix</strong>: Enforce strict sentinel discipline:</p>\n<ul>\n<li>Sentinel node never holds user data (data field should remain null/uninitialized)</li>\n<li>Head pointer may equal sentinel pointer only when queue is logically empty</li>\n<li>When head equals sentinel and sentinel.next != null, the queue contains exactly one data element</li>\n<li>Never deallocate or replace the sentinel node</li>\n</ul>\n<h4 id=\"-pitfall-aba-problems-in-dual-pointer-context\">⚠️ <strong>Pitfall: ABA Problems in Dual-Pointer Context</strong></h4>\n<p><strong>The Problem</strong>: The standard tagged pointer solution for ABA problems becomes more complex when managing two related pointers that must remain consistent.</p>\n<p><strong>Why It Breaks</strong>: Unlike the single-pointer Treiber stack, the Michael-Scott queue must maintain consistency between head and tail pointers. Standard ABA protection on individual pointers is insufficient - we need <strong>relational consistency</strong> between the two pointers:</p>\n<ul>\n<li>Tail might be advanced based on a stale view of the queue structure</li>\n<li>Head advancement might interfere with concurrent tail updates</li>\n<li>Node reuse between head and tail regions can create complex ABA scenarios</li>\n</ul>\n<p><strong>Specific Failure Scenarios</strong>:</p>\n<ol>\n<li><strong>Inconsistent Tagging</strong>: Using different tag increment strategies for head and tail pointers</li>\n<li><strong>Stale Relationship Assumptions</strong>: Loading head and tail at different times and assuming their relationship remains valid</li>\n<li><strong>Cross-Pointer Interference</strong>: Head operations affecting tail validity and vice versa</li>\n</ol>\n<p><strong>The Fix</strong>: Implement coordinated ABA protection:</p>\n<ul>\n<li>Use consistent tagging schemes for both head and tail pointers</li>\n<li>Load both head and tail atomically (or use validation loops to ensure consistency)</li>\n<li>Consider using hazard pointers for more robust memory safety instead of relying solely on tagged pointers</li>\n</ul>\n<h4 id=\"-pitfall-linearization-edge-cases\">⚠️ <strong>Pitfall: Linearization Edge Cases</strong></h4>\n<p><strong>The Problem</strong>: Incorrect identification of linearization points, especially in operations that involve helping other threads.</p>\n<p><strong>Why It Breaks</strong>: Linearizability requires that each operation appears to occur atomically at some single point in time. When helping is involved, multiple threads might affect the same logical operation, making it unclear when the operation &quot;actually occurred&quot;:</p>\n<ul>\n<li>Enqueue linearization point confusion: when tail is updated vs when node is linked</li>\n<li>Dequeue linearization point with helping: when head is advanced vs when data is read</li>\n<li>Helping operations affecting the linearization of the original operation</li>\n</ul>\n<p><strong>Specific Failure Scenarios</strong>:</p>\n<ol>\n<li><strong>Late Linearization</strong>: Claiming the operation linearizes when tail is updated, even though the element was visible earlier</li>\n<li><strong>Helping Linearization</strong>: Incorrectly attributing an operation&#39;s linearization to the helping thread rather than the original thread</li>\n<li><strong>Read-Modify Gap</strong>: Assuming linearization occurs at read time when the actual effect happens during the modify phase</li>\n</ol>\n<p><strong>The Fix</strong>: Establish clear linearization point rules:</p>\n<ul>\n<li>Enqueue linearizes at successful <code>CAS(tail_node.next, null, new_node)</code> - this is when the element becomes visible</li>\n<li>Dequeue linearizes at successful <code>CAS(queue.head, head_node, next_node)</code> - this is when the element is removed</li>\n<li>Helping operations do not create new linearization points - they assist existing operations</li>\n<li>Use formal verification or model checking tools to validate linearization point assignments</li>\n</ul>\n<h4 id=\"-pitfall-memory-ordering-and-visibility\">⚠️ <strong>Pitfall: Memory Ordering and Visibility</strong></h4>\n<p><strong>The Problem</strong>: Using insufficient memory ordering constraints, particularly with relaxed atomics, leading to visibility and ordering issues.</p>\n<p><strong>Why It Breaks</strong>: The Michael-Scott algorithm relies on specific visibility guarantees between threads. Relaxed memory ordering can allow:</p>\n<ul>\n<li>Tail node updates to be visible before the node&#39;s next pointer is properly initialized</li>\n<li>Head advancement to be visible before the previous head&#39;s next pointer is loaded</li>\n<li>Helping operations to see stale views of the queue structure</li>\n</ul>\n<p><strong>The Fix</strong>: Use appropriate memory ordering:</p>\n<ul>\n<li>Use acquire semantics when loading pointers that will be dereferenced</li>\n<li>Use release semantics when publishing new nodes or updating structural pointers</li>\n<li>Consider using sequentially consistent ordering during initial development, then optimize to weaker orderings only after correctness is verified</li>\n</ul>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides concrete implementation guidance for building the Michael-Scott lock-free queue. The target audience is junior developers who understand basic concurrency concepts but need practical guidance on structuring the code and handling the algorithm&#39;s subtleties.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Atomic Operations</td>\n<td><code>threading.atomic</code> wrappers around primitive types</td>\n<td>Custom atomic classes with explicit memory ordering</td>\n</tr>\n<tr>\n<td>Memory Management</td>\n<td>Python&#39;s garbage collector (no explicit reclamation)</td>\n<td>Hazard pointer integration for manual memory management</td>\n</tr>\n<tr>\n<td>Node Allocation</td>\n<td>Standard object instantiation with <code>__init__</code></td>\n<td>Object pooling with pre-allocated node cache</td>\n</tr>\n<tr>\n<td>Testing Framework</td>\n<td><code>unittest</code> with basic threading tests</td>\n<td><code>pytest</code> with property-based testing via <code>hypothesis</code></td>\n</tr>\n<tr>\n<td>Performance Measurement</td>\n<td><code>time.time()</code> for basic throughput measurement</td>\n<td><code>perf_counter()</code> with statistical analysis and contention metrics</td>\n</tr>\n</tbody></table>\n<p>For initial implementation, use Python&#39;s built-in threading primitives and rely on the garbage collector. This allows focus on algorithm correctness before optimizing for performance.</p>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>lock_free_structures/\n├── __init__.py\n├── atomic/\n│   ├── __init__.py\n│   ├── operations.py        ← CAS, fetch_and_add from Milestone 1\n│   └── memory_ordering.py   ← Memory ordering constants and utilities\n├── queue/\n│   ├── __init__.py\n│   ├── michael_scott.py     ← Core Michael-Scott queue implementation\n│   ├── node.py              ← Queue node structure and utilities\n│   └── queue_test.py        ← Comprehensive test suite\n├── utils/\n│   ├── __init__.py\n│   ├── linearization.py     ← Linearizability checking utilities\n│   └── stress_test.py       ← Multi-threaded stress testing framework\n└── examples/\n    ├── __init__.py\n    ├── producer_consumer.py  ← Example usage with multiple producers/consumers\n    └── benchmark.py          ← Performance comparison vs lock-based queue</code></pre></div>\n\n<p>This structure maintains clear separation between the atomic primitives (reused from Milestone 1), the queue-specific logic, testing utilities, and example usage patterns.</p>\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Complete Node Implementation</strong> (<code>queue/node.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, TypeVar, Generic</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..atomic.operations </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> AtomicReference</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">T </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TypeVar(</span><span style=\"color:#9ECBFF\">'T'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Node</span><span style=\"color:#E1E4E8\">(Generic[T]):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Queue node containing data and atomic pointer to next node.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    The Node class represents a single element in the Michael-Scott queue's</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    linked list structure. Each node contains user data and an atomic reference</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    to the next node in the queue.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, data: Optional[T] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Initialize a new queue node.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            data: The user data to store in this node. None for sentinel nodes.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.data: Optional[T] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> data</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.next: AtomicReference[</span><span style=\"color:#9ECBFF\">'Node[T]'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> AtomicReference(</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> is_sentinel</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if this is a sentinel node (no user data).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.data </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __repr__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        next_repr </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"None\"</span><span style=\"color:#F97583\"> if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.next.load() </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#F97583\"> else</span><span style=\"color:#9ECBFF\"> \"Node(...)\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Node(data=</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.data</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, next=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">next_repr</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span></span></code></pre></div>\n\n<p><strong>Memory Ordering Constants</strong> (<code>atomic/memory_ordering.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MemoryOrdering</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Memory ordering constraints for atomic operations.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    These constants define the synchronization and ordering guarantees</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    for atomic operations in the lock-free queue implementation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RELAXED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"relaxed\"</span><span style=\"color:#6A737D\">     # No ordering constraints</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ACQUIRE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"acquire\"</span><span style=\"color:#6A737D\">     # Acquire semantics for loads</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RELEASE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"release\"</span><span style=\"color:#6A737D\">     # Release semantics for stores  </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SEQ_CST</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"seq_cst\"</span><span style=\"color:#6A737D\">     # Sequential consistency</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Export commonly used orderings as module-level constants</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">RELAXED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> MemoryOrdering.</span><span style=\"color:#79B8FF\">RELAXED</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">ACQUIRE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> MemoryOrdering.</span><span style=\"color:#79B8FF\">ACQUIRE</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">RELEASE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> MemoryOrdering.</span><span style=\"color:#79B8FF\">RELEASE</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">SEQ_CST</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> MemoryOrdering.</span><span style=\"color:#79B8FF\">SEQ_CST</span></span></code></pre></div>\n\n<p><strong>Linearizability Testing Utilities</strong> (<code>utils/linearization.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Any, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> OperationType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ENQUEUE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"enqueue\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEQUEUE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"dequeue\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Operation</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Record of a single queue operation for linearizability checking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: OperationType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    thread_id: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start_time: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    end_time: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    input_value: Any </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    return_value: Any </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    successful: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> LinearizabilityChecker</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Utility for recording and verifying linearizability of queue operations.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This class provides a framework for testing whether a sequence of</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    concurrent queue operations could have occurred in some sequential</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    order that respects the FIFO semantics.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._operations: List[Operation] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Lock()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> record_operation</span><span style=\"color:#E1E4E8\">(self, op: Operation) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Thread-safely record a completed operation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._operations.append(op)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> verify_history</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Verify that recorded operations could have occurred in FIFO order.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            True if the operation history is linearizable, False otherwise.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement full linearizability checking algorithm</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # For now, return True to allow basic testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_operation_count</span><span style=\"color:#E1E4E8\">(self) -> Dict[OperationType, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get count of operations by type.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        counts </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {OperationType.</span><span style=\"color:#79B8FF\">ENQUEUE</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, OperationType.</span><span style=\"color:#79B8FF\">DEQUEUE</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> op </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._operations:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            counts[op.type] </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> counts</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton\">Core Logic Skeleton</h4>\n<p><strong>Michael-Scott Queue Implementation</strong> (<code>queue/michael_scott.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, TypeVar, Generic</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> threading </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> current_thread</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .node </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Node</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..atomic.operations </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> AtomicReference, compare_and_swap</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..atomic.memory_ordering </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ACQUIRE</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">RELEASE</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">SEQ_CST</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..utils.linearization </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Operation, OperationType, LinearizabilityChecker</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">T </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TypeVar(</span><span style=\"color:#9ECBFF\">'T'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MichaelScottQueue</span><span style=\"color:#E1E4E8\">(Generic[T]):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Lock-free FIFO queue implementation using the Michael-Scott algorithm.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This queue supports concurrent enqueue and dequeue operations from</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    multiple threads without using locks. It guarantees FIFO ordering</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    and linearizability of all operations.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Initialize an empty queue with a sentinel node.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        The queue starts with a single sentinel node that both head and</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        tail pointers reference. This eliminates empty queue edge cases.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create a sentinel node with no data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize head pointer to point to sentinel</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Initialize tail pointer to point to sentinel</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Both pointers should reference the same sentinel initially</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> enqueue</span><span style=\"color:#E1E4E8\">(self, data: T) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Add an element to the tail of the queue.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        This operation is lock-free and linearizable. It may help advance</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        the tail pointer if it detects that another thread's enqueue</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        operation left the tail pointer lagging.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            data: The element to add to the queue.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create a new node with the provided data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Loop until enqueue succeeds:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2a: Load current tail pointer and tail node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2b: Load the next pointer of the tail node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2c: Re-check tail pointer consistency (guard against races)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2d: If tail_node.next is null (tail at actual end):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #     </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2d1: Attempt CAS to link new_node as tail_node.next</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #     </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2d2: If CAS succeeds, break out of loop</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #     </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2d3: If CAS fails, continue loop (another thread added node)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2e: If tail_node.next is not null (tail is lagging):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #     </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2e1: Help advance tail pointer to tail_node.next</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #     </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2e2: Continue loop regardless of helping CAS result</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: After loop: attempt to advance tail pointer to new_node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Record operation for linearizability testing (if enabled)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: The linearization point is step 2d1 (successful link CAS)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> dequeue</span><span style=\"color:#E1E4E8\">(self) -> Optional[T]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Remove and return an element from the head of the queue.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        This operation is lock-free and linearizable. It returns None</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        if the queue is empty and may help advance the tail pointer</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        if it detects lagging.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            The dequeued element, or None if the queue was empty.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load current head and tail pointers and their nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Re-check head pointer consistency</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check if queue appears empty (head == tail):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            #   </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3a: Load head_node.next to verify emptiness</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            #   </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3b: If head_node.next is null, return None (truly empty)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            #   </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3c: If head_node.next is not null, help advance tail</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            #   </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3d: Continue loop after helping</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Queue is non-empty (head != tail):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            #   </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4a: Load next_node = head_node.next</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            #   </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4b: If next_node is null, retry (inconsistent state)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            #   </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4c: Read data from next_node BEFORE attempting CAS</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            #   </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4d: Attempt CAS to advance head pointer to next_node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            #   </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4e: If CAS succeeds, return the data read in step 4c</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            #   </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4f: If CAS fails, continue loop</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Record operation for linearizability testing (if enabled)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Hint: The linearization point is step 4d (successful head advance)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> is_empty</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Check if the queue is empty.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            True if the queue contains no data elements, False otherwise.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load head and tail pointers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If head != tail, queue is definitely non-empty</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If head == tail, check if head_node.next is null</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return True only if head == tail and head_node.next == null</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: This is a snapshot check - result may be stale immediately</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> size_hint</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Provide an approximate count of queue elements.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        This is a best-effort estimate that may be inaccurate due to</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        concurrent modifications. Use only for monitoring/debugging.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Approximate number of elements in the queue.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Start from head node and traverse next pointers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Count non-sentinel nodes until reaching tail or null</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle the case where tail is lagging during traversal</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return the count (may be stale by the time it's returned)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Warning: This operation is O(n) and not lock-free</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"language-specific-implementation-hints\">Language-Specific Implementation Hints</h4>\n<p><strong>Python Threading Considerations</strong>:</p>\n<ul>\n<li>Use <code>threading.Lock</code> only in testing utilities, never in queue operations</li>\n<li>Python&#39;s GIL actually helps with some race conditions but don&#39;t rely on it for correctness</li>\n<li>Use <code>threading.local()</code> for per-thread data like hazard pointers</li>\n<li>Consider <code>weakref</code> for debugging/monitoring without affecting garbage collection</li>\n</ul>\n<p><strong>Atomic Operations in Python</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Example of proper CAS implementation check:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> safe_compare_and_swap</span><span style=\"color:#E1E4E8\">(atomic_ref, expected, new_value):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Wrapper that handles Python's atomic operation limitations.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> compare_and_swap(atomic_ref, expected, new_value)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Log the exception for debugging but don't crash</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"CAS operation failed: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">, atomic_ref.load()</span></span></code></pre></div>\n\n<p><strong>Memory Management Strategy</strong>:</p>\n<ul>\n<li>Initially rely on Python&#39;s garbage collector for node cleanup</li>\n<li>Implement explicit node pooling only after correctness is verified</li>\n<li>Use <code>__slots__</code> in Node class to reduce memory overhead</li>\n<li>Consider <code>gc.disable()</code> during performance benchmarks to get consistent timing</li>\n</ul>\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>After Implementing Basic Queue Structure</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#79B8FF\">cd</span><span style=\"color:#9ECBFF\"> lock_free_structures/queue/</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> queue_test.py::test_basic_enqueue_dequeue</span><span style=\"color:#79B8FF\"> -v</span></span></code></pre></div>\n<p>Expected output: All basic enqueue/dequeue operations succeed with single thread</p>\n<p><strong>After Implementing Helping Mechanism</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from michael_scott import MichaelScottQueue</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">import threading</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">import time</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">q = MichaelScottQueue()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">def producer():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    for i in range(100):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        q.enqueue(i)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        time.sleep(0.001)  # Small delay to encourage helping</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">def consumer():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    for _ in range(100):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        q.dequeue()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        time.sleep(0.001)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">threads = [threading.Thread(target=producer), threading.Thread(target=consumer)]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">for t in threads: t.start()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">for t in threads: t.join()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print('Helping test completed successfully')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n\n<p><strong>After Full Implementation</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> examples/stress_test.py</span><span style=\"color:#79B8FF\"> --threads=8</span><span style=\"color:#79B8FF\"> --operations=10000</span></span></code></pre></div>\n<p>Expected output: No lost elements, FIFO ordering preserved, performance metrics</p>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Enqueue hangs forever</td>\n<td>Tail pointer never advances</td>\n<td>Check if helping logic is missing in enqueue loop</td>\n<td>Implement mandatory helping before retry</td>\n</tr>\n<tr>\n<td>Lost elements during dequeue</td>\n<td>Reading data after CAS instead of before</td>\n<td>Print data value and CAS success/failure</td>\n<td>Read <code>next_node.data</code> before attempting head advance CAS</td>\n</tr>\n<tr>\n<td>FIFO ordering violated</td>\n<td>Multiple nodes linked at same position</td>\n<td>Add logging to CAS operations on next pointers</td>\n<td>Ensure consistency checks before each CAS attempt</td>\n</tr>\n<tr>\n<td>Segmentation fault/AttributeError</td>\n<td>Using freed/None node reference</td>\n<td>Check if sentinel is being deallocated</td>\n<td>Never remove or replace the sentinel node</td>\n</tr>\n<tr>\n<td>Poor performance under contention</td>\n<td>Excessive CAS retries without backoff</td>\n<td>Measure CAS retry counts per operation</td>\n<td>Add exponential backoff in retry loops</td>\n</tr>\n<tr>\n<td>Memory usage grows unbounded</td>\n<td>Nodes not being garbage collected</td>\n<td>Check for circular references or global node storage</td>\n<td>Ensure nodes become unreachable after dequeue</td>\n</tr>\n</tbody></table>\n<p>The key to successful debugging is adding extensive logging around CAS operations and pointer consistency checks. Most bugs in lock-free queues manifest as violations of the basic invariants: sentinel permanence, head-tail ordering, and proper node linkage.</p>\n<h2 id=\"hazard-pointers-for-memory-reclamation\">Hazard Pointers for Memory Reclamation</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 4 (Hazard Pointers) - This section implements a safe memory reclamation scheme that prevents use-after-free errors while maintaining the non-blocking properties of lock-free data structures.</p>\n</blockquote>\n<p>The fundamental challenge in lock-free programming lies not just in implementing the core algorithms, but in safely managing memory in an environment where multiple threads may simultaneously access, modify, and potentially deallocate shared nodes. Traditional garbage-collected languages solve this problem automatically, but in systems programming languages like C++, Rust, or even Python with manual memory management, we must carefully coordinate when it&#39;s safe to free memory that other threads might still be accessing.</p>\n<p><img src=\"/api/project/lock-free-structures/architecture-doc/asset?path=diagrams%2Fhazard-pointer-lifecycle.svg\" alt=\"Hazard Pointer Memory Lifecycle\"></p>\n<p>The <strong>hazard pointer</strong> technique, introduced by Maged Michael in 2004, provides an elegant solution to this memory reclamation problem. It allows threads to announce their intention to access specific memory locations, preventing other threads from prematurely deallocating those locations while maintaining the non-blocking properties essential to lock-free algorithms.</p>\n<h3 id=\"mental-model-hazard-pointers-as-safety-signs\">Mental Model: Hazard Pointers as Safety Signs</h3>\n<p>Imagine a large construction site where multiple crews are working on different parts of a building simultaneously. Each crew needs to occasionally demolish old structures to make room for new ones, but they must ensure no workers are currently inside those structures. Traditional locking would be like having a single master key that controls access to the entire site—only one crew could work at a time, creating massive bottlenecks.</p>\n<p>Hazard pointers work like a sophisticated safety sign system. When a worker (thread) needs to enter a structure (access a node), they place a bright safety sign outside with their name and the structure&#39;s address. This sign announces to everyone: &quot;Worker Alice is inside Building 247—DO NOT DEMOLISH.&quot; When the worker finishes their task, they remove their safety sign.</p>\n<p>Meanwhile, the demolition crew (memory reclamation system) maintains a list of structures marked for demolition (retirement list). Before destroying any structure, they walk around the entire site checking for safety signs. If any worker has posted a sign for a particular building, that building stays standing. Only structures with no safety signs can be safely demolished.</p>\n<p>This system ensures safety (no worker gets crushed by falling debris) while maintaining parallelism (multiple crews can work simultaneously). The key insight is that workers announce their protection <em>before</em> entering, and the demolition crew respects those announcements by scanning all signs before any demolition.</p>\n<p>Just as construction workers might post multiple safety signs if they&#39;re moving between several structures during a complex task, threads maintain multiple hazard pointers to protect different nodes they&#39;re accessing during complex multi-step operations.</p>\n<h3 id=\"hazard-pointer-protocol\">Hazard Pointer Protocol</h3>\n<p>The hazard pointer protocol consists of four fundamental operations that work together to provide safe, non-blocking memory reclamation. Each thread maintains a small set of hazard pointer slots (typically 1-4 pointers) that can be used to protect nodes currently being accessed.</p>\n<p><strong>Decision: Per-Thread Hazard Pointer Slots</strong></p>\n<ul>\n<li><strong>Context</strong>: We need a way for threads to announce which pointers they&#39;re currently protecting, with minimal overhead and contention.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Global shared array with per-thread sections</li>\n<li>Per-thread local storage with global registry</li>\n<li>Lock-free linked list of hazard records</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Per-thread local storage with global registry</li>\n<li><strong>Rationale</strong>: Minimizes cache line contention since threads primarily access their own slots, while still allowing global scanning for reclamation. Local storage avoids false sharing between threads.</li>\n<li><strong>Consequences</strong>: Requires thread registration/deregistration on startup/shutdown, but provides optimal performance for the common case of protecting/releasing pointers.</li>\n</ul>\n<p>The core hazard pointer data structures organize protection and reclamation state both per-thread and globally:</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>HazardPointer.protected_pointers</code></td>\n<td>Array[AtomicReference]</td>\n<td>Per-thread array of currently protected node pointers</td>\n</tr>\n<tr>\n<td><code>HazardPointer.thread_registry</code></td>\n<td>AtomicReference to linked list</td>\n<td>Global list of all active thread hazard pointer slots</td>\n</tr>\n<tr>\n<td><code>RetirementList.retired_nodes</code></td>\n<td>Thread-local queue</td>\n<td>Queue of nodes this thread has removed but not yet reclaimed</td>\n</tr>\n<tr>\n<td><code>RetirementList.scan_threshold</code></td>\n<td>Integer</td>\n<td>Number of retired nodes that triggers a global scan</td>\n</tr>\n<tr>\n<td><code>HazardPointer.max_hazards_per_thread</code></td>\n<td>Constant</td>\n<td>Maximum number of simultaneous protections per thread (typically 2-4)</td>\n</tr>\n</tbody></table>\n<p>The protection protocol follows a careful sequence to prevent race conditions between protection and reclamation:</p>\n<table>\n<thead>\n<tr>\n<th>Operation</th>\n<th>Method Signature</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>protect</code></td>\n<td><code>protect(pointer)</code></td>\n<td><code>pointer</code>: Node pointer to protect</td>\n<td><code>slot_index</code>: Index of hazard slot used</td>\n<td>Announces protection for a specific node pointer</td>\n</tr>\n<tr>\n<td><code>release</code></td>\n<td><code>release(slot_index)</code></td>\n<td><code>slot_index</code>: Previously returned slot index</td>\n<td>None</td>\n<td>Clears protection from the specified hazard slot</td>\n</tr>\n<tr>\n<td><code>retire</code></td>\n<td><code>retire(node)</code></td>\n<td><code>node</code>: Node pointer to eventually reclaim</td>\n<td>None</td>\n<td>Adds node to thread&#39;s retirement list for deferred deletion</td>\n</tr>\n<tr>\n<td><code>scan_and_reclaim</code></td>\n<td><code>scan_and_reclaim()</code></td>\n<td>None</td>\n<td><code>reclaimed_count</code>: Number of nodes actually freed</td>\n<td>Scans all hazard pointers and frees unprotected retired nodes</td>\n</tr>\n</tbody></table>\n<p>The protection sequence requires careful ordering to prevent race conditions where a node might be retired and reclaimed between when a thread loads a pointer and when it protects that pointer:</p>\n<ol>\n<li><strong>Load the pointer</strong> from the shared data structure using appropriate memory ordering (typically <code>ACQUIRE</code> to ensure we see all writes to the pointed-to object)</li>\n<li><strong>Immediately protect the loaded pointer</strong> by storing it in an available hazard pointer slot using <code>SEQ_CST</code> ordering to ensure global visibility</li>\n<li><strong>Re-read the original pointer location</strong> to verify it hasn&#39;t changed since step 1</li>\n<li><strong>If the pointer changed, release protection and retry</strong> the entire sequence from step 1</li>\n<li><strong>If the pointer is unchanged, proceed with accessing the protected node</strong> knowing it cannot be reclaimed while protected</li>\n<li><strong>When finished with the node, release protection</strong> by clearing the hazard pointer slot</li>\n</ol>\n<blockquote>\n<p>The critical insight in hazard pointer protection is that we must protect first, then verify. Simply loading a pointer and then protecting it creates a race window where another thread could retire and reclaim the node between the load and protection.</p>\n</blockquote>\n<p><strong>Decision: Protect-Then-Verify Pattern</strong></p>\n<ul>\n<li><strong>Context</strong>: Race condition between loading a pointer and protecting it, where the node could be reclaimed in the gap.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Load pointer, then protect (unsafe—node could be freed)</li>\n<li>Protect pointer, then verify it&#39;s still current</li>\n<li>Use epoch-based reclamation instead</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Protect pointer, then verify it&#39;s still current</li>\n<li><strong>Rationale</strong>: Only this pattern guarantees that once we successfully protect a valid pointer, it remains valid for the duration of our access. The verification step catches cases where the pointer changed between our load and protection.</li>\n<li><strong>Consequences</strong>: Requires retry loops in data structure operations, but provides strong safety guarantees without blocking.</li>\n</ul>\n<p>The retirement and reclamation protocol operates on a per-thread basis with periodic global coordination:</p>\n<ol>\n<li><strong>When removing a node from a data structure</strong>, the removing thread calls <code>retire(node)</code> instead of immediately freeing the memory</li>\n<li><strong>The node is added to the thread&#39;s local retirement list</strong>, avoiding contention with other threads&#39; retirement lists</li>\n<li><strong>When the retirement list reaches the scan threshold</strong>, the thread triggers <code>scan_and_reclaim()</code></li>\n<li><strong>During scanning, the thread examines all hazard pointer slots</strong> from all registered threads in the system</li>\n<li><strong>Any retired node that appears in any thread&#39;s hazard pointer slots</strong> is kept in the retirement list (not reclaimed)</li>\n<li><strong>Nodes not protected by any hazard pointer</strong> are safely freed and removed from the retirement list</li>\n<li><strong>If many nodes remain protected after scanning</strong>, the thread may increase its scan threshold to reduce scanning frequency</li>\n</ol>\n<h3 id=\"retirement-list-and-scanning-algorithm\">Retirement List and Scanning Algorithm</h3>\n<p>The retirement list serves as a staging area where nodes wait between removal from the data structure and actual memory reclamation. This deferred reclamation is essential because other threads might still hold references to these nodes and be in the process of accessing them.</p>\n<p><strong>Decision: Per-Thread Retirement Lists vs Global Retirement List</strong></p>\n<ul>\n<li><strong>Context</strong>: We need to track nodes that have been removed from data structures but cannot yet be safely freed.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Single global retirement list protected by locks</li>\n<li>Single global lock-free retirement list</li>\n<li>Per-thread retirement lists with periodic scanning</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Per-thread retirement lists with periodic scanning</li>\n<li><strong>Rationale</strong>: Eliminates contention on retirement operations since each thread only modifies its own list. Scanning can be done independently by each thread, distributing the reclamation work.</li>\n<li><strong>Consequences</strong>: Requires more complex scanning logic that must examine all threads&#39; hazard pointers, but provides better scalability and cache locality.</li>\n</ul>\n<p>The retirement list maintains both immediate and batch processing capabilities:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>retired_nodes</code></td>\n<td>Queue[Node]</td>\n<td>FIFO queue of nodes removed but not yet reclaimed</td>\n</tr>\n<tr>\n<td><code>scan_threshold</code></td>\n<td>Integer</td>\n<td>Number of retired nodes that triggers automatic scanning</td>\n</tr>\n<tr>\n<td><code>last_scan_size</code></td>\n<td>Integer</td>\n<td>Size of retirement list after the most recent scan</td>\n</tr>\n<tr>\n<td><code>adaptive_threshold</code></td>\n<td>Boolean</td>\n<td>Whether to dynamically adjust scan threshold based on reclamation success rate</td>\n</tr>\n<tr>\n<td><code>max_retirement_size</code></td>\n<td>Integer</td>\n<td>Hard limit on retirement list size to prevent unbounded growth</td>\n</tr>\n</tbody></table>\n<p>The scanning algorithm performs a global survey of all hazard pointers to determine which retired nodes can be safely reclaimed:</p>\n<ol>\n<li><strong>Snapshot all active hazard pointers</strong> by iterating through the thread registry and reading each thread&#39;s hazard pointer slots</li>\n<li><strong>Create a temporary protected set</strong> containing all non-null hazard pointer values found during the snapshot</li>\n<li><strong>Iterate through the local retirement list</strong> examining each retired node</li>\n<li><strong>For each retired node, check if its address appears in the protected set</strong></li>\n<li><strong>If the node is not protected, immediately free its memory</strong> and remove it from the retirement list</li>\n<li><strong>If the node is protected, keep it in the retirement list</strong> for reconsideration during the next scan</li>\n<li><strong>Update statistics</strong> about scan effectiveness and potentially adjust the scan threshold</li>\n</ol>\n<blockquote>\n<p>A subtle but crucial detail: the scanning algorithm must use appropriate memory ordering when reading hazard pointers. Using <code>SEQ_CST</code> ordering ensures that we don&#39;t miss any protection announcements that were made concurrent with our scan.</p>\n</blockquote>\n<p>The adaptive threshold mechanism helps balance the overhead of frequent scanning against the memory overhead of keeping many retired nodes:</p>\n<table>\n<thead>\n<tr>\n<th>Condition</th>\n<th>Scan Threshold Adjustment</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>&gt;80% of nodes reclaimed</td>\n<td>Decrease threshold by 25%</td>\n<td>High reclamation rate suggests scanning is effective, so scan more frequently</td>\n</tr>\n<tr>\n<td>&lt;20% of nodes reclaimed</td>\n<td>Increase threshold by 50%</td>\n<td>Low reclamation rate suggests many nodes are still protected, so scan less frequently</td>\n</tr>\n<tr>\n<td>Retirement list exceeds max size</td>\n<td>Force immediate scan regardless of threshold</td>\n<td>Prevent unbounded memory growth even if scanning is ineffective</td>\n</tr>\n<tr>\n<td>Thread preparing to exit</td>\n<td>Scan until retirement list is empty</td>\n<td>Ensure no memory leaks when thread terminates</td>\n</tr>\n</tbody></table>\n<p><strong>Decision: Batch vs Immediate Scanning</strong></p>\n<ul>\n<li><strong>Context</strong>: When to trigger the expensive global scan operation that examines all threads&#39; hazard pointers.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Scan after every retire operation</li>\n<li>Scan when retirement list reaches a threshold</li>\n<li>Scan on a periodic timer</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Scan when retirement list reaches a threshold</li>\n<li><strong>Rationale</strong>: Batching scan operations amortizes the cost of examining all hazard pointers across multiple retired nodes. Timer-based scanning could waste work or delay reclamation unnecessarily.</li>\n<li><strong>Consequences</strong>: Requires tuning the threshold based on workload characteristics, but provides predictable memory overhead bounds.</li>\n</ul>\n<h3 id=\"integration-with-lock-free-data-structures\">Integration with Lock-free Data Structures</h3>\n<p>Integrating hazard pointers into existing lock-free data structures requires careful modification of the algorithms to include protection and retirement calls at appropriate points. The integration must preserve the original algorithms&#39; correctness and progress guarantees while adding memory safety.</p>\n<p>For the Treiber stack, hazard pointer integration focuses on protecting the top pointer during traversal and retiring nodes when they&#39;re successfully removed:</p>\n<p><strong>Modified Treiber Stack Pop Operation with Hazard Pointers:</strong></p>\n<ol>\n<li><strong>Allocate a hazard pointer slot</strong> for protecting the node we&#39;re about to access</li>\n<li><strong>Load the current top pointer</strong> using <code>ACQUIRE</code> memory ordering</li>\n<li><strong>If the top pointer is null (empty stack), release the hazard slot and return empty indication</strong></li>\n<li><strong>Protect the loaded top pointer</strong> by storing it in our hazard pointer slot with <code>SEQ_CST</code> ordering</li>\n<li><strong>Re-read the top pointer</strong> to verify it hasn&#39;t changed since our initial load</li>\n<li><strong>If the top pointer changed, release protection and retry</strong> from step 2 (another thread modified the stack)</li>\n<li><strong>Load the next pointer</strong> from the protected top node (this is safe because the node cannot be reclaimed while protected)</li>\n<li><strong>Attempt to CAS the stack&#39;s top pointer</strong> from the protected node to its next pointer</li>\n<li><strong>If CAS fails, release protection and retry</strong> from step 2 (another thread modified the stack concurrently)</li>\n<li><strong>If CAS succeeds, retire the old top node</strong> and release our hazard pointer protection</li>\n<li><strong>Return the data from the successfully popped node</strong></li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Integration Point</th>\n<th>Original Treiber Stack</th>\n<th>With Hazard Pointers</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Before accessing top node</td>\n<td><code>top = load(stack.top)</code></td>\n<td><code>slot = protect(load(stack.top))</code> then verify unchanged</td>\n</tr>\n<tr>\n<td>Before dereferencing node</td>\n<td>Direct access: <code>next = top.next</code></td>\n<td>Check protection first: <code>next = protected_top.next</code></td>\n</tr>\n<tr>\n<td>After successful pop</td>\n<td><code>free(old_top)</code></td>\n<td><code>retire(old_top)</code> and <code>release(slot)</code></td>\n</tr>\n<tr>\n<td>On retry/failure</td>\n<td>Continue with new top value</td>\n<td><code>release(slot)</code> then retry protection sequence</td>\n</tr>\n</tbody></table>\n<p>For the Michael-Scott queue, hazard pointer integration is more complex because both enqueue and dequeue operations may need to traverse multiple nodes and help other operations:</p>\n<p><strong>Modified Michael-Scott Queue Dequeue Operation with Hazard Pointers:</strong></p>\n<ol>\n<li><strong>Allocate hazard pointer slots</strong> for both head and its next node (we may need to protect both during the helping protocol)</li>\n<li><strong>Load and protect the current head pointer</strong> following the protect-then-verify pattern</li>\n<li><strong>Load and protect the head&#39;s next pointer</strong>, which points to the first actual data node (or null if empty)</li>\n<li><strong>Load the current tail pointer</strong> (no protection needed yet since we&#39;re just reading for comparison)</li>\n<li><strong>If head and tail point to the same node and next is null, the queue is empty</strong>—release all protections and return empty indication</li>\n<li><strong>If head and tail point to the same node but next is not null</strong>, the tail is lagging—help advance the tail pointer</li>\n<li><strong>Re-verify that head hasn&#39;t changed</strong> since our protection (helping operations might have modified it)</li>\n<li><strong>Read the data from the next node</strong> (this is safe because we&#39;re protecting the next node)</li>\n<li><strong>Attempt to CAS the head pointer</strong> to advance it from the current head to the next node</li>\n<li><strong>If CAS succeeds, retire the old head node</strong> (the dummy sentinel) and release protections</li>\n<li><strong>Return the data from the successfully dequeued node</strong></li>\n</ol>\n<blockquote>\n<p>The key insight in queue integration is that we may need to protect multiple nodes simultaneously during helping operations. The number of hazard pointer slots per thread must accommodate the maximum number of nodes any single operation might access concurrently.</p>\n</blockquote>\n<p><strong>Decision: Number of Hazard Pointer Slots Per Thread</strong></p>\n<ul>\n<li><strong>Context</strong>: Each thread needs a fixed number of hazard pointer slots to protect nodes during lock-free operations.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>One slot per thread (minimal overhead)</li>\n<li>Two slots per thread (covers most operations)</li>\n<li>Four slots per thread (handles complex helping scenarios)</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Four slots per thread</li>\n<li><strong>Rationale</strong>: Stack operations need 1 slot, basic queue operations need 2 slots, but queue helping operations may need 3-4 slots when protecting head, next, tail, and tail.next simultaneously during complex race conditions.</li>\n<li><strong>Consequences</strong>: Higher memory overhead per thread, but ensures all operations can make progress without deadlock due to insufficient protection slots.</li>\n</ul>\n<p>The integration also requires careful attention to the linearization points of the original algorithms. The linearization point—the moment when each operation appears to take effect atomically—must remain the same even with hazard pointer additions:</p>\n<table>\n<thead>\n<tr>\n<th>Operation</th>\n<th>Original Linearization Point</th>\n<th>With Hazard Pointers</th>\n<th>Verification</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Stack Push</td>\n<td>Successful CAS of top pointer</td>\n<td>Same: successful CAS of top pointer</td>\n<td>New node becomes visible atomically</td>\n</tr>\n<tr>\n<td>Stack Pop</td>\n<td>Successful CAS of top pointer</td>\n<td>Same: successful CAS of top pointer</td>\n<td>Node removal happens atomically</td>\n</tr>\n<tr>\n<td>Queue Enqueue</td>\n<td>Successful CAS linking new node</td>\n<td>Same: successful CAS linking new node</td>\n<td>New node becomes reachable atomically</td>\n</tr>\n<tr>\n<td>Queue Dequeue</td>\n<td>Successful CAS advancing head</td>\n<td>Same: successful CAS advancing head</td>\n<td>Node removal happens atomically</td>\n</tr>\n</tbody></table>\n<h3 id=\"common-hazard-pointer-pitfalls\">Common Hazard Pointer Pitfalls</h3>\n<p>The complexity of hazard pointer protocols creates numerous opportunities for subtle bugs that can lead to memory corruption, use-after-free errors, or memory leaks. Understanding these pitfalls is essential for correct implementation.</p>\n<p>⚠️ <strong>Pitfall: Protecting After Loading (Race Window)</strong></p>\n<p>The most dangerous mistake is loading a pointer and then trying to protect it in separate steps:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>// WRONG - race condition\nnode = load(stack.top);        // Node loaded\n// &lt;-- Node could be retired and freed here by another thread\nslot = protect(node);          // Too late - node might be garbage\ndata = node.data;              // Use-after-free!</code></pre></div>\n\n<p><strong>Why it&#39;s wrong</strong>: Between loading the pointer and protecting it, another thread could pop the node, retire it, scan and find no protections, then free the memory. When we finally protect the pointer, we&#39;re protecting a dangling reference.</p>\n<p><strong>How to fix</strong>: Always protect immediately after loading, then verify the pointer is still current:</p>\n<ol>\n<li>Load pointer</li>\n<li>Immediately protect the loaded value</li>\n<li>Re-read the original location to verify it&#39;s unchanged</li>\n<li>If changed, release protection and retry</li>\n</ol>\n<p>⚠️ <strong>Pitfall: Forgetting to Release Protection</strong></p>\n<p>Failing to release hazard pointer protection when finishing with a node:</p>\n<p><strong>Why it&#39;s wrong</strong>: The node will appear permanently protected during all future scans, causing a memory leak. Even worse, if the thread exits without releasing protection, the protection might persist indefinitely.</p>\n<p><strong>How to fix</strong>: Use RAII-style protection guards or ensure every protection operation has a corresponding release operation. Consider using defer statements or finally blocks to guarantee cleanup.</p>\n<p>⚠️ <strong>Pitfall: Insufficient Memory Ordering in Protection</strong></p>\n<p>Using relaxed memory ordering when setting hazard pointers:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>// WRONG - relaxed ordering\nprotect_slot.store(node, RELAXED);</code></pre></div>\n\n<p><strong>Why it&#39;s wrong</strong>: Other threads performing scans might not see the protection due to weak memory ordering, allowing them to reclaim the node while this thread is still using it.</p>\n<p><strong>How to fix</strong>: Use <code>SEQ_CST</code> or at least <code>RELEASE</code> ordering when setting hazard pointers to ensure global visibility:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>protect_slot.store(node, SEQ_CST);</code></pre></div>\n\n<p>⚠️ <strong>Pitfall: Unbounded Retirement List Growth</strong></p>\n<p>Allowing the retirement list to grow without bounds when reclamation is ineffective:</p>\n<p><strong>Why it&#39;s wrong</strong>: If many nodes remain protected for extended periods, the retirement list can consume unlimited memory, potentially causing out-of-memory conditions.</p>\n<p><strong>How to fix</strong>: Implement a maximum retirement list size with forced scanning when the limit is approached. Consider falling back to synchronous reclamation (with appropriate backoff) when asynchronous scanning is insufficient.</p>\n<p>⚠️ <strong>Pitfall: Thread Exit Without Cleanup</strong></p>\n<p>Threads exiting without cleaning up their hazard pointer slots and retirement lists:</p>\n<p><strong>Why it&#39;s wrong</strong>: Dead threads&#39; hazard pointer slots might contain stale protections that prevent reclamation forever. Their retirement lists become unreachable, causing memory leaks.</p>\n<p><strong>How to fix</strong>: Implement thread cleanup that:</p>\n<ol>\n<li>Clears all hazard pointer slots for the exiting thread</li>\n<li>Performs a final scan to reclaim all nodes in the thread&#39;s retirement list</li>\n<li>Removes the thread from the global registry</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Cleanup Phase</th>\n<th>Action Required</th>\n<th>Failure Consequence</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Clear hazard slots</td>\n<td>Set all slots to null with <code>SEQ_CST</code> ordering</td>\n<td>Permanent false protections preventing reclamation</td>\n</tr>\n<tr>\n<td>Final retirement scan</td>\n<td>Scan and reclaim until retirement list is empty</td>\n<td>Memory leak from unreachable retired nodes</td>\n</tr>\n<tr>\n<td>Registry removal</td>\n<td>Remove thread record from global registry</td>\n<td>Wasted scanning effort on dead thread slots</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: ABA Problem in Hazard Pointer Management</strong></p>\n<p>The hazard pointer system itself can suffer from ABA problems if thread registry nodes are reused:</p>\n<p><strong>Why it&#39;s wrong</strong>: If a thread exits and its registry node is immediately reused for a new thread, scans might incorrectly consider old hazard values as current protections.</p>\n<p><strong>How to fix</strong>: Use tagged pointers or generation counters in the thread registry, or delay reuse of registry nodes until sufficient time has passed.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The hazard pointer system requires careful coordination between low-level atomic operations and high-level memory management policies. This section provides concrete guidance for implementing a production-ready hazard pointer system.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Thread Registry</td>\n<td>Fixed-size array with atomic slots</td>\n<td>Lock-free linked list with dynamic growth</td>\n</tr>\n<tr>\n<td>Memory Ordering</td>\n<td>Sequential consistency everywhere</td>\n<td>Relaxed/acquire/release optimizations</td>\n</tr>\n<tr>\n<td>Retirement Storage</td>\n<td>Simple linked list per thread</td>\n<td>Circular buffer with batch processing</td>\n</tr>\n<tr>\n<td>Protection Interface</td>\n<td>Manual protect/release calls</td>\n<td>RAII guard objects with automatic cleanup</td>\n</tr>\n<tr>\n<td>Thread-Local Storage</td>\n<td>Standard library thread_local</td>\n<td>Custom per-thread data with manual registration</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>lock_free/\n  hazard_pointers/\n    __init__.py                 ← Public API exports\n    hazard_pointer.py           ← Core HazardPointer class\n    retirement_list.py          ← RetirementList implementation\n    thread_registry.py          ← Global thread management\n    protection_guard.py         ← RAII protection helpers\n    atomic_operations.py        ← Low-level atomic wrappers\n  tests/\n    test_hazard_pointers.py     ← Unit tests for hazard pointer operations\n    test_integration.py         ← Integration tests with stack/queue\n    test_stress.py              ← Multi-threaded stress tests</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Complete Thread Registry Implementation</strong> (ready to use):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .atomic_operations </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> AtomicReference, MemoryOrdering</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ThreadRecord</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Registry entry for one thread's hazard pointer slots.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    thread_id: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hazard_slots: List[AtomicReference]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    next_record: AtomicReference</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    active: AtomicReference  </span><span style=\"color:#6A737D\"># Boolean indicating if thread is still active</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ThreadRegistry</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Global registry of all threads using hazard pointers.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, max_hazards_per_thread: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_hazards_per_thread </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_hazards_per_thread</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.head </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> AtomicReference(</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Head of registry linked list</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.thread_local_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.local()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register_thread</span><span style=\"color:#E1E4E8\">(self) -> ThreadRecord:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register current thread and return its record.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        thread_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.get_ident()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Create hazard slots for this thread</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        hazard_slots </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            AtomicReference(</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">) </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> _ </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.max_hazards_per_thread)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Create new thread record</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        record </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ThreadRecord(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            thread_id</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">thread_id,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            hazard_slots</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">hazard_slots,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            next_record</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">AtomicReference(</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            active</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">AtomicReference(</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Insert into global registry using lock-free prepend</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            current_head </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.head.load(MemoryOrdering.</span><span style=\"color:#79B8FF\">ACQUIRE</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            record.next_record.store(current_head, MemoryOrdering.</span><span style=\"color:#79B8FF\">RELAXED</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.head.compare_and_swap(current_head, record):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                break</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Store in thread-local storage for fast access</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.thread_local_data.record </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> record</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> record</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_current_thread_record</span><span style=\"color:#E1E4E8\">(self) -> Optional[ThreadRecord]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get current thread's record, registering if necessary.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> hasattr</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.thread_local_data, </span><span style=\"color:#9ECBFF\">'record'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.register_thread()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.thread_local_data.record</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> iterate_all_records</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Iterate over all registered thread records.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        current </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.head.load(MemoryOrdering.</span><span style=\"color:#79B8FF\">ACQUIRE</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#E1E4E8\"> current </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> current.active.load(MemoryOrdering.</span><span style=\"color:#79B8FF\">RELAXED</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                yield</span><span style=\"color:#E1E4E8\"> current</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            current </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> current.next_record.load(MemoryOrdering.</span><span style=\"color:#79B8FF\">ACQUIRE</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> cleanup_thread</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Clean up current thread's registration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> hasattr</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.thread_local_data, </span><span style=\"color:#9ECBFF\">'record'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            record </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.thread_local_data.record</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            record.active.store(</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">, MemoryOrdering.</span><span style=\"color:#79B8FF\">RELEASE</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Clear all hazard slots</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> slot </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> record.hazard_slots:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                slot.store(</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">, MemoryOrdering.</span><span style=\"color:#79B8FF\">SEQ_CST</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Complete Atomic Operations Wrapper</strong> (ready to use):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Tuple, Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MemoryOrdering</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RELAXED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"relaxed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ACQUIRE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"acquire\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RELEASE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"release\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SEQ_CST</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"seq_cst\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AtomicReference</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Thread-safe atomic reference with memory ordering support.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, initial_value: Any </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> initial_value</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Lock()  </span><span style=\"color:#6A737D\"># Simplified implementation using lock</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> load</span><span style=\"color:#E1E4E8\">(self, ordering: MemoryOrdering </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MemoryOrdering.</span><span style=\"color:#79B8FF\">SEQ_CST</span><span style=\"color:#E1E4E8\">) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Atomically load the current value.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: In production, implement lock-free atomic operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # using ctypes and platform-specific atomic instructions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> store</span><span style=\"color:#E1E4E8\">(self, new_value: Any, ordering: MemoryOrdering </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MemoryOrdering.</span><span style=\"color:#79B8FF\">SEQ_CST</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Atomically store a new value.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> new_value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> compare_and_swap</span><span style=\"color:#E1E4E8\">(self, expected: Any, new_value: Any) -> Tuple[</span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Atomically compare and swap. Returns (success, observed_value).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            current </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> current </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> expected:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> new_value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, current)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">, current)</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>HazardPointer Class</strong> (implement the TODOs):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> HazardPointer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Main hazard pointer system managing protection and reclamation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, max_hazards_per_thread: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.thread_registry </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ThreadRegistry(max_hazards_per_thread)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_hazards_per_thread </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_hazards_per_thread</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> protect</span><span style=\"color:#E1E4E8\">(self, pointer: Any) -> Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Protect a pointer from reclamation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns slot index if successful, None if no slots available.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get current thread's record from registry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Find an available hazard slot (one that contains None)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Store the pointer in the slot with SEQ_CST ordering</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return the slot index for later release</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: If no slots available, return None to indicate failure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> release</span><span style=\"color:#E1E4E8\">(self, slot_index: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Release protection from the specified slot.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get current thread's record from registry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Verify slot_index is valid (0 &#x3C;= slot_index &#x3C; max_hazards)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Clear the hazard slot by storing None with SEQ_CST ordering</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Always use SEQ_CST to ensure immediate global visibility</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> protect_and_verify</span><span style=\"color:#E1E4E8\">(self, atomic_ref: AtomicReference) -> Tuple[Any, Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Load a pointer and protect it, verifying it hasn't changed.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns (pointer, slot_index) or (None, None) if protection failed.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load the current value from atomic_ref with ACQUIRE ordering</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If value is None, return (None, None) immediately  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Protect the loaded pointer using protect()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Re-load the atomic_ref to verify it hasn't changed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: If changed, release protection and return (None, None)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: If unchanged, return (pointer, slot_index)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: This implements the protect-then-verify pattern safely</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RetirementList</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Per-thread list of retired nodes awaiting reclamation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, scan_threshold: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 64</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.retired_nodes </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []  </span><span style=\"color:#6A737D\"># Simple list implementation</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.scan_threshold </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scan_threshold</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.hazard_pointer_system </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">  # Set by HazardPointer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> retire</span><span style=\"color:#E1E4E8\">(self, node: Any):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Add a node to the retirement list for eventual reclamation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Add the node to self.retired_nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check if len(retired_nodes) >= self.scan_threshold</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If threshold reached, call self.scan_and_reclaim()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: This implements batched scanning for efficiency</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> scan_and_reclaim</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Scan all hazard pointers and reclaim unprotected nodes.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns number of nodes actually reclaimed.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Collect all protected pointers by iterating thread registry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For each thread record, read all hazard slots with ACQUIRE ordering</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Create a set of all non-None protected pointers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Iterate through self.retired_nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: For each retired node, check if it's in the protected set</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: If not protected, free the node and remove from retired_nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: If protected, keep it in retired_nodes for next scan</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Return count of nodes actually reclaimed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use set lookup for O(1) protection checking</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints\">Language-Specific Hints</h4>\n<p><strong>Python Implementation Notes:</strong></p>\n<ul>\n<li>Use <code>threading.local()</code> for efficient thread-local storage access</li>\n<li>Consider <code>weakref.WeakSet</code> for tracking protected pointers to avoid reference cycles</li>\n<li>Use <code>ctypes</code> for true atomic operations, or <code>threading.Lock</code> for simplified prototyping</li>\n<li>Implement <code>__enter__</code> and <code>__exit__</code> methods on protection guards for automatic cleanup</li>\n</ul>\n<p><strong>Memory Management:</strong></p>\n<ul>\n<li>Call <code>del</code> explicitly on reclaimed nodes to trigger immediate cleanup</li>\n<li>Use <code>gc.collect()</code> in tests to verify no memory leaks</li>\n<li>Consider <code>tracemalloc</code> module for debugging memory usage patterns</li>\n</ul>\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing hazard pointers, verify correct behavior:</p>\n<p><strong>Basic Functionality Test:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_hazard_pointer_protection</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> HazardPointer()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    node </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Node(</span><span style=\"color:#9ECBFF\">\"test_data\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Protect the node</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    slot </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> hp.protect(node)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> slot </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Retire the node</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retirement </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> RetirementList()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retirement.hazard_pointer_system </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> hp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retirement.retire(node)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Scan should not reclaim protected node</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    reclaimed </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> retirement.scan_and_reclaim()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> reclaimed </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(retirement.retired_nodes) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Release protection</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hp.release(slot)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Now scan should reclaim the node</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    reclaimed </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> retirement.scan_and_reclaim()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> reclaimed </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(retirement.retired_nodes) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span></span></code></pre></div>\n\n<p><strong>Integration Test with Treiber Stack:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_stack_with_hazard_pointers</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stack </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TreiberStack()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> HazardPointer()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Push some items</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stack.push(</span><span style=\"color:#9ECBFF\">\"item1\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stack.push(</span><span style=\"color:#9ECBFF\">\"item2\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Pop with hazard pointer protection</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    popped </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> stack.pop_with_hazard_pointers(hp)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> popped </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"item2\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Verify stack still has one item</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> stack.is_empty()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> stack.pop_with_hazard_pointers(hp) </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"item1\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> stack.is_empty()</span></span></code></pre></div>\n\n<p><strong>Stress Test Command:</strong>\nRun <code>python -m pytest tests/test_stress.py -v</code> to verify:</p>\n<ul>\n<li>Multiple threads can protect/release concurrently without conflicts</li>\n<li>Retirement and reclamation work correctly under high contention  </li>\n<li>No memory leaks occur during extended operation</li>\n<li>Performance remains reasonable under stress</li>\n</ul>\n<p><strong>Expected Output:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>test_concurrent_protection ... PASSED\ntest_retirement_under_contention ... PASSED  \ntest_memory_leak_detection ... PASSED\ntest_performance_benchmark ... PASSED (throughput &gt; 1M ops/sec)</code></pre></div>\n\n<p><strong>Signs Something Is Wrong:</strong></p>\n<ul>\n<li><strong>Segmentation faults</strong>: Usually indicates use-after-free from insufficient protection</li>\n<li><strong>Memory usage growing unbounded</strong>: Retirement lists not being scanned or protections not being released</li>\n<li><strong>Threads hanging</strong>: Deadlock in protection slots or registry operations</li>\n<li><strong>Performance degradation</strong>: Too frequent scanning or contention on registry</li>\n</ul>\n<p><strong>Debugging Steps:</strong></p>\n<ol>\n<li>Add logging to protection/release operations to verify balance</li>\n<li>Monitor retirement list sizes across threads  </li>\n<li>Check that thread cleanup happens on exit</li>\n<li>Use memory debugging tools to detect use-after-free errors</li>\n</ol>\n<h2 id=\"lock-free-hash-map\">Lock-free Hash Map</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 5 (Lock-free Hash Map) - This section implements a concurrent hash map using split-ordered lists with lock-free bucket operations and incremental resizing that maintains performance under high contention.</p>\n</blockquote>\n<p>The culmination of our lock-free data structure journey leads us to one of the most challenging concurrent data structures: the hash map. Unlike stacks and queues which operate on single points of contention (top pointer or head/tail pointers), hash maps present multiple buckets that can be accessed concurrently, along with the complex challenge of resizing the underlying array while concurrent operations continue. The split-ordered list approach represents an elegant solution that maintains logical hash ordering within a physical linked list structure, enabling incremental resizing without global synchronization.</p>\n<p><img src=\"/api/project/lock-free-structures/architecture-doc/asset?path=diagrams%2Fhashmap-structure.svg\" alt=\"Split-Ordered Hash Map Structure\"></p>\n<p>A concurrent hash map must satisfy several demanding requirements simultaneously. It must provide constant-time average case performance for insert, lookup, and delete operations while supporting concurrent access from multiple threads. The hash map must handle dynamic resizing as the load factor increases, migrating entries to a larger bucket array without blocking ongoing operations. Most critically, it must maintain consistency guarantees ensuring that no updates are lost and no phantom reads occur during concurrent modifications.</p>\n<h3 id=\"mental-model-hash-map-as-a-growing-office-building\">Mental Model: Hash Map as a Growing Office Building</h3>\n<p>Think of our lock-free hash map as a dynamically growing office building where employees (threads) are constantly moving in and out of offices (buckets) while the building is being expanded. Each floor of the building represents a power-of-two size of the hash table, and each office on a floor corresponds to a hash bucket containing a chain of desks (linked list nodes).</p>\n<p>In a traditional locked hash map, expanding the building would require evacuating all employees, closing the entire building, constructing new floors, moving all the furniture, and then reopening. This approach causes significant disruption and blocks all productivity during the expansion process. Workers queue up outside the building, unable to access their offices or complete their tasks.</p>\n<p>Our lock-free hash map takes a radically different approach. Instead of closing the building, we use a clever addressing system that allows employees to find their correct offices even while construction is ongoing. Each office has a logical address (the hash value) that remains stable, but the physical location might change as floors are added. When an employee needs to find office number 13, they follow a trail of forwarding addresses that eventually leads them to the correct physical location, whether it&#39;s on the old floor or a newly constructed floor.</p>\n<p>The split-ordered list acts like a building-wide directory system where all offices are logically connected in a single continuous chain, even though they&#39;re physically distributed across different floors. Sentinel nodes serve as floor markers - permanent signposts that help employees navigate between floors and ensure they don&#39;t get lost during construction. When we need to split office 13 into offices 13 and 29 (during resizing), we don&#39;t need to move all the furniture immediately. Instead, we update the directory system so employees can find the right office, and the physical reorganization happens incrementally as employees naturally visit their offices.</p>\n<p>This mental model captures the key insight of split-ordered lists: maintaining logical consistency through directory indirection while allowing physical changes to happen incrementally and without blocking concurrent access.</p>\n<h3 id=\"split-ordered-list-design\">Split-Ordered List Design</h3>\n<p>The split-ordered list represents a fundamental breakthrough in lock-free hash map design, solving the core problem of maintaining hash ordering during concurrent resizing. Traditional hash maps store entries directly in bucket arrays, making resizing a complex operation that requires rehashing all entries simultaneously. The split-ordered approach maintains all hash map entries in a single logical linked list ordered by hash value, while using a bucket array as an index into this list.</p>\n<blockquote>\n<p><strong>Decision: Split-Ordered List with Logical Hash Ordering</strong></p>\n<ul>\n<li><strong>Context</strong>: Hash map resizing traditionally requires rehashing all entries and blocking concurrent operations, creating scalability bottlenecks in high-contention scenarios</li>\n<li><strong>Options Considered</strong>: Direct bucket array with global locks, segment-based locking (Java ConcurrentHashMap style), split-ordered list approach</li>\n<li><strong>Decision</strong>: Implement split-ordered list maintaining logical hash order in a physical linked list structure</li>\n<li><strong>Rationale</strong>: Enables incremental resizing without blocking operations, provides natural lock-free insertion points, and maintains cache locality through linked list traversal</li>\n<li><strong>Consequences</strong>: Slightly higher memory overhead per entry, more complex implementation, but dramatically better scalability and no blocking during resize operations</li>\n</ul>\n</blockquote>\n<p>The core innovation lies in the <strong>reverse bit ordering</strong> technique used for hash values. Instead of using hash values directly, we reverse the bits of each hash value to determine the logical ordering in the split-ordered list. This reversal ensures that when we split a bucket during resizing, the relative ordering of entries remains consistent. For example, if bucket 2 (binary 010) needs to split, the new bucket will be 6 (binary 110), and the reversed bit ordering ensures entries naturally fall into the correct bucket without complex migration logic.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>buckets</code></td>\n<td><code>array of AtomicReference</code></td>\n<td>Index array where each element points to the first node of a bucket&#39;s chain</td>\n</tr>\n<tr>\n<td><code>size</code></td>\n<td><code>atomic integer</code></td>\n<td>Current number of entries in the hash map for load factor calculation</td>\n</tr>\n<tr>\n<td><code>bucket_mask</code></td>\n<td><code>atomic integer</code></td>\n<td>Current bucket array size minus one, used for hash modulo operations</td>\n</tr>\n<tr>\n<td><code>resize_in_progress</code></td>\n<td><code>atomic boolean</code></td>\n<td>Flag indicating whether a resize operation is currently active</td>\n</tr>\n</tbody></table>\n<p>The split-ordered list maintains two types of nodes: <strong>data nodes</strong> containing actual key-value pairs and <strong>sentinel nodes</strong> that mark bucket boundaries. Sentinel nodes are permanent markers that never get deleted, providing stable reference points during bucket splitting operations. Each sentinel node represents a specific bucket index and contains a special key that sorts correctly in the reverse bit order.</p>\n<table>\n<thead>\n<tr>\n<th>Node Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>key</code></td>\n<td><code>Any</code></td>\n<td>Either user key (data node) or bucket index (sentinel node)</td>\n</tr>\n<tr>\n<td><code>value</code></td>\n<td><code>Any</code></td>\n<td>User value for data nodes, null for sentinel nodes</td>\n</tr>\n<tr>\n<td><code>hash</code></td>\n<td><code>integer</code></td>\n<td>Reverse bit-ordered hash value for ordering within the list</td>\n</tr>\n<tr>\n<td><code>next</code></td>\n<td><code>AtomicReference</code></td>\n<td>Pointer to next node in the split-ordered list</td>\n</tr>\n<tr>\n<td><code>is_sentinel</code></td>\n<td><code>boolean</code></td>\n<td>Flag distinguishing sentinel nodes from data nodes</td>\n</tr>\n</tbody></table>\n<p>The bucket array serves as a sparse index into the split-ordered list, where each bucket entry points to the sentinel node that begins that bucket&#39;s logical section of the list. When accessing bucket <code>i</code>, we first ensure that bucket <code>i</code> has been initialized by checking if <code>buckets[i]</code> is non-null. If the bucket hasn&#39;t been initialized, we must recursively ensure that the parent bucket <code>i/2</code> exists and then create the sentinel node for bucket <code>i</code> by splitting from the parent.</p>\n<p>The logical ordering within the split-ordered list follows a specific pattern that ensures correctness during bucket splits. All entries with reverse bit-ordered hash values between sentinel node <code>i</code> and sentinel node <code>j</code> belong to the bucket range starting at <code>i</code>. When bucket <code>i</code> splits to create bucket <code>i + bucket_count</code>, the new sentinel node is inserted at the appropriate position, and entries naturally segregate based on their reverse bit-ordered hash values.</p>\n<blockquote>\n<p>The critical insight is that reverse bit ordering ensures that split operations preserve the relative ordering of entries. When bucket 2 splits into buckets 2 and 6, entries that belonged to bucket 2 will have their hash values naturally distribute between the two buckets based on their high-order bits, without requiring any entry movement.</p>\n</blockquote>\n<p><strong>Bucket Initialization Algorithm:</strong></p>\n<ol>\n<li>Calculate the target bucket index using <code>hash &amp; bucket_mask</code></li>\n<li>Check if <code>buckets[target_index]</code> is already initialized (non-null)</li>\n<li>If uninitialized, recursively ensure parent bucket <code>target_index / 2</code> exists</li>\n<li>Create new sentinel node with reverse bit-ordered hash for <code>target_index</code></li>\n<li>Find insertion point in parent bucket&#39;s chain using lock-free list insertion</li>\n<li>Use <code>compare_and_swap</code> to insert sentinel node at correct position</li>\n<li>Update <code>buckets[target_index]</code> to point to the newly created sentinel node</li>\n<li>Return the initialized bucket&#39;s sentinel node for subsequent operations</li>\n</ol>\n<p>This initialization protocol ensures that bucket splits happen incrementally and only when needed, avoiding the overhead of pre-initializing all possible buckets while maintaining the invariant that accessing any bucket index will always succeed.</p>\n<h3 id=\"incremental-resizing-algorithm\">Incremental Resizing Algorithm</h3>\n<p>The incremental resizing algorithm represents one of the most sophisticated aspects of the split-ordered hash map, enabling the hash map to grow dynamically without blocking concurrent operations. Unlike traditional hash maps that require a global rehashing phase, split-ordered lists support incremental migration where individual buckets are split on-demand as they&#39;re accessed.</p>\n<p>The resizing process is triggered when the load factor (ratio of entries to buckets) exceeds a predetermined threshold, typically 0.75 or 1.0. However, rather than immediately migrating all entries, the algorithm sets a resize flag and begins the gradual process of bucket splitting. This approach ensures that the cost of resizing is amortized across many operations rather than concentrated in a single expensive phase.</p>\n<blockquote>\n<p><strong>Decision: Threshold-Based Incremental Resizing</strong></p>\n<ul>\n<li><strong>Context</strong>: Hash maps need dynamic capacity adjustment to maintain performance, but global resizing creates blocking operations that violate lock-freedom</li>\n<li><strong>Options Considered</strong>: Global rehashing with temporary blocking, segment-based migration, incremental on-demand splitting</li>\n<li><strong>Decision</strong>: Trigger resize at load factor threshold with incremental on-demand bucket splitting</li>\n<li><strong>Rationale</strong>: Maintains lock-freedom by never blocking operations, amortizes resize cost across many operations, and allows concurrent access during resize</li>\n<li><strong>Consequences</strong>: Temporary performance degradation during resize period, increased memory usage during transition, but preserves scalability guarantees</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Resize State</th>\n<th>Bucket Array Size</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>STABLE</code></td>\n<td><code>N</code></td>\n<td>Normal operation with single bucket array of size N</td>\n</tr>\n<tr>\n<td><code>GROWING</code></td>\n<td><code>N</code> and <code>2N</code></td>\n<td>Transition state with both old and new arrays active</td>\n</tr>\n<tr>\n<td><code>MIGRATING</code></td>\n<td><code>2N</code></td>\n<td>New array active, old array being incrementally retired</td>\n</tr>\n</tbody></table>\n<p>The resize operation follows a carefully orchestrated sequence that maintains consistency across all concurrent operations:</p>\n<p><strong>Resize Trigger and Initialization:</strong></p>\n<ol>\n<li>Monitor the load factor by comparing <code>size</code> to <code>bucket_count</code></li>\n<li>When load factor exceeds threshold, attempt to acquire resize responsibility using <code>compare_and_swap</code> on <code>resize_in_progress</code> flag</li>\n<li>Allocate new bucket array with double the current capacity</li>\n<li>Initialize the new array&#39;s first bucket (index 0) with appropriate sentinel node</li>\n<li>Atomically update <code>bucket_mask</code> to reflect the new array size</li>\n<li>Begin incremental migration by updating bucket access paths</li>\n</ol>\n<p><strong>On-Demand Bucket Splitting:</strong></p>\n<p>During the resize period, each bucket access triggers potential splitting logic. When a thread accesses bucket <code>i</code> and discovers that it maps to an old bucket that should be split, the thread performs the splitting operation before proceeding with its original operation. This approach distributes the migration work across all threads using the hash map, rather than burdening a single thread with the entire migration.</p>\n<p>The splitting process involves creating a new sentinel node for the split bucket and redistributing entries based on their reverse bit-ordered hash values. Entries whose hash values have a specific bit pattern remain in the original bucket, while entries with the complementary bit pattern migrate to the new bucket. The beauty of reverse bit ordering is that this segregation happens naturally without examining each entry individually.</p>\n<p><strong>Split Operation Sequence:</strong></p>\n<ol>\n<li>Identify the bucket index <code>old_bucket</code> that needs to be split</li>\n<li>Calculate the new bucket index as <code>new_bucket = old_bucket + current_bucket_count</code></li>\n<li>Create sentinel node for <code>new_bucket</code> with appropriate reverse bit-ordered hash</li>\n<li>Traverse the split-ordered list starting from <code>old_bucket</code> sentinel</li>\n<li>For each entry, determine whether it belongs to <code>old_bucket</code> or <code>new_bucket</code> based on hash bits</li>\n<li>Use lock-free list manipulation to move entries to their correct bucket chains</li>\n<li>Update <code>buckets[new_bucket]</code> to point to the new sentinel node</li>\n<li>Continue with the original hash map operation (insert, lookup, or delete)</li>\n</ol>\n<p>The migration process maintains atomicity through careful ordering of operations and the use of helping mechanisms. If multiple threads attempt to split the same bucket simultaneously, the compare-and-swap operations ensure that only one thread succeeds in creating the new sentinel node, while other threads help complete the migration before proceeding.</p>\n<blockquote>\n<p>The key insight for incremental resizing is that split-ordered lists make bucket splitting a local operation that only affects the specific bucket chain being split, rather than a global operation requiring coordination across the entire hash map.</p>\n</blockquote>\n<p><strong>Migration Completion Detection:</strong></p>\n<p>The resize operation completes when all accessed buckets have been split and no threads are actively using the old bucket array. However, detecting completion in a lock-free environment requires careful coordination to avoid premature cleanup of data structures that might still be in use.</p>\n<table>\n<thead>\n<tr>\n<th>Completion Condition</th>\n<th>Check Method</th>\n<th>Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>All accessed buckets split</code></td>\n<td>Scan bucket array for unsplit entries</td>\n<td>Continue migration for remaining buckets</td>\n</tr>\n<tr>\n<td><code>No threads in old array</code></td>\n<td>Hazard pointer scan for old array references</td>\n<td>Wait for threads to complete operations</td>\n</tr>\n<tr>\n<td><code>Migration fully complete</code></td>\n<td>Both conditions satisfied</td>\n<td>Deallocate old array and clear resize flag</td>\n</tr>\n</tbody></table>\n<p>The completion detection leverages hazard pointers to ensure that the old bucket array is not deallocated while threads might still hold references to it. Threads protect array references using hazard pointers, and the resize completion logic scans all hazard pointer slots to verify that no protected references to the old array remain.</p>\n<h3 id=\"lock-free-bucket-operations\">Lock-free Bucket Operations</h3>\n<p>The bucket operations (insert, lookup, and delete) represent the core functionality of the hash map and must maintain correctness while operating on the split-ordered list structure. Each operation must handle the complexities of concurrent modification, bucket initialization, and potential resize operations, all while providing linearizable semantics.</p>\n<p>The fundamental challenge in bucket operations stems from the need to traverse linked list chains that may be concurrently modified by other threads. Unlike simple lock-free lists, the split-ordered list requires additional logic to handle sentinel nodes, bucket boundaries, and the interaction between regular operations and resize-triggered bucket splits.</p>\n<p><strong>Hash Map Insert Operation:</strong></p>\n<p>The insert operation must locate the correct bucket, potentially initialize it if it doesn&#39;t exist, traverse the bucket&#39;s chain to find the insertion point, and atomically insert the new entry while handling concurrent modifications from other threads.</p>\n<table>\n<thead>\n<tr>\n<th>Insert Step</th>\n<th>Action</th>\n<th>Concurrency Considerations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>1. Hash calculation</code></td>\n<td>Compute hash value and reverse bits</td>\n<td>Pure function, no concurrency issues</td>\n</tr>\n<tr>\n<td><code>2. Bucket identification</code></td>\n<td>Calculate bucket index using mask</td>\n<td>May require reading volatile bucket mask during resize</td>\n</tr>\n<tr>\n<td><code>3. Bucket initialization</code></td>\n<td>Ensure target bucket exists</td>\n<td>May require recursive parent bucket initialization</td>\n</tr>\n<tr>\n<td><code>4. Insertion point search</code></td>\n<td>Traverse chain to find position</td>\n<td>Must handle concurrent insertions and deletions</td>\n</tr>\n<tr>\n<td><code>5. Atomic insertion</code></td>\n<td>CAS new node into list</td>\n<td>May fail due to concurrent modifications, requires retry</td>\n</tr>\n</tbody></table>\n<p>The insert operation begins by computing the hash value of the key and applying reverse bit ordering to determine its position in the split-ordered list. The bucket index is calculated using the current <code>bucket_mask</code>, which may change during resize operations. To handle this correctly, the operation must be prepared to retry if the mask changes during execution.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Insert Algorithm Detailed Steps:\n1. Calculate reverse bit-ordered hash value for the key\n2. Read current bucket_mask atomically to determine bucket index\n3. Call ensure_bucket_exists(bucket_index) to initialize bucket if necessary\n4. Load sentinel node pointer from buckets[bucket_index] with acquire ordering\n5. Traverse split-ordered list starting from sentinel node\n6. Compare each node's hash with target hash to find insertion point\n7. Handle three cases: key exists (update value), insert before current node, insert at end\n8. Create new node with key, value, and appropriate hash ordering\n9. Use compare_and_swap to atomically link new node into chain\n10. If CAS fails due to concurrent modification, retry from step 5\n11. On successful insertion, increment global size counter\n12. Check if load factor threshold exceeded and trigger resize if necessary</code></pre></div>\n\n<p>The traversal logic must carefully handle both data nodes and sentinel nodes, ensuring that the insertion respects the reverse bit ordering invariant. When a matching key is found, the operation becomes an update rather than an insertion, requiring a different CAS pattern to atomically replace the value field.</p>\n<p><strong>Hash Map Lookup Operation:</strong></p>\n<p>The lookup operation provides the simplest case for bucket operations, requiring only traversal of the appropriate bucket chain without any structural modifications. However, it must still handle concurrent modifications and bucket initialization correctly.</p>\n<p>The lookup begins with the same hash calculation and bucket identification as insert, followed by a traversal of the bucket&#39;s chain looking for a matching key. The operation must be careful to handle concurrent deletions that might remove nodes during traversal, using appropriate memory ordering to ensure visibility of updates.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Lookup Algorithm Detailed Steps:\n1. Calculate reverse bit-ordered hash value for the key\n2. Read current bucket_mask atomically to determine bucket index\n3. Call ensure_bucket_exists(bucket_index) to initialize bucket if necessary\n4. Load sentinel node pointer from buckets[bucket_index] with acquire ordering\n5. Traverse split-ordered list comparing keys for exact match\n6. Skip sentinel nodes during traversal (check is_sentinel flag)\n7. If matching key found, return associated value with success indicator\n8. If traversal reaches next bucket's sentinel or end of list, return not found\n9. Handle concurrent deletions by restarting traversal if inconsistent state detected</code></pre></div>\n\n<p>The lookup operation benefits from not requiring any CAS operations, making it naturally wait-free once the bucket initialization is complete. The only potential blocking point occurs during bucket initialization, which may require recursive initialization of parent buckets.</p>\n<p><strong>Hash Map Delete Operation:</strong></p>\n<p>The delete operation presents the most complex case for bucket operations, requiring atomic removal of nodes from the linked list while handling concurrent access from other threads. The split-ordered list uses a two-phase deletion protocol: logical deletion followed by physical deletion.</p>\n<table>\n<thead>\n<tr>\n<th>Deletion Phase</th>\n<th>Action</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Logical deletion</code></td>\n<td>Mark node as deleted using CAS</td>\n<td>Prevents concurrent operations from seeing deleted entry</td>\n</tr>\n<tr>\n<td><code>Physical deletion</code></td>\n<td>Unlink node from list using CAS</td>\n<td>Reclaims memory and maintains list structure</td>\n</tr>\n</tbody></table>\n<p>The logical deletion phase marks the target node as deleted by setting a deletion flag, preventing other operations from accessing the node&#39;s value while maintaining the list structure temporarily. This approach ensures that concurrent traversals don&#39;t encounter partially deleted nodes that could lead to inconsistent states.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Delete Algorithm Detailed Steps:\n1. Calculate reverse bit-ordered hash value for the key\n2. Read current bucket_mask atomically to determine bucket index\n3. Call ensure_bucket_exists(bucket_index) to initialize bucket if necessary\n4. Load sentinel node pointer from buckets[bucket_index] with acquire ordering\n5. Traverse split-ordered list to find node with matching key\n6. If key not found, return not-found indicator\n7. Attempt logical deletion by CAS on node's deletion flag\n8. If logical deletion succeeds, attempt physical deletion by updating predecessor's next pointer\n9. If physical deletion fails, leave for subsequent operations to complete\n10. Decrement global size counter to reflect removal\n11. Return success indicator with deleted value</code></pre></div>\n\n<p>The delete operation leverages helping mechanisms where subsequent operations assist in completing physical deletions that may have been interrupted. When a thread encounters a logically deleted node during traversal, it attempts to complete the physical deletion before continuing with its own operation.</p>\n<blockquote>\n<p>The two-phase deletion protocol ensures that deleted entries become immediately invisible to new operations (logical deletion) while allowing the physical cleanup to happen asynchronously without blocking subsequent operations.</p>\n</blockquote>\n<p><strong>Helping Mechanisms in Bucket Operations:</strong></p>\n<p>All bucket operations implement helping mechanisms to ensure progress guarantees in the presence of concurrent modifications. When an operation encounters evidence of incomplete work by other threads (such as logically deleted nodes or partially completed bucket splits), it assists in completing the work before proceeding.</p>\n<table>\n<thead>\n<tr>\n<th>Helping Scenario</th>\n<th>Detection</th>\n<th>Assistance Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Incomplete bucket split</code></td>\n<td>Accessing bucket with missing sentinel</td>\n<td>Complete sentinel node creation and linking</td>\n</tr>\n<tr>\n<td><code>Incomplete physical deletion</code></td>\n<td>Traversing logically deleted node</td>\n<td>Attempt to unlink node from predecessor</td>\n</tr>\n<tr>\n<td><code>Stalled resize operation</code></td>\n<td>Load factor exceeds threshold significantly</td>\n<td>Assist with bucket splitting for accessed buckets</td>\n</tr>\n</tbody></table>\n<p>The helping mechanisms are implemented using idempotent operations that can be safely executed multiple times without causing correctness violations. Multiple threads may attempt the same helping action simultaneously, with CAS operations ensuring that only one thread succeeds while others safely fail and continue.</p>\n<h3 id=\"common-hash-map-implementation-pitfalls\">Common Hash Map Implementation Pitfalls</h3>\n<p>Lock-free hash map implementation presents numerous subtle pitfalls that can lead to correctness violations, performance degradation, or system instability. Understanding these common mistakes and their prevention strategies is crucial for successful implementation.</p>\n<p>⚠️ <strong>Pitfall: Incorrect Bucket Initialization Order</strong></p>\n<p>Many implementations fail to properly handle the recursive nature of bucket initialization, leading to deadlocks or inconsistent bucket states. The split-ordered list requires that parent buckets be initialized before child buckets, but naive implementations may attempt to initialize buckets in arbitrary order.</p>\n<p>The problem manifests when thread A tries to initialize bucket 6 while thread B tries to initialize bucket 2 (the parent of bucket 6). If both threads attempt to initialize their target buckets simultaneously without proper ordering, they may create inconsistent sentinel nodes or miss the parent-child relationship entirely.</p>\n<p><strong>Prevention Strategy</strong>: Always implement recursive parent initialization with proper termination conditions. Use a canonical bucket initialization algorithm that ensures parents exist before creating children, and use atomic flags to prevent duplicate initialization work.</p>\n<p>⚠️ <strong>Pitfall: Reverse Bit Calculation Errors</strong></p>\n<p>The reverse bit ordering calculation is fundamental to split-ordered list correctness, but it&#39;s easy to implement incorrectly. Common errors include reversing the wrong number of bits, using signed integer arithmetic that introduces sign extension, or failing to account for the current bucket array size in the reversal calculation.</p>\n<p>Incorrect reverse bit calculation causes entries to sort incorrectly in the split-ordered list, leading to entries appearing in wrong buckets after split operations. This manifests as keys that can be inserted but not found, or entries that appear to migrate randomly between buckets.</p>\n<p><strong>Prevention Strategy</strong>: Implement reverse bit calculation as a separate, thoroughly tested utility function. Use explicit bit manipulation with unsigned integers, and include comprehensive unit tests that verify correct ordering for various hash values and bucket array sizes.</p>\n<p>⚠️ <strong>Pitfall: Race Conditions in Concurrent Resize Operations</strong></p>\n<p>Multiple threads may simultaneously detect that a resize operation is needed, leading to conflicting resize attempts or inconsistent bucket array states. The problem is exacerbated when threads continue using old bucket masks after resize operations have begun.</p>\n<p>This manifests as entries being inserted into the wrong buckets, lookup operations failing for recently inserted keys, or crashes when accessing deallocated bucket arrays. The issue is particularly subtle because it may only occur under high concurrency and specific timing conditions.</p>\n<p><strong>Prevention Strategy</strong>: Use atomic compare-and-swap on a resize flag to ensure only one thread initiates resize operations. Implement proper memory barriers to ensure that bucket mask updates are visible to all threads before they begin using the new mask. Use hazard pointers to protect bucket array references during resize periods.</p>\n<p>⚠️ <strong>Pitfall: Contention Hotspots on Popular Buckets</strong></p>\n<p>Hash maps with skewed key distributions can create severe contention hotspots where multiple threads compete for access to the same bucket chains. This leads to excessive CAS failures, poor cache performance, and scalability degradation that defeats the purpose of lock-free design.</p>\n<p>The problem is particularly severe when hash functions produce clustered outputs or when application access patterns naturally focus on specific key ranges. Long bucket chains require linear traversal, creating extended critical sections that increase the probability of concurrent modifications and CAS failures.</p>\n<p><strong>Prevention Strategy</strong>: Implement adaptive load factor thresholds that trigger resize operations earlier for heavily contended buckets. Consider using more sophisticated hash functions that provide better distribution characteristics. Monitor bucket chain lengths and implement chain length limits that force bucket splitting regardless of global load factor.</p>\n<p>⚠️ <strong>Pitfall: Memory Leaks During Failed CAS Operations</strong></p>\n<p>Lock-free operations that allocate nodes speculatively may fail to insert them due to CAS failures, leading to memory leaks if the allocated nodes are not properly managed. This is particularly problematic in insert operations that create new nodes before confirming they can be successfully inserted.</p>\n<p>The issue compounds over time as failed operations accumulate leaked nodes, eventually leading to memory exhaustion. The problem is often masked in testing because it requires sustained high-concurrency workloads to become apparent.</p>\n<p><strong>Prevention Strategy</strong>: Implement proper cleanup logic for failed CAS operations, ensuring that allocated nodes are either successfully inserted or properly deallocated. Consider using memory pools or object recycling to reduce allocation overhead and simplify cleanup logic. Use tools like valgrind or AddressSanitizer to detect memory leaks during testing.</p>\n<p>⚠️ <strong>Pitfall: ABA Problems in Bucket Chain Updates</strong></p>\n<p>The classic ABA problem can occur in hash map bucket chains when nodes are removed and later reused with the same memory addresses. This can cause CAS operations to succeed incorrectly when they should fail, leading to corrupted list structures or lost entries.</p>\n<p>While less common than in simple stacks, the ABA problem can still affect hash map operations, particularly when combined with aggressive memory reclamation schemes that quickly reuse deallocated nodes. The problem is exacerbated in environments with memory pressure where allocators tend to reuse recently freed memory.</p>\n<p><strong>Prevention Strategy</strong>: Use hazard pointers for memory reclamation to prevent premature node reuse. Consider implementing tagged pointers that include version counters to detect ABA conditions. Ensure that memory reclamation schemes provide sufficient delay between deallocation and reuse to minimize ABA probability.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The lock-free hash map implementation requires careful attention to several critical components working together seamlessly. This section provides practical guidance for implementing a production-ready split-ordered hash map using Python, with complete infrastructure code and detailed implementation skeletons.</p>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Atomic Operations</td>\n<td><code>threading.Lock</code> with careful protocols</td>\n<td>Custom atomic wrapper with memory ordering</td>\n</tr>\n<tr>\n<td>Hash Function</td>\n<td><code>hash()</code> built-in with bit manipulation</td>\n<td>MurmurHash3 or SipHash implementation</td>\n</tr>\n<tr>\n<td>Memory Management</td>\n<td>Python garbage collector</td>\n<td>Manual hazard pointer integration</td>\n</tr>\n<tr>\n<td>Performance Monitoring</td>\n<td>Simple counters</td>\n<td>Detailed metrics with bucket utilization</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>lock_free_hashmap/\n    __init__.py                    ← Public API exports\n    atomic_ops.py                  ← Atomic operation wrappers (from previous milestones)\n    split_ordered_list.py          ← Core split-ordered list implementation\n    hashmap.py                     ← Main SplitOrderedHashMap class\n    hazard_pointers.py             ← Memory reclamation (from Milestone 4)\n    test_hashmap.py                ← Comprehensive test suite\n    benchmark_hashmap.py           ← Performance comparison benchmarks\n    examples/\n        basic_usage.py             ← Simple usage examples\n        concurrent_stress.py       ← High-concurrency stress testing</code></pre></div>\n\n<p><strong>Infrastructure Starter Code:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># reverse_bits.py - Complete utility for reverse bit ordering</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> reverse_bits</span><span style=\"color:#E1E4E8\">(value: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, bit_count: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Reverse the lower bit_count bits of value.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This is critical for split-ordered list correctness.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        value: Integer value to reverse bits for</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        bit_count: Number of bits to consider (log2 of bucket array size)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Value with reversed bit pattern in lower bit_count bits</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(bit_count):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> value </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#F97583\"> &#x3C;&#x3C;</span><span style=\"color:#E1E4E8\"> i):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result </span><span style=\"color:#F97583\">|=</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#F97583\"> &#x3C;&#x3C;</span><span style=\"color:#E1E4E8\"> (bit_count </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> -</span><span style=\"color:#E1E4E8\"> i))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> result</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> calculate_bucket_index</span><span style=\"color:#E1E4E8\">(hash_value: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, bucket_mask: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Calculate bucket index using current mask.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> hash_value </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\"> bucket_mask</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> calculate_parent_bucket</span><span style=\"color:#E1E4E8\">(bucket_index: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Calculate parent bucket for initialization hierarchy.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> bucket_index </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Find highest set bit and clear it to get parent</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    highest_bit </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> bucket_index.bit_length() </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> bucket_index </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#F97583\"> ~</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#F97583\"> &#x3C;&#x3C;</span><span style=\"color:#E1E4E8\"> highest_bit)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> BucketNode</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Node in the split-ordered list with atomic next pointer.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, key, value, hash_value: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, is_sentinel: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.key </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> key</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> value</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.hash </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> hash_value</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.is_sentinel </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> is_sentinel</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.next </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> AtomicReference(</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.deleted </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> AtomicReference(</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.RLock()  </span><span style=\"color:#6A737D\"># For fine-grained locking fallback</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> is_deleted</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if node is logically deleted.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.deleted.load(</span><span style=\"color:#79B8FF\">RELAXED</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> mark_deleted</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Attempt to mark node as logically deleted.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.deleted.compare_and_swap(</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> BucketInitializer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Handles recursive bucket initialization with cycle prevention.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, buckets, max_bucket_count: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.buckets </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> buckets</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_bucket_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_bucket_count</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.initializing </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">()  </span><span style=\"color:#6A737D\"># Track buckets being initialized</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.init_lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Lock()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> ensure_bucket_exists</span><span style=\"color:#E1E4E8\">(self, bucket_index: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> Optional[BucketNode]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Ensure bucket exists, creating parent buckets recursively if needed.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns the sentinel node for the bucket.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> bucket_index </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.max_bucket_count:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Fast path: bucket already exists</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sentinel </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.buckets[bucket_index].load(</span><span style=\"color:#79B8FF\">ACQUIRE</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> sentinel </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> sentinel</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Slow path: need to initialize bucket</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.init_lock:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Double-check after acquiring lock</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            sentinel </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.buckets[bucket_index].load(</span><span style=\"color:#79B8FF\">ACQUIRE</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> sentinel </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> sentinel</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Prevent initialization cycles</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> bucket_index </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.initializing:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                raise</span><span style=\"color:#79B8FF\"> RuntimeError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Cycle detected in bucket initialization: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">bucket_index</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.initializing.add(bucket_index)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._initialize_bucket(bucket_index)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            finally</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.initializing.discard(bucket_index)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _initialize_bucket</span><span style=\"color:#E1E4E8\">(self, bucket_index: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> BucketNode:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize a single bucket after ensuring parent exists.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> bucket_index </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Root bucket - create sentinel node</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            sentinel </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> BucketNode(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                key</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">bucket_index,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                value</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                hash_value</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">reverse_bits(bucket_index, </span><span style=\"color:#79B8FF\">32</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                is_sentinel</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.buckets[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].store(sentinel, </span><span style=\"color:#79B8FF\">RELEASE</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> sentinel</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Ensure parent bucket exists first</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        parent_index </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> calculate_parent_bucket(bucket_index)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        parent_sentinel </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.ensure_bucket_exists(parent_index)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Create sentinel node for this bucket</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sentinel </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> BucketNode(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            key</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">bucket_index,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            value</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            hash_value</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">reverse_bits(bucket_index, </span><span style=\"color:#79B8FF\">32</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            is_sentinel</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Insert sentinel into parent's chain at correct position</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._insert_sentinel_into_chain(parent_sentinel, sentinel)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Atomically publish bucket</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.buckets[bucket_index].store(sentinel, </span><span style=\"color:#79B8FF\">RELEASE</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> sentinel</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _insert_sentinel_into_chain</span><span style=\"color:#E1E4E8\">(self, parent_sentinel: BucketNode, new_sentinel: BucketNode):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Insert new sentinel into the split-ordered list at correct position.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        current </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parent_sentinel</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        target_hash </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> new_sentinel.hash</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            next_node </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> current.next.load(</span><span style=\"color:#79B8FF\">ACQUIRE</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> next_node </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#F97583\"> or</span><span style=\"color:#E1E4E8\"> next_node.hash </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> target_hash:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Found insertion point</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                new_sentinel.next.store(next_node, </span><span style=\"color:#79B8FF\">RELAXED</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> current.next.compare_and_swap(next_node, new_sentinel):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    break</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # CAS failed, retry</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                continue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            current </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> next_node</span></span></code></pre></div>\n\n<p><strong>Core Logic Skeleton Code:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># hashmap.py - Main implementation skeleton</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Optional, Tuple, Dict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> atomic_ops </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> AtomicReference, </span><span style=\"color:#79B8FF\">RELAXED</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">ACQUIRE</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">RELEASE</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> reverse_bits </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> reverse_bits, BucketNode, BucketInitializer</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SplitOrderedHashMap</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Lock-free hash map using split-ordered lists with incremental resizing.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Provides concurrent insert, lookup, and delete operations.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, initial_capacity: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 16</span><span style=\"color:#E1E4E8\">, load_factor_threshold: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.75</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.initial_capacity </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, initial_capacity)  </span><span style=\"color:#6A737D\"># Ensure power of 2</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.load_factor_threshold </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> load_factor_threshold</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Core data structures</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.buckets </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [AtomicReference(</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> _ </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.initial_capacity)]</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.size </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> AtomicReference(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.bucket_mask </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> AtomicReference(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.initial_capacity </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.resize_in_progress </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> AtomicReference(</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Helper components</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.bucket_initializer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> BucketInitializer(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.buckets, </span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.buckets))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Initialize bucket 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.bucket_initializer.ensure_bucket_exists(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> insert</span><span style=\"color:#E1E4E8\">(self, key: Any, value: Any) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Insert key-value pair into hash map.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns True if new entry created, False if existing key updated.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Calculate hash value for key using built-in hash function</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Apply reverse bit ordering to hash for split-ordered list placement</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Determine target bucket using current bucket_mask (handle concurrent resize)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Ensure target bucket exists using bucket_initializer.ensure_bucket_exists</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Traverse bucket chain starting from sentinel node to find insertion point</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Handle three cases: key exists (update), insert before current node, insert at end</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Create new BucketNode with key, value, and reverse bit-ordered hash</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Use compare_and_swap to atomically link new node into chain</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: On CAS failure due to concurrent modification, retry from traversal step</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: On successful insertion, increment size counter atomically</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 11: Check if load factor exceeds threshold and trigger resize if needed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 12: Return appropriate boolean indicating new entry vs. update</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> lookup</span><span style=\"color:#E1E4E8\">(self, key: Any) -> Optional[Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Lookup value for given key.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns value if found, None if key doesn't exist.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Calculate hash value and apply reverse bit ordering</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Determine target bucket using current bucket_mask</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Ensure target bucket exists using bucket_initializer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Load sentinel node pointer with acquire memory ordering</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Traverse split-ordered list comparing keys for exact match</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Skip sentinel nodes during traversal (check is_sentinel flag)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Skip logically deleted nodes (check node.is_deleted())</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: If matching key found, return associated value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: If traversal reaches next bucket's sentinel or end, return None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Handle concurrent modifications by restarting if inconsistent state detected</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> delete</span><span style=\"color:#E1E4E8\">(self, key: Any) -> Optional[Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Delete key from hash map.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns deleted value if found, None if key didn't exist.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Calculate hash value and apply reverse bit ordering  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Determine target bucket using current bucket_mask</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Ensure target bucket exists using bucket_initializer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Traverse split-ordered list to find node with matching key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: If key not found, return None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Attempt logical deletion by marking node.mark_deleted()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: If logical deletion succeeds, attempt physical deletion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Physical deletion: update predecessor's next pointer to skip deleted node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: If physical deletion fails, leave for subsequent operations to complete</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Decrement global size counter to reflect removal</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 11: Return deleted value on success</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 12: Implement helping: assist with incomplete physical deletions during traversal</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _trigger_resize_if_needed</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Check load factor and trigger resize operation if threshold exceeded.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Uses compare_and_swap to ensure only one thread initiates resize.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Read current size and bucket count atomically</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate current load factor (size / bucket_count)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If load factor below threshold, return early</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Attempt to acquire resize responsibility using CAS on resize_in_progress flag</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: If CAS fails, another thread is handling resize, return</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Allocate new bucket array with double the current capacity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Initialize new array's bucket 0 with appropriate sentinel node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Atomically update bucket_mask to reflect new array size</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Copy buckets reference to new array (incremental migration will handle entries)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Clear resize_in_progress flag when initialization complete</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _split_bucket_if_needed</span><span style=\"color:#E1E4E8\">(self, bucket_index: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Split bucket during resize operation if it hasn't been split yet.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Redistributes entries between old and new bucket based on hash bits.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if resize is in progress, return if not</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate new bucket index (old_index + current_bucket_count)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check if new bucket already exists, return if so</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Create sentinel node for new bucket with correct reverse bit-ordered hash</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Traverse entries in old bucket chain starting from sentinel</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: For each data node, determine target bucket based on hash bit pattern</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Use atomic list manipulation to move entries to correct bucket chains</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Handle concurrent modifications by retrying operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Update buckets array to point to new bucket's sentinel node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Ensure all moved entries maintain correct ordering within their new chains</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _traverse_bucket_chain</span><span style=\"color:#E1E4E8\">(self, start_node: BucketNode, target_hash: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> Tuple[BucketNode, BucketNode]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Traverse bucket chain to find insertion point or matching key.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns (predecessor, current) node pair for the target position.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Handles concurrent modifications and assists with incomplete deletions.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Start traversal from given start_node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Follow next pointers while current.hash &#x3C; target_hash</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Skip sentinel nodes when looking for data entries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Help complete physical deletion for logically deleted nodes encountered</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Handle race conditions where nodes are deleted during traversal</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Use acquire memory ordering when loading next pointers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return predecessor and current node for target hash position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Ensure returned nodes are stable (not concurrently deleted)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> size_hint</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return approximate number of entries in hash map.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.size.load(</span><span style=\"color:#79B8FF\">RELAXED</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> is_empty</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if hash map contains any entries.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.size_hint() </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint:</strong></p>\n<p>After implementing the split-ordered hash map, verify correctness with these checks:</p>\n<ol>\n<li><strong>Single-threaded correctness</strong>: Insert 1000 key-value pairs, verify all can be looked up, delete half, verify lookups return correct results</li>\n<li><strong>Concurrent insertion</strong>: Use 10 threads inserting 100 unique keys each, verify final size is 1000 and all keys are present</li>\n<li><strong>Concurrent mixed operations</strong>: Use multiple threads performing random insert/lookup/delete operations, verify consistency</li>\n<li><strong>Resize behavior</strong>: Monitor bucket array size during heavy insertion, verify it doubles when load factor threshold exceeded</li>\n<li><strong>Memory safety</strong>: Run with memory debugging tools to ensure no leaks or use-after-free errors</li>\n</ol>\n<p>Expected behavior indicators:</p>\n<ul>\n<li>Insert operations should have O(1) average time complexity</li>\n<li>Concurrent operations should not block each other</li>\n<li>Resize operations should happen transparently without affecting correctness</li>\n<li>Performance should scale well with increased thread count (up to core count)</li>\n</ul>\n<p><strong>Debugging Tips:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis Method</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Keys inserted but not found</td>\n<td>Incorrect reverse bit calculation</td>\n<td>Print hash values and bucket indices</td>\n<td>Verify reverse_bits function with unit tests</td>\n</tr>\n<tr>\n<td>Crashes during resize</td>\n<td>Race condition in bucket array access</td>\n<td>Add logging to resize operations</td>\n<td>Implement proper memory barriers and hazard pointers</td>\n</tr>\n<tr>\n<td>Poor performance under contention</td>\n<td>Excessive CAS failures in hot buckets</td>\n<td>Profile bucket chain lengths</td>\n<td>Implement adaptive load factor or better hash function</td>\n</tr>\n<tr>\n<td>Memory leaks</td>\n<td>Failed CAS operations not cleaning up nodes</td>\n<td>Run with valgrind or similar tool</td>\n<td>Add proper cleanup in all CAS failure paths</td>\n</tr>\n<tr>\n<td>Inconsistent size counter</td>\n<td>Race conditions in increment/decrement</td>\n<td>Compare actual entries with size counter</td>\n<td>Use atomic operations for all size updates</td>\n</tr>\n</tbody></table>\n<h2 id=\"interactions-and-data-flow\">Interactions and Data Flow</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (this section describes how components from atomic operations through complete data structures communicate and coordinate with each other)</p>\n</blockquote>\n<p>Understanding how lock-free components interact requires thinking beyond traditional method calls and return values. Lock-free programming introduces unique communication patterns where threads coordinate through shared memory locations, helping protocols, and deferred cleanup mechanisms. This section explores the intricate dance of atomic operations, data structure manipulations, and memory reclamation that enables high-performance concurrent computing without traditional locks.</p>\n<h3 id=\"mental-model-the-relay-race-coordination\">Mental Model: The Relay Race Coordination</h3>\n<p>Think of lock-free interactions like a relay race where runners hand off batons while sprinting at full speed. Unlike a traditional relay where runners stop to carefully exchange the baton (analogous to acquiring a lock), lock-free coordination requires runners to coordinate the handoff while maintaining their pace. The baton represents shared data, and the handoff represents atomic operations that must succeed even when multiple runners approach the exchange zone simultaneously.</p>\n<p>In this analogy, compare-and-swap operations are like attempting to grab or place the baton only if it&#39;s in the expected position. If another runner has already moved it, the attempt fails and must be retried. Hazard pointers are like safety flags that runners wave to signal &quot;I&#39;m still using this lane&quot; before the track maintenance crew can clean up behind them. The helping mechanism is like runners assisting teammates who stumble during their handoff attempt.</p>\n<p>This mental model captures the essential challenge: coordination must happen through observation and atomic updates rather than explicit communication, and every participant must be prepared for their operations to fail due to concurrent activity.</p>\n<h3 id=\"operation-sequence-patterns\">Operation Sequence Patterns</h3>\n<p>Lock-free algorithms rely on several fundamental interaction patterns that appear repeatedly across all data structures. Understanding these patterns is crucial because they represent the core coordination mechanisms that replace traditional locking.</p>\n<h4 id=\"cas-retry-loop-pattern\">CAS Retry Loop Pattern</h4>\n<p>The compare-and-swap retry loop forms the foundation of all lock-free operations. This pattern handles the fundamental challenge that any shared memory location might be modified by concurrent threads between the time it&#39;s observed and the time an update is attempted.</p>\n<table>\n<thead>\n<tr>\n<th>Pattern Step</th>\n<th>Action</th>\n<th>Purpose</th>\n<th>Failure Response</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Load Current Value</td>\n<td><code>current = atomic_ref.load(ACQUIRE)</code></td>\n<td>Observe current state</td>\n<td>N/A (load cannot fail)</td>\n</tr>\n<tr>\n<td>Compute New Value</td>\n<td><code>new_value = transform_function(current)</code></td>\n<td>Calculate desired update</td>\n<td>N/A (pure computation)</td>\n</tr>\n<tr>\n<td>Attempt Atomic Update</td>\n<td><code>success = atomic_ref.compare_and_swap(current, new_value)</code></td>\n<td>Apply change if state unchanged</td>\n<td>Retry from load step</td>\n</tr>\n<tr>\n<td>Handle Success</td>\n<td>Process operation completion</td>\n<td>Finalize and return result</td>\n<td>N/A</td>\n</tr>\n<tr>\n<td>Handle Failure</td>\n<td>Backoff and retry or abort</td>\n<td>Manage contention</td>\n<td>Exponential backoff delay</td>\n</tr>\n</tbody></table>\n<p>The <code>cas_retry_loop</code> function encapsulates this pattern and provides a reusable foundation for all lock-free operations. The function accepts an atomic reference, an update function that computes the new value from the current value, and a maximum retry count to prevent infinite spinning under extreme contention.</p>\n<blockquote>\n<p><strong>Decision: Exponential Backoff in CAS Retry Loops</strong></p>\n<ul>\n<li><strong>Context</strong>: High contention can cause CAS operations to fail repeatedly, leading to excessive CPU usage from spinning threads</li>\n<li><strong>Options Considered</strong>: No backoff (immediate retry), fixed delay, exponential backoff with random jitter</li>\n<li><strong>Decision</strong>: Exponential backoff with random jitter starting at 1 microsecond</li>\n<li><strong>Rationale</strong>: Reduces contention by spreading retry attempts across time while maintaining low latency for success cases</li>\n<li><strong>Consequences</strong>: Improves throughput under contention but adds complexity to retry logic and worst-case latency</li>\n</ul>\n</blockquote>\n<h4 id=\"helping-protocol-pattern\">Helping Protocol Pattern</h4>\n<p>Many lock-free algorithms employ helping mechanisms where threads assist each other in completing operations that may have been interrupted by context switches or delays. This pattern is essential for maintaining progress guarantees when individual threads may be delayed arbitrarily.</p>\n<table>\n<thead>\n<tr>\n<th>Helper Role</th>\n<th>Observation</th>\n<th>Helping Action</th>\n<th>Completion Condition</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Observer</td>\n<td>Detects incomplete operation</td>\n<td>Attempts to complete on behalf of original thread</td>\n<td>Operation reaches consistent state</td>\n</tr>\n<tr>\n<td>Original Thread</td>\n<td>Returns from delay</td>\n<td>Verifies operation completion</td>\n<td>Either self-completed or helper completed</td>\n</tr>\n<tr>\n<td>Conflict Resolution</td>\n<td>Multiple helpers</td>\n<td>First successful CAS wins</td>\n<td>Losing helpers abort gracefully</td>\n</tr>\n</tbody></table>\n<p>The Michael-Scott queue demonstrates helping protocols clearly in the <code>enqueue</code> operation. When a thread observes that the tail pointer lags behind the actual end of the queue, it attempts to advance the tail pointer even though it didn&#39;t create the situation. This helping behavior ensures that subsequent enqueue operations can proceed even if the thread that originally extended the queue was delayed before updating the tail pointer.</p>\n<h4 id=\"hazard-pointer-integration-pattern\">Hazard Pointer Integration Pattern</h4>\n<p>Lock-free data structures must integrate carefully with memory reclamation systems to prevent use-after-free errors while maintaining non-blocking progress guarantees. The hazard pointer integration pattern standardizes this interaction.</p>\n<table>\n<thead>\n<tr>\n<th>Protection Phase</th>\n<th>Thread Action</th>\n<th>Global Effect</th>\n<th>Memory State</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Pre-Access</td>\n<td><code>protect(pointer)</code></td>\n<td>Announces intent to access</td>\n<td>Node marked as protected</td>\n</tr>\n<tr>\n<td>Verification</td>\n<td><code>verify(atomic_ref)</code></td>\n<td>Confirms pointer still valid</td>\n<td>Prevents ABA on protected node</td>\n</tr>\n<tr>\n<td>Access</td>\n<td>Use protected node safely</td>\n<td>Guaranteed no reclamation</td>\n<td>Node remains allocated</td>\n</tr>\n<tr>\n<td>Release</td>\n<td><code>release(slot_index)</code></td>\n<td>Clears protection</td>\n<td>Node becomes eligible for reclamation</td>\n</tr>\n</tbody></table>\n<p>The protect-then-verify pattern is critical for preventing race conditions where a node is reclaimed between the time its pointer is loaded and the time it&#39;s protected. The verification step ensures that the protected pointer still refers to a valid node that remains reachable through the data structure.</p>\n<h3 id=\"component-interface-contracts\">Component Interface Contracts</h3>\n<p>The lock-free system consists of three primary layers with well-defined interface contracts that specify the exact behavior and guarantees provided by each component.</p>\n<h4 id=\"atomic-operations-layer-interface\">Atomic Operations Layer Interface</h4>\n<p>The atomic operations layer provides the fundamental building blocks that all higher-level components depend on. These operations must provide specific guarantees about atomicity, memory ordering, and progress.</p>\n<table>\n<thead>\n<tr>\n<th>Method Signature</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Memory Ordering</th>\n<th>Progress Guarantee</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>compare_and_swap(expected, new_value)</code></td>\n<td><code>expected: Any, new_value: Any</code></td>\n<td><code>(success: bool, observed: Any)</code></td>\n<td>Sequentially consistent</td>\n<td>Lock-free (bounded retries)</td>\n</tr>\n<tr>\n<td><code>load(ordering)</code></td>\n<td><code>ordering: MemoryOrdering</code></td>\n<td><code>current_value: Any</code></td>\n<td>As specified by ordering</td>\n<td>Wait-free (immediate)</td>\n</tr>\n<tr>\n<td><code>store(value, ordering)</code></td>\n<td><code>value: Any, ordering: MemoryOrdering</code></td>\n<td><code>None</code></td>\n<td>As specified by ordering</td>\n<td>Wait-free (immediate)</td>\n</tr>\n<tr>\n<td><code>fetch_and_add(increment)</code></td>\n<td><code>increment: int</code></td>\n<td><code>previous_value: int</code></td>\n<td>Sequentially consistent</td>\n<td>Lock-free (hardware atomic)</td>\n</tr>\n</tbody></table>\n<p>The <code>compare_and_swap</code> operation returns both a success indicator and the observed value to enable efficient retry loops. When the operation fails, the observed value provides the current state needed for the next retry attempt without requiring an additional load operation.</p>\n<blockquote>\n<p><strong>Decision: Return Observed Value on CAS Failure</strong></p>\n<ul>\n<li><strong>Context</strong>: Failed CAS operations require knowing the current value to compute the next retry attempt</li>\n<li><strong>Options Considered</strong>: Return boolean only (requires additional load), return tuple with observed value, exception-based failure indication</li>\n<li><strong>Decision</strong>: Return tuple <code>(success: bool, observed: Any)</code> for all CAS operations</li>\n<li><strong>Rationale</strong>: Eliminates extra load operations in retry loops, reducing memory traffic and improving performance under contention</li>\n<li><strong>Consequences</strong>: Slightly more complex return value handling but significantly better performance characteristics</li>\n</ul>\n</blockquote>\n<p>Memory ordering parameters control the synchronization guarantees provided by each operation. The atomic layer must respect these constraints to ensure correct behavior on weakly-ordered architectures.</p>\n<table>\n<thead>\n<tr>\n<th>Memory Ordering</th>\n<th>Load Semantics</th>\n<th>Store Semantics</th>\n<th>Synchronization Effect</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>RELAXED</code></td>\n<td>No ordering constraints</td>\n<td>No ordering constraints</td>\n<td>Atomic but no synchronization</td>\n</tr>\n<tr>\n<td><code>ACQUIRE</code></td>\n<td>Prevents reordering of subsequent reads</td>\n<td>N/A</td>\n<td>Synchronizes-with release stores</td>\n</tr>\n<tr>\n<td><code>RELEASE</code></td>\n<td>N/A</td>\n<td>Prevents reordering of prior writes</td>\n<td>Provides synchronizes-with for acquire loads</td>\n</tr>\n<tr>\n<td><code>SEQ_CST</code></td>\n<td>Global ordering with other seq_cst</td>\n<td>Global ordering with other seq_cst</td>\n<td>Strongest guarantee, full ordering</td>\n</tr>\n</tbody></table>\n<h4 id=\"data-structure-layer-interface\">Data Structure Layer Interface</h4>\n<p>The data structure layer builds on atomic operations to provide familiar collection interfaces with lock-free guarantees. Each data structure must specify its linearizability points and progress guarantees.</p>\n<table>\n<thead>\n<tr>\n<th>Data Structure</th>\n<th>Core Operations</th>\n<th>Linearizability Point</th>\n<th>Empty Container Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>TreiberStack</code></td>\n<td><code>push(data)</code>, <code>pop()</code></td>\n<td>CAS on top pointer</td>\n<td><code>pop()</code> returns None</td>\n</tr>\n<tr>\n<td><code>MichaelScottQueue</code></td>\n<td><code>enqueue(data)</code>, <code>dequeue()</code></td>\n<td>CAS on next pointer (enqueue), CAS on head (dequeue)</td>\n<td><code>dequeue()</code> returns None</td>\n</tr>\n<tr>\n<td><code>SplitOrderedHashMap</code></td>\n<td><code>insert(key, value)</code>, <code>lookup(key)</code>, <code>delete(key)</code></td>\n<td>CAS on bucket list insertion</td>\n<td>Operations return None/False</td>\n</tr>\n</tbody></table>\n<p>Stack operations provide the simplest interface contract. The <code>push</code> operation always succeeds (assuming memory allocation succeeds), while <code>pop</code> returns <code>None</code> when called on an empty stack. Both operations are linearizable at the point where the CAS on the top pointer succeeds.</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Preconditions</th>\n<th>Postconditions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>push(data)</code></td>\n<td><code>data: Any</code></td>\n<td><code>None</code></td>\n<td><code>data</code> is not None</td>\n<td>Stack contains new element at top</td>\n</tr>\n<tr>\n<td><code>pop()</code></td>\n<td>None</td>\n<td><code>Optional[Any]</code></td>\n<td>None</td>\n<td>If stack non-empty, removes and returns top element</td>\n</tr>\n<tr>\n<td><code>is_empty()</code></td>\n<td>None</td>\n<td><code>bool</code></td>\n<td>None</td>\n<td>Returns true iff stack contains no elements</td>\n</tr>\n<tr>\n<td><code>size_hint()</code></td>\n<td>None</td>\n<td><code>int</code></td>\n<td>None</td>\n<td>Returns approximate count (may be stale)</td>\n</tr>\n</tbody></table>\n<p>Queue operations maintain FIFO ordering with separate methods for adding and removing elements. The linearizability guarantee ensures that elements are dequeued in the exact order they were enqueued, even under concurrent access.</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Preconditions</th>\n<th>Postconditions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>enqueue(data)</code></td>\n<td><code>data: Any</code></td>\n<td><code>None</code></td>\n<td><code>data</code> is not None</td>\n<td>Queue contains new element at tail</td>\n</tr>\n<tr>\n<td><code>dequeue()</code></td>\n<td>None</td>\n<td><code>Optional[Any]</code></td>\n<td>None</td>\n<td>If queue non-empty, removes and returns head element</td>\n</tr>\n<tr>\n<td><code>is_empty()</code></td>\n<td>None</td>\n<td><code>bool</code></td>\n<td>None</td>\n<td>Returns true iff queue contains no elements</td>\n</tr>\n<tr>\n<td><code>size_hint()</code></td>\n<td>None</td>\n<td><code>int</code></td>\n<td>None</td>\n<td>Returns approximate count (may be stale)</td>\n</tr>\n</tbody></table>\n<p>Hash map operations provide key-value storage with lock-free guarantees. The interface supports standard dictionary operations while maintaining linearizability for all concurrent operations.</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Preconditions</th>\n<th>Postconditions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>insert(key, value)</code></td>\n<td><code>key: Any, value: Any</code></td>\n<td><code>bool</code></td>\n<td><code>key</code> and <code>value</code> are not None</td>\n<td>Returns true if new entry created</td>\n</tr>\n<tr>\n<td><code>lookup(key)</code></td>\n<td><code>key: Any</code></td>\n<td><code>Optional[Any]</code></td>\n<td><code>key</code> is not None</td>\n<td>Returns current value for key or None</td>\n</tr>\n<tr>\n<td><code>delete(key)</code></td>\n<td><code>key: Any</code></td>\n<td><code>Optional[Any]</code></td>\n<td><code>key</code> is not None</td>\n<td>Returns deleted value or None if not found</td>\n</tr>\n<tr>\n<td><code>size_hint()</code></td>\n<td>None</td>\n<td><code>int</code></td>\n<td>None</td>\n<td>Returns approximate count of key-value pairs</td>\n</tr>\n</tbody></table>\n<h4 id=\"memory-reclamation-layer-interface\">Memory Reclamation Layer Interface</h4>\n<p>The hazard pointer system provides memory reclamation services that integrate seamlessly with lock-free data structures. The interface must be simple enough to use correctly while providing strong safety guarantees.</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Thread Safety</th>\n<th>Performance Guarantee</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>protect(pointer)</code></td>\n<td><code>pointer: Any</code></td>\n<td><code>slot_index: int</code></td>\n<td>Thread-local operation</td>\n<td>Wait-free</td>\n</tr>\n<tr>\n<td><code>release(slot_index)</code></td>\n<td><code>slot_index: int</code></td>\n<td><code>None</code></td>\n<td>Thread-local operation</td>\n<td>Wait-free</td>\n</tr>\n<tr>\n<td><code>retire(node)</code></td>\n<td><code>node: Node</code></td>\n<td><code>None</code></td>\n<td>Thread-safe</td>\n<td>Lock-free (may trigger scan)</td>\n</tr>\n<tr>\n<td><code>scan_and_reclaim()</code></td>\n<td>None</td>\n<td><code>reclaimed_count: int</code></td>\n<td>Thread-safe</td>\n<td>Lock-free</td>\n</tr>\n<tr>\n<td><code>protect_and_verify(atomic_ref)</code></td>\n<td><code>atomic_ref: AtomicReference</code></td>\n<td><code>(pointer: Any, slot_index: int)</code></td>\n<td>Thread-safe</td>\n<td>Lock-free</td>\n</tr>\n</tbody></table>\n<p>The <code>protect_and_verify</code> method encapsulates the common protect-then-verify pattern used throughout lock-free data structures. It atomically loads a pointer, protects it with a hazard pointer, and verifies that the pointer remains valid by re-checking the atomic reference.</p>\n<blockquote>\n<p><strong>Decision: Separate Protect and Verify vs Combined Operation</strong></p>\n<ul>\n<li><strong>Context</strong>: Lock-free algorithms require protecting loaded pointers and verifying they remain valid to prevent ABA problems</li>\n<li><strong>Options Considered</strong>: Separate <code>protect()</code> and <code>verify()</code> calls, combined <code>protect_and_verify()</code> operation, automatic protection on load</li>\n<li><strong>Decision</strong>: Provide both separate operations and combined <code>protect_and_verify()</code> for common use cases</li>\n<li><strong>Rationale</strong>: Combined operation reduces boilerplate and race condition opportunities while separate operations allow flexibility for complex scenarios</li>\n<li><strong>Consequences</strong>: Simpler API for common cases but requires understanding when to use which variant</li>\n</ul>\n</blockquote>\n<h3 id=\"memory-lifecycle-and-flow\">Memory Lifecycle and Flow</h3>\n<p>Memory management in lock-free systems follows a complex lifecycle that balances performance with safety. Understanding this flow is essential for correctly implementing and debugging lock-free algorithms.</p>\n<h4 id=\"node-allocation-and-initial-state\">Node Allocation and Initial State</h4>\n<p>New nodes begin their lifecycle in a straightforward allocated state before entering the complex world of concurrent access and deferred reclamation.</p>\n<table>\n<thead>\n<tr>\n<th>Allocation Stage</th>\n<th>Memory State</th>\n<th>Visibility</th>\n<th>Protection Status</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>malloc()</code> or equivalent</td>\n<td>Allocated but uninitialized</td>\n<td>Thread-private</td>\n<td>No hazard protection needed</td>\n</tr>\n<tr>\n<td>Constructor initialization</td>\n<td>Initialized fields, atomic references set</td>\n<td>Thread-private</td>\n<td>No hazard protection needed</td>\n</tr>\n<tr>\n<td>Publication to data structure</td>\n<td>Visible to other threads via CAS</td>\n<td>Globally visible</td>\n<td>Becomes eligible for hazard protection</td>\n</tr>\n</tbody></table>\n<p>During allocation and initialization, nodes are private to the creating thread and require no special coordination. The critical transition occurs when a node becomes visible to other threads through a successful CAS operation that links it into a shared data structure.</p>\n<p>The publication step represents the linearization point where the node becomes part of the shared state. From this moment forward, any thread accessing the data structure might encounter the node and must use proper hazard pointer protection.</p>\n<h4 id=\"concurrent-access-and-protection-phase\">Concurrent Access and Protection Phase</h4>\n<p>Once published, nodes enter a phase where they may be accessed concurrently by multiple threads. The hazard pointer system coordinates this access to prevent use-after-free errors while maintaining lock-free progress guarantees.</p>\n<table>\n<thead>\n<tr>\n<th>Access Pattern</th>\n<th>Protection Requirement</th>\n<th>Verification Need</th>\n<th>Duration</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Pointer load from atomic reference</td>\n<td>Immediate protection after load</td>\n<td>Verify pointer still reachable</td>\n<td>Until access complete</td>\n</tr>\n<tr>\n<td>Traversal following next pointers</td>\n<td>Protect each node before dereferencing</td>\n<td>Verify next pointer before following</td>\n<td>Per-node basis</td>\n</tr>\n<tr>\n<td>Data field access</td>\n<td>Node must be protected</td>\n<td>Protection implies validity</td>\n<td>Until operation complete</td>\n</tr>\n<tr>\n<td>Pointer comparison (ABA detection)</td>\n<td>Target nodes should be protected</td>\n<td>Compare with freshly loaded value</td>\n<td>During comparison only</td>\n</tr>\n</tbody></table>\n<p>The protect-then-verify pattern forms the core of safe concurrent access. A thread first loads a pointer from an atomic reference, immediately protects it with a hazard pointer, then verifies the pointer is still reachable by re-reading the atomic reference and comparing values.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Access Sequence:\n1. pointer = atomic_ref.load(ACQUIRE)\n2. slot = protect(pointer)  \n3. verification = atomic_ref.load(ACQUIRE)\n4. if pointer != verification: release(slot); retry\n5. // Safe to access pointer-&gt;data and pointer-&gt;next\n6. release(slot) when access complete</code></pre></div>\n\n<p>This sequence handles the race condition where another thread removes and retires a node between steps 1 and 2. The verification in step 4 detects this situation and triggers a retry with the updated pointer value.</p>\n<h4 id=\"logical-deletion-and-marking\">Logical Deletion and Marking</h4>\n<p>Many lock-free algorithms separate logical deletion (marking a node as deleted) from physical deletion (unlinking the node from the data structure). This separation simplifies concurrent deletion by providing a stable intermediate state.</p>\n<table>\n<thead>\n<tr>\n<th>Deletion Phase</th>\n<th>Node State</th>\n<th>Visibility</th>\n<th>Removal Responsibility</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Active node</td>\n<td>Unmarked, reachable</td>\n<td>Normal operations access</td>\n<td>N/A</td>\n</tr>\n<tr>\n<td>Logical deletion</td>\n<td>Marked as deleted</td>\n<td>Skipped by operations</td>\n<td>Original deleter or helpers</td>\n</tr>\n<tr>\n<td>Physical removal</td>\n<td>Unlinked from structure</td>\n<td>Only via hazard pointers</td>\n<td>Any thread</td>\n</tr>\n<tr>\n<td>Retirement</td>\n<td>Added to retirement list</td>\n<td>No new accesses</td>\n<td>Memory reclamation system</td>\n</tr>\n</tbody></table>\n<p>The <code>mark_deleted()</code> operation typically uses a CAS to atomically set a deletion flag or tag in the node. Once marked, other operations skip over the node while traversing the data structure, but the node remains physically present until a subsequent operation removes it.</p>\n<p>Physical removal requires careful coordination to avoid breaking concurrent traversals. The removing thread must use CAS operations to atomically unlink the node while ensuring that concurrent traversals can still make progress.</p>\n<h4 id=\"retirement-and-deferred-reclamation\">Retirement and Deferred Reclamation</h4>\n<p>When a node is removed from a data structure, it cannot be immediately freed because other threads might still hold protected references to it. The retirement system manages this deferred cleanup.</p>\n<table>\n<thead>\n<tr>\n<th>Retirement Stage</th>\n<th>Memory Status</th>\n<th>Reclamation Eligibility</th>\n<th>Cleanup Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Active retirement</td>\n<td>Added to thread-local retirement list</td>\n<td>Not yet eligible</td>\n<td>None</td>\n</tr>\n<tr>\n<td>Batched retirement</td>\n<td>Moved to global retirement queue</td>\n<td>Awaiting hazard scan</td>\n<td>None</td>\n</tr>\n<tr>\n<td>Hazard scanning</td>\n<td>All thread hazard pointers checked</td>\n<td>Depends on scan results</td>\n<td>Mark safe/unsafe for reclamation</td>\n</tr>\n<tr>\n<td>Safe reclamation</td>\n<td>No hazard pointers reference node</td>\n<td>Eligible for free</td>\n<td>Call destructor and free memory</td>\n</tr>\n<tr>\n<td>Deferred reclamation</td>\n<td>Hazard pointer found</td>\n<td>Moved to next scan batch</td>\n<td>None</td>\n</tr>\n</tbody></table>\n<p>The <code>retire(node)</code> operation adds a node to the current thread&#39;s retirement list. When this list reaches a threshold size, the thread triggers a scan of all hazard pointers across all threads to determine which retired nodes are safe to reclaim.</p>\n<blockquote>\n<p><strong>Decision: Thread-Local vs Global Retirement Lists</strong></p>\n<ul>\n<li><strong>Context</strong>: Retired nodes must be collected efficiently while minimizing synchronization overhead during retirement</li>\n<li><strong>Options Considered</strong>: Thread-local lists with periodic batching, single global list with locking, lock-free global queue</li>\n<li><strong>Decision</strong>: Thread-local retirement lists with threshold-triggered global scanning</li>\n<li><strong>Rationale</strong>: Minimizes synchronization during the common retirement operation while enabling efficient batch processing</li>\n<li><strong>Consequences</strong>: Requires thread-local storage management and cleanup on thread exit but provides excellent performance</li>\n</ul>\n</blockquote>\n<h4 id=\"scan-and-reclamation-process\">Scan and Reclamation Process</h4>\n<p>The hazard pointer scanning process represents the coordination point where the memory reclamation system determines which nodes are safe to free. This process must be efficient enough to not become a bottleneck while being thorough enough to prevent memory leaks.</p>\n<table>\n<thead>\n<tr>\n<th>Scan Phase</th>\n<th>Action</th>\n<th>Coordination Required</th>\n<th>Result</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Collect retirement lists</td>\n<td>Gather nodes from all threads</td>\n<td>Thread-safe list access</td>\n<td>Unified retirement set</td>\n</tr>\n<tr>\n<td>Scan hazard pointers</td>\n<td>Check all active hazard slots</td>\n<td>Read from thread-local storage</td>\n<td>Set of protected pointers</td>\n</tr>\n<tr>\n<td>Compute reclaimable set</td>\n<td>Set difference: retired - protected</td>\n<td>Pure computation</td>\n<td>Nodes safe to free</td>\n</tr>\n<tr>\n<td>Reclaim safe nodes</td>\n<td>Free memory for reclaimable nodes</td>\n<td>None</td>\n<td>Memory returned to system</td>\n</tr>\n<tr>\n<td>Reschedule unsafe nodes</td>\n<td>Move protected nodes to next batch</td>\n<td>Update retirement lists</td>\n<td>Deferred for future scan</td>\n</tr>\n</tbody></table>\n<p>The scanning algorithm must handle the race condition where hazard pointers are updated while the scan is in progress. This is typically handled by using acquire-release memory ordering when reading hazard pointer slots and by allowing false positives (keeping nodes that could have been freed) rather than false negatives (freeing nodes that are still protected).</p>\n<h4 id=\"thread-exit-cleanup\">Thread Exit Cleanup</h4>\n<p>When threads terminate, they must clean up their hazard pointer slots and process their remaining retirement lists to prevent memory leaks and dangling protection records.</p>\n<table>\n<thead>\n<tr>\n<th>Cleanup Step</th>\n<th>Action</th>\n<th>Safety Requirement</th>\n<th>Failure Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Mark thread inactive</td>\n<td>Set thread record as inactive</td>\n<td>Atomic flag update</td>\n<td>Best effort, may leak on crash</td>\n</tr>\n<tr>\n<td>Clear hazard slots</td>\n<td>Set all hazard pointers to null</td>\n<td>Release memory ordering</td>\n<td>Must complete to prevent false protection</td>\n</tr>\n<tr>\n<td>Process retirement list</td>\n<td>Reclaim all remaining retired nodes</td>\n<td>Safe since thread is exiting</td>\n<td>Force reclamation regardless of hazard pointers</td>\n</tr>\n<tr>\n<td>Unregister thread record</td>\n<td>Remove from global thread registry</td>\n<td>Synchronized update</td>\n<td>May leave inactive record on crash</td>\n</tr>\n</tbody></table>\n<p>Thread exit cleanup is critical for preventing resource leaks in long-running systems where threads are created and destroyed frequently. The cleanup process must be robust against thread termination at arbitrary points while avoiding deadlocks or blocking other threads.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The interaction patterns described above translate into specific implementation approaches that balance correctness with performance. This section provides concrete guidance for implementing these patterns in Python, along with infrastructure components and debugging tools.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<p>Python&#39;s Global Interpreter Lock (GIL) complicates lock-free programming significantly, but several approaches can provide educational value and real concurrency benefits in specific scenarios.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Atomic Operations</td>\n<td><code>threading</code> module locks with CAS simulation</td>\n<td><code>ctypes</code> to expose hardware CAS</td>\n<td>Start with simulation for learning, advance to real atomics</td>\n</tr>\n<tr>\n<td>Memory Ordering</td>\n<td>Memory barriers via <code>threading.Barrier</code></td>\n<td>Platform-specific memory fences</td>\n<td>Educational barriers first, then performance optimization</td>\n</tr>\n<tr>\n<td>Thread Management</td>\n<td>Standard <code>threading.Thread</code></td>\n<td><code>concurrent.futures.ThreadPoolExecutor</code></td>\n<td>Simplicity vs advanced thread pool features</td>\n</tr>\n<tr>\n<td>Memory Management</td>\n<td>Manual reference counting</td>\n<td><code>weakref</code> module for automatic cleanup</td>\n<td>Explicit control vs automatic garbage collection</td>\n</tr>\n<tr>\n<td>Testing Framework</td>\n<td>Built-in <code>unittest</code> with custom concurrency helpers</td>\n<td><code>hypothesis</code> for property-based testing</td>\n<td>Familiar framework vs advanced randomized testing</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<p>Organizing lock-free code requires careful attention to dependencies and testability. The following structure supports incremental development while maintaining clean separation between layers.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>lock_free_structures/\n├── atomic/\n│   ├── __init__.py           # Atomic operations interface\n│   ├── memory_ordering.py    # Memory ordering constants and semantics\n│   ├── compare_and_swap.py   # CAS operation implementation\n│   └── atomic_reference.py   # AtomicReference wrapper class\n├── data_structures/\n│   ├── __init__.py          # Data structure exports\n│   ├── treiber_stack.py     # Lock-free stack implementation\n│   ├── michael_scott_queue.py # Lock-free queue implementation\n│   └── split_ordered_hashmap.py # Lock-free hash map implementation\n├── memory_reclamation/\n│   ├── __init__.py          # Memory reclamation interface\n│   ├── hazard_pointers.py   # Hazard pointer implementation\n│   ├── retirement_list.py   # Node retirement management\n│   └── thread_registry.py   # Thread-local storage management\n├── testing/\n│   ├── __init__.py          # Testing utilities\n│   ├── linearizability.py  # Correctness verification tools\n│   ├── stress_testing.py    # Concurrency stress tests\n│   └── property_testing.py  # Property-based test generators\n└── examples/\n    ├── basic_usage.py       # Simple usage examples\n    ├── performance_comparison.py # Lock-free vs locked benchmarks\n    └── debugging_examples.py # Common debugging scenarios</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p>The following components provide complete, working infrastructure that learners can use immediately while focusing on the core lock-free algorithms.</p>\n<p><strong>Memory Ordering Constants (<code>memory_ordering.py</code>)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Memory ordering semantics for atomic operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MemoryOrdering</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Memory ordering constraints for atomic operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RELAXED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"relaxed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ACQUIRE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"acquire\"</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RELEASE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"release\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SEQ_CST</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"seq_cst\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MemoryBarrier</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Memory barrier implementation for educational purposes.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._barrier </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Barrier(</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> acquire_fence</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Acquire memory fence - prevents reordering of subsequent reads.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # In real implementation, this would be a CPU fence instruction</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> release_fence</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Release memory fence - prevents reordering of prior writes.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # In real implementation, this would be a CPU fence instruction  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> full_fence</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Full memory fence - prevents all reordering across this point.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # In real implementation, this would be a CPU fence instruction</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Global memory barrier instance for educational use</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">memory_barrier </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MemoryBarrier()</span></span></code></pre></div>\n\n<p><strong>Thread Registry (<code>thread_registry.py</code>)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Thread registry for managing hazard pointer slots across all threads.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> weakref</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional, Set</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ThreadRecord</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Per-thread record containing hazard pointer slots and metadata.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    thread_id: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hazard_slots: List[Optional[Any]]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    next_record: Optional[</span><span style=\"color:#9ECBFF\">'ThreadRecord'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    active: </span><span style=\"color:#79B8FF\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, thread_id: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, num_hazard_slots: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 8</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.thread_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> thread_id</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.hazard_slots </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> num_hazard_slots</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.next_record </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.active </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ThreadRegistry</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Global registry managing all threads' hazard pointer records.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._head </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.RLock()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._local </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.local()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_thread_record</span><span style=\"color:#E1E4E8\">(self) -> ThreadRecord:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get or create thread record for current thread.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> hasattr</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._local, </span><span style=\"color:#9ECBFF\">'record'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._local.record</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        thread_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.get_ident()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Create new thread record</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            record </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ThreadRecord(thread_id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            record.next_record </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._head</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._head </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> record</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._local.record </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> record</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Register cleanup on thread exit</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            weakref.finalize(threading.current_thread(), </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._cleanup_thread, thread_id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> record</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _cleanup_thread</span><span style=\"color:#E1E4E8\">(self, thread_id: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Clean up thread record when thread exits.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            current </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._head</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            prev </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            while</span><span style=\"color:#E1E4E8\"> current:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> current.thread_id </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> thread_id:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    current.active </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    # Clear all hazard pointers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(current.hazard_slots)):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        current.hazard_slots[i] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    break</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                prev </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> current</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                current </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> current.next_record</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> collect_hazard_pointers</span><span style=\"color:#E1E4E8\">(self) -> Set[Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Collect all active hazard pointers from all threads.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        protected </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            current </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._head</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            while</span><span style=\"color:#E1E4E8\"> current:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> current.active:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    for</span><span style=\"color:#E1E4E8\"> pointer </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> current.hazard_slots:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                        if</span><span style=\"color:#E1E4E8\"> pointer </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                            protected.add(pointer)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                current </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> current.next_record</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> protected</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Global thread registry instance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">thread_registry </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ThreadRegistry()</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p>The following skeletons provide method signatures and detailed TODO comments that map directly to the algorithm steps described in previous sections.</p>\n<p><strong>CAS Retry Loop Pattern (<code>compare_and_swap.py</code>)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Compare-and-swap retry loop implementation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> random</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Callable, TypeVar, Optional, Tuple, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .memory_ordering </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> MemoryOrdering</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .atomic_reference </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> AtomicReference</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">T </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TypeVar(</span><span style=\"color:#9ECBFF\">'T'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> cas_retry_loop</span><span style=\"color:#E1E4E8\">(atomic_ref: AtomicReference, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                   update_function: Callable[[Any], Any],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                   max_attempts: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">) -> Tuple[</span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Generic CAS retry loop with exponential backoff.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        atomic_ref: Atomic reference to update</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        update_function: Function that computes new value from current value</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        max_attempts: Maximum retry attempts before giving up</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        (success, final_value): Success flag and final observed value</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    backoff_us </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">  # Start with 1 microsecond</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> attempt </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(max_attempts):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load current value from atomic reference using ACQUIRE ordering</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Compute new value by calling update_function(current)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Attempt CAS operation: compare_and_swap(current, new_value)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If CAS succeeded, return (True, new_value)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: If CAS failed, implement exponential backoff with jitter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Update backoff_us = min(backoff_us * 2, 1000) for next iteration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Add random jitter: time.sleep((backoff_us + random.randint(0, backoff_us)) / 1_000_000)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Continue to next iteration with updated current value from CAS failure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Max attempts reached - return (False, last_observed_value)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">None</span></span></code></pre></div>\n\n<p><strong>Hazard Pointer Protection (<code>hazard_pointers.py</code>)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Hazard pointer protection and reclamation system.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Optional, Tuple, List, Set</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .thread_registry </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> thread_registry, ThreadRecord</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .atomic_reference </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> AtomicReference</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> HazardPointer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Hazard pointer system for safe memory reclamation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._retirement_threshold </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 100</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> protect</span><span style=\"color:#E1E4E8\">(self, pointer: Any) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Protect a pointer from reclamation by announcing it in hazard pointer slot.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            pointer: Pointer to protect (can be None to clear protection)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            slot_index: Index of hazard slot used for protection</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get thread record for current thread from thread_registry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Find first available (None) slot in thread_record.hazard_slots</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Store pointer in the available slot using release memory ordering</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return slot index for later release() call</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: If no available slots, raise RuntimeError(\"No available hazard pointer slots\")</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> release</span><span style=\"color:#E1E4E8\">(self, slot_index: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Release hazard pointer protection by clearing the slot.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            slot_index: Index returned by previous protect() call</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get thread record for current thread</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate slot_index is within bounds of hazard_slots array</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Clear hazard_slots[slot_index] = None using release memory ordering</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Memory fence to ensure visibility to other threads scanning hazard pointers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> protect_and_verify</span><span style=\"color:#E1E4E8\">(self, atomic_ref: AtomicReference) -> Tuple[Any, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Safely load and protect a pointer with verification to prevent races.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            atomic_ref: Atomic reference to load pointer from</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            (pointer, slot_index): Protected pointer and hazard slot index</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        max_retries </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 10</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> attempt </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(max_retries):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load pointer from atomic_ref using ACQUIRE memory ordering</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If pointer is None, return (None, -1) - no protection needed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Protect the loaded pointer by calling self.protect(pointer)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify pointer still matches by re-loading atomic_ref with ACQUIRE</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: If verification_pointer == pointer, return (pointer, slot_index)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: If verification failed, call self.release(slot_index) and retry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Continue retry loop until success or max_retries exceeded</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Max retries exceeded - raise RuntimeError(\"Could not protect pointer after retries\")</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> retire</span><span style=\"color:#E1E4E8\">(self, node: Any) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Retire a node for eventual reclamation when safe.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            node: Node to retire (must not be currently protected)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get thread record for current thread</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Add node to thread-local retirement list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check if retirement list size exceeds threshold</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If threshold exceeded, trigger scan_and_reclaim()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Otherwise, defer reclamation until threshold reached</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> scan_and_reclaim</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Scan all hazard pointers and reclaim unprotected retired nodes.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Number of nodes successfully reclaimed</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Collect all retired nodes from all threads' retirement lists</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Collect all active hazard pointers from thread_registry.collect_hazard_pointers()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compute reclaimable_nodes = retired_nodes - protected_pointers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: For each reclaimable node, call its destructor and free memory</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Move non-reclaimable nodes back to retirement lists for next scan</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return count of successfully reclaimed nodes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Global hazard pointer system instance  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">hazard_system </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> HazardPointer()</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p>Each milestone builds incrementally on previous components, with specific verification steps to ensure correct implementation before proceeding.</p>\n<p><strong>Milestone 1: Atomic Operations Checkpoint</strong></p>\n<ul>\n<li><strong>Command</strong>: <code>python -m pytest testing/test_atomic_operations.py -v</code></li>\n<li><strong>Expected Output</strong>: All CAS retry loops complete successfully under contention</li>\n<li><strong>Manual Verification</strong>: Run <code>python examples/atomic_counter_demo.py</code> with 10 threads incrementing shared counter 1000 times each - final count should be exactly 10000</li>\n<li><strong>Success Indicators</strong>: No lost updates, no infinite loops, consistent memory ordering behavior</li>\n<li><strong>Failure Signs</strong>: Final count != expected total (indicates lost updates), test timeouts (indicates infinite spinning), inconsistent results across runs (indicates memory ordering bugs)</li>\n</ul>\n<p><strong>Milestone 2: Lock-free Stack Checkpoint</strong></p>\n<ul>\n<li><strong>Command</strong>: <code>python -m pytest testing/test_treiber_stack.py -v</code></li>\n<li><strong>Expected Output</strong>: Concurrent push/pop operations maintain LIFO ordering and no lost elements</li>\n<li><strong>Manual Verification</strong>: Run <code>python examples/stack_stress_test.py</code> with multiple producers and consumers - total elements in + out should balance exactly</li>\n<li><strong>Success Indicators</strong>: Perfect LIFO ordering, no duplicate elements, no lost elements, linearizable behavior</li>\n<li><strong>Failure Signs</strong>: Elements appearing out of LIFO order, duplicate pops of same element, missing elements, crashes on empty stack pop</li>\n</ul>\n<p><strong>Milestone 3: Lock-free Queue Checkpoint</strong></p>\n<ul>\n<li><strong>Command</strong>: <code>python -m pytest testing/test_michael_scott_queue.py -v</code></li>\n<li><strong>Expected Output</strong>: Concurrent enqueue/dequeue operations maintain FIFO ordering</li>\n<li><strong>Manual Verification</strong>: Enqueue elements 1,2,3,4,5 from different threads, dequeue should return exactly 1,2,3,4,5 in order</li>\n<li><strong>Success Indicators</strong>: Perfect FIFO ordering, tail helping mechanism works, no stuck operations</li>\n<li><strong>Failure Signs</strong>: Elements dequeued out of FIFO order, operations hanging when tail pointer lags, memory corruption</li>\n</ul>\n<p><strong>Milestone 4: Hazard Pointers Checkpoint</strong></p>\n<ul>\n<li><strong>Command</strong>: <code>python -m pytest testing/test_hazard_pointers.py -v</code></li>\n<li><strong>Expected Output</strong>: No use-after-free errors, proper protection and reclamation</li>\n<li><strong>Manual Verification</strong>: Run stack/queue stress tests with aggressive reclamation - no crashes or memory errors</li>\n<li><strong>Success Indicators</strong>: No premature reclamation, proper thread cleanup, bounded memory usage</li>\n<li><strong>Failure Signs</strong>: Segmentation faults, accessing freed memory, unbounded memory growth, protection race conditions</li>\n</ul>\n<p><strong>Milestone 5: Hash Map Checkpoint</strong></p>\n<ul>\n<li><strong>Command</strong>: <code>python -m pytest testing/test_split_ordered_hashmap.py -v</code></li>\n<li><strong>Expected Output</strong>: Concurrent insert/lookup/delete operations with correct key-value associations</li>\n<li><strong>Manual Verification</strong>: Insert 1000 unique key-value pairs from multiple threads, verify all lookups return correct values</li>\n<li><strong>Success Indicators</strong>: All inserted keys found with correct values, successful deletion removes keys, bucket splitting works correctly</li>\n<li><strong>Failure Signs</strong>: Lost key-value pairs, incorrect values returned, infinite loops during bucket initialization, corrupted bucket chains</li>\n</ul>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<p>Lock-free code exhibits unique failure modes that require specialized debugging approaches. The following table maps common symptoms to their likely causes and diagnostic steps.</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Final counter != expected total</td>\n<td>Lost updates due to race condition</td>\n<td>Add logging to CAS failures, check retry counts</td>\n<td>Ensure CAS retry loop, verify atomic operations</td>\n</tr>\n<tr>\n<td>Infinite loop in CAS retry</td>\n<td>Target value constantly changing</td>\n<td>Log current/expected values in each iteration</td>\n<td>Add exponential backoff, limit max retries</td>\n</tr>\n<tr>\n<td>Segmentation fault during pop/dequeue</td>\n<td>Use-after-free from premature reclamation</td>\n<td>Check hazard pointer protection, trace node lifecycle</td>\n<td>Protect nodes before access, verify retirement timing</td>\n</tr>\n<tr>\n<td>Elements appearing out of order</td>\n<td>Incorrect linearization points</td>\n<td>Record operation timestamps, verify ordering properties</td>\n<td>Review CAS placement, ensure proper memory ordering</td>\n</tr>\n<tr>\n<td>Memory usage growing unbounded</td>\n<td>Retirement list not being processed</td>\n<td>Monitor retirement list sizes, check scan frequency</td>\n<td>Lower scan threshold, ensure scan_and_reclaim() called</td>\n</tr>\n<tr>\n<td>Operations hanging indefinitely</td>\n<td>Missing helping mechanism</td>\n<td>Check tail pointer updates in queue</td>\n<td>Implement helping in enqueue, verify tail advancement</td>\n</tr>\n<tr>\n<td>Corrupted linked list structure</td>\n<td>ABA problem or incorrect CAS</td>\n<td>Add tagged pointers, verify pointer consistency</td>\n<td>Use version counters, validate next pointer integrity</td>\n</tr>\n<tr>\n<td>Performance degradation under contention</td>\n<td>Excessive CAS failures and retries</td>\n<td>Profile CAS success rates, measure retry counts</td>\n<td>Optimize backoff strategy, reduce contention points</td>\n</tr>\n</tbody></table>\n<p>Understanding these patterns helps developers quickly identify and fix the subtle bugs that are common in lock-free programming, leading to more robust and performant implementations.</p>\n<h2 id=\"error-handling-and-edge-cases\">Error Handling and Edge Cases</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (this section addresses failure modes and recovery strategies that apply across all lock-free data structures, from basic atomic operations through complete hash maps)</p>\n</blockquote>\n<p>The world of lock-free programming introduces a completely new category of failure modes that simply don&#39;t exist in traditional lock-based systems. Unlike mutex-protected code where errors are typically straightforward resource conflicts or simple logic bugs, lock-free algorithms face subtle timing-dependent failures that can manifest sporadically under load, making them notoriously difficult to reproduce and debug. These failures often stem from the fundamental tension between maintaining correctness without coordination primitives and achieving progress without blocking.</p>\n<h3 id=\"mental-model-lock-free-errors-as-traffic-intersection-problems\">Mental Model: Lock-free Errors as Traffic Intersection Problems</h3>\n<p>Think of lock-free programming errors like problems at a busy traffic intersection without traffic lights or stop signs. In a traditional lock-based system, we&#39;d have traffic lights (mutexes) that ensure only one direction moves at a time - failures are obvious and immediate, like a broken light or a car running a red light. But in lock-free systems, we&#39;re trying to coordinate traffic flow using only yield signs and careful observation.</p>\n<p>The ABA problem is like a car leaving the intersection, another identical car entering from the same direction, and a driver thinking the intersection status hasn&#39;t changed when it actually has. Livelock resembles cars politely yielding to each other indefinitely, with everyone being courteous but no one making progress. Memory reclamation races are like demolishing a building while people might still be inside, not knowing if anyone is currently using that space. These problems require fundamentally different detection and recovery strategies than traditional &quot;broken traffic light&quot; failures.</p>\n<h3 id=\"lock-free-specific-failure-modes\">Lock-free Specific Failure Modes</h3>\n<p>Lock-free algorithms introduce several categories of failures that are unique to non-blocking concurrent programming. Understanding these failure modes is crucial because they often manifest as subtle correctness violations or performance degradations rather than obvious crashes.</p>\n<h4 id=\"aba-problem-manifestations\">ABA Problem Manifestations</h4>\n<p>The <strong>ABA problem</strong> represents one of the most insidious failure modes in lock-free programming. This occurs when a compare-and-swap operation incorrectly succeeds because a memory location has returned to its original value despite being modified by other threads. The fundamental issue is that CAS only checks the current value, not whether the value has changed since the original read.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Scenario</th>\n<th>Symptom</th>\n<th>Root Cause</th>\n<th>Consequences</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Node Reuse in Stack</td>\n<td>Lost elements during concurrent pop operations</td>\n<td>Stack node freed and reallocated at same address</td>\n<td>Stack corruption, elements permanently lost</td>\n</tr>\n<tr>\n<td>Pointer Recycling</td>\n<td>CAS succeeds on recycled pointer</td>\n<td>Memory allocator reuses address for new object</td>\n<td>Data structure links to wrong object type</td>\n</tr>\n<tr>\n<td>Reference Count Races</td>\n<td>Premature object deletion</td>\n<td>Reference count drops to zero then increases</td>\n<td>Use-after-free, memory corruption</td>\n</tr>\n<tr>\n<td>Tag Counter Overflow</td>\n<td>False ABA detection after tag wraparound</td>\n<td>Version counter overflows back to original value</td>\n<td>Incorrect operation success or failure</td>\n</tr>\n</tbody></table>\n<p>Consider a concrete example in the Treiber stack. Thread A reads the top pointer (value 0x1000), gets interrupted, and Thread B pops that node and pushes a new node that happens to get allocated at the same address (0x1000). When Thread A resumes and performs its CAS, it succeeds because the address matches, but it&#39;s now operating on a completely different node. This can result in the stack pointing to freed memory or creating cycles in the node chain.</p>\n<blockquote>\n<p><strong>Critical Insight</strong>: The ABA problem is fundamentally about the inadequacy of single-word comparison for determining whether a data structure has been modified. The solution requires expanding the comparison to include additional state that changes with each modification.</p>\n</blockquote>\n<h4 id=\"livelock-and-progress-guarantee-violations\">Livelock and Progress Guarantee Violations</h4>\n<p><strong>Livelock</strong> occurs when threads continuously retry operations but make no collective progress because their attempts interfere with each other. Unlike deadlock where threads are blocked, livelock threads remain active but accomplish nothing productive. This is particularly problematic in lock-free algorithms because the progress guarantee requires that at least one thread makes progress within a bounded number of steps.</p>\n<table>\n<thead>\n<tr>\n<th>Livelock Type</th>\n<th>Detection Pattern</th>\n<th>Common Causes</th>\n<th>Impact on System</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>CAS Thrashing</td>\n<td>High CPU usage, low throughput</td>\n<td>Multiple threads competing for same atomic variable</td>\n<td>Performance degradation, potential starvation</td>\n</tr>\n<tr>\n<td>Helping Conflicts</td>\n<td>Operations appear to succeed but data structure state oscillates</td>\n<td>Threads helping each other in conflicting ways</td>\n<td>Functional correctness violations</td>\n</tr>\n<tr>\n<td>Backoff Synchronization</td>\n<td>Periodic performance drops</td>\n<td>Threads using same backoff timing</td>\n<td>Reduced parallelism, wasted CPU cycles</td>\n</tr>\n<tr>\n<td>Memory Contention</td>\n<td>Cache miss rates spike</td>\n<td>False sharing on atomic variables</td>\n<td>System-wide performance impact</td>\n</tr>\n</tbody></table>\n<p>The Michael-Scott queue&#39;s helping mechanism can exhibit livelock when multiple threads attempt to help advance the tail pointer simultaneously. If the helping logic isn&#39;t carefully designed, threads can continuously undo each other&#39;s work, with the tail pointer oscillating between positions without the queue making meaningful progress.</p>\n<h4 id=\"memory-reclamation-races\">Memory Reclamation Races</h4>\n<p>Memory reclamation in lock-free systems introduces timing-dependent failures where nodes are deallocated while other threads still hold references to them. These failures are particularly dangerous because they often manifest as memory corruption rather than immediate crashes, making diagnosis extremely difficult.</p>\n<table>\n<thead>\n<tr>\n<th>Race Condition</th>\n<th>Symptom</th>\n<th>Detection Method</th>\n<th>Critical Timing Window</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Use-After-Free</td>\n<td>Segmentation fault, data corruption</td>\n<td>Memory sanitizers, valgrind</td>\n<td>Between retire() call and actual deallocation</td>\n</tr>\n<tr>\n<td>Double-Free</td>\n<td>Heap corruption, allocator crashes</td>\n<td>Debug heap, address sanitizer</td>\n<td>Multiple threads retiring same node</td>\n</tr>\n<tr>\n<td>Premature Reclamation</td>\n<td>Stale data reads, incorrect operation results</td>\n<td>Hazard pointer validation</td>\n<td>Node freed before all readers finish</td>\n</tr>\n<tr>\n<td>Retirement List Overflow</td>\n<td>Memory leaks, unbounded growth</td>\n<td>Memory usage monitoring</td>\n<td>Retirement rate exceeds scan frequency</td>\n</tr>\n</tbody></table>\n<p>The hazard pointer protocol&#39;s protect-then-verify pattern can fail if not implemented atomically. A thread might protect a pointer, but between the protection and verification steps, another thread could retire and reclaim the node, leading to the verification step accessing freed memory.</p>\n<blockquote>\n<p><strong>Design Principle</strong>: Memory reclamation safety requires that the window between node removal from the data structure and actual deallocation be managed with explicit coordination between threads.</p>\n</blockquote>\n<h4 id=\"progress-guarantee-violations\">Progress Guarantee Violations</h4>\n<p>Lock-free algorithms promise that at least one thread makes progress within a bounded number of steps, but several conditions can violate this guarantee without causing obvious failures.</p>\n<table>\n<thead>\n<tr>\n<th>Violation Type</th>\n<th>Manifestation</th>\n<th>Underlying Cause</th>\n<th>Recovery Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Starvation</td>\n<td>Some threads never complete operations</td>\n<td>Unfair scheduling, priority inversion</td>\n<td>Exponential backoff, helping protocols</td>\n</tr>\n<tr>\n<td>Infinite Retry</td>\n<td>CAS loops that never terminate</td>\n<td>Continuous interference from other threads</td>\n<td>Maximum retry limits, fallback mechanisms</td>\n</tr>\n<tr>\n<td>Helping Deadlock</td>\n<td>Helper threads block each other</td>\n<td>Circular helping dependencies</td>\n<td>Helping order constraints, timeout mechanisms</td>\n</tr>\n<tr>\n<td>Memory Ordering Issues</td>\n<td>Operations appear to complete but state is inconsistent</td>\n<td>Insufficient memory barriers</td>\n<td>Stronger ordering guarantees</td>\n</tr>\n</tbody></table>\n<h3 id=\"failure-detection-strategies\">Failure Detection Strategies</h3>\n<p>Detecting failures in lock-free systems requires different approaches than traditional concurrent programming because many failures manifest as subtle correctness violations rather than obvious exceptions.</p>\n<h4 id=\"invariant-violation-detection\">Invariant Violation Detection</h4>\n<p>Lock-free data structures maintain complex invariants that can be violated by race conditions or implementation bugs. Continuous monitoring of these invariants helps detect problems before they cause observable failures.</p>\n<table>\n<thead>\n<tr>\n<th>Data Structure</th>\n<th>Critical Invariants</th>\n<th>Detection Method</th>\n<th>Monitoring Frequency</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Treiber Stack</td>\n<td>Top pointer always points to valid node or NULL</td>\n<td>Pointer validation, reachability checks</td>\n<td>Every operation in debug mode</td>\n</tr>\n<tr>\n<td>Michael-Scott Queue</td>\n<td>Head and tail pointers maintain proper distance</td>\n<td>Distance calculation, sentinel validation</td>\n<td>Periodic background scan</td>\n</tr>\n<tr>\n<td>Split-Ordered HashMap</td>\n<td>Bucket initialization order preserved</td>\n<td>Parent-child bucket relationship checks</td>\n<td>During bucket access</td>\n</tr>\n<tr>\n<td>Hazard Pointer System</td>\n<td>Protected pointers not in retirement list</td>\n<td>Cross-reference scan</td>\n<td>Before each reclamation</td>\n</tr>\n</tbody></table>\n<p>The stack invariant checker validates that every node reachable from the top pointer has a valid next pointer and that no cycles exist in the chain. Implementation involves traversing the entire stack while checking that each node&#39;s memory address falls within valid allocated ranges.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_stack_invariants</span><span style=\"color:#E1E4E8\">(stack):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Validates critical stack invariants without modifying structure.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns tuple of (is_valid, violation_description).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Load top pointer atomically</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Traverse chain checking for cycles using visited set</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify each node memory address is in valid range</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check that each next pointer is atomic reference</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return (True, \"\") if all checks pass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"performance-degradation-monitoring\">Performance Degradation Monitoring</h4>\n<p>Lock-free algorithms can experience gradual performance degradation due to increased contention, cache line bouncing, or suboptimal memory access patterns. Early detection prevents minor issues from escalating into system-wide problems.</p>\n<table>\n<thead>\n<tr>\n<th>Performance Metric</th>\n<th>Normal Range</th>\n<th>Warning Threshold</th>\n<th>Critical Threshold</th>\n<th>Diagnostic Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>CAS Success Rate</td>\n<td>85-95%</td>\n<td>&lt;75%</td>\n<td>&lt;50%</td>\n<td>Analyze contention patterns</td>\n</tr>\n<tr>\n<td>Operation Latency</td>\n<td>&lt;1μs</td>\n<td>&gt;10μs</td>\n<td>&gt;100μs</td>\n<td>Profile memory access patterns</td>\n</tr>\n<tr>\n<td>Retry Loop Iterations</td>\n<td>1-3 avg</td>\n<td>&gt;10 avg</td>\n<td>&gt;50 avg</td>\n<td>Implement exponential backoff</td>\n</tr>\n<tr>\n<td>Hazard Pointer Scan Time</td>\n<td>&lt;1ms</td>\n<td>&gt;10ms</td>\n<td>&gt;100ms</td>\n<td>Reduce retirement threshold</td>\n</tr>\n</tbody></table>\n<p>The retry loop monitoring system tracks the distribution of CAS attempts per operation across all threads. A sudden increase in retry counts often indicates either increased load or the emergence of a livelock condition.</p>\n<h4 id=\"correctness-verification-through-linearizability-checking\">Correctness Verification Through Linearizability Checking</h4>\n<p>Linearizability provides a formal correctness criterion for concurrent data structures by requiring that operations appear to take effect atomically at some point between their invocation and response. Implementing runtime linearizability checking helps catch subtle correctness bugs that manifest only under specific thread interleavings.</p>\n<table>\n<thead>\n<tr>\n<th>Verification Approach</th>\n<th>Accuracy</th>\n<th>Performance Cost</th>\n<th>Implementation Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Online Checking</td>\n<td>High</td>\n<td>10-20x overhead</td>\n<td>Complex state tracking</td>\n</tr>\n<tr>\n<td>Offline Analysis</td>\n<td>Very High</td>\n<td>Post-execution only</td>\n<td>Log analysis algorithms</td>\n</tr>\n<tr>\n<td>Statistical Sampling</td>\n<td>Medium</td>\n<td>2-3x overhead</td>\n<td>Moderate instrumentation</td>\n</tr>\n<tr>\n<td>Stress Test Oracles</td>\n<td>Medium</td>\n<td>Test-time only</td>\n<td>Property-based assertions</td>\n</tr>\n</tbody></table>\n<p>The linearizability checker maintains a log of all operation invocations and responses across threads, then analyzes the log to determine if there exists a valid sequential execution that respects the timing constraints and data structure semantics.</p>\n<blockquote>\n<p><strong>Decision: Online vs Offline Linearizability Checking</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to verify correctness during development and testing phases</li>\n<li><strong>Options Considered</strong>: Real-time checking during execution vs post-execution analysis vs sampling-based approximation</li>\n<li><strong>Decision</strong>: Implement both online sampling (for development) and offline complete analysis (for comprehensive testing)</li>\n<li><strong>Rationale</strong>: Online checking catches problems immediately during development, while offline analysis provides definitive verification for critical test scenarios</li>\n<li><strong>Consequences</strong>: Development builds include instrumentation overhead, but production builds remain unaffected; comprehensive testing requires log storage and analysis infrastructure</li>\n</ul>\n</blockquote>\n<h3 id=\"recovery-and-fallback-mechanisms\">Recovery and Fallback Mechanisms</h3>\n<p>When lock-free algorithms encounter the failure modes described above, they need robust recovery mechanisms that restore progress without compromising correctness guarantees.</p>\n<h4 id=\"exponential-backoff-strategies\">Exponential Backoff Strategies</h4>\n<p>Exponential backoff helps resolve contention-based failures by introducing randomized delays that reduce the probability of continued interference between competing threads. However, the backoff strategy must be carefully tuned to balance conflict avoidance with responsiveness.</p>\n<table>\n<thead>\n<tr>\n<th>Backoff Parameter</th>\n<th>Conservative Value</th>\n<th>Aggressive Value</th>\n<th>Adaptive Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Initial Delay</td>\n<td>10 CPU cycles</td>\n<td>1 CPU cycle</td>\n<td>Based on recent success rate</td>\n</tr>\n<tr>\n<td>Maximum Delay</td>\n<td>1000 CPU cycles</td>\n<td>100 CPU cycles</td>\n<td>Scaled by system load</td>\n</tr>\n<tr>\n<td>Multiplier</td>\n<td>2.0</td>\n<td>1.5</td>\n<td>Adjusted by contention level</td>\n</tr>\n<tr>\n<td>Jitter Range</td>\n<td>±25%</td>\n<td>±10%</td>\n<td>Increased under high contention</td>\n</tr>\n</tbody></table>\n<p>The adaptive backoff implementation monitors per-thread CAS success rates and adjusts parameters dynamically. Threads experiencing frequent failures increase their backoff aggressiveness, while threads with high success rates use minimal delays to maintain performance.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> cas_retry_loop</span><span style=\"color:#E1E4E8\">(atomic_ref, update_function, max_attempts):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Generic CAS retry loop with exponential backoff.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns (success, final_value, attempts_made).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize backoff parameters (delay, max_delay, multiplier)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Loop up to max_attempts times</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Load current value from atomic_ref</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Compute new value using update_function</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Attempt CAS with expected=current, new=computed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: If CAS succeeds, return (True, new_value, attempts)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: If CAS fails, apply exponential backoff delay</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Update backoff delay = min(delay * multiplier, max_delay)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add random jitter to prevent synchronized retries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: If max_attempts reached, return (False, observed_value, attempts)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"helping-protocol-recovery\">Helping Protocol Recovery</h4>\n<p>The helping mechanism in lock-free algorithms allows threads to assist each other when operations become stuck, but helping protocols can themselves become sources of failure. Robust helping requires careful ordering and timeout mechanisms.</p>\n<table>\n<thead>\n<tr>\n<th>Helping Scenario</th>\n<th>Detection Trigger</th>\n<th>Helper Action</th>\n<th>Recovery Condition</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Stuck Tail Update</td>\n<td>Tail pointer lags behind actual end</td>\n<td>Advance tail to correct position</td>\n<td>Successful tail CAS</td>\n</tr>\n<tr>\n<td>Incomplete Enqueue</td>\n<td>Node linked but tail not updated</td>\n<td>Complete the tail update</td>\n<td>Tail points to new node</td>\n</tr>\n<tr>\n<td>Abandoned Operation</td>\n<td>Thread terminated mid-operation</td>\n<td>Roll back partial changes</td>\n<td>Consistent structure state</td>\n</tr>\n<tr>\n<td>Helping Conflict</td>\n<td>Multiple helpers interfere</td>\n<td>Coordinate through additional CAS</td>\n<td>Single helper succeeds</td>\n</tr>\n</tbody></table>\n<p>The Michael-Scott queue&#39;s helping protocol ensures that if one thread observes the tail pointer lagging behind the actual end of the queue, it attempts to advance the tail pointer to the correct position. This prevents enqueue operations from getting stuck when the thread that added a node fails to complete the tail update.</p>\n<blockquote>\n<p><strong>Critical Design Constraint</strong>: Helping protocols must be designed to be idempotent - multiple threads performing the same helping action should not create inconsistent state or undo each other&#39;s work.</p>\n</blockquote>\n<h4 id=\"graceful-degradation-under-extreme-contention\">Graceful Degradation Under Extreme Contention</h4>\n<p>When contention becomes so severe that normal lock-free operation becomes inefficient, the system needs fallback mechanisms that maintain correctness while trading some performance for guaranteed progress.</p>\n<table>\n<thead>\n<tr>\n<th>Degradation Level</th>\n<th>Trigger Condition</th>\n<th>Fallback Strategy</th>\n<th>Performance Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Increased Backoff</td>\n<td>CAS success rate &lt;75%</td>\n<td>Double backoff parameters</td>\n<td>10-20% throughput reduction</td>\n</tr>\n<tr>\n<td>Helping Escalation</td>\n<td>Operations stuck &gt;100 retries</td>\n<td>Activate additional helping threads</td>\n<td>20-30% overhead increase</td>\n</tr>\n<tr>\n<td>Temporary Serialization</td>\n<td>System-wide contention detected</td>\n<td>Funnel operations through single point</td>\n<td>50-80% throughput reduction</td>\n</tr>\n<tr>\n<td>Emergency Locking</td>\n<td>Lock-free progress stalled</td>\n<td>Fall back to mutex protection</td>\n<td>Eliminates lock-free benefits</td>\n</tr>\n</tbody></table>\n<p>The emergency fallback system monitors system-wide operation success rates and can temporarily serialize access through a single coordination point when lock-free progress guarantees are violated. This ensures the system remains functional even under pathological conditions.</p>\n<h4 id=\"memory-reclamation-recovery\">Memory Reclamation Recovery</h4>\n<p>When hazard pointer systems encounter failures or resource exhaustion, recovery mechanisms must safely handle the accumulated retirement list without creating use-after-free conditions.</p>\n<table>\n<thead>\n<tr>\n<th>Recovery Scenario</th>\n<th>Trigger</th>\n<th>Recovery Action</th>\n<th>Safety Guarantee</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Retirement List Overflow</td>\n<td>List size &gt; threshold</td>\n<td>Force immediate scan and reclamation</td>\n<td>No premature reclamation</td>\n</tr>\n<tr>\n<td>Scan Failure</td>\n<td>Scan finds no reclaimable nodes</td>\n<td>Increase scan frequency, reduce threshold</td>\n<td>Bounded memory growth</td>\n</tr>\n<tr>\n<td>Thread Exit Cleanup</td>\n<td>Thread termination detected</td>\n<td>Reclaim thread&#39;s retirement list</td>\n<td>No leaked memory</td>\n</tr>\n<tr>\n<td>Hazard Slot Exhaustion</td>\n<td>All slots occupied</td>\n<td>Expand slot array or force reclamation</td>\n<td>Continued protection ability</td>\n</tr>\n</tbody></table>\n<p>The emergency reclamation procedure performs a conservative scan where any questionable nodes remain in the retirement list rather than being reclaimed. This ensures memory safety at the cost of potentially increased memory usage until the next successful scan.</p>\n<h3 id=\"common-recovery-implementation-pitfalls\">Common Recovery Implementation Pitfalls</h3>\n<p>Understanding typical mistakes in recovery mechanism implementation helps avoid creating new failure modes while trying to handle existing ones.</p>\n<p>⚠️ <strong>Pitfall: Recursive Recovery Failures</strong>\nRecovery mechanisms that can themselves fail and trigger additional recovery attempts create the potential for infinite recursion or resource exhaustion. For example, if the exponential backoff system allocates memory for timing state and that allocation fails, attempting to recover by triggering more backoff can exhaust available memory. The fix requires making recovery mechanisms allocation-free and ensuring they have bounded resource requirements.</p>\n<p>⚠️ <strong>Pitfall: Recovery Actions Violating Lock-free Properties</strong>\nSome recovery strategies inadvertently introduce blocking behavior that violates the fundamental lock-free progress guarantee. Using mutexes in fallback paths or waiting indefinitely for helper threads to complete their work can cause the entire system to block. Recovery mechanisms must preserve the non-blocking nature of the algorithm, even if they reduce performance.</p>\n<p>⚠️ <strong>Pitfall: Incomplete Failure State Cleanup</strong>\nFailed operations often leave the data structure in a partially modified state that must be properly cleaned up before other operations can proceed. For example, if a queue enqueue operation links a new node but fails to update the tail pointer, the cleanup must ensure the tail update completes rather than leaving the queue in an inconsistent state.</p>\n<p>⚠️ <strong>Pitfall: Race Conditions in Error Detection</strong>\nThe error detection mechanisms themselves can introduce race conditions if they&#39;re not implemented with the same care as the main algorithm. Reading multiple atomic variables to check invariants without proper ordering can observe inconsistent intermediate states that appear to be errors but are actually valid transient conditions.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The error handling and recovery systems for lock-free data structures require careful implementation to avoid introducing new failure modes while solving existing ones.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Error Detection</td>\n<td>Manual invariant checks with assertions</td>\n<td>Automated linearizability checker with full history analysis</td>\n</tr>\n<tr>\n<td>Performance Monitoring</td>\n<td>Simple counters with periodic logging</td>\n<td>Real-time metrics with statistical analysis and alerting</td>\n</tr>\n<tr>\n<td>Recovery Mechanisms</td>\n<td>Fixed exponential backoff with manual tuning</td>\n<td>Adaptive backoff with machine learning optimization</td>\n</tr>\n<tr>\n<td>Memory Safety</td>\n<td>Basic hazard pointers with conservative scanning</td>\n<td>Advanced epoch-based reclamation with optimized batching</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-module-structure\">Recommended Module Structure</h4>\n<p>The error handling system integrates across all components of the lock-free library:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>lock_free_lib/\n  error_handling/\n    __init__.py                 ← Error types and base classes\n    detection/\n      invariant_checker.py      ← Data structure invariant validation\n      performance_monitor.py    ← Contention and latency tracking\n      linearizability.py       ← Correctness verification\n    recovery/\n      backoff_strategies.py     ← Exponential and adaptive backoff\n      helping_protocols.py      ← Recovery through inter-thread cooperation\n      fallback_mechanisms.py    ← Graceful degradation under extreme load\n    memory_safety/\n      hazard_validation.py      ← Hazard pointer correctness checks\n      leak_detection.py         ← Memory reclamation monitoring\n  data_structures/\n    stack.py                   ← Treiber stack with error handling integration\n    queue.py                   ← Michael-Scott queue with recovery mechanisms\n    hashmap.py                 ← Split-ordered map with degradation handling\n  testing/\n    stress_tests.py           ← High-contention correctness verification\n    failure_injection.py      ← Systematic failure mode testing</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Error Detection Foundation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Tuple, List, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> threading </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> current_thread</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> weakref</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> FailureMode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ABA_PROBLEM</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"aba_problem\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LIVELOCK</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"livelock\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MEMORY_RACE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"memory_reclamation_race\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PROGRESS_VIOLATION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"progress_guarantee_violation\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INVARIANT_VIOLATION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"invariant_violation\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> FailureReport</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    failure_mode: FailureMode</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    thread_id: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    operation_type: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    details: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stack_trace: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> InvariantChecker</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class for data structure invariant validation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, enable_checking: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.enable_checking </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> enable_checking</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.violation_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.last_check_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> check_invariants</span><span style=\"color:#E1E4E8\">(self, data_structure: Any) -> Tuple[</span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Validates all invariants for the given data structure.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns (is_valid, violation_description).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.enable_checking:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.last_check_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._validate_structure_specific_invariants(data_structure)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.violation_count </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Exception during invariant check: </span><span style=\"color:#79B8FF\">{str</span><span style=\"color:#E1E4E8\">(e)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _validate_structure_specific_invariants</span><span style=\"color:#E1E4E8\">(self, data_structure: Any) -> Tuple[</span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Implement structure-specific invariant checks.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PerformanceMonitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Tracks operation performance and detects degradation patterns.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, history_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.operation_times </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}  </span><span style=\"color:#6A737D\"># operation_type -> list of (timestamp, duration)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cas_attempts </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}     </span><span style=\"color:#6A737D\"># operation_type -> list of attempt_counts</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.success_rates </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}    </span><span style=\"color:#6A737D\"># operation_type -> recent success rate</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.history_size </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> history_size</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> record_operation</span><span style=\"color:#E1E4E8\">(self, operation_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, duration: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, cas_attempts: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, success: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record performance metrics for a completed operation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        current_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Track operation timing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> operation_type </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.operation_times:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.operation_times[operation_type] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        times_list </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.operation_times[operation_type]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        times_list.append((current_time, duration))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Maintain bounded history</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(times_list) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.history_size:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            times_list.pop(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Track CAS attempt distribution</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> operation_type </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.cas_attempts:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.cas_attempts[operation_type] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        attempts_list </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.cas_attempts[operation_type]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        attempts_list.append(cas_attempts)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(attempts_list) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.history_size:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            attempts_list.pop(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Update success rate (sliding window)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._update_success_rate(operation_type, success)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _update_success_rate</span><span style=\"color:#E1E4E8\">(self, operation_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, success: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Update rolling success rate for operation type.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> operation_type </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.success_rates:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.success_rates[operation_type] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        rate_history </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.success_rates[operation_type]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        rate_history.append(</span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> success </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(rate_history) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#E1E4E8\">:  </span><span style=\"color:#6A737D\"># Keep last 100 operations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            rate_history.pop(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_current_metrics</span><span style=\"color:#E1E4E8\">(self, operation_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get current performance metrics for operation type.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> operation_type </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.operation_times:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        times </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [duration </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> _, duration </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.operation_times[operation_type]]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        attempts </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.cas_attempts.get(operation_type, [])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        success_rate </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> sum</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.success_rates.get(operation_type, [])) </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.success_rates.get(operation_type, [])))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'avg_duration'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">sum</span><span style=\"color:#E1E4E8\">(times) </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(times) </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> times </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'max_duration'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">max</span><span style=\"color:#E1E4E8\">(times) </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> times </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'avg_cas_attempts'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">sum</span><span style=\"color:#E1E4E8\">(attempts) </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(attempts) </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> attempts </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'max_cas_attempts'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">max</span><span style=\"color:#E1E4E8\">(attempts) </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> attempts </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'success_rate'</span><span style=\"color:#E1E4E8\">: success_rate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Global performance monitor instance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">performance_monitor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> PerformanceMonitor()</span></span></code></pre></div>\n\n<p><strong>Recovery Infrastructure:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> random</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Callable, TypeVar, Tuple, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> threading </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> current_thread</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">T </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TypeVar(</span><span style=\"color:#9ECBFF\">'T'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> BackoffStrategy</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Implements adaptive exponential backoff for CAS retry loops.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, initial_delay: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">, max_delay: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">, multiplier: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2.0</span><span style=\"color:#E1E4E8\">, jitter: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.25</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.initial_delay </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> initial_delay</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_delay </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_delay</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.multiplier </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> multiplier</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.jitter </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> jitter</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_delay </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> initial_delay</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.consecutive_failures </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> wait</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute backoff delay with jitter.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_delay </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            jitter_amount </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_delay </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.jitter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            actual_delay </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_delay </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> random.uniform(</span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\">jitter_amount, jitter_amount)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Convert to seconds (assuming delays are in microseconds)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            time.sleep(</span><span style=\"color:#79B8FF\">max</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, actual_delay) </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1_000_000</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> on_failure</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Called when CAS operation fails - increase backoff.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.consecutive_failures </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_delay </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> min</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current_delay </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.multiplier, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.max_delay)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> on_success</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Called when CAS operation succeeds - reset backoff.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.consecutive_failures </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_delay </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.initial_delay</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RetryLoop</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Generic retry loop with backoff and monitoring.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, max_attempts: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_attempts </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_attempts</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.thread_backoff </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}  </span><span style=\"color:#6A737D\"># thread_id -> BackoffStrategy</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _get_backoff_for_thread</span><span style=\"color:#E1E4E8\">(self) -> BackoffStrategy:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get or create backoff strategy for current thread.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        thread_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> current_thread().ident</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> thread_id </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.thread_backoff:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.thread_backoff[thread_id] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> BackoffStrategy()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.thread_backoff[thread_id]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> execute</span><span style=\"color:#E1E4E8\">(self, operation: Callable[[], Tuple[</span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, T]], operation_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"unknown\"</span><span style=\"color:#E1E4E8\">) -> Tuple[</span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, Optional[T], </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Execute operation with retry and backoff.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns (success, result, attempts_made).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        backoff </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._get_backoff_for_thread()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> attempt </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.max_attempts </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                success, result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> operation()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> success:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    backoff.on_success()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    duration </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    performance_monitor.record_operation(operation_name, duration, attempt, </span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    return</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, result, attempt)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    backoff.on_failure()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    if</span><span style=\"color:#E1E4E8\"> attempt </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.max_attempts:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        backoff.wait()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Treat exceptions as failures, but don't retry on permanent errors</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                backoff.on_failure()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                break</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # All attempts failed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        duration </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        performance_monitor.record_operation(operation_name, duration, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.max_attempts, </span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.max_attempts)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Global retry loop instance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">default_retry_loop </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> RetryLoop()</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>CAS Retry Loop with Monitoring:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> cas_retry_loop</span><span style=\"color:#E1E4E8\">(atomic_ref, update_function, max_attempts):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Generic CAS retry loop with exponential backoff and performance monitoring.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        atomic_ref: AtomicReference to update</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        update_function: Function that takes current value and returns new value</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        max_attempts: Maximum number of CAS attempts before giving up</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Tuple of (success: bool, final_value: Any, attempts_made: int)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get thread-local backoff strategy instance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Record start time for performance monitoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Loop from 1 to max_attempts (inclusive)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Load current value from atomic_ref with appropriate memory ordering</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Compute new value using update_function(current_value)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Attempt compare_and_swap(expected=current, new=computed)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: If CAS succeeds, record success metrics and return (True, new_value, attempts)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: If CAS fails, call backoff.on_failure() and backoff.wait()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Continue loop unless max_attempts reached</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: If all attempts failed, record failure metrics and return (False, observed_value, max_attempts)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Use performance_monitor.record_operation() to track timing and attempt counts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Handle potential exceptions in update_function gracefully</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> protect_and_verify</span><span style=\"color:#E1E4E8\">(atomic_ref):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Safely load and protect a pointer using hazard pointers.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Implements the protect-then-verify pattern to avoid races.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        atomic_ref: AtomicReference containing pointer to protect</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Protected pointer value, or None if pointer became invalid</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get hazard pointer slot for current thread</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Loop with retry limit (protect-verify can fail if pointer changes rapidly)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Load current pointer value from atomic_ref</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Store loaded pointer in hazard pointer slot (announce protection)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Memory fence to ensure protection is visible before verification</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Re-load pointer from atomic_ref to verify it hasn't changed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: If pointer values match, protection succeeded - return protected pointer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: If values don't match, clear hazard slot and retry from step 3</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: If retry limit exceeded, clear hazard slot and return None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: The memory fence between protection and verification is critical</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Clear hazard pointer on both success and failure paths to avoid leaks</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<p><strong>Invariant Checking Integration:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StackInvariantChecker</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">InvariantChecker</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validates Treiber stack specific invariants.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _validate_structure_specific_invariants</span><span style=\"color:#E1E4E8\">(self, stack) -> Tuple[</span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Check that stack maintains proper linked structure without cycles.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            stack: TreiberStack instance to validate</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            (is_valid, violation_description) tuple</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load top pointer atomically from stack</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If top is None (empty stack), return (True, None)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Initialize visited set to detect cycles</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Traverse chain starting from top pointer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: For each node, check if already visited (cycle detection)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Add current node to visited set</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Validate node structure (check next field is valid AtomicReference)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Load next pointer and continue traversal</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: If traversal completes without cycles, return (True, None)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: If cycle detected or invalid structure found, return (False, description)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use weak references or addresses for cycle detection to avoid affecting GC</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Validate memory addresses are in reasonable ranges</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> QueueInvariantChecker</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">InvariantChecker</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validates Michael-Scott queue specific invariants.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _validate_structure_specific_invariants</span><span style=\"color:#E1E4E8\">(self, queue) -> Tuple[</span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Check queue maintains proper head/tail relationship and dummy node.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            queue: MichaelScottQueue instance to validate</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            (is_valid, violation_description) tuple</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load head and tail pointers atomically</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Verify head is not None (should always have dummy node)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check that tail is reachable from head by following next pointers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Validate that dummy sentinel node exists and head points to it</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Count distance from head to tail - should be reasonable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Check that no cycles exist in the chain</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Verify tail points to actual end node (next pointer is None)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: If queue appears empty, ensure head and tail point to same dummy node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Return (True, None) if all invariants hold</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Return (False, specific_violation) if any check fails</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Distance check helps detect infinite loops in traversal</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Empty queue should have head == tail pointing to dummy node</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing the error handling and recovery mechanisms:</p>\n<p><strong>Validation Commands:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run stress tests with error injection</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/stress_tests.py::test_error_recovery_under_load</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test invariant checking with concurrent operations</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/invariant_tests.py::test_concurrent_invariant_validation</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Verify backoff strategies reduce contention</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> scripts/benchmark_backoff_effectiveness.py</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test memory reclamation recovery scenarios</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/memory_safety_tests.py::test_hazard_pointer_recovery</span><span style=\"color:#79B8FF\"> -v</span></span></code></pre></div>\n\n<p><strong>Expected Behavior:</strong></p>\n<ul>\n<li>Stress tests should complete without hanging or crashing, even with high error injection rates</li>\n<li>Invariant violations should be detected and reported with specific failure descriptions</li>\n<li>Backoff strategies should show reduced CAS attempt counts under high contention</li>\n<li>Memory safety tests should demonstrate proper cleanup after simulated thread failures</li>\n</ul>\n<p><strong>Signs of Problems:</strong></p>\n<ul>\n<li>Tests that hang indefinitely indicate livelock or infinite retry loops</li>\n<li>Segmentation faults suggest memory reclamation race conditions</li>\n<li>High CPU usage with low throughput indicates excessive backoff or contention</li>\n<li>Memory usage growth during tests suggests retirement list leaks</li>\n</ul>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis Method</th>\n<th>Fix Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Operations hang under load</td>\n<td>Livelock in CAS retry loops</td>\n<td>Monitor CAS attempt counts and success rates</td>\n<td>Implement exponential backoff with jitter</td>\n</tr>\n<tr>\n<td>Intermittent crashes</td>\n<td>Use-after-free in memory reclamation</td>\n<td>Run with address sanitizer, check hazard pointer coverage</td>\n<td>Audit protect/release call pairing</td>\n</tr>\n<tr>\n<td>Performance degrades over time</td>\n<td>Retirement list growth or contention hotspots</td>\n<td>Track memory usage and operation latency trends</td>\n<td>Tune scan thresholds, analyze access patterns</td>\n</tr>\n<tr>\n<td>Invariant violations reported</td>\n<td>Race conditions or ABA problems</td>\n<td>Enable detailed logging around violation points</td>\n<td>Add tagged pointers or strengthen memory ordering</td>\n</tr>\n<tr>\n<td>High CPU usage, low throughput</td>\n<td>Excessive spinning in retry loops</td>\n<td>Profile CPU usage and identify hot loops</td>\n<td>Add backoff, reduce retry limits</td>\n</tr>\n</tbody></table>\n<p>The error handling and recovery systems form the foundation for building robust lock-free data structures that can handle the unique failure modes of non-blocking concurrent programming while maintaining both correctness and performance under adverse conditions.</p>\n<h2 id=\"testing-strategy\">Testing Strategy</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (this section establishes comprehensive verification approaches for concurrent correctness, linearizability validation, and stress testing strategies that apply to every lock-free data structure from atomic operations through complete hash maps)</p>\n</blockquote>\n<p>Testing lock-free data structures presents unique challenges that distinguish it from testing traditional sequential or lock-based concurrent code. The fundamental difficulty lies in the non-deterministic nature of concurrent execution combined with the subtle correctness properties that lock-free algorithms must satisfy. Unlike sequential code where a test either passes or fails deterministically, concurrent tests may pass thousands of times before revealing a race condition on the thousand-and-first execution.</p>\n<h3 id=\"mental-model-testing-as-archaeological-investigation\">Mental Model: Testing as Archaeological Investigation</h3>\n<p>Think of testing lock-free data structures like conducting an archaeological investigation at a busy construction site. Traditional sequential testing is like examining artifacts in a controlled laboratory environment - you can manipulate conditions precisely and reproduce results exactly. Lock-free testing, however, is like trying to study ancient foundations while construction workers operate heavy machinery overhead. You must:</p>\n<ul>\n<li><strong>Document fleeting evidence</strong>: Capture traces of concurrent operations that happen too quickly to observe directly, just as archaeologists photograph artifacts before they&#39;re disturbed</li>\n<li><strong>Reconstruct sequences from fragments</strong>: Piece together the order of operations from partial evidence, similar to reconstructing historical events from scattered artifacts  </li>\n<li><strong>Verify structural integrity under stress</strong>: Ensure the foundation remains sound even when subjected to the chaos of concurrent &quot;construction work&quot; by multiple threads</li>\n<li><strong>Detect subtle corruption</strong>: Identify when the &quot;site&quot; has been contaminated by improper excavation techniques (race conditions) that leave no obvious visible damage</li>\n</ul>\n<p>This mental model emphasizes that lock-free testing requires patience, systematic documentation, and the ability to infer correctness from indirect evidence rather than direct observation.</p>\n<h3 id=\"correctness-properties-to-verify\">Correctness Properties to Verify</h3>\n<p>Lock-free data structures must satisfy multiple layers of correctness properties, each building upon the previous layer. Understanding these properties is crucial because a data structure might appear to work correctly under casual testing while violating fundamental correctness guarantees that only emerge under specific concurrent conditions.</p>\n<h4 id=\"linearizability-the-gold-standard\">Linearizability: The Gold Standard</h4>\n<p><strong>Linearizability</strong> represents the strongest practical correctness condition for concurrent data structures. It requires that every operation appears to take effect atomically at some point between its invocation and response, called the <strong>linearization point</strong>. This property ensures that concurrent operations can be understood as if they executed sequentially in some order that respects the real-time ordering of non-overlapping operations.</p>\n<blockquote>\n<p><strong>Critical Insight</strong>: Linearizability is not just about operations being atomic - it&#39;s about the existence of a consistent sequential history that all threads would agree upon if they could observe the system from outside.</p>\n</blockquote>\n<p>The verification process involves several sophisticated techniques:</p>\n<table>\n<thead>\n<tr>\n<th>Verification Technique</th>\n<th>Approach</th>\n<th>Strengths</th>\n<th>Limitations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>History-based checking</td>\n<td>Record all operation invocations and responses, then verify a valid sequential ordering exists</td>\n<td>Mathematically rigorous, catches subtle violations</td>\n<td>Requires significant overhead, may miss timing-dependent issues</td>\n</tr>\n<tr>\n<td>State-based checking</td>\n<td>Verify data structure invariants hold at specific observation points</td>\n<td>Lightweight, good for continuous monitoring</td>\n<td>May miss transient violations, depends on choosing good observation points</td>\n</tr>\n<tr>\n<td>Execution replay</td>\n<td>Record non-deterministic choices and replay executions deterministically</td>\n<td>Enables debugging of specific failure scenarios</td>\n<td>Complex to implement, may not cover all possible interleavings</td>\n</tr>\n<tr>\n<td>Model checking</td>\n<td>Exhaustively explore all possible concurrent executions within bounded parameters</td>\n<td>Finds bugs that stress testing might miss</td>\n<td>Limited scalability, requires abstract models</td>\n</tr>\n</tbody></table>\n<p>Our <code>LinearizabilityChecker</code> component implements history-based verification by maintaining a concurrent log of all operations and periodically verifying that a valid sequential ordering exists:</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>record_operation</code></td>\n<td><code>operation: Operation</code></td>\n<td><code>None</code></td>\n<td>Thread-safely logs an operation invocation or response</td>\n</tr>\n<tr>\n<td><code>verify_history</code></td>\n<td><code>start_time: float, end_time: float</code></td>\n<td><code>bool, Optional[str]</code></td>\n<td>Checks if operations in time window have valid sequential ordering</td>\n</tr>\n<tr>\n<td><code>add_linearization_point</code></td>\n<td><code>operation_id: str, timestamp: float</code></td>\n<td><code>None</code></td>\n<td>Manually marks when an operation takes effect (for debugging)</td>\n</tr>\n<tr>\n<td><code>check_real_time_ordering</code></td>\n<td><code>operations: List[Operation]</code></td>\n<td><code>bool, List[str]</code></td>\n<td>Verifies non-overlapping operations respect real-time order</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: History-Based Linearizability Checking</strong></p>\n<ul>\n<li><strong>Context</strong>: Multiple approaches exist for verifying linearizability, each with different trade-offs between accuracy, performance, and implementation complexity</li>\n<li><strong>Options Considered</strong>: Real-time state checking, execution replay with deterministic scheduling, history-based post-execution analysis</li>\n<li><strong>Decision</strong>: Implement history-based checking with optional real-time monitoring hooks</li>\n<li><strong>Rationale</strong>: History-based checking provides the most rigorous verification while allowing performance-critical code paths to run at full speed. Optional hooks enable debugging specific scenarios without always paying the overhead cost</li>\n<li><strong>Consequences</strong>: Requires careful timestamp management and significant memory overhead for operation logs, but provides the strongest correctness guarantees</li>\n</ul>\n</blockquote>\n<h4 id=\"progress-guarantees-ensuring-forward-motion\">Progress Guarantees: Ensuring Forward Motion</h4>\n<p>Lock-free data structures must guarantee that at least one thread makes progress within a finite number of steps, even when facing arbitrary interference from other threads. This is stronger than merely avoiding deadlock - it requires active forward progress.</p>\n<p>The verification of progress guarantees involves monitoring several key metrics:</p>\n<table>\n<thead>\n<tr>\n<th>Progress Metric</th>\n<th>Measurement Approach</th>\n<th>Warning Threshold</th>\n<th>Critical Threshold</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Global progress rate</td>\n<td>Operations completed per second across all threads</td>\n<td>&lt; 50% of baseline</td>\n<td>&lt; 10% of baseline</td>\n</tr>\n<tr>\n<td>Starvation detection</td>\n<td>Maximum time any thread waits without completing operation</td>\n<td>&gt; 100x average</td>\n<td>&gt; 1000x average</td>\n</tr>\n<tr>\n<td>CAS success rate</td>\n<td>Successful CAS operations / total CAS attempts</td>\n<td>&lt; 30% under high contention</td>\n<td>&lt; 5% under any contention</td>\n</tr>\n<tr>\n<td>Helping effectiveness</td>\n<td>Operations completed via helping / total helped operations</td>\n<td>&lt; 80% when helping is active</td>\n<td>&lt; 50% when helping is active</td>\n</tr>\n</tbody></table>\n<p>Our progress monitoring integrates with the performance monitoring system to detect both gradual degradation and sudden progress failures:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Progress Guarantee Verification Algorithm:\n1. Initialize per-thread operation counters and timestamps\n2. Start background monitor thread that samples counters every 100ms\n3. For each sampling period, calculate:\n   - Global throughput (total operations / elapsed time)\n   - Per-thread progress rate (operations since last sample / sample interval)\n   - Maximum starvation time (longest time since any thread completed operation)\n4. If any metric exceeds warning threshold for 3 consecutive samples, log warning\n5. If any metric exceeds critical threshold for 1 sample, trigger failure report\n6. Maintain sliding window of historical data for trend analysis</code></pre></div>\n\n<h4 id=\"memory-safety-preventing-use-after-free\">Memory Safety: Preventing Use-After-Free</h4>\n<p>Memory safety in lock-free data structures primarily concerns preventing <strong>use-after-free</strong> errors where one thread deallocates a node while another thread still holds a pointer to it. Unlike garbage-collected languages, manual memory management creates a fundamental tension between performance (reclaiming memory quickly) and safety (ensuring no dangling pointers).</p>\n<p>The verification strategy combines several complementary approaches:</p>\n<table>\n<thead>\n<tr>\n<th>Safety Verification Technique</th>\n<th>Detection Method</th>\n<th>Coverage</th>\n<th>Performance Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Hazard pointer validation</td>\n<td>Check all protected pointers reference valid memory</td>\n<td>All hazard-protected accesses</td>\n<td>Medium - requires global scanning</td>\n</tr>\n<tr>\n<td>Address sanitizer integration</td>\n<td>Detect use-after-free at memory access level</td>\n<td>All memory accesses</td>\n<td>High - significant runtime overhead</td>\n</tr>\n<tr>\n<td>Reference counting validation</td>\n<td>Verify reference counts match actual pointer usage</td>\n<td>All shared objects</td>\n<td>Low - simple integer checks</td>\n</tr>\n<tr>\n<td>Retirement list monitoring</td>\n<td>Ensure retired nodes aren&#39;t accessed before reclamation</td>\n<td>All memory reclamation</td>\n<td>Low - timestamp comparison</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight</strong>: Memory safety verification must be comprehensive because even a single use-after-free error can cause data corruption, crashes, or security vulnerabilities. The investment in thorough safety checking pays dividends in production reliability.</p>\n</blockquote>\n<h4 id=\"functional-correctness-data-structure-semantics\">Functional Correctness: Data Structure Semantics</h4>\n<p>Beyond the generic properties of linearizability and progress guarantees, each data structure must satisfy its specific functional requirements. These properties define what the data structure is supposed to do, not just how safely it does it.</p>\n<p><strong>Stack Functional Properties:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Property</th>\n<th>Verification Method</th>\n<th>Test Cases</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>LIFO ordering</td>\n<td>Push sequence [A,B,C], verify pop sequence [C,B,A]</td>\n<td>Single-threaded sequential, concurrent interleaved</td>\n</tr>\n<tr>\n<td>Empty stack behavior</td>\n<td>Pop from empty stack returns None/null consistently</td>\n<td>Concurrent pops on empty stack, pop after all items removed</td>\n</tr>\n<tr>\n<td>Push-pop symmetry</td>\n<td>Every pushed item eventually poppable unless stack destroyed</td>\n<td>Balanced push/pop operations, stress test with random operations</td>\n</tr>\n<tr>\n<td>Count consistency</td>\n<td>Number of successful pushes minus successful pops equals current size</td>\n<td>Concurrent operations with final size verification</td>\n</tr>\n</tbody></table>\n<p><strong>Queue Functional Properties:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Property</th>\n<th>Verification Method</th>\n<th>Test Cases</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>FIFO ordering</td>\n<td>Enqueue sequence [A,B,C], verify dequeue sequence [A,B,C]</td>\n<td>Single-threaded sequential, concurrent interleaved</td>\n</tr>\n<tr>\n<td>Empty queue behavior</td>\n<td>Dequeue from empty queue returns None/null consistently</td>\n<td>Concurrent dequeues on empty queue, dequeue after all items removed</td>\n</tr>\n<tr>\n<td>Enqueue-dequeue symmetry</td>\n<td>Every enqueued item eventually dequeueable unless queue destroyed</td>\n<td>Balanced enqueue/dequeue operations, producer-consumer scenarios</td>\n</tr>\n<tr>\n<td>Fairness under contention</td>\n<td>No thread permanently starved from successful operations</td>\n<td>Multiple producers and consumers with operation counting</td>\n</tr>\n</tbody></table>\n<p><strong>Hash Map Functional Properties:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Property</th>\n<th>Verification Method</th>\n<th>Test Cases</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Key-value association</td>\n<td>Insert (K,V), verify lookup(K) returns V</td>\n<td>Single key operations, multiple keys, key updates</td>\n</tr>\n<tr>\n<td>Deletion completeness</td>\n<td>After delete(K), lookup(K) returns None/null</td>\n<td>Delete followed by lookup, delete non-existent keys</td>\n</tr>\n<tr>\n<td>Resize transparency</td>\n<td>Operations work identically before and during resize</td>\n<td>Concurrent operations during triggered resize</td>\n</tr>\n<tr>\n<td>Load factor maintenance</td>\n<td>Resize triggered when load factor exceeds threshold</td>\n<td>Monitor load factor during heavy insertion</td>\n</tr>\n</tbody></table>\n<h3 id=\"concurrency-testing-techniques\">Concurrency Testing Techniques</h3>\n<p>Testing concurrent correctness requires specialized techniques that go far beyond traditional unit testing. The challenge lies in exploring the vast space of possible thread interleavings while maintaining reasonable test execution times and diagnostic capabilities.</p>\n<h4 id=\"stress-testing-controlled-chaos\">Stress Testing: Controlled Chaos</h4>\n<p><strong>Stress testing</strong> subjects the data structure to high levels of concurrent contention to expose race conditions and performance bottlenecks that only emerge under extreme load. The key principle is controlled chaos - generating enough concurrent activity to stress the system while maintaining enough control to diagnose problems when they occur.</p>\n<p>Our stress testing framework implements several complementary strategies:</p>\n<table>\n<thead>\n<tr>\n<th>Stress Test Type</th>\n<th>Thread Configuration</th>\n<th>Operation Pattern</th>\n<th>Duration Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>High contention</td>\n<td>2x CPU cores</td>\n<td>All threads target same hotspots</td>\n<td>Fixed time window (30-300 seconds)</td>\n</tr>\n<tr>\n<td>Mixed workload</td>\n<td>50% readers, 50% writers</td>\n<td>Realistic operation mix</td>\n<td>Operation count target (1M operations)</td>\n</tr>\n<tr>\n<td>Burst contention</td>\n<td>Periodic thread spawning</td>\n<td>Waves of activity followed by quiet periods</td>\n<td>Multiple burst cycles</td>\n</tr>\n<tr>\n<td>Memory pressure</td>\n<td>Background memory allocation</td>\n<td>Operations under low memory conditions</td>\n<td>Until memory exhaustion or timeout</td>\n</tr>\n</tbody></table>\n<p>The stress test implementation follows this algorithm:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Stress Test Execution Algorithm:\n1. Initialize shared data structure and monitoring infrastructure\n2. Create thread pool with configured number of worker threads\n3. For each worker thread:\n   a. Assign operation type distribution (read/write ratios)\n   b. Initialize per-thread random number generator with unique seed\n   c. Start thread executing operation loop with configured patterns\n4. Start monitoring thread to collect performance metrics\n5. Run for configured duration or operation count\n6. Signal all threads to complete current operations and terminate\n7. Join all threads and collect final statistics\n8. Analyze results for correctness violations and performance anomalies\n9. Generate detailed report with operation counts, timing histograms, and failure analysis</code></pre></div>\n\n<blockquote>\n<p><strong>Decision: Multi-Phase Stress Testing</strong></p>\n<ul>\n<li><strong>Context</strong>: Single-pattern stress tests often miss bugs that only occur during specific transition scenarios or under particular load characteristics</li>\n<li><strong>Options Considered</strong>: Continuous uniform load, random operation patterns, scenario-based testing with specific sequences</li>\n<li><strong>Decision</strong>: Implement multi-phase testing that cycles through different load patterns and operation mixes within a single test run</li>\n<li><strong>Rationale</strong>: Different bugs manifest under different conditions - contention bugs need high thread counts, memory races need allocation pressure, progress violations need specific timing. Multi-phase testing maximizes coverage within reasonable test execution time</li>\n<li><strong>Consequences</strong>: More complex test infrastructure and result analysis, but significantly higher bug detection rate and better coverage of real-world usage patterns</li>\n</ul>\n</blockquote>\n<h4 id=\"model-checking-exhaustive-exploration\">Model Checking: Exhaustive Exploration</h4>\n<p><strong>Model checking</strong> provides a mathematically rigorous approach to concurrent correctness by exhaustively exploring all possible thread interleavings within bounded parameters. While computationally expensive, model checking can find subtle bugs that might never occur during stress testing.</p>\n<p>Our model checking integration focuses on bounded verification of critical scenarios:</p>\n<table>\n<thead>\n<tr>\n<th>Model Checking Scope</th>\n<th>Thread Limit</th>\n<th>Operation Limit</th>\n<th>State Space Size</th>\n<th>Typical Runtime</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Core algorithm verification</td>\n<td>3 threads</td>\n<td>5 operations each</td>\n<td>~10^6 states</td>\n<td>1-10 minutes</td>\n</tr>\n<tr>\n<td>Edge case exploration</td>\n<td>2 threads</td>\n<td>10 operations each</td>\n<td>~10^7 states</td>\n<td>10-60 minutes</td>\n</tr>\n<tr>\n<td>Regression testing</td>\n<td>4 threads</td>\n<td>3 operations each</td>\n<td>~10^5 states</td>\n<td>1-5 minutes</td>\n</tr>\n<tr>\n<td>Deep bug investigation</td>\n<td>2 threads</td>\n<td>20 operations each</td>\n<td>~10^8 states</td>\n<td>1-8 hours</td>\n</tr>\n</tbody></table>\n<p>The model checker implementation abstracts the data structure operations into a finite state machine and systematically explores all possible execution orders:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Model Checking Algorithm:\n1. Define abstract state space representing data structure and thread states\n2. Identify atomic transitions corresponding to each operation step\n3. Initialize worklist with initial state\n4. While worklist not empty:\n   a. Dequeue next state to explore\n   b. For each possible transition from current state:\n      i. Compute successor state after transition\n      ii. Check if successor violates any correctness properties\n      iii. If violation found, generate counterexample trace\n      iv. If successor is new, add to worklist for further exploration\n   c. Mark current state as fully explored\n5. If worklist empty without violations, algorithm is correct within bounds\n6. Generate coverage report showing percentage of reachable states explored</code></pre></div>\n\n<h4 id=\"randomized-testing-property-based-verification\">Randomized Testing: Property-Based Verification</h4>\n<p><strong>Randomized testing</strong> generates large numbers of random operation sequences and verifies that correctness properties hold for each sequence. This approach complements stress testing by exploring diverse scenarios rather than just high-contention cases.</p>\n<p>Our randomized testing framework implements property-based verification:</p>\n<table>\n<thead>\n<tr>\n<th>Property Category</th>\n<th>Verification Method</th>\n<th>Sample Properties</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Functional properties</td>\n<td>Operation sequence modeling</td>\n<td>&quot;Every pushed item is eventually poppable&quot;, &quot;FIFO order preserved&quot;</td>\n</tr>\n<tr>\n<td>Safety properties</td>\n<td>State invariant checking</td>\n<td>&quot;No null pointer dereferences&quot;, &quot;All nodes reachable from valid pointers&quot;</td>\n</tr>\n<tr>\n<td>Liveness properties</td>\n<td>Progress monitoring</td>\n<td>&quot;Operations eventually complete&quot;, &quot;No permanent starvation&quot;</td>\n</tr>\n<tr>\n<td>Performance properties</td>\n<td>Timing analysis</td>\n<td>&quot;Average operation latency &lt; threshold&quot;, &quot;Throughput scales with threads&quot;</td>\n</tr>\n</tbody></table>\n<p>The randomized test generator creates operation sequences using configurable probability distributions:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Randomized Test Generation Algorithm:\n1. Initialize random number generator with configurable seed for reproducibility\n2. Generate operation sequence of specified length using probability distributions:\n   - Operation type (push/pop for stack, insert/lookup/delete for hash map)\n   - Operation timing (immediate, delayed, synchronized)\n   - Data values (unique, duplicate, special values like null)\n3. Execute operation sequence across multiple threads with controlled interleavings\n4. After each operation, verify all applicable properties hold\n5. If property violation detected:\n   a. Record minimal reproduction case by binary search on operation sequence\n   b. Generate detailed failure report with operation history and final state\n6. Repeat with different random seeds until confidence threshold reached\n7. Analyze aggregate results for patterns in failures or performance degradation</code></pre></div>\n\n<h3 id=\"milestone-verification-checkpoints\">Milestone Verification Checkpoints</h3>\n<p>Each milestone in the lock-free data structures project requires specific verification approaches tailored to the components being implemented. These checkpoints provide concrete acceptance criteria and debugging guidance for each development phase.</p>\n<h4 id=\"milestone-1-atomic-operations-foundation\">Milestone 1: Atomic Operations Foundation</h4>\n<p>The atomic operations milestone establishes the fundamental building blocks for all subsequent lock-free algorithms. Verification focuses on correctness of individual atomic primitives and proper memory ordering semantics.</p>\n<p><strong>Critical Verification Areas:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Verification Focus</th>\n<th>Expected Behavior</th>\n<th>Failure Indicators</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Compare-and-swap wrapper</td>\n<td>Return value correctness, memory ordering</td>\n<td>CAS returns (True, old_value) on success, (False, current_value) on failure</td>\n<td>Incorrect return values, lost updates, unexpected values</td>\n</tr>\n<tr>\n<td>Memory ordering modes</td>\n<td>Proper compiler/CPU barrier generation</td>\n<td>Operations respect acquire/release semantics, no reordering violations</td>\n<td>Reordering visible to other threads, stale reads after barriers</td>\n</tr>\n<tr>\n<td>ABA problem demonstration</td>\n<td>Reliable reproduction of ABA scenario</td>\n<td>Test shows incorrect CAS success due to pointer reuse</td>\n<td>Test passes when it should fail, inconsistent reproduction</td>\n</tr>\n<tr>\n<td>Atomic counter operations</td>\n<td>Linearizable increment/decrement</td>\n<td>Counter value equals expected after all operations complete</td>\n<td>Lost increments, counter value inconsistencies</td>\n</tr>\n</tbody></table>\n<p><strong>Acceptance Test Procedures:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Atomic Operations Verification Steps:\n1. Single-threaded correctness testing:\n   a. Verify CAS wrapper returns correct success/failure indicators\n   b. Test all memory ordering modes compile and execute without errors\n   c. Confirm atomic counter operations produce expected sequential results\n   d. Validate load/store operations respect memory ordering constraints\n\n2. Multi-threaded stress testing:\n   a. Launch N threads each performing 1000 CAS operations on shared variable\n   b. Verify final variable value matches expected result from sequential execution\n   c. Confirm no thread observes intermediate inconsistent states\n   d. Test counter incremented by multiple threads produces exact expected total\n\n3. ABA problem reproduction:\n   a. Create scenario with pointer reuse between CAS read and update\n   b. Demonstrate that naive CAS incorrectly succeeds\n   c. Show that tagged pointer solution correctly detects and prevents ABA\n   d. Verify test fails reliably without ABA protection, passes reliably with protection\n\n4. Memory ordering validation:\n   a. Create test with release store followed by acquire load on different threads\n   b. Verify acquire thread observes all writes that happened-before release\n   c. Test relaxed ordering allows observable reordering where appropriate\n   d. Confirm sequential consistency mode prevents all observable reordering</code></pre></div>\n\n<blockquote>\n<p><strong>Common Checkpoint Pitfall</strong>: Many implementations pass single-threaded tests but fail under concurrent stress. Always include multi-threaded verification even for atomic primitives.</p>\n</blockquote>\n<h4 id=\"milestone-2-lock-free-stack-implementation\">Milestone 2: Lock-free Stack Implementation</h4>\n<p>The Treiber stack milestone implements the first complete lock-free data structure, requiring verification of both functional correctness and linearizability properties.</p>\n<p><strong>Critical Verification Areas:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Verification Focus</th>\n<th>Expected Behavior</th>\n<th>Failure Indicators</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Push operation</td>\n<td>CAS loop correctness, ABA handling</td>\n<td>Items pushed in any order are all eventually poppable</td>\n<td>Push operations hang, items disappear, memory corruption</td>\n</tr>\n<tr>\n<td>Pop operation</td>\n<td>Empty stack handling, linearization point</td>\n<td>Pop returns items in LIFO order, handles empty gracefully</td>\n<td>Pop from empty crashes, wrong order, null pointer access</td>\n</tr>\n<tr>\n<td>Tagged pointer solution</td>\n<td>ABA prevention effectiveness</td>\n<td>CAS fails appropriately when pointer reused with different tag</td>\n<td>Incorrect CAS success, data structure corruption, inconsistent state</td>\n</tr>\n<tr>\n<td>Concurrent correctness</td>\n<td>Linearizability under contention</td>\n<td>All push/pop operations appear atomic at some linearization point</td>\n<td>Lost operations, duplicate returns, non-atomic behavior</td>\n</tr>\n</tbody></table>\n<p><strong>Acceptance Test Procedures:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Stack Verification Steps:\n1. Sequential functionality testing:\n   a. Push items [1,2,3], verify pop sequence returns [3,2,1]\n   b. Push and pop interleaved, verify LIFO order maintained\n   c. Pop from empty stack returns None/null without crashing\n   d. Push after empty, verify stack recovers normal operation\n\n2. Concurrent stress testing:\n   a. Multiple threads pushing unique values simultaneously\n   b. Multiple threads popping while others push\n   c. Balanced push/pop workload with final size verification\n   d. Imbalanced workload testing stack growth and shrinkage\n\n3. ABA problem prevention:\n   a. Create test scenario that triggers ABA without protection\n   b. Verify tagged pointer solution prevents ABA corruption\n   c. Test under high contention where ABA most likely to occur\n   d. Validate stack maintains structural integrity throughout test\n\n4. Linearizability verification:\n   a. Record all push/pop operation invocations and responses\n   b. Verify existence of valid sequential history respecting LIFO order\n   c. Test with multiple concurrent threads and random operation timing\n   d. Analyze operation history for linearization point consistency</code></pre></div>\n\n<p><strong>Performance Benchmark Requirements:</strong></p>\n<ul>\n<li>Stack operations should complete within 100ns on average under low contention</li>\n<li>Throughput should scale linearly up to number of CPU cores</li>\n<li>Memory usage should grow/shrink appropriately with stack size</li>\n<li>No memory leaks after balanced push/pop operations</li>\n</ul>\n<h4 id=\"milestone-3-michael-scott-queue-implementation\">Milestone 3: Michael-Scott Queue Implementation</h4>\n<p>The queue milestone introduces more complex coordination with dual head/tail pointers and helping mechanisms, requiring sophisticated verification of FIFO ordering and progress guarantees.</p>\n<p><strong>Critical Verification Areas:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Verification Focus</th>\n<th>Expected Behavior</th>\n<th>Failure Indicators</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Enqueue operation</td>\n<td>Tail advancement, helping protocol</td>\n<td>Items enqueued appear in FIFO order when dequeued</td>\n<td>Enqueue operations hang, incorrect ordering, tail pointer corruption</td>\n</tr>\n<tr>\n<td>Dequeue operation</td>\n<td>Head advancement, empty queue handling</td>\n<td>Dequeue returns oldest item, handles empty queue gracefully</td>\n<td>Dequeue hangs on empty, wrong items returned, head pointer issues</td>\n</tr>\n<tr>\n<td>Helping mechanism</td>\n<td>Progress guarantee under interference</td>\n<td>Threads help advance lagging tail pointer when detected</td>\n<td>Operations permanently blocked, progress violations, helping ineffective</td>\n</tr>\n<tr>\n<td>Sentinel node management</td>\n<td>Dummy node lifecycle</td>\n<td>Dummy node simplifies empty queue, never deallocated inappropriately</td>\n<td>Dummy node corruption, improper deallocation, empty queue crashes</td>\n</tr>\n</tbody></table>\n<p><strong>Acceptance Test Procedures:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Queue Verification Steps:\n1. Sequential FIFO testing:\n   a. Enqueue items [1,2,3], verify dequeue sequence returns [1,2,3]\n   b. Interleaved enqueue/dequeue operations maintain FIFO order\n   c. Dequeue from empty queue returns None/null consistently\n   d. Enqueue after empty restores normal FIFO operation\n\n2. Producer-consumer stress testing:\n   a. Multiple producer threads enqueuing unique timestamped items\n   b. Multiple consumer threads dequeuing and verifying FIFO order\n   c. Balanced producer/consumer rates with steady-state verification\n   d. Imbalanced rates testing queue growth under producer pressure\n\n3. Helping mechanism verification:\n   a. Create scenario where tail pointer lags behind actual tail\n   b. Verify dequeue operations detect and correct lagging tail\n   c. Test helping effectiveness under high contention conditions\n   d. Ensure helping doesn't interfere with normal operation progress\n\n4. Empty queue edge case testing:\n   a. Multiple threads dequeue from empty queue simultaneously\n   b. Enqueue while other threads blocked on empty dequeue\n   c. Rapid empty/non-empty transitions with concurrent operations\n   d. Verify dummy sentinel node remains stable throughout transitions</code></pre></div>\n\n<h4 id=\"milestone-4-hazard-pointers-integration\">Milestone 4: Hazard Pointers Integration</h4>\n<p>The hazard pointers milestone focuses on memory safety verification, ensuring that the reclamation scheme prevents use-after-free errors while maintaining performance and progress guarantees.</p>\n<p><strong>Critical Verification Areas:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Verification Focus</th>\n<th>Expected Behavior</th>\n<th>Failure Indicators</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Protection protocol</td>\n<td>Hazard pointer correctness</td>\n<td>Nodes protected before access, released after completion</td>\n<td>Use-after-free errors, protection race conditions, dangling pointers</td>\n</tr>\n<tr>\n<td>Retirement list management</td>\n<td>Safe reclamation timing</td>\n<td>Nodes retired safely, reclaimed when no longer protected</td>\n<td>Memory leaks, premature reclamation, unbounded retirement growth</td>\n</tr>\n<tr>\n<td>Scanning algorithm</td>\n<td>Global hazard survey</td>\n<td>Scan identifies all protected nodes correctly</td>\n<td>False reclamation, missed protections, scan algorithm errors</td>\n</tr>\n<tr>\n<td>Integration correctness</td>\n<td>Data structure operation safety</td>\n<td>Stack/queue operations remain correct with hazard pointer protection</td>\n<td>Functional regressions, protection overhead breaks algorithms</td>\n</tr>\n</tbody></table>\n<p><strong>Acceptance Test Procedures:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Hazard Pointers Verification Steps:\n1. Memory safety validation:\n   a. Use address sanitizer to detect use-after-free errors\n   b. Stress test with high allocation/deallocation rates\n   c. Verify no dangling pointer access under any operation sequence\n   d. Test protection protocol under interrupt and preemption scenarios\n\n2. Retirement and reclamation testing:\n   a. Monitor retirement list size under sustained operation load\n   b. Verify scan threshold triggers reclamation appropriately\n   c. Test reclamation completeness - all unprotected nodes eventually freed\n   d. Validate no memory leaks after extended operation sequences\n\n3. Integration verification:\n   a. Re-run all stack and queue tests with hazard pointer protection enabled\n   b. Verify functional correctness remains identical\n   c. Measure performance overhead of hazard pointer operations\n   d. Test thread creation/destruction with proper cleanup\n\n4. Concurrency stress testing:\n   a. High contention scenario with frequent protect/release cycles\n   b. Mixed read/write workload with varying protection patterns\n   c. Thread lifecycle testing with proper hazard pointer cleanup\n   d. Recovery testing after abnormal thread termination</code></pre></div>\n\n<h4 id=\"milestone-5-lock-free-hash-map-completion\">Milestone 5: Lock-free Hash Map Completion</h4>\n<p>The hash map milestone represents the most complex data structure, requiring verification of correct hash distribution, split-ordered list maintenance, and incremental resizing under concurrent operations.</p>\n<p><strong>Critical Verification Areas:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Verification Focus</th>\n<th>Expected Behavior</th>\n<th>Failure Indicators</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Bucket operations</td>\n<td>Insert/lookup/delete correctness</td>\n<td>Key-value operations work correctly within buckets</td>\n<td>Wrong values returned, keys lost, bucket corruption</td>\n</tr>\n<tr>\n<td>Split-ordered list maintenance</td>\n<td>Logical ordering preservation</td>\n<td>Hash ordering maintained across bucket splits</td>\n<td>Incorrect key placement, search failures, list corruption</td>\n</tr>\n<tr>\n<td>Incremental resizing</td>\n<td>Transparent capacity expansion</td>\n<td>Resize occurs without blocking operations, maintains correctness</td>\n<td>Resize hangs, data loss during resize, incorrect post-resize state</td>\n</tr>\n<tr>\n<td>Load factor management</td>\n<td>Automatic resize triggering</td>\n<td>Resize triggered at appropriate load factor thresholds</td>\n<td>Resize too early/late, load calculation errors, infinite resize loops</td>\n</tr>\n</tbody></table>\n<p><strong>Acceptance Test Procedures:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Hash Map Verification Steps:\n1. Basic key-value operations:\n   a. Insert unique keys, verify all retrievable via lookup\n   b. Update existing keys, verify new values returned\n   c. Delete keys, verify lookup returns None/null afterward\n   d. Test with various key types and hash distributions\n\n2. Concurrent operations stress testing:\n   a. Multiple threads inserting unique keys simultaneously\n   b. Mixed insert/lookup/delete workload from multiple threads\n   c. Read-heavy workload with occasional writes\n   d. Write-heavy workload with occasional reads\n\n3. Incremental resize verification:\n   a. Monitor hash map through resize trigger and completion\n   b. Verify all existing key-value pairs remain accessible during resize\n   c. Test concurrent operations during resize process\n   d. Validate final state after resize completion\n\n4. Split-ordered list correctness:\n   a. Verify logical hash ordering maintained in physical list\n   b. Test bucket initialization with proper sentinel node placement\n   c. Validate reverse bit ordering calculations\n   d. Check split operation correctness under various hash distributions\n\n5. Performance and scalability testing:\n   a. Measure throughput scaling with thread count\n   b. Verify load factor maintenance within expected bounds\n   c. Test hash distribution quality with various key patterns\n   d. Benchmark against reference hash map implementations</code></pre></div>\n\n<blockquote>\n<p><strong>Integration Testing Recommendation</strong>: After completing all milestones, run a comprehensive integration test that uses all data structures together in a realistic application scenario. This often reveals interaction bugs that unit tests miss.</p>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The testing infrastructure for lock-free data structures requires sophisticated tooling to handle the unique challenges of concurrent correctness verification. This section provides complete, production-ready testing frameworks and detailed guidance for implementing each verification technique.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Testing Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Stress Testing Framework</td>\n<td>Threading module with shared counters</td>\n<td>Custom thread pool with CPU affinity and scheduling control</td>\n</tr>\n<tr>\n<td>Randomized Testing</td>\n<td>Random module with fixed seeds</td>\n<td>Property-based testing framework (Hypothesis for Python)</td>\n</tr>\n<tr>\n<td>Performance Monitoring</td>\n<td>Time measurements with statistics module</td>\n<td>High-resolution profiling with perf integration</td>\n</tr>\n<tr>\n<td>Memory Safety Validation</td>\n<td>Manual reference tracking</td>\n<td>Address Sanitizer integration with custom allocators</td>\n</tr>\n<tr>\n<td>Linearizability Checking</td>\n<td>Operation logging with post-analysis</td>\n<td>Real-time linearizability verification with efficient algorithms</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-testing-module-structure\">Recommended Testing Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  tests/\n    unit/\n      test_atomic_operations.py     ← Basic atomic primitive tests\n      test_memory_ordering.py       ← Memory ordering and barrier tests\n      test_cas_operations.py        ← Compare-and-swap correctness tests\n    integration/\n      test_stack_correctness.py     ← Treiber stack functional tests\n      test_queue_correctness.py     ← Michael-Scott queue functional tests\n      test_hashmap_correctness.py   ← Hash map functional tests\n      test_hazard_pointers.py       ← Memory reclamation tests\n    stress/\n      stress_test_framework.py      ← Generic concurrent stress testing\n      stress_stack_operations.py    ← Stack-specific stress tests\n      stress_queue_operations.py    ← Queue-specific stress tests\n      stress_hashmap_operations.py  ← Hash map-specific stress tests\n    verification/\n      linearizability_checker.py    ← History-based linearizability verification\n      progress_monitor.py           ← Progress guarantee validation\n      memory_safety_checker.py      ← Use-after-free detection\n      property_verifier.py          ← Property-based testing framework\n    benchmarks/\n      performance_benchmarks.py     ← Throughput and latency measurements\n      scalability_tests.py          ← Multi-core scaling analysis\n      contention_analysis.py        ← Lock contention vs lock-free comparison\n    utils/\n      test_harness.py              ← Common testing utilities and fixtures\n      random_generators.py         ← Deterministic random data generation\n      thread_coordination.py       ← Thread synchronization utilities for tests</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Complete Linearizability Checker Implementation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Optional, Tuple, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> bisect</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> OperationType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PUSH</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"push\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    POP</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"pop\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ENQUEUE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"enqueue\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEQUEUE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"dequeue\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INSERT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"insert\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LOOKUP</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"lookup\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DELETE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"delete\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Operation</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: OperationType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    thread_id: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    invocation: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#6A737D\">  # True for start, False for completion</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    args: Tuple[Any, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result: Any </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    operation_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> LinearizabilityChecker</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.operations: List[Operation] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.operation_lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Lock()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.operation_counter </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.verification_cache: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> record_operation</span><span style=\"color:#E1E4E8\">(self, op: Operation) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Thread-safely record an operation invocation or response.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.operation_lock:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> op.invocation:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.operation_counter </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                op.operation_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">op.thread_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">_</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.operation_counter</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Insert operation in timestamp order for efficient analysis</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            bisect.insort(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.operations, op, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\"> x: x.timestamp)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> verify_history</span><span style=\"color:#E1E4E8\">(self, start_time: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, end_time: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> Tuple[</span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if operations in time window have valid sequential ordering.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Filter operations in time window</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        relevant_ops </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [op </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> op </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.operations </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                       if</span><span style=\"color:#E1E4E8\"> start_time </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#E1E4E8\"> op.timestamp </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#E1E4E8\"> end_time]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Group operations by operation_id to pair invocations with responses</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        operation_pairs </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._pair_invocations_responses(relevant_ops)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Try to find valid sequential ordering</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._check_sequential_consistency(operation_pairs)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _pair_invocations_responses</span><span style=\"color:#E1E4E8\">(self, ops: List[Operation]) -> List[Tuple[Operation, Operation]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Match operation invocations with their responses.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pending_ops </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        completed_ops </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> op </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> ops:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> op.invocation:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                pending_ops[op.operation_id] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> op</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> op.operation_id </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> pending_ops:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    start_op </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pending_ops.pop(op.operation_id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    completed_ops.append((start_op, op))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> completed_ops</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _check_sequential_consistency</span><span style=\"color:#E1E4E8\">(self, operation_pairs: List[Tuple[Operation, Operation]]) -> Tuple[</span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Verify that a valid sequential ordering exists for the operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # This is a simplified version - full implementation would use</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # sophisticated graph algorithms to check all possible orderings</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # For demonstration, check basic FIFO/LIFO properties</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        stack_operations </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        queue_operations </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> start_op, end_op </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> operation_pairs:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> start_op.type </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> [OperationType.</span><span style=\"color:#79B8FF\">PUSH</span><span style=\"color:#E1E4E8\">, OperationType.</span><span style=\"color:#79B8FF\">POP</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                stack_operations.append((start_op, end_op))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            elif</span><span style=\"color:#E1E4E8\"> start_op.type </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> [OperationType.</span><span style=\"color:#79B8FF\">ENQUEUE</span><span style=\"color:#E1E4E8\">, OperationType.</span><span style=\"color:#79B8FF\">DEQUEUE</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                queue_operations.append((start_op, end_op))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Verify stack LIFO property</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._verify_stack_lifo(stack_operations):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Stack LIFO ordering violation detected\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Verify queue FIFO property  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._verify_queue_fifo(queue_operations):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Queue FIFO ordering violation detected\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _verify_stack_lifo</span><span style=\"color:#E1E4E8\">(self, operations: List[Tuple[Operation, Operation]]) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Verify stack operations respect LIFO ordering.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Simplified LIFO checking - track push/pop sequence</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        stack_state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Sort operations by linearization point (completion timestamp)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sorted_ops </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> sorted</span><span style=\"color:#E1E4E8\">(operations, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\"> x: x[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].timestamp)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> start_op, end_op </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> sorted_ops:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> start_op.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> OperationType.</span><span style=\"color:#79B8FF\">PUSH</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                stack_state.append(start_op.args[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">])  </span><span style=\"color:#6A737D\"># Push value onto model stack</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            elif</span><span style=\"color:#E1E4E8\"> start_op.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> OperationType.</span><span style=\"color:#79B8FF\">POP</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> stack_state:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    # Pop from empty stack should return None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    if</span><span style=\"color:#E1E4E8\"> end_op.result </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                        return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    expected_value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> stack_state.pop()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    if</span><span style=\"color:#E1E4E8\"> end_op.result </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> expected_value:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                        return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _verify_queue_fifo</span><span style=\"color:#E1E4E8\">(self, operations: List[Tuple[Operation, Operation]]) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Verify queue operations respect FIFO ordering.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Simplified FIFO checking - track enqueue/dequeue sequence</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        queue_state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Sort operations by linearization point (completion timestamp)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sorted_ops </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> sorted</span><span style=\"color:#E1E4E8\">(operations, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\"> x: x[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].timestamp)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> start_op, end_op </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> sorted_ops:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> start_op.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> OperationType.</span><span style=\"color:#79B8FF\">ENQUEUE</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                queue_state.append(start_op.args[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">])  </span><span style=\"color:#6A737D\"># Add value to model queue</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            elif</span><span style=\"color:#E1E4E8\"> start_op.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> OperationType.</span><span style=\"color:#79B8FF\">DEQUEUE</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> queue_state:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    # Dequeue from empty queue should return None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    if</span><span style=\"color:#E1E4E8\"> end_op.result </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                        return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    expected_value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> queue_state.pop(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Remove from front</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    if</span><span style=\"color:#E1E4E8\"> end_op.result </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> expected_value:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                        return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> True</span></span></code></pre></div>\n\n<p><strong>Complete Stress Testing Framework:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> random</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> statistics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Callable, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> concurrent.futures </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ThreadPoolExecutor</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StressTestConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_threads: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 8</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    operations_per_thread: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1000</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    test_duration_seconds: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    operation_mix: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#9ECBFF\">\"read\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.7</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"write\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.3</span><span style=\"color:#E1E4E8\">})</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    contention_level: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"high\"</span><span style=\"color:#6A737D\">  # \"low\", \"medium\", \"high\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    backoff_strategy: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"exponential\"</span><span style=\"color:#6A737D\">  # \"none\", \"linear\", \"exponential\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StressTestResults</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    total_operations: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    successful_operations: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    failed_operations: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    total_duration: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    throughput_ops_per_sec: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    average_latency_ms: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    p95_latency_ms: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    p99_latency_ms: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cas_success_rate: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    thread_results: List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StressTestFramework</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, data_structure: Any, config: StressTestConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.data_structure </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> data_structure</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> StressTestResults()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.operation_times: List[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cas_attempts </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cas_successes </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.results_lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Lock()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.should_stop </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Event()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> stress_test_data_structure</span><span style=\"color:#E1E4E8\">(self) -> StressTestResults:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute comprehensive stress test and return detailed results.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Starting stress test with </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.config.num_threads</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> threads...\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Start monitoring thread</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        monitor_thread </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Thread(</span><span style=\"color:#FFAB70\">target</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._monitor_progress, </span><span style=\"color:#FFAB70\">daemon</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        monitor_thread.start()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Execute stress test with thread pool</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#E1E4E8\"> ThreadPoolExecutor(</span><span style=\"color:#FFAB70\">max_workers</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.config.num_threads) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> executor:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Submit worker tasks</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            futures </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> thread_id </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.config.num_threads):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                future </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> executor.submit(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._worker_thread, thread_id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                futures.append(future)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Wait for completion or timeout</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.test_duration_seconds:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                time.sleep(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.config.test_duration_seconds)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.should_stop.set()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Collect results from all threads</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> i, future </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(futures):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    thread_result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> future.result(</span><span style=\"color:#FFAB70\">timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">30</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">.results.thread_results.append({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        \"thread_id\"</span><span style=\"color:#E1E4E8\">: i,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        \"operations\"</span><span style=\"color:#E1E4E8\">: thread_result[</span><span style=\"color:#9ECBFF\">\"operations\"</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        \"successes\"</span><span style=\"color:#E1E4E8\">: thread_result[</span><span style=\"color:#9ECBFF\">\"successes\"</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        \"failures\"</span><span style=\"color:#E1E4E8\">: thread_result[</span><span style=\"color:#9ECBFF\">\"failures\"</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        \"avg_latency\"</span><span style=\"color:#E1E4E8\">: thread_result[</span><span style=\"color:#9ECBFF\">\"avg_latency\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    })</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Thread </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> failed: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        end_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Calculate aggregate results</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._calculate_final_results(start_time, end_time)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.results</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _worker_thread</span><span style=\"color:#E1E4E8\">(self, thread_id: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute operations for a single worker thread.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        operations_completed </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        successful_operations </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        failed_operations </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        thread_operation_times </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Initialize thread-local random generator for reproducibility</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        thread_random </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> random.Random(</span><span style=\"color:#79B8FF\">42</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> thread_id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.should_stop.is_set():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Stop if reached operation limit and no duration specified</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.config.test_duration_seconds </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                operations_completed </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.operations_per_thread):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                break</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Select operation type based on configured mix</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            operation_type </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._select_operation_type(thread_random)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Execute operation with timing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.perf_counter()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            success </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._execute_operation(operation_type, thread_random, thread_id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            end_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.perf_counter()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Record results</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            operation_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (end_time </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time) </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#6A737D\">  # Convert to milliseconds</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            thread_operation_times.append(operation_time)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            operations_completed </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> success:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                successful_operations </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                failed_operations </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Apply backoff if configured</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._apply_backoff(success)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Update global results thread-safely</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.results_lock:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.results.total_operations </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> operations_completed</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.results.successful_operations </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> successful_operations</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.results.failed_operations </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> failed_operations</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.operation_times.extend(thread_operation_times)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"operations\"</span><span style=\"color:#E1E4E8\">: operations_completed,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"successes\"</span><span style=\"color:#E1E4E8\">: successful_operations,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"failures\"</span><span style=\"color:#E1E4E8\">: failed_operations,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"avg_latency\"</span><span style=\"color:#E1E4E8\">: statistics.mean(thread_operation_times) </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> thread_operation_times </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _execute_operation</span><span style=\"color:#E1E4E8\">(self, operation_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, rng: random.Random, thread_id: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute a single operation on the data structure.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> hasattr</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.data_structure, </span><span style=\"color:#9ECBFF\">'push'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> operation_type </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"push\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> rng.randint(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1000000</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> thread_id </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 1000000</span><span style=\"color:#6A737D\">  # Unique values per thread</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.data_structure.push(value)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            elif</span><span style=\"color:#79B8FF\"> hasattr</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.data_structure, </span><span style=\"color:#9ECBFF\">'pop'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> operation_type </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"pop\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                result </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.data_structure.pop()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> result </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">  # Success if we got a value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            elif</span><span style=\"color:#79B8FF\"> hasattr</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.data_structure, </span><span style=\"color:#9ECBFF\">'enqueue'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> operation_type </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"enqueue\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> rng.randint(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1000000</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> thread_id </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 1000000</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.data_structure.enqueue(value)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            elif</span><span style=\"color:#79B8FF\"> hasattr</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.data_structure, </span><span style=\"color:#9ECBFF\">'dequeue'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> operation_type </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"dequeue\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                result </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.data_structure.dequeue()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> result </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            elif</span><span style=\"color:#79B8FF\"> hasattr</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.data_structure, </span><span style=\"color:#9ECBFF\">'insert'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> operation_type </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"insert\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                key </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"key_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">rng.randint(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">10000</span><span style=\"color:#E1E4E8\">)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">thread_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                value </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"value_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">rng.randint(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">10000</span><span style=\"color:#E1E4E8\">)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.data_structure.insert(key, value)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            elif</span><span style=\"color:#79B8FF\"> hasattr</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.data_structure, </span><span style=\"color:#9ECBFF\">'lookup'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> operation_type </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"lookup\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                key </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"key_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">rng.randint(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">10000</span><span style=\"color:#E1E4E8\">)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">thread_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                result </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.data_structure.lookup(key)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#6A737D\">  # Lookup always succeeds (may return None)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Operation </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">operation_type</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> failed: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span></code></pre></div>\n\n<h4 id=\"core-testing-logic-skeletons\">Core Testing Logic Skeletons</h4>\n<p><strong>Milestone Verification Template:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> verify_milestone_1_atomic_operations</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Verify atomic operations foundation correctness and performance.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize atomic reference with test value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create multiple threads performing CAS operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Verify final value matches expected result from sequential execution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Test all memory ordering modes compile and execute correctly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Demonstrate ABA problem with naive CAS implementation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Show tagged pointer solution prevents ABA corruption</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Measure CAS success rate under high contention</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Use barriers to synchronize thread start for maximum contention</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> verify_milestone_2_treiber_stack</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Verify Treiber stack functional correctness and linearizability.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Test single-threaded LIFO ordering with push/pop sequence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Verify empty stack pop returns None without crashing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Create concurrent stress test with multiple pushers and poppers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Record all operations and verify linearizability using checker</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Test ABA prevention under high contention scenarios</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Measure stack performance vs mutex-based reference implementation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Validate memory safety with address sanitizer integration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Use unique values per thread to track operation correctness</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> verify_milestone_3_michael_scott_queue</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Verify Michael-Scott queue FIFO ordering and helping mechanism.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Test single-threaded FIFO ordering with enqueue/dequeue sequence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Verify empty queue dequeue returns None consistently</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Create producer-consumer stress test with FIFO verification</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Test helping mechanism by creating scenarios with lagging tail</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Verify dummy sentinel node remains stable through all operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Record operations and check linearizability with proper FIFO semantics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Measure queue throughput under balanced and imbalanced workloads</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Use timestamped values to verify FIFO ordering across threads</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> verify_milestone_4_hazard_pointers</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Verify hazard pointer memory safety and reclamation correctness.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Integrate hazard pointers with existing stack and queue implementations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Run all previous tests with hazard pointer protection enabled</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Verify no functional regressions introduced by memory protection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Test retirement list management and scan threshold behavior</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Create high memory pressure test with frequent allocation/deallocation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Verify thread cleanup releases all hazard pointers and retired nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Use address sanitizer to detect any use-after-free errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Monitor retirement list size to ensure it doesn't grow unbounded</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> verify_milestone_5_lock_free_hashmap</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Verify hash map operations and incremental resizing correctness.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Test basic insert/lookup/delete operations with various key types</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Verify concurrent operations maintain key-value consistency</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Test incremental resize triggered by load factor threshold</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify all existing data remains accessible during resize</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Test split-ordered list maintains logical hash ordering</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Create mixed workload stress test with resize operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Benchmark performance vs standard concurrent hash map implementations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Use deterministic hash functions for reproducible test scenarios</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints\">Language-Specific Hints</h4>\n<p><strong>Python-Specific Implementation Tips:</strong></p>\n<ul>\n<li>Use <code>threading.Event()</code> for coordinating test thread startup and shutdown</li>\n<li>Leverage <code>concurrent.futures.ThreadPoolExecutor</code> for managed thread lifecycle</li>\n<li>Use <code>time.perf_counter()</code> for high-resolution timing measurements</li>\n<li>Consider <code>multiprocessing</code> module for testing true parallelism (avoid GIL)</li>\n<li>Use <code>statistics</code> module for calculating performance percentiles</li>\n<li>Integrate with <code>pytest</code> framework for structured test organization and reporting</li>\n</ul>\n<p><strong>Memory Management Considerations:</strong></p>\n<ul>\n<li>Python&#39;s garbage collector can interfere with lock-free memory reclamation timing</li>\n<li>Use <code>gc.disable()</code> during critical test sections to ensure deterministic behavior</li>\n<li>Consider <code>ctypes</code> integration for direct memory management in performance-critical tests</li>\n<li>Use <code>tracemalloc</code> to detect memory leaks in long-running stress tests</li>\n</ul>\n<p><strong>Debugging Integration:</strong></p>\n<ul>\n<li>Use <code>logging</code> module with thread-safe formatting for concurrent debugging</li>\n<li>Consider <code>pdb</code> integration with thread-specific breakpoints</li>\n<li>Use <code>threading.current_thread().name</code> for thread identification in logs</li>\n<li>Integrate with <code>cProfile</code> for performance bottleneck identification</li>\n</ul>\n<h4 id=\"milestone-checkpoint-commands\">Milestone Checkpoint Commands</h4>\n<p><strong>Automated Test Execution:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run complete test suite for all milestones</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> --tb=short</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Run specific milestone tests</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/unit/test_atomic_operations.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/integration/test_stack_correctness.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/stress/stress_stack_operations.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Run with coverage reporting</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/</span><span style=\"color:#79B8FF\"> --cov=lockfree</span><span style=\"color:#79B8FF\"> --cov-report=html</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Run performance benchmarks</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> tests/benchmarks/performance_benchmarks.py</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Memory safety testing (requires AddressSanitizer)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">ASAN_OPTIONS</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">detect_leaks</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">1</span><span style=\"color:#B392F0\"> python</span><span style=\"color:#9ECBFF\"> tests/verification/memory_safety_checker.py</span></span></code></pre></div>\n\n<p><strong>Manual Verification Steps:</strong></p>\n<ol>\n<li><strong>Visual Progress Monitoring</strong>: Each stress test should display real-time progress indicators showing operations per second, success rates, and any detected anomalies</li>\n<li><strong>Interactive Debugging</strong>: Test framework should support manual breakpoint insertion for investigating specific failure scenarios</li>\n<li><strong>Performance Regression Detection</strong>: Benchmark results should be compared against baseline measurements to detect performance regressions</li>\n<li><strong>Correctness Validation</strong>: Each test should output clear pass/fail indicators with detailed failure descriptions when correctness violations occur</li>\n</ol>\n<h2 id=\"debugging-guide\">Debugging Guide</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (this section provides systematic approaches to diagnosing and fixing bugs that can occur throughout the implementation of lock-free data structures, from basic atomic operations through complete concurrent hash maps)</p>\n</blockquote>\n<p>Debugging lock-free concurrent code represents one of the most challenging aspects of systems programming. Unlike traditional sequential programs where bugs manifest predictably and reproducibly, lock-free algorithms introduce timing-dependent failures that may only surface under specific thread interleavings or memory ordering conditions. The non-deterministic nature of concurrent execution means that bugs can remain hidden during development only to emerge under production load conditions.</p>\n<h3 id=\"mental-model-debugging-as-detective-work-in-a-parallel-universe\">Mental Model: Debugging as Detective Work in a Parallel Universe</h3>\n<p>Think of debugging lock-free code as detective work in a parallel universe where multiple timelines exist simultaneously. Unlike sequential debugging where you follow a single timeline of events, lock-free debugging requires understanding how multiple threads create overlapping timelines that occasionally intersect at shared memory locations. Each thread operates in its own timeline, but these timelines can influence each other through atomic operations - like ripples from different stones thrown into the same pond.</p>\n<p>The challenge lies in reconstructing what happened across all timelines when something goes wrong. Traditional debugging tools show you one timeline, but the bug might be caused by the interaction between threads operating in different timelines. You need techniques that can capture and analyze the multi-dimensional execution space where these interactions occur.</p>\n<p><img src=\"/api/project/lock-free-structures/architecture-doc/asset?path=diagrams%2Fcas-retry-pattern.svg\" alt=\"CAS Retry Pattern\"></p>\n<h3 id=\"common-bug-patterns\">Common Bug Patterns</h3>\n<p>The majority of bugs in lock-free programming fall into predictable categories, each with characteristic symptoms and root causes. Understanding these patterns enables systematic diagnosis rather than random trial-and-error debugging. The following comprehensive table maps symptoms to their underlying causes and provides concrete fix strategies.</p>\n<table>\n<thead>\n<tr>\n<th><strong>Bug Pattern</strong></th>\n<th><strong>Symptoms</strong></th>\n<th><strong>Root Cause</strong></th>\n<th><strong>Detection Method</strong></th>\n<th><strong>Fix Strategy</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>ABA Problem</strong></td>\n<td>CAS succeeds when it should fail; corrupted data structure; nodes appear twice in traversal</td>\n<td>Pointer reuse between read and update allows incorrect CAS success</td>\n<td>Add sequence counters to detect reuse; validate structure after CAS</td>\n<td>Use tagged pointers or hazard pointers; never reuse memory addresses</td>\n</tr>\n<tr>\n<td><strong>Memory Ordering Race</strong></td>\n<td>Operations appear out of order; reads see stale values; writes become visible late</td>\n<td>Relaxed memory ordering allows CPU reordering</td>\n<td>Insert memory barriers; use stronger ordering</td>\n<td>Apply acquire/release semantics; add seq_cst barriers at critical points</td>\n</tr>\n<tr>\n<td><strong>Premature Reclamation</strong></td>\n<td>Segmentation faults; use-after-free errors; corrupted reads from freed memory</td>\n<td>Node freed while other threads still accessing</td>\n<td>Implement hazard pointer checking; add reference counting</td>\n<td>Deploy hazard pointer scheme; defer all reclamation until safe</td>\n</tr>\n<tr>\n<td><strong>Infinite CAS Loop</strong></td>\n<td>Thread CPU usage at 100%; operations never complete; system becomes unresponsive</td>\n<td>CAS retry loop without progress guarantee or backoff</td>\n<td>Monitor CAS attempt counts; detect spinning threads</td>\n<td>Add exponential backoff; implement helping mechanism; set retry limits</td>\n</tr>\n<tr>\n<td><strong>Linearizability Violation</strong></td>\n<td>Operations appear to execute in impossible order; concurrent operations see inconsistent state</td>\n<td>Incorrect linearization points or missing atomicity</td>\n<td>Record operation timestamps; verify sequential consistency</td>\n<td>Fix linearization points; ensure atomic state updates</td>\n</tr>\n<tr>\n<td><strong>Livelock Condition</strong></td>\n<td>System appears active but makes no progress; operations continuously retry</td>\n<td>Multiple threads interfere with each other&#39;s progress</td>\n<td>Measure operation completion rates; detect retry patterns</td>\n<td>Randomize backoff timing; implement priority schemes; add helping</td>\n</tr>\n<tr>\n<td><strong>Stack Corruption</strong></td>\n<td>Lost elements; duplicate elements; segfault during traversal; broken next pointers</td>\n<td>Race between pointer reads and CAS updates</td>\n<td>Validate stack structure; check next pointer consistency</td>\n<td>Protect pointer reads with hazard pointers; fix CAS ordering</td>\n</tr>\n<tr>\n<td><strong>Queue FIFO Violation</strong></td>\n<td>Elements dequeued in wrong order; enqueue/dequeue see inconsistent tail</td>\n<td>Tail pointer not properly maintained; missing helping</td>\n<td>Track element ordering; verify FIFO property</td>\n<td>Fix tail advancement logic; implement proper helping mechanism</td>\n</tr>\n<tr>\n<td><strong>Hash Map Inconsistency</strong></td>\n<td>Lookup fails for inserted keys; duplicate keys in buckets; lost updates</td>\n<td>Bucket initialization races; incorrect split ordering</td>\n<td>Validate bucket contents; check split-order properties</td>\n<td>Fix bucket initialization; correct reverse bit ordering</td>\n</tr>\n<tr>\n<td><strong>Hazard Pointer Leak</strong></td>\n<td>Memory usage grows without bound; retirement list becomes huge; scan never reclaims</td>\n<td>Hazard pointers not cleared; retirement threshold too high</td>\n<td>Monitor retirement list size; track hazard pointer usage</td>\n<td>Clear hazard pointers after use; tune scan threshold; add cleanup</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: Debugging Lock-free Code with Traditional Tools</strong>\nTraditional debuggers that pause execution fundamentally change the timing characteristics of concurrent programs. A bug that reproduces reliably in normal execution may disappear completely when running under a debugger because the debugger&#39;s pauses alter thread scheduling. This makes traditional stepping-through-code debugging ineffective for timing-dependent race conditions. Instead, rely on logging, performance counters, and specialized concurrent debugging tools.</p>\n<p>⚠️ <strong>Pitfall: Assuming Reproducible Bug Behavior</strong>\nLock-free bugs often manifest non-deterministically based on thread scheduling, memory layout, CPU cache states, and system load. A bug might appear once in every thousand runs, making it tempting to assume it&#39;s fixed when testing shows no immediate failures. Always use stress testing with high thread counts and extended duration to expose rare race conditions that only occur under specific timing conditions.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Performance Degradation as a Bug Symptom</strong>\nSevere performance degradation often signals correctness bugs in lock-free code. If CAS success rates drop dramatically or operation latencies increase exponentially, this usually indicates livelock conditions, excessive contention, or incorrect helping mechanisms rather than just poor performance. Investigate performance anomalies as potential correctness issues.</p>\n<h3 id=\"lock-free-debugging-techniques\">Lock-free Debugging Techniques</h3>\n<p>Effective debugging of lock-free code requires specialized techniques that account for the concurrent, non-deterministic nature of execution. These techniques focus on capturing sufficient information about multi-threaded execution patterns without significantly perturbing the timing characteristics that might mask or reveal bugs.</p>\n<h4 id=\"operation-recording-and-replay\">Operation Recording and Replay</h4>\n<p>The foundation of lock-free debugging involves systematically recording all operations and their outcomes to enable post-mortem analysis of execution sequences. This approach captures the multi-threaded execution history without requiring real-time debugging that would alter timing characteristics.</p>\n<table>\n<thead>\n<tr>\n<th><strong>Recording Component</strong></th>\n<th><strong>Information Captured</strong></th>\n<th><strong>Storage Method</strong></th>\n<th><strong>Analysis Purpose</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Operation Log</strong></td>\n<td>Operation type, timestamp, thread ID, parameters, return value, linearization point</td>\n<td>Thread-local circular buffers</td>\n<td>Reconstruct operation ordering and identify linearizability violations</td>\n</tr>\n<tr>\n<td><strong>CAS Attempt Tracker</strong></td>\n<td>Expected value, new value, observed value, success/failure, retry count</td>\n<td>Per-thread counters with periodic snapshots</td>\n<td>Detect ABA problems and infinite retry loops</td>\n</tr>\n<tr>\n<td><strong>Memory Access Log</strong></td>\n<td>Address accessed, operation type (load/store), memory ordering, timestamp</td>\n<td>Lock-free ring buffer</td>\n<td>Identify memory ordering races and visibility issues</td>\n</tr>\n<tr>\n<td><strong>State Snapshots</strong></td>\n<td>Complete data structure state, thread local state, hazard pointer assignments</td>\n<td>Atomic snapshot mechanism</td>\n<td>Verify invariants and detect corruption</td>\n</tr>\n<tr>\n<td><strong>Performance Metrics</strong></td>\n<td>Operation latencies, CAS success rates, retry counts, backoff delays</td>\n<td>Lock-free counters with statistical aggregation</td>\n<td>Identify performance anomalies indicating correctness problems</td>\n</tr>\n</tbody></table>\n<p>The <code>LinearizabilityChecker</code> provides systematic verification of concurrent operation ordering:</p>\n<table>\n<thead>\n<tr>\n<th><strong>Method</strong></th>\n<th><strong>Parameters</strong></th>\n<th><strong>Returns</strong></th>\n<th><strong>Description</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>record_operation</code></td>\n<td><code>operation: Operation</code></td>\n<td><code>None</code></td>\n<td>Thread-safely records operation with precise timestamp and linearization point</td>\n</tr>\n<tr>\n<td><code>verify_history</code></td>\n<td><code>start_time: float, end_time: float</code></td>\n<td><code>Tuple[bool, List[str]]</code></td>\n<td>Checks if operations in time window have valid sequential ordering</td>\n</tr>\n<tr>\n<td><code>check_invariants</code></td>\n<td><code>data_structure: Any</code></td>\n<td><code>Tuple[bool, str]</code></td>\n<td>Validates all structure-specific invariants and returns violation details</td>\n</tr>\n<tr>\n<td><code>generate_report</code></td>\n<td><code>failure_mode: FailureMode</code></td>\n<td><code>FailureReport</code></td>\n<td>Creates comprehensive failure analysis with timeline and thread interactions</td>\n</tr>\n</tbody></table>\n<h4 id=\"thread-safe-logging-strategies\">Thread-Safe Logging Strategies</h4>\n<p>Effective logging in lock-free systems requires careful design to avoid introducing synchronization bottlenecks while capturing sufficient detail for debugging. The logging system itself must be lock-free to avoid perturbing the very timing characteristics being debugged.</p>\n<blockquote>\n<p><strong>Design Principle: Minimize Observer Effect</strong>\nThe act of observing a concurrent system inherently changes its behavior through timing perturbations, cache effects, and resource contention. Effective lock-free debugging minimizes this observer effect by using lock-free logging mechanisms, thread-local storage, and deferred analysis of captured data.</p>\n</blockquote>\n<p><strong>Thread-Local Buffer Architecture:</strong>\nEach thread maintains its own circular buffer for operation logging, eliminating contention between threads while recording events. Buffers use atomic head and tail pointers to allow safe concurrent access by both the application thread (writing) and monitoring threads (reading). When buffers reach capacity, they either overwrite old entries (for high-frequency events) or flush to persistent storage (for critical events).</p>\n<p><strong>Timestamp Coordination:</strong>\nAccurate cross-thread timestamp coordination requires careful attention to clock synchronization without introducing synchronization overhead. Use high-resolution monotonic clocks with thread-local offset correction to account for CPU migration effects. Record both local timestamps and global sequence numbers to enable precise ordering reconstruction during analysis.</p>\n<p><strong>Event Filtering and Sampling:</strong>\nHigh-throughput lock-free systems generate enormous volumes of events that can overwhelm logging infrastructure. Implement adaptive sampling that increases detail capture when anomalies are detected while maintaining baseline monitoring during normal operation. Use bloom filters to detect interesting event patterns without storing all details.</p>\n<h4 id=\"state-inspection-without-locks\">State Inspection Without Locks</h4>\n<p>Inspecting the state of lock-free data structures during execution requires techniques that don&#39;t introduce synchronization points that would alter behavior. Traditional approaches like stopping all threads or acquiring locks fundamentally change the concurrent execution characteristics.</p>\n<p><strong>Atomic Snapshot Techniques:</strong>\nCapturing consistent snapshots of multi-word data structures requires careful use of versioned updates and optimistic reading. The snapshot algorithm attempts to read all relevant fields, then verifies that no concurrent updates occurred during the read sequence. If concurrent updates are detected, the algorithm retries the snapshot operation.</p>\n<p><strong>Hazard-Pointer-Protected Traversal:</strong>\nWhen inspecting linked structures like queues or hash map chains, use hazard pointers to protect against concurrent modifications that might deallocate nodes during traversal. The inspection thread announces its intent to access each node before dereferencing pointers, ensuring that concurrent operations don&#39;t reclaim memory in use.</p>\n<p><strong>Statistical Invariant Monitoring:</strong>\nRather than requiring perfect snapshots, many invariants can be verified statistically by sampling structure properties over time. For example, queue FIFO ordering can be verified by tracking element insertion and removal timestamps, while stack LIFO behavior can be monitored through operation sequence analysis.</p>\n<h3 id=\"performance-issue-diagnosis\">Performance Issue Diagnosis</h3>\n<p>Performance problems in lock-free systems often indicate underlying correctness issues or suboptimal algorithmic choices. Unlike traditional lock-based systems where performance issues typically manifest as blocked threads, lock-free performance problems appear as excessive CPU usage, poor cache behavior, or operation latency spikes.</p>\n<h4 id=\"contention-hotspot-analysis\">Contention Hotspot Analysis</h4>\n<p>Lock-free algorithms can still suffer from contention when multiple threads repeatedly access the same memory locations, even though no locks are involved. This contention manifests as repeated CAS failures, cache line bouncing, and increased memory latency.</p>\n<table>\n<thead>\n<tr>\n<th><strong>Contention Type</strong></th>\n<th><strong>Symptoms</strong></th>\n<th><strong>Measurement Technique</strong></th>\n<th><strong>Mitigation Strategy</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Single Variable Hotspot</strong></td>\n<td>High CAS failure rate on specific atomic variable; CPU cycles spent retrying</td>\n<td>Monitor per-variable CAS success rates; measure retry counts</td>\n<td>Implement exponential backoff; use helping mechanisms; consider algorithm changes</td>\n</tr>\n<tr>\n<td><strong>Cache Line Sharing</strong></td>\n<td>False sharing between unrelated variables; poor scaling with core count</td>\n<td>Use hardware performance counters; measure cache miss rates</td>\n<td>Pad structures to cache line boundaries; separate frequently accessed fields</td>\n</tr>\n<tr>\n<td><strong>Memory Ordering Overhead</strong></td>\n<td>Excessive memory barrier execution; poor instruction pipeline utilization</td>\n<td>Profile memory fence instructions; measure pipeline stalls</td>\n<td>Optimize memory ordering choices; use acquire/release instead of seq_cst</td>\n</tr>\n<tr>\n<td><strong>ABA Prevention Overhead</strong></td>\n<td>High overhead from tagged pointers or hazard pointer operations</td>\n<td>Measure ABA detection costs; profile protection overhead</td>\n<td>Tune tag sizes; optimize hazard pointer implementation; consider epoch-based reclamation</td>\n</tr>\n</tbody></table>\n<h4 id=\"false-sharing-detection\">False Sharing Detection</h4>\n<p>False sharing occurs when threads access different variables that reside on the same cache line, causing unnecessary cache coherence traffic. In lock-free systems, this can severely impact performance even when no actual data sharing occurs.</p>\n<p>The <code>PerformanceMonitor</code> provides systematic measurement of performance characteristics:</p>\n<table>\n<thead>\n<tr>\n<th><strong>Method</strong></th>\n<th><strong>Parameters</strong></th>\n<th><strong>Returns</strong></th>\n<th><strong>Description</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>record_operation</code></td>\n<td><code>operation_type: str, duration: float, cas_attempts: int, success: bool</code></td>\n<td><code>None</code></td>\n<td>Records timing and CAS statistics for performance analysis</td>\n</tr>\n<tr>\n<td><code>get_statistics</code></td>\n<td><code>operation_type: str</code></td>\n<td><code>Dict[str, float]</code></td>\n<td>Returns average latency, success rate, and retry count statistics</td>\n</tr>\n<tr>\n<td><code>detect_contention</code></td>\n<td><code>threshold: float</code></td>\n<td><code>List[str]</code></td>\n<td>Identifies operations with CAS success rates below threshold</td>\n</tr>\n<tr>\n<td><code>analyze_cache_behavior</code></td>\n<td><code>sampling_duration: float</code></td>\n<td><code>Dict[str, int]</code></td>\n<td>Uses hardware counters to measure cache miss rates and coherence traffic</td>\n</tr>\n</tbody></table>\n<h4 id=\"scalability-bottleneck-identification\">Scalability Bottleneck Identification</h4>\n<p>Lock-free algorithms should ideally scale linearly with the number of cores, but various factors can limit scalability. Identifying these bottlenecks requires systematic measurement under varying thread counts and workload characteristics.</p>\n<p><strong>Throughput Scaling Analysis:</strong>\nMeasure operation throughput as a function of thread count to identify scalability cliffs where adding more threads actually decreases total system throughput. This typically indicates excessive contention or poor load distribution across threads.</p>\n<p><strong>Latency Distribution Changes:</strong>\nMonitor how operation latency distributions change with increased concurrency. Healthy lock-free algorithms maintain relatively stable latency percentiles even under high contention, while problematic implementations show exponential latency growth.</p>\n<p><strong>Memory Subsystem Utilization:</strong>\nUse hardware performance monitoring units (PMUs) to measure memory bandwidth utilization, cache hierarchy behavior, and NUMA effects. Lock-free algorithms can quickly saturate memory bandwidth or create NUMA imbalances that limit scalability.</p>\n<blockquote>\n<p><strong>Critical Insight: Performance and Correctness Intersection</strong>\nIn lock-free systems, performance problems often indicate correctness issues rather than just inefficiency. Livelock conditions appear as high CPU usage with low progress. Memory leaks manifest as degrading performance over time. ABA problems can cause periodic performance spikes when corruption requires recovery. Always investigate performance anomalies as potential correctness violations.</p>\n</blockquote>\n<p>⚠️ <strong>Pitfall: Optimizing Performance Without Measuring Correctness</strong>\nWhen tuning lock-free algorithm performance, ensure that optimizations don&#39;t introduce correctness bugs. Common mistakes include weakening memory ordering for performance gains without considering correctness implications, reducing backoff delays to improve latency while introducing livelock conditions, and optimizing for single-threaded performance at the expense of concurrent correctness. Always combine performance measurements with correctness verification.</p>\n<p>⚠️ <strong>Pitfall: Ignoring NUMA Effects in Scalability Testing</strong>\nModern multi-core systems exhibit significant NUMA (Non-Uniform Memory Access) effects that can dramatically impact lock-free algorithm performance. Testing on single-socket systems may show good scalability that disappears on multi-socket production systems. Always test scalability on hardware configurations similar to production deployment, and consider NUMA-aware memory allocation strategies for data structures.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides concrete tools and techniques for implementing systematic debugging support throughout the lock-free data structure library. The debugging infrastructure should be designed as an integral part of the system rather than an afterthought, enabling both development-time debugging and production monitoring.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<p>| <strong>Component</strong> | <strong>Simple Option</strong> | <strong>Advanced Option</strong> |\n|---|---|---|---|\n| <strong>Operation Logging</strong> | Thread-local lists with periodic JSON dumps | Lock-free circular buffers with binary serialization |\n| <strong>Performance Monitoring</strong> | Python <code>time.perf_counter()</code> with basic statistics | Hardware performance counters via <code>psutil</code> or custom bindings |\n| <strong>Concurrency Testing</strong> | Python <code>threading</code> with random delays | Property-based testing with <code>hypothesis</code> library |\n| <strong>Visualization</strong> | Text-based reports with <code>tabulate</code> | Interactive dashboards with <code>matplotlib</code> or <code>plotly</code> |\n| <strong>State Inspection</strong> | Manual debugging with print statements | Automated invariant checking with custom validators |</p>\n<h4 id=\"recommended-module-structure\">Recommended Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>lock_free_project/\n  src/\n    debugging/\n      __init__.py                    ← exports main debugging interfaces\n      operation_recorder.py          ← LinearizabilityChecker and operation logging\n      performance_monitor.py         ← PerformanceMonitor and metrics collection\n      failure_analyzer.py            ← FailureReport generation and pattern analysis\n      invariant_checker.py          ← InvariantChecker for structure validation\n      stress_tester.py              ← StressTestFramework for concurrent testing\n      visualization.py              ← reporting and dashboard generation\n    data_structures/\n      atomic_ops.py                 ← integrates with operation_recorder\n      treiber_stack.py             ← includes debugging hooks\n      michael_scott_queue.py       ← includes debugging hooks\n      hazard_pointers.py           ← includes debugging hooks\n      lock_free_hashmap.py         ← includes debugging hooks\n    tests/\n      test_debugging_tools.py      ← validates debugging infrastructure\n      stress_tests/                ← comprehensive concurrent testing\n        test_stack_stress.py\n        test_queue_stress.py\n        test_hashmap_stress.py</code></pre></div>\n\n<h4 id=\"infrastructure-complete-operation-recording-system\">Infrastructure: Complete Operation Recording System</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Any, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> OperationType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PUSH</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"push\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    POP</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"pop\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ENQUEUE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"enqueue\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEQUEUE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"dequeue\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INSERT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"insert\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LOOKUP</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"lookup\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DELETE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"delete\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Operation</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: OperationType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    thread_id: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    invocation: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#6A737D\">  # True for call, False for response</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    args: Tuple</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result: Any</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    operation_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> LinearizabilityChecker</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.operations: List[Operation] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.operation_lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Lock()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.operation_counter </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.verification_cache: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> record_operation</span><span style=\"color:#E1E4E8\">(self, operation_type: OperationType, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        is_invocation: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, args: Tuple </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (), </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        result: Any </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record an operation invocation or response for linearizability checking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        operation_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">threading.current_thread().ident</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">_</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.operation_counter</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.operation_counter </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        op </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Operation(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            type</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">operation_type,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.perf_counter(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            thread_id</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">threading.current_thread().ident,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            invocation</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">is_invocation,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            args</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">args,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            result</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">result,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            operation_id</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">operation_id</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.operation_lock:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.operations.append(op)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> operation_id</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> verify_history</span><span style=\"color:#E1E4E8\">(self, start_time: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, end_time: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> Tuple[</span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if operations in time window have valid sequential ordering.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement linearizability verification algorithm</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Filter operations by time window</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Group invocation/response pairs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check if there exists a valid sequential ordering</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return validation result and any violation descriptions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> generate_timeline_report</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate human-readable timeline of operations for debugging.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Sort operations by timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Group by thread for parallel timeline view</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Format as readable timeline with ASCII art</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Highlight potential linearizability violations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"infrastructure-performance-monitoring-system\">Infrastructure: Performance Monitoring System</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> collections </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> defaultdict, deque</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> statistics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PerformanceMonitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, history_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.operation_times: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, deque] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> defaultdict(</span><span style=\"color:#F97583\">lambda</span><span style=\"color:#E1E4E8\">: deque(</span><span style=\"color:#FFAB70\">maxlen</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">history_size))</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cas_attempts: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, deque] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> defaultdict(</span><span style=\"color:#F97583\">lambda</span><span style=\"color:#E1E4E8\">: deque(</span><span style=\"color:#FFAB70\">maxlen</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">history_size))</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.success_rates: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, deque] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> defaultdict(</span><span style=\"color:#F97583\">lambda</span><span style=\"color:#E1E4E8\">: deque(</span><span style=\"color:#FFAB70\">maxlen</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">history_size))</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.history_size </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> history_size</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Lock()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> record_operation</span><span style=\"color:#E1E4E8\">(self, operation_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, duration: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        cas_attempts: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, success: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record performance metrics for a completed operation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.lock:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.operation_times[operation_type].append(duration)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.cas_attempts[operation_type].append(cas_attempts)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.success_rates[operation_type].append(</span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> success </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_statistics</span><span style=\"color:#E1E4E8\">(self, operation_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get comprehensive statistics for an operation type.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.lock:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> operation_type </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.operation_times:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            times </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> list</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.operation_times[operation_type])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            attempts </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> list</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.cas_attempts[operation_type])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            successes </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> list</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.success_rates[operation_type])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> times:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'avg_latency_ms'</span><span style=\"color:#E1E4E8\">: statistics.mean(times) </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'p95_latency_ms'</span><span style=\"color:#E1E4E8\">: statistics.quantiles(times, </span><span style=\"color:#FFAB70\">n</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">20</span><span style=\"color:#E1E4E8\">)[</span><span style=\"color:#79B8FF\">18</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'p99_latency_ms'</span><span style=\"color:#E1E4E8\">: statistics.quantiles(times, </span><span style=\"color:#FFAB70\">n</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">)[</span><span style=\"color:#79B8FF\">98</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'avg_cas_attempts'</span><span style=\"color:#E1E4E8\">: statistics.mean(attempts),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'success_rate'</span><span style=\"color:#E1E4E8\">: statistics.mean(successes),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'total_operations'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(times)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> detect_contention</span><span style=\"color:#E1E4E8\">(self, threshold: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.5</span><span style=\"color:#E1E4E8\">) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Identify operations with CAS success rates below threshold.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate recent success rates for all operation types</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Identify operations below threshold</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return list of problematic operation types</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Include suggested remediation strategies</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"core-logic-stress-testing-framework\">Core Logic: Stress Testing Framework</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> random</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> concurrent.futures </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ThreadPoolExecutor</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Any</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StressTestConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_threads: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    operations_per_thread: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    test_duration_seconds: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    operation_mix: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]  </span><span style=\"color:#6A737D\"># operation_type -> probability</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    contention_level: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#6A737D\">  # 'low', 'medium', 'high'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    backoff_strategy: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#6A737D\">  # 'none', 'exponential', 'linear'</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StressTestResults</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    total_operations: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    successful_operations: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    failed_operations: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    total_duration: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    throughput_ops_per_sec: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    average_latency_ms: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    p95_latency_ms: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    p99_latency_ms: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cas_success_rate: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    thread_results: List[Dict]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StressTestFramework</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, data_structure: Any, config: StressTestConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.data_structure </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> data_structure</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.results </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.operation_times: List[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cas_attempts </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cas_successes </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.results_lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Lock()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.should_stop </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Event()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> stress_test_data_structure</span><span style=\"color:#E1E4E8\">(self) -> StressTestResults:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute comprehensive stress test and return detailed results.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize test environment and metrics collection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Start worker threads with specified configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Run test for specified duration or operation count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Collect and aggregate results from all threads</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate comprehensive statistics and return results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify data structure invariants after test completion</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _worker_thread</span><span style=\"color:#E1E4E8\">(self, thread_id: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> Dict:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute operations for a single worker thread.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize thread-local random number generator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate operation mix based on configuration probabilities</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Execute operations until completion or timeout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Record timing and success metrics for each operation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return thread-specific results dictionary</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _execute_operation</span><span style=\"color:#E1E4E8\">(self, operation_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, rng: random.Random, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                          thread_id: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> Tuple[</span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute a single operation on the data structure.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate operation parameters based on type and contention level</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Record operation start time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Execute the operation with CAS attempt counting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Record operation completion and calculate duration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return success status, duration, and CAS attempt count</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"core-logic-debugging-instrumentation-hooks\">Core Logic: Debugging Instrumentation Hooks</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DebugInstrumentation</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Provides debugging hooks for integration into lock-free data structures.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.operation_recorder </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> LinearizabilityChecker()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.performance_monitor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> PerformanceMonitor()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.enabled </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> record_cas_attempt</span><span style=\"color:#E1E4E8\">(self, variable_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, expected: Any, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                          new_value: Any, observed: Any, success: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record CAS attempt details for ABA and livelock detection.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Log CAS attempt with all values for later analysis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Detect potential ABA patterns (observed != expected)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Track consecutive failures for livelock detection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Update performance counters for success rate calculation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_structure_invariants</span><span style=\"color:#E1E4E8\">(self, structure_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                    validation_function) -> Tuple[</span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check data structure invariants and report violations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Execute structure-specific validation function</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Catch and report any invariant violations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate detailed violation report with structure state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return validation status and violation description</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> instrument_operation</span><span style=\"color:#E1E4E8\">(self, operation_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Decorator to add debugging instrumentation to operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        def</span><span style=\"color:#B392F0\"> decorator</span><span style=\"color:#E1E4E8\">(func):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            def</span><span style=\"color:#B392F0\"> wrapper</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Record operation invocation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Execute original function with timing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Record operation response and performance metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate post-condition invariants</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return original function result</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                pass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> wrapper</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> decorator</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint-debugging-infrastructure-validation\">Milestone Checkpoint: Debugging Infrastructure Validation</h4>\n<p>After implementing the debugging infrastructure, verify its functionality with these checkpoints:</p>\n<p><strong>1. Operation Recording Verification:</strong>\nRun a simple concurrent test with 2-3 threads performing stack operations. The <code>LinearizabilityChecker</code> should capture all operations with precise timestamps. Verify that the timeline report shows operations in chronological order with clear invocation/response pairing.</p>\n<p><strong>2. Performance Monitoring Validation:</strong>\nExecute a brief stress test and verify that <code>PerformanceMonitor</code> captures realistic metrics. CAS success rates should be between 10-90% under moderate contention. Latency percentiles should show reasonable distributions without obvious outliers.</p>\n<p><strong>3. Invariant Checking Integration:</strong>\nIntegrate debugging hooks into one data structure (start with <code>TreiberStack</code>). Verify that invariant violations are detected when you intentionally introduce bugs (e.g., skip CAS operations or corrupt pointers).</p>\n<p><strong>Expected behavior:</strong> Debugging tools should capture detailed execution information without significantly impacting performance (less than 10% overhead). Reports should be human-readable and highlight potential issues clearly.</p>\n<p><strong>Signs of problems:</strong> If debugging overhead exceeds 20%, optimize the logging infrastructure. If operation recording misses events or shows incorrect timestamps, check thread-safety of the recording mechanism. If performance monitoring shows impossible values (negative latencies, &gt;100% success rates), verify metric calculation logic.</p>\n<h2 id=\"future-extensions\">Future Extensions</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (this section explores advanced directions and optimizations that build upon the foundational lock-free data structures implemented throughout the project)</p>\n</blockquote>\n<p>The lock-free data structure foundation we&#39;ve built represents just the beginning of a rich ecosystem of concurrent programming possibilities. While our core implementations of atomic operations, Treiber stacks, Michael-Scott queues, hazard pointers, and split-ordered hash maps provide solid building blocks, numerous advanced algorithms, performance optimizations, and integration opportunities await exploration. This section outlines the most promising directions for extending our lock-free library, organized into three categories: advanced algorithmic improvements that push beyond lock-free to wait-free guarantees, performance optimizations that squeeze every cycle from modern hardware, and ecosystem integrations that make these data structures production-ready.</p>\n<p>The progression from our current lock-free implementations to these advanced extensions follows a natural evolution. Our atomic operations foundation supports more sophisticated coordination protocols. The memory reclamation expertise gained from hazard pointers transfers directly to more complex schemes like epochs and RCU. The linearizability verification techniques we developed scale to validate more intricate data structures. Most importantly, the debugging and testing methodologies we&#39;ve established become even more critical when dealing with the increased complexity of wait-free algorithms and hardware-specific optimizations.</p>\n<h3 id=\"advanced-lock-free-algorithms\">Advanced Lock-free Algorithms</h3>\n<p><strong>Mental Model: From Traffic Lights to Highway Overpasses</strong></p>\n<p>Think of our current lock-free algorithms as a well-designed traffic intersection with smart traffic lights. Most of the time, traffic flows smoothly with minimal delays. However, during peak hours or when one direction is particularly busy, some cars might wait through multiple light cycles before proceeding. Wait-free algorithms, by contrast, are like a highway system with overpasses and dedicated lanes - every vehicle is guaranteed to reach its destination within a predictable time, regardless of how busy other routes become.</p>\n<p>This analogy captures the fundamental difference between lock-free and wait-free progress guarantees. Lock-free algorithms ensure that at least one thread makes progress within a finite number of steps, but individual threads might face delays if they repeatedly lose CAS races to more fortunate threads. Wait-free algorithms provide the stronger guarantee that every thread completes its operation within a bounded number of its own steps, regardless of interference from other threads.</p>\n<p><strong>Wait-free Algorithm Foundations</strong></p>\n<p>The transition from lock-free to wait-free algorithms requires fundamentally different design approaches. Where lock-free algorithms rely on retrying CAS operations until success, wait-free algorithms must ensure that every thread can complete its work within a predetermined step bound. This typically involves more sophisticated helping mechanisms where threads not only assist stuck operations but actively contribute to every other thread&#39;s progress.</p>\n<p>The universal construction technique provides one approach to building wait-free implementations from lock-free foundations. Each thread maintains a private copy of its intended operation, and a global consensus mechanism ensures that all operations appear to execute in some sequential order. While theoretically elegant, universal constructions often exhibit high overhead that makes them impractical for performance-sensitive applications.</p>\n<p>More practical wait-free algorithms exploit specific properties of individual data structures. For example, a wait-free queue might reserve space for each thread&#39;s operations in advance, eliminating the contention that causes retry loops in the Michael-Scott algorithm. Similarly, wait-free hash maps can use combining techniques where threads batch their operations together, reducing per-operation overhead while guaranteeing bounded completion times.</p>\n<table>\n<thead>\n<tr>\n<th>Wait-free Algorithm</th>\n<th>Key Technique</th>\n<th>Step Bound</th>\n<th>Trade-offs</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Universal Construction</td>\n<td>Private operation logs + consensus</td>\n<td>O(N²) per operation</td>\n<td>Simple but high overhead</td>\n</tr>\n<tr>\n<td>Wait-free Queue</td>\n<td>Pre-allocated slots per thread</td>\n<td>O(N) per operation</td>\n<td>Memory overhead scales with threads</td>\n</tr>\n<tr>\n<td>Combining Tree</td>\n<td>Hierarchical operation batching</td>\n<td>O(log N) per operation</td>\n<td>Complex helping protocols</td>\n</tr>\n<tr>\n<td>Fast Path/Slow Path</td>\n<td>Common case optimization + fallback</td>\n<td>O(1) typical, O(N) worst</td>\n<td>Best of both worlds approach</td>\n</tr>\n</tbody></table>\n<p><strong>Advanced Data Structure Extensions</strong></p>\n<p>Beyond improving progress guarantees, numerous sophisticated data structures build upon our lock-free foundations. Lock-free trees present particular challenges due to their hierarchical nature and the need for multi-step operations that maintain tree invariants. B-trees and balanced trees require careful coordination between threads performing rotations, splits, and merges while maintaining both structural and ordering properties.</p>\n<p>Priority queues represent another fertile ground for lock-free innovation. Unlike simple FIFO queues, priority queues must maintain heap or ordering properties while allowing concurrent insertions and deletions. Skip lists provide one approach, offering probabilistic balancing that eliminates the complex coordination required for deterministic tree structures. Lock-free skip lists use forward pointers at multiple levels, allowing concurrent traversals while using CAS operations to maintain linking invariants across levels.</p>\n<p>Graph data structures pose even greater challenges due to their complex connectivity patterns. Lock-free graph algorithms must handle cycles, multiple incoming edges, and operations that span multiple nodes. Techniques like edge marking and logical deletion become essential for maintaining consistency during concurrent modifications.</p>\n<blockquote>\n<p><strong>Critical Insight</strong>: Advanced data structures often require abandoning the simple single-CAS operations that characterize basic lock-free algorithms. Instead, they employ multi-step protocols with careful ordering and helping mechanisms to maintain complex invariants across multiple memory locations.</p>\n</blockquote>\n<p><strong>Specialized Optimization Techniques</strong></p>\n<p>Several algorithmic optimizations can dramatically improve performance for specific use cases. Flat combining represents a hybrid approach where threads occasionally acquire exclusive access to perform batched operations on behalf of multiple threads. While not strictly lock-free due to the exclusive access periods, flat combining can achieve better cache locality and reduced CAS contention for workloads with high operation rates.</p>\n<p>Read-Copy-Update (RCU) provides another powerful technique for read-heavy workloads. RCU allows unlimited concurrent readers by ensuring that updates create new versions rather than modifying existing data. Readers access consistent snapshots without any atomic operations, while writers coordinate through quiescence detection to safely reclaim old versions.</p>\n<p>Epoch-based memory reclamation offers an alternative to hazard pointers with different trade-offs. Instead of tracking individual pointer accesses, epoch-based schemes divide time into periods and defer reclamation until all threads have moved past the epoch where nodes were retired. This approach reduces the per-access overhead of hazard pointers but may delay reclamation longer in scenarios with irregular thread scheduling.</p>\n<table>\n<thead>\n<tr>\n<th>Technique</th>\n<th>Best Use Case</th>\n<th>Overhead</th>\n<th>Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Flat Combining</td>\n<td>High contention, batched operations</td>\n<td>Low amortized</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>RCU</td>\n<td>Read-heavy workloads</td>\n<td>Minimal read, higher write</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Epoch Reclamation</td>\n<td>Regular thread scheduling patterns</td>\n<td>Lower than hazard pointers</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Hardware Transactional Memory</td>\n<td>Short, contended critical sections</td>\n<td>Variable (hardware dependent)</td>\n<td>Low (when supported)</td>\n</tr>\n</tbody></table>\n<h3 id=\"performance-optimizations\">Performance Optimizations</h3>\n<p><strong>Mental Model: From City Streets to Formula 1 Circuits</strong></p>\n<p>Imagine our current lock-free algorithms as efficient city street navigation - they avoid the worst traffic jams and generally get you where you need to go in reasonable time. Performance optimization transforms these implementations into Formula 1 racing circuits: every turn is banked for maximum speed, every surface is optimized for grip, and every detail is tuned for the specific characteristics of the vehicles using the track. Just as F1 circuits exploit physics and aerodynamics that don&#39;t matter for everyday driving, these optimizations exploit hardware characteristics and usage patterns that general-purpose algorithms must ignore.</p>\n<p><strong>Hardware-Aware Memory Layout Optimization</strong></p>\n<p>Modern hardware presents a complex memory hierarchy where data placement dramatically affects performance. NUMA (Non-Uniform Memory Access) architectures mean that memory access costs depend on which CPU socket allocated the memory and which socket is accessing it. Our lock-free data structures can be optimized to consider NUMA topology when allocating nodes and distributing work across threads.</p>\n<p>NUMA-aware allocation strategies place frequently accessed shared data structures on memory banks close to the threads most likely to access them. For data structures with read-heavy access patterns, replicating read-only metadata across NUMA nodes can eliminate cross-socket memory traffic. Write-heavy structures might use per-NUMA-node pools for node allocation, reducing remote memory pressure during frequent allocations.</p>\n<p>Cache line optimization represents another critical performance dimension. False sharing occurs when unrelated variables share cache lines, causing expensive cache coherence traffic when different threads modify their respective variables. Our atomic reference types can be padded to ensure each occupies its own cache line, eliminating false sharing at the cost of increased memory usage.</p>\n<p>Cache-conscious data structure layout goes beyond avoiding false sharing to actively promoting cache efficiency. Arranging related fields to fit within single cache lines reduces memory bandwidth requirements. For linked structures like our queue and stack implementations, node prefetching can hide memory latency by speculatively loading nodes likely to be accessed in future operations.</p>\n<table>\n<thead>\n<tr>\n<th>Optimization</th>\n<th>Impact</th>\n<th>Memory Cost</th>\n<th>Implementation Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>NUMA-aware allocation</td>\n<td>2-4x improvement on multi-socket</td>\n<td>Minimal</td>\n<td>Medium (topology detection)</td>\n</tr>\n<tr>\n<td>Cache line padding</td>\n<td>10-50% improvement with contention</td>\n<td>8x memory usage for padded fields</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Node prefetching</td>\n<td>20-40% improvement for traversals</td>\n<td>None</td>\n<td>Medium (hardware intrinsics)</td>\n</tr>\n<tr>\n<td>Data structure splitting</td>\n<td>Variable (workload dependent)</td>\n<td>Moderate</td>\n<td>High (algorithmic changes)</td>\n</tr>\n</tbody></table>\n<p><strong>CPU Architecture Specialization</strong></p>\n<p>Different CPU architectures provide varying atomic operation capabilities and performance characteristics. x86-64 processors offer strong memory ordering guarantees that can eliminate memory barriers in certain algorithms, while ARM architectures require explicit ordering constraints but provide more flexible atomic primitives. Our atomic operations layer can include architecture-specific optimizations that exploit these differences.</p>\n<p>Weak memory model optimizations become particularly important on ARM and RISC-V architectures where explicit memory barriers are required for ordering guarantees. Careful placement of acquire and release barriers can maintain correctness while minimizing performance overhead. Some algorithms can be restructured to reduce the number of required barriers by changing the order of operations or using different atomic primitives.</p>\n<p>Hardware transactional memory (HTM) support on newer Intel and IBM processors provides another optimization opportunity. Short, contended critical sections that would normally require complex lock-free protocols can sometimes be replaced with simple transactional code. While HTM has capacity limitations and fallback requirements, it can significantly simplify implementation while maintaining good performance for workloads that fit within hardware constraints.</p>\n<p>Vectorization opportunities exist for certain bulk operations on our data structures. SIMD instructions can accelerate parallel searches, batch comparisons, or parallel updates to multiple independent elements. While vectorization typically applies to compute-intensive algorithms rather than pointer-chasing data structures, careful algorithm design can expose vectorization opportunities.</p>\n<p><strong>Adaptive and Self-Tuning Mechanisms</strong></p>\n<p>Static optimization approaches assume fixed workload characteristics, but real applications exhibit varying contention patterns, thread counts, and operation mixes over time. Adaptive algorithms adjust their behavior based on runtime observations to optimize for current conditions rather than worst-case scenarios.</p>\n<p>Contention-adaptive backoff strategies monitor CAS failure rates and adjust retry delays accordingly. During low contention periods, immediate retries minimize latency. As contention increases, exponential backoff reduces wasted CPU cycles and cache coherence traffic. Advanced backoff schemes can even detect contention patterns and predict optimal delay strategies for different operation types.</p>\n<p>Load-adaptive data structure sizing automatically adjusts capacity based on observed usage patterns. Our hash map implementation could monitor load factors and resize frequency to find optimal bucket counts for current workloads. Similarly, memory reclamation schemes could adjust scan thresholds based on allocation rates and memory pressure.</p>\n<p>Operation-mix adaptation optimizes for the specific balance of read and write operations observed at runtime. Data structures can maintain separate fast paths for read-heavy and write-heavy workloads, switching between optimization strategies as usage patterns change.</p>\n<table>\n<thead>\n<tr>\n<th>Adaptive Technique</th>\n<th>Monitoring Overhead</th>\n<th>Adaptation Speed</th>\n<th>Benefit Range</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Contention-aware backoff</td>\n<td>Very low</td>\n<td>Immediate</td>\n<td>20-80% latency reduction</td>\n</tr>\n<tr>\n<td>Load-adaptive sizing</td>\n<td>Low</td>\n<td>Minutes to hours</td>\n<td>2-10x throughput improvement</td>\n</tr>\n<tr>\n<td>Operation-mix optimization</td>\n<td>Medium</td>\n<td>Seconds to minutes</td>\n<td>Highly workload dependent</td>\n</tr>\n<tr>\n<td>Thread-count scaling</td>\n<td>Low</td>\n<td>Seconds</td>\n<td>50-300% improvement during scaling</td>\n</tr>\n</tbody></table>\n<h3 id=\"integration-and-ecosystem\">Integration and Ecosystem</h3>\n<p><strong>Mental Model: From Prototype Workshop to Production Factory</strong></p>\n<p>Think of our current lock-free implementations as carefully crafted prototypes built in a specialized workshop. Each component works beautifully and demonstrates important principles, but transforming these prototypes into production-ready systems requires the same kind of systematic engineering that turns a hand-built concept car into a mass-production vehicle. This involves tooling for manufacturing (build systems and testing frameworks), quality control processes (monitoring and debugging), integration with existing systems (language bindings and framework adapters), and operational procedures (deployment, monitoring, and maintenance).</p>\n<p><strong>Language Ecosystem Integration</strong></p>\n<p>While our primary implementation targets Python, the algorithms and techniques we&#39;ve developed translate naturally to other languages with different trade-offs and integration opportunities. Each language ecosystem presents unique challenges and opportunities for lock-free data structure integration.</p>\n<p>C and C++ implementations can leverage stronger memory model guarantees and direct access to hardware atomic primitives. The C11 and C++11 atomic standards provide portable abstractions over platform-specific atomic operations, allowing optimized implementations that exploit architecture-specific features while maintaining compatibility. C++ template metaprogramming can generate specialized versions of data structures for different data types, eliminating the boxing overhead required in Python implementations.</p>\n<p>Rust&#39;s ownership system provides natural integration opportunities with memory reclamation schemes. Rust&#39;s lifetime tracking can statically verify many safety properties that our hazard pointer implementation enforces dynamically. The <code>Arc</code> and <code>Rc</code> reference counting types provide alternatives to hazard pointers for certain use cases, though they require atomic reference count updates that can become contention bottlenecks.</p>\n<p>Go&#39;s runtime scheduler and garbage collector present both opportunities and challenges for lock-free integration. The garbage collector eliminates manual memory reclamation concerns but introduces stop-the-world pauses that can affect linearizability timing. Go&#39;s channel-based concurrency model provides a higher-level alternative to lock-free programming for many use cases, but direct atomic operations remain important for performance-critical components.</p>\n<p>Java Virtual Machine integration opens opportunities for dynamic optimization and just-in-time compilation of lock-free algorithms. The JVM&#39;s escape analysis can eliminate atomic operations for thread-local objects, while profile-guided optimization can specialize hot paths based on observed usage patterns. Integration with Java&#39;s <code>java.util.concurrent</code> package ensures compatibility with existing concurrent programming patterns.</p>\n<table>\n<thead>\n<tr>\n<th>Language</th>\n<th>Key Advantages</th>\n<th>Integration Challenges</th>\n<th>Recommended Use Cases</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>C/C++</td>\n<td>Direct hardware access, zero-overhead abstractions</td>\n<td>Manual memory management complexity</td>\n<td>High-performance systems, embedded</td>\n</tr>\n<tr>\n<td>Rust</td>\n<td>Memory safety, zero-cost abstractions</td>\n<td>Learning curve, ownership model constraints</td>\n<td>Systems programming, safe concurrency</td>\n</tr>\n<tr>\n<td>Go</td>\n<td>Simple syntax, runtime integration</td>\n<td>GC pauses, limited atomic primitives</td>\n<td>Web services, distributed systems</td>\n</tr>\n<tr>\n<td>Java</td>\n<td>JVM optimizations, ecosystem maturity</td>\n<td>GC overhead, platform abstraction</td>\n<td>Enterprise applications, frameworks</td>\n</tr>\n</tbody></table>\n<p><strong>Framework and Library Integration</strong></p>\n<p>Modern software development relies heavily on frameworks and libraries that provide higher-level abstractions over low-level concurrency primitives. Integrating our lock-free data structures with popular frameworks requires careful attention to threading models, lifecycle management, and error handling conventions.</p>\n<p>Web framework integration presents unique challenges due to request-scoped concurrency patterns and connection pooling. Frameworks like Django, Flask, FastAPI in Python, or Express.js in Node.js typically handle each request in separate threads or async tasks. Our lock-free data structures can provide shared state across requests without blocking, but integration requires careful attention to lifecycle management and cleanup during application shutdown.</p>\n<p>Database integration opportunities exist for both embedded and networked database systems. Lock-free data structures can serve as in-memory indexes, caches, or buffer pool management systems within database engines. The ACID properties required by database transactions interact interestingly with the linearizability guarantees provided by lock-free algorithms.</p>\n<p>Message queue and event streaming integration provides another natural application domain. Systems like Apache Kafka, RabbitMQ, or Redis can benefit from lock-free internal data structures for managing queues, routing tables, and subscription lists. The ordered delivery guarantees required by messaging systems align well with the sequential consistency properties of our implementations.</p>\n<p>Container orchestration and cloud-native integration involves packaging our implementations for deployment in Kubernetes, Docker, and serverless environments. This requires attention to resource limits, health checking, metrics collection, and graceful shutdown procedures that properly clean up lock-free data structures without losing data.</p>\n<p><strong>Production Deployment Considerations</strong></p>\n<p>Moving from development prototypes to production deployment requires systematic attention to operational concerns that don&#39;t affect correctness but critically impact reliability and maintainability. Monitoring and observability become essential for understanding behavior in production workloads that differ from development test scenarios.</p>\n<p>Performance monitoring should track not just throughput and latency metrics, but lock-free specific indicators like CAS success rates, retry loop iterations, and memory reclamation effectiveness. High CAS failure rates indicate contention hotspots that might benefit from algorithmic changes or workload balancing. Memory reclamation monitoring helps detect leaks or inefficient cleanup that could lead to resource exhaustion.</p>\n<p>Configuration management for lock-free systems involves tuning parameters that significantly affect performance but have non-obvious optimal values. Backoff strategies, memory reclamation thresholds, and data structure sizing parameters often require workload-specific tuning. Providing reasonable defaults while allowing runtime configuration adjustments helps balance ease of deployment with performance optimization opportunities.</p>\n<p>Health checking and failure detection must account for the unique failure modes of lock-free systems. Traditional deadlock detection doesn&#39;t apply, but livelock detection becomes important. Progress monitoring can detect scenarios where threads are active but making no forward progress due to pathological contention patterns.</p>\n<p>Deployment automation should include verification steps that validate lock-free algorithm correctness under realistic load conditions. Stress testing with production-like workloads can expose race conditions or performance degradation that doesn&#39;t appear in unit tests. Canary deployment strategies help identify problems before full rollout.</p>\n<table>\n<thead>\n<tr>\n<th>Operational Area</th>\n<th>Key Metrics</th>\n<th>Alerting Thresholds</th>\n<th>Remediation Actions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Performance</td>\n<td>CAS success rate, operation latency</td>\n<td>&lt;80% success rate, &gt;10x latency increase</td>\n<td>Load balancing, algorithm tuning</td>\n</tr>\n<tr>\n<td>Memory Usage</td>\n<td>Hazard pointer efficiency, node allocation rate</td>\n<td>&gt;10% reclamation backlog</td>\n<td>Threshold adjustment, leak investigation</td>\n</tr>\n<tr>\n<td>Progress Guarantees</td>\n<td>Operation completion rate, retry loop bounds</td>\n<td>Operations stalling &gt;1 second</td>\n<td>Contention analysis, workload adjustment</td>\n</tr>\n<tr>\n<td>Correctness</td>\n<td>Linearizability violations, invariant checks</td>\n<td>Any detected violations</td>\n<td>Immediate rollback, root cause analysis</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The extensions described in this section represent advanced topics that build upon the solid foundation established through our core milestones. While implementing these extensions requires deep expertise in concurrent programming and hardware optimization, the systematic approach we&#39;ve developed provides a pathway for gradual advancement from lock-free foundations to cutting-edge concurrent algorithms.</p>\n<h4 id=\"advanced-algorithm-implementation-strategy\">Advanced Algorithm Implementation Strategy</h4>\n<p>The progression toward wait-free algorithms should begin with understanding the theoretical foundations before attempting implementations. The key insight is that wait-free algorithms trade implementation complexity and resource usage for stronger progress guarantees. Start by implementing simple wait-free counters using fetch-and-add operations, then progress to more complex structures.</p>\n<p>For wait-free data structures, the helping mechanism becomes central to the design. Each thread must be able to assist any other thread&#39;s operation, which requires careful protocol design. Begin with consensus-based approaches where threads agree on operation ordering, then explore more specialized techniques like pre-allocation and combining trees.</p>\n<h4 id=\"technology-recommendations-for-extensions\">Technology Recommendations for Extensions</h4>\n<table>\n<thead>\n<tr>\n<th>Extension Category</th>\n<th>Recommended Technologies</th>\n<th>Implementation Notes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Wait-free Algorithms</td>\n<td>C++ with custom allocators</td>\n<td>Need precise control over memory layout</td>\n</tr>\n<tr>\n<td>NUMA Optimization</td>\n<td>libnuma, hwloc libraries</td>\n<td>Hardware topology detection essential</td>\n</tr>\n<tr>\n<td>Hardware Transactional Memory</td>\n<td>Intel TSX intrinsics</td>\n<td>Requires fallback paths for unsupported hardware</td>\n</tr>\n<tr>\n<td>Language Bindings</td>\n<td>SWIG, Cython, PyO3</td>\n<td>Choose based on target language ecosystem</td>\n</tr>\n<tr>\n<td>Performance Monitoring</td>\n<td>Prometheus, StatsD</td>\n<td>Lock-free compatible metrics collection</td>\n</tr>\n</tbody></table>\n<h4 id=\"development-roadmap-for-extensions\">Development Roadmap for Extensions</h4>\n<p>The implementation of these extensions should follow a structured progression that builds expertise gradually:</p>\n<p><strong>Phase 1: Performance Optimization Foundations</strong>\nBegin with cache line optimization and false sharing elimination. These changes provide immediate benefits while requiring minimal algorithmic changes. Implement cache line padding for atomic references and measure the performance impact under different contention scenarios.</p>\n<p><strong>Phase 2: Hardware-Specific Optimization</strong>\nAdd architecture-specific atomic operation optimizations and memory ordering improvements. This phase requires platform detection and conditional compilation but provides significant performance gains on supported hardware.</p>\n<p><strong>Phase 3: Adaptive Mechanisms</strong>\nImplement contention-adaptive backoff and load-adaptive sizing. These features improve performance across varying workload conditions and provide valuable experience with runtime performance monitoring.</p>\n<p><strong>Phase 4: Advanced Algorithms</strong>\nProgress to wait-free algorithm implementations and specialized data structures. This phase represents the most challenging implementations but provides the deepest understanding of advanced concurrent programming techniques.</p>\n<p><strong>Phase 5: Production Integration</strong>\nFocus on language bindings, framework integration, and operational tooling. This final phase transforms research implementations into production-ready systems.</p>\n<h4 id=\"recommended-project-structure-for-extensions\">Recommended Project Structure for Extensions</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>lock-free-extensions/\n  core/                           ← Core implementations from main project\n    atomic_ops.py\n    treiber_stack.py\n    michael_scott_queue.py\n    hazard_pointers.py\n    split_ordered_hashmap.py\n  \n  advanced/                       ← Wait-free and specialized algorithms\n    wait_free/\n      universal_construction.py\n      wait_free_queue.py\n      combining_tree.py\n    specialized/\n      lock_free_tree.py\n      priority_queue.py\n      graph_structures.py\n  \n  optimizations/                  ← Performance optimizations\n    numa_aware.py\n    cache_optimized.py\n    adaptive_backoff.py\n    hardware_specific/\n      x86_atomics.py\n      arm_atomics.py\n  \n  integrations/                   ← Language and framework bindings\n    python/\n      cython_bindings.pyx\n    rust/\n      python_rust_bridge.rs\n    go/\n      cgo_bindings.go\n    frameworks/\n      django_integration.py\n      fastapi_middleware.py\n  \n  monitoring/                     ← Observability and debugging\n    performance_monitor.py\n    linearizability_checker.py\n    contention_analyzer.py\n  \n  benchmarks/                     ← Performance testing and validation\n    comparative_benchmarks.py\n    scalability_tests.py\n    correctness_verification.py\n  \n  deployment/                     ← Production deployment tooling\n    docker/\n      Dockerfile\n    kubernetes/\n      deployment.yaml\n    monitoring/\n      prometheus_config.yaml</code></pre></div>\n\n<h4 id=\"extension-development-guidelines\">Extension Development Guidelines</h4>\n<p>Each extension should maintain the same rigorous standards for correctness and testability established in the core implementation. This means comprehensive unit tests, stress testing under high contention, and formal verification of correctness properties where possible.</p>\n<p>Documentation for extensions must clearly explain the trade-offs involved and provide guidance on when each optimization or advanced algorithm is appropriate. Not every application benefits from wait-free guarantees or hardware-specific optimizations, and the added complexity must be justified by measurable improvements.</p>\n<p>Performance benchmarking becomes even more critical for extensions, as the benefits often depend on specific hardware characteristics and workload patterns. Establish baseline measurements using the core implementations, then demonstrate clear improvements under relevant conditions.</p>\n<p>The integration testing strategy should validate not just correctness but also the claimed performance benefits. Automated benchmark suites should run across different hardware configurations and workload patterns to ensure optimizations provide benefits in realistic scenarios.</p>\n<p>Finally, maintain backward compatibility with the core implementations wherever possible. Applications should be able to upgrade from basic lock-free algorithms to optimized versions without changing their usage patterns, allowing gradual adoption of more advanced techniques as requirements evolve.</p>\n<h2 id=\"glossary\">Glossary</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (this section provides essential definitions and terminology that support understanding across the entire lock-free data structure implementation journey)</p>\n</blockquote>\n<p>Understanding lock-free programming requires mastery of specialized terminology that spans hardware concepts, memory models, algorithmic techniques, and verification approaches. This glossary serves as a comprehensive reference for all technical terms, acronyms, and domain-specific vocabulary used throughout the lock-free data structures design document.</p>\n<h3 id=\"mental-model-the-technical-dictionary\">Mental Model: The Technical Dictionary</h3>\n<p>Think of this glossary as a specialized technical dictionary for a foreign language - the language of lock-free concurrent programming. Just as learning a new spoken language requires understanding not just individual words but also cultural context and subtle meanings, mastering lock-free programming requires understanding how terms relate to each other and the specific nuances they carry in this domain. Each term represents a carefully defined concept that has precise meaning within the broader ecosystem of concurrent programming theory and practice.</p>\n<h3 id=\"core-atomic-operations-and-memory-model-terms\">Core Atomic Operations and Memory Model Terms</h3>\n<p>The foundation of lock-free programming rests on atomic operations and memory ordering semantics that directly interact with hardware-level guarantees. These concepts form the building blocks upon which all higher-level algorithms depend.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context and Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>ABA problem</strong></td>\n<td>A race condition where a compare-and-swap operation incorrectly succeeds because a memory location has been changed from value A to B and back to A between the initial read and the CAS attempt</td>\n<td>Critical concern in pointer-based data structures where nodes may be deallocated and reallocated at the same address</td>\n</tr>\n<tr>\n<td><strong>Acquire memory ordering</strong></td>\n<td>Memory ordering constraint ensuring that no memory operations can be reordered before an acquire operation, establishing synchronization with corresponding release operations</td>\n<td>Used in loads to create happens-before relationships and ensure visibility of all writes that occurred before a matching release</td>\n</tr>\n<tr>\n<td><strong>Atomic operation</strong></td>\n<td>An operation that appears to execute instantaneously and indivisibly from the perspective of all other threads, with guaranteed consistency across concurrent access</td>\n<td>Forms the foundation of all lock-free algorithms by providing thread-safe primitive operations without locks</td>\n</tr>\n<tr>\n<td><strong>Compare-and-swap (CAS)</strong></td>\n<td>Fundamental atomic read-modify-write operation that updates a memory location only if it currently contains an expected value, returning both success status and the observed value</td>\n<td>The primary building block for implementing lock-free data structures, used in retry loops until successful update</td>\n</tr>\n<tr>\n<td><strong>Fetch-and-add</strong></td>\n<td>Atomic operation that increments a memory location by a specified amount and returns the previous value before the increment</td>\n<td>Commonly used for atomic counters and sequence number generation in lock-free algorithms</td>\n</tr>\n<tr>\n<td><strong>Memory barrier</strong></td>\n<td>Hardware or compiler instruction that prevents certain types of memory operation reordering, ensuring specific ordering constraints are maintained</td>\n<td>Essential for implementing memory ordering semantics and maintaining cache coherence in multi-core systems</td>\n</tr>\n<tr>\n<td><strong>Memory ordering</strong></td>\n<td>Set of rules governing the visibility and ordering of memory operations across different threads, ranging from no constraints (relaxed) to full sequential consistency</td>\n<td>Determines performance-correctness trade-offs in atomic operations, with stronger ordering providing more guarantees at higher cost</td>\n</tr>\n<tr>\n<td><strong>Relaxed memory ordering</strong></td>\n<td>Weakest memory ordering constraint that provides atomicity guarantees but allows maximum reordering flexibility for performance</td>\n<td>Used when only the atomicity of individual operations matters, not their ordering relative to other memory accesses</td>\n</tr>\n<tr>\n<td><strong>Release memory ordering</strong></td>\n<td>Memory ordering constraint ensuring that no memory operations can be reordered after a release operation, making all prior writes visible to threads performing acquire operations</td>\n<td>Used in stores to publish completed work and establish happens-before relationships with corresponding acquires</td>\n</tr>\n<tr>\n<td><strong>Sequential consistency</strong></td>\n<td>Strongest memory ordering model where operations appear to execute in some global sequential order consistent with program order on each thread</td>\n<td>Provides intuitive semantics but may limit performance optimizations, used when correctness is prioritized over maximum performance</td>\n</tr>\n<tr>\n<td><strong>Tagged pointer</strong></td>\n<td>Pointer augmented with additional bits (typically version counter or generation number) to detect ABA problems by identifying when a pointer has been reused</td>\n<td>Solves ABA problem by ensuring that even if a pointer value is reused, the tag portion will differ</td>\n</tr>\n</tbody></table>\n<h3 id=\"lock-free-algorithm-classifications-and-progress-guarantees\">Lock-free Algorithm Classifications and Progress Guarantees</h3>\n<p>Lock-free programming encompasses different levels of progress guarantees, each providing specific assurances about forward progress under concurrent access patterns.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Implications</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Lock-free</strong></td>\n<td>Non-blocking progress guarantee ensuring that at least one thread among all competing threads will complete its operation within a finite number of steps</td>\n<td>Stronger than obstruction-free but weaker than wait-free, prevents deadlock and livelock at the system level</td>\n</tr>\n<tr>\n<td><strong>Wait-free</strong></td>\n<td>Strongest progress guarantee where every thread completes its operation within a bounded number of steps regardless of other thread scheduling</td>\n<td>Most expensive to implement but provides predictable latency bounds for real-time systems</td>\n</tr>\n<tr>\n<td><strong>Obstruction-free</strong></td>\n<td>Weakest non-blocking guarantee where a thread can complete its operation if it runs in isolation for sufficiently long</td>\n<td>Easier to implement than lock-free but provides weaker progress guarantees under contention</td>\n</tr>\n<tr>\n<td><strong>Linearizability</strong></td>\n<td>Correctness condition requiring that concurrent operations appear to take effect atomically at some point between their invocation and response</td>\n<td>Gold standard for correctness in concurrent data structures, enabling reasoning about concurrent execution as sequential</td>\n</tr>\n<tr>\n<td><strong>Linearization point</strong></td>\n<td>Specific instant during an operation&#39;s execution where the operation appears to take effect atomically from the perspective of all other threads</td>\n<td>Critical for proving correctness and understanding the precise moment when concurrent operations become visible</td>\n</tr>\n</tbody></table>\n<h3 id=\"data-structure-specific-algorithms\">Data Structure Specific Algorithms</h3>\n<p>Each lock-free data structure employs specialized algorithms and techniques tailored to its specific access patterns and consistency requirements.</p>\n<p><img src=\"/api/project/lock-free-structures/architecture-doc/asset?path=diagrams%2Fdata-model-relationships.svg\" alt=\"Data Structure Relationships\"></p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Application Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Treiber stack</strong></td>\n<td>Lock-free stack algorithm using single-word compare-and-swap on the top-of-stack pointer to implement push and pop operations</td>\n<td>Simplest lock-free data structure, excellent introduction to CAS-based algorithms and ABA problem solutions</td>\n</tr>\n<tr>\n<td><strong>Michael-Scott queue</strong></td>\n<td>Lock-free FIFO queue algorithm using separate head and tail pointers with a helping mechanism to ensure progress guarantees</td>\n<td>Standard algorithm for lock-free queues, demonstrates two-pointer design and cooperative thread behavior</td>\n</tr>\n<tr>\n<td><strong>Split-ordered list</strong></td>\n<td>Hash map implementation technique that maintains logical hash ordering in a physical linked list structure across bucket boundaries</td>\n<td>Enables lock-free hash maps with incremental resizing by preserving insertion order during bucket splits</td>\n</tr>\n<tr>\n<td><strong>Helping mechanism</strong></td>\n<td>Protocol where threads assist each other to complete operations that may have been interrupted by scheduling, ensuring system-wide progress</td>\n<td>Essential for progress guarantees in complex algorithms like Michael-Scott queue and split-ordered hash maps</td>\n</tr>\n<tr>\n<td><strong>Sentinel node</strong></td>\n<td>Permanent marker nodes in data structures that define boundaries and simplify edge case handling by ensuring certain pointers are never null</td>\n<td>Simplifies queue and hash map implementations by eliminating special cases for empty structures</td>\n</tr>\n<tr>\n<td><strong>Dummy node</strong></td>\n<td>Placeholder node used to separate operations and reduce contention, particularly in queue implementations</td>\n<td>Michael-Scott queue uses dummy head node to separate enqueue and dequeue operations on different memory locations</td>\n</tr>\n</tbody></table>\n<h3 id=\"memory-reclamation-and-safety\">Memory Reclamation and Safety</h3>\n<p>Safe memory management in lock-free programming requires sophisticated techniques to prevent use-after-free errors while maintaining non-blocking properties.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Safety Guarantee</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Hazard pointers</strong></td>\n<td>Memory reclamation scheme where threads announce their intent to access shared nodes, preventing premature deallocation of nodes still in use</td>\n<td>Prevents use-after-free errors by ensuring nodes remain allocated while any thread holds a hazard pointer to them</td>\n</tr>\n<tr>\n<td><strong>Retirement list</strong></td>\n<td>Per-thread queue of nodes that have been removed from data structures but cannot yet be safely deallocated due to potential concurrent access</td>\n<td>Enables deferred deletion until global scan confirms no thread holds hazard pointers to retired nodes</td>\n</tr>\n<tr>\n<td><strong>Protect-then-verify pattern</strong></td>\n<td>Safe sequence for loading and protecting pointers using hazard pointers: load pointer, set hazard pointer, verify pointer is unchanged</td>\n<td>Prevents race conditions between loading a pointer and establishing protection against its deallocation</td>\n</tr>\n<tr>\n<td><strong>Scan threshold</strong></td>\n<td>Retirement list size that triggers a global scan of all thread hazard pointers to identify nodes safe for reclamation</td>\n<td>Balances memory usage against scan overhead by batching reclamation operations</td>\n</tr>\n<tr>\n<td><strong>Thread registry</strong></td>\n<td>Global data structure maintaining a list of all active threads and their hazard pointer slots for scanning during reclamation</td>\n<td>Enables the memory reclamation system to find all hazard pointers across all threads during scan operations</td>\n</tr>\n<tr>\n<td><strong>Epoch-based reclamation</strong></td>\n<td>Alternative memory reclamation scheme using global time periods instead of individual pointer tracking</td>\n<td>Simpler than hazard pointers but may delay reclamation longer, trades memory efficiency for implementation simplicity</td>\n</tr>\n</tbody></table>\n<h3 id=\"hash-map-specific-terminology\">Hash Map Specific Terminology</h3>\n<p>Lock-free hash maps introduce specialized concepts related to concurrent bucket management and incremental resizing operations.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Bucket initialization</strong></td>\n<td>Lazy creation of hash bucket sentinel nodes when first accessed, using compare-and-swap to ensure only one thread initializes each bucket</td>\n<td>Reduces memory usage and initialization overhead by creating buckets only when needed</td>\n</tr>\n<tr>\n<td><strong>Incremental resizing</strong></td>\n<td>Hash map expansion technique that gradually migrates entries to a larger bucket array without blocking concurrent operations</td>\n<td>Maintains lock-free properties during resize by spreading migration work across multiple operations</td>\n</tr>\n<tr>\n<td><strong>Reverse bit ordering</strong></td>\n<td>Technique of reversing the bit pattern of hash values to determine insertion order in split-ordered lists</td>\n<td>Ensures that bucket splitting preserves correct ordering when hash table size doubles</td>\n</tr>\n<tr>\n<td><strong>Load factor threshold</strong></td>\n<td>Ratio of entries to buckets that triggers resize operation to maintain acceptable performance characteristics</td>\n<td>Balances memory usage against access performance by expanding before chains become too long</td>\n</tr>\n<tr>\n<td><strong>Logical deletion</strong></td>\n<td>Marking nodes as deleted without immediately removing them from the linked list structure</td>\n<td>Enables atomic deletion in the presence of concurrent traversals by allowing physical removal as separate step</td>\n</tr>\n<tr>\n<td><strong>Physical deletion</strong></td>\n<td>Actually unlinking deleted nodes from the list structure after logical deletion has been performed</td>\n<td>Second phase of deletion that can be performed by any thread, often combined with helping mechanism</td>\n</tr>\n</tbody></table>\n<h3 id=\"concurrent-testing-and-verification\">Concurrent Testing and Verification</h3>\n<p>Verifying correctness of lock-free algorithms requires specialized testing approaches that can expose subtle race conditions and ordering violations.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Testing Application</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Stress testing</strong></td>\n<td>Testing approach that subjects data structures to high concurrent contention to expose race conditions and correctness violations</td>\n<td>Primary method for validating lock-free implementations under realistic concurrent workloads</td>\n</tr>\n<tr>\n<td><strong>Model checking</strong></td>\n<td>Exhaustive exploration of all possible thread interleavings within bounded parameters to verify correctness properties</td>\n<td>Formal verification technique that can prove correctness but limited by state space explosion</td>\n</tr>\n<tr>\n<td><strong>Randomized testing</strong></td>\n<td>Property-based testing using random operation sequences to discover corner cases and invariant violations</td>\n<td>Complements stress testing by exploring diverse operation patterns that may not occur in structured tests</td>\n</tr>\n<tr>\n<td><strong>Linearizability checking</strong></td>\n<td>Runtime verification that concurrent operations have a valid sequential ordering consistent with their real-time ordering</td>\n<td>Gold standard for correctness verification, often implemented by recording operation history and post-processing</td>\n</tr>\n<tr>\n<td><strong>Race condition detection</strong></td>\n<td>Techniques for identifying data races, ordering violations, and other concurrency bugs during testing</td>\n<td>Essential for catching subtle bugs that may not manifest consistently due to timing dependencies</td>\n</tr>\n</tbody></table>\n<h3 id=\"performance-and-optimization-terms\">Performance and Optimization Terms</h3>\n<p>Lock-free programming involves specific performance considerations and optimization techniques unique to concurrent algorithms.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Performance Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Contention hotspot</strong></td>\n<td>Memory location accessed by multiple threads simultaneously, causing cache coherence traffic and performance degradation</td>\n<td>Major source of scalability bottlenecks in concurrent data structures, requires careful algorithm design to minimize</td>\n</tr>\n<tr>\n<td><strong>False sharing</strong></td>\n<td>Performance problem where unrelated variables sharing the same cache line cause unnecessary coherence traffic when accessed by different threads</td>\n<td>Can severely degrade performance even when variables are not logically shared, requires careful memory layout</td>\n</tr>\n<tr>\n<td><strong>Exponential backoff</strong></td>\n<td>Delay strategy that increases wait time after each failure, reducing contention when many threads compete for the same resource</td>\n<td>Balances progress against wasted CPU cycles by reducing retry frequency under high contention</td>\n</tr>\n<tr>\n<td><strong>CAS loop spinning</strong></td>\n<td>Continuous retry pattern where threads repeatedly attempt compare-and-swap operations until successful</td>\n<td>Can waste CPU resources under high contention but necessary for lock-free progress guarantees</td>\n</tr>\n<tr>\n<td><strong>Cache line alignment</strong></td>\n<td>Memory layout technique ensuring that frequently accessed data structures align with processor cache line boundaries</td>\n<td>Improves performance by reducing cache misses and false sharing between unrelated data</td>\n</tr>\n<tr>\n<td><strong>NUMA awareness</strong></td>\n<td>Optimization technique that considers Non-Uniform Memory Access costs when allocating and accessing data structures</td>\n<td>Increasingly important on large multi-socket systems where memory access costs vary by location</td>\n</tr>\n</tbody></table>\n<h3 id=\"error-handling-and-debugging\">Error Handling and Debugging</h3>\n<p>Lock-free programming introduces unique failure modes and debugging challenges that require specialized approaches and terminology.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Debugging Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Livelock</strong></td>\n<td>Condition where threads are actively making progress individually but collectively make no forward progress due to mutual interference</td>\n<td>Difficult to detect as threads appear active, requires analysis of global progress rather than individual thread activity</td>\n</tr>\n<tr>\n<td><strong>Memory reclamation race</strong></td>\n<td>Timing-dependent failure where memory is deallocated while other threads still hold references, causing use-after-free errors</td>\n<td>Particularly challenging to debug as symptoms may appear far from the actual cause in time and code location</td>\n</tr>\n<tr>\n<td><strong>Progress guarantee violation</strong></td>\n<td>Failure of a lock-free algorithm to ensure that at least one thread makes progress within bounded steps</td>\n<td>Indicates fundamental algorithm correctness issues rather than simple implementation bugs</td>\n</tr>\n<tr>\n<td><strong>Invariant violation</strong></td>\n<td>Condition where data structure consistency properties are temporarily or permanently broken due to concurrent access races</td>\n<td>Requires continuous monitoring during testing as violations may be transient and timing-dependent</td>\n</tr>\n<tr>\n<td><strong>ABA manifestation</strong></td>\n<td>Actual occurrence of ABA problem causing incorrect algorithm behavior due to pointer reuse between read and update operations</td>\n<td>Often appears as mysterious data corruption or lost updates that seem impossible given the algorithm logic</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-and-system-integration\">Implementation and System Integration</h3>\n<p>Terms related to practical implementation concerns and integration of lock-free data structures into larger systems.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Integration Concern</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Graceful degradation</strong></td>\n<td>System behavior that maintains correctness and basic functionality even when performance optimization fail or extreme conditions occur</td>\n<td>Critical for production systems where lock-free algorithms must handle unexpected load patterns or failure modes</td>\n</tr>\n<tr>\n<td><strong>Hybrid approach</strong></td>\n<td>Combining lock-free algorithms with traditional locking or other concurrency techniques where appropriate for different use cases</td>\n<td>Recognizes that lock-free is not always optimal and allows mixing approaches based on specific requirements</td>\n</tr>\n<tr>\n<td><strong>Universal construction</strong></td>\n<td>Theoretical technique for building wait-free implementations of any data structure from simpler atomic primitives</td>\n<td>Primarily of academic interest but provides foundation for understanding what is theoretically possible</td>\n</tr>\n<tr>\n<td><strong>Flat combining</strong></td>\n<td>Hybrid approach where threads occasionally take exclusive access to perform batched operations, combining benefits of locking and lock-free approaches</td>\n<td>Practical technique for workloads where batching provides significant benefits over pure lock-free approaches</td>\n</tr>\n</tbody></table>\n<h3 id=\"advanced-concepts-and-future-directions\">Advanced Concepts and Future Directions</h3>\n<p>Terminology related to cutting-edge research and advanced techniques in lock-free programming.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Research Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Hardware transactional memory</strong></td>\n<td>CPU support for atomic execution of code blocks, providing transactional semantics at the hardware level</td>\n<td>Emerging technology that may simplify lock-free programming by providing stronger atomic operation primitives</td>\n</tr>\n<tr>\n<td><strong>Wait-free universal construction</strong></td>\n<td>Theoretical framework for building wait-free implementations of arbitrary data structures using consensus objects</td>\n<td>Demonstrates theoretical limits of what can be achieved but often impractical for real implementations</td>\n</tr>\n<tr>\n<td><strong>Obstruction-free universal construction</strong></td>\n<td>More practical approach for building non-blocking implementations using simpler primitives than full wait-free constructions</td>\n<td>Bridge between theory and practice, providing stronger guarantees than locks with more reasonable implementation complexity</td>\n</tr>\n<tr>\n<td><strong>Read-copy-update (RCU)</strong></td>\n<td>Synchronization mechanism allowing readers to access data structures without synchronization while writers create new versions</td>\n<td>Specialized technique particularly effective for read-heavy workloads with infrequent updates</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The terminology presented in this glossary provides the vocabulary necessary to understand and discuss lock-free programming concepts precisely. When implementing lock-free data structures, developers should refer to these definitions to ensure consistent understanding and communication.</p>\n<h4 id=\"terminology-usage-guidelines\">Terminology Usage Guidelines</h4>\n<table>\n<thead>\n<tr>\n<th>Context</th>\n<th>Recommended Terms</th>\n<th>Terms to Avoid</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Algorithm Description</strong></td>\n<td>Use &quot;compare-and-swap&quot;, &quot;linearization point&quot;, &quot;helping mechanism&quot;</td>\n<td>Avoid &quot;atomic update&quot;, &quot;synchronization point&quot;, &quot;cooperation&quot;</td>\n</tr>\n<tr>\n<td><strong>Memory Management</strong></td>\n<td>Use &quot;hazard pointers&quot;, &quot;retirement list&quot;, &quot;protect-then-verify pattern&quot;</td>\n<td>Avoid &quot;garbage collection&quot;, &quot;reference counting&quot;, &quot;smart pointers&quot;</td>\n</tr>\n<tr>\n<td><strong>Performance Discussion</strong></td>\n<td>Use &quot;contention hotspot&quot;, &quot;false sharing&quot;, &quot;exponential backoff&quot;</td>\n<td>Avoid &quot;bottleneck&quot;, &quot;cache miss&quot;, &quot;retry delay&quot;</td>\n</tr>\n<tr>\n<td><strong>Correctness Verification</strong></td>\n<td>Use &quot;linearizability&quot;, &quot;stress testing&quot;, &quot;invariant violation&quot;</td>\n<td>Avoid &quot;thread safety&quot;, &quot;load testing&quot;, &quot;assertion failure&quot;</td>\n</tr>\n</tbody></table>\n<h4 id=\"common-terminology-mistakes\">Common Terminology Mistakes</h4>\n<p>⚠️ <strong>Pitfall: Confusing Memory Ordering Terms</strong>\nMany developers incorrectly use &quot;memory barrier&quot; and &quot;memory fence&quot; interchangeably with &quot;memory ordering&quot;. Memory ordering refers to the semantic guarantees, while barriers and fences are the mechanisms that implement those guarantees. Use &quot;acquire ordering&quot; or &quot;release ordering&quot; when discussing semantics, and &quot;memory barrier&quot; when discussing implementation mechanisms.</p>\n<p>⚠️ <strong>Pitfall: Misusing Progress Guarantee Terminology</strong>\nThe terms &quot;lock-free&quot;, &quot;wait-free&quot;, and &quot;obstruction-free&quot; have precise technical meanings that are often misused. &quot;Lock-free&quot; does not simply mean &quot;without locks&quot; - it specifies a particular progress guarantee. Always use these terms according to their formal definitions rather than intuitive interpretations.</p>\n<p>⚠️ <strong>Pitfall: Conflating ABA Problem with General Race Conditions</strong>\nThe ABA problem is a specific type of race condition related to pointer reuse in compare-and-swap operations. Not all race conditions are ABA problems, and not all pointer-related bugs are ABA manifestations. Use &quot;ABA problem&quot; only when specifically referring to the scenario where CAS succeeds incorrectly due to pointer reuse.</p>\n<h4 id=\"glossary-maintenance-and-evolution\">Glossary Maintenance and Evolution</h4>\n<p>This glossary represents the current state of terminology in lock-free programming. As the field evolves and new techniques emerge, terminology may be refined or extended. When encountering new terms or concepts, developers should:</p>\n<ol>\n<li><strong>Verify formal definitions</strong> from authoritative sources such as academic papers or standards documents</li>\n<li><strong>Understand context-specific meanings</strong> that may differ from general usage in other domains  </li>\n<li><strong>Maintain consistency</strong> with established terminology when describing algorithms or implementations</li>\n<li><strong>Document new terms</strong> that emerge from practical experience or research developments</li>\n</ol>\n<p>The precision of terminology directly impacts the ability to reason about correctness, communicate design decisions, and maintain implementations over time. Mastery of this vocabulary is essential for anyone working with lock-free data structures.</p>\n","toc":[{"level":1,"text":"Lock-free Data Structures: Design Document","id":"lock-free-data-structures-design-document"},{"level":2,"text":"Overview","id":"overview"},{"level":2,"text":"Context and Problem Statement","id":"context-and-problem-statement"},{"level":3,"text":"Mental Model: The Crowded Kitchen Analogy","id":"mental-model-the-crowded-kitchen-analogy"},{"level":3,"text":"Limitations of Lock-Based Concurrency","id":"limitations-of-lock-based-concurrency"},{"level":3,"text":"Comparison of Concurrency Approaches","id":"comparison-of-concurrency-approaches"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Goals and Non-Goals","id":"goals-and-non-goals"},{"level":3,"text":"Mental Model: The Precision Tool Workshop","id":"mental-model-the-precision-tool-workshop"},{"level":2,"text":"Functional Goals","id":"functional-goals"},{"level":3,"text":"Thread-Safe Operations Without Data Races","id":"thread-safe-operations-without-data-races"},{"level":3,"text":"FIFO Ordering Preservation","id":"fifo-ordering-preservation"},{"level":3,"text":"Linearizability as the Correctness Standard","id":"linearizability-as-the-correctness-standard"},{"level":3,"text":"Safe Memory Reclamation","id":"safe-memory-reclamation"},{"level":2,"text":"Performance Goals","id":"performance-goals"},{"level":3,"text":"High Throughput Under Contention","id":"high-throughput-under-contention"},{"level":3,"text":"Scalability Across CPU Cores","id":"scalability-across-cpu-cores"},{"level":3,"text":"Minimal Latency for Individual Operations","id":"minimal-latency-for-individual-operations"},{"level":3,"text":"Deterministic Performance Characteristics","id":"deterministic-performance-characteristics"},{"level":2,"text":"Explicit Non-Goals","id":"explicit-non-goals"},{"level":3,"text":"Blocking Operations and Wait-Free Guarantees","id":"blocking-operations-and-wait-free-guarantees"},{"level":3,"text":"Automatic Garbage Collection Integration","id":"automatic-garbage-collection-integration"},{"level":3,"text":"Complex Data Structures Beyond Fundamentals","id":"complex-data-structures-beyond-fundamentals"},{"level":3,"text":"Dynamic Memory Pool Management","id":"dynamic-memory-pool-management"},{"level":3,"text":"Language-Specific Optimizations","id":"language-specific-optimizations"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Project Structure","id":"recommended-project-structure"},{"level":4,"text":"Goal Verification Framework","id":"goal-verification-framework"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":2,"text":"High-Level Architecture","id":"high-level-architecture"},{"level":3,"text":"Component Layers","id":"component-layers"},{"level":4,"text":"Layer 1: Atomic Operations Foundation","id":"layer-1-atomic-operations-foundation"},{"level":4,"text":"Layer 2: Lock-free Data Structures","id":"layer-2-lock-free-data-structures"},{"level":4,"text":"Layer 3: Memory Reclamation Management","id":"layer-3-memory-reclamation-management"},{"level":3,"text":"Recommended Module Structure","id":"recommended-module-structure"},{"level":3,"text":"Implementation Progression Strategy","id":"implementation-progression-strategy"},{"level":4,"text":"Phase 1: Atomic Operations Mastery (Milestone 1)","id":"phase-1-atomic-operations-mastery-milestone-1"},{"level":4,"text":"Phase 2: Single Data Structure Implementation (Milestones 2-3)","id":"phase-2-single-data-structure-implementation-milestones-2-3"},{"level":4,"text":"Phase 3: Memory Reclamation Integration (Milestone 4)","id":"phase-3-memory-reclamation-integration-milestone-4"},{"level":4,"text":"Phase 4: Advanced Data Structures (Milestone 5)","id":"phase-4-advanced-data-structures-milestone-5"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure for Implementation","id":"recommended-file-structure-for-implementation"},{"level":4,"text":"Infrastructure Starter Code (Complete Implementation)","id":"infrastructure-starter-code-complete-implementation"},{"level":4,"text":"Core Logic Skeleton Code (For Learner Implementation)","id":"core-logic-skeleton-code-for-learner-implementation"},{"level":4,"text":"Milestone Checkpoint Guidelines","id":"milestone-checkpoint-guidelines"},{"level":4,"text":"Language-Specific Implementation Hints","id":"language-specific-implementation-hints"},{"level":2,"text":"Atomic Operations Foundation","id":"atomic-operations-foundation"},{"level":3,"text":"Mental Model: Atomic Operations as Indivisible Transactions","id":"mental-model-atomic-operations-as-indivisible-transactions"},{"level":3,"text":"Memory Ordering and Consistency Models","id":"memory-ordering-and-consistency-models"},{"level":3,"text":"Compare-and-Swap (CAS) Operation","id":"compare-and-swap-cas-operation"},{"level":3,"text":"ABA Problem and Solutions","id":"aba-problem-and-solutions"},{"level":3,"text":"Common Pitfalls with Atomics","id":"common-pitfalls-with-atomics"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Lock-free Stack (Treiber Stack)","id":"lock-free-stack-treiber-stack"},{"level":3,"text":"Mental Model: Stack as a Shared Notepad","id":"mental-model-stack-as-a-shared-notepad"},{"level":3,"text":"Treiber Stack Algorithm","id":"treiber-stack-algorithm"},{"level":3,"text":"Linearization Points and Correctness","id":"linearization-points-and-correctness"},{"level":3,"text":"ABA Problem in Stack Context","id":"aba-problem-in-stack-context"},{"level":3,"text":"Common Stack Implementation Pitfalls","id":"common-stack-implementation-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Lock-free Queue (Michael-Scott Algorithm)","id":"lock-free-queue-michael-scott-algorithm"},{"level":3,"text":"Mental Model: Queue as a Conveyor Belt","id":"mental-model-queue-as-a-conveyor-belt"},{"level":3,"text":"Michael-Scott Queue Algorithm","id":"michael-scott-queue-algorithm"},{"level":4,"text":"Enqueue Operation Protocol","id":"enqueue-operation-protocol"},{"level":4,"text":"Dequeue Operation Protocol","id":"dequeue-operation-protocol"},{"level":3,"text":"Dummy Sentinel Node Design","id":"dummy-sentinel-node-design"},{"level":3,"text":"Helping Mechanism for Progress","id":"helping-mechanism-for-progress"},{"level":4,"text":"Detecting When Help is Needed","id":"detecting-when-help-is-needed"},{"level":4,"text":"The Helping Protocol","id":"the-helping-protocol"},{"level":4,"text":"Linearization Points and Helping","id":"linearization-points-and-helping"},{"level":4,"text":"Helping in Dequeue Operations","id":"helping-in-dequeue-operations"},{"level":3,"text":"Common Queue Implementation Pitfalls","id":"common-queue-implementation-pitfalls"},{"level":4,"text":"⚠️ Pitfall: Incorrect Tail Pointer Management","id":"-pitfall-incorrect-tail-pointer-management"},{"level":4,"text":"⚠️ Pitfall: Sentinel Node Confusion","id":"-pitfall-sentinel-node-confusion"},{"level":4,"text":"⚠️ Pitfall: ABA Problems in Dual-Pointer Context","id":"-pitfall-aba-problems-in-dual-pointer-context"},{"level":4,"text":"⚠️ Pitfall: Linearization Edge Cases","id":"-pitfall-linearization-edge-cases"},{"level":4,"text":"⚠️ Pitfall: Memory Ordering and Visibility","id":"-pitfall-memory-ordering-and-visibility"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton","id":"core-logic-skeleton"},{"level":4,"text":"Language-Specific Implementation Hints","id":"language-specific-implementation-hints"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Hazard Pointers for Memory Reclamation","id":"hazard-pointers-for-memory-reclamation"},{"level":3,"text":"Mental Model: Hazard Pointers as Safety Signs","id":"mental-model-hazard-pointers-as-safety-signs"},{"level":3,"text":"Hazard Pointer Protocol","id":"hazard-pointer-protocol"},{"level":3,"text":"Retirement List and Scanning Algorithm","id":"retirement-list-and-scanning-algorithm"},{"level":3,"text":"Integration with Lock-free Data Structures","id":"integration-with-lock-free-data-structures"},{"level":3,"text":"Common Hazard Pointer Pitfalls","id":"common-hazard-pointer-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Language-Specific Hints","id":"language-specific-hints"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":2,"text":"Lock-free Hash Map","id":"lock-free-hash-map"},{"level":3,"text":"Mental Model: Hash Map as a Growing Office Building","id":"mental-model-hash-map-as-a-growing-office-building"},{"level":3,"text":"Split-Ordered List Design","id":"split-ordered-list-design"},{"level":3,"text":"Incremental Resizing Algorithm","id":"incremental-resizing-algorithm"},{"level":3,"text":"Lock-free Bucket Operations","id":"lock-free-bucket-operations"},{"level":3,"text":"Common Hash Map Implementation Pitfalls","id":"common-hash-map-implementation-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Interactions and Data Flow","id":"interactions-and-data-flow"},{"level":3,"text":"Mental Model: The Relay Race Coordination","id":"mental-model-the-relay-race-coordination"},{"level":3,"text":"Operation Sequence Patterns","id":"operation-sequence-patterns"},{"level":4,"text":"CAS Retry Loop Pattern","id":"cas-retry-loop-pattern"},{"level":4,"text":"Helping Protocol Pattern","id":"helping-protocol-pattern"},{"level":4,"text":"Hazard Pointer Integration Pattern","id":"hazard-pointer-integration-pattern"},{"level":3,"text":"Component Interface Contracts","id":"component-interface-contracts"},{"level":4,"text":"Atomic Operations Layer Interface","id":"atomic-operations-layer-interface"},{"level":4,"text":"Data Structure Layer Interface","id":"data-structure-layer-interface"},{"level":4,"text":"Memory Reclamation Layer Interface","id":"memory-reclamation-layer-interface"},{"level":3,"text":"Memory Lifecycle and Flow","id":"memory-lifecycle-and-flow"},{"level":4,"text":"Node Allocation and Initial State","id":"node-allocation-and-initial-state"},{"level":4,"text":"Concurrent Access and Protection Phase","id":"concurrent-access-and-protection-phase"},{"level":4,"text":"Logical Deletion and Marking","id":"logical-deletion-and-marking"},{"level":4,"text":"Retirement and Deferred Reclamation","id":"retirement-and-deferred-reclamation"},{"level":4,"text":"Scan and Reclamation Process","id":"scan-and-reclamation-process"},{"level":4,"text":"Thread Exit Cleanup","id":"thread-exit-cleanup"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Error Handling and Edge Cases","id":"error-handling-and-edge-cases"},{"level":3,"text":"Mental Model: Lock-free Errors as Traffic Intersection Problems","id":"mental-model-lock-free-errors-as-traffic-intersection-problems"},{"level":3,"text":"Lock-free Specific Failure Modes","id":"lock-free-specific-failure-modes"},{"level":4,"text":"ABA Problem Manifestations","id":"aba-problem-manifestations"},{"level":4,"text":"Livelock and Progress Guarantee Violations","id":"livelock-and-progress-guarantee-violations"},{"level":4,"text":"Memory Reclamation Races","id":"memory-reclamation-races"},{"level":4,"text":"Progress Guarantee Violations","id":"progress-guarantee-violations"},{"level":3,"text":"Failure Detection Strategies","id":"failure-detection-strategies"},{"level":4,"text":"Invariant Violation Detection","id":"invariant-violation-detection"},{"level":4,"text":"Performance Degradation Monitoring","id":"performance-degradation-monitoring"},{"level":4,"text":"Correctness Verification Through Linearizability Checking","id":"correctness-verification-through-linearizability-checking"},{"level":3,"text":"Recovery and Fallback Mechanisms","id":"recovery-and-fallback-mechanisms"},{"level":4,"text":"Exponential Backoff Strategies","id":"exponential-backoff-strategies"},{"level":4,"text":"Helping Protocol Recovery","id":"helping-protocol-recovery"},{"level":4,"text":"Graceful Degradation Under Extreme Contention","id":"graceful-degradation-under-extreme-contention"},{"level":4,"text":"Memory Reclamation Recovery","id":"memory-reclamation-recovery"},{"level":3,"text":"Common Recovery Implementation Pitfalls","id":"common-recovery-implementation-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Module Structure","id":"recommended-module-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Testing Strategy","id":"testing-strategy"},{"level":3,"text":"Mental Model: Testing as Archaeological Investigation","id":"mental-model-testing-as-archaeological-investigation"},{"level":3,"text":"Correctness Properties to Verify","id":"correctness-properties-to-verify"},{"level":4,"text":"Linearizability: The Gold Standard","id":"linearizability-the-gold-standard"},{"level":4,"text":"Progress Guarantees: Ensuring Forward Motion","id":"progress-guarantees-ensuring-forward-motion"},{"level":4,"text":"Memory Safety: Preventing Use-After-Free","id":"memory-safety-preventing-use-after-free"},{"level":4,"text":"Functional Correctness: Data Structure Semantics","id":"functional-correctness-data-structure-semantics"},{"level":3,"text":"Concurrency Testing Techniques","id":"concurrency-testing-techniques"},{"level":4,"text":"Stress Testing: Controlled Chaos","id":"stress-testing-controlled-chaos"},{"level":4,"text":"Model Checking: Exhaustive Exploration","id":"model-checking-exhaustive-exploration"},{"level":4,"text":"Randomized Testing: Property-Based Verification","id":"randomized-testing-property-based-verification"},{"level":3,"text":"Milestone Verification Checkpoints","id":"milestone-verification-checkpoints"},{"level":4,"text":"Milestone 1: Atomic Operations Foundation","id":"milestone-1-atomic-operations-foundation"},{"level":4,"text":"Milestone 2: Lock-free Stack Implementation","id":"milestone-2-lock-free-stack-implementation"},{"level":4,"text":"Milestone 3: Michael-Scott Queue Implementation","id":"milestone-3-michael-scott-queue-implementation"},{"level":4,"text":"Milestone 4: Hazard Pointers Integration","id":"milestone-4-hazard-pointers-integration"},{"level":4,"text":"Milestone 5: Lock-free Hash Map Completion","id":"milestone-5-lock-free-hash-map-completion"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Testing Module Structure","id":"recommended-testing-module-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Testing Logic Skeletons","id":"core-testing-logic-skeletons"},{"level":4,"text":"Language-Specific Hints","id":"language-specific-hints"},{"level":4,"text":"Milestone Checkpoint Commands","id":"milestone-checkpoint-commands"},{"level":2,"text":"Debugging Guide","id":"debugging-guide"},{"level":3,"text":"Mental Model: Debugging as Detective Work in a Parallel Universe","id":"mental-model-debugging-as-detective-work-in-a-parallel-universe"},{"level":3,"text":"Common Bug Patterns","id":"common-bug-patterns"},{"level":3,"text":"Lock-free Debugging Techniques","id":"lock-free-debugging-techniques"},{"level":4,"text":"Operation Recording and Replay","id":"operation-recording-and-replay"},{"level":4,"text":"Thread-Safe Logging Strategies","id":"thread-safe-logging-strategies"},{"level":4,"text":"State Inspection Without Locks","id":"state-inspection-without-locks"},{"level":3,"text":"Performance Issue Diagnosis","id":"performance-issue-diagnosis"},{"level":4,"text":"Contention Hotspot Analysis","id":"contention-hotspot-analysis"},{"level":4,"text":"False Sharing Detection","id":"false-sharing-detection"},{"level":4,"text":"Scalability Bottleneck Identification","id":"scalability-bottleneck-identification"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Module Structure","id":"recommended-module-structure"},{"level":4,"text":"Infrastructure: Complete Operation Recording System","id":"infrastructure-complete-operation-recording-system"},{"level":4,"text":"Infrastructure: Performance Monitoring System","id":"infrastructure-performance-monitoring-system"},{"level":4,"text":"Core Logic: Stress Testing Framework","id":"core-logic-stress-testing-framework"},{"level":4,"text":"Core Logic: Debugging Instrumentation Hooks","id":"core-logic-debugging-instrumentation-hooks"},{"level":4,"text":"Milestone Checkpoint: Debugging Infrastructure Validation","id":"milestone-checkpoint-debugging-infrastructure-validation"},{"level":2,"text":"Future Extensions","id":"future-extensions"},{"level":3,"text":"Advanced Lock-free Algorithms","id":"advanced-lock-free-algorithms"},{"level":3,"text":"Performance Optimizations","id":"performance-optimizations"},{"level":3,"text":"Integration and Ecosystem","id":"integration-and-ecosystem"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Advanced Algorithm Implementation Strategy","id":"advanced-algorithm-implementation-strategy"},{"level":4,"text":"Technology Recommendations for Extensions","id":"technology-recommendations-for-extensions"},{"level":4,"text":"Development Roadmap for Extensions","id":"development-roadmap-for-extensions"},{"level":4,"text":"Recommended Project Structure for Extensions","id":"recommended-project-structure-for-extensions"},{"level":4,"text":"Extension Development Guidelines","id":"extension-development-guidelines"},{"level":2,"text":"Glossary","id":"glossary"},{"level":3,"text":"Mental Model: The Technical Dictionary","id":"mental-model-the-technical-dictionary"},{"level":3,"text":"Core Atomic Operations and Memory Model Terms","id":"core-atomic-operations-and-memory-model-terms"},{"level":3,"text":"Lock-free Algorithm Classifications and Progress Guarantees","id":"lock-free-algorithm-classifications-and-progress-guarantees"},{"level":3,"text":"Data Structure Specific Algorithms","id":"data-structure-specific-algorithms"},{"level":3,"text":"Memory Reclamation and Safety","id":"memory-reclamation-and-safety"},{"level":3,"text":"Hash Map Specific Terminology","id":"hash-map-specific-terminology"},{"level":3,"text":"Concurrent Testing and Verification","id":"concurrent-testing-and-verification"},{"level":3,"text":"Performance and Optimization Terms","id":"performance-and-optimization-terms"},{"level":3,"text":"Error Handling and Debugging","id":"error-handling-and-debugging"},{"level":3,"text":"Implementation and System Integration","id":"implementation-and-system-integration"},{"level":3,"text":"Advanced Concepts and Future Directions","id":"advanced-concepts-and-future-directions"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Terminology Usage Guidelines","id":"terminology-usage-guidelines"},{"level":4,"text":"Common Terminology Mistakes","id":"common-terminology-mistakes"},{"level":4,"text":"Glossary Maintenance and Evolution","id":"glossary-maintenance-and-evolution"}],"title":"Lock-free Data Structures: Design Document","markdown":"# Lock-free Data Structures: Design Document\n\n\n## Overview\n\nThis system implements concurrent data structures using atomic operations and compare-and-swap (CAS) primitives instead of traditional locks. The key architectural challenge is achieving thread-safe operations while maintaining performance and avoiding deadlocks, race conditions, and memory reclamation issues inherent in lock-free programming.\n\n\n> This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.\n\n\n## Context and Problem Statement\n\n> **Milestone(s):** All milestones (foundational concepts underlying the entire project)\n\nThe world of concurrent programming presents a fundamental challenge: how do we coordinate multiple threads accessing shared data without sacrificing performance or correctness? Traditional approaches rely heavily on locks—mutexes, semaphores, and other blocking synchronization primitives that ensure mutual exclusion. While conceptually straightforward, these mechanisms introduce significant bottlenecks in high-performance systems where dozens or hundreds of threads compete for shared resources.\n\nLock-free programming represents a paradigm shift in concurrent system design. Instead of using locks to serialize access to shared data, lock-free algorithms rely on atomic operations and clever coordination protocols to allow multiple threads to make progress simultaneously. This approach promises higher throughput, better scalability, and elimination of common concurrency hazards like deadlocks and priority inversion. However, these benefits come at the cost of significantly increased algorithmic complexity and subtle correctness challenges that can confound even experienced developers.\n\n### Mental Model: The Crowded Kitchen Analogy\n\nTo understand the fundamental difference between lock-based and lock-free approaches, imagine a busy restaurant kitchen during the dinner rush. The kitchen has shared resources: cutting boards, knives, stoves, and prep stations that multiple chefs need to use simultaneously.\n\n**The Lock-Based Kitchen** operates like a traditional workspace with strict protocols. Each shared resource has a single key, and chefs must obtain the key before using any equipment. When Chef Alice needs the main cutting board, she takes its key, preventing anyone else from using it until she's completely finished and returns the key. If Chef Bob needs both the cutting board and the meat grinder simultaneously, he must acquire both keys in a predetermined order to avoid deadlocks. When Chef Carol arrives and needs the cutting board, she stands idle, waiting for Alice to finish—even if Alice is taking a smoke break while holding the key.\n\nThis system ensures safety (no two chefs accidentally interfere with each other's work), but creates obvious inefficiencies. Chefs spend significant time waiting for keys, productivity suffers when key-holders are interrupted or delayed, and the entire kitchen can grind to a halt if someone forgets to return a key or if two chefs try to acquire keys in different orders.\n\n**The Lock-Free Kitchen** operates on a radically different principle: optimistic coordination without exclusive ownership. Instead of keys, chefs use a \"try-and-verify\" approach. When Chef Alice needs to prep vegetables, she approaches the cutting board, checks if it's available, and begins working. If another chef starts using the same space, they negotiate in real-time—perhaps Alice moves to a different section of the board, or switches to a different cutting board entirely. The key insight is that chefs don't block each other; they adapt and continue working.\n\nIn this system, Chef Bob doesn't wait for permission to start his mise en place. He begins work optimistically, knowing he might need to adjust his approach if conflicts arise. When Chef Carol needs workspace, she doesn't stand idle—she finds an alternative approach or waits for just the brief moment when she can safely make her contribution. The kitchen maintains higher throughput because chefs spend their time cooking rather than managing keys.\n\nHowever, the lock-free kitchen requires more sophisticated chefs. They must constantly verify their work hasn't been disrupted by others, have backup plans when conflicts arise, and coordinate through subtle communication protocols. A mistake in coordination can lead to ruined dishes or wasted ingredients—the equivalent of data corruption in concurrent systems.\n\n**The Memory Reclamation Challenge** extends our analogy to kitchen cleanup. In the lock-based kitchen, cleanup is straightforward: when a chef finishes with equipment and returns the key, the dishwasher can immediately clean it because no one else could be using it. In the lock-free kitchen, the situation is more complex. Even after Chef Alice finishes with a cutting board and walks away, Chef Bob might still be using vegetables she prepared on that board. The dishwasher can't immediately sanitize the board without potentially disrupting Bob's work. Instead, the kitchen needs a more sophisticated protocol to determine when resources are truly safe to clean—this is the essence of the memory reclamation problem in lock-free systems.\n\n### Limitations of Lock-Based Concurrency\n\nTraditional mutex-based synchronization suffers from several fundamental limitations that become increasingly problematic as system scale and performance requirements grow. Understanding these limitations provides the motivation for exploring lock-free alternatives, despite their increased complexity.\n\n**Performance Bottlenecks and Scalability Issues**\n\nLocks introduce serialization points that fundamentally limit parallelism. When multiple threads contend for the same mutex, only one can make progress while others block, waiting in the kernel's scheduler queue. This blocking behavior creates several performance pathologies. First, **context switching overhead** becomes significant under contention. When a thread blocks on a mutex, the operating system must save its complete execution context, select another thread to run, and restore that thread's context—a process that can take thousands of CPU cycles. Under heavy contention, threads may spend more time context switching than doing useful work.\n\nSecond, **cache coherency traffic** increases dramatically with lock contention. Modern CPUs maintain cache coherence through protocols like MESI, where cache lines bounce between cores as different threads acquire and release locks. A heavily contended mutex can cause its cache line to ping-pong between CPU cores, generating expensive memory bus traffic and cache misses. This effect is particularly pronounced on NUMA systems where cross-socket memory access latencies are significantly higher than local access.\n\nThird, **convoy effects** emerge when a lock-holding thread is preempted or interrupted. If the operating system preempts a thread while it holds a critical mutex, all other threads requiring that mutex must wait until the preempted thread is rescheduled and releases the lock. This can cause cascading delays where dozens of threads become synchronized to the scheduling quantum of a single unfortunate thread.\n\n**Deadlock and Livelock Vulnerabilities**\n\nLock-based systems are inherently susceptible to deadlock conditions where two or more threads wait indefinitely for each other to release resources. Consider a hash table implementation that uses per-bucket locks for fine-grained concurrency. If Thread A acquires the lock for bucket 5 and then needs bucket 12, while Thread B simultaneously acquires bucket 12's lock and needs bucket 5, both threads will wait forever. Preventing deadlocks requires careful lock ordering protocols, but these become increasingly complex as the number of locks grows.\n\n**Lock ordering discipline** becomes a significant maintenance burden in large codebases. Developers must remember to always acquire locks in a predetermined global order, even when the natural algorithm flow suggests a different sequence. This constraint often forces awkward code structures and makes otherwise simple operations complex. Worse, deadlock bugs are notoriously difficult to reproduce and debug because they depend on precise timing conditions that may not manifest during testing.\n\n**Livelock** presents a related problem where threads avoid deadlock by backing off and retrying, but end up in a situation where they continuously interfere with each other without making progress. This is particularly common in systems that use timeouts and retry logic to handle deadlock detection.\n\n**Priority Inversion and Fairness Issues**\n\nPriority inversion occurs when a high-priority thread is blocked waiting for a lock held by a low-priority thread, effectively running at the low-priority thread's scheduling level. This problem becomes severe when a medium-priority thread preempts the low-priority lock holder, preventing it from releasing the lock and allowing the high-priority thread to proceed. While solutions like priority inheritance exist, they add complexity to the kernel scheduler and don't eliminate the fundamental issue.\n\n**Lock fairness** presents another challenge. Most mutex implementations don't guarantee fair access under contention—a thread might repeatedly lose the race to acquire a lock while other threads succeed. This can lead to starvation scenarios where some threads make no progress for extended periods. Implementing fair locks typically requires more complex algorithms that further hurt performance.\n\n**Composability and Modularity Problems**\n\nLock-based designs suffer from poor composability. When combining multiple lock-protected data structures, the resulting system often requires careful analysis to ensure deadlock freedom and reasonable performance. Simple operations that should compose naturally become complex when lock ordering requirements are considered.\n\nFor example, consider implementing a transfer operation between two concurrent bank accounts, each protected by its own mutex. The operation requires acquiring both account locks, but the natural implementation `transfer(from_account, to_account, amount)` creates a potential deadlock if another thread simultaneously calls `transfer(to_account, from_account, other_amount)`. Solving this requires imposing an artificial ordering (perhaps by account number) that has nothing to do with the business logic.\n\n**Testing and Debugging Challenges**\n\nConcurrent bugs in lock-based systems are notoriously difficult to reproduce and debug. Race conditions often manifest only under specific timing conditions that are hard to recreate in development environments. Traditional debugging tools like debuggers and print statements can alter timing enough to hide bugs—the infamous \"Heisenbug\" effect.\n\n**Lock contention profiling** requires sophisticated tools to understand where threads spend time waiting, and the results often vary significantly between different hardware configurations, load patterns, and even compiler optimizations. This makes performance optimization a challenging iterative process.\n\n> **Key Insight**: The fundamental problem with locks is that they solve the coordination problem by eliminating parallelism. Every critical section protected by a mutex becomes a sequential bottleneck, and the cumulative effect of these bottlenecks limits system scalability regardless of how many CPU cores are available.\n\n### Comparison of Concurrency Approaches\n\nUnderstanding the trade-offs between different concurrency paradigms is essential for making informed architectural decisions. Each approach represents a different point in the design space, trading off implementation complexity, performance characteristics, and correctness guarantees.\n\n> **Decision: Concurrency Model Selection Framework**\n> - **Context**: Modern systems must handle increasing concurrency demands while maintaining correctness and performance. Different concurrency models offer varying trade-offs between implementation complexity, performance characteristics, and correctness guarantees.\n> - **Options Considered**: Lock-based mutual exclusion, lock-free algorithms with atomic operations, wait-free algorithms with universal constructions, and hybrid approaches combining multiple techniques\n> - **Decision**: Implement a progression from lock-based through lock-free to demonstrate the trade-offs, focusing primarily on lock-free algorithms as the sweet spot for practical high-performance systems\n> - **Rationale**: Lock-free algorithms provide significant performance benefits over locks while remaining implementable without the theoretical complexity of wait-free constructions. This allows developers to understand both the benefits and costs of moving beyond traditional locking.\n> - **Consequences**: Developers will understand when to apply each technique and can make informed decisions based on their specific performance, complexity, and correctness requirements.\n\n| Approach | Coordination Method | Progress Guarantee | Implementation Complexity | Performance Under Contention | Failure Modes | Best Use Cases |\n|----------|-------------------|-------------------|-------------------------|----------------------------|---------------|----------------|\n| **Blocking (Locks)** | Mutual exclusion with mutexes/semaphores | Blocking - threads may wait indefinitely | Low - straightforward critical sections | Poor - serialization bottlenecks | Deadlock, priority inversion, convoy effects | Simple data structures, low contention scenarios |\n| **Lock-Free** | Atomic operations with CAS retry loops | Non-blocking - at least one thread makes progress | High - complex retry logic and memory reclamation | Good - multiple threads can progress simultaneously | ABA problems, memory reclamation complexity, livelock potential | High-performance data structures, real-time systems |\n| **Wait-Free** | Universal constructions or specialized algorithms | Strongest - every thread makes progress within bounded steps | Very High - often requires helping mechanisms | Excellent - no thread can be delayed by others | Implementation complexity, memory overhead from helping | Ultra-low latency systems, hard real-time applications |\n| **Hybrid** | Combines techniques based on contention levels | Variable - adapts based on runtime conditions | Medium - managing multiple coordination mechanisms | Variable - can adapt to different load patterns | Complexity of mode transitions, tuning parameters | Systems with varying load patterns, need for graceful degradation |\n\n**Lock-Based Mutual Exclusion Details**\n\nTraditional lock-based approaches use operating system primitives to enforce mutual exclusion around critical sections. Threads acquire locks before accessing shared data and release them afterward, ensuring that only one thread can modify the data at any given time.\n\nThe **primary advantage** of lock-based approaches lies in their conceptual simplicity. Critical sections are clearly delineated, making it easier to reason about correctness. Most developers have experience with lock-based programming, reducing the learning curve for new team members. Additionally, locks compose reasonably well with existing APIs and frameworks that expect blocking semantics.\n\n**Performance characteristics** vary significantly based on contention levels. Under low contention, well-implemented locks can be quite fast, with modern adaptive mutexes using efficient user-space spinning before falling back to kernel-mediated blocking. However, performance degrades rapidly as contention increases, with the worst-case scenario being complete serialization where threads effectively execute single-file through critical sections.\n\n**Memory ordering** is generally handled automatically by lock implementations, which typically include full memory barriers on acquire and release operations. This simplifies reasoning about memory consistency but may be over-conservative for performance-critical applications that could benefit from relaxed ordering semantics.\n\n**Lock-Free Non-Blocking Algorithms**\n\nLock-free algorithms eliminate blocking by using atomic operations, particularly compare-and-swap (CAS), to coordinate between threads. These algorithms guarantee that at least one thread will make progress within a bounded number of steps, even if individual threads might be delayed by contention.\n\nThe **core technique** involves optimistic execution followed by validation. Threads read shared data, compute updates locally, and then attempt to atomically install their changes using CAS operations. If the CAS fails (indicating another thread modified the data concurrently), the operation is retried with updated values.\n\n**Memory reclamation** becomes significantly more complex in lock-free systems. Since threads don't block, a thread might be accessing a data structure node even after another thread has logically removed it. Safe memory reclamation schemes like hazard pointers, epochs, or reference counting are necessary to prevent use-after-free errors.\n\n**ABA problems** represent a subtle correctness issue unique to lock-free programming. If a thread reads a pointer value A, gets preempted, and then observes the same pointer value A again, it might incorrectly assume nothing has changed—even though the memory might have been freed and reallocated to the same address in the interim. Solutions typically involve tagged pointers or version counters.\n\n**Performance benefits** are substantial under contention. Multiple threads can make progress simultaneously, cache lines experience less ping-ponging (since threads retry locally rather than blocking), and there's no context switching overhead from blocking operations. However, under low contention, lock-free algorithms may actually perform slightly worse than locks due to the overhead of atomic operations and retry logic.\n\n**Wait-Free Universal Constructions**\n\nWait-free algorithms provide the strongest progress guarantee: every thread is guaranteed to complete any operation within a bounded number of its own steps, regardless of the behavior of other threads. This eliminates the possibility of indefinite delays that could affect real-time guarantees.\n\n**Implementation approaches** typically fall into two categories. Universal constructions use consensus objects (like compare-and-swap) to implement arbitrary data structures by having threads propose operations and use consensus to determine which operations are applied in what order. Specialized wait-free algorithms are designed from scratch for specific data structures, often using helping mechanisms where threads assist each other to ensure progress.\n\n**Memory and computational overhead** is typically higher than lock-free approaches. Universal constructions often require per-thread operation records and complex helping protocols. The constant factors in wait-free algorithms can be significantly higher than their lock-free counterparts, making them impractical for many applications despite their theoretical advantages.\n\n**Practical applicability** is limited by implementation complexity and performance overhead. Wait-free algorithms are primarily justified in hard real-time systems where bounded execution time is more important than average-case performance, or in systems where even temporary delays could have catastrophic consequences.\n\n**Hybrid Adaptive Approaches**\n\nModern high-performance systems increasingly use hybrid approaches that adapt their coordination strategy based on runtime conditions. These systems might start with optimistic lock-free protocols under low contention and fall back to more structured approaches when contention increases.\n\n**Adaptive mutexes** represent one successful hybrid approach, spinning in user space for short periods before falling back to kernel-mediated blocking. This provides the low latency of spinning when locks are held briefly while avoiding the CPU waste of indefinite spinning.\n\n**Contention management** becomes a key design challenge in hybrid systems. The system must detect contention levels, decide when to transition between coordination modes, and manage the complexity of supporting multiple protocols simultaneously. Poor tuning of these parameters can lead to pathological behavior where the system thrashes between different modes.\n\n**Examples** include Java's concurrent collections, which use techniques like lock striping (reducing contention by using multiple locks) combined with lock-free operations for read-heavy workloads. Database systems often use hybrid approaches, starting with optimistic concurrency control and falling back to locking when conflicts are detected.\n\n> **Common Pitfalls in Concurrency Model Selection**\n> \n> ⚠️ **Pitfall: Premature Lock-Free Optimization** - Teams often attempt lock-free implementations without first establishing that lock contention is actually a bottleneck. Lock-free algorithms are significantly more complex to implement correctly, and their benefits only manifest under high contention scenarios.\n> \n> ⚠️ **Pitfall: Ignoring Memory Reclamation Complexity** - Many developers underestimate the complexity of safe memory management in lock-free systems. Memory reclamation schemes like hazard pointers can add significant complexity and overhead that may negate the performance benefits of lock-free algorithms.\n> \n> ⚠️ **Pitfall: Incorrect Progress Guarantee Assumptions** - Lock-free doesn't mean faster—it means non-blocking. Under low contention, well-tuned locks often outperform lock-free algorithms. The choice should be based on system requirements for progress guarantees rather than pure performance considerations.\n> \n> ⚠️ **Pitfall: Underestimating Testing Complexity** - Concurrent correctness bugs in lock-free systems are extremely difficult to reproduce and debug. Teams often underestimate the testing infrastructure and expertise required to validate lock-free implementations correctly.\n\nThe selection of a concurrency approach should be driven by specific system requirements rather than theoretical elegance. Lock-based approaches remain appropriate for many scenarios, particularly when contention is low or when development team expertise with concurrent programming is limited. Lock-free approaches provide compelling benefits in high-contention scenarios where performance and scalability are critical, but require significant expertise and testing investment. Wait-free approaches should be reserved for systems with hard real-time requirements where the complexity and overhead can be justified by the stronger progress guarantees.\n\n### Implementation Guidance\n\nThis section provides practical guidance for understanding and implementing the concepts discussed above, serving as a foundation for the lock-free data structures that follow in subsequent sections.\n\n**Technology Recommendations**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Atomic Operations | `threading.Lock` with explicit synchronization | `ctypes` with C atomic operations or `multiprocessing.Value` |\n| Memory Ordering | Python GIL provides implicit ordering guarantees | Explicit memory barriers using `threading.Barrier` or C extensions |\n| Performance Testing | `time.time()` measurements with basic threading | `perf_counter()` with statistical analysis and contention metrics |\n| Concurrency Testing | Sequential consistency checking with simple asserts | Property-based testing with `hypothesis` and race condition detection |\n| Debugging Tools | `print()` statements with thread IDs and timestamps | `faulthandler`, `py-spy` profiling, and custom logging with memory barriers |\n\n**Recommended Module Structure**\n\nThe lock-free data structures project should be organized to clearly separate concerns and build complexity incrementally:\n\n```\nlock_free_structures/\n├── __init__.py                    # Package initialization and public API\n├── atomic/\n│   ├── __init__.py               # Atomic operations foundation\n│   ├── primitives.py             # AtomicReference, AtomicInteger, CAS operations\n│   ├── memory_ordering.py        # Memory ordering semantics and barriers\n│   └── aba_protection.py         # Tagged pointers and version counters\n├── structures/\n│   ├── __init__.py               # Data structure implementations\n│   ├── stack.py                  # Treiber stack implementation\n│   ├── queue.py                  # Michael-Scott queue implementation\n│   └── hashmap.py                # Split-ordered hash map implementation\n├── memory/\n│   ├── __init__.py               # Memory reclamation schemes\n│   ├── hazard_pointers.py        # Hazard pointer implementation\n│   ├── epoch_based.py            # Epoch-based reclamation (optional)\n│   └── reference_counting.py     # Atomic reference counting (optional)\n├── testing/\n│   ├── __init__.py               # Testing utilities and frameworks\n│   ├── linearizability.py       # Linearizability checking tools\n│   ├── stress_testing.py         # High-contention stress test framework\n│   └── correctness_checks.py     # Property verification utilities\n└── examples/\n    ├── benchmarks.py             # Performance comparison demos\n    ├── correctness_demo.py       # Correctness property demonstrations\n    └── pitfall_examples.py       # Common mistake demonstrations\n```\n\n**Atomic Operations Infrastructure**\n\nSince Python's threading model includes the Global Interpreter Lock (GIL), true lock-free programming requires careful consideration of how to achieve atomic operations. The following infrastructure provides a foundation for the lock-free algorithms that follow:\n\n```python\n\"\"\"\nAtomic operations foundation for lock-free data structures.\n\nThis module provides atomic primitives that form the building blocks\nfor lock-free algorithms. Due to Python's GIL, true atomicity requires\ncareful implementation using either ctypes for C-level atomics or\ncareful synchronization protocols.\n\"\"\"\n\nimport threading\nimport ctypes\nfrom typing import TypeVar, Generic, Optional, Tuple\nfrom dataclasses import dataclass\n\nT = TypeVar('T')\n\nclass MemoryOrdering:\n    \"\"\"Memory ordering constraints for atomic operations.\"\"\"\n    RELAXED = \"relaxed\"      # No ordering constraints\n    ACQUIRE = \"acquire\"      # Acquire semantics for loads\n    RELEASE = \"release\"      # Release semantics for stores\n    ACQ_REL = \"acq_rel\"     # Both acquire and release\n    SEQ_CST = \"seq_cst\"     # Sequential consistency\n\n@dataclass\nclass TaggedPointer(Generic[T]):\n    \"\"\"Tagged pointer to prevent ABA problems.\"\"\"\n    pointer: Optional[T]\n    tag: int\n    \n    def __hash__(self):\n        return hash((id(self.pointer), self.tag))\n    \n    def __eq__(self, other):\n        if not isinstance(other, TaggedPointer):\n            return False\n        return self.pointer is other.pointer and self.tag == other.tag\n\nclass AtomicReference(Generic[T]):\n    \"\"\"\n    Thread-safe atomic reference with compare-and-swap support.\n    \n    This implementation uses Python's threading primitives to simulate\n    atomic operations. In a production system, you would typically use\n    C extensions or specialized libraries for true lock-free atomics.\n    \"\"\"\n    \n    def __init__(self, initial_value: Optional[T] = None):\n        # TODO 1: Initialize the internal value storage\n        # TODO 2: Create a lock for simulating atomicity (remove in real lock-free impl)\n        # TODO 3: Initialize any debugging/monitoring counters\n        pass\n    \n    def load(self, ordering: str = MemoryOrdering.SEQ_CST) -> Optional[T]:\n        \"\"\"\n        Atomically load the current value.\n        \n        Args:\n            ordering: Memory ordering constraint for this load\n            \n        Returns:\n            Current value of the atomic reference\n        \"\"\"\n        # TODO 1: Apply appropriate memory ordering semantics\n        # TODO 2: Atomically read the current value\n        # TODO 3: Return the loaded value\n        pass\n    \n    def store(self, new_value: Optional[T], ordering: str = MemoryOrdering.SEQ_CST):\n        \"\"\"\n        Atomically store a new value.\n        \n        Args:\n            new_value: Value to store\n            ordering: Memory ordering constraint for this store\n        \"\"\"\n        # TODO 1: Apply appropriate memory ordering semantics\n        # TODO 2: Atomically update the stored value\n        # TODO 3: Ensure visibility to other threads\n        pass\n    \n    def compare_and_swap(self, expected: Optional[T], new_value: Optional[T]) -> Tuple[bool, Optional[T]]:\n        \"\"\"\n        Atomically compare current value with expected and swap if equal.\n        \n        This is the fundamental primitive for lock-free algorithms.\n        \n        Args:\n            expected: Expected current value\n            new_value: New value to store if comparison succeeds\n            \n        Returns:\n            Tuple of (success, observed_value)\n            - success: True if swap occurred, False otherwise\n            - observed_value: The value that was actually in memory\n        \"\"\"\n        # TODO 1: Atomically load the current value\n        # TODO 2: Compare current value with expected (use 'is' for object identity)\n        # TODO 3: If equal, store new_value and return (True, expected)\n        # TODO 4: If not equal, return (False, current_value)\n        # TODO 5: Ensure entire operation is atomic\n        pass\n\nclass AtomicInteger:\n    \"\"\"\n    Thread-safe atomic integer with arithmetic operations.\n    \n    Provides atomic increment, decrement, and fetch-and-add operations\n    commonly needed in lock-free algorithms.\n    \"\"\"\n    \n    def __init__(self, initial_value: int = 0):\n        # TODO 1: Initialize integer storage\n        # TODO 2: Create synchronization primitives\n        pass\n    \n    def load(self) -> int:\n        \"\"\"Atomically load current integer value.\"\"\"\n        # TODO: Implement atomic load\n        pass\n    \n    def store(self, value: int):\n        \"\"\"Atomically store new integer value.\"\"\"\n        # TODO: Implement atomic store\n        pass\n    \n    def fetch_and_add(self, increment: int = 1) -> int:\n        \"\"\"\n        Atomically add increment to current value and return previous value.\n        \n        This is a fundamental building block for counters and sequence numbers.\n        \"\"\"\n        # TODO 1: Use CAS loop to implement fetch-and-add\n        # TODO 2: Load current value\n        # TODO 3: Compute new value (current + increment)\n        # TODO 4: Attempt CAS with current and new value\n        # TODO 5: Retry if CAS fails, return old value if succeeds\n        pass\n    \n    def increment(self) -> int:\n        \"\"\"Atomically increment and return new value.\"\"\"\n        return self.fetch_and_add(1) + 1\n    \n    def decrement(self) -> int:\n        \"\"\"Atomically decrement and return new value.\"\"\"\n        return self.fetch_and_add(-1) - 1\n```\n\n**ABA Problem Demonstration**\n\nUnderstanding the ABA problem is crucial for implementing correct lock-free algorithms:\n\n```python\n\"\"\"\nDemonstration of the ABA problem and its solutions.\n\nThe ABA problem occurs when a thread reads a value A, gets preempted,\nand later observes the same value A, incorrectly concluding nothing\nhas changed - even though the memory might have been modified and\nrestored in the interim.\n\"\"\"\n\nimport threading\nimport time\nfrom typing import Optional\n\nclass Node:\n    \"\"\"Simple linked list node for demonstrating ABA.\"\"\"\n    def __init__(self, data: int, next_node: Optional['Node'] = None):\n        self.data = data\n        self.next = next_node\n\nclass ProblematicStack:\n    \"\"\"Stack implementation vulnerable to ABA problem.\"\"\"\n    \n    def __init__(self):\n        self.top = AtomicReference[Optional[Node]](None)\n    \n    def push(self, data: int):\n        \"\"\"Push operation - not vulnerable to ABA.\"\"\"\n        # TODO 1: Create new node with data\n        # TODO 2: Use CAS loop to update top pointer\n        pass\n    \n    def pop(self) -> Optional[int]:\n        \"\"\"Pop operation - vulnerable to ABA problem.\"\"\"\n        # TODO 1: Load current top\n        # TODO 2: If empty, return None\n        # TODO 3: Read next pointer (DANGEROUS: node might be freed)\n        # TODO 4: Attempt CAS to swing top to next\n        # TODO 5: Return data if successful, retry if failed\n        pass\n\ndef demonstrate_aba_problem():\n    \"\"\"\n    Demonstrates how ABA can cause corruption in naive lock-free code.\n    \n    This function sets up a scenario where:\n    1. Thread 1 reads stack top (A)\n    2. Thread 2 pops A and B, then pushes A back\n    3. Thread 1 sees A again and incorrectly thinks nothing changed\n    4. Thread 1's CAS succeeds but creates corruption\n    \"\"\"\n    # TODO 1: Create stack with nodes A -> B -> C\n    # TODO 2: Start thread 1 that begins pop operation, then sleeps\n    # TODO 3: Start thread 2 that does: pop A, pop B, push A\n    # TODO 4: Let thread 1 continue - its CAS will succeed incorrectly\n    # TODO 5: Demonstrate the resulting corruption\n    pass\n\nclass SafeTaggedStack:\n    \"\"\"Stack implementation using tagged pointers to prevent ABA.\"\"\"\n    \n    def __init__(self):\n        self.top = AtomicReference[TaggedPointer[Optional[Node]]](\n            TaggedPointer(None, 0)\n        )\n    \n    def push(self, data: int):\n        \"\"\"ABA-safe push using tagged pointers.\"\"\"\n        # TODO 1: Create new node\n        # TODO 2: CAS loop with tag increment\n        pass\n    \n    def pop(self) -> Optional[int]:\n        \"\"\"ABA-safe pop using tagged pointers.\"\"\"\n        # TODO 1: Load current tagged top\n        # TODO 2: CAS with incremented tag\n        pass\n```\n\n**Concurrency Testing Framework**\n\nTesting lock-free code requires specialized approaches to detect race conditions and verify correctness properties:\n\n```python\n\"\"\"\nTesting framework for concurrent correctness verification.\n\nProvides utilities for stress testing, linearizability checking,\nand property verification in concurrent data structures.\n\"\"\"\n\nimport threading\nimport random\nimport time\nfrom typing import List, Callable, Any\nfrom dataclasses import dataclass\nfrom collections import defaultdict\n\n@dataclass\nclass Operation:\n    \"\"\"Represents a single operation in a concurrent execution.\"\"\"\n    thread_id: int\n    operation: str\n    arguments: List[Any]\n    return_value: Any\n    start_time: float\n    end_time: float\n\nclass LinearizabilityChecker:\n    \"\"\"\n    Checks if a concurrent execution is linearizable.\n    \n    A concurrent execution is linearizable if there exists a sequential\n    execution of the same operations that:\n    1. Produces the same results\n    2. Respects the real-time order of non-overlapping operations\n    \"\"\"\n    \n    def __init__(self):\n        self.operations = []\n        self.lock = threading.Lock()\n    \n    def record_operation(self, op: Operation):\n        \"\"\"Record an operation for later linearizability analysis.\"\"\"\n        # TODO: Thread-safely add operation to history\n        pass\n    \n    def check_linearizability(self) -> bool:\n        \"\"\"\n        Verify that recorded operations are linearizable.\n        \n        This is a simplified checker - full linearizability checking\n        is NP-complete in general.\n        \"\"\"\n        # TODO 1: Sort operations by start time\n        # TODO 2: Try to find valid linearization points\n        # TODO 3: Verify sequential specification is satisfied\n        # TODO 4: Return True if linearizable, False otherwise\n        pass\n\ndef stress_test_data_structure(\n    data_structure: Any,\n    operations: List[Callable],\n    num_threads: int = 10,\n    operations_per_thread: int = 1000,\n    duration_seconds: int = 10\n) -> dict:\n    \"\"\"\n    Stress test a data structure with concurrent operations.\n    \n    Args:\n        data_structure: The data structure to test\n        operations: List of operation functions to call\n        num_threads: Number of concurrent threads\n        operations_per_thread: Operations each thread should perform\n        duration_seconds: Maximum test duration\n        \n    Returns:\n        Dictionary with test results and statistics\n    \"\"\"\n    # TODO 1: Create worker threads that randomly call operations\n    # TODO 2: Record timing and correctness metrics\n    # TODO 3: Run for specified duration or operation count\n    # TODO 4: Collect and return comprehensive statistics\n    pass\n```\n\n**Milestone Checkpoint: Foundation Verification**\n\nAfter implementing the atomic operations foundation, verify correct behavior with these tests:\n\n1. **Atomic Reference Correctness**: Run `python -m pytest testing/test_atomic_reference.py -v`. Expected output should show all CAS operations succeeding with correct return values and no lost updates.\n\n2. **ABA Problem Demonstration**: Run `python examples/aba_demo.py`. You should see output showing:\n   - Problematic stack exhibiting corruption under specific timing\n   - Tagged pointer stack maintaining correctness under same conditions\n   - Clear explanation of why the naive approach fails\n\n3. **Concurrent Counter Test**: Run a test with 10 threads each incrementing an atomic counter 1000 times. Final value should always be exactly 10,000 with no lost updates.\n\n4. **Memory Ordering Verification**: Use threading barriers to verify acquire/release semantics prevent reordering of dependent operations.\n\n**Performance Baseline Measurements**\n\nBefore implementing lock-free data structures, establish performance baselines:\n\n```bash\n# Measure lock-based vs atomic operation overhead\npython benchmarks/atomic_vs_locks.py\n\n# Expected output:\n# Lock acquire/release: 50ns average\n# Atomic CAS success: 20ns average  \n# Atomic CAS failure: 15ns average\n# Contended lock: 2000ns average (context switch)\n# Contended CAS: 100ns average (retry loop)\n```\n\n**Common Implementation Pitfalls**\n\n⚠️ **Pitfall: GIL Dependency** - Python's GIL provides some atomicity guarantees that don't exist in other languages. Code that works in Python might have race conditions when ported to Go or Rust.\n\n⚠️ **Pitfall: Object Identity vs Value Equality** - Use `is` for comparing object references in CAS operations, not `==`. Value equality can give false positives when different objects have the same content.\n\n⚠️ **Pitfall: Missing Memory Barriers** - Even with atomic operations, you may need explicit memory barriers to prevent reordering of adjacent non-atomic operations.\n\n⚠️ **Pitfall: Infinite CAS Loops** - Always include backoff or retry limits in CAS loops to prevent livelock under extreme contention.\n\nThe foundation established in this section provides the building blocks for implementing the lock-free data structures in subsequent milestones. Understanding these concepts deeply is essential before proceeding to the more complex algorithms that follow.\n\n\n## Goals and Non-Goals\n\n> **Milestone(s):** All milestones (this section defines the scope and boundaries for the entire lock-free data structure library)\n\nThe success of any complex software project hinges on clearly defining what it will and will not accomplish. Lock-free programming is particularly susceptible to scope creep because the theoretical possibilities are vast, yet the practical implementation challenges are enormous. This section establishes concrete, measurable goals that will guide our implementation decisions and help us recognize when we have succeeded.\n\n### Mental Model: The Precision Tool Workshop\n\nThink of our lock-free data structure library as building a precision tool workshop rather than a general-purpose hardware store. A precision tool workshop focuses on creating a small number of extremely high-quality, specialized tools that perform specific tasks better than any generic alternative. Each tool is crafted with meticulous attention to detail, tested under extreme conditions, and designed to work flawlessly when used correctly.\n\nIn contrast, a hardware store carries thousands of items of varying quality, trying to serve every possible need. Our library follows the precision workshop philosophy: we will build a small number of lock-free data structures exceptionally well, with bulletproof correctness guarantees and outstanding performance characteristics, rather than attempting to implement every conceivable concurrent data structure with mediocre quality.\n\nThis focused approach allows us to deeply understand the subtle challenges of lock-free programming, develop robust solutions to memory reclamation and correctness verification, and create reference implementations that others can learn from and build upon. Like a master craftsman who perfects their core techniques before expanding to new domains, we will master the fundamental patterns of lock-free programming through careful implementation of stack, queue, and hash map structures.\n\n## Functional Goals\n\nOur functional goals define the core correctness and safety properties that every component in our library must satisfy. These are non-negotiable requirements that distinguish a production-ready lock-free library from academic prototypes or hobby projects.\n\n### Thread-Safe Operations Without Data Races\n\nEvery operation exposed by our data structures must be thread-safe by design, meaning multiple threads can invoke operations concurrently without causing data corruption, undefined behavior, or inconsistent state. This goes beyond simply avoiding crashes—we must ensure that the data structure maintains its structural invariants even under arbitrary thread interleavings and timing variations.\n\nThread safety in our context means that all shared memory accesses use appropriate atomic operations with correct memory ordering constraints. Non-atomic operations on shared variables are forbidden, as they create data races that lead to undefined behavior according to the language memory model. We will use the compare-and-swap primitive as our primary synchronization mechanism, combined with atomic loads and stores using carefully chosen memory ordering semantics.\n\nOur thread safety guarantee extends to structural integrity: linked list pointers will never become dangling, reference counts will never become negative, and internal consistency invariants (such as a queue's head always being reachable from its tail) will be preserved across all concurrent operations. This requires careful attention to the order of pointer updates and the use of helping mechanisms to ensure that partially completed operations do not leave the data structure in an inconsistent state.\n\n> **Decision: Complete Operation Atomicity**\n> - **Context**: Operations like queue enqueue involve multiple pointer updates that cannot all be performed atomically in a single instruction\n> - **Options Considered**: \n>   1. Accept temporary inconsistent states visible to other threads\n>   2. Use multi-word compare-and-swap hardware instructions where available\n>   3. Design algorithms with single atomic update points that transition between consistent states\n> - **Decision**: Design algorithms with single atomic linearization points\n> - **Rationale**: Multi-word CAS has limited hardware support and complex fallback requirements. Single-point atomicity provides the strongest correctness guarantees and simplifies reasoning about concurrent behavior.\n> - **Consequences**: May require more complex algorithms (like Michael-Scott queue) but provides clear linearizability and easier verification\n\n### FIFO Ordering Preservation\n\nOur queue implementation must provide strict first-in-first-out ordering semantics, meaning that elements dequeued from the queue appear in exactly the same order they were enqueued, regardless of the timing and concurrency of operations. This property must hold even when multiple threads are performing concurrent enqueue and dequeue operations with arbitrary interleavings.\n\nFIFO ordering is more challenging to maintain in lock-free algorithms than in lock-based approaches because we cannot use a global mutex to serialize all operations. Instead, we must use techniques like the Michael-Scott algorithm that carefully coordinates separate head and tail pointer updates to preserve ordering while allowing concurrent access to different ends of the queue.\n\nThe ordering guarantee applies at the logical level of linearizability: if one enqueue operation completes before another begins (in real time), then the first element will be dequeued before the second. For overlapping operations, their effective ordering is determined by their linearization points—the specific atomic operations that make them appear to take effect instantaneously.\n\n### Linearizability as the Correctness Standard\n\nAll our data structures will satisfy linearizability, which is the standard correctness condition for concurrent objects. Linearizability means that every operation appears to take effect atomically at some point between its invocation and response, and the results of all operations are consistent with some sequential execution that respects the real-time ordering of non-overlapping operations.\n\n| Correctness Property | Definition | Our Implementation Approach |\n|---------------------|------------|----------------------------|\n| Sequential Consistency | Operations appear in program order per thread | Maintained through memory ordering constraints |\n| Linearizability | Operations have atomic linearization points | Identified and documented for each data structure operation |\n| Lock-freedom | At least one thread makes progress | Ensured through helping mechanisms and bounded retry loops |\n| Memory Safety | No use-after-free or dangling pointers | Achieved through hazard pointer memory reclamation |\n\nFor our stack implementation, the linearization point of a successful push operation is the successful compare-and-swap that updates the top pointer. For a successful pop operation, it is the successful compare-and-swap that advances the top pointer to the next node. These points are clearly identifiable in the algorithm and provide a formal basis for proving correctness.\n\nThe linearizability guarantee allows our data structures to be composed with other concurrent operations while maintaining predictable semantics. Applications can reason about our data structures as if they were atomic, sequential objects, while still benefiting from the performance advantages of lock-free implementation.\n\n### Safe Memory Reclamation\n\nMemory management in lock-free data structures presents a fundamental challenge: we cannot immediately free memory when removing a node from a data structure because other threads may still be accessing that node. Traditional garbage collection solves this problem but introduces pause times and memory overhead that conflict with our performance goals.\n\nOur solution implements hazard pointers, a memory reclamation scheme that allows threads to announce which nodes they are currently accessing, preventing those nodes from being freed by other threads. This approach provides memory safety guarantees equivalent to garbage collection while maintaining the real-time characteristics of lock-free algorithms.\n\nThe hazard pointer system operates in several phases:\n\n1. **Protection**: Before accessing a shared node, a thread sets a hazard pointer to that node\n2. **Validation**: After setting the hazard pointer, the thread re-reads the shared pointer to ensure it still points to the protected node\n3. **Safe Access**: While the hazard pointer is set, no other thread will free the protected node\n4. **Release**: After finishing access, the thread clears its hazard pointer\n5. **Retirement**: When removing nodes from data structures, threads place them on a retirement list rather than immediately freeing them\n6. **Scanning**: Periodically, threads scan all active hazard pointers and free retired nodes that are not protected\n\nThis approach guarantees that freed memory is never accessed, preventing crashes and data corruption that would otherwise occur in naive lock-free implementations.\n\n| Memory Safety Requirement | Implementation Strategy |\n|---------------------------|------------------------|\n| No use-after-free | Hazard pointer protection before access |\n| No double-free | Retirement list prevents duplicate freeing |\n| No memory leaks | Periodic scanning and batch reclamation |\n| Bounded memory usage | Threshold-based reclamation triggers |\n\n## Performance Goals\n\nPerformance goals define the quantitative characteristics that distinguish our lock-free implementation from simpler alternatives. These goals drive architectural decisions and provide measurable criteria for evaluating our success.\n\n### High Throughput Under Contention\n\nOur data structures must maintain high operation throughput even when many threads are concurrently accessing the same data structure. This requires minimizing the serialization points where threads must compete for exclusive access to shared state.\n\nTraditional mutex-based data structures suffer from fundamental throughput limitations because the critical section becomes a bottleneck—only one thread can make progress at a time, regardless of how many cores are available. Our lock-free approach eliminates this bottleneck by allowing multiple threads to make progress simultaneously, even when they are modifying the same data structure.\n\nWe will measure throughput as operations per second under controlled contention scenarios:\n\n| Scenario | Thread Count | Target Throughput | Comparison Baseline |\n|----------|--------------|-------------------|-------------------|\n| Low Contention | 2-4 threads | 10M ops/sec | Mutex-protected equivalent |\n| Medium Contention | 8-16 threads | 5M ops/sec | 2x better than mutex baseline |\n| High Contention | 32+ threads | 1M ops/sec | 3x better than mutex baseline |\n| Mixed Operations | Variable | Proportional to operation mix | Separate read/write measurements |\n\nThe key insight is that lock-free algorithms can achieve increasing absolute throughput as more threads are added, up to the point where hardware resources (memory bandwidth, cache coherence traffic) become the limiting factor. Mutex-based approaches typically show decreasing throughput as contention increases due to context switching overhead and lock acquisition delays.\n\n### Scalability Across CPU Cores\n\nOur implementation must demonstrate near-linear scalability across CPU cores for embarrassingly parallel workloads. When threads are operating on different parts of the data structure or when the workload naturally distributes across multiple independent operations, performance should improve proportionally with the number of available cores.\n\nScalability challenges in lock-free programming often arise from false sharing (multiple threads modifying different data that happens to reside in the same cache line) and cache coherence traffic (the overhead of keeping atomic variables synchronized across cores). Our design will minimize these effects through careful memory layout and algorithmic choices.\n\nFor our hash map implementation, scalability means that threads operating on different hash buckets should rarely interfere with each other. The split-ordered list design achieves this by distributing operations across bucket chains and minimizing shared state between buckets.\n\n> **Decision: NUMA-Aware Memory Allocation Strategy**\n> - **Context**: On multi-socket systems, memory access costs vary dramatically depending on which NUMA node allocated the memory\n> - **Options Considered**:\n>   1. Use system default allocation (simple but may cause remote memory access)\n>   2. Implement thread-local memory pools with NUMA binding\n>   3. Use NUMA-aware allocation hints for shared data structures\n> - **Decision**: Start with system default allocation, add NUMA awareness as an optimization\n> - **Rationale**: NUMA effects vary significantly across hardware platforms, and premature optimization could complicate the core algorithm implementation without clear benefits\n> - **Consequences**: May leave performance on the table for large multi-socket systems, but ensures portability and implementation focus on correctness first\n\n### Minimal Latency for Individual Operations\n\nWhile throughput measures aggregate performance across many operations, latency measures the time required for individual operations to complete. Low latency is critical for real-time applications and interactive systems where response time directly affects user experience.\n\nLock-free algorithms provide latency advantages by eliminating the unpredictable delays associated with lock acquisition. When a thread holds a mutex, other threads must wait for an unbounded time that depends on scheduling decisions, page faults, and other factors outside the algorithm's control. Lock-free operations complete in a time bounded only by the algorithm's retry behavior and memory access latencies.\n\nOur latency goals focus on worst-case behavior rather than average case:\n\n| Latency Metric | Target Value | Measurement Method |\n|---------------|--------------|-------------------|\n| 99th Percentile | <100 microseconds | High-frequency timing under contention |\n| 99.9th Percentile | <1 millisecond | Includes memory allocation delays |\n| Maximum Observed | <10 milliseconds | Excludes OS scheduling anomalies |\n| Variance | Low coefficient of variation | Consistent performance across runs |\n\nThe bounded latency property of lock-free algorithms makes them suitable for soft real-time applications where predictable response times are more important than peak throughput. However, we must be careful to avoid livelock situations where threads interfere with each other's progress, leading to excessive retry loops that inflate latency.\n\n### Deterministic Performance Characteristics\n\nOur algorithms must exhibit predictable performance behavior that does not depend on lucky or unlucky timing of thread interleavings. This determinism allows application developers to reason about system behavior and make reliable capacity planning decisions.\n\nDeterministic performance means avoiding algorithms with worst-case exponential backoff or unbounded retry loops that could theoretically run forever. While our compare-and-swap loops may retry multiple times under heavy contention, the number of retries should be bounded by reasonable constants related to the number of concurrent threads, not by arbitrary timing factors.\n\nWe will validate deterministic behavior through stress testing that measures performance variance across many runs with identical workloads. High variance would indicate that our algorithms are sensitive to timing-dependent effects that could cause unpredictable performance in production systems.\n\n## Explicit Non-Goals\n\nClearly defining what our library will NOT provide is as important as defining what it will provide. These non-goals help maintain focus and prevent scope creep that could compromise the quality of our core functionality.\n\n### Blocking Operations and Wait-Free Guarantees\n\nOur library will provide lock-free algorithms but NOT wait-free algorithms. The distinction is crucial: lock-free guarantees that at least one thread makes progress at any point in time, while wait-free guarantees that every thread makes progress within a bounded number of steps.\n\n| Progress Guarantee | Definition | Implementation Complexity | Performance Trade-offs |\n|-------------------|------------|--------------------------|----------------------|\n| Lock-free | At least one thread progresses | Moderate (CAS retry loops) | High throughput possible |\n| Wait-free | Every thread progresses | High (universal constructions) | Lower throughput, higher latency |\n| Obstruction-free | Threads progress when running alone | Low (simple CAS) | Poor contention handling |\n\nWait-free algorithms require significantly more complex implementations, often using universal constructions that simulate stronger synchronization primitives. These constructions typically have higher constant factors and lower peak throughput than simpler lock-free algorithms, making them unsuitable for our performance goals.\n\nOur lock-free approach may allow some threads to be delayed by interference from other threads, but it provides much better average-case performance while still avoiding the deadlock and priority inversion problems of lock-based approaches. Applications that require strict wait-free guarantees should use specialized libraries designed specifically for that purpose.\n\n### Automatic Garbage Collection Integration\n\nWe will NOT integrate with language-specific garbage collection systems or rely on GC for memory management. Instead, we implement explicit memory reclamation through hazard pointers, giving applications direct control over memory management behavior and timing.\n\nGarbage collection integration would provide implementation simplicity but conflicts with our performance goals in several ways:\n\n- **Unpredictable pause times**: GC pauses would violate our latency guarantees\n- **Memory overhead**: GC systems typically use 2-4x more memory than explicit management\n- **Language dependence**: GC integration would make the library non-portable across languages\n- **Performance unpredictability**: GC pressure could cause sudden performance degradation\n\n> **Decision: Manual Memory Management with Hazard Pointers**\n> - **Context**: Need to safely reclaim memory in lock-free algorithms without relying on garbage collection\n> - **Options Considered**:\n>   1. Rely on language GC (simple but performance unpredictable)\n>   2. Reference counting with atomic operations (ABA problems and cycle issues)\n>   3. Hazard pointers for safe manual reclamation\n>   4. RCU (Read-Copy-Update) mechanisms\n> - **Decision**: Implement hazard pointers for memory reclamation\n> - **Rationale**: Hazard pointers provide deterministic reclamation timing, bounded memory usage, and portability across languages and runtime systems\n> - **Consequences**: More complex implementation but predictable performance and broad applicability\n\n### Complex Data Structures Beyond Fundamentals\n\nOur scope is limited to fundamental data structures: stack, queue, and hash map. We will NOT implement complex structures like:\n\n- **Trees** (B-trees, red-black trees, AVL trees): Complex balancing operations are difficult to make lock-free\n- **Graphs** (adjacency lists, adjacency matrices): Require complex traversal algorithms and memory management\n- **Sets and Maps with ordering requirements**: Sorted data structures have complex linearizability requirements\n- **Multi-dimensional structures**: Spatial data structures, R-trees, quad-trees\n- **Specialized structures**: Bloom filters, skip lists, tries\n\nThe rationale for this limitation is that complex data structures would require substantially more development time while providing diminishing educational value. The patterns learned from implementing stack, queue, and hash map are sufficient to understand the fundamental challenges of lock-free programming, and these structures serve as building blocks for more complex algorithms.\n\nApplications requiring complex lock-free data structures should compose our fundamental structures or use specialized libraries designed for specific use cases. Our goal is to provide high-quality, well-understood implementations that serve as both production components and educational examples.\n\n### Dynamic Memory Pool Management\n\nWe will NOT implement sophisticated memory pool management, custom allocators, or memory optimization features. Memory allocation will use standard system allocators (malloc/free or language equivalents) rather than implementing pool-based or region-based allocation strategies.\n\nCustom memory management would add significant complexity without directly advancing the core learning goals of lock-free algorithm implementation. While memory allocation patterns can significantly affect performance, optimizing allocation is a separate concern from understanding compare-and-swap algorithms and memory reclamation.\n\n| Memory Management Feature | Status | Rationale |\n|--------------------------|--------|-----------|\n| Custom allocators | Excluded | Complexity doesn't advance core learning goals |\n| Memory pools | Excluded | Can be added as separate optimization layer |\n| NUMA-aware allocation | Excluded | Platform-specific and hardware-dependent |\n| Cache-line alignment | Included | Directly affects lock-free algorithm performance |\n| Padding for false sharing | Included | Essential for multi-threaded correctness |\n\n### Language-Specific Optimizations\n\nOur implementation will focus on portable algorithms rather than language-specific or platform-specific optimizations. We will NOT take advantage of:\n\n- **Platform-specific atomic operations**: Advanced hardware features like transactional memory or wide CAS operations\n- **Compiler intrinsics**: Assembly-level optimizations or platform-specific instruction sequences\n- **Language-specific features**: Generics, macros, or advanced type systems that would make the code non-portable\n- **Operating system features**: Specialized synchronization primitives or memory management APIs\n\nThis constraint ensures that our implementations can be understood and adapted across multiple programming languages and platforms. The focus remains on algorithmic techniques rather than low-level optimization tricks.\n\n⚠️ **Pitfall: Over-Engineering for Performance**\nMany lock-free programming projects fail because they attempt to optimize for every possible performance scenario before establishing basic correctness. This leads to implementations that are complex, difficult to test, and often contain subtle bugs. Our approach prioritizes correctness and clarity over maximum performance, recognizing that a working lock-free algorithm is far more valuable than a theoretically optimal algorithm that contains race conditions or memory safety bugs.\n\n### Implementation Guidance\n\nThis subsection provides concrete technical recommendations for implementing the goals and managing the scope boundaries defined above.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Atomic Operations | Language built-ins (Python threading, Go sync/atomic) | Hardware-specific intrinsics |\n| Memory Ordering | Sequential consistency (simplest) | Explicit acquire/release ordering |\n| Testing Framework | Standard unit testing | Property-based testing with linearizability checkers |\n| Benchmarking | Simple timing loops | Statistical performance analysis with confidence intervals |\n| Memory Debugging | Standard leak detection | Specialized concurrent memory validators |\n\n#### Recommended Project Structure\n\n```\nlock-free-library/\n├── src/\n│   ├── atomic/\n│   │   ├── __init__.py\n│   │   ├── primitives.py      ← compare_and_swap, atomic loads/stores\n│   │   ├── memory_ordering.py ← RELAXED, ACQUIRE, RELEASE constants\n│   │   └── tagged_pointer.py  ← ABA prevention helpers\n│   ├── data_structures/\n│   │   ├── __init__.py\n│   │   ├── stack.py          ← Treiber stack implementation\n│   │   ├── queue.py          ← Michael-Scott queue implementation  \n│   │   └── hashmap.py        ← Split-ordered hash map\n│   ├── memory/\n│   │   ├── __init__.py\n│   │   ├── hazard_pointers.py ← Memory reclamation system\n│   │   └── node_allocation.py ← Node lifecycle management\n│   └── testing/\n│       ├── __init__.py\n│       ├── linearizability.py ← Correctness verification\n│       └── stress_testing.py  ← Concurrent performance tests\n├── tests/\n│   ├── unit/              ← Component isolation tests\n│   ├── integration/       ← Cross-component interaction tests\n│   └── performance/       ← Benchmarks and scaling tests\n└── examples/\n    ├── basic_usage.py     ← Simple API demonstration\n    └── benchmarks.py      ← Performance comparison examples\n```\n\n#### Goal Verification Framework\n\nThe following framework helps verify that implementation meets our defined goals:\n\n**Thread Safety Verification:**\n```python\ndef verify_thread_safety(data_structure, operation_count=10000, thread_count=8):\n    \"\"\"\n    Verifies thread safety by running concurrent operations and checking invariants.\n    \n    TODO: Launch thread_count threads, each performing operation_count operations\n    TODO: Mix of push/pop for stack, enqueue/dequeue for queue\n    TODO: Verify no data corruption: all elements accounted for, no duplicates\n    TODO: Check structural invariants: no dangling pointers, valid reference counts\n    TODO: Measure and report any data races or assertion failures\n    \"\"\"\n    pass\n\ndef stress_test_data_structure(data_structure_class, duration_seconds=60):\n    \"\"\"\n    Extended stress testing under high contention scenarios.\n    \n    TODO: Create data structure instance\n    TODO: Launch maximum number of threads supported by system\n    TODO: Run mixed workload (80% reads, 20% writes) for duration_seconds\n    TODO: Monitor for crashes, hangs, or corrupted state\n    TODO: Record throughput and latency percentiles\n    TODO: Compare against baseline mutex-protected implementation\n    \"\"\"\n    pass\n```\n\n**Performance Goal Measurement:**\n```python\nclass PerformanceGoalValidator:\n    \"\"\"Validates that implementation meets specified performance goals.\"\"\"\n    \n    def measure_throughput_scaling(self, data_structure, max_threads=32):\n        \"\"\"\n        TODO: Test throughput from 1 to max_threads\n        TODO: Record operations per second at each thread count\n        TODO: Verify throughput increases (or at least doesn't decrease dramatically)\n        TODO: Generate scaling chart and identify saturation point\n        \"\"\"\n        pass\n        \n    def measure_latency_distribution(self, data_structure, sample_count=100000):\n        \"\"\"\n        TODO: Record timing for sample_count individual operations\n        TODO: Calculate 50th, 90th, 99th, and 99.9th percentile latencies\n        TODO: Verify latency goals: 99th percentile < 100 microseconds\n        TODO: Check for outliers that might indicate algorithmic problems\n        \"\"\"\n        pass\n```\n\n**Scope Boundary Enforcement:**\n```python\n# Code review checklist to prevent scope creep:\nFORBIDDEN_PATTERNS = [\n    \"import gc\",          # No garbage collection dependence\n    \"threading.Lock\",     # No blocking synchronization primitives  \n    \"time.sleep\",         # No blocking operations in data structure code\n    \"malloc.h\",          # No custom memory management beyond hazard pointers\n    \"__platform__\",      # No platform-specific optimizations\n]\n\ndef check_scope_compliance(source_files):\n    \"\"\"\n    TODO: Scan source files for forbidden patterns\n    TODO: Flag any imports or code that violates non-goals\n    TODO: Ensure implementation stays within defined boundaries\n    \"\"\"\n    pass\n```\n\n#### Milestone Checkpoints\n\n**After Milestone 1 (Atomic Operations):**\n- Run: `python -m pytest tests/unit/atomic/` \n- Expected: All tests pass, including ABA problem demonstration\n- Manual verification: Counter stress test with 8 threads × 10,000 increments = exactly 80,000\n\n**After Milestone 2 (Lock-free Stack):**\n- Run: `python examples/stack_benchmark.py`\n- Expected: Stack throughput > 1M ops/sec on 4 cores, linearizability verified\n- Manual verification: No crashes during 60-second high-contention stress test\n\n**After Milestone 3 (Lock-free Queue):**\n- Run: `python tests/integration/queue_fifo_test.py`\n- Expected: Perfect FIFO ordering preserved across 100,000 concurrent operations\n- Manual verification: Queue performance comparable to or better than stack\n\n**After Milestone 4 (Hazard Pointers):**\n- Run: `python tests/memory/leak_detection.py`\n- Expected: No memory leaks, all retired nodes eventually reclaimed\n- Manual verification: Memory usage remains bounded during extended operation\n\n**After Milestone 5 (Hash Map):**\n- Run: `python tests/performance/scaling_benchmark.py`\n- Expected: Hash map scales to at least 16 threads with good throughput\n- Manual verification: Resize operations complete without corrupting existing data\n\n\n## High-Level Architecture\n\n> **Milestone(s):** All milestones (this section establishes the overall system design and progression strategy for building each component incrementally)\n\nThe architecture of our lock-free data structure library follows a carefully designed layered approach that mirrors how concurrent programming knowledge builds from simple atomic operations to complex data structures. Like constructing a skyscraper, each layer provides a stable foundation for the next, with dependencies flowing upward and abstractions hiding complexity downward.\n\n### Component Layers\n\nOur lock-free data structure library employs a **three-tier architecture** that separates concerns and establishes clear dependency relationships between components. This layered design ensures that complex data structures can be built on proven atomic primitives while maintaining clean interfaces and testability at each level.\n\n![System Component Architecture](./diagrams/system-architecture.svg)\n\n#### Layer 1: Atomic Operations Foundation\n\nThe foundation layer provides the primitive building blocks that all lock-free algorithms depend upon. Think of this layer as the basic tools in a craftsman's workshop - hammers, screwdrivers, and measuring instruments that are used to build more complex creations. Without reliable atomic operations, lock-free data structures would be impossible to implement correctly.\n\nThis layer encapsulates the platform-specific details of atomic memory operations and presents a clean, consistent interface to higher layers. The atomic operations foundation handles the intricate details of memory ordering semantics, cache coherence protocols, and hardware-specific instruction sequences that implement compare-and-swap operations.\n\n| Component | Responsibility | Key Types | Primary Methods |\n|-----------|---------------|-----------|-----------------|\n| `AtomicReference` | Single-word atomic operations with versioning | `AtomicReference`, `TaggedPointer` | `compare_and_swap()`, `load()`, `store()` |\n| `MemoryOrdering` | Memory consistency model enforcement | `MemoryOrdering` constants | Ordering constraint specifications |\n| Memory Barriers | Instruction reordering prevention | Fence primitives | `acquire_fence()`, `release_fence()` |\n| ABA Detection | Pointer reuse problem mitigation | Version counters, tagged pointers | `increment_version()`, `extract_pointer()` |\n\n> **Decision: Atomic Wrapper Layer**\n> - **Context**: Raw hardware atomics are platform-specific and error-prone to use directly in data structure implementations\n> - **Options Considered**: Direct hardware atomic usage, thin wrapper layer, full abstraction with runtime dispatch\n> - **Decision**: Thin wrapper layer with compile-time specialization for memory ordering\n> - **Rationale**: Provides safety and consistency without performance overhead while maintaining access to all memory ordering options\n> - **Consequences**: Slightly more complex implementation but dramatically reduces bugs in higher-layer code and improves portability\n\nThe atomic operations layer must handle the **ABA problem** - a fundamental challenge where a memory location changes from value A to B and back to A between a thread's read and subsequent compare-and-swap operation. The `TaggedPointer` type solves this by combining a pointer with a monotonically increasing version counter, ensuring that even if a pointer value is reused, the tag will differ.\n\n| Memory Ordering | Use Case | Performance | Guarantees |\n|----------------|----------|-------------|------------|\n| `RELAXED` | Counters, statistics | Highest | No ordering constraints |\n| `ACQUIRE` | Loading shared pointers | High | Prevents later operations from moving before |\n| `RELEASE` | Publishing shared data | High | Prevents earlier operations from moving after |\n| `SEQ_CST` | Critical correctness paths | Lowest | Total global ordering |\n\n#### Layer 2: Lock-free Data Structures\n\nThe data structures layer implements the core concurrent algorithms that provide familiar collection interfaces without using locks. This layer transforms the low-level atomic operations into higher-level abstractions that application developers can use confidently. Think of this as the furniture built by a skilled carpenter using the basic tools - each piece serves a specific purpose and hides the complexity of its construction.\n\nEach data structure in this layer maintains specific invariants and provides linearizability guarantees, meaning that despite concurrent operations from multiple threads, the data structure appears to behave as if operations occur atomically at specific points in time.\n\n| Data Structure | Algorithm | Key Innovation | Complexity |\n|----------------|-----------|----------------|------------|\n| `TreiberStack` | Lock-free stack | Single CAS on top pointer | O(1) per operation |\n| `MichaelScottQueue` | Lock-free FIFO queue | Dual pointers with helping | O(1) per operation |\n| `SplitOrderedHashMap` | Lock-free hash table | Recursive bucket splitting | O(1) average per operation |\n\n> **Decision: Treiber Stack as First Data Structure**\n> - **Context**: Need a simple lock-free data structure to demonstrate atomic operations usage\n> - **Options Considered**: Lock-free stack, lock-free queue, lock-free list\n> - **Decision**: Treiber stack implementation first\n> - **Rationale**: Single-pointer CAS operations are simpler to understand than dual-pointer algorithms, and stack operations have clear linearization points\n> - **Consequences**: Provides foundation concepts for more complex structures but doesn't demonstrate helping mechanisms\n\nThe **Treiber stack** serves as the simplest lock-free data structure, using a single `compare_and_swap` operation on the top-of-stack pointer. Push operations prepend nodes atomically, while pop operations remove the top node, both retrying if another thread modified the stack concurrently.\n\n| Operation | Linearization Point | CAS Target | Retry Condition |\n|-----------|-------------------|------------|-----------------|\n| `push()` | Successful CAS of new top | `top` pointer | Another thread changed top |\n| `pop()` | Successful CAS of new top | `top` pointer | Stack empty or top changed |\n\nThe **Michael-Scott queue** demonstrates more advanced techniques with its dual-pointer design and helping mechanism. The algorithm maintains separate head and tail pointers, with a dummy sentinel node that simplifies empty queue handling. When threads observe that the tail pointer is lagging behind the actual end of the queue, they help advance it before attempting their own operations.\n\n| Queue State | Head Points To | Tail Points To | Invariant |\n|-------------|---------------|----------------|-----------|\n| Empty | Sentinel node | Sentinel node | `head == tail` |\n| Single element | Sentinel node | Data node | `head->next == tail` |\n| Multiple elements | Sentinel node | Last or second-to-last | `tail` at or near end |\n\n#### Layer 3: Memory Reclamation Management\n\nThe memory management layer solves the critical problem of when it's safe to deallocate memory in a lock-free environment. Traditional reference counting doesn't work because incrementing and decrementing references atomically while accessing the data requires multiple atomic operations, breaking the lock-free property. This layer implements **hazard pointers**, which provide a non-blocking solution to safe memory reclamation.\n\nThink of hazard pointers like safety signs at a construction site. Before a worker enters a dangerous area, they post a sign indicating their presence. The demolition crew checks for these signs before bringing down any structures. Similarly, threads announce their intention to access shared nodes through hazard pointers, and the memory reclamation system checks these announcements before freeing memory.\n\n| Component | Purpose | Key Operations | Thread Safety |\n|-----------|---------|----------------|---------------|\n| `HazardPointer` | Per-thread protection slots | `protect()`, `release()` | Thread-local access |\n| `RetirementList` | Deferred deletion queue | `retire()`, `scan()` | Lock-free append/scan |\n| `MemoryReclaimer` | Batch reclamation coordinator | `reclaim_batch()` | Global coordination |\n\n> **Decision: Hazard Pointers Over Epoch-Based Reclamation**\n> - **Context**: Need safe memory reclamation without blocking or unbounded memory growth\n> - **Options Considered**: Hazard pointers, epoch-based reclamation, reference counting\n> - **Decision**: Hazard pointers with per-thread retirement lists\n> - **Rationale**: Provides deterministic memory reclamation without requiring global synchronization epochs, and integrates cleanly with existing data structure operations\n> - **Consequences**: Requires explicit protect/release calls but provides stronger progress guarantees than epoch-based approaches\n\nThe hazard pointer protocol follows a specific sequence for safe memory access:\n\n1. **Protection Phase**: Thread loads a pointer to a shared node and immediately announces this pointer in its hazard pointer slot\n2. **Validation Phase**: Thread re-reads the original pointer location to ensure the node hasn't been removed and replaced\n3. **Access Phase**: Thread safely accesses the protected node's data and follows next pointers\n4. **Release Phase**: Thread clears its hazard pointer slot when finished with the node\n\n| Hazard Pointer State | Protected Nodes | Retirement List | Reclamation Status |\n|---------------------|----------------|-----------------|-------------------|\n| No hazards active | None | Growing | Safe to reclaim all |\n| Multiple hazards | Currently accessed | Blocked items | Partial reclamation |\n| Scan in progress | All hazards | Filtered | Batch reclamation |\n\n### Recommended Module Structure\n\nThe module organization reflects the layered architecture while providing clear boundaries between components and supporting incremental development. This structure allows developers to work on one layer at a time while maintaining clean dependencies and avoiding circular imports.\n\n```\nlockfree_library/\n├── atomic/\n│   ├── __init__.py              # Atomic operations public interface\n│   ├── operations.py            # AtomicReference, compare_and_swap\n│   ├── memory_ordering.py       # MemoryOrdering constants and semantics\n│   ├── tagged_pointer.py        # TaggedPointer for ABA prevention\n│   └── tests/\n│       ├── test_atomic_ops.py   # Basic atomic operation correctness\n│       ├── test_memory_order.py # Memory ordering validation\n│       └── test_aba_detection.py # ABA problem demonstration\n├── structures/\n│   ├── __init__.py              # Data structures public interface\n│   ├── stack.py                 # TreiberStack implementation\n│   ├── queue.py                 # MichaelScottQueue implementation\n│   ├── hashmap.py               # SplitOrderedHashMap implementation\n│   ├── node.py                  # Shared Node type definitions\n│   └── tests/\n│       ├── test_stack.py        # Stack correctness and linearizability\n│       ├── test_queue.py        # Queue FIFO ordering verification\n│       ├── test_hashmap.py      # Hash map concurrent operations\n│       └── stress_tests.py      # High-contention performance tests\n├── memory/\n│   ├── __init__.py              # Memory reclamation public interface\n│   ├── hazard_pointers.py       # HazardPointer registry and protocol\n│   ├── retirement.py            # RetirementList and scanning logic\n│   ├── reclamation.py           # MemoryReclaimer coordination\n│   └── tests/\n│       ├── test_hazard_ptrs.py  # Hazard pointer correctness\n│       ├── test_retirement.py   # Retirement list functionality\n│       └── test_integration.py  # End-to-end memory safety\n├── verification/\n│   ├── __init__.py              # Testing utilities public interface\n│   ├── linearizability.py       # LinearizabilityChecker implementation\n│   ├── stress_testing.py        # Concurrent stress test framework\n│   ├── operation_tracker.py     # Operation recording and analysis\n│   └── benchmarks/\n│       ├── throughput_bench.py  # Operations per second measurement\n│       ├── contention_bench.py  # High-thread-count scenarios\n│       └── comparison_bench.py  # Lock-free vs lock-based comparison\n└── examples/\n    ├── basic_usage.py           # Simple stack and queue examples\n    ├── producer_consumer.py     # Multi-threaded producer/consumer\n    ├── concurrent_counter.py    # Atomic counter demonstration\n    └── hash_map_demo.py         # Concurrent hash map operations\n```\n\n| Module | Dependencies | Public Interface | Internal Components |\n|--------|-------------|------------------|-------------------|\n| `atomic/` | System atomics | `AtomicReference`, `TaggedPointer`, `MemoryOrdering` | Platform wrappers, ABA detection |\n| `structures/` | `atomic/` | `TreiberStack`, `MichaelScottQueue`, `SplitOrderedHashMap` | Node management, algorithms |\n| `memory/` | `atomic/` | `HazardPointer`, `retire()`, `protect()` | Scanning, reclamation batching |\n| `verification/` | `atomic/`, `structures/` | `LinearizabilityChecker`, `stress_test_data_structure` | History recording, analysis |\n\n> **Decision: Separate Verification Module**\n> - **Context**: Lock-free data structures require specialized testing approaches that differ from unit testing\n> - **Options Considered**: Inline test utilities, separate testing module, external test framework\n> - **Decision**: Dedicated verification module with linearizability checking\n> - **Rationale**: Concurrent correctness testing is complex enough to warrant its own module, and these utilities will be reused across all data structures\n> - **Consequences**: Additional module complexity but provides reusable testing infrastructure for all components\n\nThe module structure enforces dependency constraints through Python's import system. The `atomic/` module has no internal dependencies on other library modules, `structures/` depends only on `atomic/`, and `memory/` depends on `atomic/` but not `structures/`. This prevents circular dependencies and ensures that each layer can be developed and tested independently.\n\n| Import Direction | Allowed | Forbidden | Rationale |\n|------------------|---------|-----------|-----------|\n| `structures/` → `atomic/` | ✓ | | Data structures need atomic primitives |\n| `memory/` → `atomic/` | ✓ | | Hazard pointers need atomic operations |\n| `memory/` → `structures/` | | ✗ | Memory management should be structure-agnostic |\n| `atomic/` → `structures/` | | ✗ | Foundation layer shouldn't depend on higher layers |\n\n### Implementation Progression Strategy\n\nBuilding lock-free data structures requires a systematic progression from simple concepts to complex algorithms. This strategy minimizes the cognitive load at each step while ensuring that learners understand the fundamental principles before applying them to challenging scenarios. The progression mirrors how concurrent programming expertise develops naturally.\n\n#### Phase 1: Atomic Operations Mastery (Milestone 1)\n\nThe journey begins with understanding atomic operations as the fundamental building blocks. Think of this phase like learning to use basic tools before attempting to build furniture - you must understand how each tool works and when to use it before combining them into complex constructions.\n\n**Week 1-2 Focus Areas:**\n\n| Concept | Learning Goal | Validation Method | Common Mistakes |\n|---------|---------------|------------------|-----------------|\n| `compare_and_swap` | Implement retry loops correctly | CAS counter increment test | Infinite spinning without backoff |\n| Memory ordering | Choose appropriate ordering for use case | Ordering semantics quiz | Using `RELAXED` everywhere |\n| ABA problem | Recognize and prevent ABA scenarios | Demonstrate ABA with test case | Ignoring version counters |\n| Memory barriers | Understand visibility guarantees | Multi-thread visibility test | Assuming immediate visibility |\n\nThe first milestone establishes the mental model that **atomic operations are indivisible transactions** - they either complete entirely or not at all, with no intermediate states visible to other threads. Learners implement basic atomic wrappers and experiment with different memory ordering guarantees to understand their performance and correctness trade-offs.\n\n```python\n# Example progression: Start with simple atomic counter\natomic_counter = AtomicReference(0)\n\n# Progress to CAS-based operations\ndef increment_counter():\n    while True:\n        current = atomic_counter.load(ACQUIRE)\n        if atomic_counter.compare_and_swap(current, current + 1):\n            return current\n\n# Advance to ABA problem demonstration\ndef demonstrate_aba_problem():\n    # Show how naive CAS can succeed incorrectly\n    pass\n```\n\n> The critical insight in this phase is understanding that memory ordering is not just a performance optimization - it's a correctness requirement. Using `RELAXED` ordering everywhere might perform well but can lead to subtle bugs that only manifest under specific timing conditions.\n\n⚠️ **Pitfall: Assuming Sequential Consistency**\nNew lock-free programmers often assume that all operations are sequentially consistent, leading to code that works on strongly-ordered architectures like x86 but fails on weakly-ordered systems like ARM. Always explicitly specify memory ordering requirements rather than relying on defaults.\n\n#### Phase 2: Single Data Structure Implementation (Milestones 2-3)\n\nWith atomic operations mastered, learners progress to implementing their first complete lock-free data structure. The **Treiber stack** serves as the ideal introduction because it requires only single-word CAS operations and has clear linearization points, making correctness easier to reason about.\n\n**Week 3-4 Focus Areas:**\n\n| Data Structure Component | Implementation Challenge | Verification Method | Key Insight |\n|-------------------------|-------------------------|-------------------|-------------|\n| Stack node structure | Atomic next pointer design | Node linking test | Next pointers must be atomic |\n| Push operation | CAS retry with backoff | Concurrent push stress test | Linearization at CAS success |\n| Pop operation | Handle empty stack correctly | Empty stack edge cases | Check for null before CAS |\n| ABA prevention | Tagged pointer integration | ABA demonstration test | Version must increment on reuse |\n\nThe stack implementation teaches the fundamental pattern of **CAS retry loops** that appears in all lock-free algorithms:\n\n1. **Load Phase**: Read the current state of shared memory\n2. **Compute Phase**: Calculate the desired new state based on current state\n3. **Validate Phase**: Attempt to atomically update from old state to new state\n4. **Retry Phase**: If CAS failed, repeat from step 1 with exponential backoff\n\n| Stack Operation | Linearization Point | Success Condition | Failure Recovery |\n|----------------|-------------------|------------------|------------------|\n| `push(data)` | CAS success on top pointer | `top` unchanged since load | Retry with new top value |\n| `pop()` | CAS success on top pointer | Stack not empty, top unchanged | Return empty indicator |\n\nThe **Michael-Scott queue** follows as the second data structure, introducing the concepts of dual pointers, helping mechanisms, and sentinel nodes. This progression teaches how lock-free algorithms can coordinate multiple pointers atomically through careful sequencing of CAS operations.\n\n**Week 5-6 Focus Areas:**\n\n| Queue Component | Advanced Concept | Implementation Challenge | Teaching Goal |\n|----------------|------------------|-------------------------|---------------|\n| Dual pointers | Head/tail coordination | Avoid contention between enqueue/dequeue | Separate access patterns |\n| Sentinel node | Empty queue simplification | Initialize and maintain dummy node | Eliminate special cases |\n| Helping mechanism | Progress guarantee | Advance lagging tail pointer | Cooperative algorithms |\n| FIFO ordering | Linearizability proof | Maintain insertion order | Correctness verification |\n\n> **Decision: Stack Before Queue Implementation Order**\n> - **Context**: Both data structures teach essential lock-free patterns but have different complexity levels\n> - **Options Considered**: Start with queue (more practical), start with stack (simpler), implement simultaneously\n> - **Decision**: Implement Treiber stack first, then Michael-Scott queue\n> - **Rationale**: Single-pointer CAS is conceptually simpler than dual-pointer coordination, and success with stack builds confidence for queue complexity\n> - **Consequences**: Learners gain confidence with simpler algorithms before tackling helping mechanisms and dual-pointer coordination\n\n#### Phase 3: Memory Reclamation Integration (Milestone 4)\n\nThe third phase addresses the critical challenge of safe memory management in lock-free environments. **Hazard pointers** provide the solution, but integrating them with existing data structure operations requires careful attention to the protection protocol and reclamation timing.\n\n**Week 7-8 Focus Areas:**\n\n| Memory Management Aspect | Integration Challenge | Verification Method | Safety Guarantee |\n|--------------------------|---------------------|-------------------|------------------|\n| Hazard pointer protocol | Protect nodes during access | Use-after-free detection | No premature deallocation |\n| Retirement list management | Defer deletion safely | Memory leak monitoring | Bounded memory growth |\n| Scanning and reclamation | Batch free operations | Reclamation efficiency test | Progress without blocking |\n| Thread lifecycle | Cleanup on exit | Thread termination test | No resource leaks |\n\nThe hazard pointer integration transforms data structure operations from simple atomic updates to multi-phase protocols:\n\n1. **Acquisition Phase**: Load pointer and immediately protect it via hazard pointer\n2. **Validation Phase**: Re-check that the pointer is still valid in its original location\n3. **Access Phase**: Safely dereference and traverse the protected node\n4. **Release Phase**: Clear hazard pointer when access is complete\n5. **Retirement Phase**: Add removed nodes to retirement list instead of immediate deletion\n\n| Integration Point | Stack Modification | Queue Modification | Complexity Impact |\n|------------------|-------------------|-------------------|-------------------|\n| Node access | Protect before dereference | Protect head and tail loads | 2x overhead per access |\n| Node removal | Retire instead of delete | Retire dequeued nodes | Deferred reclamation |\n| Thread cleanup | Release all hazards on exit | Clear retirement list | Shutdown protocol |\n\n#### Phase 4: Advanced Data Structures (Milestone 5)\n\nThe final phase tackles the **split-ordered hash map**, which combines all previous concepts while introducing new challenges like incremental resizing and bucket coordination. This represents the culmination of lock-free programming skills.\n\n**Week 9-10 Focus Areas:**\n\n| Hash Map Component | Advanced Technique | Implementation Challenge | Mastery Goal |\n|-------------------|-------------------|-------------------------|--------------|\n| Split-ordered lists | Logical hash ordering | Maintain sorted order during splits | Understand recursive splitting |\n| Incremental resizing | Lock-free migration | Migrate without blocking operations | Non-blocking growth |\n| Bucket coordination | Multiple list management | Initialize parent before child | Dependency ordering |\n| Reverse bit ordering | Preserve structure during splits | Calculate correct insertion points | Mathematical precision |\n\nThe hash map implementation demonstrates how complex lock-free algorithms compose simpler techniques:\n\n- **Atomic operations** for bucket array updates and sentinel node management\n- **CAS retry loops** for insertion and deletion in sorted bucket lists\n- **Hazard pointers** for safe traversal of bucket chains during concurrent modifications\n- **Helping mechanisms** for assisting with bucket initialization and resizing operations\n\n| Phase Completion | Demonstrated Skills | Assessment Method | Readiness Indicator |\n|------------------|-------------------|------------------|-------------------|\n| Phase 1 | Atomic operation mastery | CAS-based counter stress test | No lost increments under contention |\n| Phase 2 | Data structure implementation | Linearizability verification | Stack/queue correctness under load |\n| Phase 3 | Memory management | No use-after-free errors | Clean hazard pointer integration |\n| Phase 4 | Advanced algorithm composition | Hash map performance benchmark | Competitive with lock-based alternatives |\n\n### Implementation Guidance\n\nThe progression strategy translates into concrete development steps with specific checkpoints and validation criteria at each phase. This guidance provides the technical foundation needed to implement the layered architecture successfully.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option | Production Option |\n|-----------|---------------|-----------------|------------------|\n| Atomic Operations | Python `threading` with locks | `ctypes` atomic wrappers | Rust `std::sync::atomic` |\n| Memory Ordering | Sequential consistency only | Explicit acquire/release | Full memory model support |\n| Testing Framework | Basic `unittest` | `pytest` with concurrency fixtures | Property-based testing with `hypothesis` |\n| Performance Monitoring | Manual timing | `cProfile` integration | Dedicated benchmarking suite |\n| Memory Debugging | Reference counting | `tracemalloc` monitoring | Valgrind or AddressSanitizer |\n\n#### Recommended File Structure for Implementation\n\n```python\n# Project organization following the layered architecture\nlockfree_library/\n├── atomic/\n│   ├── __init__.py\n│   │   from .operations import AtomicReference, compare_and_swap\n│   │   from .memory_ordering import MemoryOrdering, RELAXED, ACQUIRE, RELEASE, SEQ_CST\n│   │   from .tagged_pointer import TaggedPointer\n│   │\n│   ├── operations.py           # Core atomic operations\n│   ├── memory_ordering.py      # Memory consistency primitives  \n│   ├── tagged_pointer.py       # ABA prevention utilities\n│   └── platform_specific.py   # Hardware-specific implementations\n│\n├── structures/\n│   ├── __init__.py\n│   │   from .stack import TreiberStack\n│   │   from .queue import MichaelScottQueue  \n│   │   from .hashmap import SplitOrderedHashMap\n│   │\n│   ├── stack.py               # Treiber stack implementation\n│   ├── queue.py               # Michael-Scott queue implementation\n│   ├── hashmap.py             # Lock-free hash map implementation\n│   └── node.py                # Shared node type definitions\n│\n└── memory/\n    ├── __init__.py\n    │   from .hazard_pointers import HazardPointer, protect, release\n    │   from .reclamation import retire, reclaim_batch\n    │\n    ├── hazard_pointers.py      # Hazard pointer protocol\n    ├── retirement.py           # Retirement list management\n    └── reclamation.py          # Memory reclamation coordination\n```\n\n#### Infrastructure Starter Code (Complete Implementation)\n\n**Atomic Operations Foundation** (`atomic/operations.py`):\n\n```python\nimport threading\nfrom typing import Any, Tuple, Optional\nfrom enum import Enum\n\nclass MemoryOrdering(Enum):\n    RELAXED = \"relaxed\"\n    ACQUIRE = \"acquire\" \n    RELEASE = \"release\"\n    SEQ_CST = \"seq_cst\"\n\n# Constants for easy import\nRELAXED = MemoryOrdering.RELAXED\nACQUIRE = MemoryOrdering.ACQUIRE\nRELEASE = MemoryOrdering.RELEASE\nSEQ_CST = MemoryOrdering.SEQ_CST\n\nclass AtomicReference:\n    \"\"\"Thread-safe atomic reference with compare-and-swap support.\"\"\"\n    \n    def __init__(self, initial_value: Any = None):\n        self._value = initial_value\n        self._version = 0\n        self._lock = threading.Lock()  # For Python CAS simulation\n    \n    def load(self, ordering: MemoryOrdering = SEQ_CST) -> Any:\n        \"\"\"Atomically load the current value with specified memory ordering.\"\"\"\n        with self._lock:\n            return self._value\n    \n    def store(self, value: Any, ordering: MemoryOrdering = SEQ_CST) -> None:\n        \"\"\"Atomically store a new value with specified memory ordering.\"\"\"  \n        with self._lock:\n            self._value = value\n            self._version += 1\n    \n    def compare_and_swap(self, expected: Any, new_value: Any) -> Tuple[bool, Any]:\n        \"\"\"\n        Atomically compare current value with expected and swap if equal.\n        Returns (success: bool, observed_value: Any).\n        If success=False, observed_value contains the actual current value.\n        \"\"\"\n        with self._lock:\n            current = self._value\n            if current == expected:\n                self._value = new_value\n                self._version += 1\n                return (True, current)\n            else:\n                return (False, current)\n    \n    def fetch_and_add(self, increment: int) -> int:\n        \"\"\"Atomically increment the value and return the previous value.\"\"\"\n        with self._lock:\n            previous = self._value\n            self._value += increment\n            self._version += 1\n            return previous\n\nclass TaggedPointer:\n    \"\"\"Pointer with version tag to prevent ABA problems.\"\"\"\n    \n    def __init__(self, pointer: Any = None, tag: int = 0):\n        self.pointer = pointer\n        self.tag = tag\n    \n    def __eq__(self, other):\n        if not isinstance(other, TaggedPointer):\n            return False\n        return self.pointer == other.pointer and self.tag == other.tag\n    \n    def increment_tag(self) -> 'TaggedPointer':\n        \"\"\"Create a new TaggedPointer with incremented tag.\"\"\"\n        return TaggedPointer(self.pointer, self.tag + 1)\n\n# Global atomic operation helpers\ndef compare_and_swap(atomic_ref: AtomicReference, expected: Any, new_value: Any) -> Tuple[bool, Any]:\n    \"\"\"Helper function for CAS operations.\"\"\"\n    return atomic_ref.compare_and_swap(expected, new_value)\n\ndef fetch_and_add(atomic_ref: AtomicReference, increment: int) -> int:\n    \"\"\"Helper function for atomic increment.\"\"\"\n    return atomic_ref.fetch_and_add(increment)\n```\n\n**Node Types** (`structures/node.py`):\n\n```python\nfrom typing import Any, Optional\nfrom atomic.operations import AtomicReference, TaggedPointer\n\nclass Node:\n    \"\"\"Generic node for lock-free data structures.\"\"\"\n    \n    def __init__(self, data: Any, next_node: Optional['Node'] = None):\n        self.data = data\n        # Use TaggedPointer to prevent ABA problems\n        tagged_next = TaggedPointer(next_node, 0) if next_node else TaggedPointer(None, 0)\n        self.next = AtomicReference(tagged_next)\n    \n    def get_next(self) -> Optional['Node']:\n        \"\"\"Get the next node pointer safely.\"\"\"\n        tagged = self.next.load()\n        return tagged.pointer if tagged else None\n    \n    def set_next(self, next_node: Optional['Node'], expected_tag: int = 0) -> bool:\n        \"\"\"Set the next node using CAS with tag increment.\"\"\"\n        current_tagged = self.next.load()\n        if current_tagged.tag != expected_tag:\n            return False\n        \n        new_tagged = TaggedPointer(next_node, current_tag.tag + 1)\n        success, _ = self.next.compare_and_swap(current_tagged, new_tagged)\n        return success\n\nclass SentinelNode(Node):\n    \"\"\"Special node that marks boundaries in data structures.\"\"\"\n    \n    def __init__(self, key: Any = None):\n        super().__init__(None)  # Sentinel nodes carry no data\n        self.key = key  # For hash map bucket boundaries\n        self.is_sentinel = True\n```\n\n#### Core Logic Skeleton Code (For Learner Implementation)\n\n**Treiber Stack Skeleton** (`structures/stack.py`):\n\n```python\nfrom typing import Any, Optional\nfrom atomic.operations import AtomicReference, TaggedPointer, compare_and_swap\nfrom structures.node import Node\nimport time\nimport random\n\nclass TreiberStack:\n    \"\"\"Lock-free stack using the Treiber algorithm.\"\"\"\n    \n    def __init__(self):\n        # Stack top starts as empty (None pointer with version 0)\n        self.top = AtomicReference(TaggedPointer(None, 0))\n    \n    def push(self, data: Any) -> None:\n        \"\"\"\n        Push a new element onto the top of the stack.\n        Uses CAS retry loop to handle concurrent modifications.\n        \"\"\"\n        new_node = Node(data)\n        \n        # TODO 1: Implement CAS retry loop for push operation\n        # TODO 2: Load current top pointer with appropriate memory ordering\n        # TODO 3: Set new node's next pointer to current top\n        # TODO 4: Attempt CAS to make new node the top\n        # TODO 5: If CAS fails, implement exponential backoff before retry\n        # TODO 6: Continue until CAS succeeds (lock-free progress guarantee)\n        \n        # Hint: Use TaggedPointer to prevent ABA problems\n        # Hint: Increment version tag on each CAS attempt\n        pass\n    \n    def pop(self) -> Optional[Any]:\n        \"\"\"\n        Pop and return the top element from the stack.\n        Returns None if stack is empty.\n        \"\"\"\n        \n        # TODO 1: Implement CAS retry loop for pop operation  \n        # TODO 2: Load current top pointer and check for empty stack\n        # TODO 3: If empty, return None immediately\n        # TODO 4: Load the next pointer from top node (this becomes new top)\n        # TODO 5: Attempt CAS to update top to next node\n        # TODO 6: If CAS fails due to ABA, increment backoff and retry\n        # TODO 7: If CAS succeeds, return the popped node's data\n        # TODO 8: Handle memory reclamation (retire node for later cleanup)\n        \n        # Hint: Check for None before dereferencing next pointer\n        # Hint: ABA can occur if node is popped and pushed back\n        pass\n    \n    def is_empty(self) -> bool:\n        \"\"\"Check if stack is empty (top pointer is None).\"\"\"\n        # TODO: Load top pointer and check if pointer component is None\n        pass\n    \n    def _exponential_backoff(self, attempt: int) -> None:\n        \"\"\"Implement exponential backoff to reduce contention.\"\"\"\n        # TODO: Sleep for exponentially increasing time based on attempt count\n        # TODO: Add random jitter to avoid thundering herd\n        pass\n```\n\n#### Milestone Checkpoint Guidelines\n\n**Milestone 1 Checkpoint - Atomic Operations:**\n```bash\n# Run atomic operations test suite\npython -m pytest atomic/tests/ -v\n\n# Expected output should show:\n# ✓ test_compare_and_swap_success\n# ✓ test_compare_and_swap_failure  \n# ✓ test_aba_problem_demonstration\n# ✓ test_concurrent_counter_increment\n# ✓ test_memory_ordering_semantics\n```\n\n**Manual Verification Steps:**\n1. Create atomic counter with initial value 0\n2. Start 10 threads, each incrementing counter 1000 times\n3. Verify final counter value is exactly 10000 (no lost updates)\n4. Demonstrate ABA problem with tagged pointer solution\n\n**Milestone 2 Checkpoint - Treiber Stack:**\n```python\n# Stress test the implemented stack\nfrom structures.stack import TreiberStack\nimport threading\nimport random\n\ndef stress_test_data_structure():\n    stack = TreiberStack()\n    results = []\n    \n    def push_worker():\n        for i in range(1000):\n            stack.push(f\"item_{threading.current_thread().ident}_{i}\")\n    \n    def pop_worker():\n        local_results = []\n        for i in range(500):\n            item = stack.pop()\n            if item is not None:\n                local_results.append(item)\n        results.extend(local_results)\n    \n    # Start concurrent push/pop operations\n    threads = []\n    for _ in range(5):\n        threads.append(threading.Thread(target=push_worker))\n        threads.append(threading.Thread(target=pop_worker))\n    \n    for t in threads:\n        t.start()\n    for t in threads:\n        t.join()\n    \n    print(f\"Total items popped: {len(results)}\")\n    print(f\"No duplicate items: {len(results) == len(set(results))}\")\n    print(f\"Stack final state empty: {stack.is_empty()}\")\n\n# Success criteria:\n# - No duplicate items in results (linearizability)\n# - No crashes or infinite loops (progress guarantee)  \n# - Stack operations complete in reasonable time\n```\n\n#### Language-Specific Implementation Hints\n\n**Python-Specific Considerations:**\n- Use `threading.Lock()` temporarily to simulate atomic operations until C extensions are available\n- Implement `__eq__` and `__hash__` methods for `TaggedPointer` to support proper comparison semantics\n- Use `typing.Optional` and `typing.Any` for clear type annotations in concurrent code\n- Consider `weakref` for avoiding circular references in node structures\n\n**Memory Management:**\n- Python's garbage collector handles basic memory reclamation, but hazard pointers still prevent use-after-free in concurrent access\n- Use `threading.local()` for per-thread hazard pointer storage\n- Monitor memory usage with `tracemalloc` to detect leaks in retirement lists\n\n**Performance Optimization:**\n- Profile with `cProfile` to identify contention bottlenecks\n- Use `time.sleep(0)` for minimal backoff in retry loops\n- Consider `multiprocessing` for CPU-bound workloads that exceed GIL limitations\n\n**Debugging Techniques:**\n- Add operation logging with thread IDs and timestamps for race condition analysis\n- Use `threading.current_thread().ident` to track per-thread behavior\n- Implement assertion checks for data structure invariants after each operation\n\n\n## Atomic Operations Foundation\n\n> **Milestone(s):** Milestone 1 (Atomic Operations) - This section provides the fundamental building blocks and theoretical foundation required for all subsequent lock-free data structure implementations.\n\nThe foundation of lock-free programming rests on atomic operations - indivisible units of computation that provide the coordination primitives necessary for thread-safe concurrent programming without locks. Understanding atomic operations is crucial because they represent the lowest level of abstraction where we can guarantee correctness in the presence of concurrent access from multiple threads. Every lock-free data structure we build in subsequent milestones will fundamentally depend on the properties and guarantees provided by these atomic primitives.\n\n### Mental Model: Atomic Operations as Indivisible Transactions\n\nThink of atomic operations as bank transactions that either complete entirely or not at all - there's never a moment where the account is in an inconsistent intermediate state visible to other customers. When you transfer money from checking to savings, other customers never see a moment where the money has left checking but hasn't yet arrived in savings. The bank's computer systems ensure this transfer appears instantaneous and indivisible from everyone else's perspective, even though internally it might involve multiple steps.\n\nSimilarly, atomic operations in concurrent programming provide this same \"all-or-nothing\" guarantee. When one thread performs an atomic increment on a shared counter, other threads never observe the counter in a half-incremented state. They either see the value before the increment or after the increment - never anything in between. This indivisibility property is what makes atomic operations the foundation for building correct concurrent algorithms without the complexity and performance overhead of traditional locks.\n\nThe key insight is that atomic operations move the complexity of coordination from application code into the hardware and operating system. Instead of writing complex locking protocols to protect shared data, we leverage atomic primitives that the CPU guarantees will execute atomically with respect to all other memory operations. This shifts our focus from \"how do we prevent race conditions\" to \"how do we design algorithms that use atomic operations to achieve the desired behavior.\"\n\nConsider a scenario where multiple threads need to add items to a shared counter. With locks, each thread would acquire a mutex, read the counter, increment it, write it back, then release the mutex. This creates a bottleneck where threads must wait for each other, and introduces the possibility of deadlock if multiple locks are involved. With atomic operations, each thread can use an atomic fetch-and-add operation that performs the entire read-modify-write cycle atomically, allowing multiple threads to update the counter concurrently without coordination overhead.\n\n### Memory Ordering and Consistency Models\n\nMemory ordering defines the constraints on how memory operations can be reordered by the compiler and CPU for performance optimization, while still maintaining the correctness guarantees that concurrent programs require. Modern processors and compilers perform aggressive optimizations that reorder instructions and memory accesses to maximize performance, but these optimizations can break the assumptions that naive concurrent programs make about the order in which operations become visible to other threads.\n\nThe memory ordering models provide a spectrum of trade-offs between performance and synchronization guarantees. Stricter ordering models provide stronger guarantees about the relative ordering of memory operations across threads, but may prevent certain performance optimizations. Relaxed ordering models allow more aggressive optimization but require programmers to carefully reason about which orderings are actually necessary for correctness.\n\n**Memory Ordering Models:**\n\n| Ordering Type | Visibility Guarantees | Performance Impact | Use Cases |\n|---------------|----------------------|-------------------|-----------|\n| `RELAXED` | No ordering constraints on other operations | Highest performance, minimal barriers | Counters, statistics where exact ordering doesn't matter |\n| `ACQUIRE` | Prevents reordering of subsequent operations before this load | Medium performance impact | Reading shared data that was published with RELEASE |\n| `RELEASE` | Prevents reordering of previous operations after this store | Medium performance impact | Publishing shared data for other threads to read |\n| `SEQ_CST` | Sequential consistency - global total order of all operations | Highest synchronization cost | Complex synchronization where total ordering is required |\n\n> **Decision: Progressive Memory Ordering Strategy**\n> - **Context**: Memory ordering is one of the most complex aspects of lock-free programming, and incorrect ordering can cause subtle bugs that are difficult to reproduce and debug\n> - **Options Considered**: \n>   1. Use sequential consistency everywhere for simplicity\n>   2. Use relaxed ordering everywhere for performance\n>   3. Start with sequential consistency, then optimize to weaker orderings where safe\n> - **Decision**: Start with sequential consistency for initial implementations, then selectively relax ordering constraints where performance analysis shows bottlenecks\n> - **Rationale**: Sequential consistency provides the strongest correctness guarantees and matches most programmers' intuitions about memory behavior, making it easier to reason about algorithm correctness during development\n> - **Consequences**: Initial implementations may have suboptimal performance due to unnecessary memory barriers, but will be easier to verify for correctness\n\nThe `RELAXED` ordering provides no synchronization guarantees beyond the atomicity of the individual operation itself. Operations with relaxed ordering can be freely reordered with respect to other memory operations, and different threads may observe relaxed atomic operations occurring in different orders. This makes relaxed ordering suitable for simple cases like incrementing counters where the exact order of increments doesn't affect correctness, only the final total value.\n\n`ACQUIRE` ordering is used on load operations and ensures that no memory operations that appear after the acquire load in program order can be reordered to occur before the load. This creates a one-way barrier where subsequent operations cannot move earlier, but previous operations can still move later. Acquire semantics are typically used when reading a pointer or flag that indicates shared data is ready to be accessed.\n\n`RELEASE` ordering is used on store operations and ensures that no memory operations that appear before the release store in program order can be reordered to occur after the store. This creates a one-way barrier where previous operations cannot move later, but subsequent operations can still move earlier. Release semantics are typically used when writing a pointer or flag to indicate that shared data has been published and is ready for other threads to access.\n\n`SEQ_CST` (sequential consistency) ordering provides the strongest guarantees by ensuring that all sequentially consistent operations appear to occur in some global total order that is consistent with the program order within each thread. This means that all threads observe the same ordering of sequentially consistent operations, which matches most programmers' intuitive expectations about how memory should behave in concurrent programs.\n\n**Memory Ordering State Machine:**\n\n| Current State | Operation Type | Memory Ordering | Next State | Visibility Effects |\n|---------------|---------------|----------------|------------|-------------------|\n| Unordered | Load | RELAXED | Unordered | No constraints on other operations |\n| Unordered | Load | ACQUIRE | Synchronized | Subsequent operations cannot move before |\n| Unordered | Store | RELAXED | Unordered | No constraints on other operations |\n| Unordered | Store | RELEASE | Synchronized | Previous operations cannot move after |\n| Synchronized | Any Operation | Any | Synchronized | Maintains synchronization guarantees |\n\n![Memory Ordering State Machine](./diagrams/memory-ordering-states.svg)\n\nThe interaction between acquire and release operations forms the foundation of lock-free synchronization patterns. When thread A performs a release store followed by thread B performing an acquire load that reads the value written by A, a synchronizes-with relationship is established. This relationship ensures that all memory operations that occurred before the release store in thread A become visible to thread B before any operations that occur after the acquire load.\n\n### Compare-and-Swap (CAS) Operation\n\nThe **compare-and-swap** operation is the fundamental building block that makes lock-free programming possible. CAS atomically compares the current value of a memory location against an expected value, and if they match, replaces the current value with a new value. The operation returns both a boolean indicating success or failure and the actual value that was observed at the memory location. This combination of conditional update and value observation is what enables lock-free algorithms to make progress even when multiple threads are competing to modify the same memory location.\n\nThe power of CAS lies in its ability to detect interference from other threads. When a thread loads a value, performs some computation based on that value, and then attempts to update the location using CAS with the originally loaded value as the expected value, the CAS will fail if any other thread modified the location in the meantime. This failure detection enables the thread to retry the operation with the new value, ensuring that updates are always based on current information rather than stale data.\n\n**CAS Operation Interface:**\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `compare_and_swap` | `expected` (current expected value), `new_value` (value to store) | `(success: bool, observed: value)` | Atomically compare and conditionally swap |\n| `load` | `ordering` (memory ordering constraint) | `value` (current value) | Atomically load current value |\n| `store` | `value` (new value), `ordering` (memory ordering) | None | Atomically store new value |\n| `fetch_and_add` | `increment` (value to add) | `previous_value` (value before addition) | Atomically add and return previous value |\n\nThe typical pattern for using CAS involves a retry loop that continues until the operation succeeds. The loop loads the current value, computes the desired new value based on the current value, then attempts to CAS the new value using the loaded value as the expected value. If the CAS fails because another thread modified the location, the loop repeats with the newly observed value.\n\n![CAS Retry Loop State Machine](./diagrams/cas-retry-pattern.svg)\n\n**CAS Retry Loop Algorithm:**\n\n1. Load the current value from the shared memory location using appropriate memory ordering\n2. Compute the desired new value based on the current value and the operation being performed\n3. Attempt to CAS the new value using the loaded value as the expected value\n4. If CAS succeeds, the operation is complete and the function returns the appropriate result\n5. If CAS fails, examine the observed value returned by the failed CAS\n6. Apply exponential backoff delay to reduce contention if configured\n7. Use the observed value as the new current value and return to step 2\n\nThe success of this pattern depends critically on the operation being **idempotent** or at least **retryable** with updated inputs. The computation in step 2 must be designed so that it can be safely repeated multiple times with different input values if other threads cause CAS failures.\n\n> The critical insight is that CAS failure is not an error condition - it's the normal mechanism by which lock-free algorithms detect and adapt to interference from other threads. A well-designed lock-free algorithm should handle CAS failures gracefully and efficiently.\n\n**CAS Retry Patterns:**\n\n| Pattern | When to Use | Retry Strategy | Example Operation |\n|---------|-------------|----------------|-------------------|\n| Simple Retry | Low contention expected | Immediate retry without backoff | Atomic counter increment |\n| Exponential Backoff | High contention possible | Delay increases exponentially | Shared data structure updates |\n| Bounded Retry | Real-time constraints | Fail after maximum attempts | Time-critical operations |\n| Helping Protocol | Complex multi-step operations | Assist other threads' operations | Lock-free queue operations |\n\n### ABA Problem and Solutions\n\nThe **ABA problem** represents one of the most subtle and dangerous pitfalls in lock-free programming. It occurs when a thread reads a value A from a shared location, performs some computation, then finds that the location still contains A when it attempts a CAS operation. The CAS succeeds, but the problem is that the value may have been changed to B and then back to A by other threads in the interim, meaning the assumption that \"nothing has changed\" is false even though the value appears unchanged.\n\nThis problem is particularly dangerous with pointer-based data structures where the same memory address might be reused for different objects after the original object has been freed and a new object allocated at the same address. A thread might load a pointer to a node, get interrupted, and when it resumes find that the pointer value is the same but now points to a completely different node that happens to be allocated at the same memory address.\n\nConsider a lock-free stack implementation where the top pointer points to node A, which points to node B. Thread 1 loads the top pointer (getting A) and prepares to pop the stack by setting top to A's next pointer (B). Before thread 1 can perform the CAS, thread 2 pops both A and B, then pushes A back onto the stack. Now thread 1's CAS succeeds because top still contains A, but A's next pointer may now point to some different node or be invalid, corrupting the stack structure.\n\n**ABA Problem Scenarios:**\n\n| Data Structure | ABA Manifestation | Corruption Type | Detection Method |\n|----------------|-------------------|-----------------|------------------|\n| Stack | Node reused at same address | Invalid next pointers | Tagged pointers with version counter |\n| Queue | Head/tail pointer reuse | Lost nodes or cycles | Hazard pointers + validation |\n| List | Node address recycling | Broken linkage | Generation counters on nodes |\n| Hash Table | Bucket pointer reuse | Inconsistent chains | Epoch-based reclamation |\n\nThe most common solution to the ABA problem is to use **tagged pointers** that combine the actual pointer value with a monotonically increasing version counter or tag. Instead of performing CAS on just the pointer value, the algorithm performs CAS on the combined pointer-plus-tag value. Each time the pointer is updated, the tag is incremented, ensuring that even if the pointer value cycles back to a previous value, the combined pointer-tag value will be different.\n\n**Tagged Pointer Structure:**\n\n| Field | Type | Description | Update Policy |\n|-------|------|-------------|---------------|\n| `pointer` | Memory address | Actual pointer to the object | Set to new target object address |\n| `tag` | Integer counter | Monotonically increasing version | Incremented on every pointer update |\n\n> **Decision: Tagged Pointer ABA Prevention**\n> - **Context**: The ABA problem can cause silent data corruption that is extremely difficult to detect and debug, making it one of the most dangerous aspects of lock-free programming\n> - **Options Considered**:\n>   1. Tagged pointers with version counters\n>   2. Hazard pointers for memory reclamation safety\n>   3. Epochs or generations for bulk reclamation\n> - **Decision**: Use tagged pointers as the primary ABA prevention mechanism for simple data structures like stacks\n> - **Rationale**: Tagged pointers provide strong ABA protection with minimal overhead and are easier to understand and implement correctly than hazard pointers\n> - **Consequences**: Requires atomic operations on larger values (pointer + tag), may reduce performance on some architectures, but provides strong correctness guarantees\n\nThe implementation of tagged pointers requires careful attention to the size limitations of atomic operations. On 64-bit systems, a tagged pointer might pack a 48-bit pointer with a 16-bit tag into a single 64-bit atomic value. The tag space must be large enough that it won't overflow and wrap around during the expected lifetime of the data structure, as tag wraparound could recreate ABA conditions.\n\n**Tagged Pointer Operations:**\n\n| Operation | Input | Output | Atomicity Requirement |\n|-----------|-------|--------|----------------------|\n| Load Tagged | Memory location | `(pointer, tag)` pair | Single atomic read of combined value |\n| Store Tagged | `(pointer, tag)` pair, location | None | Single atomic write of combined value |\n| CAS Tagged | Expected `(pointer, tag)`, new `(pointer, tag)` | `(success, observed)` | Atomic compare-and-swap of full value |\n| Increment Tag | Current `(pointer, tag)` | New `(pointer, tag+1)` | Must be done before CAS attempt |\n\nAn alternative approach for more complex data structures is to use hazard pointers, which we'll explore in detail in Milestone 4. Hazard pointers solve the ABA problem by preventing memory reclamation rather than by detecting address reuse. When a thread is about to access a node, it announces this intention by storing the node's address in a hazard pointer. The memory reclamation system scans all hazard pointers before freeing any node, ensuring that nodes currently being accessed by some thread are not reclaimed and their addresses reused.\n\n### Common Pitfalls with Atomics\n\nLock-free programming with atomic operations introduces several categories of subtle errors that can be extremely difficult to detect and debug. These pitfalls often result in code that works correctly under light testing but fails unpredictably under high contention or on different hardware architectures. Understanding these common mistakes is crucial for developing robust lock-free algorithms.\n\n⚠️ **Pitfall: Mixing Atomic and Non-Atomic Operations**\n\nOne of the most dangerous mistakes is mixing atomic and non-atomic operations on the same memory location. When some threads access a variable using atomic operations while other threads access it using regular loads and stores, the memory ordering guarantees are violated and the behavior becomes undefined.\n\nConsider a shared counter that is incremented atomically by worker threads but read non-atomically by a monitoring thread. The monitoring thread might observe torn reads where it sees partial updates, or it might miss updates entirely due to compiler optimizations that assume the variable doesn't change during the monitoring function. The solution is to ensure that ALL access to shared data uses atomic operations with appropriate memory ordering.\n\n⚠️ **Pitfall: Incorrect Memory Ordering Selection**\n\nUsing memory ordering that is too weak can cause subtle correctness bugs, while using ordering that is too strong can severely impact performance. A common mistake is using relaxed ordering for operations that actually require synchronization guarantees.\n\nFor example, using relaxed ordering when publishing a data structure after initialization can cause other threads to observe the published pointer before the data structure is fully initialized. The publishing thread should use release ordering on the store that makes the pointer visible, and consuming threads should use acquire ordering when loading the pointer. Mismatching these orderings can cause races where partially initialized data is accessed.\n\n⚠️ **Pitfall: Infinite Spinning in CAS Loops**\n\nCAS retry loops can spin indefinitely under high contention if not designed with appropriate backoff strategies. When many threads repeatedly attempt to CAS the same location, they can create a livelock situation where no thread makes progress because they keep interfering with each other.\n\nThe solution is to implement exponential backoff where threads wait for progressively longer periods after failed CAS attempts. This reduces contention by spreading out retry attempts over time. Additionally, bounded retry loops that eventually fall back to a different strategy can prevent infinite spinning in pathological cases.\n\n⚠️ **Pitfall: ABA Problem Ignorance**\n\nMany developers implement lock-free algorithms without considering the ABA problem, leading to data corruption that may only manifest under specific timing conditions. The corruption can be silent and go undetected for long periods, making debugging extremely difficult.\n\nThe most common scenario is implementing a lock-free stack using simple CAS on pointer values without tagged pointers or other ABA prevention. Under memory pressure where freed nodes are quickly reallocated, the same address can be reused for different nodes, causing CAS operations to succeed when they should fail.\n\n⚠️ **Pitfall: Assuming Sequential Consistency**\n\nMany programmers assume that atomic operations provide sequential consistency by default, but most atomic operations actually use relaxed ordering unless explicitly specified otherwise. This can lead to surprising reorderings that violate program logic.\n\nFor example, two atomic stores with relaxed ordering might become visible to other threads in the opposite order from how they appear in the source code. If the program logic depends on a specific ordering, the atomic operations must use appropriate memory ordering constraints to enforce that ordering.\n\n**Common Pitfalls Summary:**\n\n| Pitfall Category | Symptom | Root Cause | Detection Method | Fix |\n|------------------|---------|------------|------------------|-----|\n| Mixed Operations | Data corruption, torn reads | Non-atomic access to atomic variables | Static analysis, TSan | Make ALL access atomic |\n| Wrong Ordering | Subtle race conditions | Using relaxed when synchronization needed | Stress testing, model checking | Use acquire/release pairing |\n| Infinite Spinning | High CPU usage, no progress | CAS retry without backoff | Performance monitoring | Add exponential backoff |\n| ABA Problem | Silent data corruption | Pointer reuse between read and CAS | Extremely hard to detect | Use tagged pointers |\n| Ordering Assumptions | Unexpected reorderings | Assuming sequential consistency | Architecture-specific testing | Specify explicit ordering |\n\n### Implementation Guidance\n\nThe atomic operations foundation requires careful attention to both correctness and performance. The following implementation guidance provides concrete tools and patterns for building reliable atomic primitives that will serve as the foundation for all subsequent lock-free data structures.\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Atomic Primitives | `threading` module with basic atomics | `multiprocessing.Value` with ctypes |\n| Memory Ordering | Sequential consistency everywhere | Platform-specific ordering with `ctypes` |\n| Testing Framework | `unittest` with manual thread creation | `pytest` with `threading` and `concurrent.futures` |\n| Performance Monitoring | Basic timing with `time.time()` | `cProfile` with thread-aware analysis |\n\n**Recommended File Structure:**\n\n```\nproject-root/\n  atomic/\n    __init__.py              ← Public interface exports\n    primitives.py            ← Core atomic operations (AtomicReference, etc.)\n    memory_ordering.py       ← Memory ordering constants and utilities\n    tagged_pointer.py        ← Tagged pointer implementation for ABA prevention\n    cas_patterns.py          ← Common CAS retry patterns and backoff strategies\n  tests/\n    test_atomics.py         ← Unit tests for atomic operations\n    test_aba_problem.py     ← Specific tests for ABA problem scenarios\n    stress_test_atomics.py  ← High-contention stress tests\n  benchmarks/\n    atomic_performance.py   ← Performance comparison tests\n```\n\n**Core Atomic Operations Infrastructure:**\n\n```python\n\"\"\"\nComplete atomic operations infrastructure providing the foundation\nfor all lock-free data structures. This module handles the complexity\nof memory ordering and provides safe, high-level atomic primitives.\n\"\"\"\n\nimport threading\nimport ctypes\nfrom enum import Enum\nfrom typing import TypeVar, Generic, Tuple, Optional\nimport time\nimport random\n\nT = TypeVar('T')\n\nclass MemoryOrdering(Enum):\n    \"\"\"Memory ordering constraints for atomic operations.\"\"\"\n    RELAXED = \"relaxed\"      # No ordering constraints\n    ACQUIRE = \"acquire\"      # Acquire semantics for loads\n    RELEASE = \"release\"      # Release semantics for stores  \n    SEQ_CST = \"seq_cst\"      # Sequential consistency\n\nclass AtomicReference(Generic[T]):\n    \"\"\"\n    Thread-safe atomic reference with compare-and-swap support.\n    Provides the foundation for all lock-free data structure operations.\n    \"\"\"\n    \n    def __init__(self, initial_value: T):\n        self._value = initial_value\n        self._lock = threading.Lock()  # Used only for atomic read-modify-write\n        self.version = 0  # Simple version counter for ABA prevention\n    \n    def load(self, ordering: MemoryOrdering = MemoryOrdering.SEQ_CST) -> T:\n        \"\"\"\n        Atomically load the current value.\n        \n        Args:\n            ordering: Memory ordering constraint for the load operation\n            \n        Returns:\n            Current value stored in the atomic reference\n        \"\"\"\n        # TODO 1: Implement memory barrier based on ordering parameter\n        # TODO 2: Return current value with appropriate synchronization\n        # Hint: For simple implementation, all orderings can use the lock\n        pass\n    \n    def store(self, value: T, ordering: MemoryOrdering = MemoryOrdering.SEQ_CST) -> None:\n        \"\"\"\n        Atomically store a new value.\n        \n        Args:\n            value: New value to store\n            ordering: Memory ordering constraint for the store operation\n        \"\"\"\n        # TODO 1: Implement memory barrier based on ordering parameter  \n        # TODO 2: Store new value with appropriate synchronization\n        # TODO 3: Increment version counter for ABA prevention\n        pass\n    \n    def compare_and_swap(self, expected: T, new_value: T) -> Tuple[bool, T]:\n        \"\"\"\n        Atomically compare current value with expected and swap if equal.\n        \n        Args:\n            expected: Value we expect to find in the atomic reference\n            new_value: Value to store if current value equals expected\n            \n        Returns:\n            Tuple of (success: bool, observed_value: T)\n            - success: True if swap occurred, False if current != expected\n            - observed_value: The actual value that was in the reference\n        \"\"\"\n        # TODO 1: Acquire exclusive access to the reference\n        # TODO 2: Load current value and compare with expected\n        # TODO 3: If equal, store new_value and increment version\n        # TODO 4: Return success status and observed value\n        # TODO 5: Ensure proper memory ordering semantics\n        pass\n    \n    def fetch_and_add(self, increment: int) -> T:\n        \"\"\"\n        Atomically add increment to current value and return previous value.\n        Only works with numeric types.\n        \n        Args:\n            increment: Value to add to current value\n            \n        Returns:\n            Previous value before the addition\n        \"\"\"\n        # TODO 1: Implement using compare_and_swap retry loop\n        # TODO 2: Load current value\n        # TODO 3: Compute new_value = current + increment  \n        # TODO 4: Attempt CAS with current as expected, new_value as new\n        # TODO 5: Retry on failure with observed value from failed CAS\n        # TODO 6: Return original value when CAS succeeds\n        pass\n\nclass TaggedPointer(Generic[T]):\n    \"\"\"\n    Pointer combined with version tag to prevent ABA problem.\n    Essential for lock-free data structures that reuse node memory.\n    \"\"\"\n    \n    def __init__(self, pointer: Optional[T] = None, tag: int = 0):\n        self.pointer = pointer\n        self.tag = tag\n    \n    def __eq__(self, other) -> bool:\n        \"\"\"Tagged pointers are equal only if both pointer and tag match.\"\"\"\n        return (isinstance(other, TaggedPointer) and \n                self.pointer == other.pointer and \n                self.tag == other.tag)\n    \n    def next_version(self, new_pointer: Optional[T]) -> 'TaggedPointer[T]':\n        \"\"\"Create new tagged pointer with incremented tag.\"\"\"\n        return TaggedPointer(new_pointer, self.tag + 1)\n\ndef cas_retry_loop(atomic_ref: AtomicReference[T], \n                   update_function, \n                   max_attempts: int = 1000) -> T:\n    \"\"\"\n    Generic CAS retry loop with exponential backoff.\n    Used throughout lock-free data structures for safe updates.\n    \n    Args:\n        atomic_ref: AtomicReference to update\n        update_function: Function that takes current value, returns new value\n        max_attempts: Maximum retry attempts before giving up\n        \n    Returns:\n        Final value after successful update\n    \"\"\"\n    # TODO 1: Initialize backoff delay and attempt counter\n    # TODO 2: Load current value from atomic reference\n    # TODO 3: Compute new value using update_function(current)\n    # TODO 4: Attempt compare_and_swap with current as expected\n    # TODO 5: If CAS succeeds, return the new value\n    # TODO 6: If CAS fails, apply exponential backoff delay\n    # TODO 7: Use observed value from failed CAS as new current\n    # TODO 8: Increment attempt counter and check max_attempts limit\n    # TODO 9: Repeat from step 3 with new current value\n    pass\n```\n\n**ABA Problem Demonstration and Testing:**\n\n```python\n\"\"\"\nComprehensive test suite demonstrating the ABA problem and validating\nthat tagged pointer solutions correctly prevent ABA conditions.\n\"\"\"\n\nimport threading\nimport time\nfrom typing import Optional\nfrom atomic.primitives import AtomicReference, TaggedPointer\n\nclass Node:\n    \"\"\"Simple node for demonstrating ABA problem in linked structures.\"\"\"\n    \n    def __init__(self, data: int):\n        self.data = data\n        self.next: Optional['Node'] = None\n\ndef demonstrate_aba_problem():\n    \"\"\"\n    Concrete demonstration of ABA problem causing data corruption.\n    Shows how naive CAS can succeed incorrectly due to address reuse.\n    \"\"\"\n    # TODO 1: Create initial stack: A -> B -> C\n    # TODO 2: Thread 1 loads top pointer (gets A), prepares to pop\n    # TODO 3: Thread 2 pops A and B, then pushes A back  \n    # TODO 4: Thread 1's CAS succeeds but A.next now invalid\n    # TODO 5: Demonstrate resulting corruption in stack structure\n    pass\n\ndef stress_test_atomic_counter():\n    \"\"\"\n    Stress test for atomic counter under high contention.\n    Validates that no increments are lost even with many concurrent threads.\n    \"\"\"\n    # TODO 1: Create AtomicReference[int] initialized to 0\n    # TODO 2: Create N threads that each increment counter M times\n    # TODO 3: Start all threads simultaneously\n    # TODO 4: Wait for all threads to complete\n    # TODO 5: Verify final value equals N * M (no lost updates)\n    # TODO 6: Measure throughput (operations per second)\n    pass\n```\n\n**Milestone Checkpoint:**\n\nAfter implementing the atomic operations foundation, you should be able to:\n\n1. **Run the stress test**: `python -m pytest tests/stress_test_atomics.py -v`\n   - Expected: All atomic counter tests pass with exact expected totals\n   - Expected: ABA problem demonstration shows the corruption scenario\n   - Expected: Performance tests show atomic operations scale with thread count\n\n2. **Verify memory ordering**: Create threads that publish data with RELEASE and consume with ACQUIRE\n   - Expected: No torn reads or initialization races observed\n   - Expected: Proper happens-before relationships maintained\n\n3. **Test CAS retry patterns**: High-contention scenarios with exponential backoff\n   - Expected: Operations complete without infinite spinning\n   - Expected: Backoff reduces contention under high thread counts\n\n**Debugging Tips for Atomic Operations:**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|-------------|-----------------|-----|\n| Test failures under stress only | Race condition in CAS logic | Add logging to CAS retry loops | Check expected value handling |\n| Infinite loops in CAS | No backoff strategy | Monitor CPU usage and retry counts | Add exponential backoff |\n| Data corruption | ABA problem | Check if same addresses reused | Use tagged pointers |\n| Performance degradation | Too much contention | Profile retry rates and backoff | Tune backoff parameters |\n| Inconsistent test results | Memory ordering issues | Test on different architectures | Use stronger ordering |\n\nThe atomic operations foundation provides the essential building blocks that every subsequent milestone will depend on. Understanding these primitives deeply is crucial because errors at this level can cause subtle bugs that propagate through all higher-level data structures. Take time to thoroughly test and validate your atomic operations before proceeding to implement the lock-free data structures in the following milestones.\n\n\n## Lock-free Stack (Treiber Stack)\n\n> **Milestone(s):** Milestone 2 (Lock-free Stack) - This section implements the Treiber stack algorithm using atomic compare-and-swap operations, addresses the ABA problem through tagged pointers, and establishes linearizability guarantees for concurrent stack operations.\n\nThe lock-free stack represents the first substantial data structure built upon our atomic operations foundation. Unlike traditional mutex-protected stacks that serialize all operations through exclusive locking, the Treiber stack algorithm achieves thread-safety through careful use of atomic compare-and-swap operations on a single top-of-stack pointer. This approach eliminates the performance bottlenecks and deadlock risks inherent in lock-based designs while providing strong correctness guarantees about the linearizable behavior of concurrent push and pop operations.\n\n![Treiber Stack Push/Pop Flow](./diagrams/treiber-stack-operations.svg)\n\n### Mental Model: Stack as a Shared Notepad\n\nImagine a busy office where multiple people need to share a single notepad for writing quick notes. In a traditional lock-based approach, only one person could hold the notepad at any time - they would pick it up, write their note on top, and put it back down before anyone else could use it. This creates a bottleneck where everyone else must wait in line, even if they only need a few seconds to jot down a quick message.\n\nThe lock-free stack operates more like a magic notepad with special properties. Multiple people can simultaneously reach for the notepad, but the magic ensures that only one person's action succeeds at any given instant. When someone wants to add a note, they prepare their page separately, then attempt to place it on top of the stack. If someone else added a page in the meantime, they notice this immediately and retry with the new top page visible. When removing a note, they grab what appears to be the top page, but if someone else already took it or added something new, they automatically retry with the current state.\n\nThe critical insight is that everyone can work simultaneously without blocking each other, but the magic notepad ensures that each individual action (adding or removing a single page) appears to happen atomically and in a well-defined order. No pages are lost, no one gets confused about what's on top, and the system keeps working even if someone gets distracted halfway through their action. This elimination of waiting and coordination overhead is what makes lock-free data structures so powerful for high-performance concurrent systems.\n\n### Treiber Stack Algorithm\n\nThe Treiber stack algorithm achieves lock-free operation through a deceptively simple approach: maintain a single atomic pointer to the top of the stack and use compare-and-swap operations to atomically update this pointer when pushing or popping nodes. The elegance lies in how this single atomic variable coordinates all concurrent operations without requiring any locks or blocking synchronization.\n\n**Stack Structure and Node Design**\n\nThe core stack structure contains only one field: an atomic pointer to the topmost node. Each node in the stack contains the data payload and an atomic pointer to the next node further down in the stack. This linked-list structure allows unbounded growth and provides the foundation for atomic pointer manipulation.\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `top` | `AtomicReference<Node>` | Atomic pointer to the topmost stack node, null when stack is empty |\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `data` | Generic type T | The actual data payload stored in this stack node |\n| `next` | `AtomicReference<Node>` | Atomic pointer to the next node down in the stack, null for bottom node |\n\n**Push Operation Algorithm**\n\nThe push operation must atomically prepend a new node to the front of the linked list by updating the top pointer. This requires a careful sequence of steps to handle concurrent modifications from other threads.\n\n1. **Allocate and initialize the new node** with the data payload and a null next pointer\n2. **Load the current top pointer** using an atomic load operation to get a consistent snapshot\n3. **Set the new node's next pointer** to point to the current top node, linking it into the chain\n4. **Attempt atomic compare-and-swap** on the top pointer, changing from current top to the new node\n5. **Check CAS result** - if successful, the push is complete and the new node is now the stack top\n6. **Handle CAS failure** by returning to step 2 and retrying with the updated top pointer value\n7. **Apply exponential backoff** if experiencing high contention to reduce CPU usage and improve success rates\n\nThe critical insight is that steps 2-4 must be atomic as a group, which is exactly what compare-and-swap provides. The CAS ensures that the top pointer is updated only if it still contains the same value that was read in step 2, guaranteeing that no other thread modified the stack between the read and the update.\n\n**Pop Operation Algorithm**\n\nThe pop operation must atomically remove the topmost node and return its data while updating the top pointer to the next node in the chain. This operation faces additional complexity because it must handle the empty stack case and ensure that the removed node's next pointer is read before any concurrent modifications occur.\n\n1. **Load the current top pointer** using an atomic load to get the current stack state\n2. **Check for empty stack** - if top is null, return a \"not found\" indicator immediately\n3. **Read the next pointer** from the current top node to identify the new stack top\n4. **Attempt atomic compare-and-swap** on the top pointer, changing from current top to next node\n5. **Check CAS result** - if successful, the old top node has been atomically removed from the stack\n6. **Extract and return the data** from the removed node, which is now safe to access\n7. **Handle CAS failure** by returning to step 1 and retrying with the updated stack state\n8. **Apply exponential backoff** if experiencing contention to improve overall system performance\n\nThe most subtle aspect of the pop operation is step 3, where the next pointer must be read before the CAS attempt. This creates a potential race condition where another thread could modify or deallocate the node between reading its next pointer and successfully completing the CAS. This is where hazard pointers become essential for safe memory reclamation.\n\n> **Design Insight**: The Treiber stack's power comes from reducing all stack operations to single-word atomic operations on the top pointer. This eliminates the need for complex multi-step coordination while providing strong consistency guarantees through the linearizability of compare-and-swap.\n\n**CAS Retry Loop Implementation Pattern**\n\nBoth push and pop operations follow a common pattern called the CAS retry loop, which handles the fundamental challenge that multiple threads may attempt to modify the same atomic variable simultaneously. Only one thread's CAS can succeed, while others must retry with updated information.\n\n| Loop State | Description | Action Taken | Next Transition |\n|------------|-------------|--------------|-----------------|\n| `LOAD_CURRENT` | Reading the current atomic value | Execute atomic load with appropriate memory ordering | Move to `PREPARE_UPDATE` |\n| `PREPARE_UPDATE` | Computing the new value based on current state | Perform local computation and prepare new node linkages | Move to `ATTEMPT_CAS` |\n| `ATTEMPT_CAS` | Executing the compare-and-swap operation | Call CAS with expected (loaded) and desired (computed) values | Success: `COMPLETE`, Failure: `BACKOFF` |\n| `BACKOFF` | Applying delay before retry to reduce contention | Execute exponential backoff delay increasing with attempt count | Return to `LOAD_CURRENT` |\n| `COMPLETE` | Operation completed successfully | Return result to caller and exit retry loop | Terminal state |\n\n> **Decision: Exponential Backoff in CAS Retry Loops**\n> - **Context**: High contention scenarios can cause CAS retry loops to consume excessive CPU cycles through constant spinning, degrading overall system performance.\n> - **Options Considered**: \n>   1. Immediate retry with no delay\n>   2. Fixed delay between retry attempts  \n>   3. Exponential backoff with randomization\n> - **Decision**: Implement exponential backoff starting at 1 microsecond, doubling each retry up to 1 millisecond maximum, with random jitter.\n> - **Rationale**: Exponential backoff reduces CPU usage during contention while maintaining low latency for lightly contested operations. Random jitter prevents thundering herd effects where all threads retry simultaneously.\n> - **Consequences**: Improves overall throughput under high contention at the cost of slightly higher latency for individual operations in contested scenarios.\n\n### Linearization Points and Correctness\n\nLinearizability is the gold standard correctness condition for concurrent data structures. It requires that each operation appear to execute atomically at some point during its actual execution time, and that all operations can be ordered in a way that respects both the sequential semantics of the data structure and the real-time ordering of non-overlapping operations. For the Treiber stack, identifying these linearization points is crucial for reasoning about correctness.\n\n**Linearization Points for Push Operations**\n\nThe linearization point of a successful push operation occurs at the exact moment when the compare-and-swap operation on the top pointer succeeds. At this instant, the new node becomes atomically visible to all other threads, and the stack transitions from its previous state to the new state with the pushed element on top.\n\nFor failed CAS attempts within the same push operation, no linearization occurs - these are considered internal implementation details that don't affect the external behavior of the stack. The operation only becomes linearized when it finally succeeds, which may require multiple CAS attempts due to interference from concurrent operations.\n\n**Linearization Points for Pop Operations**\n\nSimilarly, the linearization point for a successful pop operation occurs when the compare-and-swap on the top pointer succeeds, atomically removing the top node and making the next node visible as the new stack top. The critical property is that the data value returned by the pop operation must be from the node that was atomically removed at this linearization point.\n\nFor pop operations that find an empty stack, the linearization point occurs at the moment when the atomic load observes a null top pointer. This observation point establishes that the stack was empty at that instant, justifying the \"not found\" return value.\n\n**Sequential Consistency and Ordering Properties**\n\nThe Treiber stack provides sequential consistency, meaning that all operations appear to execute in some total order that is consistent with the program order of each individual thread. This is a stronger guarantee than many lock-free data structures provide, and it comes from the sequential consistency of the underlying compare-and-swap operations on most modern architectures.\n\n| Operation Type | Linearization Point | Observable Effect | Ordering Guarantee |\n|----------------|-------------------|-------------------|-------------------|\n| `push(data)` | Successful CAS on top pointer | New node becomes stack top | Happens-before all subsequent operations that observe the new top |\n| `pop()` returning data | Successful CAS on top pointer | Node removed from stack top | Happens-before all subsequent operations that observe the updated top |\n| `pop()` returning empty | Load observing null top | Confirmation of empty stack state | Happens-before relationship established with last successful push |\n\n> **Critical Correctness Property**: The linearizability of the Treiber stack ensures that concurrent operations appear to execute in some sequential order, even though they may actually execute in an interleaved fashion. This allows developers to reason about the stack using familiar sequential semantics while gaining the performance benefits of lock-free concurrency.\n\n**Proving Stack Invariants**\n\nSeveral key invariants must be maintained to ensure the correctness of the Treiber stack implementation:\n\n1. **Single-ownership invariant**: Each node appears in at most one stack at any given time, preventing corruption from shared ownership\n2. **Reachability invariant**: All nodes reachable from the top pointer form a valid linked list with proper next-pointer chains\n3. **Atomicity invariant**: The top pointer always refers to a valid node or is null, never pointing to partially-constructed or deallocated memory\n4. **LIFO ordering invariant**: Elements are removed in the reverse order of their insertion, maintaining stack semantics across all interleavings\n\nThese invariants are preserved by the atomic nature of the compare-and-swap operations and the careful ordering of pointer updates within each push and pop operation.\n\n### ABA Problem in Stack Context\n\nThe ABA problem represents one of the most subtle and dangerous pitfalls in lock-free programming. It occurs when a compare-and-swap operation incorrectly succeeds because a memory location has changed from value A to value B and back to value A between the time a thread reads the location and attempts to update it. In the context of the Treiber stack, this can lead to severe corruption of the stack structure and loss of data.\n\n**ABA Scenario in Stack Operations**\n\nConsider a stack initially containing nodes A and B, with A on top. Thread 1 begins a pop operation by reading the top pointer (value A) and then reading A's next pointer (value B) to prepare for the CAS that will make B the new top. However, before Thread 1 can complete its CAS, Thread 2 executes two complete operations: it pops A from the stack, and then pushes A back onto the stack.\n\nFrom Thread 1's perspective, the top pointer still contains value A when it performs the CAS, so the operation succeeds. However, the internal structure of the stack may have changed dramatically. If Thread 2 modified A's next pointer or if other nodes were pushed and popped in the meantime, Thread 1's CAS will corrupt the stack by incorrectly setting the top pointer to what it believes is A's next pointer.\n\n| Timeline Step | Thread 1 Action | Thread 2 Action | Stack State | Problem |\n|---------------|------------------|-----------------|-------------|---------|\n| 1 | Load top → A, Load A.next → B | (waiting) | [A, B, ...] | None yet |\n| 2 | (preparing CAS) | Pop A from stack | [B, ...] | A is no longer in stack |\n| 3 | (preparing CAS) | Push A back to stack | [A, B, ...] | A.next may have changed |\n| 4 | CAS(A, B) succeeds! | (done) | [B, ...] but A.next corrupted | Stack structure damaged |\n\nThe fundamental issue is that pointer equality doesn't guarantee structural equality. Even though the top pointer contains the same address A, the node at that address may have different contents or may have been used in a completely different context.\n\n**Memory Reuse Amplifies ABA Impact**\n\nThe ABA problem becomes even more severe when combined with memory reuse from allocators or memory pools. If Thread 2 pops node A and the memory allocator immediately reuses that memory for a completely different node (perhaps containing different data), Thread 1's CAS will succeed but will be operating on a node with entirely different semantics.\n\nThis memory reuse scenario can cause:\n- **Data corruption**: Thread 1 updates the wrong node's next pointer\n- **Memory leaks**: Nodes become unreachable due to broken pointer chains  \n- **Use-after-free errors**: Accessing deallocated memory through stale pointers\n- **Infinite loops**: Circular references created by incorrect pointer updates\n\n**Tagged Pointer Solution**\n\nThe standard solution to the ABA problem is to augment each pointer with a monotonically increasing version tag. Instead of storing just a pointer to a node, we store a tagged pointer containing both the node address and a version number that increments with each update operation.\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `pointer` | `Node*` | Memory address of the actual node object |\n| `tag` | `uint64_t` | Monotonically increasing version counter |\n\nThe compare-and-swap operation now operates on the entire tagged pointer structure, comparing both the address and the tag. This ensures that even if the same node address is reused, the version tag will be different, causing the CAS to fail and forcing the thread to retry with current information.\n\n**Tagged Pointer Implementation Considerations**\n\nMost modern 64-bit architectures provide sufficient address space to reserve some bits for the version tag without affecting the valid pointer range. A common approach is to use the upper 16 bits for the tag and the lower 48 bits for the address, providing 65,536 versions before wraparound while supporting the full virtual address space.\n\n| Bit Range | Purpose | Description |\n|-----------|---------|-------------|\n| 63-48 | Version Tag | 16-bit counter incremented on each CAS |\n| 47-0 | Node Pointer | 48-bit address supporting full virtual memory space |\n\nThe atomic compare-and-swap operates on the full 64-bit tagged pointer value, ensuring that both components must match for the operation to succeed. This provides ABA protection while maintaining the single-word CAS requirement for optimal performance.\n\n> **Decision: 16-bit Version Tags vs 32-bit Tags**\n> - **Context**: Need to choose the split between pointer bits and version bits for ABA protection while maintaining single-word CAS performance.\n> - **Options Considered**:\n>   1. 8-bit tag (256 versions) with 56-bit pointers\n>   2. 16-bit tag (65,536 versions) with 48-bit pointers\n>   3. 32-bit tag (4 billion versions) with 32-bit pointers\n> - **Decision**: Use 16-bit tags with 48-bit pointers for 64-bit architectures\n> - **Rationale**: 48 bits provide sufficient virtual address space for all practical applications (256TB), while 16-bit tags offer strong ABA protection with wraparound only after 65,536 operations on the same memory location.\n> - **Consequences**: Enables single-word CAS performance while providing robust ABA protection. Requires careful pointer packing/unpacking but maintains full memory addressing capabilities.\n\n### Common Stack Implementation Pitfalls\n\nLock-free stack implementation contains several subtle pitfalls that can lead to difficult-to-debug issues in concurrent systems. Understanding these common mistakes and their solutions is crucial for building robust lock-free data structures.\n\n⚠️ **Pitfall: Premature Node Reclamation**\n\nThe most dangerous mistake in lock-free stack implementation is deallocating nodes too early, before ensuring that no other threads are accessing them. Consider a pop operation that successfully removes a node from the stack - the node is no longer reachable through the top pointer, but other threads may still hold references to it from earlier reads.\n\n**Why it's wrong**: Other threads may have loaded a pointer to the node before it was removed from the stack. If these threads proceed with their operations after the node is deallocated, they will access freed memory, causing undefined behavior, crashes, or data corruption.\n\n**How to avoid**: Implement hazard pointers or epoch-based reclamation to defer deallocation until all threads have finished accessing the node. Never call free() or delete immediately after removing a node from the stack structure.\n\n⚠️ **Pitfall: Incorrect Empty Stack Handling in Pop**\n\nA common implementation error is failing to properly handle the case where the stack becomes empty between loading the top pointer and attempting the compare-and-swap operation. This can lead to null pointer dereferences or incorrect CAS operations.\n\n**Why it's wrong**: If the top pointer is loaded as non-null but becomes null before the CAS (due to another thread popping the last element), attempting to read the next pointer will dereference a null pointer. Alternatively, performing CAS with stale information can lead to inconsistent stack state.\n\n**How to fix**: Always check for null after loading the top pointer and before accessing the node's fields. Structure the retry loop to handle the transition to empty stack gracefully:\n\n1. Load top pointer atomically\n2. If null, return \"empty\" immediately  \n3. If non-null, read next pointer from the node\n4. Attempt CAS to change top from current to next\n5. If CAS fails, retry from step 1\n\n⚠️ **Pitfall: Missing Memory Ordering Constraints**\n\nUsing relaxed memory ordering for all atomic operations can lead to subtle bugs where operations appear to execute out of order due to CPU reordering optimizations. This can cause nodes to appear corrupted or temporarily inconsistent to other threads.\n\n**Why it's wrong**: Without proper memory ordering, a thread might observe a new node in the stack before observing the initialization of that node's data fields. This can lead to reading uninitialized data or seeing partially-constructed nodes.\n\n**How to fix**: Use appropriate memory ordering for each operation:\n- Push operations should use `RELEASE` semantics on the final CAS to ensure node initialization is visible before the node becomes reachable\n- Pop operations should use `ACQUIRE` semantics when loading the top pointer to ensure they see all writes that happened before the node was added to the stack\n- Use `SEQ_CST` ordering for simplicity when performance is not critical\n\n⚠️ **Pitfall: Infinite Spinning Without Backoff**\n\nImplementing CAS retry loops without any backoff strategy can lead to excessive CPU consumption and poor performance under contention. Multiple threads spinning at full speed can actually increase contention and reduce overall throughput.\n\n**Why it's wrong**: Constant spinning consumes CPU cycles without making progress, and can cause cache line bouncing between cores as multiple threads repeatedly access the same atomic variable. This creates a positive feedback loop where contention increases CPU usage, which increases contention.\n\n**How to fix**: Implement exponential backoff with the following strategy:\n1. Start with no delay for the first retry (optimistic case)\n2. Double the delay after each failed attempt, starting from 1 microsecond\n3. Cap the maximum delay at 1 millisecond to maintain responsiveness\n4. Add random jitter (±25%) to prevent synchronized retries\n5. Reset delay to zero after any successful operation\n\n⚠️ **Pitfall: Mixing Atomic and Non-Atomic Access**\n\nAccessing stack nodes or the top pointer through non-atomic operations while other threads are performing atomic operations creates data races and undefined behavior. This often occurs in debugging code or when implementing additional operations like size() or iteration.\n\n**Why it's wrong**: The C++ and other language memory models specify that mixing atomic and non-atomic access to the same memory location is undefined behavior. Even seemingly harmless reads can interfere with atomic operations or observe inconsistent state.\n\n**How to fix**: Either make all access atomic with appropriate memory ordering, or carefully design protocols where non-atomic access occurs only when atomics are guaranteed not to be used (such as during single-threaded initialization or after synchronization barriers).\n\n| Pitfall Category | Detection Method | Prevention Strategy |\n|------------------|------------------|-------------------|\n| Premature reclamation | AddressSanitizer, Valgrind | Implement hazard pointers before testing |\n| Empty stack races | Stress testing with frequent empty/non-empty transitions | Add explicit null checks in all pointer operations |\n| Memory ordering bugs | ThreadSanitizer, careful code review | Use stronger ordering (ACQUIRE/RELEASE) by default |\n| Infinite spinning | CPU profiling, performance monitoring | Implement exponential backoff from the start |\n| Mixed atomic access | ThreadSanitizer, static analysis | Establish clear atomic/non-atomic boundaries |\n\n### Implementation Guidance\n\nThe lock-free stack implementation requires careful attention to atomic operations, memory ordering, and the integration of hazard pointers for safe memory reclamation. This section provides concrete guidance for building a production-ready Treiber stack.\n\n**A. Technology Recommendations**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Atomic Operations | `threading.Lock` with CAS simulation | `ctypes` with platform-specific atomic libraries |\n| Memory Ordering | Sequential consistency only | Full acquire/release/relaxed semantics |\n| Testing Framework | `unittest` with basic threading | `pytest` with `concurrent.futures` for stress testing |\n| Performance Monitoring | Basic timing with `time.perf_counter()` | `cProfile` with custom metrics collection |\n| Memory Safety | Reference counting with `weakref` | Custom hazard pointer implementation |\n\n**B. Recommended Module Structure**\n\n```\nlock_free/\n├── __init__.py                    ← package initialization\n├── atomic/\n│   ├── __init__.py               ← atomic operations module\n│   ├── primitives.py             ← AtomicReference, compare_and_swap, memory ordering\n│   └── tagged_pointer.py         ← TaggedPointer implementation for ABA prevention\n├── stack/\n│   ├── __init__.py               ← stack module exports\n│   ├── treiber_stack.py          ← main TreiberStack implementation (YOUR FOCUS)\n│   ├── node.py                   ← Node structure definition\n│   └── stack_test.py             ← comprehensive testing including linearizability\n├── memory/\n│   ├── __init__.py               ← memory management exports  \n│   ├── hazard_pointers.py        ← safe memory reclamation (next milestone)\n│   └── retirement_list.py        ← deferred deletion support\n└── testing/\n    ├── __init__.py               ← testing utilities\n    ├── linearizability.py        ← LinearizabilityChecker implementation\n    └── stress_test.py             ← concurrent correctness validation\n```\n\n**C. Infrastructure Starter Code (Complete Implementation)**\n\n**File: `atomic/primitives.py`**\n```python\nimport threading\nfrom enum import Enum\nfrom typing import Any, Tuple, TypeVar, Generic, Optional\nimport time\n\nT = TypeVar('T')\n\nclass MemoryOrdering(Enum):\n    \"\"\"Memory ordering constraints for atomic operations.\"\"\"\n    RELAXED = \"relaxed\"      # No ordering constraints\n    ACQUIRE = \"acquire\"      # Acquire semantics for loads\n    RELEASE = \"release\"      # Release semantics for stores  \n    SEQ_CST = \"seq_cst\"     # Sequential consistency\n\nclass AtomicReference(Generic[T]):\n    \"\"\"Thread-safe atomic reference with compare-and-swap support.\"\"\"\n    \n    def __init__(self, initial_value: Optional[T] = None):\n        self._value = initial_value\n        self._lock = threading.Lock()  # Simulates atomic hardware operations\n        \n    def load(self, ordering: MemoryOrdering = MemoryOrdering.SEQ_CST) -> Optional[T]:\n        \"\"\"Atomically load the current value.\"\"\"\n        with self._lock:\n            return self._value\n            \n    def store(self, value: Optional[T], ordering: MemoryOrdering = MemoryOrdering.SEQ_CST) -> None:\n        \"\"\"Atomically store a new value.\"\"\"\n        with self._lock:\n            self._value = value\n            \n    def compare_and_swap(self, expected: Optional[T], new_value: Optional[T]) -> Tuple[bool, Optional[T]]:\n        \"\"\"\n        Atomically compare current value with expected and swap if equal.\n        Returns: (success: bool, observed_value: T)\n        \"\"\"\n        with self._lock:\n            current = self._value\n            if current is expected:  # Using 'is' for object identity comparison\n                self._value = new_value\n                return True, current\n            else:\n                return False, current\n\ndef cas_retry_loop(atomic_ref: AtomicReference[T], update_function, max_attempts: int = 1000) -> bool:\n    \"\"\"Generic CAS retry loop with exponential backoff.\"\"\"\n    attempt = 0\n    delay = 0.000001  # Start with 1 microsecond\n    \n    while attempt < max_attempts:\n        current = atomic_ref.load()\n        new_value = update_function(current)\n        \n        success, observed = atomic_ref.compare_and_swap(current, new_value)\n        if success:\n            return True\n            \n        # Exponential backoff with jitter\n        if delay > 0:\n            jitter = delay * 0.25 * (2 * time.time() % 1 - 0.5)  # ±25% random\n            time.sleep(delay + jitter)\n            \n        delay = min(delay * 2, 0.001)  # Cap at 1 millisecond\n        attempt += 1\n        \n    return False  # Failed after max attempts\n```\n\n**File: `atomic/tagged_pointer.py`**\n```python\nfrom typing import TypeVar, Optional, NamedTuple\nfrom .primitives import AtomicReference, MemoryOrdering\n\nT = TypeVar('T')\n\nclass TaggedPointer(NamedTuple):\n    \"\"\"Pointer with version tag to prevent ABA problems.\"\"\"\n    pointer: Optional[T]\n    tag: int\n    \n    @classmethod\n    def create(cls, pointer: Optional[T], tag: int = 0) -> 'TaggedPointer[T]':\n        \"\"\"Create a new tagged pointer with specified tag.\"\"\"\n        return cls(pointer=pointer, tag=tag)\n        \n    def increment_tag(self) -> 'TaggedPointer[T]':\n        \"\"\"Create new tagged pointer with incremented tag.\"\"\"\n        return TaggedPointer(pointer=self.pointer, tag=(self.tag + 1) & 0xFFFF)\n        \n    def with_pointer(self, new_pointer: Optional[T]) -> 'TaggedPointer[T]':\n        \"\"\"Create new tagged pointer with different pointer, incremented tag.\"\"\"\n        return TaggedPointer(pointer=new_pointer, tag=(self.tag + 1) & 0xFFFF)\n\nclass AtomicTaggedPointer(AtomicReference[TaggedPointer[T]]):\n    \"\"\"Atomic reference to tagged pointer for ABA-safe operations.\"\"\"\n    \n    def __init__(self, initial_pointer: Optional[T] = None):\n        initial_tagged = TaggedPointer.create(initial_pointer, tag=0)\n        super().__init__(initial_tagged)\n        \n    def load_pointer(self, ordering: MemoryOrdering = MemoryOrdering.SEQ_CST) -> Optional[T]:\n        \"\"\"Load just the pointer component.\"\"\"\n        tagged = self.load(ordering)\n        return tagged.pointer if tagged else None\n        \n    def compare_and_swap_pointer(self, expected_pointer: Optional[T], \n                                new_pointer: Optional[T]) -> tuple[bool, Optional[T]]:\n        \"\"\"CAS on pointer with automatic tag increment.\"\"\"\n        current_tagged = self.load()\n        if current_tagged.pointer is not expected_pointer:\n            return False, current_tagged.pointer\n            \n        new_tagged = current_tagged.with_pointer(new_pointer)\n        success, observed_tagged = self.compare_and_swap(current_tagged, new_tagged)\n        return success, observed_tagged.pointer\n```\n\n**D. Core Logic Skeleton Code (For Student Implementation)**\n\n**File: `stack/node.py`**\n```python\nfrom typing import TypeVar, Optional, Any\nfrom ..atomic.primitives import AtomicReference\n\nT = TypeVar('T')\n\nclass Node:\n    \"\"\"Stack node containing data and atomic next pointer.\"\"\"\n    \n    def __init__(self, data: T):\n        # TODO 1: Store the data payload in self.data\n        # TODO 2: Initialize self.next as AtomicReference[Node] starting with None\n        # Hint: Use AtomicReference(None) for the next pointer\n        pass\n```\n\n**File: `stack/treiber_stack.py`**\n```python\nfrom typing import TypeVar, Optional\nfrom ..atomic.primitives import AtomicReference, cas_retry_loop\nfrom ..atomic.tagged_pointer import AtomicTaggedPointer\nfrom .node import Node\n\nT = TypeVar('T')\n\nclass TreiberStack:\n    \"\"\"Lock-free stack implementation using Treiber algorithm.\"\"\"\n    \n    def __init__(self):\n        # TODO 1: Initialize self.top as AtomicTaggedPointer[Node] starting with None\n        # Hint: Use AtomicTaggedPointer(None) to create empty stack\n        pass\n    \n    def push(self, data: T) -> None:\n        \"\"\"\n        Push data onto the stack using lock-free CAS operation.\n        This operation is lock-free and linearizable.\n        \"\"\"\n        # TODO 1: Create new node with the data\n        # TODO 2: Start CAS retry loop - load current top pointer  \n        # TODO 3: Set new node's next to point to current top\n        # TODO 4: Attempt CAS to make new node the top\n        # TODO 5: If CAS succeeds, return. If fails, retry from step 2\n        # TODO 6: Add exponential backoff for failed CAS attempts\n        # Hint: Use self.top.compare_and_swap_pointer(expected, new_node)\n        pass\n        \n    def pop(self) -> Optional[T]:\n        \"\"\"\n        Pop data from the stack using lock-free CAS operation.\n        Returns None if stack is empty.\n        This operation is lock-free and linearizable.\n        \"\"\"\n        # TODO 1: Start CAS retry loop - load current top pointer\n        # TODO 2: Check if stack is empty (top is None) and return None\n        # TODO 3: Load the next pointer from current top node  \n        # TODO 4: Attempt CAS to make next node the new top\n        # TODO 5: If CAS succeeds, return the data from old top node\n        # TODO 6: If CAS fails, retry from step 1\n        # TODO 7: Add exponential backoff for failed CAS attempts\n        # Hint: Read top.next before CAS, then CAS from top to top.next\n        pass\n    \n    def is_empty(self) -> bool:\n        \"\"\"Check if stack is empty. This is a snapshot operation.\"\"\"\n        # TODO 1: Load current top pointer atomically\n        # TODO 2: Return True if top is None, False otherwise  \n        # Hint: Use self.top.load_pointer() to get just the pointer\n        pass\n    \n    def size(self) -> int:\n        \"\"\"\n        Count nodes in stack. WARNING: This is not atomic with other operations!\n        Result may be inconsistent in concurrent environment.\n        \"\"\"\n        # TODO 1: Load current top pointer\n        # TODO 2: Traverse the linked list counting nodes\n        # TODO 3: Return total count\n        # Hint: Follow next pointers but don't use this for synchronization\n        pass\n```\n\n**E. Language-Specific Hints**\n\n**Python Atomic Operations**: Since Python doesn't have built-in atomic operations, we simulate them using `threading.Lock`. In production, consider using `ctypes` to call platform-specific atomic libraries or libraries like `python-atomics`.\n\n**Memory Management**: Python's garbage collector handles basic memory management, but for true lock-free behavior, you'll need to implement hazard pointers to prevent premature collection of nodes still being accessed by other threads.\n\n**Testing with Threading**: Use `threading.Thread` for basic testing, but for comprehensive stress testing, consider `concurrent.futures.ThreadPoolExecutor` to easily manage multiple worker threads.\n\n**Performance Measurement**: Use `time.perf_counter()` for high-resolution timing measurements. For production workloads, implement custom metrics to track CAS success rates and retry counts.\n\n**F. Milestone Checkpoint**\n\nAfter implementing the Treiber stack, verify correct behavior with these steps:\n\n**Basic Functionality Test**:\n```bash\npython -m pytest lock_free/stack/stack_test.py::test_basic_push_pop -v\n```\nExpected: All pushes and pops work correctly on single thread, LIFO ordering preserved.\n\n**Concurrent Correctness Test**:  \n```bash\npython -m pytest lock_free/stack/stack_test.py::test_concurrent_operations -v\n```\nExpected: Multiple threads pushing and popping concurrently produce no lost elements, no duplicates, maintain LIFO ordering globally.\n\n**ABA Problem Demonstration**:\n```bash\npython -m pytest lock_free/stack/stack_test.py::test_aba_problem -v  \n```\nExpected: Tagged pointer implementation prevents ABA corruption, naive implementation shows the problem.\n\n**Performance Comparison**:\n```bash\npython lock_free/stack/benchmark.py\n```\nExpected output:\n```\nLock-free stack: 1,250,000 ops/sec (4 threads)\nMutex-based stack: 340,000 ops/sec (4 threads)\nSpeedup: 3.68x under contention\n```\n\n**Signs of Problems**:\n- **Lost elements**: Check CAS retry logic, ensure no race conditions in node linking\n- **Duplicate elements**: Verify that successful CAS doesn't allow double-returns  \n- **Crashes**: Implement hazard pointers, check for null pointer dereferences\n- **Poor performance**: Add exponential backoff, check for infinite spinning\n\n**G. Debugging Tips**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Elements disappear | CAS retry bug or premature deallocation | Add logging to track each CAS attempt | Fix retry loop logic, add hazard pointers |\n| Duplicate elements returned | Multiple threads getting same node | Log successful CAS operations | Ensure CAS success prevents multiple returns |\n| Infinite hanging | Livelock in CAS retry | Monitor CAS success/failure rates | Implement exponential backoff with jitter |  \n| Crashes on pop | Null pointer dereference | Check stack traces for node access | Add null checks before dereferencing nodes |\n| Poor performance | Excessive contention | Profile CAS retry counts | Tune backoff parameters, check for false sharing |\n\n\n## Lock-free Queue (Michael-Scott Algorithm)\n\n> **Milestone(s):** Milestone 3 (Lock-free Queue) - This section implements the Michael-Scott FIFO queue algorithm using dual head/tail pointers, sentinel nodes, and helping mechanisms to achieve lock-free concurrent operations with linearizability guarantees.\n\nThe transition from stack to queue represents a significant leap in lock-free algorithm complexity. While the Treiber stack operates on a single point of contention (the top pointer), a queue must coordinate operations at two distinct ends: producers adding elements at the tail and consumers removing elements at the head. This dual-pointer design introduces new challenges around consistency, progress guarantees, and the prevention of interference between concurrent enqueue and dequeue operations.\n\nThe **Michael-Scott queue algorithm** stands as one of the most elegant solutions to lock-free FIFO ordering, published by Maged Michael and Michael Scott in 1996. This algorithm demonstrates several advanced lock-free programming techniques: the use of a dummy sentinel node to simplify empty queue handling, a helping mechanism where threads assist each other to ensure progress, and careful management of two atomic pointers that must remain consistent despite concurrent updates from multiple threads.\n\n### Mental Model: Queue as a Conveyor Belt\n\nBefore diving into the technical complexities of the Michael-Scott algorithm, it's helpful to visualize the queue using a factory conveyor belt analogy. Imagine a manufacturing assembly line with workers at both ends: **packers** at one end who place items onto the conveyor belt, and **shippers** at the other end who remove completed items for delivery.\n\nIn this mental model, the conveyor belt itself represents the queue's linked list of nodes. The **tail pointer** is like a supervisor standing near the packing station, always pointing to the last position where an item was placed (or where the next item should go). The **head pointer** is another supervisor at the shipping station, pointing to the position where the next item should be removed.\n\nThe key insight from this analogy is that both supervisors (pointers) need to coordinate without directly communicating. When a packer places a new item, they must update the tail supervisor's position. When a shipper removes an item, they must advance the head supervisor. But here's the crucial challenge: what happens when multiple packers try to place items simultaneously, or when the conveyor belt is empty and both supervisors are pointing to the same empty position?\n\nThis is where the **dummy sentinel node** comes into play. Think of it as a permanent \"placeholder\" item that never gets shipped - it serves as a reference point that ensures the head and tail supervisors never lose track of the belt structure. The sentinel node means the belt is never truly empty; there's always at least one reference point.\n\nThe **helping mechanism** is like workers occasionally glancing over to help their colleagues. If a packer notices that the tail supervisor hasn't moved to point to the latest item (perhaps they were distracted), the packer will helpfully update the supervisor's position before adding their own item. This mutual assistance ensures that work never gets permanently stuck, even when individual workers are temporarily delayed.\n\n![Michael-Scott Queue Operation Sequence](./diagrams/michael-scott-sequence.svg)\n\nThis conveyor belt model captures the essence of why lock-free queues are more complex than stacks: we're coordinating activity at two distinct locations while maintaining the invariant that items move in FIFO order from the tail end to the head end.\n\n### Michael-Scott Queue Algorithm\n\nThe Michael-Scott algorithm elegantly solves the dual-pointer coordination challenge through a carefully designed protocol that allows enqueue and dequeue operations to proceed concurrently with minimal interference. The algorithm's brilliance lies in its two-phase approach to pointer updates and the helping mechanism that prevents operations from blocking indefinitely.\n\n> **Decision: Two-Pointer Design with Separate Endpoints**\n> - **Context**: A queue requires FIFO ordering with operations at both ends - additions at tail and removals at head. We need to choose how to coordinate these two access points.\n> - **Options Considered**: \n>   1. Single pointer with full traversal for enqueue operations\n>   2. Two pointers (head and tail) with independent CAS operations\n>   3. Single pointer with reverse traversal links\n> - **Decision**: Use separate atomic head and tail pointers with independent CAS operations\n> - **Rationale**: This design maximizes concurrency by allowing enqueue and dequeue to proceed independently most of the time, avoiding the O(n) traversal cost of single-pointer approaches while maintaining cache locality for common operations\n> - **Consequences**: Increased complexity in maintaining consistency between two pointers, but significant performance gains under contention and better scalability across multiple producer/consumer threads\n\nThe core data structures that enable this algorithm are carefully designed to support atomic updates while maintaining referential integrity:\n\n| Structure | Field | Type | Description |\n|-----------|-------|------|-------------|\n| `Node` | `data` | `Any` | The actual payload stored in this queue element |\n| `Node` | `next` | `AtomicReference<Node>` | Atomic pointer to the next node in the queue |\n| `MichaelScottQueue` | `head` | `AtomicReference<Node>` | Points to the sentinel node or first dequeueable node |\n| `MichaelScottQueue` | `tail` | `AtomicReference<Node>` | Points to the last node or one node behind the actual tail |\n\nThe queue operations follow a precise protocol that ensures linearizability while allowing maximum concurrency. Let's examine each operation in detail:\n\n#### Enqueue Operation Protocol\n\nThe enqueue operation follows a two-step protocol that first links the new node into the list structure, then advances the tail pointer. This ordering is crucial for maintaining consistency and enabling the helping mechanism:\n\n1. **Allocate and initialize a new node** with the provided data and a null `next` pointer\n2. **Load the current tail pointer** and the node it points to (call this `tail_node`)\n3. **Read the `next` pointer of `tail_node`** to determine if the tail is pointing to the actual end\n4. **Check consistency**: re-read the tail pointer to ensure it hasn't changed since step 2\n5. **If `tail_node.next` is null** (tail points to actual end):\n   - Attempt `compare_and_swap(tail_node.next, null, new_node)`\n   - If successful, break to step 7\n   - If failed, retry from step 2 (another thread added a node)\n6. **If `tail_node.next` is not null** (tail is lagging):\n   - Help advance the tail: `compare_and_swap(queue.tail, tail_node, tail_node.next)`\n   - Retry from step 2 whether the helping CAS succeeded or failed\n7. **Advance the tail pointer**: `compare_and_swap(queue.tail, tail_node, new_node)`\n   - This step may fail if another thread already advanced the tail, which is acceptable\n\n#### Dequeue Operation Protocol\n\nThe dequeue operation is somewhat simpler since it only needs to advance the head pointer, but it must handle the empty queue case and coordinate with the sentinel node:\n\n1. **Load the current head and tail pointers** and the nodes they point to\n2. **Check consistency**: re-read the head pointer to ensure it hasn't changed\n3. **If head equals tail**:\n   - Check if `head_node.next` is null (truly empty queue)\n   - If null, return \"queue empty\" indicator\n   - If not null, help advance tail: `compare_and_swap(queue.tail, tail_node, head_node.next)`\n   - Retry from step 1\n4. **If head does not equal tail**:\n   - Read `next_node = head_node.next` (this will be the new head)\n   - If `next_node` is null, retry from step 1 (inconsistent state)\n   - Read the data from `next_node` before proceeding\n5. **Advance the head pointer**: `compare_and_swap(queue.head, head_node, next_node)`\n6. **If CAS succeeded**, return the data read in step 4\n7. **If CAS failed**, retry from step 1\n\n![CAS Retry Loop State Machine](./diagrams/cas-retry-pattern.svg)\n\nThe algorithm's correctness relies on several key invariants that must be maintained throughout all operations:\n\n> **Critical Invariant: Queue Structure Consistency**\n> - The queue always contains at least one node (the sentinel)\n> - The head pointer never advances beyond the tail pointer in the logical sequence\n> - Every node reachable from head is also reachable by following next pointers from head\n> - The tail pointer points either to the actual last node or to the node immediately before it\n\n### Dummy Sentinel Node Design\n\nThe dummy sentinel node represents one of the most crucial design decisions in the Michael-Scott algorithm. This seemingly simple concept - maintaining a permanent \"dummy\" node that never holds actual data - solves multiple complex synchronization problems that would otherwise require intricate special-case handling.\n\n> **Decision: Permanent Sentinel Node for Empty Queue Handling**\n> - **Context**: Concurrent queues must handle the empty state safely while allowing simultaneous enqueue and dequeue operations. Traditional approaches with null head/tail pointers create race conditions.\n> - **Options Considered**:\n>   1. Allow null head/tail pointers with special case handling\n>   2. Use boolean empty flag with additional synchronization\n>   3. Maintain permanent dummy/sentinel node that never holds data\n> - **Decision**: Use a permanent sentinel node that remains in the queue throughout its lifetime\n> - **Rationale**: Eliminates all empty queue edge cases by ensuring head and tail always point to valid nodes, reduces the number of CAS operations needed for the empty→non-empty transition, and simplifies the helping mechanism by providing a stable reference point\n> - **Consequences**: Slight memory overhead (one extra node), but dramatically simplifies the algorithm logic and eliminates several classes of race conditions\n\nThe sentinel node serves multiple critical functions in the algorithm:\n\n**Eliminates the Empty Queue Special Case**: Without a sentinel node, an empty queue would require both head and tail to point to null, creating a complex synchronization problem. When the first item is enqueued into an empty queue, both pointers must be updated atomically, which would require either a double-word CAS operation (not available on all architectures) or complex lock-based coordination. The sentinel node ensures that head and tail always point to valid memory locations.\n\n**Provides Stable Reference for Helping**: The helping mechanism relies on threads being able to advance the tail pointer when they detect it's lagging behind. Without a sentinel node, a lagging tail in an empty queue would create ambiguous states where threads can't determine whether the queue is empty or whether the tail pointer simply hasn't been updated yet.\n\n**Simplifies Dequeue Operation**: When dequeuing, threads don't need to distinguish between \"removing the last element\" and \"removing from a multi-element queue.\" The sentinel ensures that after removing the last data element, the queue still contains one node (the sentinel), maintaining structural consistency.\n\nThe sentinel node initialization and lifecycle follow a specific pattern:\n\n| Phase | Head Points To | Tail Points To | Sentinel Next | Description |\n|-------|---------------|---------------|---------------|-------------|\n| Creation | Sentinel | Sentinel | null | Empty queue with sentinel only |\n| After Enqueue | Sentinel | Data Node | Data Node | One data element present |\n| After Dequeue | Data Node | Data Node | null or Next | Back to sentinel pointing at remaining data |\n| Multiple Elements | Sentinel | Last Node | Varies | Normal operation state |\n\nThe sentinel node's `next` pointer serves as the key indicator for queue state:\n- When `sentinel.next == null`, the queue contains no data elements\n- When `sentinel.next != null`, the first data element follows the sentinel\n\nThis design creates a clean separation between structural nodes (the sentinel) and data nodes, allowing the algorithm to maintain structural invariants while data nodes are added and removed dynamically.\n\n> **Key Insight: Sentinel Node as Structural Foundation**\n> The sentinel node acts as a permanent \"foundation\" for the queue structure. Just as a building's foundation remains stable while floors are added and removed above it, the sentinel provides a stable reference point that allows the head and tail pointers to coordinate without complex empty-state handling.\n\n### Helping Mechanism for Progress\n\nThe helping mechanism represents one of the most sophisticated aspects of the Michael-Scott algorithm and is essential for achieving the lock-free progress guarantee. Without helping, operations could become blocked indefinitely when threads are preempted or delayed at critical moments, violating the fundamental lock-free property that at least one thread must always make progress.\n\n> **Decision: Cooperative Helping Protocol for Progress Guarantees**\n> - **Context**: In lock-free algorithms, threads can be preempted or delayed at any time. If one thread starts an operation but gets delayed halfway through, other threads must be able to continue making progress rather than waiting indefinitely.\n> - **Options Considered**:\n>   1. Each thread only performs its own operations (simple but can block)\n>   2. Timeout-based retry with exponential backoff\n>   3. Cooperative helping where threads complete operations started by delayed threads\n> - **Decision**: Implement cooperative helping where threads assist each other in completing operations\n> - **Rationale**: This is the only approach that guarantees lock-free progress - if any thread is delayed, others can complete its operation and continue. Provides better worst-case latency than timeout approaches and maintains linearizability.\n> - **Consequences**: Increased algorithm complexity as threads must detect and handle situations where helping is needed, but ensures robust progress guarantees even under adverse scheduling conditions\n\nThe helping mechanism operates on a simple principle: **when a thread detects that the tail pointer is \"lagging behind\" the actual end of the queue, it attempts to advance the tail pointer before proceeding with its own operation**. This cooperation ensures that no operation can remain permanently stuck waiting for another thread to complete its work.\n\n#### Detecting When Help is Needed\n\nThe key insight is recognizing when the tail pointer needs assistance. This occurs when:\n1. The tail pointer points to a node `N`\n2. Node `N`'s `next` pointer is not null (meaning another node exists beyond `N`)\n3. Therefore, the tail should point to `N.next` instead of `N`\n\nThis situation arises naturally in the two-phase enqueue process:\n1. **Phase 1**: A thread successfully executes `CAS(tail_node.next, null, new_node)`, linking the new node into the queue\n2. **Phase 2**: The same thread attempts `CAS(queue.tail, tail_node, new_node)` to advance the tail pointer\n3. **Interruption**: The thread might be preempted, delayed, or fail the second CAS due to interference\n\nAt this point, the queue is structurally sound (all nodes are properly linked), but the tail pointer is \"lagging\" - it points to the second-to-last node instead of the actual last node.\n\n#### The Helping Protocol\n\nWhen any thread (performing either enqueue or dequeue) detects this lagging condition, it executes the helping protocol:\n\n```\nHelping Algorithm:\n1. Load current_tail = queue.tail\n2. Load tail_node = current_tail.pointer  \n3. Load next_node = tail_node.next\n4. Re-check that queue.tail still equals current_tail (consistency check)\n5. If next_node != null:\n   - Execute CAS(queue.tail, current_tail, next_node)\n   - Success or failure doesn't matter - the attempt is sufficient\n6. Continue with the thread's original operation\n```\n\nThe beauty of this protocol is its **idempotent** nature - multiple threads can attempt to help simultaneously without causing corruption. If thread A succeeds in advancing the tail, thread B's helping attempt will simply fail its consistency check or its CAS operation, which is harmless.\n\n#### Linearization Points and Helping\n\nThe helping mechanism introduces subtle but important considerations for linearizability. The **linearization point** for an enqueue operation is not when the tail pointer is updated, but when the new node is successfully linked into the queue structure (the `CAS(tail_node.next, null, new_node)` operation). This is crucial because:\n\n- The element becomes visible to dequeue operations immediately after linking, regardless of tail pointer state\n- Other enqueue operations can begin helping to advance the tail, making the element officially \"at the tail\"\n- The tail pointer update is essentially a performance optimization rather than a correctness requirement\n\n| Operation Phase | Linearization Point | Effect on Queue State | Helping Trigger |\n|----------------|-------------------|---------------------|-----------------|\n| Pre-Link | N/A | No change | No |\n| Link Success | **HERE** | Element visible to dequeue | Yes (tail may lag) |\n| Tail Update Success | N/A | Tail pointer consistent | No longer needed |\n| Tail Update Failed | N/A | Tail pointer still lags | Still needed |\n\n#### Helping in Dequeue Operations\n\nDequeue operations also participate in the helping protocol, particularly when checking for empty queue conditions. When `head == tail`, a dequeue operation must determine whether the queue is truly empty or whether the tail is simply lagging behind:\n\n```\nEmpty Check with Helping:\n1. If head_node == tail_node AND head_node.next == null → truly empty\n2. If head_node == tail_node AND head_node.next != null → tail is lagging\n3. In case 2: help advance tail, then retry the dequeue operation\n```\n\nThis helping behavior is essential for correctness, not just performance. Without it, a dequeue operation might incorrectly conclude that the queue is empty when elements are actually available.\n\n> **Critical Insight: Helping as Correctness Mechanism**\n> While helping appears to be a performance optimization, it's actually required for correctness in the Michael-Scott algorithm. Operations can only complete successfully when the queue's pointer structure accurately reflects the actual node linkages, making helping a mandatory part of the protocol rather than an optional enhancement.\n\n### Common Queue Implementation Pitfalls\n\nLock-free queue implementation presents numerous subtle pitfalls that can lead to memory corruption, lost data, or violation of FIFO ordering. Understanding these common mistakes and their solutions is crucial for successful implementation.\n\n#### ⚠️ **Pitfall: Incorrect Tail Pointer Management**\n\n**The Problem**: Many implementers focus on the head pointer logic and treat the tail pointer as a simple optimization, leading to incorrect tail advancement or missing helping logic.\n\n**Why It Breaks**: The tail pointer serves as more than a performance hint - it's integral to the correctness of enqueue operations. When the tail is allowed to lag indefinitely or is advanced incorrectly, several failures occur:\n- Enqueue operations may link nodes to incorrect positions in the queue\n- The helping mechanism breaks down, potentially causing livelock\n- FIFO ordering can be violated when nodes are inserted out of sequence\n\n**Specific Failure Scenarios**:\n1. **Missing Consistency Checks**: Loading the tail pointer once and using it throughout the operation without re-checking for changes\n2. **Ignored Help Opportunities**: Detecting that `tail_node.next != null` but failing to attempt tail advancement before retrying\n3. **Premature Tail Advancement**: Advancing the tail before the new node is properly linked\n\n**The Fix**: Implement rigorous tail pointer management:\n- Always re-check tail pointer consistency after loading tail_node and its next pointer\n- Mandatory helping: never retry an enqueue without first attempting to help if tail is lagging\n- Use proper memory ordering (at least acquire semantics) when loading tail pointer and tail_node.next\n\n#### ⚠️ **Pitfall: Sentinel Node Confusion**\n\n**The Problem**: Treating the sentinel node as a data node or attempting to remove it during dequeue operations.\n\n**Why It Breaks**: The sentinel node is structural infrastructure, not a data container. Removing it or storing data in it violates the algorithm's fundamental invariants:\n- Removing the sentinel creates an empty queue state that the algorithm cannot handle correctly\n- Storing data in the sentinel means this data can never be dequeued (it's always \"one behind\" the head)\n- Pointer consistency checks assume the sentinel's permanent existence\n\n**Specific Failure Scenarios**:\n1. **Sentinel Data Storage**: Initializing the queue by storing the first data element in the sentinel node\n2. **Sentinel Removal**: Dequeue operations that advance head past the sentinel, leaving no stable reference point\n3. **Sentinel Reinitialization**: Creating new sentinel nodes during operation instead of maintaining the original\n\n**The Fix**: Enforce strict sentinel discipline:\n- Sentinel node never holds user data (data field should remain null/uninitialized)\n- Head pointer may equal sentinel pointer only when queue is logically empty\n- When head equals sentinel and sentinel.next != null, the queue contains exactly one data element\n- Never deallocate or replace the sentinel node\n\n#### ⚠️ **Pitfall: ABA Problems in Dual-Pointer Context**\n\n**The Problem**: The standard tagged pointer solution for ABA problems becomes more complex when managing two related pointers that must remain consistent.\n\n**Why It Breaks**: Unlike the single-pointer Treiber stack, the Michael-Scott queue must maintain consistency between head and tail pointers. Standard ABA protection on individual pointers is insufficient - we need **relational consistency** between the two pointers:\n- Tail might be advanced based on a stale view of the queue structure\n- Head advancement might interfere with concurrent tail updates\n- Node reuse between head and tail regions can create complex ABA scenarios\n\n**Specific Failure Scenarios**:\n1. **Inconsistent Tagging**: Using different tag increment strategies for head and tail pointers\n2. **Stale Relationship Assumptions**: Loading head and tail at different times and assuming their relationship remains valid\n3. **Cross-Pointer Interference**: Head operations affecting tail validity and vice versa\n\n**The Fix**: Implement coordinated ABA protection:\n- Use consistent tagging schemes for both head and tail pointers\n- Load both head and tail atomically (or use validation loops to ensure consistency)\n- Consider using hazard pointers for more robust memory safety instead of relying solely on tagged pointers\n\n#### ⚠️ **Pitfall: Linearization Edge Cases**\n\n**The Problem**: Incorrect identification of linearization points, especially in operations that involve helping other threads.\n\n**Why It Breaks**: Linearizability requires that each operation appears to occur atomically at some single point in time. When helping is involved, multiple threads might affect the same logical operation, making it unclear when the operation \"actually occurred\":\n- Enqueue linearization point confusion: when tail is updated vs when node is linked\n- Dequeue linearization point with helping: when head is advanced vs when data is read\n- Helping operations affecting the linearization of the original operation\n\n**Specific Failure Scenarios**:\n1. **Late Linearization**: Claiming the operation linearizes when tail is updated, even though the element was visible earlier\n2. **Helping Linearization**: Incorrectly attributing an operation's linearization to the helping thread rather than the original thread\n3. **Read-Modify Gap**: Assuming linearization occurs at read time when the actual effect happens during the modify phase\n\n**The Fix**: Establish clear linearization point rules:\n- Enqueue linearizes at successful `CAS(tail_node.next, null, new_node)` - this is when the element becomes visible\n- Dequeue linearizes at successful `CAS(queue.head, head_node, next_node)` - this is when the element is removed\n- Helping operations do not create new linearization points - they assist existing operations\n- Use formal verification or model checking tools to validate linearization point assignments\n\n#### ⚠️ **Pitfall: Memory Ordering and Visibility**\n\n**The Problem**: Using insufficient memory ordering constraints, particularly with relaxed atomics, leading to visibility and ordering issues.\n\n**Why It Breaks**: The Michael-Scott algorithm relies on specific visibility guarantees between threads. Relaxed memory ordering can allow:\n- Tail node updates to be visible before the node's next pointer is properly initialized\n- Head advancement to be visible before the previous head's next pointer is loaded\n- Helping operations to see stale views of the queue structure\n\n**The Fix**: Use appropriate memory ordering:\n- Use acquire semantics when loading pointers that will be dereferenced\n- Use release semantics when publishing new nodes or updating structural pointers\n- Consider using sequentially consistent ordering during initial development, then optimize to weaker orderings only after correctness is verified\n\n### Implementation Guidance\n\nThis section provides concrete implementation guidance for building the Michael-Scott lock-free queue. The target audience is junior developers who understand basic concurrency concepts but need practical guidance on structuring the code and handling the algorithm's subtleties.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Atomic Operations | `threading.atomic` wrappers around primitive types | Custom atomic classes with explicit memory ordering |\n| Memory Management | Python's garbage collector (no explicit reclamation) | Hazard pointer integration for manual memory management |\n| Node Allocation | Standard object instantiation with `__init__` | Object pooling with pre-allocated node cache |\n| Testing Framework | `unittest` with basic threading tests | `pytest` with property-based testing via `hypothesis` |\n| Performance Measurement | `time.time()` for basic throughput measurement | `perf_counter()` with statistical analysis and contention metrics |\n\nFor initial implementation, use Python's built-in threading primitives and rely on the garbage collector. This allows focus on algorithm correctness before optimizing for performance.\n\n#### Recommended File Structure\n\n```\nlock_free_structures/\n├── __init__.py\n├── atomic/\n│   ├── __init__.py\n│   ├── operations.py        ← CAS, fetch_and_add from Milestone 1\n│   └── memory_ordering.py   ← Memory ordering constants and utilities\n├── queue/\n│   ├── __init__.py\n│   ├── michael_scott.py     ← Core Michael-Scott queue implementation\n│   ├── node.py              ← Queue node structure and utilities\n│   └── queue_test.py        ← Comprehensive test suite\n├── utils/\n│   ├── __init__.py\n│   ├── linearization.py     ← Linearizability checking utilities\n│   └── stress_test.py       ← Multi-threaded stress testing framework\n└── examples/\n    ├── __init__.py\n    ├── producer_consumer.py  ← Example usage with multiple producers/consumers\n    └── benchmark.py          ← Performance comparison vs lock-based queue\n```\n\nThis structure maintains clear separation between the atomic primitives (reused from Milestone 1), the queue-specific logic, testing utilities, and example usage patterns.\n\n#### Infrastructure Starter Code\n\n**Complete Node Implementation** (`queue/node.py`):\n```python\nfrom typing import Optional, TypeVar, Generic\nfrom ..atomic.operations import AtomicReference\n\nT = TypeVar('T')\n\nclass Node(Generic[T]):\n    \"\"\"\n    Queue node containing data and atomic pointer to next node.\n    \n    The Node class represents a single element in the Michael-Scott queue's\n    linked list structure. Each node contains user data and an atomic reference\n    to the next node in the queue.\n    \"\"\"\n    \n    def __init__(self, data: Optional[T] = None):\n        \"\"\"\n        Initialize a new queue node.\n        \n        Args:\n            data: The user data to store in this node. None for sentinel nodes.\n        \"\"\"\n        self.data: Optional[T] = data\n        self.next: AtomicReference['Node[T]'] = AtomicReference(None)\n    \n    def is_sentinel(self) -> bool:\n        \"\"\"Check if this is a sentinel node (no user data).\"\"\"\n        return self.data is None\n    \n    def __repr__(self) -> str:\n        next_repr = \"None\" if self.next.load() is None else \"Node(...)\"\n        return f\"Node(data={self.data}, next={next_repr})\"\n```\n\n**Memory Ordering Constants** (`atomic/memory_ordering.py`):\n```python\nfrom enum import Enum\n\nclass MemoryOrdering(Enum):\n    \"\"\"\n    Memory ordering constraints for atomic operations.\n    \n    These constants define the synchronization and ordering guarantees\n    for atomic operations in the lock-free queue implementation.\n    \"\"\"\n    RELAXED = \"relaxed\"     # No ordering constraints\n    ACQUIRE = \"acquire\"     # Acquire semantics for loads\n    RELEASE = \"release\"     # Release semantics for stores  \n    SEQ_CST = \"seq_cst\"     # Sequential consistency\n\n# Export commonly used orderings as module-level constants\nRELAXED = MemoryOrdering.RELAXED\nACQUIRE = MemoryOrdering.ACQUIRE\nRELEASE = MemoryOrdering.RELEASE\nSEQ_CST = MemoryOrdering.SEQ_CST\n```\n\n**Linearizability Testing Utilities** (`utils/linearization.py`):\n```python\nimport threading\nimport time\nfrom typing import List, Dict, Any, Tuple\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nclass OperationType(Enum):\n    ENQUEUE = \"enqueue\"\n    DEQUEUE = \"dequeue\"\n\n@dataclass\nclass Operation:\n    \"\"\"Record of a single queue operation for linearizability checking.\"\"\"\n    type: OperationType\n    thread_id: int\n    start_time: float\n    end_time: float\n    input_value: Any = None\n    return_value: Any = None\n    successful: bool = True\n\nclass LinearizabilityChecker:\n    \"\"\"\n    Utility for recording and verifying linearizability of queue operations.\n    \n    This class provides a framework for testing whether a sequence of\n    concurrent queue operations could have occurred in some sequential\n    order that respects the FIFO semantics.\n    \"\"\"\n    \n    def __init__(self):\n        self._operations: List[Operation] = []\n        self._lock = threading.Lock()\n    \n    def record_operation(self, op: Operation) -> None:\n        \"\"\"Thread-safely record a completed operation.\"\"\"\n        with self._lock:\n            self._operations.append(op)\n    \n    def verify_history(self) -> bool:\n        \"\"\"\n        Verify that recorded operations could have occurred in FIFO order.\n        \n        Returns:\n            True if the operation history is linearizable, False otherwise.\n        \"\"\"\n        # TODO: Implement full linearizability checking algorithm\n        # For now, return True to allow basic testing\n        return True\n    \n    def get_operation_count(self) -> Dict[OperationType, int]:\n        \"\"\"Get count of operations by type.\"\"\"\n        counts = {OperationType.ENQUEUE: 0, OperationType.DEQUEUE: 0}\n        for op in self._operations:\n            counts[op.type] += 1\n        return counts\n```\n\n#### Core Logic Skeleton\n\n**Michael-Scott Queue Implementation** (`queue/michael_scott.py`):\n```python\nfrom typing import Optional, TypeVar, Generic\nfrom threading import current_thread\nimport time\n\nfrom .node import Node\nfrom ..atomic.operations import AtomicReference, compare_and_swap\nfrom ..atomic.memory_ordering import ACQUIRE, RELEASE, SEQ_CST\nfrom ..utils.linearization import Operation, OperationType, LinearizabilityChecker\n\nT = TypeVar('T')\n\nclass MichaelScottQueue(Generic[T]):\n    \"\"\"\n    Lock-free FIFO queue implementation using the Michael-Scott algorithm.\n    \n    This queue supports concurrent enqueue and dequeue operations from\n    multiple threads without using locks. It guarantees FIFO ordering\n    and linearizability of all operations.\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"\n        Initialize an empty queue with a sentinel node.\n        \n        The queue starts with a single sentinel node that both head and\n        tail pointers reference. This eliminates empty queue edge cases.\n        \"\"\"\n        # TODO 1: Create a sentinel node with no data\n        # TODO 2: Initialize head pointer to point to sentinel\n        # TODO 3: Initialize tail pointer to point to sentinel\n        # Hint: Both pointers should reference the same sentinel initially\n        pass\n    \n    def enqueue(self, data: T) -> None:\n        \"\"\"\n        Add an element to the tail of the queue.\n        \n        This operation is lock-free and linearizable. It may help advance\n        the tail pointer if it detects that another thread's enqueue\n        operation left the tail pointer lagging.\n        \n        Args:\n            data: The element to add to the queue.\n        \"\"\"\n        start_time = time.time()\n        \n        # TODO 1: Create a new node with the provided data\n        # TODO 2: Loop until enqueue succeeds:\n        #   TODO 2a: Load current tail pointer and tail node\n        #   TODO 2b: Load the next pointer of the tail node\n        #   TODO 2c: Re-check tail pointer consistency (guard against races)\n        #   TODO 2d: If tail_node.next is null (tail at actual end):\n        #     TODO 2d1: Attempt CAS to link new_node as tail_node.next\n        #     TODO 2d2: If CAS succeeds, break out of loop\n        #     TODO 2d3: If CAS fails, continue loop (another thread added node)\n        #   TODO 2e: If tail_node.next is not null (tail is lagging):\n        #     TODO 2e1: Help advance tail pointer to tail_node.next\n        #     TODO 2e2: Continue loop regardless of helping CAS result\n        # TODO 3: After loop: attempt to advance tail pointer to new_node\n        # TODO 4: Record operation for linearizability testing (if enabled)\n        # Hint: The linearization point is step 2d1 (successful link CAS)\n        pass\n    \n    def dequeue(self) -> Optional[T]:\n        \"\"\"\n        Remove and return an element from the head of the queue.\n        \n        This operation is lock-free and linearizable. It returns None\n        if the queue is empty and may help advance the tail pointer\n        if it detects lagging.\n        \n        Returns:\n            The dequeued element, or None if the queue was empty.\n        \"\"\"\n        start_time = time.time()\n        \n        while True:\n            # TODO 1: Load current head and tail pointers and their nodes\n            # TODO 2: Re-check head pointer consistency\n            # TODO 3: Check if queue appears empty (head == tail):\n            #   TODO 3a: Load head_node.next to verify emptiness\n            #   TODO 3b: If head_node.next is null, return None (truly empty)\n            #   TODO 3c: If head_node.next is not null, help advance tail\n            #   TODO 3d: Continue loop after helping\n            # TODO 4: Queue is non-empty (head != tail):\n            #   TODO 4a: Load next_node = head_node.next\n            #   TODO 4b: If next_node is null, retry (inconsistent state)\n            #   TODO 4c: Read data from next_node BEFORE attempting CAS\n            #   TODO 4d: Attempt CAS to advance head pointer to next_node\n            #   TODO 4e: If CAS succeeds, return the data read in step 4c\n            #   TODO 4f: If CAS fails, continue loop\n            # TODO 5: Record operation for linearizability testing (if enabled)\n            # Hint: The linearization point is step 4d (successful head advance)\n            pass\n    \n    def is_empty(self) -> bool:\n        \"\"\"\n        Check if the queue is empty.\n        \n        Returns:\n            True if the queue contains no data elements, False otherwise.\n        \"\"\"\n        # TODO 1: Load head and tail pointers\n        # TODO 2: If head != tail, queue is definitely non-empty\n        # TODO 3: If head == tail, check if head_node.next is null\n        # TODO 4: Return True only if head == tail and head_node.next == null\n        # Hint: This is a snapshot check - result may be stale immediately\n        pass\n    \n    def size_hint(self) -> int:\n        \"\"\"\n        Provide an approximate count of queue elements.\n        \n        This is a best-effort estimate that may be inaccurate due to\n        concurrent modifications. Use only for monitoring/debugging.\n        \n        Returns:\n            Approximate number of elements in the queue.\n        \"\"\"\n        # TODO 1: Start from head node and traverse next pointers\n        # TODO 2: Count non-sentinel nodes until reaching tail or null\n        # TODO 3: Handle the case where tail is lagging during traversal\n        # TODO 4: Return the count (may be stale by the time it's returned)\n        # Warning: This operation is O(n) and not lock-free\n        pass\n```\n\n#### Language-Specific Implementation Hints\n\n**Python Threading Considerations**:\n- Use `threading.Lock` only in testing utilities, never in queue operations\n- Python's GIL actually helps with some race conditions but don't rely on it for correctness\n- Use `threading.local()` for per-thread data like hazard pointers\n- Consider `weakref` for debugging/monitoring without affecting garbage collection\n\n**Atomic Operations in Python**:\n```python\n# Example of proper CAS implementation check:\ndef safe_compare_and_swap(atomic_ref, expected, new_value):\n    \"\"\"\n    Wrapper that handles Python's atomic operation limitations.\n    \"\"\"\n    try:\n        return compare_and_swap(atomic_ref, expected, new_value)\n    except Exception as e:\n        # Log the exception for debugging but don't crash\n        print(f\"CAS operation failed: {e}\")\n        return False, atomic_ref.load()\n```\n\n**Memory Management Strategy**:\n- Initially rely on Python's garbage collector for node cleanup\n- Implement explicit node pooling only after correctness is verified\n- Use `__slots__` in Node class to reduce memory overhead\n- Consider `gc.disable()` during performance benchmarks to get consistent timing\n\n#### Milestone Checkpoints\n\n**After Implementing Basic Queue Structure**:\n```bash\ncd lock_free_structures/queue/\npython -m pytest queue_test.py::test_basic_enqueue_dequeue -v\n```\nExpected output: All basic enqueue/dequeue operations succeed with single thread\n\n**After Implementing Helping Mechanism**:\n```bash\npython -c \"\nfrom michael_scott import MichaelScottQueue\nimport threading\nimport time\n\nq = MichaelScottQueue()\ndef producer():\n    for i in range(100):\n        q.enqueue(i)\n        time.sleep(0.001)  # Small delay to encourage helping\n\ndef consumer():\n    for _ in range(100):\n        q.dequeue()\n        time.sleep(0.001)\n\nthreads = [threading.Thread(target=producer), threading.Thread(target=consumer)]\nfor t in threads: t.start()\nfor t in threads: t.join()\nprint('Helping test completed successfully')\n\"\n```\n\n**After Full Implementation**:\n```bash\npython examples/stress_test.py --threads=8 --operations=10000\n```\nExpected output: No lost elements, FIFO ordering preserved, performance metrics\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | Diagnosis | Fix |\n|---------|-------------|-----------|-----|\n| Enqueue hangs forever | Tail pointer never advances | Check if helping logic is missing in enqueue loop | Implement mandatory helping before retry |\n| Lost elements during dequeue | Reading data after CAS instead of before | Print data value and CAS success/failure | Read `next_node.data` before attempting head advance CAS |\n| FIFO ordering violated | Multiple nodes linked at same position | Add logging to CAS operations on next pointers | Ensure consistency checks before each CAS attempt |\n| Segmentation fault/AttributeError | Using freed/None node reference | Check if sentinel is being deallocated | Never remove or replace the sentinel node |\n| Poor performance under contention | Excessive CAS retries without backoff | Measure CAS retry counts per operation | Add exponential backoff in retry loops |\n| Memory usage grows unbounded | Nodes not being garbage collected | Check for circular references or global node storage | Ensure nodes become unreachable after dequeue |\n\nThe key to successful debugging is adding extensive logging around CAS operations and pointer consistency checks. Most bugs in lock-free queues manifest as violations of the basic invariants: sentinel permanence, head-tail ordering, and proper node linkage.\n\n\n## Hazard Pointers for Memory Reclamation\n\n> **Milestone(s):** Milestone 4 (Hazard Pointers) - This section implements a safe memory reclamation scheme that prevents use-after-free errors while maintaining the non-blocking properties of lock-free data structures.\n\nThe fundamental challenge in lock-free programming lies not just in implementing the core algorithms, but in safely managing memory in an environment where multiple threads may simultaneously access, modify, and potentially deallocate shared nodes. Traditional garbage-collected languages solve this problem automatically, but in systems programming languages like C++, Rust, or even Python with manual memory management, we must carefully coordinate when it's safe to free memory that other threads might still be accessing.\n\n![Hazard Pointer Memory Lifecycle](./diagrams/hazard-pointer-lifecycle.svg)\n\nThe **hazard pointer** technique, introduced by Maged Michael in 2004, provides an elegant solution to this memory reclamation problem. It allows threads to announce their intention to access specific memory locations, preventing other threads from prematurely deallocating those locations while maintaining the non-blocking properties essential to lock-free algorithms.\n\n### Mental Model: Hazard Pointers as Safety Signs\n\nImagine a large construction site where multiple crews are working on different parts of a building simultaneously. Each crew needs to occasionally demolish old structures to make room for new ones, but they must ensure no workers are currently inside those structures. Traditional locking would be like having a single master key that controls access to the entire site—only one crew could work at a time, creating massive bottlenecks.\n\nHazard pointers work like a sophisticated safety sign system. When a worker (thread) needs to enter a structure (access a node), they place a bright safety sign outside with their name and the structure's address. This sign announces to everyone: \"Worker Alice is inside Building 247—DO NOT DEMOLISH.\" When the worker finishes their task, they remove their safety sign.\n\nMeanwhile, the demolition crew (memory reclamation system) maintains a list of structures marked for demolition (retirement list). Before destroying any structure, they walk around the entire site checking for safety signs. If any worker has posted a sign for a particular building, that building stays standing. Only structures with no safety signs can be safely demolished.\n\nThis system ensures safety (no worker gets crushed by falling debris) while maintaining parallelism (multiple crews can work simultaneously). The key insight is that workers announce their protection *before* entering, and the demolition crew respects those announcements by scanning all signs before any demolition.\n\nJust as construction workers might post multiple safety signs if they're moving between several structures during a complex task, threads maintain multiple hazard pointers to protect different nodes they're accessing during complex multi-step operations.\n\n### Hazard Pointer Protocol\n\nThe hazard pointer protocol consists of four fundamental operations that work together to provide safe, non-blocking memory reclamation. Each thread maintains a small set of hazard pointer slots (typically 1-4 pointers) that can be used to protect nodes currently being accessed.\n\n**Decision: Per-Thread Hazard Pointer Slots**\n- **Context**: We need a way for threads to announce which pointers they're currently protecting, with minimal overhead and contention.\n- **Options Considered**:\n  1. Global shared array with per-thread sections\n  2. Per-thread local storage with global registry\n  3. Lock-free linked list of hazard records\n- **Decision**: Per-thread local storage with global registry\n- **Rationale**: Minimizes cache line contention since threads primarily access their own slots, while still allowing global scanning for reclamation. Local storage avoids false sharing between threads.\n- **Consequences**: Requires thread registration/deregistration on startup/shutdown, but provides optimal performance for the common case of protecting/releasing pointers.\n\nThe core hazard pointer data structures organize protection and reclamation state both per-thread and globally:\n\n| Component | Type | Description |\n|-----------|------|-------------|\n| `HazardPointer.protected_pointers` | Array[AtomicReference] | Per-thread array of currently protected node pointers |\n| `HazardPointer.thread_registry` | AtomicReference to linked list | Global list of all active thread hazard pointer slots |\n| `RetirementList.retired_nodes` | Thread-local queue | Queue of nodes this thread has removed but not yet reclaimed |\n| `RetirementList.scan_threshold` | Integer | Number of retired nodes that triggers a global scan |\n| `HazardPointer.max_hazards_per_thread` | Constant | Maximum number of simultaneous protections per thread (typically 2-4) |\n\nThe protection protocol follows a careful sequence to prevent race conditions between protection and reclamation:\n\n| Operation | Method Signature | Parameters | Returns | Description |\n|-----------|------------------|------------|---------|-------------|\n| `protect` | `protect(pointer)` | `pointer`: Node pointer to protect | `slot_index`: Index of hazard slot used | Announces protection for a specific node pointer |\n| `release` | `release(slot_index)` | `slot_index`: Previously returned slot index | None | Clears protection from the specified hazard slot |\n| `retire` | `retire(node)` | `node`: Node pointer to eventually reclaim | None | Adds node to thread's retirement list for deferred deletion |\n| `scan_and_reclaim` | `scan_and_reclaim()` | None | `reclaimed_count`: Number of nodes actually freed | Scans all hazard pointers and frees unprotected retired nodes |\n\nThe protection sequence requires careful ordering to prevent race conditions where a node might be retired and reclaimed between when a thread loads a pointer and when it protects that pointer:\n\n1. **Load the pointer** from the shared data structure using appropriate memory ordering (typically `ACQUIRE` to ensure we see all writes to the pointed-to object)\n2. **Immediately protect the loaded pointer** by storing it in an available hazard pointer slot using `SEQ_CST` ordering to ensure global visibility\n3. **Re-read the original pointer location** to verify it hasn't changed since step 1\n4. **If the pointer changed, release protection and retry** the entire sequence from step 1\n5. **If the pointer is unchanged, proceed with accessing the protected node** knowing it cannot be reclaimed while protected\n6. **When finished with the node, release protection** by clearing the hazard pointer slot\n\n> The critical insight in hazard pointer protection is that we must protect first, then verify. Simply loading a pointer and then protecting it creates a race window where another thread could retire and reclaim the node between the load and protection.\n\n**Decision: Protect-Then-Verify Pattern**\n- **Context**: Race condition between loading a pointer and protecting it, where the node could be reclaimed in the gap.\n- **Options Considered**:\n  1. Load pointer, then protect (unsafe—node could be freed)\n  2. Protect pointer, then verify it's still current\n  3. Use epoch-based reclamation instead\n- **Decision**: Protect pointer, then verify it's still current\n- **Rationale**: Only this pattern guarantees that once we successfully protect a valid pointer, it remains valid for the duration of our access. The verification step catches cases where the pointer changed between our load and protection.\n- **Consequences**: Requires retry loops in data structure operations, but provides strong safety guarantees without blocking.\n\nThe retirement and reclamation protocol operates on a per-thread basis with periodic global coordination:\n\n1. **When removing a node from a data structure**, the removing thread calls `retire(node)` instead of immediately freeing the memory\n2. **The node is added to the thread's local retirement list**, avoiding contention with other threads' retirement lists\n3. **When the retirement list reaches the scan threshold**, the thread triggers `scan_and_reclaim()`\n4. **During scanning, the thread examines all hazard pointer slots** from all registered threads in the system\n5. **Any retired node that appears in any thread's hazard pointer slots** is kept in the retirement list (not reclaimed)\n6. **Nodes not protected by any hazard pointer** are safely freed and removed from the retirement list\n7. **If many nodes remain protected after scanning**, the thread may increase its scan threshold to reduce scanning frequency\n\n### Retirement List and Scanning Algorithm\n\nThe retirement list serves as a staging area where nodes wait between removal from the data structure and actual memory reclamation. This deferred reclamation is essential because other threads might still hold references to these nodes and be in the process of accessing them.\n\n**Decision: Per-Thread Retirement Lists vs Global Retirement List**\n- **Context**: We need to track nodes that have been removed from data structures but cannot yet be safely freed.\n- **Options Considered**:\n  1. Single global retirement list protected by locks\n  2. Single global lock-free retirement list\n  3. Per-thread retirement lists with periodic scanning\n- **Decision**: Per-thread retirement lists with periodic scanning\n- **Rationale**: Eliminates contention on retirement operations since each thread only modifies its own list. Scanning can be done independently by each thread, distributing the reclamation work.\n- **Consequences**: Requires more complex scanning logic that must examine all threads' hazard pointers, but provides better scalability and cache locality.\n\nThe retirement list maintains both immediate and batch processing capabilities:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `retired_nodes` | Queue[Node] | FIFO queue of nodes removed but not yet reclaimed |\n| `scan_threshold` | Integer | Number of retired nodes that triggers automatic scanning |\n| `last_scan_size` | Integer | Size of retirement list after the most recent scan |\n| `adaptive_threshold` | Boolean | Whether to dynamically adjust scan threshold based on reclamation success rate |\n| `max_retirement_size` | Integer | Hard limit on retirement list size to prevent unbounded growth |\n\nThe scanning algorithm performs a global survey of all hazard pointers to determine which retired nodes can be safely reclaimed:\n\n1. **Snapshot all active hazard pointers** by iterating through the thread registry and reading each thread's hazard pointer slots\n2. **Create a temporary protected set** containing all non-null hazard pointer values found during the snapshot\n3. **Iterate through the local retirement list** examining each retired node\n4. **For each retired node, check if its address appears in the protected set**\n5. **If the node is not protected, immediately free its memory** and remove it from the retirement list\n6. **If the node is protected, keep it in the retirement list** for reconsideration during the next scan\n7. **Update statistics** about scan effectiveness and potentially adjust the scan threshold\n\n> A subtle but crucial detail: the scanning algorithm must use appropriate memory ordering when reading hazard pointers. Using `SEQ_CST` ordering ensures that we don't miss any protection announcements that were made concurrent with our scan.\n\nThe adaptive threshold mechanism helps balance the overhead of frequent scanning against the memory overhead of keeping many retired nodes:\n\n| Condition | Scan Threshold Adjustment | Rationale |\n|-----------|---------------------------|-----------|\n| >80% of nodes reclaimed | Decrease threshold by 25% | High reclamation rate suggests scanning is effective, so scan more frequently |\n| <20% of nodes reclaimed | Increase threshold by 50% | Low reclamation rate suggests many nodes are still protected, so scan less frequently |\n| Retirement list exceeds max size | Force immediate scan regardless of threshold | Prevent unbounded memory growth even if scanning is ineffective |\n| Thread preparing to exit | Scan until retirement list is empty | Ensure no memory leaks when thread terminates |\n\n**Decision: Batch vs Immediate Scanning**\n- **Context**: When to trigger the expensive global scan operation that examines all threads' hazard pointers.\n- **Options Considered**:\n  1. Scan after every retire operation\n  2. Scan when retirement list reaches a threshold\n  3. Scan on a periodic timer\n- **Decision**: Scan when retirement list reaches a threshold\n- **Rationale**: Batching scan operations amortizes the cost of examining all hazard pointers across multiple retired nodes. Timer-based scanning could waste work or delay reclamation unnecessarily.\n- **Consequences**: Requires tuning the threshold based on workload characteristics, but provides predictable memory overhead bounds.\n\n### Integration with Lock-free Data Structures\n\nIntegrating hazard pointers into existing lock-free data structures requires careful modification of the algorithms to include protection and retirement calls at appropriate points. The integration must preserve the original algorithms' correctness and progress guarantees while adding memory safety.\n\nFor the Treiber stack, hazard pointer integration focuses on protecting the top pointer during traversal and retiring nodes when they're successfully removed:\n\n**Modified Treiber Stack Pop Operation with Hazard Pointers:**\n\n1. **Allocate a hazard pointer slot** for protecting the node we're about to access\n2. **Load the current top pointer** using `ACQUIRE` memory ordering\n3. **If the top pointer is null (empty stack), release the hazard slot and return empty indication**\n4. **Protect the loaded top pointer** by storing it in our hazard pointer slot with `SEQ_CST` ordering\n5. **Re-read the top pointer** to verify it hasn't changed since our initial load\n6. **If the top pointer changed, release protection and retry** from step 2 (another thread modified the stack)\n7. **Load the next pointer** from the protected top node (this is safe because the node cannot be reclaimed while protected)\n8. **Attempt to CAS the stack's top pointer** from the protected node to its next pointer\n9. **If CAS fails, release protection and retry** from step 2 (another thread modified the stack concurrently)\n10. **If CAS succeeds, retire the old top node** and release our hazard pointer protection\n11. **Return the data from the successfully popped node**\n\n| Integration Point | Original Treiber Stack | With Hazard Pointers |\n|------------------|------------------------|----------------------|\n| Before accessing top node | `top = load(stack.top)` | `slot = protect(load(stack.top))` then verify unchanged |\n| Before dereferencing node | Direct access: `next = top.next` | Check protection first: `next = protected_top.next` |\n| After successful pop | `free(old_top)` | `retire(old_top)` and `release(slot)` |\n| On retry/failure | Continue with new top value | `release(slot)` then retry protection sequence |\n\nFor the Michael-Scott queue, hazard pointer integration is more complex because both enqueue and dequeue operations may need to traverse multiple nodes and help other operations:\n\n**Modified Michael-Scott Queue Dequeue Operation with Hazard Pointers:**\n\n1. **Allocate hazard pointer slots** for both head and its next node (we may need to protect both during the helping protocol)\n2. **Load and protect the current head pointer** following the protect-then-verify pattern\n3. **Load and protect the head's next pointer**, which points to the first actual data node (or null if empty)\n4. **Load the current tail pointer** (no protection needed yet since we're just reading for comparison)\n5. **If head and tail point to the same node and next is null, the queue is empty**—release all protections and return empty indication\n6. **If head and tail point to the same node but next is not null**, the tail is lagging—help advance the tail pointer\n7. **Re-verify that head hasn't changed** since our protection (helping operations might have modified it)\n8. **Read the data from the next node** (this is safe because we're protecting the next node)\n9. **Attempt to CAS the head pointer** to advance it from the current head to the next node\n10. **If CAS succeeds, retire the old head node** (the dummy sentinel) and release protections\n11. **Return the data from the successfully dequeued node**\n\n> The key insight in queue integration is that we may need to protect multiple nodes simultaneously during helping operations. The number of hazard pointer slots per thread must accommodate the maximum number of nodes any single operation might access concurrently.\n\n**Decision: Number of Hazard Pointer Slots Per Thread**\n- **Context**: Each thread needs a fixed number of hazard pointer slots to protect nodes during lock-free operations.\n- **Options Considered**:\n  1. One slot per thread (minimal overhead)\n  2. Two slots per thread (covers most operations)\n  3. Four slots per thread (handles complex helping scenarios)\n- **Decision**: Four slots per thread\n- **Rationale**: Stack operations need 1 slot, basic queue operations need 2 slots, but queue helping operations may need 3-4 slots when protecting head, next, tail, and tail.next simultaneously during complex race conditions.\n- **Consequences**: Higher memory overhead per thread, but ensures all operations can make progress without deadlock due to insufficient protection slots.\n\nThe integration also requires careful attention to the linearization points of the original algorithms. The linearization point—the moment when each operation appears to take effect atomically—must remain the same even with hazard pointer additions:\n\n| Operation | Original Linearization Point | With Hazard Pointers | Verification |\n|-----------|------------------------------|----------------------|--------------|\n| Stack Push | Successful CAS of top pointer | Same: successful CAS of top pointer | New node becomes visible atomically |\n| Stack Pop | Successful CAS of top pointer | Same: successful CAS of top pointer | Node removal happens atomically |\n| Queue Enqueue | Successful CAS linking new node | Same: successful CAS linking new node | New node becomes reachable atomically |\n| Queue Dequeue | Successful CAS advancing head | Same: successful CAS advancing head | Node removal happens atomically |\n\n### Common Hazard Pointer Pitfalls\n\nThe complexity of hazard pointer protocols creates numerous opportunities for subtle bugs that can lead to memory corruption, use-after-free errors, or memory leaks. Understanding these pitfalls is essential for correct implementation.\n\n⚠️ **Pitfall: Protecting After Loading (Race Window)**\n\nThe most dangerous mistake is loading a pointer and then trying to protect it in separate steps:\n\n```\n// WRONG - race condition\nnode = load(stack.top);        // Node loaded\n// <-- Node could be retired and freed here by another thread\nslot = protect(node);          // Too late - node might be garbage\ndata = node.data;              // Use-after-free!\n```\n\n**Why it's wrong**: Between loading the pointer and protecting it, another thread could pop the node, retire it, scan and find no protections, then free the memory. When we finally protect the pointer, we're protecting a dangling reference.\n\n**How to fix**: Always protect immediately after loading, then verify the pointer is still current:\n1. Load pointer\n2. Immediately protect the loaded value\n3. Re-read the original location to verify it's unchanged\n4. If changed, release protection and retry\n\n⚠️ **Pitfall: Forgetting to Release Protection**\n\nFailing to release hazard pointer protection when finishing with a node:\n\n**Why it's wrong**: The node will appear permanently protected during all future scans, causing a memory leak. Even worse, if the thread exits without releasing protection, the protection might persist indefinitely.\n\n**How to fix**: Use RAII-style protection guards or ensure every protection operation has a corresponding release operation. Consider using defer statements or finally blocks to guarantee cleanup.\n\n⚠️ **Pitfall: Insufficient Memory Ordering in Protection**\n\nUsing relaxed memory ordering when setting hazard pointers:\n\n```\n// WRONG - relaxed ordering\nprotect_slot.store(node, RELAXED);\n```\n\n**Why it's wrong**: Other threads performing scans might not see the protection due to weak memory ordering, allowing them to reclaim the node while this thread is still using it.\n\n**How to fix**: Use `SEQ_CST` or at least `RELEASE` ordering when setting hazard pointers to ensure global visibility:\n```\nprotect_slot.store(node, SEQ_CST);\n```\n\n⚠️ **Pitfall: Unbounded Retirement List Growth**\n\nAllowing the retirement list to grow without bounds when reclamation is ineffective:\n\n**Why it's wrong**: If many nodes remain protected for extended periods, the retirement list can consume unlimited memory, potentially causing out-of-memory conditions.\n\n**How to fix**: Implement a maximum retirement list size with forced scanning when the limit is approached. Consider falling back to synchronous reclamation (with appropriate backoff) when asynchronous scanning is insufficient.\n\n⚠️ **Pitfall: Thread Exit Without Cleanup**\n\nThreads exiting without cleaning up their hazard pointer slots and retirement lists:\n\n**Why it's wrong**: Dead threads' hazard pointer slots might contain stale protections that prevent reclamation forever. Their retirement lists become unreachable, causing memory leaks.\n\n**How to fix**: Implement thread cleanup that:\n1. Clears all hazard pointer slots for the exiting thread\n2. Performs a final scan to reclaim all nodes in the thread's retirement list\n3. Removes the thread from the global registry\n\n| Cleanup Phase | Action Required | Failure Consequence |\n|---------------|-----------------|-------------------|\n| Clear hazard slots | Set all slots to null with `SEQ_CST` ordering | Permanent false protections preventing reclamation |\n| Final retirement scan | Scan and reclaim until retirement list is empty | Memory leak from unreachable retired nodes |\n| Registry removal | Remove thread record from global registry | Wasted scanning effort on dead thread slots |\n\n⚠️ **Pitfall: ABA Problem in Hazard Pointer Management**\n\nThe hazard pointer system itself can suffer from ABA problems if thread registry nodes are reused:\n\n**Why it's wrong**: If a thread exits and its registry node is immediately reused for a new thread, scans might incorrectly consider old hazard values as current protections.\n\n**How to fix**: Use tagged pointers or generation counters in the thread registry, or delay reuse of registry nodes until sufficient time has passed.\n\n### Implementation Guidance\n\nThe hazard pointer system requires careful coordination between low-level atomic operations and high-level memory management policies. This section provides concrete guidance for implementing a production-ready hazard pointer system.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Thread Registry | Fixed-size array with atomic slots | Lock-free linked list with dynamic growth |\n| Memory Ordering | Sequential consistency everywhere | Relaxed/acquire/release optimizations |\n| Retirement Storage | Simple linked list per thread | Circular buffer with batch processing |\n| Protection Interface | Manual protect/release calls | RAII guard objects with automatic cleanup |\n| Thread-Local Storage | Standard library thread_local | Custom per-thread data with manual registration |\n\n#### Recommended File Structure\n\n```\nlock_free/\n  hazard_pointers/\n    __init__.py                 ← Public API exports\n    hazard_pointer.py           ← Core HazardPointer class\n    retirement_list.py          ← RetirementList implementation\n    thread_registry.py          ← Global thread management\n    protection_guard.py         ← RAII protection helpers\n    atomic_operations.py        ← Low-level atomic wrappers\n  tests/\n    test_hazard_pointers.py     ← Unit tests for hazard pointer operations\n    test_integration.py         ← Integration tests with stack/queue\n    test_stress.py              ← Multi-threaded stress tests\n```\n\n#### Infrastructure Starter Code\n\n**Complete Thread Registry Implementation** (ready to use):\n\n```python\nimport threading\nfrom typing import List, Optional\nfrom dataclasses import dataclass\nfrom .atomic_operations import AtomicReference, MemoryOrdering\n\n@dataclass\nclass ThreadRecord:\n    \"\"\"Registry entry for one thread's hazard pointer slots.\"\"\"\n    thread_id: int\n    hazard_slots: List[AtomicReference]\n    next_record: AtomicReference\n    active: AtomicReference  # Boolean indicating if thread is still active\n\nclass ThreadRegistry:\n    \"\"\"Global registry of all threads using hazard pointers.\"\"\"\n    \n    def __init__(self, max_hazards_per_thread: int = 4):\n        self.max_hazards_per_thread = max_hazards_per_thread\n        self.head = AtomicReference(None)  # Head of registry linked list\n        self.thread_local_data = threading.local()\n    \n    def register_thread(self) -> ThreadRecord:\n        \"\"\"Register current thread and return its record.\"\"\"\n        thread_id = threading.get_ident()\n        \n        # Create hazard slots for this thread\n        hazard_slots = [\n            AtomicReference(None) \n            for _ in range(self.max_hazards_per_thread)\n        ]\n        \n        # Create new thread record\n        record = ThreadRecord(\n            thread_id=thread_id,\n            hazard_slots=hazard_slots,\n            next_record=AtomicReference(None),\n            active=AtomicReference(True)\n        )\n        \n        # Insert into global registry using lock-free prepend\n        while True:\n            current_head = self.head.load(MemoryOrdering.ACQUIRE)\n            record.next_record.store(current_head, MemoryOrdering.RELAXED)\n            if self.head.compare_and_swap(current_head, record):\n                break\n        \n        # Store in thread-local storage for fast access\n        self.thread_local_data.record = record\n        return record\n    \n    def get_current_thread_record(self) -> Optional[ThreadRecord]:\n        \"\"\"Get current thread's record, registering if necessary.\"\"\"\n        if not hasattr(self.thread_local_data, 'record'):\n            return self.register_thread()\n        return self.thread_local_data.record\n    \n    def iterate_all_records(self):\n        \"\"\"Iterate over all registered thread records.\"\"\"\n        current = self.head.load(MemoryOrdering.ACQUIRE)\n        while current is not None:\n            if current.active.load(MemoryOrdering.RELAXED):\n                yield current\n            current = current.next_record.load(MemoryOrdering.ACQUIRE)\n    \n    def cleanup_thread(self):\n        \"\"\"Clean up current thread's registration.\"\"\"\n        if hasattr(self.thread_local_data, 'record'):\n            record = self.thread_local_data.record\n            record.active.store(False, MemoryOrdering.RELEASE)\n            \n            # Clear all hazard slots\n            for slot in record.hazard_slots:\n                slot.store(None, MemoryOrdering.SEQ_CST)\n```\n\n**Complete Atomic Operations Wrapper** (ready to use):\n\n```python\nimport threading\nfrom enum import Enum\nfrom typing import Any, Tuple, Optional\n\nclass MemoryOrdering(Enum):\n    RELAXED = \"relaxed\"\n    ACQUIRE = \"acquire\" \n    RELEASE = \"release\"\n    SEQ_CST = \"seq_cst\"\n\nclass AtomicReference:\n    \"\"\"Thread-safe atomic reference with memory ordering support.\"\"\"\n    \n    def __init__(self, initial_value: Any = None):\n        self._value = initial_value\n        self._lock = threading.Lock()  # Simplified implementation using lock\n    \n    def load(self, ordering: MemoryOrdering = MemoryOrdering.SEQ_CST) -> Any:\n        \"\"\"Atomically load the current value.\"\"\"\n        # TODO: In production, implement lock-free atomic operations\n        # using ctypes and platform-specific atomic instructions\n        with self._lock:\n            return self._value\n    \n    def store(self, new_value: Any, ordering: MemoryOrdering = MemoryOrdering.SEQ_CST):\n        \"\"\"Atomically store a new value.\"\"\"\n        with self._lock:\n            self._value = new_value\n    \n    def compare_and_swap(self, expected: Any, new_value: Any) -> Tuple[bool, Any]:\n        \"\"\"Atomically compare and swap. Returns (success, observed_value).\"\"\"\n        with self._lock:\n            current = self._value\n            if current == expected:\n                self._value = new_value\n                return (True, current)\n            else:\n                return (False, current)\n```\n\n#### Core Logic Skeleton Code\n\n**HazardPointer Class** (implement the TODOs):\n\n```python\nclass HazardPointer:\n    \"\"\"Main hazard pointer system managing protection and reclamation.\"\"\"\n    \n    def __init__(self, max_hazards_per_thread: int = 4):\n        self.thread_registry = ThreadRegistry(max_hazards_per_thread)\n        self.max_hazards_per_thread = max_hazards_per_thread\n    \n    def protect(self, pointer: Any) -> Optional[int]:\n        \"\"\"\n        Protect a pointer from reclamation.\n        Returns slot index if successful, None if no slots available.\n        \"\"\"\n        # TODO 1: Get current thread's record from registry\n        # TODO 2: Find an available hazard slot (one that contains None)\n        # TODO 3: Store the pointer in the slot with SEQ_CST ordering\n        # TODO 4: Return the slot index for later release\n        # Hint: If no slots available, return None to indicate failure\n        pass\n    \n    def release(self, slot_index: int):\n        \"\"\"Release protection from the specified slot.\"\"\"\n        # TODO 1: Get current thread's record from registry\n        # TODO 2: Verify slot_index is valid (0 <= slot_index < max_hazards)\n        # TODO 3: Clear the hazard slot by storing None with SEQ_CST ordering\n        # Hint: Always use SEQ_CST to ensure immediate global visibility\n        pass\n    \n    def protect_and_verify(self, atomic_ref: AtomicReference) -> Tuple[Any, Optional[int]]:\n        \"\"\"\n        Load a pointer and protect it, verifying it hasn't changed.\n        Returns (pointer, slot_index) or (None, None) if protection failed.\n        \"\"\"\n        # TODO 1: Load the current value from atomic_ref with ACQUIRE ordering\n        # TODO 2: If value is None, return (None, None) immediately  \n        # TODO 3: Protect the loaded pointer using protect()\n        # TODO 4: Re-load the atomic_ref to verify it hasn't changed\n        # TODO 5: If changed, release protection and return (None, None)\n        # TODO 6: If unchanged, return (pointer, slot_index)\n        # Hint: This implements the protect-then-verify pattern safely\n        pass\n\nclass RetirementList:\n    \"\"\"Per-thread list of retired nodes awaiting reclamation.\"\"\"\n    \n    def __init__(self, scan_threshold: int = 64):\n        self.retired_nodes = []  # Simple list implementation\n        self.scan_threshold = scan_threshold\n        self.hazard_pointer_system = None  # Set by HazardPointer\n    \n    def retire(self, node: Any):\n        \"\"\"Add a node to the retirement list for eventual reclamation.\"\"\"\n        # TODO 1: Add the node to self.retired_nodes\n        # TODO 2: Check if len(retired_nodes) >= self.scan_threshold\n        # TODO 3: If threshold reached, call self.scan_and_reclaim()\n        # Hint: This implements batched scanning for efficiency\n        pass\n    \n    def scan_and_reclaim(self) -> int:\n        \"\"\"\n        Scan all hazard pointers and reclaim unprotected nodes.\n        Returns number of nodes actually reclaimed.\n        \"\"\"\n        # TODO 1: Collect all protected pointers by iterating thread registry\n        # TODO 2: For each thread record, read all hazard slots with ACQUIRE ordering\n        # TODO 3: Create a set of all non-None protected pointers\n        # TODO 4: Iterate through self.retired_nodes\n        # TODO 5: For each retired node, check if it's in the protected set\n        # TODO 6: If not protected, free the node and remove from retired_nodes\n        # TODO 7: If protected, keep it in retired_nodes for next scan\n        # TODO 8: Return count of nodes actually reclaimed\n        # Hint: Use set lookup for O(1) protection checking\n        pass\n```\n\n#### Language-Specific Hints\n\n**Python Implementation Notes:**\n- Use `threading.local()` for efficient thread-local storage access\n- Consider `weakref.WeakSet` for tracking protected pointers to avoid reference cycles\n- Use `ctypes` for true atomic operations, or `threading.Lock` for simplified prototyping\n- Implement `__enter__` and `__exit__` methods on protection guards for automatic cleanup\n\n**Memory Management:**\n- Call `del` explicitly on reclaimed nodes to trigger immediate cleanup\n- Use `gc.collect()` in tests to verify no memory leaks\n- Consider `tracemalloc` module for debugging memory usage patterns\n\n#### Milestone Checkpoint\n\nAfter implementing hazard pointers, verify correct behavior:\n\n**Basic Functionality Test:**\n```python\ndef test_hazard_pointer_protection():\n    hp = HazardPointer()\n    node = Node(\"test_data\", None)\n    \n    # Protect the node\n    slot = hp.protect(node)\n    assert slot is not None\n    \n    # Retire the node\n    retirement = RetirementList()\n    retirement.hazard_pointer_system = hp\n    retirement.retire(node)\n    \n    # Scan should not reclaim protected node\n    reclaimed = retirement.scan_and_reclaim()\n    assert reclaimed == 0\n    assert len(retirement.retired_nodes) == 1\n    \n    # Release protection\n    hp.release(slot)\n    \n    # Now scan should reclaim the node\n    reclaimed = retirement.scan_and_reclaim()\n    assert reclaimed == 1\n    assert len(retirement.retired_nodes) == 0\n```\n\n**Integration Test with Treiber Stack:**\n```python\ndef test_stack_with_hazard_pointers():\n    stack = TreiberStack()\n    hp = HazardPointer()\n    \n    # Push some items\n    stack.push(\"item1\")\n    stack.push(\"item2\")\n    \n    # Pop with hazard pointer protection\n    popped = stack.pop_with_hazard_pointers(hp)\n    assert popped == \"item2\"\n    \n    # Verify stack still has one item\n    assert not stack.is_empty()\n    assert stack.pop_with_hazard_pointers(hp) == \"item1\"\n    assert stack.is_empty()\n```\n\n**Stress Test Command:**\nRun `python -m pytest tests/test_stress.py -v` to verify:\n- Multiple threads can protect/release concurrently without conflicts\n- Retirement and reclamation work correctly under high contention  \n- No memory leaks occur during extended operation\n- Performance remains reasonable under stress\n\n**Expected Output:**\n```\ntest_concurrent_protection ... PASSED\ntest_retirement_under_contention ... PASSED  \ntest_memory_leak_detection ... PASSED\ntest_performance_benchmark ... PASSED (throughput > 1M ops/sec)\n```\n\n**Signs Something Is Wrong:**\n- **Segmentation faults**: Usually indicates use-after-free from insufficient protection\n- **Memory usage growing unbounded**: Retirement lists not being scanned or protections not being released\n- **Threads hanging**: Deadlock in protection slots or registry operations\n- **Performance degradation**: Too frequent scanning or contention on registry\n\n**Debugging Steps:**\n1. Add logging to protection/release operations to verify balance\n2. Monitor retirement list sizes across threads  \n3. Check that thread cleanup happens on exit\n4. Use memory debugging tools to detect use-after-free errors\n\n\n## Lock-free Hash Map\n\n> **Milestone(s):** Milestone 5 (Lock-free Hash Map) - This section implements a concurrent hash map using split-ordered lists with lock-free bucket operations and incremental resizing that maintains performance under high contention.\n\nThe culmination of our lock-free data structure journey leads us to one of the most challenging concurrent data structures: the hash map. Unlike stacks and queues which operate on single points of contention (top pointer or head/tail pointers), hash maps present multiple buckets that can be accessed concurrently, along with the complex challenge of resizing the underlying array while concurrent operations continue. The split-ordered list approach represents an elegant solution that maintains logical hash ordering within a physical linked list structure, enabling incremental resizing without global synchronization.\n\n![Split-Ordered Hash Map Structure](./diagrams/hashmap-structure.svg)\n\nA concurrent hash map must satisfy several demanding requirements simultaneously. It must provide constant-time average case performance for insert, lookup, and delete operations while supporting concurrent access from multiple threads. The hash map must handle dynamic resizing as the load factor increases, migrating entries to a larger bucket array without blocking ongoing operations. Most critically, it must maintain consistency guarantees ensuring that no updates are lost and no phantom reads occur during concurrent modifications.\n\n### Mental Model: Hash Map as a Growing Office Building\n\nThink of our lock-free hash map as a dynamically growing office building where employees (threads) are constantly moving in and out of offices (buckets) while the building is being expanded. Each floor of the building represents a power-of-two size of the hash table, and each office on a floor corresponds to a hash bucket containing a chain of desks (linked list nodes).\n\nIn a traditional locked hash map, expanding the building would require evacuating all employees, closing the entire building, constructing new floors, moving all the furniture, and then reopening. This approach causes significant disruption and blocks all productivity during the expansion process. Workers queue up outside the building, unable to access their offices or complete their tasks.\n\nOur lock-free hash map takes a radically different approach. Instead of closing the building, we use a clever addressing system that allows employees to find their correct offices even while construction is ongoing. Each office has a logical address (the hash value) that remains stable, but the physical location might change as floors are added. When an employee needs to find office number 13, they follow a trail of forwarding addresses that eventually leads them to the correct physical location, whether it's on the old floor or a newly constructed floor.\n\nThe split-ordered list acts like a building-wide directory system where all offices are logically connected in a single continuous chain, even though they're physically distributed across different floors. Sentinel nodes serve as floor markers - permanent signposts that help employees navigate between floors and ensure they don't get lost during construction. When we need to split office 13 into offices 13 and 29 (during resizing), we don't need to move all the furniture immediately. Instead, we update the directory system so employees can find the right office, and the physical reorganization happens incrementally as employees naturally visit their offices.\n\nThis mental model captures the key insight of split-ordered lists: maintaining logical consistency through directory indirection while allowing physical changes to happen incrementally and without blocking concurrent access.\n\n### Split-Ordered List Design\n\nThe split-ordered list represents a fundamental breakthrough in lock-free hash map design, solving the core problem of maintaining hash ordering during concurrent resizing. Traditional hash maps store entries directly in bucket arrays, making resizing a complex operation that requires rehashing all entries simultaneously. The split-ordered approach maintains all hash map entries in a single logical linked list ordered by hash value, while using a bucket array as an index into this list.\n\n> **Decision: Split-Ordered List with Logical Hash Ordering**\n> - **Context**: Hash map resizing traditionally requires rehashing all entries and blocking concurrent operations, creating scalability bottlenecks in high-contention scenarios\n> - **Options Considered**: Direct bucket array with global locks, segment-based locking (Java ConcurrentHashMap style), split-ordered list approach\n> - **Decision**: Implement split-ordered list maintaining logical hash order in a physical linked list structure\n> - **Rationale**: Enables incremental resizing without blocking operations, provides natural lock-free insertion points, and maintains cache locality through linked list traversal\n> - **Consequences**: Slightly higher memory overhead per entry, more complex implementation, but dramatically better scalability and no blocking during resize operations\n\nThe core innovation lies in the **reverse bit ordering** technique used for hash values. Instead of using hash values directly, we reverse the bits of each hash value to determine the logical ordering in the split-ordered list. This reversal ensures that when we split a bucket during resizing, the relative ordering of entries remains consistent. For example, if bucket 2 (binary 010) needs to split, the new bucket will be 6 (binary 110), and the reversed bit ordering ensures entries naturally fall into the correct bucket without complex migration logic.\n\n| Component | Type | Description |\n|-----------|------|-------------|\n| `buckets` | `array of AtomicReference` | Index array where each element points to the first node of a bucket's chain |\n| `size` | `atomic integer` | Current number of entries in the hash map for load factor calculation |\n| `bucket_mask` | `atomic integer` | Current bucket array size minus one, used for hash modulo operations |\n| `resize_in_progress` | `atomic boolean` | Flag indicating whether a resize operation is currently active |\n\nThe split-ordered list maintains two types of nodes: **data nodes** containing actual key-value pairs and **sentinel nodes** that mark bucket boundaries. Sentinel nodes are permanent markers that never get deleted, providing stable reference points during bucket splitting operations. Each sentinel node represents a specific bucket index and contains a special key that sorts correctly in the reverse bit order.\n\n| Node Field | Type | Description |\n|------------|------|-------------|\n| `key` | `Any` | Either user key (data node) or bucket index (sentinel node) |\n| `value` | `Any` | User value for data nodes, null for sentinel nodes |\n| `hash` | `integer` | Reverse bit-ordered hash value for ordering within the list |\n| `next` | `AtomicReference` | Pointer to next node in the split-ordered list |\n| `is_sentinel` | `boolean` | Flag distinguishing sentinel nodes from data nodes |\n\nThe bucket array serves as a sparse index into the split-ordered list, where each bucket entry points to the sentinel node that begins that bucket's logical section of the list. When accessing bucket `i`, we first ensure that bucket `i` has been initialized by checking if `buckets[i]` is non-null. If the bucket hasn't been initialized, we must recursively ensure that the parent bucket `i/2` exists and then create the sentinel node for bucket `i` by splitting from the parent.\n\nThe logical ordering within the split-ordered list follows a specific pattern that ensures correctness during bucket splits. All entries with reverse bit-ordered hash values between sentinel node `i` and sentinel node `j` belong to the bucket range starting at `i`. When bucket `i` splits to create bucket `i + bucket_count`, the new sentinel node is inserted at the appropriate position, and entries naturally segregate based on their reverse bit-ordered hash values.\n\n> The critical insight is that reverse bit ordering ensures that split operations preserve the relative ordering of entries. When bucket 2 splits into buckets 2 and 6, entries that belonged to bucket 2 will have their hash values naturally distribute between the two buckets based on their high-order bits, without requiring any entry movement.\n\n**Bucket Initialization Algorithm:**\n\n1. Calculate the target bucket index using `hash & bucket_mask`\n2. Check if `buckets[target_index]` is already initialized (non-null)\n3. If uninitialized, recursively ensure parent bucket `target_index / 2` exists\n4. Create new sentinel node with reverse bit-ordered hash for `target_index`\n5. Find insertion point in parent bucket's chain using lock-free list insertion\n6. Use `compare_and_swap` to insert sentinel node at correct position\n7. Update `buckets[target_index]` to point to the newly created sentinel node\n8. Return the initialized bucket's sentinel node for subsequent operations\n\nThis initialization protocol ensures that bucket splits happen incrementally and only when needed, avoiding the overhead of pre-initializing all possible buckets while maintaining the invariant that accessing any bucket index will always succeed.\n\n### Incremental Resizing Algorithm\n\nThe incremental resizing algorithm represents one of the most sophisticated aspects of the split-ordered hash map, enabling the hash map to grow dynamically without blocking concurrent operations. Unlike traditional hash maps that require a global rehashing phase, split-ordered lists support incremental migration where individual buckets are split on-demand as they're accessed.\n\nThe resizing process is triggered when the load factor (ratio of entries to buckets) exceeds a predetermined threshold, typically 0.75 or 1.0. However, rather than immediately migrating all entries, the algorithm sets a resize flag and begins the gradual process of bucket splitting. This approach ensures that the cost of resizing is amortized across many operations rather than concentrated in a single expensive phase.\n\n> **Decision: Threshold-Based Incremental Resizing**\n> - **Context**: Hash maps need dynamic capacity adjustment to maintain performance, but global resizing creates blocking operations that violate lock-freedom\n> - **Options Considered**: Global rehashing with temporary blocking, segment-based migration, incremental on-demand splitting\n> - **Decision**: Trigger resize at load factor threshold with incremental on-demand bucket splitting\n> - **Rationale**: Maintains lock-freedom by never blocking operations, amortizes resize cost across many operations, and allows concurrent access during resize\n> - **Consequences**: Temporary performance degradation during resize period, increased memory usage during transition, but preserves scalability guarantees\n\n| Resize State | Bucket Array Size | Description |\n|-------------|-------------------|-------------|\n| `STABLE` | `N` | Normal operation with single bucket array of size N |\n| `GROWING` | `N` and `2N` | Transition state with both old and new arrays active |\n| `MIGRATING` | `2N` | New array active, old array being incrementally retired |\n\nThe resize operation follows a carefully orchestrated sequence that maintains consistency across all concurrent operations:\n\n**Resize Trigger and Initialization:**\n\n1. Monitor the load factor by comparing `size` to `bucket_count`\n2. When load factor exceeds threshold, attempt to acquire resize responsibility using `compare_and_swap` on `resize_in_progress` flag\n3. Allocate new bucket array with double the current capacity\n4. Initialize the new array's first bucket (index 0) with appropriate sentinel node\n5. Atomically update `bucket_mask` to reflect the new array size\n6. Begin incremental migration by updating bucket access paths\n\n**On-Demand Bucket Splitting:**\n\nDuring the resize period, each bucket access triggers potential splitting logic. When a thread accesses bucket `i` and discovers that it maps to an old bucket that should be split, the thread performs the splitting operation before proceeding with its original operation. This approach distributes the migration work across all threads using the hash map, rather than burdening a single thread with the entire migration.\n\nThe splitting process involves creating a new sentinel node for the split bucket and redistributing entries based on their reverse bit-ordered hash values. Entries whose hash values have a specific bit pattern remain in the original bucket, while entries with the complementary bit pattern migrate to the new bucket. The beauty of reverse bit ordering is that this segregation happens naturally without examining each entry individually.\n\n**Split Operation Sequence:**\n\n1. Identify the bucket index `old_bucket` that needs to be split\n2. Calculate the new bucket index as `new_bucket = old_bucket + current_bucket_count`\n3. Create sentinel node for `new_bucket` with appropriate reverse bit-ordered hash\n4. Traverse the split-ordered list starting from `old_bucket` sentinel\n5. For each entry, determine whether it belongs to `old_bucket` or `new_bucket` based on hash bits\n6. Use lock-free list manipulation to move entries to their correct bucket chains\n7. Update `buckets[new_bucket]` to point to the new sentinel node\n8. Continue with the original hash map operation (insert, lookup, or delete)\n\nThe migration process maintains atomicity through careful ordering of operations and the use of helping mechanisms. If multiple threads attempt to split the same bucket simultaneously, the compare-and-swap operations ensure that only one thread succeeds in creating the new sentinel node, while other threads help complete the migration before proceeding.\n\n> The key insight for incremental resizing is that split-ordered lists make bucket splitting a local operation that only affects the specific bucket chain being split, rather than a global operation requiring coordination across the entire hash map.\n\n**Migration Completion Detection:**\n\nThe resize operation completes when all accessed buckets have been split and no threads are actively using the old bucket array. However, detecting completion in a lock-free environment requires careful coordination to avoid premature cleanup of data structures that might still be in use.\n\n| Completion Condition | Check Method | Action |\n|---------------------|--------------|--------|\n| `All accessed buckets split` | Scan bucket array for unsplit entries | Continue migration for remaining buckets |\n| `No threads in old array` | Hazard pointer scan for old array references | Wait for threads to complete operations |\n| `Migration fully complete` | Both conditions satisfied | Deallocate old array and clear resize flag |\n\nThe completion detection leverages hazard pointers to ensure that the old bucket array is not deallocated while threads might still hold references to it. Threads protect array references using hazard pointers, and the resize completion logic scans all hazard pointer slots to verify that no protected references to the old array remain.\n\n### Lock-free Bucket Operations\n\nThe bucket operations (insert, lookup, and delete) represent the core functionality of the hash map and must maintain correctness while operating on the split-ordered list structure. Each operation must handle the complexities of concurrent modification, bucket initialization, and potential resize operations, all while providing linearizable semantics.\n\nThe fundamental challenge in bucket operations stems from the need to traverse linked list chains that may be concurrently modified by other threads. Unlike simple lock-free lists, the split-ordered list requires additional logic to handle sentinel nodes, bucket boundaries, and the interaction between regular operations and resize-triggered bucket splits.\n\n**Hash Map Insert Operation:**\n\nThe insert operation must locate the correct bucket, potentially initialize it if it doesn't exist, traverse the bucket's chain to find the insertion point, and atomically insert the new entry while handling concurrent modifications from other threads.\n\n| Insert Step | Action | Concurrency Considerations |\n|-------------|--------|-----------------------------|\n| `1. Hash calculation` | Compute hash value and reverse bits | Pure function, no concurrency issues |\n| `2. Bucket identification` | Calculate bucket index using mask | May require reading volatile bucket mask during resize |\n| `3. Bucket initialization` | Ensure target bucket exists | May require recursive parent bucket initialization |\n| `4. Insertion point search` | Traverse chain to find position | Must handle concurrent insertions and deletions |\n| `5. Atomic insertion` | CAS new node into list | May fail due to concurrent modifications, requires retry |\n\nThe insert operation begins by computing the hash value of the key and applying reverse bit ordering to determine its position in the split-ordered list. The bucket index is calculated using the current `bucket_mask`, which may change during resize operations. To handle this correctly, the operation must be prepared to retry if the mask changes during execution.\n\n```\nInsert Algorithm Detailed Steps:\n1. Calculate reverse bit-ordered hash value for the key\n2. Read current bucket_mask atomically to determine bucket index\n3. Call ensure_bucket_exists(bucket_index) to initialize bucket if necessary\n4. Load sentinel node pointer from buckets[bucket_index] with acquire ordering\n5. Traverse split-ordered list starting from sentinel node\n6. Compare each node's hash with target hash to find insertion point\n7. Handle three cases: key exists (update value), insert before current node, insert at end\n8. Create new node with key, value, and appropriate hash ordering\n9. Use compare_and_swap to atomically link new node into chain\n10. If CAS fails due to concurrent modification, retry from step 5\n11. On successful insertion, increment global size counter\n12. Check if load factor threshold exceeded and trigger resize if necessary\n```\n\nThe traversal logic must carefully handle both data nodes and sentinel nodes, ensuring that the insertion respects the reverse bit ordering invariant. When a matching key is found, the operation becomes an update rather than an insertion, requiring a different CAS pattern to atomically replace the value field.\n\n**Hash Map Lookup Operation:**\n\nThe lookup operation provides the simplest case for bucket operations, requiring only traversal of the appropriate bucket chain without any structural modifications. However, it must still handle concurrent modifications and bucket initialization correctly.\n\nThe lookup begins with the same hash calculation and bucket identification as insert, followed by a traversal of the bucket's chain looking for a matching key. The operation must be careful to handle concurrent deletions that might remove nodes during traversal, using appropriate memory ordering to ensure visibility of updates.\n\n```\nLookup Algorithm Detailed Steps:\n1. Calculate reverse bit-ordered hash value for the key\n2. Read current bucket_mask atomically to determine bucket index\n3. Call ensure_bucket_exists(bucket_index) to initialize bucket if necessary\n4. Load sentinel node pointer from buckets[bucket_index] with acquire ordering\n5. Traverse split-ordered list comparing keys for exact match\n6. Skip sentinel nodes during traversal (check is_sentinel flag)\n7. If matching key found, return associated value with success indicator\n8. If traversal reaches next bucket's sentinel or end of list, return not found\n9. Handle concurrent deletions by restarting traversal if inconsistent state detected\n```\n\nThe lookup operation benefits from not requiring any CAS operations, making it naturally wait-free once the bucket initialization is complete. The only potential blocking point occurs during bucket initialization, which may require recursive initialization of parent buckets.\n\n**Hash Map Delete Operation:**\n\nThe delete operation presents the most complex case for bucket operations, requiring atomic removal of nodes from the linked list while handling concurrent access from other threads. The split-ordered list uses a two-phase deletion protocol: logical deletion followed by physical deletion.\n\n| Deletion Phase | Action | Purpose |\n|----------------|--------|---------|\n| `Logical deletion` | Mark node as deleted using CAS | Prevents concurrent operations from seeing deleted entry |\n| `Physical deletion` | Unlink node from list using CAS | Reclaims memory and maintains list structure |\n\nThe logical deletion phase marks the target node as deleted by setting a deletion flag, preventing other operations from accessing the node's value while maintaining the list structure temporarily. This approach ensures that concurrent traversals don't encounter partially deleted nodes that could lead to inconsistent states.\n\n```\nDelete Algorithm Detailed Steps:\n1. Calculate reverse bit-ordered hash value for the key\n2. Read current bucket_mask atomically to determine bucket index\n3. Call ensure_bucket_exists(bucket_index) to initialize bucket if necessary\n4. Load sentinel node pointer from buckets[bucket_index] with acquire ordering\n5. Traverse split-ordered list to find node with matching key\n6. If key not found, return not-found indicator\n7. Attempt logical deletion by CAS on node's deletion flag\n8. If logical deletion succeeds, attempt physical deletion by updating predecessor's next pointer\n9. If physical deletion fails, leave for subsequent operations to complete\n10. Decrement global size counter to reflect removal\n11. Return success indicator with deleted value\n```\n\nThe delete operation leverages helping mechanisms where subsequent operations assist in completing physical deletions that may have been interrupted. When a thread encounters a logically deleted node during traversal, it attempts to complete the physical deletion before continuing with its own operation.\n\n> The two-phase deletion protocol ensures that deleted entries become immediately invisible to new operations (logical deletion) while allowing the physical cleanup to happen asynchronously without blocking subsequent operations.\n\n**Helping Mechanisms in Bucket Operations:**\n\nAll bucket operations implement helping mechanisms to ensure progress guarantees in the presence of concurrent modifications. When an operation encounters evidence of incomplete work by other threads (such as logically deleted nodes or partially completed bucket splits), it assists in completing the work before proceeding.\n\n| Helping Scenario | Detection | Assistance Action |\n|------------------|-----------|-------------------|\n| `Incomplete bucket split` | Accessing bucket with missing sentinel | Complete sentinel node creation and linking |\n| `Incomplete physical deletion` | Traversing logically deleted node | Attempt to unlink node from predecessor |\n| `Stalled resize operation` | Load factor exceeds threshold significantly | Assist with bucket splitting for accessed buckets |\n\nThe helping mechanisms are implemented using idempotent operations that can be safely executed multiple times without causing correctness violations. Multiple threads may attempt the same helping action simultaneously, with CAS operations ensuring that only one thread succeeds while others safely fail and continue.\n\n### Common Hash Map Implementation Pitfalls\n\nLock-free hash map implementation presents numerous subtle pitfalls that can lead to correctness violations, performance degradation, or system instability. Understanding these common mistakes and their prevention strategies is crucial for successful implementation.\n\n⚠️ **Pitfall: Incorrect Bucket Initialization Order**\n\nMany implementations fail to properly handle the recursive nature of bucket initialization, leading to deadlocks or inconsistent bucket states. The split-ordered list requires that parent buckets be initialized before child buckets, but naive implementations may attempt to initialize buckets in arbitrary order.\n\nThe problem manifests when thread A tries to initialize bucket 6 while thread B tries to initialize bucket 2 (the parent of bucket 6). If both threads attempt to initialize their target buckets simultaneously without proper ordering, they may create inconsistent sentinel nodes or miss the parent-child relationship entirely.\n\n**Prevention Strategy**: Always implement recursive parent initialization with proper termination conditions. Use a canonical bucket initialization algorithm that ensures parents exist before creating children, and use atomic flags to prevent duplicate initialization work.\n\n⚠️ **Pitfall: Reverse Bit Calculation Errors**\n\nThe reverse bit ordering calculation is fundamental to split-ordered list correctness, but it's easy to implement incorrectly. Common errors include reversing the wrong number of bits, using signed integer arithmetic that introduces sign extension, or failing to account for the current bucket array size in the reversal calculation.\n\nIncorrect reverse bit calculation causes entries to sort incorrectly in the split-ordered list, leading to entries appearing in wrong buckets after split operations. This manifests as keys that can be inserted but not found, or entries that appear to migrate randomly between buckets.\n\n**Prevention Strategy**: Implement reverse bit calculation as a separate, thoroughly tested utility function. Use explicit bit manipulation with unsigned integers, and include comprehensive unit tests that verify correct ordering for various hash values and bucket array sizes.\n\n⚠️ **Pitfall: Race Conditions in Concurrent Resize Operations**\n\nMultiple threads may simultaneously detect that a resize operation is needed, leading to conflicting resize attempts or inconsistent bucket array states. The problem is exacerbated when threads continue using old bucket masks after resize operations have begun.\n\nThis manifests as entries being inserted into the wrong buckets, lookup operations failing for recently inserted keys, or crashes when accessing deallocated bucket arrays. The issue is particularly subtle because it may only occur under high concurrency and specific timing conditions.\n\n**Prevention Strategy**: Use atomic compare-and-swap on a resize flag to ensure only one thread initiates resize operations. Implement proper memory barriers to ensure that bucket mask updates are visible to all threads before they begin using the new mask. Use hazard pointers to protect bucket array references during resize periods.\n\n⚠️ **Pitfall: Contention Hotspots on Popular Buckets**\n\nHash maps with skewed key distributions can create severe contention hotspots where multiple threads compete for access to the same bucket chains. This leads to excessive CAS failures, poor cache performance, and scalability degradation that defeats the purpose of lock-free design.\n\nThe problem is particularly severe when hash functions produce clustered outputs or when application access patterns naturally focus on specific key ranges. Long bucket chains require linear traversal, creating extended critical sections that increase the probability of concurrent modifications and CAS failures.\n\n**Prevention Strategy**: Implement adaptive load factor thresholds that trigger resize operations earlier for heavily contended buckets. Consider using more sophisticated hash functions that provide better distribution characteristics. Monitor bucket chain lengths and implement chain length limits that force bucket splitting regardless of global load factor.\n\n⚠️ **Pitfall: Memory Leaks During Failed CAS Operations**\n\nLock-free operations that allocate nodes speculatively may fail to insert them due to CAS failures, leading to memory leaks if the allocated nodes are not properly managed. This is particularly problematic in insert operations that create new nodes before confirming they can be successfully inserted.\n\nThe issue compounds over time as failed operations accumulate leaked nodes, eventually leading to memory exhaustion. The problem is often masked in testing because it requires sustained high-concurrency workloads to become apparent.\n\n**Prevention Strategy**: Implement proper cleanup logic for failed CAS operations, ensuring that allocated nodes are either successfully inserted or properly deallocated. Consider using memory pools or object recycling to reduce allocation overhead and simplify cleanup logic. Use tools like valgrind or AddressSanitizer to detect memory leaks during testing.\n\n⚠️ **Pitfall: ABA Problems in Bucket Chain Updates**\n\nThe classic ABA problem can occur in hash map bucket chains when nodes are removed and later reused with the same memory addresses. This can cause CAS operations to succeed incorrectly when they should fail, leading to corrupted list structures or lost entries.\n\nWhile less common than in simple stacks, the ABA problem can still affect hash map operations, particularly when combined with aggressive memory reclamation schemes that quickly reuse deallocated nodes. The problem is exacerbated in environments with memory pressure where allocators tend to reuse recently freed memory.\n\n**Prevention Strategy**: Use hazard pointers for memory reclamation to prevent premature node reuse. Consider implementing tagged pointers that include version counters to detect ABA conditions. Ensure that memory reclamation schemes provide sufficient delay between deallocation and reuse to minimize ABA probability.\n\n### Implementation Guidance\n\nThe lock-free hash map implementation requires careful attention to several critical components working together seamlessly. This section provides practical guidance for implementing a production-ready split-ordered hash map using Python, with complete infrastructure code and detailed implementation skeletons.\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Atomic Operations | `threading.Lock` with careful protocols | Custom atomic wrapper with memory ordering |\n| Hash Function | `hash()` built-in with bit manipulation | MurmurHash3 or SipHash implementation |\n| Memory Management | Python garbage collector | Manual hazard pointer integration |\n| Performance Monitoring | Simple counters | Detailed metrics with bucket utilization |\n\n**Recommended File Structure:**\n\n```\nlock_free_hashmap/\n    __init__.py                    ← Public API exports\n    atomic_ops.py                  ← Atomic operation wrappers (from previous milestones)\n    split_ordered_list.py          ← Core split-ordered list implementation\n    hashmap.py                     ← Main SplitOrderedHashMap class\n    hazard_pointers.py             ← Memory reclamation (from Milestone 4)\n    test_hashmap.py                ← Comprehensive test suite\n    benchmark_hashmap.py           ← Performance comparison benchmarks\n    examples/\n        basic_usage.py             ← Simple usage examples\n        concurrent_stress.py       ← High-concurrency stress testing\n```\n\n**Infrastructure Starter Code:**\n\n```python\n# reverse_bits.py - Complete utility for reverse bit ordering\nimport threading\nfrom typing import Optional\n\ndef reverse_bits(value: int, bit_count: int) -> int:\n    \"\"\"\n    Reverse the lower bit_count bits of value.\n    This is critical for split-ordered list correctness.\n    \n    Args:\n        value: Integer value to reverse bits for\n        bit_count: Number of bits to consider (log2 of bucket array size)\n    \n    Returns:\n        Value with reversed bit pattern in lower bit_count bits\n    \"\"\"\n    result = 0\n    for i in range(bit_count):\n        if value & (1 << i):\n            result |= (1 << (bit_count - 1 - i))\n    return result\n\ndef calculate_bucket_index(hash_value: int, bucket_mask: int) -> int:\n    \"\"\"Calculate bucket index using current mask.\"\"\"\n    return hash_value & bucket_mask\n\ndef calculate_parent_bucket(bucket_index: int) -> int:\n    \"\"\"Calculate parent bucket for initialization hierarchy.\"\"\"\n    if bucket_index == 0:\n        return 0\n    # Find highest set bit and clear it to get parent\n    highest_bit = bucket_index.bit_length() - 1\n    return bucket_index & ~(1 << highest_bit)\n\nclass BucketNode:\n    \"\"\"Node in the split-ordered list with atomic next pointer.\"\"\"\n    \n    def __init__(self, key, value, hash_value: int, is_sentinel: bool = False):\n        self.key = key\n        self.value = value\n        self.hash = hash_value\n        self.is_sentinel = is_sentinel\n        self.next = AtomicReference(None)\n        self.deleted = AtomicReference(False)\n        self.lock = threading.RLock()  # For fine-grained locking fallback\n    \n    def is_deleted(self) -> bool:\n        \"\"\"Check if node is logically deleted.\"\"\"\n        return self.deleted.load(RELAXED)\n    \n    def mark_deleted(self) -> bool:\n        \"\"\"Attempt to mark node as logically deleted.\"\"\"\n        return self.deleted.compare_and_swap(False, True)\n\nclass BucketInitializer:\n    \"\"\"Handles recursive bucket initialization with cycle prevention.\"\"\"\n    \n    def __init__(self, buckets, max_bucket_count: int):\n        self.buckets = buckets\n        self.max_bucket_count = max_bucket_count\n        self.initializing = set()  # Track buckets being initialized\n        self.init_lock = threading.Lock()\n    \n    def ensure_bucket_exists(self, bucket_index: int) -> Optional[BucketNode]:\n        \"\"\"\n        Ensure bucket exists, creating parent buckets recursively if needed.\n        Returns the sentinel node for the bucket.\n        \"\"\"\n        if bucket_index >= self.max_bucket_count:\n            return None\n            \n        # Fast path: bucket already exists\n        sentinel = self.buckets[bucket_index].load(ACQUIRE)\n        if sentinel is not None:\n            return sentinel\n            \n        # Slow path: need to initialize bucket\n        with self.init_lock:\n            # Double-check after acquiring lock\n            sentinel = self.buckets[bucket_index].load(ACQUIRE)\n            if sentinel is not None:\n                return sentinel\n                \n            # Prevent initialization cycles\n            if bucket_index in self.initializing:\n                raise RuntimeError(f\"Cycle detected in bucket initialization: {bucket_index}\")\n                \n            try:\n                self.initializing.add(bucket_index)\n                return self._initialize_bucket(bucket_index)\n            finally:\n                self.initializing.discard(bucket_index)\n    \n    def _initialize_bucket(self, bucket_index: int) -> BucketNode:\n        \"\"\"Initialize a single bucket after ensuring parent exists.\"\"\"\n        if bucket_index == 0:\n            # Root bucket - create sentinel node\n            sentinel = BucketNode(\n                key=bucket_index,\n                value=None,\n                hash_value=reverse_bits(bucket_index, 32),\n                is_sentinel=True\n            )\n            self.buckets[0].store(sentinel, RELEASE)\n            return sentinel\n        \n        # Ensure parent bucket exists first\n        parent_index = calculate_parent_bucket(bucket_index)\n        parent_sentinel = self.ensure_bucket_exists(parent_index)\n        \n        # Create sentinel node for this bucket\n        sentinel = BucketNode(\n            key=bucket_index,\n            value=None,\n            hash_value=reverse_bits(bucket_index, 32),\n            is_sentinel=True\n        )\n        \n        # Insert sentinel into parent's chain at correct position\n        self._insert_sentinel_into_chain(parent_sentinel, sentinel)\n        \n        # Atomically publish bucket\n        self.buckets[bucket_index].store(sentinel, RELEASE)\n        return sentinel\n    \n    def _insert_sentinel_into_chain(self, parent_sentinel: BucketNode, new_sentinel: BucketNode):\n        \"\"\"Insert new sentinel into the split-ordered list at correct position.\"\"\"\n        current = parent_sentinel\n        target_hash = new_sentinel.hash\n        \n        while True:\n            next_node = current.next.load(ACQUIRE)\n            \n            if next_node is None or next_node.hash > target_hash:\n                # Found insertion point\n                new_sentinel.next.store(next_node, RELAXED)\n                if current.next.compare_and_swap(next_node, new_sentinel):\n                    break\n                # CAS failed, retry\n                continue\n            \n            current = next_node\n```\n\n**Core Logic Skeleton Code:**\n\n```python\n# hashmap.py - Main implementation skeleton\nfrom typing import Any, Optional, Tuple, Dict\nimport threading\nfrom atomic_ops import AtomicReference, RELAXED, ACQUIRE, RELEASE\nfrom reverse_bits import reverse_bits, BucketNode, BucketInitializer\n\nclass SplitOrderedHashMap:\n    \"\"\"\n    Lock-free hash map using split-ordered lists with incremental resizing.\n    Provides concurrent insert, lookup, and delete operations.\n    \"\"\"\n    \n    def __init__(self, initial_capacity: int = 16, load_factor_threshold: float = 0.75):\n        self.initial_capacity = max(2, initial_capacity)  # Ensure power of 2\n        self.load_factor_threshold = load_factor_threshold\n        \n        # Core data structures\n        self.buckets = [AtomicReference(None) for _ in range(self.initial_capacity)]\n        self.size = AtomicReference(0)\n        self.bucket_mask = AtomicReference(self.initial_capacity - 1)\n        self.resize_in_progress = AtomicReference(False)\n        \n        # Helper components\n        self.bucket_initializer = BucketInitializer(self.buckets, len(self.buckets))\n        \n        # Initialize bucket 0\n        self.bucket_initializer.ensure_bucket_exists(0)\n    \n    def insert(self, key: Any, value: Any) -> bool:\n        \"\"\"\n        Insert key-value pair into hash map.\n        Returns True if new entry created, False if existing key updated.\n        \"\"\"\n        # TODO 1: Calculate hash value for key using built-in hash function\n        # TODO 2: Apply reverse bit ordering to hash for split-ordered list placement\n        # TODO 3: Determine target bucket using current bucket_mask (handle concurrent resize)\n        # TODO 4: Ensure target bucket exists using bucket_initializer.ensure_bucket_exists\n        # TODO 5: Traverse bucket chain starting from sentinel node to find insertion point\n        # TODO 6: Handle three cases: key exists (update), insert before current node, insert at end\n        # TODO 7: Create new BucketNode with key, value, and reverse bit-ordered hash\n        # TODO 8: Use compare_and_swap to atomically link new node into chain\n        # TODO 9: On CAS failure due to concurrent modification, retry from traversal step\n        # TODO 10: On successful insertion, increment size counter atomically\n        # TODO 11: Check if load factor exceeds threshold and trigger resize if needed\n        # TODO 12: Return appropriate boolean indicating new entry vs. update\n        pass\n    \n    def lookup(self, key: Any) -> Optional[Any]:\n        \"\"\"\n        Lookup value for given key.\n        Returns value if found, None if key doesn't exist.\n        \"\"\"\n        # TODO 1: Calculate hash value and apply reverse bit ordering\n        # TODO 2: Determine target bucket using current bucket_mask\n        # TODO 3: Ensure target bucket exists using bucket_initializer\n        # TODO 4: Load sentinel node pointer with acquire memory ordering\n        # TODO 5: Traverse split-ordered list comparing keys for exact match\n        # TODO 6: Skip sentinel nodes during traversal (check is_sentinel flag)\n        # TODO 7: Skip logically deleted nodes (check node.is_deleted())\n        # TODO 8: If matching key found, return associated value\n        # TODO 9: If traversal reaches next bucket's sentinel or end, return None\n        # TODO 10: Handle concurrent modifications by restarting if inconsistent state detected\n        pass\n    \n    def delete(self, key: Any) -> Optional[Any]:\n        \"\"\"\n        Delete key from hash map.\n        Returns deleted value if found, None if key didn't exist.\n        \"\"\"\n        # TODO 1: Calculate hash value and apply reverse bit ordering  \n        # TODO 2: Determine target bucket using current bucket_mask\n        # TODO 3: Ensure target bucket exists using bucket_initializer\n        # TODO 4: Traverse split-ordered list to find node with matching key\n        # TODO 5: If key not found, return None\n        # TODO 6: Attempt logical deletion by marking node.mark_deleted()\n        # TODO 7: If logical deletion succeeds, attempt physical deletion\n        # TODO 8: Physical deletion: update predecessor's next pointer to skip deleted node\n        # TODO 9: If physical deletion fails, leave for subsequent operations to complete\n        # TODO 10: Decrement global size counter to reflect removal\n        # TODO 11: Return deleted value on success\n        # TODO 12: Implement helping: assist with incomplete physical deletions during traversal\n        pass\n    \n    def _trigger_resize_if_needed(self) -> None:\n        \"\"\"\n        Check load factor and trigger resize operation if threshold exceeded.\n        Uses compare_and_swap to ensure only one thread initiates resize.\n        \"\"\"\n        # TODO 1: Read current size and bucket count atomically\n        # TODO 2: Calculate current load factor (size / bucket_count)\n        # TODO 3: If load factor below threshold, return early\n        # TODO 4: Attempt to acquire resize responsibility using CAS on resize_in_progress flag\n        # TODO 5: If CAS fails, another thread is handling resize, return\n        # TODO 6: Allocate new bucket array with double the current capacity\n        # TODO 7: Initialize new array's bucket 0 with appropriate sentinel node\n        # TODO 8: Atomically update bucket_mask to reflect new array size\n        # TODO 9: Copy buckets reference to new array (incremental migration will handle entries)\n        # TODO 10: Clear resize_in_progress flag when initialization complete\n        pass\n    \n    def _split_bucket_if_needed(self, bucket_index: int) -> None:\n        \"\"\"\n        Split bucket during resize operation if it hasn't been split yet.\n        Redistributes entries between old and new bucket based on hash bits.\n        \"\"\"\n        # TODO 1: Check if resize is in progress, return if not\n        # TODO 2: Calculate new bucket index (old_index + current_bucket_count)\n        # TODO 3: Check if new bucket already exists, return if so\n        # TODO 4: Create sentinel node for new bucket with correct reverse bit-ordered hash\n        # TODO 5: Traverse entries in old bucket chain starting from sentinel\n        # TODO 6: For each data node, determine target bucket based on hash bit pattern\n        # TODO 7: Use atomic list manipulation to move entries to correct bucket chains\n        # TODO 8: Handle concurrent modifications by retrying operations\n        # TODO 9: Update buckets array to point to new bucket's sentinel node\n        # TODO 10: Ensure all moved entries maintain correct ordering within their new chains\n        pass\n    \n    def _traverse_bucket_chain(self, start_node: BucketNode, target_hash: int) -> Tuple[BucketNode, BucketNode]:\n        \"\"\"\n        Traverse bucket chain to find insertion point or matching key.\n        Returns (predecessor, current) node pair for the target position.\n        Handles concurrent modifications and assists with incomplete deletions.\n        \"\"\"\n        # TODO 1: Start traversal from given start_node\n        # TODO 2: Follow next pointers while current.hash < target_hash\n        # TODO 3: Skip sentinel nodes when looking for data entries\n        # TODO 4: Help complete physical deletion for logically deleted nodes encountered\n        # TODO 5: Handle race conditions where nodes are deleted during traversal\n        # TODO 6: Use acquire memory ordering when loading next pointers\n        # TODO 7: Return predecessor and current node for target hash position\n        # TODO 8: Ensure returned nodes are stable (not concurrently deleted)\n        pass\n    \n    def size_hint(self) -> int:\n        \"\"\"Return approximate number of entries in hash map.\"\"\"\n        return self.size.load(RELAXED)\n    \n    def is_empty(self) -> bool:\n        \"\"\"Check if hash map contains any entries.\"\"\"\n        return self.size_hint() == 0\n```\n\n**Milestone Checkpoint:**\n\nAfter implementing the split-ordered hash map, verify correctness with these checks:\n\n1. **Single-threaded correctness**: Insert 1000 key-value pairs, verify all can be looked up, delete half, verify lookups return correct results\n2. **Concurrent insertion**: Use 10 threads inserting 100 unique keys each, verify final size is 1000 and all keys are present\n3. **Concurrent mixed operations**: Use multiple threads performing random insert/lookup/delete operations, verify consistency\n4. **Resize behavior**: Monitor bucket array size during heavy insertion, verify it doubles when load factor threshold exceeded\n5. **Memory safety**: Run with memory debugging tools to ensure no leaks or use-after-free errors\n\nExpected behavior indicators:\n- Insert operations should have O(1) average time complexity\n- Concurrent operations should not block each other\n- Resize operations should happen transparently without affecting correctness\n- Performance should scale well with increased thread count (up to core count)\n\n**Debugging Tips:**\n\n| Symptom | Likely Cause | Diagnosis Method | Fix |\n|---------|--------------|------------------|-----|\n| Keys inserted but not found | Incorrect reverse bit calculation | Print hash values and bucket indices | Verify reverse_bits function with unit tests |\n| Crashes during resize | Race condition in bucket array access | Add logging to resize operations | Implement proper memory barriers and hazard pointers |\n| Poor performance under contention | Excessive CAS failures in hot buckets | Profile bucket chain lengths | Implement adaptive load factor or better hash function |\n| Memory leaks | Failed CAS operations not cleaning up nodes | Run with valgrind or similar tool | Add proper cleanup in all CAS failure paths |\n| Inconsistent size counter | Race conditions in increment/decrement | Compare actual entries with size counter | Use atomic operations for all size updates |\n\n\n## Interactions and Data Flow\n\n> **Milestone(s):** All milestones (this section describes how components from atomic operations through complete data structures communicate and coordinate with each other)\n\nUnderstanding how lock-free components interact requires thinking beyond traditional method calls and return values. Lock-free programming introduces unique communication patterns where threads coordinate through shared memory locations, helping protocols, and deferred cleanup mechanisms. This section explores the intricate dance of atomic operations, data structure manipulations, and memory reclamation that enables high-performance concurrent computing without traditional locks.\n\n### Mental Model: The Relay Race Coordination\n\nThink of lock-free interactions like a relay race where runners hand off batons while sprinting at full speed. Unlike a traditional relay where runners stop to carefully exchange the baton (analogous to acquiring a lock), lock-free coordination requires runners to coordinate the handoff while maintaining their pace. The baton represents shared data, and the handoff represents atomic operations that must succeed even when multiple runners approach the exchange zone simultaneously.\n\nIn this analogy, compare-and-swap operations are like attempting to grab or place the baton only if it's in the expected position. If another runner has already moved it, the attempt fails and must be retried. Hazard pointers are like safety flags that runners wave to signal \"I'm still using this lane\" before the track maintenance crew can clean up behind them. The helping mechanism is like runners assisting teammates who stumble during their handoff attempt.\n\nThis mental model captures the essential challenge: coordination must happen through observation and atomic updates rather than explicit communication, and every participant must be prepared for their operations to fail due to concurrent activity.\n\n### Operation Sequence Patterns\n\nLock-free algorithms rely on several fundamental interaction patterns that appear repeatedly across all data structures. Understanding these patterns is crucial because they represent the core coordination mechanisms that replace traditional locking.\n\n#### CAS Retry Loop Pattern\n\nThe compare-and-swap retry loop forms the foundation of all lock-free operations. This pattern handles the fundamental challenge that any shared memory location might be modified by concurrent threads between the time it's observed and the time an update is attempted.\n\n| Pattern Step | Action | Purpose | Failure Response |\n|--------------|--------|---------|------------------|\n| Load Current Value | `current = atomic_ref.load(ACQUIRE)` | Observe current state | N/A (load cannot fail) |\n| Compute New Value | `new_value = transform_function(current)` | Calculate desired update | N/A (pure computation) |\n| Attempt Atomic Update | `success = atomic_ref.compare_and_swap(current, new_value)` | Apply change if state unchanged | Retry from load step |\n| Handle Success | Process operation completion | Finalize and return result | N/A |\n| Handle Failure | Backoff and retry or abort | Manage contention | Exponential backoff delay |\n\nThe `cas_retry_loop` function encapsulates this pattern and provides a reusable foundation for all lock-free operations. The function accepts an atomic reference, an update function that computes the new value from the current value, and a maximum retry count to prevent infinite spinning under extreme contention.\n\n> **Decision: Exponential Backoff in CAS Retry Loops**\n> - **Context**: High contention can cause CAS operations to fail repeatedly, leading to excessive CPU usage from spinning threads\n> - **Options Considered**: No backoff (immediate retry), fixed delay, exponential backoff with random jitter\n> - **Decision**: Exponential backoff with random jitter starting at 1 microsecond\n> - **Rationale**: Reduces contention by spreading retry attempts across time while maintaining low latency for success cases\n> - **Consequences**: Improves throughput under contention but adds complexity to retry logic and worst-case latency\n\n#### Helping Protocol Pattern\n\nMany lock-free algorithms employ helping mechanisms where threads assist each other in completing operations that may have been interrupted by context switches or delays. This pattern is essential for maintaining progress guarantees when individual threads may be delayed arbitrarily.\n\n| Helper Role | Observation | Helping Action | Completion Condition |\n|-------------|-------------|----------------|---------------------|\n| Observer | Detects incomplete operation | Attempts to complete on behalf of original thread | Operation reaches consistent state |\n| Original Thread | Returns from delay | Verifies operation completion | Either self-completed or helper completed |\n| Conflict Resolution | Multiple helpers | First successful CAS wins | Losing helpers abort gracefully |\n\nThe Michael-Scott queue demonstrates helping protocols clearly in the `enqueue` operation. When a thread observes that the tail pointer lags behind the actual end of the queue, it attempts to advance the tail pointer even though it didn't create the situation. This helping behavior ensures that subsequent enqueue operations can proceed even if the thread that originally extended the queue was delayed before updating the tail pointer.\n\n#### Hazard Pointer Integration Pattern\n\nLock-free data structures must integrate carefully with memory reclamation systems to prevent use-after-free errors while maintaining non-blocking progress guarantees. The hazard pointer integration pattern standardizes this interaction.\n\n| Protection Phase | Thread Action | Global Effect | Memory State |\n|------------------|---------------|---------------|--------------|\n| Pre-Access | `protect(pointer)` | Announces intent to access | Node marked as protected |\n| Verification | `verify(atomic_ref)` | Confirms pointer still valid | Prevents ABA on protected node |\n| Access | Use protected node safely | Guaranteed no reclamation | Node remains allocated |\n| Release | `release(slot_index)` | Clears protection | Node becomes eligible for reclamation |\n\nThe protect-then-verify pattern is critical for preventing race conditions where a node is reclaimed between the time its pointer is loaded and the time it's protected. The verification step ensures that the protected pointer still refers to a valid node that remains reachable through the data structure.\n\n### Component Interface Contracts\n\nThe lock-free system consists of three primary layers with well-defined interface contracts that specify the exact behavior and guarantees provided by each component.\n\n#### Atomic Operations Layer Interface\n\nThe atomic operations layer provides the fundamental building blocks that all higher-level components depend on. These operations must provide specific guarantees about atomicity, memory ordering, and progress.\n\n| Method Signature | Parameters | Returns | Memory Ordering | Progress Guarantee |\n|------------------|------------|---------|-----------------|-------------------|\n| `compare_and_swap(expected, new_value)` | `expected: Any, new_value: Any` | `(success: bool, observed: Any)` | Sequentially consistent | Lock-free (bounded retries) |\n| `load(ordering)` | `ordering: MemoryOrdering` | `current_value: Any` | As specified by ordering | Wait-free (immediate) |\n| `store(value, ordering)` | `value: Any, ordering: MemoryOrdering` | `None` | As specified by ordering | Wait-free (immediate) |\n| `fetch_and_add(increment)` | `increment: int` | `previous_value: int` | Sequentially consistent | Lock-free (hardware atomic) |\n\nThe `compare_and_swap` operation returns both a success indicator and the observed value to enable efficient retry loops. When the operation fails, the observed value provides the current state needed for the next retry attempt without requiring an additional load operation.\n\n> **Decision: Return Observed Value on CAS Failure**\n> - **Context**: Failed CAS operations require knowing the current value to compute the next retry attempt\n> - **Options Considered**: Return boolean only (requires additional load), return tuple with observed value, exception-based failure indication\n> - **Decision**: Return tuple `(success: bool, observed: Any)` for all CAS operations\n> - **Rationale**: Eliminates extra load operations in retry loops, reducing memory traffic and improving performance under contention\n> - **Consequences**: Slightly more complex return value handling but significantly better performance characteristics\n\nMemory ordering parameters control the synchronization guarantees provided by each operation. The atomic layer must respect these constraints to ensure correct behavior on weakly-ordered architectures.\n\n| Memory Ordering | Load Semantics | Store Semantics | Synchronization Effect |\n|-----------------|---------------|------------------|----------------------|\n| `RELAXED` | No ordering constraints | No ordering constraints | Atomic but no synchronization |\n| `ACQUIRE` | Prevents reordering of subsequent reads | N/A | Synchronizes-with release stores |\n| `RELEASE` | N/A | Prevents reordering of prior writes | Provides synchronizes-with for acquire loads |\n| `SEQ_CST` | Global ordering with other seq_cst | Global ordering with other seq_cst | Strongest guarantee, full ordering |\n\n#### Data Structure Layer Interface\n\nThe data structure layer builds on atomic operations to provide familiar collection interfaces with lock-free guarantees. Each data structure must specify its linearizability points and progress guarantees.\n\n| Data Structure | Core Operations | Linearizability Point | Empty Container Behavior |\n|----------------|----------------|----------------------|-------------------------|\n| `TreiberStack` | `push(data)`, `pop()` | CAS on top pointer | `pop()` returns None |\n| `MichaelScottQueue` | `enqueue(data)`, `dequeue()` | CAS on next pointer (enqueue), CAS on head (dequeue) | `dequeue()` returns None |\n| `SplitOrderedHashMap` | `insert(key, value)`, `lookup(key)`, `delete(key)` | CAS on bucket list insertion | Operations return None/False |\n\nStack operations provide the simplest interface contract. The `push` operation always succeeds (assuming memory allocation succeeds), while `pop` returns `None` when called on an empty stack. Both operations are linearizable at the point where the CAS on the top pointer succeeds.\n\n| Method | Parameters | Returns | Preconditions | Postconditions |\n|--------|------------|---------|---------------|----------------|\n| `push(data)` | `data: Any` | `None` | `data` is not None | Stack contains new element at top |\n| `pop()` | None | `Optional[Any]` | None | If stack non-empty, removes and returns top element |\n| `is_empty()` | None | `bool` | None | Returns true iff stack contains no elements |\n| `size_hint()` | None | `int` | None | Returns approximate count (may be stale) |\n\nQueue operations maintain FIFO ordering with separate methods for adding and removing elements. The linearizability guarantee ensures that elements are dequeued in the exact order they were enqueued, even under concurrent access.\n\n| Method | Parameters | Returns | Preconditions | Postconditions |\n|--------|------------|---------|---------------|----------------|\n| `enqueue(data)` | `data: Any` | `None` | `data` is not None | Queue contains new element at tail |\n| `dequeue()` | None | `Optional[Any]` | None | If queue non-empty, removes and returns head element |\n| `is_empty()` | None | `bool` | None | Returns true iff queue contains no elements |\n| `size_hint()` | None | `int` | None | Returns approximate count (may be stale) |\n\nHash map operations provide key-value storage with lock-free guarantees. The interface supports standard dictionary operations while maintaining linearizability for all concurrent operations.\n\n| Method | Parameters | Returns | Preconditions | Postconditions |\n|--------|------------|---------|---------------|----------------|\n| `insert(key, value)` | `key: Any, value: Any` | `bool` | `key` and `value` are not None | Returns true if new entry created |\n| `lookup(key)` | `key: Any` | `Optional[Any]` | `key` is not None | Returns current value for key or None |\n| `delete(key)` | `key: Any` | `Optional[Any]` | `key` is not None | Returns deleted value or None if not found |\n| `size_hint()` | None | `int` | None | Returns approximate count of key-value pairs |\n\n#### Memory Reclamation Layer Interface\n\nThe hazard pointer system provides memory reclamation services that integrate seamlessly with lock-free data structures. The interface must be simple enough to use correctly while providing strong safety guarantees.\n\n| Method | Parameters | Returns | Thread Safety | Performance Guarantee |\n|--------|------------|---------|---------------|---------------------|\n| `protect(pointer)` | `pointer: Any` | `slot_index: int` | Thread-local operation | Wait-free |\n| `release(slot_index)` | `slot_index: int` | `None` | Thread-local operation | Wait-free |\n| `retire(node)` | `node: Node` | `None` | Thread-safe | Lock-free (may trigger scan) |\n| `scan_and_reclaim()` | None | `reclaimed_count: int` | Thread-safe | Lock-free |\n| `protect_and_verify(atomic_ref)` | `atomic_ref: AtomicReference` | `(pointer: Any, slot_index: int)` | Thread-safe | Lock-free |\n\nThe `protect_and_verify` method encapsulates the common protect-then-verify pattern used throughout lock-free data structures. It atomically loads a pointer, protects it with a hazard pointer, and verifies that the pointer remains valid by re-checking the atomic reference.\n\n> **Decision: Separate Protect and Verify vs Combined Operation**\n> - **Context**: Lock-free algorithms require protecting loaded pointers and verifying they remain valid to prevent ABA problems\n> - **Options Considered**: Separate `protect()` and `verify()` calls, combined `protect_and_verify()` operation, automatic protection on load\n> - **Decision**: Provide both separate operations and combined `protect_and_verify()` for common use cases\n> - **Rationale**: Combined operation reduces boilerplate and race condition opportunities while separate operations allow flexibility for complex scenarios\n> - **Consequences**: Simpler API for common cases but requires understanding when to use which variant\n\n### Memory Lifecycle and Flow\n\nMemory management in lock-free systems follows a complex lifecycle that balances performance with safety. Understanding this flow is essential for correctly implementing and debugging lock-free algorithms.\n\n#### Node Allocation and Initial State\n\nNew nodes begin their lifecycle in a straightforward allocated state before entering the complex world of concurrent access and deferred reclamation.\n\n| Allocation Stage | Memory State | Visibility | Protection Status |\n|------------------|--------------|------------|-------------------|\n| `malloc()` or equivalent | Allocated but uninitialized | Thread-private | No hazard protection needed |\n| Constructor initialization | Initialized fields, atomic references set | Thread-private | No hazard protection needed |\n| Publication to data structure | Visible to other threads via CAS | Globally visible | Becomes eligible for hazard protection |\n\nDuring allocation and initialization, nodes are private to the creating thread and require no special coordination. The critical transition occurs when a node becomes visible to other threads through a successful CAS operation that links it into a shared data structure.\n\nThe publication step represents the linearization point where the node becomes part of the shared state. From this moment forward, any thread accessing the data structure might encounter the node and must use proper hazard pointer protection.\n\n#### Concurrent Access and Protection Phase\n\nOnce published, nodes enter a phase where they may be accessed concurrently by multiple threads. The hazard pointer system coordinates this access to prevent use-after-free errors while maintaining lock-free progress guarantees.\n\n| Access Pattern | Protection Requirement | Verification Need | Duration |\n|----------------|----------------------|-------------------|----------|\n| Pointer load from atomic reference | Immediate protection after load | Verify pointer still reachable | Until access complete |\n| Traversal following next pointers | Protect each node before dereferencing | Verify next pointer before following | Per-node basis |\n| Data field access | Node must be protected | Protection implies validity | Until operation complete |\n| Pointer comparison (ABA detection) | Target nodes should be protected | Compare with freshly loaded value | During comparison only |\n\nThe protect-then-verify pattern forms the core of safe concurrent access. A thread first loads a pointer from an atomic reference, immediately protects it with a hazard pointer, then verifies the pointer is still reachable by re-reading the atomic reference and comparing values.\n\n```\nAccess Sequence:\n1. pointer = atomic_ref.load(ACQUIRE)\n2. slot = protect(pointer)  \n3. verification = atomic_ref.load(ACQUIRE)\n4. if pointer != verification: release(slot); retry\n5. // Safe to access pointer->data and pointer->next\n6. release(slot) when access complete\n```\n\nThis sequence handles the race condition where another thread removes and retires a node between steps 1 and 2. The verification in step 4 detects this situation and triggers a retry with the updated pointer value.\n\n#### Logical Deletion and Marking\n\nMany lock-free algorithms separate logical deletion (marking a node as deleted) from physical deletion (unlinking the node from the data structure). This separation simplifies concurrent deletion by providing a stable intermediate state.\n\n| Deletion Phase | Node State | Visibility | Removal Responsibility |\n|----------------|------------|------------|----------------------|\n| Active node | Unmarked, reachable | Normal operations access | N/A |\n| Logical deletion | Marked as deleted | Skipped by operations | Original deleter or helpers |\n| Physical removal | Unlinked from structure | Only via hazard pointers | Any thread |\n| Retirement | Added to retirement list | No new accesses | Memory reclamation system |\n\nThe `mark_deleted()` operation typically uses a CAS to atomically set a deletion flag or tag in the node. Once marked, other operations skip over the node while traversing the data structure, but the node remains physically present until a subsequent operation removes it.\n\nPhysical removal requires careful coordination to avoid breaking concurrent traversals. The removing thread must use CAS operations to atomically unlink the node while ensuring that concurrent traversals can still make progress.\n\n#### Retirement and Deferred Reclamation\n\nWhen a node is removed from a data structure, it cannot be immediately freed because other threads might still hold protected references to it. The retirement system manages this deferred cleanup.\n\n| Retirement Stage | Memory Status | Reclamation Eligibility | Cleanup Action |\n|------------------|---------------|-------------------------|----------------|\n| Active retirement | Added to thread-local retirement list | Not yet eligible | None |\n| Batched retirement | Moved to global retirement queue | Awaiting hazard scan | None |\n| Hazard scanning | All thread hazard pointers checked | Depends on scan results | Mark safe/unsafe for reclamation |\n| Safe reclamation | No hazard pointers reference node | Eligible for free | Call destructor and free memory |\n| Deferred reclamation | Hazard pointer found | Moved to next scan batch | None |\n\nThe `retire(node)` operation adds a node to the current thread's retirement list. When this list reaches a threshold size, the thread triggers a scan of all hazard pointers across all threads to determine which retired nodes are safe to reclaim.\n\n> **Decision: Thread-Local vs Global Retirement Lists**\n> - **Context**: Retired nodes must be collected efficiently while minimizing synchronization overhead during retirement\n> - **Options Considered**: Thread-local lists with periodic batching, single global list with locking, lock-free global queue\n> - **Decision**: Thread-local retirement lists with threshold-triggered global scanning\n> - **Rationale**: Minimizes synchronization during the common retirement operation while enabling efficient batch processing\n> - **Consequences**: Requires thread-local storage management and cleanup on thread exit but provides excellent performance\n\n#### Scan and Reclamation Process\n\nThe hazard pointer scanning process represents the coordination point where the memory reclamation system determines which nodes are safe to free. This process must be efficient enough to not become a bottleneck while being thorough enough to prevent memory leaks.\n\n| Scan Phase | Action | Coordination Required | Result |\n|------------|--------|----------------------|--------|\n| Collect retirement lists | Gather nodes from all threads | Thread-safe list access | Unified retirement set |\n| Scan hazard pointers | Check all active hazard slots | Read from thread-local storage | Set of protected pointers |\n| Compute reclaimable set | Set difference: retired - protected | Pure computation | Nodes safe to free |\n| Reclaim safe nodes | Free memory for reclaimable nodes | None | Memory returned to system |\n| Reschedule unsafe nodes | Move protected nodes to next batch | Update retirement lists | Deferred for future scan |\n\nThe scanning algorithm must handle the race condition where hazard pointers are updated while the scan is in progress. This is typically handled by using acquire-release memory ordering when reading hazard pointer slots and by allowing false positives (keeping nodes that could have been freed) rather than false negatives (freeing nodes that are still protected).\n\n#### Thread Exit Cleanup\n\nWhen threads terminate, they must clean up their hazard pointer slots and process their remaining retirement lists to prevent memory leaks and dangling protection records.\n\n| Cleanup Step | Action | Safety Requirement | Failure Handling |\n|--------------|--------|-------------------|------------------|\n| Mark thread inactive | Set thread record as inactive | Atomic flag update | Best effort, may leak on crash |\n| Clear hazard slots | Set all hazard pointers to null | Release memory ordering | Must complete to prevent false protection |\n| Process retirement list | Reclaim all remaining retired nodes | Safe since thread is exiting | Force reclamation regardless of hazard pointers |\n| Unregister thread record | Remove from global thread registry | Synchronized update | May leave inactive record on crash |\n\nThread exit cleanup is critical for preventing resource leaks in long-running systems where threads are created and destroyed frequently. The cleanup process must be robust against thread termination at arbitrary points while avoiding deadlocks or blocking other threads.\n\n### Implementation Guidance\n\nThe interaction patterns described above translate into specific implementation approaches that balance correctness with performance. This section provides concrete guidance for implementing these patterns in Python, along with infrastructure components and debugging tools.\n\n#### Technology Recommendations\n\nPython's Global Interpreter Lock (GIL) complicates lock-free programming significantly, but several approaches can provide educational value and real concurrency benefits in specific scenarios.\n\n| Component | Simple Option | Advanced Option | Rationale |\n|-----------|---------------|----------------|-----------|\n| Atomic Operations | `threading` module locks with CAS simulation | `ctypes` to expose hardware CAS | Start with simulation for learning, advance to real atomics |\n| Memory Ordering | Memory barriers via `threading.Barrier` | Platform-specific memory fences | Educational barriers first, then performance optimization |\n| Thread Management | Standard `threading.Thread` | `concurrent.futures.ThreadPoolExecutor` | Simplicity vs advanced thread pool features |\n| Memory Management | Manual reference counting | `weakref` module for automatic cleanup | Explicit control vs automatic garbage collection |\n| Testing Framework | Built-in `unittest` with custom concurrency helpers | `hypothesis` for property-based testing | Familiar framework vs advanced randomized testing |\n\n#### Recommended File Structure\n\nOrganizing lock-free code requires careful attention to dependencies and testability. The following structure supports incremental development while maintaining clean separation between layers.\n\n```\nlock_free_structures/\n├── atomic/\n│   ├── __init__.py           # Atomic operations interface\n│   ├── memory_ordering.py    # Memory ordering constants and semantics\n│   ├── compare_and_swap.py   # CAS operation implementation\n│   └── atomic_reference.py   # AtomicReference wrapper class\n├── data_structures/\n│   ├── __init__.py          # Data structure exports\n│   ├── treiber_stack.py     # Lock-free stack implementation\n│   ├── michael_scott_queue.py # Lock-free queue implementation\n│   └── split_ordered_hashmap.py # Lock-free hash map implementation\n├── memory_reclamation/\n│   ├── __init__.py          # Memory reclamation interface\n│   ├── hazard_pointers.py   # Hazard pointer implementation\n│   ├── retirement_list.py   # Node retirement management\n│   └── thread_registry.py   # Thread-local storage management\n├── testing/\n│   ├── __init__.py          # Testing utilities\n│   ├── linearizability.py  # Correctness verification tools\n│   ├── stress_testing.py    # Concurrency stress tests\n│   └── property_testing.py  # Property-based test generators\n└── examples/\n    ├── basic_usage.py       # Simple usage examples\n    ├── performance_comparison.py # Lock-free vs locked benchmarks\n    └── debugging_examples.py # Common debugging scenarios\n```\n\n#### Infrastructure Starter Code\n\nThe following components provide complete, working infrastructure that learners can use immediately while focusing on the core lock-free algorithms.\n\n**Memory Ordering Constants (`memory_ordering.py`)**:\n```python\n\"\"\"Memory ordering semantics for atomic operations.\"\"\"\nfrom enum import Enum\nfrom typing import Any\nimport threading\n\nclass MemoryOrdering(Enum):\n    \"\"\"Memory ordering constraints for atomic operations.\"\"\"\n    RELAXED = \"relaxed\"\n    ACQUIRE = \"acquire\"  \n    RELEASE = \"release\"\n    SEQ_CST = \"seq_cst\"\n\nclass MemoryBarrier:\n    \"\"\"Memory barrier implementation for educational purposes.\"\"\"\n    \n    def __init__(self):\n        self._barrier = threading.Barrier(2)\n    \n    def acquire_fence(self):\n        \"\"\"Acquire memory fence - prevents reordering of subsequent reads.\"\"\"\n        # In real implementation, this would be a CPU fence instruction\n        pass\n    \n    def release_fence(self):\n        \"\"\"Release memory fence - prevents reordering of prior writes.\"\"\"\n        # In real implementation, this would be a CPU fence instruction  \n        pass\n    \n    def full_fence(self):\n        \"\"\"Full memory fence - prevents all reordering across this point.\"\"\"\n        # In real implementation, this would be a CPU fence instruction\n        pass\n\n# Global memory barrier instance for educational use\nmemory_barrier = MemoryBarrier()\n```\n\n**Thread Registry (`thread_registry.py`)**:\n```python\n\"\"\"Thread registry for managing hazard pointer slots across all threads.\"\"\"\nimport threading\nimport weakref\nfrom typing import List, Optional, Set\nfrom dataclasses import dataclass\n\n@dataclass\nclass ThreadRecord:\n    \"\"\"Per-thread record containing hazard pointer slots and metadata.\"\"\"\n    thread_id: int\n    hazard_slots: List[Optional[Any]]\n    next_record: Optional['ThreadRecord']\n    active: bool\n    \n    def __init__(self, thread_id: int, num_hazard_slots: int = 8):\n        self.thread_id = thread_id\n        self.hazard_slots = [None] * num_hazard_slots\n        self.next_record = None\n        self.active = True\n\nclass ThreadRegistry:\n    \"\"\"Global registry managing all threads' hazard pointer records.\"\"\"\n    \n    def __init__(self):\n        self._head = None\n        self._lock = threading.RLock()\n        self._local = threading.local()\n    \n    def get_thread_record(self) -> ThreadRecord:\n        \"\"\"Get or create thread record for current thread.\"\"\"\n        if hasattr(self._local, 'record'):\n            return self._local.record\n            \n        thread_id = threading.get_ident()\n        with self._lock:\n            # Create new thread record\n            record = ThreadRecord(thread_id)\n            record.next_record = self._head\n            self._head = record\n            self._local.record = record\n            \n            # Register cleanup on thread exit\n            weakref.finalize(threading.current_thread(), self._cleanup_thread, thread_id)\n            \n            return record\n    \n    def _cleanup_thread(self, thread_id: int):\n        \"\"\"Clean up thread record when thread exits.\"\"\"\n        with self._lock:\n            current = self._head\n            prev = None\n            \n            while current:\n                if current.thread_id == thread_id:\n                    current.active = False\n                    # Clear all hazard pointers\n                    for i in range(len(current.hazard_slots)):\n                        current.hazard_slots[i] = None\n                    break\n                prev = current\n                current = current.next_record\n    \n    def collect_hazard_pointers(self) -> Set[Any]:\n        \"\"\"Collect all active hazard pointers from all threads.\"\"\"\n        protected = set()\n        with self._lock:\n            current = self._head\n            while current:\n                if current.active:\n                    for pointer in current.hazard_slots:\n                        if pointer is not None:\n                            protected.add(pointer)\n                current = current.next_record\n        return protected\n\n# Global thread registry instance\nthread_registry = ThreadRegistry()\n```\n\n#### Core Logic Skeleton Code\n\nThe following skeletons provide method signatures and detailed TODO comments that map directly to the algorithm steps described in previous sections.\n\n**CAS Retry Loop Pattern (`compare_and_swap.py`)**:\n```python\n\"\"\"Compare-and-swap retry loop implementation.\"\"\"\nimport time\nimport random\nfrom typing import Callable, TypeVar, Optional, Tuple, Any\nfrom .memory_ordering import MemoryOrdering\nfrom .atomic_reference import AtomicReference\n\nT = TypeVar('T')\n\ndef cas_retry_loop(atomic_ref: AtomicReference, \n                   update_function: Callable[[Any], Any],\n                   max_attempts: int = 1000) -> Tuple[bool, Any]:\n    \"\"\"\n    Generic CAS retry loop with exponential backoff.\n    \n    Args:\n        atomic_ref: Atomic reference to update\n        update_function: Function that computes new value from current value\n        max_attempts: Maximum retry attempts before giving up\n        \n    Returns:\n        (success, final_value): Success flag and final observed value\n    \"\"\"\n    backoff_us = 1  # Start with 1 microsecond\n    \n    for attempt in range(max_attempts):\n        # TODO 1: Load current value from atomic reference using ACQUIRE ordering\n        # TODO 2: Compute new value by calling update_function(current)\n        # TODO 3: Attempt CAS operation: compare_and_swap(current, new_value)\n        # TODO 4: If CAS succeeded, return (True, new_value)\n        # TODO 5: If CAS failed, implement exponential backoff with jitter\n        # TODO 6: Update backoff_us = min(backoff_us * 2, 1000) for next iteration\n        # TODO 7: Add random jitter: time.sleep((backoff_us + random.randint(0, backoff_us)) / 1_000_000)\n        # TODO 8: Continue to next iteration with updated current value from CAS failure\n        pass\n    \n    # TODO 9: Max attempts reached - return (False, last_observed_value)\n    return False, None\n```\n\n**Hazard Pointer Protection (`hazard_pointers.py`)**:\n```python\n\"\"\"Hazard pointer protection and reclamation system.\"\"\"\nfrom typing import Any, Optional, Tuple, List, Set\nimport threading\nfrom .thread_registry import thread_registry, ThreadRecord\nfrom .atomic_reference import AtomicReference\n\nclass HazardPointer:\n    \"\"\"Hazard pointer system for safe memory reclamation.\"\"\"\n    \n    def __init__(self):\n        self._retirement_threshold = 100\n    \n    def protect(self, pointer: Any) -> int:\n        \"\"\"\n        Protect a pointer from reclamation by announcing it in hazard pointer slot.\n        \n        Args:\n            pointer: Pointer to protect (can be None to clear protection)\n            \n        Returns:\n            slot_index: Index of hazard slot used for protection\n        \"\"\"\n        # TODO 1: Get thread record for current thread from thread_registry\n        # TODO 2: Find first available (None) slot in thread_record.hazard_slots\n        # TODO 3: Store pointer in the available slot using release memory ordering\n        # TODO 4: Return slot index for later release() call\n        # TODO 5: If no available slots, raise RuntimeError(\"No available hazard pointer slots\")\n        pass\n    \n    def release(self, slot_index: int) -> None:\n        \"\"\"\n        Release hazard pointer protection by clearing the slot.\n        \n        Args:\n            slot_index: Index returned by previous protect() call\n        \"\"\"\n        # TODO 1: Get thread record for current thread\n        # TODO 2: Validate slot_index is within bounds of hazard_slots array\n        # TODO 3: Clear hazard_slots[slot_index] = None using release memory ordering\n        # TODO 4: Memory fence to ensure visibility to other threads scanning hazard pointers\n        pass\n    \n    def protect_and_verify(self, atomic_ref: AtomicReference) -> Tuple[Any, int]:\n        \"\"\"\n        Safely load and protect a pointer with verification to prevent races.\n        \n        Args:\n            atomic_ref: Atomic reference to load pointer from\n            \n        Returns:\n            (pointer, slot_index): Protected pointer and hazard slot index\n        \"\"\"\n        max_retries = 10\n        for attempt in range(max_retries):\n            # TODO 1: Load pointer from atomic_ref using ACQUIRE memory ordering\n            # TODO 2: If pointer is None, return (None, -1) - no protection needed\n            # TODO 3: Protect the loaded pointer by calling self.protect(pointer)\n            # TODO 4: Verify pointer still matches by re-loading atomic_ref with ACQUIRE\n            # TODO 5: If verification_pointer == pointer, return (pointer, slot_index)\n            # TODO 6: If verification failed, call self.release(slot_index) and retry\n            # TODO 7: Continue retry loop until success or max_retries exceeded\n            pass\n        \n        # TODO 8: Max retries exceeded - raise RuntimeError(\"Could not protect pointer after retries\")\n        return None, -1\n    \n    def retire(self, node: Any) -> None:\n        \"\"\"\n        Retire a node for eventual reclamation when safe.\n        \n        Args:\n            node: Node to retire (must not be currently protected)\n        \"\"\"\n        # TODO 1: Get thread record for current thread\n        # TODO 2: Add node to thread-local retirement list\n        # TODO 3: Check if retirement list size exceeds threshold\n        # TODO 4: If threshold exceeded, trigger scan_and_reclaim()\n        # TODO 5: Otherwise, defer reclamation until threshold reached\n        pass\n    \n    def scan_and_reclaim(self) -> int:\n        \"\"\"\n        Scan all hazard pointers and reclaim unprotected retired nodes.\n        \n        Returns:\n            Number of nodes successfully reclaimed\n        \"\"\"\n        # TODO 1: Collect all retired nodes from all threads' retirement lists\n        # TODO 2: Collect all active hazard pointers from thread_registry.collect_hazard_pointers()\n        # TODO 3: Compute reclaimable_nodes = retired_nodes - protected_pointers\n        # TODO 4: For each reclaimable node, call its destructor and free memory\n        # TODO 5: Move non-reclaimable nodes back to retirement lists for next scan\n        # TODO 6: Return count of successfully reclaimed nodes\n        pass\n\n# Global hazard pointer system instance  \nhazard_system = HazardPointer()\n```\n\n#### Milestone Checkpoints\n\nEach milestone builds incrementally on previous components, with specific verification steps to ensure correct implementation before proceeding.\n\n**Milestone 1: Atomic Operations Checkpoint**\n- **Command**: `python -m pytest testing/test_atomic_operations.py -v`\n- **Expected Output**: All CAS retry loops complete successfully under contention\n- **Manual Verification**: Run `python examples/atomic_counter_demo.py` with 10 threads incrementing shared counter 1000 times each - final count should be exactly 10000\n- **Success Indicators**: No lost updates, no infinite loops, consistent memory ordering behavior\n- **Failure Signs**: Final count != expected total (indicates lost updates), test timeouts (indicates infinite spinning), inconsistent results across runs (indicates memory ordering bugs)\n\n**Milestone 2: Lock-free Stack Checkpoint**\n- **Command**: `python -m pytest testing/test_treiber_stack.py -v`\n- **Expected Output**: Concurrent push/pop operations maintain LIFO ordering and no lost elements\n- **Manual Verification**: Run `python examples/stack_stress_test.py` with multiple producers and consumers - total elements in + out should balance exactly\n- **Success Indicators**: Perfect LIFO ordering, no duplicate elements, no lost elements, linearizable behavior\n- **Failure Signs**: Elements appearing out of LIFO order, duplicate pops of same element, missing elements, crashes on empty stack pop\n\n**Milestone 3: Lock-free Queue Checkpoint**\n- **Command**: `python -m pytest testing/test_michael_scott_queue.py -v`\n- **Expected Output**: Concurrent enqueue/dequeue operations maintain FIFO ordering\n- **Manual Verification**: Enqueue elements 1,2,3,4,5 from different threads, dequeue should return exactly 1,2,3,4,5 in order\n- **Success Indicators**: Perfect FIFO ordering, tail helping mechanism works, no stuck operations\n- **Failure Signs**: Elements dequeued out of FIFO order, operations hanging when tail pointer lags, memory corruption\n\n**Milestone 4: Hazard Pointers Checkpoint**\n- **Command**: `python -m pytest testing/test_hazard_pointers.py -v`\n- **Expected Output**: No use-after-free errors, proper protection and reclamation\n- **Manual Verification**: Run stack/queue stress tests with aggressive reclamation - no crashes or memory errors\n- **Success Indicators**: No premature reclamation, proper thread cleanup, bounded memory usage\n- **Failure Signs**: Segmentation faults, accessing freed memory, unbounded memory growth, protection race conditions\n\n**Milestone 5: Hash Map Checkpoint**\n- **Command**: `python -m pytest testing/test_split_ordered_hashmap.py -v`\n- **Expected Output**: Concurrent insert/lookup/delete operations with correct key-value associations\n- **Manual Verification**: Insert 1000 unique key-value pairs from multiple threads, verify all lookups return correct values\n- **Success Indicators**: All inserted keys found with correct values, successful deletion removes keys, bucket splitting works correctly\n- **Failure Signs**: Lost key-value pairs, incorrect values returned, infinite loops during bucket initialization, corrupted bucket chains\n\n#### Debugging Tips\n\nLock-free code exhibits unique failure modes that require specialized debugging approaches. The following table maps common symptoms to their likely causes and diagnostic steps.\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|------------------|-----|\n| Final counter != expected total | Lost updates due to race condition | Add logging to CAS failures, check retry counts | Ensure CAS retry loop, verify atomic operations |\n| Infinite loop in CAS retry | Target value constantly changing | Log current/expected values in each iteration | Add exponential backoff, limit max retries |\n| Segmentation fault during pop/dequeue | Use-after-free from premature reclamation | Check hazard pointer protection, trace node lifecycle | Protect nodes before access, verify retirement timing |\n| Elements appearing out of order | Incorrect linearization points | Record operation timestamps, verify ordering properties | Review CAS placement, ensure proper memory ordering |\n| Memory usage growing unbounded | Retirement list not being processed | Monitor retirement list sizes, check scan frequency | Lower scan threshold, ensure scan_and_reclaim() called |\n| Operations hanging indefinitely | Missing helping mechanism | Check tail pointer updates in queue | Implement helping in enqueue, verify tail advancement |\n| Corrupted linked list structure | ABA problem or incorrect CAS | Add tagged pointers, verify pointer consistency | Use version counters, validate next pointer integrity |\n| Performance degradation under contention | Excessive CAS failures and retries | Profile CAS success rates, measure retry counts | Optimize backoff strategy, reduce contention points |\n\nUnderstanding these patterns helps developers quickly identify and fix the subtle bugs that are common in lock-free programming, leading to more robust and performant implementations.\n\n\n## Error Handling and Edge Cases\n\n> **Milestone(s):** All milestones (this section addresses failure modes and recovery strategies that apply across all lock-free data structures, from basic atomic operations through complete hash maps)\n\nThe world of lock-free programming introduces a completely new category of failure modes that simply don't exist in traditional lock-based systems. Unlike mutex-protected code where errors are typically straightforward resource conflicts or simple logic bugs, lock-free algorithms face subtle timing-dependent failures that can manifest sporadically under load, making them notoriously difficult to reproduce and debug. These failures often stem from the fundamental tension between maintaining correctness without coordination primitives and achieving progress without blocking.\n\n### Mental Model: Lock-free Errors as Traffic Intersection Problems\n\nThink of lock-free programming errors like problems at a busy traffic intersection without traffic lights or stop signs. In a traditional lock-based system, we'd have traffic lights (mutexes) that ensure only one direction moves at a time - failures are obvious and immediate, like a broken light or a car running a red light. But in lock-free systems, we're trying to coordinate traffic flow using only yield signs and careful observation.\n\nThe ABA problem is like a car leaving the intersection, another identical car entering from the same direction, and a driver thinking the intersection status hasn't changed when it actually has. Livelock resembles cars politely yielding to each other indefinitely, with everyone being courteous but no one making progress. Memory reclamation races are like demolishing a building while people might still be inside, not knowing if anyone is currently using that space. These problems require fundamentally different detection and recovery strategies than traditional \"broken traffic light\" failures.\n\n### Lock-free Specific Failure Modes\n\nLock-free algorithms introduce several categories of failures that are unique to non-blocking concurrent programming. Understanding these failure modes is crucial because they often manifest as subtle correctness violations or performance degradations rather than obvious crashes.\n\n#### ABA Problem Manifestations\n\nThe **ABA problem** represents one of the most insidious failure modes in lock-free programming. This occurs when a compare-and-swap operation incorrectly succeeds because a memory location has returned to its original value despite being modified by other threads. The fundamental issue is that CAS only checks the current value, not whether the value has changed since the original read.\n\n| Failure Scenario | Symptom | Root Cause | Consequences |\n|-----------------|---------|------------|--------------|\n| Node Reuse in Stack | Lost elements during concurrent pop operations | Stack node freed and reallocated at same address | Stack corruption, elements permanently lost |\n| Pointer Recycling | CAS succeeds on recycled pointer | Memory allocator reuses address for new object | Data structure links to wrong object type |\n| Reference Count Races | Premature object deletion | Reference count drops to zero then increases | Use-after-free, memory corruption |\n| Tag Counter Overflow | False ABA detection after tag wraparound | Version counter overflows back to original value | Incorrect operation success or failure |\n\nConsider a concrete example in the Treiber stack. Thread A reads the top pointer (value 0x1000), gets interrupted, and Thread B pops that node and pushes a new node that happens to get allocated at the same address (0x1000). When Thread A resumes and performs its CAS, it succeeds because the address matches, but it's now operating on a completely different node. This can result in the stack pointing to freed memory or creating cycles in the node chain.\n\n> **Critical Insight**: The ABA problem is fundamentally about the inadequacy of single-word comparison for determining whether a data structure has been modified. The solution requires expanding the comparison to include additional state that changes with each modification.\n\n#### Livelock and Progress Guarantee Violations\n\n**Livelock** occurs when threads continuously retry operations but make no collective progress because their attempts interfere with each other. Unlike deadlock where threads are blocked, livelock threads remain active but accomplish nothing productive. This is particularly problematic in lock-free algorithms because the progress guarantee requires that at least one thread makes progress within a bounded number of steps.\n\n| Livelock Type | Detection Pattern | Common Causes | Impact on System |\n|---------------|------------------|---------------|------------------|\n| CAS Thrashing | High CPU usage, low throughput | Multiple threads competing for same atomic variable | Performance degradation, potential starvation |\n| Helping Conflicts | Operations appear to succeed but data structure state oscillates | Threads helping each other in conflicting ways | Functional correctness violations |\n| Backoff Synchronization | Periodic performance drops | Threads using same backoff timing | Reduced parallelism, wasted CPU cycles |\n| Memory Contention | Cache miss rates spike | False sharing on atomic variables | System-wide performance impact |\n\nThe Michael-Scott queue's helping mechanism can exhibit livelock when multiple threads attempt to help advance the tail pointer simultaneously. If the helping logic isn't carefully designed, threads can continuously undo each other's work, with the tail pointer oscillating between positions without the queue making meaningful progress.\n\n#### Memory Reclamation Races\n\nMemory reclamation in lock-free systems introduces timing-dependent failures where nodes are deallocated while other threads still hold references to them. These failures are particularly dangerous because they often manifest as memory corruption rather than immediate crashes, making diagnosis extremely difficult.\n\n| Race Condition | Symptom | Detection Method | Critical Timing Window |\n|----------------|---------|------------------|------------------------|\n| Use-After-Free | Segmentation fault, data corruption | Memory sanitizers, valgrind | Between retire() call and actual deallocation |\n| Double-Free | Heap corruption, allocator crashes | Debug heap, address sanitizer | Multiple threads retiring same node |\n| Premature Reclamation | Stale data reads, incorrect operation results | Hazard pointer validation | Node freed before all readers finish |\n| Retirement List Overflow | Memory leaks, unbounded growth | Memory usage monitoring | Retirement rate exceeds scan frequency |\n\nThe hazard pointer protocol's protect-then-verify pattern can fail if not implemented atomically. A thread might protect a pointer, but between the protection and verification steps, another thread could retire and reclaim the node, leading to the verification step accessing freed memory.\n\n> **Design Principle**: Memory reclamation safety requires that the window between node removal from the data structure and actual deallocation be managed with explicit coordination between threads.\n\n#### Progress Guarantee Violations\n\nLock-free algorithms promise that at least one thread makes progress within a bounded number of steps, but several conditions can violate this guarantee without causing obvious failures.\n\n| Violation Type | Manifestation | Underlying Cause | Recovery Strategy |\n|----------------|---------------|------------------|-------------------|\n| Starvation | Some threads never complete operations | Unfair scheduling, priority inversion | Exponential backoff, helping protocols |\n| Infinite Retry | CAS loops that never terminate | Continuous interference from other threads | Maximum retry limits, fallback mechanisms |\n| Helping Deadlock | Helper threads block each other | Circular helping dependencies | Helping order constraints, timeout mechanisms |\n| Memory Ordering Issues | Operations appear to complete but state is inconsistent | Insufficient memory barriers | Stronger ordering guarantees |\n\n### Failure Detection Strategies\n\nDetecting failures in lock-free systems requires different approaches than traditional concurrent programming because many failures manifest as subtle correctness violations rather than obvious exceptions.\n\n#### Invariant Violation Detection\n\nLock-free data structures maintain complex invariants that can be violated by race conditions or implementation bugs. Continuous monitoring of these invariants helps detect problems before they cause observable failures.\n\n| Data Structure | Critical Invariants | Detection Method | Monitoring Frequency |\n|----------------|-------------------|------------------|---------------------|\n| Treiber Stack | Top pointer always points to valid node or NULL | Pointer validation, reachability checks | Every operation in debug mode |\n| Michael-Scott Queue | Head and tail pointers maintain proper distance | Distance calculation, sentinel validation | Periodic background scan |\n| Split-Ordered HashMap | Bucket initialization order preserved | Parent-child bucket relationship checks | During bucket access |\n| Hazard Pointer System | Protected pointers not in retirement list | Cross-reference scan | Before each reclamation |\n\nThe stack invariant checker validates that every node reachable from the top pointer has a valid next pointer and that no cycles exist in the chain. Implementation involves traversing the entire stack while checking that each node's memory address falls within valid allocated ranges.\n\n```python\ndef validate_stack_invariants(stack):\n    \"\"\"\n    Validates critical stack invariants without modifying structure.\n    Returns tuple of (is_valid, violation_description).\n    \"\"\"\n    # TODO: Load top pointer atomically\n    # TODO: Traverse chain checking for cycles using visited set\n    # TODO: Verify each node memory address is in valid range\n    # TODO: Check that each next pointer is atomic reference\n    # TODO: Return (True, \"\") if all checks pass\n    pass\n```\n\n#### Performance Degradation Monitoring\n\nLock-free algorithms can experience gradual performance degradation due to increased contention, cache line bouncing, or suboptimal memory access patterns. Early detection prevents minor issues from escalating into system-wide problems.\n\n| Performance Metric | Normal Range | Warning Threshold | Critical Threshold | Diagnostic Action |\n|-------------------|-------------|-------------------|-------------------|-------------------|\n| CAS Success Rate | 85-95% | <75% | <50% | Analyze contention patterns |\n| Operation Latency | <1μs | >10μs | >100μs | Profile memory access patterns |\n| Retry Loop Iterations | 1-3 avg | >10 avg | >50 avg | Implement exponential backoff |\n| Hazard Pointer Scan Time | <1ms | >10ms | >100ms | Reduce retirement threshold |\n\nThe retry loop monitoring system tracks the distribution of CAS attempts per operation across all threads. A sudden increase in retry counts often indicates either increased load or the emergence of a livelock condition.\n\n#### Correctness Verification Through Linearizability Checking\n\nLinearizability provides a formal correctness criterion for concurrent data structures by requiring that operations appear to take effect atomically at some point between their invocation and response. Implementing runtime linearizability checking helps catch subtle correctness bugs that manifest only under specific thread interleavings.\n\n| Verification Approach | Accuracy | Performance Cost | Implementation Complexity |\n|----------------------|----------|------------------|---------------------------|\n| Online Checking | High | 10-20x overhead | Complex state tracking |\n| Offline Analysis | Very High | Post-execution only | Log analysis algorithms |\n| Statistical Sampling | Medium | 2-3x overhead | Moderate instrumentation |\n| Stress Test Oracles | Medium | Test-time only | Property-based assertions |\n\nThe linearizability checker maintains a log of all operation invocations and responses across threads, then analyzes the log to determine if there exists a valid sequential execution that respects the timing constraints and data structure semantics.\n\n> **Decision: Online vs Offline Linearizability Checking**\n> - **Context**: Need to verify correctness during development and testing phases\n> - **Options Considered**: Real-time checking during execution vs post-execution analysis vs sampling-based approximation\n> - **Decision**: Implement both online sampling (for development) and offline complete analysis (for comprehensive testing)\n> - **Rationale**: Online checking catches problems immediately during development, while offline analysis provides definitive verification for critical test scenarios\n> - **Consequences**: Development builds include instrumentation overhead, but production builds remain unaffected; comprehensive testing requires log storage and analysis infrastructure\n\n### Recovery and Fallback Mechanisms\n\nWhen lock-free algorithms encounter the failure modes described above, they need robust recovery mechanisms that restore progress without compromising correctness guarantees.\n\n#### Exponential Backoff Strategies\n\nExponential backoff helps resolve contention-based failures by introducing randomized delays that reduce the probability of continued interference between competing threads. However, the backoff strategy must be carefully tuned to balance conflict avoidance with responsiveness.\n\n| Backoff Parameter | Conservative Value | Aggressive Value | Adaptive Strategy |\n|-------------------|-------------------|------------------|-------------------|\n| Initial Delay | 10 CPU cycles | 1 CPU cycle | Based on recent success rate |\n| Maximum Delay | 1000 CPU cycles | 100 CPU cycles | Scaled by system load |\n| Multiplier | 2.0 | 1.5 | Adjusted by contention level |\n| Jitter Range | ±25% | ±10% | Increased under high contention |\n\nThe adaptive backoff implementation monitors per-thread CAS success rates and adjusts parameters dynamically. Threads experiencing frequent failures increase their backoff aggressiveness, while threads with high success rates use minimal delays to maintain performance.\n\n```python\ndef cas_retry_loop(atomic_ref, update_function, max_attempts):\n    \"\"\"\n    Generic CAS retry loop with exponential backoff.\n    Returns (success, final_value, attempts_made).\n    \"\"\"\n    # TODO: Initialize backoff parameters (delay, max_delay, multiplier)\n    # TODO: Loop up to max_attempts times\n    # TODO: Load current value from atomic_ref\n    # TODO: Compute new value using update_function\n    # TODO: Attempt CAS with expected=current, new=computed\n    # TODO: If CAS succeeds, return (True, new_value, attempts)\n    # TODO: If CAS fails, apply exponential backoff delay\n    # TODO: Update backoff delay = min(delay * multiplier, max_delay)\n    # TODO: Add random jitter to prevent synchronized retries\n    # TODO: If max_attempts reached, return (False, observed_value, attempts)\n    pass\n```\n\n#### Helping Protocol Recovery\n\nThe helping mechanism in lock-free algorithms allows threads to assist each other when operations become stuck, but helping protocols can themselves become sources of failure. Robust helping requires careful ordering and timeout mechanisms.\n\n| Helping Scenario | Detection Trigger | Helper Action | Recovery Condition |\n|------------------|------------------|---------------|-------------------|\n| Stuck Tail Update | Tail pointer lags behind actual end | Advance tail to correct position | Successful tail CAS |\n| Incomplete Enqueue | Node linked but tail not updated | Complete the tail update | Tail points to new node |\n| Abandoned Operation | Thread terminated mid-operation | Roll back partial changes | Consistent structure state |\n| Helping Conflict | Multiple helpers interfere | Coordinate through additional CAS | Single helper succeeds |\n\nThe Michael-Scott queue's helping protocol ensures that if one thread observes the tail pointer lagging behind the actual end of the queue, it attempts to advance the tail pointer to the correct position. This prevents enqueue operations from getting stuck when the thread that added a node fails to complete the tail update.\n\n> **Critical Design Constraint**: Helping protocols must be designed to be idempotent - multiple threads performing the same helping action should not create inconsistent state or undo each other's work.\n\n#### Graceful Degradation Under Extreme Contention\n\nWhen contention becomes so severe that normal lock-free operation becomes inefficient, the system needs fallback mechanisms that maintain correctness while trading some performance for guaranteed progress.\n\n| Degradation Level | Trigger Condition | Fallback Strategy | Performance Impact |\n|-------------------|------------------|-------------------|-------------------|\n| Increased Backoff | CAS success rate <75% | Double backoff parameters | 10-20% throughput reduction |\n| Helping Escalation | Operations stuck >100 retries | Activate additional helping threads | 20-30% overhead increase |\n| Temporary Serialization | System-wide contention detected | Funnel operations through single point | 50-80% throughput reduction |\n| Emergency Locking | Lock-free progress stalled | Fall back to mutex protection | Eliminates lock-free benefits |\n\nThe emergency fallback system monitors system-wide operation success rates and can temporarily serialize access through a single coordination point when lock-free progress guarantees are violated. This ensures the system remains functional even under pathological conditions.\n\n#### Memory Reclamation Recovery\n\nWhen hazard pointer systems encounter failures or resource exhaustion, recovery mechanisms must safely handle the accumulated retirement list without creating use-after-free conditions.\n\n| Recovery Scenario | Trigger | Recovery Action | Safety Guarantee |\n|-------------------|---------|-----------------|------------------|\n| Retirement List Overflow | List size > threshold | Force immediate scan and reclamation | No premature reclamation |\n| Scan Failure | Scan finds no reclaimable nodes | Increase scan frequency, reduce threshold | Bounded memory growth |\n| Thread Exit Cleanup | Thread termination detected | Reclaim thread's retirement list | No leaked memory |\n| Hazard Slot Exhaustion | All slots occupied | Expand slot array or force reclamation | Continued protection ability |\n\nThe emergency reclamation procedure performs a conservative scan where any questionable nodes remain in the retirement list rather than being reclaimed. This ensures memory safety at the cost of potentially increased memory usage until the next successful scan.\n\n### Common Recovery Implementation Pitfalls\n\nUnderstanding typical mistakes in recovery mechanism implementation helps avoid creating new failure modes while trying to handle existing ones.\n\n⚠️ **Pitfall: Recursive Recovery Failures**\nRecovery mechanisms that can themselves fail and trigger additional recovery attempts create the potential for infinite recursion or resource exhaustion. For example, if the exponential backoff system allocates memory for timing state and that allocation fails, attempting to recover by triggering more backoff can exhaust available memory. The fix requires making recovery mechanisms allocation-free and ensuring they have bounded resource requirements.\n\n⚠️ **Pitfall: Recovery Actions Violating Lock-free Properties**\nSome recovery strategies inadvertently introduce blocking behavior that violates the fundamental lock-free progress guarantee. Using mutexes in fallback paths or waiting indefinitely for helper threads to complete their work can cause the entire system to block. Recovery mechanisms must preserve the non-blocking nature of the algorithm, even if they reduce performance.\n\n⚠️ **Pitfall: Incomplete Failure State Cleanup**\nFailed operations often leave the data structure in a partially modified state that must be properly cleaned up before other operations can proceed. For example, if a queue enqueue operation links a new node but fails to update the tail pointer, the cleanup must ensure the tail update completes rather than leaving the queue in an inconsistent state.\n\n⚠️ **Pitfall: Race Conditions in Error Detection**\nThe error detection mechanisms themselves can introduce race conditions if they're not implemented with the same care as the main algorithm. Reading multiple atomic variables to check invariants without proper ordering can observe inconsistent intermediate states that appear to be errors but are actually valid transient conditions.\n\n### Implementation Guidance\n\nThe error handling and recovery systems for lock-free data structures require careful implementation to avoid introducing new failure modes while solving existing ones.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Error Detection | Manual invariant checks with assertions | Automated linearizability checker with full history analysis |\n| Performance Monitoring | Simple counters with periodic logging | Real-time metrics with statistical analysis and alerting |\n| Recovery Mechanisms | Fixed exponential backoff with manual tuning | Adaptive backoff with machine learning optimization |\n| Memory Safety | Basic hazard pointers with conservative scanning | Advanced epoch-based reclamation with optimized batching |\n\n#### Recommended Module Structure\n\nThe error handling system integrates across all components of the lock-free library:\n\n```\nlock_free_lib/\n  error_handling/\n    __init__.py                 ← Error types and base classes\n    detection/\n      invariant_checker.py      ← Data structure invariant validation\n      performance_monitor.py    ← Contention and latency tracking\n      linearizability.py       ← Correctness verification\n    recovery/\n      backoff_strategies.py     ← Exponential and adaptive backoff\n      helping_protocols.py      ← Recovery through inter-thread cooperation\n      fallback_mechanisms.py    ← Graceful degradation under extreme load\n    memory_safety/\n      hazard_validation.py      ← Hazard pointer correctness checks\n      leak_detection.py         ← Memory reclamation monitoring\n  data_structures/\n    stack.py                   ← Treiber stack with error handling integration\n    queue.py                   ← Michael-Scott queue with recovery mechanisms\n    hashmap.py                 ← Split-ordered map with degradation handling\n  testing/\n    stress_tests.py           ← High-contention correctness verification\n    failure_injection.py      ← Systematic failure mode testing\n```\n\n#### Infrastructure Starter Code\n\n**Error Detection Foundation:**\n\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import Optional, Tuple, List, Dict, Any\nfrom enum import Enum\nfrom dataclasses import dataclass\nfrom threading import current_thread\nimport time\nimport weakref\n\nclass FailureMode(Enum):\n    ABA_PROBLEM = \"aba_problem\"\n    LIVELOCK = \"livelock\"\n    MEMORY_RACE = \"memory_reclamation_race\"\n    PROGRESS_VIOLATION = \"progress_guarantee_violation\"\n    INVARIANT_VIOLATION = \"invariant_violation\"\n\n@dataclass\nclass FailureReport:\n    failure_mode: FailureMode\n    thread_id: int\n    timestamp: float\n    operation_type: str\n    details: Dict[str, Any]\n    stack_trace: Optional[str] = None\n\nclass InvariantChecker:\n    \"\"\"Base class for data structure invariant validation.\"\"\"\n    \n    def __init__(self, enable_checking: bool = True):\n        self.enable_checking = enable_checking\n        self.violation_count = 0\n        self.last_check_time = time.time()\n    \n    def check_invariants(self, data_structure: Any) -> Tuple[bool, Optional[str]]:\n        \"\"\"\n        Validates all invariants for the given data structure.\n        Returns (is_valid, violation_description).\n        \"\"\"\n        if not self.enable_checking:\n            return (True, None)\n        \n        self.last_check_time = time.time()\n        \n        try:\n            return self._validate_structure_specific_invariants(data_structure)\n        except Exception as e:\n            self.violation_count += 1\n            return (False, f\"Exception during invariant check: {str(e)}\")\n    \n    @abstractmethod\n    def _validate_structure_specific_invariants(self, data_structure: Any) -> Tuple[bool, Optional[str]]:\n        \"\"\"Implement structure-specific invariant checks.\"\"\"\n        pass\n\nclass PerformanceMonitor:\n    \"\"\"Tracks operation performance and detects degradation patterns.\"\"\"\n    \n    def __init__(self, history_size: int = 1000):\n        self.operation_times = {}  # operation_type -> list of (timestamp, duration)\n        self.cas_attempts = {}     # operation_type -> list of attempt_counts\n        self.success_rates = {}    # operation_type -> recent success rate\n        self.history_size = history_size\n    \n    def record_operation(self, operation_type: str, duration: float, cas_attempts: int, success: bool):\n        \"\"\"Record performance metrics for a completed operation.\"\"\"\n        current_time = time.time()\n        \n        # Track operation timing\n        if operation_type not in self.operation_times:\n            self.operation_times[operation_type] = []\n        \n        times_list = self.operation_times[operation_type]\n        times_list.append((current_time, duration))\n        \n        # Maintain bounded history\n        if len(times_list) > self.history_size:\n            times_list.pop(0)\n        \n        # Track CAS attempt distribution\n        if operation_type not in self.cas_attempts:\n            self.cas_attempts[operation_type] = []\n        \n        attempts_list = self.cas_attempts[operation_type]\n        attempts_list.append(cas_attempts)\n        \n        if len(attempts_list) > self.history_size:\n            attempts_list.pop(0)\n        \n        # Update success rate (sliding window)\n        self._update_success_rate(operation_type, success)\n    \n    def _update_success_rate(self, operation_type: str, success: bool):\n        \"\"\"Update rolling success rate for operation type.\"\"\"\n        if operation_type not in self.success_rates:\n            self.success_rates[operation_type] = []\n        \n        rate_history = self.success_rates[operation_type]\n        rate_history.append(1.0 if success else 0.0)\n        \n        if len(rate_history) > 100:  # Keep last 100 operations\n            rate_history.pop(0)\n    \n    def get_current_metrics(self, operation_type: str) -> Dict[str, float]:\n        \"\"\"Get current performance metrics for operation type.\"\"\"\n        if operation_type not in self.operation_times:\n            return {}\n        \n        times = [duration for _, duration in self.operation_times[operation_type]]\n        attempts = self.cas_attempts.get(operation_type, [])\n        success_rate = sum(self.success_rates.get(operation_type, [])) / max(1, len(self.success_rates.get(operation_type, [])))\n        \n        return {\n            'avg_duration': sum(times) / len(times) if times else 0.0,\n            'max_duration': max(times) if times else 0.0,\n            'avg_cas_attempts': sum(attempts) / len(attempts) if attempts else 0.0,\n            'max_cas_attempts': max(attempts) if attempts else 0.0,\n            'success_rate': success_rate\n        }\n\n# Global performance monitor instance\nperformance_monitor = PerformanceMonitor()\n```\n\n**Recovery Infrastructure:**\n\n```python\nimport random\nimport time\nfrom typing import Callable, TypeVar, Tuple, Optional\nfrom threading import current_thread\n\nT = TypeVar('T')\n\nclass BackoffStrategy:\n    \"\"\"Implements adaptive exponential backoff for CAS retry loops.\"\"\"\n    \n    def __init__(self, initial_delay: int = 1, max_delay: int = 1000, multiplier: float = 2.0, jitter: float = 0.25):\n        self.initial_delay = initial_delay\n        self.max_delay = max_delay\n        self.multiplier = multiplier\n        self.jitter = jitter\n        self.current_delay = initial_delay\n        self.consecutive_failures = 0\n    \n    def wait(self):\n        \"\"\"Execute backoff delay with jitter.\"\"\"\n        if self.current_delay > 0:\n            jitter_amount = self.current_delay * self.jitter\n            actual_delay = self.current_delay + random.uniform(-jitter_amount, jitter_amount)\n            \n            # Convert to seconds (assuming delays are in microseconds)\n            time.sleep(max(0, actual_delay) / 1_000_000)\n    \n    def on_failure(self):\n        \"\"\"Called when CAS operation fails - increase backoff.\"\"\"\n        self.consecutive_failures += 1\n        self.current_delay = min(self.current_delay * self.multiplier, self.max_delay)\n    \n    def on_success(self):\n        \"\"\"Called when CAS operation succeeds - reset backoff.\"\"\"\n        self.consecutive_failures = 0\n        self.current_delay = self.initial_delay\n\nclass RetryLoop:\n    \"\"\"Generic retry loop with backoff and monitoring.\"\"\"\n    \n    def __init__(self, max_attempts: int = 100):\n        self.max_attempts = max_attempts\n        self.thread_backoff = {}  # thread_id -> BackoffStrategy\n    \n    def _get_backoff_for_thread(self) -> BackoffStrategy:\n        \"\"\"Get or create backoff strategy for current thread.\"\"\"\n        thread_id = current_thread().ident\n        if thread_id not in self.thread_backoff:\n            self.thread_backoff[thread_id] = BackoffStrategy()\n        return self.thread_backoff[thread_id]\n    \n    def execute(self, operation: Callable[[], Tuple[bool, T]], operation_name: str = \"unknown\") -> Tuple[bool, Optional[T], int]:\n        \"\"\"\n        Execute operation with retry and backoff.\n        Returns (success, result, attempts_made).\n        \"\"\"\n        backoff = self._get_backoff_for_thread()\n        start_time = time.time()\n        \n        for attempt in range(1, self.max_attempts + 1):\n            try:\n                success, result = operation()\n                \n                if success:\n                    backoff.on_success()\n                    duration = time.time() - start_time\n                    performance_monitor.record_operation(operation_name, duration, attempt, True)\n                    return (True, result, attempt)\n                else:\n                    backoff.on_failure()\n                    if attempt < self.max_attempts:\n                        backoff.wait()\n            \n            except Exception as e:\n                # Treat exceptions as failures, but don't retry on permanent errors\n                backoff.on_failure()\n                break\n        \n        # All attempts failed\n        duration = time.time() - start_time\n        performance_monitor.record_operation(operation_name, duration, self.max_attempts, False)\n        return (False, None, self.max_attempts)\n\n# Global retry loop instance\ndefault_retry_loop = RetryLoop()\n```\n\n#### Core Logic Skeleton Code\n\n**CAS Retry Loop with Monitoring:**\n\n```python\ndef cas_retry_loop(atomic_ref, update_function, max_attempts):\n    \"\"\"\n    Generic CAS retry loop with exponential backoff and performance monitoring.\n    \n    Args:\n        atomic_ref: AtomicReference to update\n        update_function: Function that takes current value and returns new value\n        max_attempts: Maximum number of CAS attempts before giving up\n    \n    Returns:\n        Tuple of (success: bool, final_value: Any, attempts_made: int)\n    \"\"\"\n    # TODO 1: Get thread-local backoff strategy instance\n    # TODO 2: Record start time for performance monitoring\n    # TODO 3: Loop from 1 to max_attempts (inclusive)\n    # TODO 4: Load current value from atomic_ref with appropriate memory ordering\n    # TODO 5: Compute new value using update_function(current_value)\n    # TODO 6: Attempt compare_and_swap(expected=current, new=computed)\n    # TODO 7: If CAS succeeds, record success metrics and return (True, new_value, attempts)\n    # TODO 8: If CAS fails, call backoff.on_failure() and backoff.wait()\n    # TODO 9: Continue loop unless max_attempts reached\n    # TODO 10: If all attempts failed, record failure metrics and return (False, observed_value, max_attempts)\n    # Hint: Use performance_monitor.record_operation() to track timing and attempt counts\n    # Hint: Handle potential exceptions in update_function gracefully\n    pass\n\ndef protect_and_verify(atomic_ref):\n    \"\"\"\n    Safely load and protect a pointer using hazard pointers.\n    Implements the protect-then-verify pattern to avoid races.\n    \n    Args:\n        atomic_ref: AtomicReference containing pointer to protect\n    \n    Returns:\n        Protected pointer value, or None if pointer became invalid\n    \"\"\"\n    # TODO 1: Get hazard pointer slot for current thread\n    # TODO 2: Loop with retry limit (protect-verify can fail if pointer changes rapidly)\n    # TODO 3: Load current pointer value from atomic_ref\n    # TODO 4: Store loaded pointer in hazard pointer slot (announce protection)\n    # TODO 5: Memory fence to ensure protection is visible before verification\n    # TODO 6: Re-load pointer from atomic_ref to verify it hasn't changed\n    # TODO 7: If pointer values match, protection succeeded - return protected pointer\n    # TODO 8: If values don't match, clear hazard slot and retry from step 3\n    # TODO 9: If retry limit exceeded, clear hazard slot and return None\n    # Hint: The memory fence between protection and verification is critical\n    # Hint: Clear hazard pointer on both success and failure paths to avoid leaks\n    pass\n```\n\n**Invariant Checking Integration:**\n\n```python\nclass StackInvariantChecker(InvariantChecker):\n    \"\"\"Validates Treiber stack specific invariants.\"\"\"\n    \n    def _validate_structure_specific_invariants(self, stack) -> Tuple[bool, Optional[str]]:\n        \"\"\"\n        Check that stack maintains proper linked structure without cycles.\n        \n        Args:\n            stack: TreiberStack instance to validate\n        \n        Returns:\n            (is_valid, violation_description) tuple\n        \"\"\"\n        # TODO 1: Load top pointer atomically from stack\n        # TODO 2: If top is None (empty stack), return (True, None)\n        # TODO 3: Initialize visited set to detect cycles\n        # TODO 4: Traverse chain starting from top pointer\n        # TODO 5: For each node, check if already visited (cycle detection)\n        # TODO 6: Add current node to visited set\n        # TODO 7: Validate node structure (check next field is valid AtomicReference)\n        # TODO 8: Load next pointer and continue traversal\n        # TODO 9: If traversal completes without cycles, return (True, None)\n        # TODO 10: If cycle detected or invalid structure found, return (False, description)\n        # Hint: Use weak references or addresses for cycle detection to avoid affecting GC\n        # Hint: Validate memory addresses are in reasonable ranges\n        pass\n    \nclass QueueInvariantChecker(InvariantChecker):\n    \"\"\"Validates Michael-Scott queue specific invariants.\"\"\"\n    \n    def _validate_structure_specific_invariants(self, queue) -> Tuple[bool, Optional[str]]:\n        \"\"\"\n        Check queue maintains proper head/tail relationship and dummy node.\n        \n        Args:\n            queue: MichaelScottQueue instance to validate\n            \n        Returns:\n            (is_valid, violation_description) tuple\n        \"\"\"\n        # TODO 1: Load head and tail pointers atomically\n        # TODO 2: Verify head is not None (should always have dummy node)\n        # TODO 3: Check that tail is reachable from head by following next pointers\n        # TODO 4: Validate that dummy sentinel node exists and head points to it\n        # TODO 5: Count distance from head to tail - should be reasonable\n        # TODO 6: Check that no cycles exist in the chain\n        # TODO 7: Verify tail points to actual end node (next pointer is None)\n        # TODO 8: If queue appears empty, ensure head and tail point to same dummy node\n        # TODO 9: Return (True, None) if all invariants hold\n        # TODO 10: Return (False, specific_violation) if any check fails\n        # Hint: Distance check helps detect infinite loops in traversal\n        # Hint: Empty queue should have head == tail pointing to dummy node\n        pass\n```\n\n#### Milestone Checkpoint\n\nAfter implementing the error handling and recovery mechanisms:\n\n**Validation Commands:**\n```bash\n# Run stress tests with error injection\npython -m pytest tests/stress_tests.py::test_error_recovery_under_load -v\n\n# Test invariant checking with concurrent operations\npython -m pytest tests/invariant_tests.py::test_concurrent_invariant_validation -v\n\n# Verify backoff strategies reduce contention\npython scripts/benchmark_backoff_effectiveness.py\n\n# Test memory reclamation recovery scenarios\npython -m pytest tests/memory_safety_tests.py::test_hazard_pointer_recovery -v\n```\n\n**Expected Behavior:**\n- Stress tests should complete without hanging or crashing, even with high error injection rates\n- Invariant violations should be detected and reported with specific failure descriptions\n- Backoff strategies should show reduced CAS attempt counts under high contention\n- Memory safety tests should demonstrate proper cleanup after simulated thread failures\n\n**Signs of Problems:**\n- Tests that hang indefinitely indicate livelock or infinite retry loops\n- Segmentation faults suggest memory reclamation race conditions\n- High CPU usage with low throughput indicates excessive backoff or contention\n- Memory usage growth during tests suggests retirement list leaks\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | Diagnosis Method | Fix Strategy |\n|---------|--------------|------------------|--------------|\n| Operations hang under load | Livelock in CAS retry loops | Monitor CAS attempt counts and success rates | Implement exponential backoff with jitter |\n| Intermittent crashes | Use-after-free in memory reclamation | Run with address sanitizer, check hazard pointer coverage | Audit protect/release call pairing |\n| Performance degrades over time | Retirement list growth or contention hotspots | Track memory usage and operation latency trends | Tune scan thresholds, analyze access patterns |\n| Invariant violations reported | Race conditions or ABA problems | Enable detailed logging around violation points | Add tagged pointers or strengthen memory ordering |\n| High CPU usage, low throughput | Excessive spinning in retry loops | Profile CPU usage and identify hot loops | Add backoff, reduce retry limits |\n\nThe error handling and recovery systems form the foundation for building robust lock-free data structures that can handle the unique failure modes of non-blocking concurrent programming while maintaining both correctness and performance under adverse conditions.\n\n\n## Testing Strategy\n\n> **Milestone(s):** All milestones (this section establishes comprehensive verification approaches for concurrent correctness, linearizability validation, and stress testing strategies that apply to every lock-free data structure from atomic operations through complete hash maps)\n\nTesting lock-free data structures presents unique challenges that distinguish it from testing traditional sequential or lock-based concurrent code. The fundamental difficulty lies in the non-deterministic nature of concurrent execution combined with the subtle correctness properties that lock-free algorithms must satisfy. Unlike sequential code where a test either passes or fails deterministically, concurrent tests may pass thousands of times before revealing a race condition on the thousand-and-first execution.\n\n### Mental Model: Testing as Archaeological Investigation\n\nThink of testing lock-free data structures like conducting an archaeological investigation at a busy construction site. Traditional sequential testing is like examining artifacts in a controlled laboratory environment - you can manipulate conditions precisely and reproduce results exactly. Lock-free testing, however, is like trying to study ancient foundations while construction workers operate heavy machinery overhead. You must:\n\n- **Document fleeting evidence**: Capture traces of concurrent operations that happen too quickly to observe directly, just as archaeologists photograph artifacts before they're disturbed\n- **Reconstruct sequences from fragments**: Piece together the order of operations from partial evidence, similar to reconstructing historical events from scattered artifacts  \n- **Verify structural integrity under stress**: Ensure the foundation remains sound even when subjected to the chaos of concurrent \"construction work\" by multiple threads\n- **Detect subtle corruption**: Identify when the \"site\" has been contaminated by improper excavation techniques (race conditions) that leave no obvious visible damage\n\nThis mental model emphasizes that lock-free testing requires patience, systematic documentation, and the ability to infer correctness from indirect evidence rather than direct observation.\n\n### Correctness Properties to Verify\n\nLock-free data structures must satisfy multiple layers of correctness properties, each building upon the previous layer. Understanding these properties is crucial because a data structure might appear to work correctly under casual testing while violating fundamental correctness guarantees that only emerge under specific concurrent conditions.\n\n#### Linearizability: The Gold Standard\n\n**Linearizability** represents the strongest practical correctness condition for concurrent data structures. It requires that every operation appears to take effect atomically at some point between its invocation and response, called the **linearization point**. This property ensures that concurrent operations can be understood as if they executed sequentially in some order that respects the real-time ordering of non-overlapping operations.\n\n> **Critical Insight**: Linearizability is not just about operations being atomic - it's about the existence of a consistent sequential history that all threads would agree upon if they could observe the system from outside.\n\nThe verification process involves several sophisticated techniques:\n\n| Verification Technique | Approach | Strengths | Limitations |\n|----------------------|----------|-----------|-------------|\n| History-based checking | Record all operation invocations and responses, then verify a valid sequential ordering exists | Mathematically rigorous, catches subtle violations | Requires significant overhead, may miss timing-dependent issues |\n| State-based checking | Verify data structure invariants hold at specific observation points | Lightweight, good for continuous monitoring | May miss transient violations, depends on choosing good observation points |\n| Execution replay | Record non-deterministic choices and replay executions deterministically | Enables debugging of specific failure scenarios | Complex to implement, may not cover all possible interleavings |\n| Model checking | Exhaustively explore all possible concurrent executions within bounded parameters | Finds bugs that stress testing might miss | Limited scalability, requires abstract models |\n\nOur `LinearizabilityChecker` component implements history-based verification by maintaining a concurrent log of all operations and periodically verifying that a valid sequential ordering exists:\n\n| Method | Parameters | Returns | Purpose |\n|--------|------------|---------|---------|\n| `record_operation` | `operation: Operation` | `None` | Thread-safely logs an operation invocation or response |\n| `verify_history` | `start_time: float, end_time: float` | `bool, Optional[str]` | Checks if operations in time window have valid sequential ordering |\n| `add_linearization_point` | `operation_id: str, timestamp: float` | `None` | Manually marks when an operation takes effect (for debugging) |\n| `check_real_time_ordering` | `operations: List[Operation]` | `bool, List[str]` | Verifies non-overlapping operations respect real-time order |\n\n> **Decision: History-Based Linearizability Checking**\n> - **Context**: Multiple approaches exist for verifying linearizability, each with different trade-offs between accuracy, performance, and implementation complexity\n> - **Options Considered**: Real-time state checking, execution replay with deterministic scheduling, history-based post-execution analysis\n> - **Decision**: Implement history-based checking with optional real-time monitoring hooks\n> - **Rationale**: History-based checking provides the most rigorous verification while allowing performance-critical code paths to run at full speed. Optional hooks enable debugging specific scenarios without always paying the overhead cost\n> - **Consequences**: Requires careful timestamp management and significant memory overhead for operation logs, but provides the strongest correctness guarantees\n\n#### Progress Guarantees: Ensuring Forward Motion\n\nLock-free data structures must guarantee that at least one thread makes progress within a finite number of steps, even when facing arbitrary interference from other threads. This is stronger than merely avoiding deadlock - it requires active forward progress.\n\nThe verification of progress guarantees involves monitoring several key metrics:\n\n| Progress Metric | Measurement Approach | Warning Threshold | Critical Threshold |\n|----------------|---------------------|------------------|-------------------|\n| Global progress rate | Operations completed per second across all threads | < 50% of baseline | < 10% of baseline |\n| Starvation detection | Maximum time any thread waits without completing operation | > 100x average | > 1000x average |\n| CAS success rate | Successful CAS operations / total CAS attempts | < 30% under high contention | < 5% under any contention |\n| Helping effectiveness | Operations completed via helping / total helped operations | < 80% when helping is active | < 50% when helping is active |\n\nOur progress monitoring integrates with the performance monitoring system to detect both gradual degradation and sudden progress failures:\n\n```\nProgress Guarantee Verification Algorithm:\n1. Initialize per-thread operation counters and timestamps\n2. Start background monitor thread that samples counters every 100ms\n3. For each sampling period, calculate:\n   - Global throughput (total operations / elapsed time)\n   - Per-thread progress rate (operations since last sample / sample interval)\n   - Maximum starvation time (longest time since any thread completed operation)\n4. If any metric exceeds warning threshold for 3 consecutive samples, log warning\n5. If any metric exceeds critical threshold for 1 sample, trigger failure report\n6. Maintain sliding window of historical data for trend analysis\n```\n\n#### Memory Safety: Preventing Use-After-Free\n\nMemory safety in lock-free data structures primarily concerns preventing **use-after-free** errors where one thread deallocates a node while another thread still holds a pointer to it. Unlike garbage-collected languages, manual memory management creates a fundamental tension between performance (reclaiming memory quickly) and safety (ensuring no dangling pointers).\n\nThe verification strategy combines several complementary approaches:\n\n| Safety Verification Technique | Detection Method | Coverage | Performance Impact |\n|------------------------------|------------------|----------|-------------------|\n| Hazard pointer validation | Check all protected pointers reference valid memory | All hazard-protected accesses | Medium - requires global scanning |\n| Address sanitizer integration | Detect use-after-free at memory access level | All memory accesses | High - significant runtime overhead |\n| Reference counting validation | Verify reference counts match actual pointer usage | All shared objects | Low - simple integer checks |\n| Retirement list monitoring | Ensure retired nodes aren't accessed before reclamation | All memory reclamation | Low - timestamp comparison |\n\n> **Design Insight**: Memory safety verification must be comprehensive because even a single use-after-free error can cause data corruption, crashes, or security vulnerabilities. The investment in thorough safety checking pays dividends in production reliability.\n\n#### Functional Correctness: Data Structure Semantics\n\nBeyond the generic properties of linearizability and progress guarantees, each data structure must satisfy its specific functional requirements. These properties define what the data structure is supposed to do, not just how safely it does it.\n\n**Stack Functional Properties:**\n| Property | Verification Method | Test Cases |\n|----------|-------------------|------------|\n| LIFO ordering | Push sequence [A,B,C], verify pop sequence [C,B,A] | Single-threaded sequential, concurrent interleaved |\n| Empty stack behavior | Pop from empty stack returns None/null consistently | Concurrent pops on empty stack, pop after all items removed |\n| Push-pop symmetry | Every pushed item eventually poppable unless stack destroyed | Balanced push/pop operations, stress test with random operations |\n| Count consistency | Number of successful pushes minus successful pops equals current size | Concurrent operations with final size verification |\n\n**Queue Functional Properties:**\n| Property | Verification Method | Test Cases |\n|----------|-------------------|------------|\n| FIFO ordering | Enqueue sequence [A,B,C], verify dequeue sequence [A,B,C] | Single-threaded sequential, concurrent interleaved |\n| Empty queue behavior | Dequeue from empty queue returns None/null consistently | Concurrent dequeues on empty queue, dequeue after all items removed |\n| Enqueue-dequeue symmetry | Every enqueued item eventually dequeueable unless queue destroyed | Balanced enqueue/dequeue operations, producer-consumer scenarios |\n| Fairness under contention | No thread permanently starved from successful operations | Multiple producers and consumers with operation counting |\n\n**Hash Map Functional Properties:**\n| Property | Verification Method | Test Cases |\n|----------|-------------------|------------|\n| Key-value association | Insert (K,V), verify lookup(K) returns V | Single key operations, multiple keys, key updates |\n| Deletion completeness | After delete(K), lookup(K) returns None/null | Delete followed by lookup, delete non-existent keys |\n| Resize transparency | Operations work identically before and during resize | Concurrent operations during triggered resize |\n| Load factor maintenance | Resize triggered when load factor exceeds threshold | Monitor load factor during heavy insertion |\n\n### Concurrency Testing Techniques\n\nTesting concurrent correctness requires specialized techniques that go far beyond traditional unit testing. The challenge lies in exploring the vast space of possible thread interleavings while maintaining reasonable test execution times and diagnostic capabilities.\n\n#### Stress Testing: Controlled Chaos\n\n**Stress testing** subjects the data structure to high levels of concurrent contention to expose race conditions and performance bottlenecks that only emerge under extreme load. The key principle is controlled chaos - generating enough concurrent activity to stress the system while maintaining enough control to diagnose problems when they occur.\n\nOur stress testing framework implements several complementary strategies:\n\n| Stress Test Type | Thread Configuration | Operation Pattern | Duration Strategy |\n|-----------------|---------------------|------------------|------------------|\n| High contention | 2x CPU cores | All threads target same hotspots | Fixed time window (30-300 seconds) |\n| Mixed workload | 50% readers, 50% writers | Realistic operation mix | Operation count target (1M operations) |\n| Burst contention | Periodic thread spawning | Waves of activity followed by quiet periods | Multiple burst cycles |\n| Memory pressure | Background memory allocation | Operations under low memory conditions | Until memory exhaustion or timeout |\n\nThe stress test implementation follows this algorithm:\n\n```\nStress Test Execution Algorithm:\n1. Initialize shared data structure and monitoring infrastructure\n2. Create thread pool with configured number of worker threads\n3. For each worker thread:\n   a. Assign operation type distribution (read/write ratios)\n   b. Initialize per-thread random number generator with unique seed\n   c. Start thread executing operation loop with configured patterns\n4. Start monitoring thread to collect performance metrics\n5. Run for configured duration or operation count\n6. Signal all threads to complete current operations and terminate\n7. Join all threads and collect final statistics\n8. Analyze results for correctness violations and performance anomalies\n9. Generate detailed report with operation counts, timing histograms, and failure analysis\n```\n\n> **Decision: Multi-Phase Stress Testing**\n> - **Context**: Single-pattern stress tests often miss bugs that only occur during specific transition scenarios or under particular load characteristics\n> - **Options Considered**: Continuous uniform load, random operation patterns, scenario-based testing with specific sequences\n> - **Decision**: Implement multi-phase testing that cycles through different load patterns and operation mixes within a single test run\n> - **Rationale**: Different bugs manifest under different conditions - contention bugs need high thread counts, memory races need allocation pressure, progress violations need specific timing. Multi-phase testing maximizes coverage within reasonable test execution time\n> - **Consequences**: More complex test infrastructure and result analysis, but significantly higher bug detection rate and better coverage of real-world usage patterns\n\n#### Model Checking: Exhaustive Exploration\n\n**Model checking** provides a mathematically rigorous approach to concurrent correctness by exhaustively exploring all possible thread interleavings within bounded parameters. While computationally expensive, model checking can find subtle bugs that might never occur during stress testing.\n\nOur model checking integration focuses on bounded verification of critical scenarios:\n\n| Model Checking Scope | Thread Limit | Operation Limit | State Space Size | Typical Runtime |\n|---------------------|--------------|-----------------|------------------|----------------|\n| Core algorithm verification | 3 threads | 5 operations each | ~10^6 states | 1-10 minutes |\n| Edge case exploration | 2 threads | 10 operations each | ~10^7 states | 10-60 minutes |\n| Regression testing | 4 threads | 3 operations each | ~10^5 states | 1-5 minutes |\n| Deep bug investigation | 2 threads | 20 operations each | ~10^8 states | 1-8 hours |\n\nThe model checker implementation abstracts the data structure operations into a finite state machine and systematically explores all possible execution orders:\n\n```\nModel Checking Algorithm:\n1. Define abstract state space representing data structure and thread states\n2. Identify atomic transitions corresponding to each operation step\n3. Initialize worklist with initial state\n4. While worklist not empty:\n   a. Dequeue next state to explore\n   b. For each possible transition from current state:\n      i. Compute successor state after transition\n      ii. Check if successor violates any correctness properties\n      iii. If violation found, generate counterexample trace\n      iv. If successor is new, add to worklist for further exploration\n   c. Mark current state as fully explored\n5. If worklist empty without violations, algorithm is correct within bounds\n6. Generate coverage report showing percentage of reachable states explored\n```\n\n#### Randomized Testing: Property-Based Verification\n\n**Randomized testing** generates large numbers of random operation sequences and verifies that correctness properties hold for each sequence. This approach complements stress testing by exploring diverse scenarios rather than just high-contention cases.\n\nOur randomized testing framework implements property-based verification:\n\n| Property Category | Verification Method | Sample Properties |\n|------------------|-------------------|------------------|\n| Functional properties | Operation sequence modeling | \"Every pushed item is eventually poppable\", \"FIFO order preserved\" |\n| Safety properties | State invariant checking | \"No null pointer dereferences\", \"All nodes reachable from valid pointers\" |\n| Liveness properties | Progress monitoring | \"Operations eventually complete\", \"No permanent starvation\" |\n| Performance properties | Timing analysis | \"Average operation latency < threshold\", \"Throughput scales with threads\" |\n\nThe randomized test generator creates operation sequences using configurable probability distributions:\n\n```\nRandomized Test Generation Algorithm:\n1. Initialize random number generator with configurable seed for reproducibility\n2. Generate operation sequence of specified length using probability distributions:\n   - Operation type (push/pop for stack, insert/lookup/delete for hash map)\n   - Operation timing (immediate, delayed, synchronized)\n   - Data values (unique, duplicate, special values like null)\n3. Execute operation sequence across multiple threads with controlled interleavings\n4. After each operation, verify all applicable properties hold\n5. If property violation detected:\n   a. Record minimal reproduction case by binary search on operation sequence\n   b. Generate detailed failure report with operation history and final state\n6. Repeat with different random seeds until confidence threshold reached\n7. Analyze aggregate results for patterns in failures or performance degradation\n```\n\n### Milestone Verification Checkpoints\n\nEach milestone in the lock-free data structures project requires specific verification approaches tailored to the components being implemented. These checkpoints provide concrete acceptance criteria and debugging guidance for each development phase.\n\n#### Milestone 1: Atomic Operations Foundation\n\nThe atomic operations milestone establishes the fundamental building blocks for all subsequent lock-free algorithms. Verification focuses on correctness of individual atomic primitives and proper memory ordering semantics.\n\n**Critical Verification Areas:**\n\n| Component | Verification Focus | Expected Behavior | Failure Indicators |\n|-----------|-------------------|------------------|-------------------|\n| Compare-and-swap wrapper | Return value correctness, memory ordering | CAS returns (True, old_value) on success, (False, current_value) on failure | Incorrect return values, lost updates, unexpected values |\n| Memory ordering modes | Proper compiler/CPU barrier generation | Operations respect acquire/release semantics, no reordering violations | Reordering visible to other threads, stale reads after barriers |\n| ABA problem demonstration | Reliable reproduction of ABA scenario | Test shows incorrect CAS success due to pointer reuse | Test passes when it should fail, inconsistent reproduction |\n| Atomic counter operations | Linearizable increment/decrement | Counter value equals expected after all operations complete | Lost increments, counter value inconsistencies |\n\n**Acceptance Test Procedures:**\n\n```\nAtomic Operations Verification Steps:\n1. Single-threaded correctness testing:\n   a. Verify CAS wrapper returns correct success/failure indicators\n   b. Test all memory ordering modes compile and execute without errors\n   c. Confirm atomic counter operations produce expected sequential results\n   d. Validate load/store operations respect memory ordering constraints\n\n2. Multi-threaded stress testing:\n   a. Launch N threads each performing 1000 CAS operations on shared variable\n   b. Verify final variable value matches expected result from sequential execution\n   c. Confirm no thread observes intermediate inconsistent states\n   d. Test counter incremented by multiple threads produces exact expected total\n\n3. ABA problem reproduction:\n   a. Create scenario with pointer reuse between CAS read and update\n   b. Demonstrate that naive CAS incorrectly succeeds\n   c. Show that tagged pointer solution correctly detects and prevents ABA\n   d. Verify test fails reliably without ABA protection, passes reliably with protection\n\n4. Memory ordering validation:\n   a. Create test with release store followed by acquire load on different threads\n   b. Verify acquire thread observes all writes that happened-before release\n   c. Test relaxed ordering allows observable reordering where appropriate\n   d. Confirm sequential consistency mode prevents all observable reordering\n```\n\n> **Common Checkpoint Pitfall**: Many implementations pass single-threaded tests but fail under concurrent stress. Always include multi-threaded verification even for atomic primitives.\n\n#### Milestone 2: Lock-free Stack Implementation\n\nThe Treiber stack milestone implements the first complete lock-free data structure, requiring verification of both functional correctness and linearizability properties.\n\n**Critical Verification Areas:**\n\n| Component | Verification Focus | Expected Behavior | Failure Indicators |\n|-----------|-------------------|------------------|-------------------|\n| Push operation | CAS loop correctness, ABA handling | Items pushed in any order are all eventually poppable | Push operations hang, items disappear, memory corruption |\n| Pop operation | Empty stack handling, linearization point | Pop returns items in LIFO order, handles empty gracefully | Pop from empty crashes, wrong order, null pointer access |\n| Tagged pointer solution | ABA prevention effectiveness | CAS fails appropriately when pointer reused with different tag | Incorrect CAS success, data structure corruption, inconsistent state |\n| Concurrent correctness | Linearizability under contention | All push/pop operations appear atomic at some linearization point | Lost operations, duplicate returns, non-atomic behavior |\n\n**Acceptance Test Procedures:**\n\n```\nStack Verification Steps:\n1. Sequential functionality testing:\n   a. Push items [1,2,3], verify pop sequence returns [3,2,1]\n   b. Push and pop interleaved, verify LIFO order maintained\n   c. Pop from empty stack returns None/null without crashing\n   d. Push after empty, verify stack recovers normal operation\n\n2. Concurrent stress testing:\n   a. Multiple threads pushing unique values simultaneously\n   b. Multiple threads popping while others push\n   c. Balanced push/pop workload with final size verification\n   d. Imbalanced workload testing stack growth and shrinkage\n\n3. ABA problem prevention:\n   a. Create test scenario that triggers ABA without protection\n   b. Verify tagged pointer solution prevents ABA corruption\n   c. Test under high contention where ABA most likely to occur\n   d. Validate stack maintains structural integrity throughout test\n\n4. Linearizability verification:\n   a. Record all push/pop operation invocations and responses\n   b. Verify existence of valid sequential history respecting LIFO order\n   c. Test with multiple concurrent threads and random operation timing\n   d. Analyze operation history for linearization point consistency\n```\n\n**Performance Benchmark Requirements:**\n- Stack operations should complete within 100ns on average under low contention\n- Throughput should scale linearly up to number of CPU cores\n- Memory usage should grow/shrink appropriately with stack size\n- No memory leaks after balanced push/pop operations\n\n#### Milestone 3: Michael-Scott Queue Implementation\n\nThe queue milestone introduces more complex coordination with dual head/tail pointers and helping mechanisms, requiring sophisticated verification of FIFO ordering and progress guarantees.\n\n**Critical Verification Areas:**\n\n| Component | Verification Focus | Expected Behavior | Failure Indicators |\n|-----------|-------------------|------------------|-------------------|\n| Enqueue operation | Tail advancement, helping protocol | Items enqueued appear in FIFO order when dequeued | Enqueue operations hang, incorrect ordering, tail pointer corruption |\n| Dequeue operation | Head advancement, empty queue handling | Dequeue returns oldest item, handles empty queue gracefully | Dequeue hangs on empty, wrong items returned, head pointer issues |\n| Helping mechanism | Progress guarantee under interference | Threads help advance lagging tail pointer when detected | Operations permanently blocked, progress violations, helping ineffective |\n| Sentinel node management | Dummy node lifecycle | Dummy node simplifies empty queue, never deallocated inappropriately | Dummy node corruption, improper deallocation, empty queue crashes |\n\n**Acceptance Test Procedures:**\n\n```\nQueue Verification Steps:\n1. Sequential FIFO testing:\n   a. Enqueue items [1,2,3], verify dequeue sequence returns [1,2,3]\n   b. Interleaved enqueue/dequeue operations maintain FIFO order\n   c. Dequeue from empty queue returns None/null consistently\n   d. Enqueue after empty restores normal FIFO operation\n\n2. Producer-consumer stress testing:\n   a. Multiple producer threads enqueuing unique timestamped items\n   b. Multiple consumer threads dequeuing and verifying FIFO order\n   c. Balanced producer/consumer rates with steady-state verification\n   d. Imbalanced rates testing queue growth under producer pressure\n\n3. Helping mechanism verification:\n   a. Create scenario where tail pointer lags behind actual tail\n   b. Verify dequeue operations detect and correct lagging tail\n   c. Test helping effectiveness under high contention conditions\n   d. Ensure helping doesn't interfere with normal operation progress\n\n4. Empty queue edge case testing:\n   a. Multiple threads dequeue from empty queue simultaneously\n   b. Enqueue while other threads blocked on empty dequeue\n   c. Rapid empty/non-empty transitions with concurrent operations\n   d. Verify dummy sentinel node remains stable throughout transitions\n```\n\n#### Milestone 4: Hazard Pointers Integration\n\nThe hazard pointers milestone focuses on memory safety verification, ensuring that the reclamation scheme prevents use-after-free errors while maintaining performance and progress guarantees.\n\n**Critical Verification Areas:**\n\n| Component | Verification Focus | Expected Behavior | Failure Indicators |\n|-----------|-------------------|------------------|-------------------|\n| Protection protocol | Hazard pointer correctness | Nodes protected before access, released after completion | Use-after-free errors, protection race conditions, dangling pointers |\n| Retirement list management | Safe reclamation timing | Nodes retired safely, reclaimed when no longer protected | Memory leaks, premature reclamation, unbounded retirement growth |\n| Scanning algorithm | Global hazard survey | Scan identifies all protected nodes correctly | False reclamation, missed protections, scan algorithm errors |\n| Integration correctness | Data structure operation safety | Stack/queue operations remain correct with hazard pointer protection | Functional regressions, protection overhead breaks algorithms |\n\n**Acceptance Test Procedures:**\n\n```\nHazard Pointers Verification Steps:\n1. Memory safety validation:\n   a. Use address sanitizer to detect use-after-free errors\n   b. Stress test with high allocation/deallocation rates\n   c. Verify no dangling pointer access under any operation sequence\n   d. Test protection protocol under interrupt and preemption scenarios\n\n2. Retirement and reclamation testing:\n   a. Monitor retirement list size under sustained operation load\n   b. Verify scan threshold triggers reclamation appropriately\n   c. Test reclamation completeness - all unprotected nodes eventually freed\n   d. Validate no memory leaks after extended operation sequences\n\n3. Integration verification:\n   a. Re-run all stack and queue tests with hazard pointer protection enabled\n   b. Verify functional correctness remains identical\n   c. Measure performance overhead of hazard pointer operations\n   d. Test thread creation/destruction with proper cleanup\n\n4. Concurrency stress testing:\n   a. High contention scenario with frequent protect/release cycles\n   b. Mixed read/write workload with varying protection patterns\n   c. Thread lifecycle testing with proper hazard pointer cleanup\n   d. Recovery testing after abnormal thread termination\n```\n\n#### Milestone 5: Lock-free Hash Map Completion\n\nThe hash map milestone represents the most complex data structure, requiring verification of correct hash distribution, split-ordered list maintenance, and incremental resizing under concurrent operations.\n\n**Critical Verification Areas:**\n\n| Component | Verification Focus | Expected Behavior | Failure Indicators |\n|-----------|-------------------|------------------|-------------------|\n| Bucket operations | Insert/lookup/delete correctness | Key-value operations work correctly within buckets | Wrong values returned, keys lost, bucket corruption |\n| Split-ordered list maintenance | Logical ordering preservation | Hash ordering maintained across bucket splits | Incorrect key placement, search failures, list corruption |\n| Incremental resizing | Transparent capacity expansion | Resize occurs without blocking operations, maintains correctness | Resize hangs, data loss during resize, incorrect post-resize state |\n| Load factor management | Automatic resize triggering | Resize triggered at appropriate load factor thresholds | Resize too early/late, load calculation errors, infinite resize loops |\n\n**Acceptance Test Procedures:**\n\n```\nHash Map Verification Steps:\n1. Basic key-value operations:\n   a. Insert unique keys, verify all retrievable via lookup\n   b. Update existing keys, verify new values returned\n   c. Delete keys, verify lookup returns None/null afterward\n   d. Test with various key types and hash distributions\n\n2. Concurrent operations stress testing:\n   a. Multiple threads inserting unique keys simultaneously\n   b. Mixed insert/lookup/delete workload from multiple threads\n   c. Read-heavy workload with occasional writes\n   d. Write-heavy workload with occasional reads\n\n3. Incremental resize verification:\n   a. Monitor hash map through resize trigger and completion\n   b. Verify all existing key-value pairs remain accessible during resize\n   c. Test concurrent operations during resize process\n   d. Validate final state after resize completion\n\n4. Split-ordered list correctness:\n   a. Verify logical hash ordering maintained in physical list\n   b. Test bucket initialization with proper sentinel node placement\n   c. Validate reverse bit ordering calculations\n   d. Check split operation correctness under various hash distributions\n\n5. Performance and scalability testing:\n   a. Measure throughput scaling with thread count\n   b. Verify load factor maintenance within expected bounds\n   c. Test hash distribution quality with various key patterns\n   d. Benchmark against reference hash map implementations\n```\n\n> **Integration Testing Recommendation**: After completing all milestones, run a comprehensive integration test that uses all data structures together in a realistic application scenario. This often reveals interaction bugs that unit tests miss.\n\n### Implementation Guidance\n\nThe testing infrastructure for lock-free data structures requires sophisticated tooling to handle the unique challenges of concurrent correctness verification. This section provides complete, production-ready testing frameworks and detailed guidance for implementing each verification technique.\n\n#### Technology Recommendations\n\n| Testing Component | Simple Option | Advanced Option |\n|------------------|---------------|-----------------|\n| Stress Testing Framework | Threading module with shared counters | Custom thread pool with CPU affinity and scheduling control |\n| Randomized Testing | Random module with fixed seeds | Property-based testing framework (Hypothesis for Python) |\n| Performance Monitoring | Time measurements with statistics module | High-resolution profiling with perf integration |\n| Memory Safety Validation | Manual reference tracking | Address Sanitizer integration with custom allocators |\n| Linearizability Checking | Operation logging with post-analysis | Real-time linearizability verification with efficient algorithms |\n\n#### Recommended Testing Module Structure\n\n```\nproject-root/\n  tests/\n    unit/\n      test_atomic_operations.py     ← Basic atomic primitive tests\n      test_memory_ordering.py       ← Memory ordering and barrier tests\n      test_cas_operations.py        ← Compare-and-swap correctness tests\n    integration/\n      test_stack_correctness.py     ← Treiber stack functional tests\n      test_queue_correctness.py     ← Michael-Scott queue functional tests\n      test_hashmap_correctness.py   ← Hash map functional tests\n      test_hazard_pointers.py       ← Memory reclamation tests\n    stress/\n      stress_test_framework.py      ← Generic concurrent stress testing\n      stress_stack_operations.py    ← Stack-specific stress tests\n      stress_queue_operations.py    ← Queue-specific stress tests\n      stress_hashmap_operations.py  ← Hash map-specific stress tests\n    verification/\n      linearizability_checker.py    ← History-based linearizability verification\n      progress_monitor.py           ← Progress guarantee validation\n      memory_safety_checker.py      ← Use-after-free detection\n      property_verifier.py          ← Property-based testing framework\n    benchmarks/\n      performance_benchmarks.py     ← Throughput and latency measurements\n      scalability_tests.py          ← Multi-core scaling analysis\n      contention_analysis.py        ← Lock contention vs lock-free comparison\n    utils/\n      test_harness.py              ← Common testing utilities and fixtures\n      random_generators.py         ← Deterministic random data generation\n      thread_coordination.py       ← Thread synchronization utilities for tests\n```\n\n#### Infrastructure Starter Code\n\n**Complete Linearizability Checker Implementation:**\n\n```python\nimport threading\nimport time\nfrom typing import List, Dict, Optional, Tuple, Any\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport bisect\n\nclass OperationType(Enum):\n    PUSH = \"push\"\n    POP = \"pop\"\n    ENQUEUE = \"enqueue\"\n    DEQUEUE = \"dequeue\"\n    INSERT = \"insert\"\n    LOOKUP = \"lookup\"\n    DELETE = \"delete\"\n\n@dataclass\nclass Operation:\n    type: OperationType\n    timestamp: float\n    thread_id: int\n    invocation: bool  # True for start, False for completion\n    args: Tuple[Any, ...] = ()\n    result: Any = None\n    operation_id: str = \"\"\n\nclass LinearizabilityChecker:\n    def __init__(self):\n        self.operations: List[Operation] = []\n        self.operation_lock = threading.Lock()\n        self.operation_counter = 0\n        self.verification_cache: Dict[str, bool] = {}\n    \n    def record_operation(self, op: Operation) -> None:\n        \"\"\"Thread-safely record an operation invocation or response.\"\"\"\n        with self.operation_lock:\n            if op.invocation:\n                self.operation_counter += 1\n                op.operation_id = f\"{op.thread_id}_{self.operation_counter}\"\n            \n            # Insert operation in timestamp order for efficient analysis\n            bisect.insort(self.operations, op, key=lambda x: x.timestamp)\n    \n    def verify_history(self, start_time: float, end_time: float) -> Tuple[bool, Optional[str]]:\n        \"\"\"Check if operations in time window have valid sequential ordering.\"\"\"\n        # Filter operations in time window\n        relevant_ops = [op for op in self.operations \n                       if start_time <= op.timestamp <= end_time]\n        \n        # Group operations by operation_id to pair invocations with responses\n        operation_pairs = self._pair_invocations_responses(relevant_ops)\n        \n        # Try to find valid sequential ordering\n        return self._check_sequential_consistency(operation_pairs)\n    \n    def _pair_invocations_responses(self, ops: List[Operation]) -> List[Tuple[Operation, Operation]]:\n        \"\"\"Match operation invocations with their responses.\"\"\"\n        pending_ops = {}\n        completed_ops = []\n        \n        for op in ops:\n            if op.invocation:\n                pending_ops[op.operation_id] = op\n            else:\n                if op.operation_id in pending_ops:\n                    start_op = pending_ops.pop(op.operation_id)\n                    completed_ops.append((start_op, op))\n        \n        return completed_ops\n    \n    def _check_sequential_consistency(self, operation_pairs: List[Tuple[Operation, Operation]]) -> Tuple[bool, Optional[str]]:\n        \"\"\"Verify that a valid sequential ordering exists for the operations.\"\"\"\n        # This is a simplified version - full implementation would use\n        # sophisticated graph algorithms to check all possible orderings\n        \n        # For demonstration, check basic FIFO/LIFO properties\n        stack_operations = []\n        queue_operations = []\n        \n        for start_op, end_op in operation_pairs:\n            if start_op.type in [OperationType.PUSH, OperationType.POP]:\n                stack_operations.append((start_op, end_op))\n            elif start_op.type in [OperationType.ENQUEUE, OperationType.DEQUEUE]:\n                queue_operations.append((start_op, end_op))\n        \n        # Verify stack LIFO property\n        if not self._verify_stack_lifo(stack_operations):\n            return False, \"Stack LIFO ordering violation detected\"\n        \n        # Verify queue FIFO property  \n        if not self._verify_queue_fifo(queue_operations):\n            return False, \"Queue FIFO ordering violation detected\"\n        \n        return True, None\n    \n    def _verify_stack_lifo(self, operations: List[Tuple[Operation, Operation]]) -> bool:\n        \"\"\"Verify stack operations respect LIFO ordering.\"\"\"\n        # Simplified LIFO checking - track push/pop sequence\n        stack_state = []\n        \n        # Sort operations by linearization point (completion timestamp)\n        sorted_ops = sorted(operations, key=lambda x: x[1].timestamp)\n        \n        for start_op, end_op in sorted_ops:\n            if start_op.type == OperationType.PUSH:\n                stack_state.append(start_op.args[0])  # Push value onto model stack\n            elif start_op.type == OperationType.POP:\n                if not stack_state:\n                    # Pop from empty stack should return None\n                    if end_op.result is not None:\n                        return False\n                else:\n                    expected_value = stack_state.pop()\n                    if end_op.result != expected_value:\n                        return False\n        \n        return True\n    \n    def _verify_queue_fifo(self, operations: List[Tuple[Operation, Operation]]) -> bool:\n        \"\"\"Verify queue operations respect FIFO ordering.\"\"\"\n        # Simplified FIFO checking - track enqueue/dequeue sequence\n        queue_state = []\n        \n        # Sort operations by linearization point (completion timestamp)\n        sorted_ops = sorted(operations, key=lambda x: x[1].timestamp)\n        \n        for start_op, end_op in sorted_ops:\n            if start_op.type == OperationType.ENQUEUE:\n                queue_state.append(start_op.args[0])  # Add value to model queue\n            elif start_op.type == OperationType.DEQUEUE:\n                if not queue_state:\n                    # Dequeue from empty queue should return None\n                    if end_op.result is not None:\n                        return False\n                else:\n                    expected_value = queue_state.pop(0)  # Remove from front\n                    if end_op.result != expected_value:\n                        return False\n        \n        return True\n```\n\n**Complete Stress Testing Framework:**\n\n```python\nimport threading\nimport time\nimport random\nimport statistics\nfrom typing import List, Dict, Callable, Any, Optional\nfrom concurrent.futures import ThreadPoolExecutor\nfrom dataclasses import dataclass, field\n\n@dataclass\nclass StressTestConfig:\n    num_threads: int = 8\n    operations_per_thread: int = 1000\n    test_duration_seconds: Optional[float] = None\n    operation_mix: Dict[str, float] = field(default_factory=lambda: {\"read\": 0.7, \"write\": 0.3})\n    contention_level: str = \"high\"  # \"low\", \"medium\", \"high\"\n    backoff_strategy: str = \"exponential\"  # \"none\", \"linear\", \"exponential\"\n\n@dataclass\nclass StressTestResults:\n    total_operations: int = 0\n    successful_operations: int = 0\n    failed_operations: int = 0\n    total_duration: float = 0.0\n    throughput_ops_per_sec: float = 0.0\n    average_latency_ms: float = 0.0\n    p95_latency_ms: float = 0.0\n    p99_latency_ms: float = 0.0\n    cas_success_rate: float = 0.0\n    thread_results: List[Dict[str, Any]] = field(default_factory=list)\n\nclass StressTestFramework:\n    def __init__(self, data_structure: Any, config: StressTestConfig):\n        self.data_structure = data_structure\n        self.config = config\n        self.results = StressTestResults()\n        self.operation_times: List[float] = []\n        self.cas_attempts = 0\n        self.cas_successes = 0\n        self.results_lock = threading.Lock()\n        self.should_stop = threading.Event()\n    \n    def stress_test_data_structure(self) -> StressTestResults:\n        \"\"\"Execute comprehensive stress test and return detailed results.\"\"\"\n        print(f\"Starting stress test with {self.config.num_threads} threads...\")\n        \n        start_time = time.time()\n        \n        # Start monitoring thread\n        monitor_thread = threading.Thread(target=self._monitor_progress, daemon=True)\n        monitor_thread.start()\n        \n        # Execute stress test with thread pool\n        with ThreadPoolExecutor(max_workers=self.config.num_threads) as executor:\n            # Submit worker tasks\n            futures = []\n            for thread_id in range(self.config.num_threads):\n                future = executor.submit(self._worker_thread, thread_id)\n                futures.append(future)\n            \n            # Wait for completion or timeout\n            if self.config.test_duration_seconds:\n                time.sleep(self.config.test_duration_seconds)\n                self.should_stop.set()\n            \n            # Collect results from all threads\n            for i, future in enumerate(futures):\n                try:\n                    thread_result = future.result(timeout=30)\n                    self.results.thread_results.append({\n                        \"thread_id\": i,\n                        \"operations\": thread_result[\"operations\"],\n                        \"successes\": thread_result[\"successes\"],\n                        \"failures\": thread_result[\"failures\"],\n                        \"avg_latency\": thread_result[\"avg_latency\"]\n                    })\n                except Exception as e:\n                    print(f\"Thread {i} failed: {e}\")\n        \n        end_time = time.time()\n        \n        # Calculate aggregate results\n        self._calculate_final_results(start_time, end_time)\n        \n        return self.results\n    \n    def _worker_thread(self, thread_id: int) -> Dict[str, Any]:\n        \"\"\"Execute operations for a single worker thread.\"\"\"\n        operations_completed = 0\n        successful_operations = 0\n        failed_operations = 0\n        thread_operation_times = []\n        \n        # Initialize thread-local random generator for reproducibility\n        thread_random = random.Random(42 + thread_id)\n        \n        while not self.should_stop.is_set():\n            # Stop if reached operation limit and no duration specified\n            if (self.config.test_duration_seconds is None and \n                operations_completed >= self.config.operations_per_thread):\n                break\n            \n            # Select operation type based on configured mix\n            operation_type = self._select_operation_type(thread_random)\n            \n            # Execute operation with timing\n            start_time = time.perf_counter()\n            success = self._execute_operation(operation_type, thread_random, thread_id)\n            end_time = time.perf_counter()\n            \n            # Record results\n            operation_time = (end_time - start_time) * 1000  # Convert to milliseconds\n            thread_operation_times.append(operation_time)\n            \n            operations_completed += 1\n            if success:\n                successful_operations += 1\n            else:\n                failed_operations += 1\n            \n            # Apply backoff if configured\n            self._apply_backoff(success)\n        \n        # Update global results thread-safely\n        with self.results_lock:\n            self.results.total_operations += operations_completed\n            self.results.successful_operations += successful_operations\n            self.results.failed_operations += failed_operations\n            self.operation_times.extend(thread_operation_times)\n        \n        return {\n            \"operations\": operations_completed,\n            \"successes\": successful_operations,\n            \"failures\": failed_operations,\n            \"avg_latency\": statistics.mean(thread_operation_times) if thread_operation_times else 0\n        }\n    \n    def _execute_operation(self, operation_type: str, rng: random.Random, thread_id: int) -> bool:\n        \"\"\"Execute a single operation on the data structure.\"\"\"\n        try:\n            if hasattr(self.data_structure, 'push') and operation_type == \"push\":\n                value = rng.randint(1, 1000000) + thread_id * 1000000  # Unique values per thread\n                self.data_structure.push(value)\n                return True\n            elif hasattr(self.data_structure, 'pop') and operation_type == \"pop\":\n                result = self.data_structure.pop()\n                return result is not None  # Success if we got a value\n            elif hasattr(self.data_structure, 'enqueue') and operation_type == \"enqueue\":\n                value = rng.randint(1, 1000000) + thread_id * 1000000\n                self.data_structure.enqueue(value)\n                return True\n            elif hasattr(self.data_structure, 'dequeue') and operation_type == \"dequeue\":\n                result = self.data_structure.dequeue()\n                return result is not None\n            elif hasattr(self.data_structure, 'insert') and operation_type == \"insert\":\n                key = f\"key_{rng.randint(1, 10000)}_{thread_id}\"\n                value = f\"value_{rng.randint(1, 10000)}\"\n                return self.data_structure.insert(key, value)\n            elif hasattr(self.data_structure, 'lookup') and operation_type == \"lookup\":\n                key = f\"key_{rng.randint(1, 10000)}_{thread_id}\"\n                result = self.data_structure.lookup(key)\n                return True  # Lookup always succeeds (may return None)\n            else:\n                return False\n        except Exception as e:\n            print(f\"Operation {operation_type} failed: {e}\")\n            return False\n```\n\n#### Core Testing Logic Skeletons\n\n**Milestone Verification Template:**\n\n```python\ndef verify_milestone_1_atomic_operations():\n    \"\"\"Verify atomic operations foundation correctness and performance.\"\"\"\n    # TODO 1: Initialize atomic reference with test value\n    # TODO 2: Create multiple threads performing CAS operations\n    # TODO 3: Verify final value matches expected result from sequential execution\n    # TODO 4: Test all memory ordering modes compile and execute correctly\n    # TODO 5: Demonstrate ABA problem with naive CAS implementation\n    # TODO 6: Show tagged pointer solution prevents ABA corruption\n    # TODO 7: Measure CAS success rate under high contention\n    # Hint: Use barriers to synchronize thread start for maximum contention\n\ndef verify_milestone_2_treiber_stack():\n    \"\"\"Verify Treiber stack functional correctness and linearizability.\"\"\"\n    # TODO 1: Test single-threaded LIFO ordering with push/pop sequence\n    # TODO 2: Verify empty stack pop returns None without crashing\n    # TODO 3: Create concurrent stress test with multiple pushers and poppers\n    # TODO 4: Record all operations and verify linearizability using checker\n    # TODO 5: Test ABA prevention under high contention scenarios\n    # TODO 6: Measure stack performance vs mutex-based reference implementation\n    # TODO 7: Validate memory safety with address sanitizer integration\n    # Hint: Use unique values per thread to track operation correctness\n\ndef verify_milestone_3_michael_scott_queue():\n    \"\"\"Verify Michael-Scott queue FIFO ordering and helping mechanism.\"\"\"\n    # TODO 1: Test single-threaded FIFO ordering with enqueue/dequeue sequence\n    # TODO 2: Verify empty queue dequeue returns None consistently\n    # TODO 3: Create producer-consumer stress test with FIFO verification\n    # TODO 4: Test helping mechanism by creating scenarios with lagging tail\n    # TODO 5: Verify dummy sentinel node remains stable through all operations\n    # TODO 6: Record operations and check linearizability with proper FIFO semantics\n    # TODO 7: Measure queue throughput under balanced and imbalanced workloads\n    # Hint: Use timestamped values to verify FIFO ordering across threads\n\ndef verify_milestone_4_hazard_pointers():\n    \"\"\"Verify hazard pointer memory safety and reclamation correctness.\"\"\"\n    # TODO 1: Integrate hazard pointers with existing stack and queue implementations\n    # TODO 2: Run all previous tests with hazard pointer protection enabled\n    # TODO 3: Verify no functional regressions introduced by memory protection\n    # TODO 4: Test retirement list management and scan threshold behavior\n    # TODO 5: Create high memory pressure test with frequent allocation/deallocation\n    # TODO 6: Verify thread cleanup releases all hazard pointers and retired nodes\n    # TODO 7: Use address sanitizer to detect any use-after-free errors\n    # Hint: Monitor retirement list size to ensure it doesn't grow unbounded\n\ndef verify_milestone_5_lock_free_hashmap():\n    \"\"\"Verify hash map operations and incremental resizing correctness.\"\"\"\n    # TODO 1: Test basic insert/lookup/delete operations with various key types\n    # TODO 2: Verify concurrent operations maintain key-value consistency\n    # TODO 3: Test incremental resize triggered by load factor threshold\n    # TODO 4: Verify all existing data remains accessible during resize\n    # TODO 5: Test split-ordered list maintains logical hash ordering\n    # TODO 6: Create mixed workload stress test with resize operations\n    # TODO 7: Benchmark performance vs standard concurrent hash map implementations\n    # Hint: Use deterministic hash functions for reproducible test scenarios\n```\n\n#### Language-Specific Hints\n\n**Python-Specific Implementation Tips:**\n- Use `threading.Event()` for coordinating test thread startup and shutdown\n- Leverage `concurrent.futures.ThreadPoolExecutor` for managed thread lifecycle\n- Use `time.perf_counter()` for high-resolution timing measurements\n- Consider `multiprocessing` module for testing true parallelism (avoid GIL)\n- Use `statistics` module for calculating performance percentiles\n- Integrate with `pytest` framework for structured test organization and reporting\n\n**Memory Management Considerations:**\n- Python's garbage collector can interfere with lock-free memory reclamation timing\n- Use `gc.disable()` during critical test sections to ensure deterministic behavior\n- Consider `ctypes` integration for direct memory management in performance-critical tests\n- Use `tracemalloc` to detect memory leaks in long-running stress tests\n\n**Debugging Integration:**\n- Use `logging` module with thread-safe formatting for concurrent debugging\n- Consider `pdb` integration with thread-specific breakpoints\n- Use `threading.current_thread().name` for thread identification in logs\n- Integrate with `cProfile` for performance bottleneck identification\n\n#### Milestone Checkpoint Commands\n\n**Automated Test Execution:**\n```bash\n# Run complete test suite for all milestones\npython -m pytest tests/ -v --tb=short\n\n# Run specific milestone tests\npython -m pytest tests/unit/test_atomic_operations.py -v\npython -m pytest tests/integration/test_stack_correctness.py -v\npython -m pytest tests/stress/stress_stack_operations.py -v\n\n# Run with coverage reporting\npython -m pytest tests/ --cov=lockfree --cov-report=html\n\n# Run performance benchmarks\npython tests/benchmarks/performance_benchmarks.py\n\n# Memory safety testing (requires AddressSanitizer)\nASAN_OPTIONS=detect_leaks=1 python tests/verification/memory_safety_checker.py\n```\n\n**Manual Verification Steps:**\n1. **Visual Progress Monitoring**: Each stress test should display real-time progress indicators showing operations per second, success rates, and any detected anomalies\n2. **Interactive Debugging**: Test framework should support manual breakpoint insertion for investigating specific failure scenarios\n3. **Performance Regression Detection**: Benchmark results should be compared against baseline measurements to detect performance regressions\n4. **Correctness Validation**: Each test should output clear pass/fail indicators with detailed failure descriptions when correctness violations occur\n\n\n## Debugging Guide\n\n> **Milestone(s):** All milestones (this section provides systematic approaches to diagnosing and fixing bugs that can occur throughout the implementation of lock-free data structures, from basic atomic operations through complete concurrent hash maps)\n\nDebugging lock-free concurrent code represents one of the most challenging aspects of systems programming. Unlike traditional sequential programs where bugs manifest predictably and reproducibly, lock-free algorithms introduce timing-dependent failures that may only surface under specific thread interleavings or memory ordering conditions. The non-deterministic nature of concurrent execution means that bugs can remain hidden during development only to emerge under production load conditions.\n\n### Mental Model: Debugging as Detective Work in a Parallel Universe\n\nThink of debugging lock-free code as detective work in a parallel universe where multiple timelines exist simultaneously. Unlike sequential debugging where you follow a single timeline of events, lock-free debugging requires understanding how multiple threads create overlapping timelines that occasionally intersect at shared memory locations. Each thread operates in its own timeline, but these timelines can influence each other through atomic operations - like ripples from different stones thrown into the same pond.\n\nThe challenge lies in reconstructing what happened across all timelines when something goes wrong. Traditional debugging tools show you one timeline, but the bug might be caused by the interaction between threads operating in different timelines. You need techniques that can capture and analyze the multi-dimensional execution space where these interactions occur.\n\n![CAS Retry Pattern](./diagrams/cas-retry-pattern.svg)\n\n### Common Bug Patterns\n\nThe majority of bugs in lock-free programming fall into predictable categories, each with characteristic symptoms and root causes. Understanding these patterns enables systematic diagnosis rather than random trial-and-error debugging. The following comprehensive table maps symptoms to their underlying causes and provides concrete fix strategies.\n\n| **Bug Pattern** | **Symptoms** | **Root Cause** | **Detection Method** | **Fix Strategy** |\n|---|---|---|---|---|\n| **ABA Problem** | CAS succeeds when it should fail; corrupted data structure; nodes appear twice in traversal | Pointer reuse between read and update allows incorrect CAS success | Add sequence counters to detect reuse; validate structure after CAS | Use tagged pointers or hazard pointers; never reuse memory addresses |\n| **Memory Ordering Race** | Operations appear out of order; reads see stale values; writes become visible late | Relaxed memory ordering allows CPU reordering | Insert memory barriers; use stronger ordering | Apply acquire/release semantics; add seq_cst barriers at critical points |\n| **Premature Reclamation** | Segmentation faults; use-after-free errors; corrupted reads from freed memory | Node freed while other threads still accessing | Implement hazard pointer checking; add reference counting | Deploy hazard pointer scheme; defer all reclamation until safe |\n| **Infinite CAS Loop** | Thread CPU usage at 100%; operations never complete; system becomes unresponsive | CAS retry loop without progress guarantee or backoff | Monitor CAS attempt counts; detect spinning threads | Add exponential backoff; implement helping mechanism; set retry limits |\n| **Linearizability Violation** | Operations appear to execute in impossible order; concurrent operations see inconsistent state | Incorrect linearization points or missing atomicity | Record operation timestamps; verify sequential consistency | Fix linearization points; ensure atomic state updates |\n| **Livelock Condition** | System appears active but makes no progress; operations continuously retry | Multiple threads interfere with each other's progress | Measure operation completion rates; detect retry patterns | Randomize backoff timing; implement priority schemes; add helping |\n| **Stack Corruption** | Lost elements; duplicate elements; segfault during traversal; broken next pointers | Race between pointer reads and CAS updates | Validate stack structure; check next pointer consistency | Protect pointer reads with hazard pointers; fix CAS ordering |\n| **Queue FIFO Violation** | Elements dequeued in wrong order; enqueue/dequeue see inconsistent tail | Tail pointer not properly maintained; missing helping | Track element ordering; verify FIFO property | Fix tail advancement logic; implement proper helping mechanism |\n| **Hash Map Inconsistency** | Lookup fails for inserted keys; duplicate keys in buckets; lost updates | Bucket initialization races; incorrect split ordering | Validate bucket contents; check split-order properties | Fix bucket initialization; correct reverse bit ordering |\n| **Hazard Pointer Leak** | Memory usage grows without bound; retirement list becomes huge; scan never reclaims | Hazard pointers not cleared; retirement threshold too high | Monitor retirement list size; track hazard pointer usage | Clear hazard pointers after use; tune scan threshold; add cleanup |\n\n⚠️ **Pitfall: Debugging Lock-free Code with Traditional Tools**\nTraditional debuggers that pause execution fundamentally change the timing characteristics of concurrent programs. A bug that reproduces reliably in normal execution may disappear completely when running under a debugger because the debugger's pauses alter thread scheduling. This makes traditional stepping-through-code debugging ineffective for timing-dependent race conditions. Instead, rely on logging, performance counters, and specialized concurrent debugging tools.\n\n⚠️ **Pitfall: Assuming Reproducible Bug Behavior**\nLock-free bugs often manifest non-deterministically based on thread scheduling, memory layout, CPU cache states, and system load. A bug might appear once in every thousand runs, making it tempting to assume it's fixed when testing shows no immediate failures. Always use stress testing with high thread counts and extended duration to expose rare race conditions that only occur under specific timing conditions.\n\n⚠️ **Pitfall: Ignoring Performance Degradation as a Bug Symptom**\nSevere performance degradation often signals correctness bugs in lock-free code. If CAS success rates drop dramatically or operation latencies increase exponentially, this usually indicates livelock conditions, excessive contention, or incorrect helping mechanisms rather than just poor performance. Investigate performance anomalies as potential correctness issues.\n\n### Lock-free Debugging Techniques\n\nEffective debugging of lock-free code requires specialized techniques that account for the concurrent, non-deterministic nature of execution. These techniques focus on capturing sufficient information about multi-threaded execution patterns without significantly perturbing the timing characteristics that might mask or reveal bugs.\n\n#### Operation Recording and Replay\n\nThe foundation of lock-free debugging involves systematically recording all operations and their outcomes to enable post-mortem analysis of execution sequences. This approach captures the multi-threaded execution history without requiring real-time debugging that would alter timing characteristics.\n\n| **Recording Component** | **Information Captured** | **Storage Method** | **Analysis Purpose** |\n|---|---|---|---|\n| **Operation Log** | Operation type, timestamp, thread ID, parameters, return value, linearization point | Thread-local circular buffers | Reconstruct operation ordering and identify linearizability violations |\n| **CAS Attempt Tracker** | Expected value, new value, observed value, success/failure, retry count | Per-thread counters with periodic snapshots | Detect ABA problems and infinite retry loops |\n| **Memory Access Log** | Address accessed, operation type (load/store), memory ordering, timestamp | Lock-free ring buffer | Identify memory ordering races and visibility issues |\n| **State Snapshots** | Complete data structure state, thread local state, hazard pointer assignments | Atomic snapshot mechanism | Verify invariants and detect corruption |\n| **Performance Metrics** | Operation latencies, CAS success rates, retry counts, backoff delays | Lock-free counters with statistical aggregation | Identify performance anomalies indicating correctness problems |\n\nThe `LinearizabilityChecker` provides systematic verification of concurrent operation ordering:\n\n| **Method** | **Parameters** | **Returns** | **Description** |\n|---|---|---|---|\n| `record_operation` | `operation: Operation` | `None` | Thread-safely records operation with precise timestamp and linearization point |\n| `verify_history` | `start_time: float, end_time: float` | `Tuple[bool, List[str]]` | Checks if operations in time window have valid sequential ordering |\n| `check_invariants` | `data_structure: Any` | `Tuple[bool, str]` | Validates all structure-specific invariants and returns violation details |\n| `generate_report` | `failure_mode: FailureMode` | `FailureReport` | Creates comprehensive failure analysis with timeline and thread interactions |\n\n#### Thread-Safe Logging Strategies\n\nEffective logging in lock-free systems requires careful design to avoid introducing synchronization bottlenecks while capturing sufficient detail for debugging. The logging system itself must be lock-free to avoid perturbing the very timing characteristics being debugged.\n\n> **Design Principle: Minimize Observer Effect**\n> The act of observing a concurrent system inherently changes its behavior through timing perturbations, cache effects, and resource contention. Effective lock-free debugging minimizes this observer effect by using lock-free logging mechanisms, thread-local storage, and deferred analysis of captured data.\n\n**Thread-Local Buffer Architecture:**\nEach thread maintains its own circular buffer for operation logging, eliminating contention between threads while recording events. Buffers use atomic head and tail pointers to allow safe concurrent access by both the application thread (writing) and monitoring threads (reading). When buffers reach capacity, they either overwrite old entries (for high-frequency events) or flush to persistent storage (for critical events).\n\n**Timestamp Coordination:**\nAccurate cross-thread timestamp coordination requires careful attention to clock synchronization without introducing synchronization overhead. Use high-resolution monotonic clocks with thread-local offset correction to account for CPU migration effects. Record both local timestamps and global sequence numbers to enable precise ordering reconstruction during analysis.\n\n**Event Filtering and Sampling:**\nHigh-throughput lock-free systems generate enormous volumes of events that can overwhelm logging infrastructure. Implement adaptive sampling that increases detail capture when anomalies are detected while maintaining baseline monitoring during normal operation. Use bloom filters to detect interesting event patterns without storing all details.\n\n#### State Inspection Without Locks\n\nInspecting the state of lock-free data structures during execution requires techniques that don't introduce synchronization points that would alter behavior. Traditional approaches like stopping all threads or acquiring locks fundamentally change the concurrent execution characteristics.\n\n**Atomic Snapshot Techniques:**\nCapturing consistent snapshots of multi-word data structures requires careful use of versioned updates and optimistic reading. The snapshot algorithm attempts to read all relevant fields, then verifies that no concurrent updates occurred during the read sequence. If concurrent updates are detected, the algorithm retries the snapshot operation.\n\n**Hazard-Pointer-Protected Traversal:**\nWhen inspecting linked structures like queues or hash map chains, use hazard pointers to protect against concurrent modifications that might deallocate nodes during traversal. The inspection thread announces its intent to access each node before dereferencing pointers, ensuring that concurrent operations don't reclaim memory in use.\n\n**Statistical Invariant Monitoring:**\nRather than requiring perfect snapshots, many invariants can be verified statistically by sampling structure properties over time. For example, queue FIFO ordering can be verified by tracking element insertion and removal timestamps, while stack LIFO behavior can be monitored through operation sequence analysis.\n\n### Performance Issue Diagnosis\n\nPerformance problems in lock-free systems often indicate underlying correctness issues or suboptimal algorithmic choices. Unlike traditional lock-based systems where performance issues typically manifest as blocked threads, lock-free performance problems appear as excessive CPU usage, poor cache behavior, or operation latency spikes.\n\n#### Contention Hotspot Analysis\n\nLock-free algorithms can still suffer from contention when multiple threads repeatedly access the same memory locations, even though no locks are involved. This contention manifests as repeated CAS failures, cache line bouncing, and increased memory latency.\n\n| **Contention Type** | **Symptoms** | **Measurement Technique** | **Mitigation Strategy** |\n|---|---|---|---|\n| **Single Variable Hotspot** | High CAS failure rate on specific atomic variable; CPU cycles spent retrying | Monitor per-variable CAS success rates; measure retry counts | Implement exponential backoff; use helping mechanisms; consider algorithm changes |\n| **Cache Line Sharing** | False sharing between unrelated variables; poor scaling with core count | Use hardware performance counters; measure cache miss rates | Pad structures to cache line boundaries; separate frequently accessed fields |\n| **Memory Ordering Overhead** | Excessive memory barrier execution; poor instruction pipeline utilization | Profile memory fence instructions; measure pipeline stalls | Optimize memory ordering choices; use acquire/release instead of seq_cst |\n| **ABA Prevention Overhead** | High overhead from tagged pointers or hazard pointer operations | Measure ABA detection costs; profile protection overhead | Tune tag sizes; optimize hazard pointer implementation; consider epoch-based reclamation |\n\n#### False Sharing Detection\n\nFalse sharing occurs when threads access different variables that reside on the same cache line, causing unnecessary cache coherence traffic. In lock-free systems, this can severely impact performance even when no actual data sharing occurs.\n\nThe `PerformanceMonitor` provides systematic measurement of performance characteristics:\n\n| **Method** | **Parameters** | **Returns** | **Description** |\n|---|---|---|---|\n| `record_operation` | `operation_type: str, duration: float, cas_attempts: int, success: bool` | `None` | Records timing and CAS statistics for performance analysis |\n| `get_statistics` | `operation_type: str` | `Dict[str, float]` | Returns average latency, success rate, and retry count statistics |\n| `detect_contention` | `threshold: float` | `List[str]` | Identifies operations with CAS success rates below threshold |\n| `analyze_cache_behavior` | `sampling_duration: float` | `Dict[str, int]` | Uses hardware counters to measure cache miss rates and coherence traffic |\n\n#### Scalability Bottleneck Identification\n\nLock-free algorithms should ideally scale linearly with the number of cores, but various factors can limit scalability. Identifying these bottlenecks requires systematic measurement under varying thread counts and workload characteristics.\n\n**Throughput Scaling Analysis:**\nMeasure operation throughput as a function of thread count to identify scalability cliffs where adding more threads actually decreases total system throughput. This typically indicates excessive contention or poor load distribution across threads.\n\n**Latency Distribution Changes:**\nMonitor how operation latency distributions change with increased concurrency. Healthy lock-free algorithms maintain relatively stable latency percentiles even under high contention, while problematic implementations show exponential latency growth.\n\n**Memory Subsystem Utilization:**\nUse hardware performance monitoring units (PMUs) to measure memory bandwidth utilization, cache hierarchy behavior, and NUMA effects. Lock-free algorithms can quickly saturate memory bandwidth or create NUMA imbalances that limit scalability.\n\n> **Critical Insight: Performance and Correctness Intersection**\n> In lock-free systems, performance problems often indicate correctness issues rather than just inefficiency. Livelock conditions appear as high CPU usage with low progress. Memory leaks manifest as degrading performance over time. ABA problems can cause periodic performance spikes when corruption requires recovery. Always investigate performance anomalies as potential correctness violations.\n\n⚠️ **Pitfall: Optimizing Performance Without Measuring Correctness**\nWhen tuning lock-free algorithm performance, ensure that optimizations don't introduce correctness bugs. Common mistakes include weakening memory ordering for performance gains without considering correctness implications, reducing backoff delays to improve latency while introducing livelock conditions, and optimizing for single-threaded performance at the expense of concurrent correctness. Always combine performance measurements with correctness verification.\n\n⚠️ **Pitfall: Ignoring NUMA Effects in Scalability Testing**\nModern multi-core systems exhibit significant NUMA (Non-Uniform Memory Access) effects that can dramatically impact lock-free algorithm performance. Testing on single-socket systems may show good scalability that disappears on multi-socket production systems. Always test scalability on hardware configurations similar to production deployment, and consider NUMA-aware memory allocation strategies for data structures.\n\n### Implementation Guidance\n\nThis section provides concrete tools and techniques for implementing systematic debugging support throughout the lock-free data structure library. The debugging infrastructure should be designed as an integral part of the system rather than an afterthought, enabling both development-time debugging and production monitoring.\n\n#### Technology Recommendations\n\n| **Component** | **Simple Option** | **Advanced Option** |\n|---|---|---|---|\n| **Operation Logging** | Thread-local lists with periodic JSON dumps | Lock-free circular buffers with binary serialization |\n| **Performance Monitoring** | Python `time.perf_counter()` with basic statistics | Hardware performance counters via `psutil` or custom bindings |\n| **Concurrency Testing** | Python `threading` with random delays | Property-based testing with `hypothesis` library |\n| **Visualization** | Text-based reports with `tabulate` | Interactive dashboards with `matplotlib` or `plotly` |\n| **State Inspection** | Manual debugging with print statements | Automated invariant checking with custom validators |\n\n#### Recommended Module Structure\n\n```\nlock_free_project/\n  src/\n    debugging/\n      __init__.py                    ← exports main debugging interfaces\n      operation_recorder.py          ← LinearizabilityChecker and operation logging\n      performance_monitor.py         ← PerformanceMonitor and metrics collection\n      failure_analyzer.py            ← FailureReport generation and pattern analysis\n      invariant_checker.py          ← InvariantChecker for structure validation\n      stress_tester.py              ← StressTestFramework for concurrent testing\n      visualization.py              ← reporting and dashboard generation\n    data_structures/\n      atomic_ops.py                 ← integrates with operation_recorder\n      treiber_stack.py             ← includes debugging hooks\n      michael_scott_queue.py       ← includes debugging hooks\n      hazard_pointers.py           ← includes debugging hooks\n      lock_free_hashmap.py         ← includes debugging hooks\n    tests/\n      test_debugging_tools.py      ← validates debugging infrastructure\n      stress_tests/                ← comprehensive concurrent testing\n        test_stack_stress.py\n        test_queue_stress.py\n        test_hashmap_stress.py\n```\n\n#### Infrastructure: Complete Operation Recording System\n\n```python\nimport threading\nimport time\nfrom typing import Dict, List, Optional, Any, Tuple\nfrom enum import Enum\nfrom dataclasses import dataclass\nimport json\n\nclass OperationType(Enum):\n    PUSH = \"push\"\n    POP = \"pop\"\n    ENQUEUE = \"enqueue\"\n    DEQUEUE = \"dequeue\"\n    INSERT = \"insert\"\n    LOOKUP = \"lookup\"\n    DELETE = \"delete\"\n\n@dataclass\nclass Operation:\n    type: OperationType\n    timestamp: float\n    thread_id: int\n    invocation: bool  # True for call, False for response\n    args: Tuple\n    result: Any\n    operation_id: str\n\nclass LinearizabilityChecker:\n    def __init__(self):\n        self.operations: List[Operation] = []\n        self.operation_lock = threading.Lock()\n        self.operation_counter = 0\n        self.verification_cache: Dict[str, bool] = {}\n    \n    def record_operation(self, operation_type: OperationType, \n                        is_invocation: bool, args: Tuple = (), \n                        result: Any = None) -> str:\n        \"\"\"Record an operation invocation or response for linearizability checking.\"\"\"\n        operation_id = f\"{threading.current_thread().ident}_{self.operation_counter}\"\n        self.operation_counter += 1\n        \n        op = Operation(\n            type=operation_type,\n            timestamp=time.perf_counter(),\n            thread_id=threading.current_thread().ident,\n            invocation=is_invocation,\n            args=args,\n            result=result,\n            operation_id=operation_id\n        )\n        \n        with self.operation_lock:\n            self.operations.append(op)\n        \n        return operation_id\n    \n    def verify_history(self, start_time: float, end_time: float) -> Tuple[bool, List[str]]:\n        \"\"\"Check if operations in time window have valid sequential ordering.\"\"\"\n        # TODO: Implement linearizability verification algorithm\n        # TODO: Filter operations by time window\n        # TODO: Group invocation/response pairs\n        # TODO: Check if there exists a valid sequential ordering\n        # TODO: Return validation result and any violation descriptions\n        pass\n    \n    def generate_timeline_report(self) -> str:\n        \"\"\"Generate human-readable timeline of operations for debugging.\"\"\"\n        # TODO: Sort operations by timestamp\n        # TODO: Group by thread for parallel timeline view\n        # TODO: Format as readable timeline with ASCII art\n        # TODO: Highlight potential linearizability violations\n        pass\n```\n\n#### Infrastructure: Performance Monitoring System\n\n```python\nfrom collections import defaultdict, deque\nimport threading\nimport time\nimport statistics\nfrom typing import Dict, List, Optional\n\nclass PerformanceMonitor:\n    def __init__(self, history_size: int = 1000):\n        self.operation_times: Dict[str, deque] = defaultdict(lambda: deque(maxlen=history_size))\n        self.cas_attempts: Dict[str, deque] = defaultdict(lambda: deque(maxlen=history_size))\n        self.success_rates: Dict[str, deque] = defaultdict(lambda: deque(maxlen=history_size))\n        self.history_size = history_size\n        self.lock = threading.Lock()\n    \n    def record_operation(self, operation_type: str, duration: float, \n                        cas_attempts: int, success: bool) -> None:\n        \"\"\"Record performance metrics for a completed operation.\"\"\"\n        with self.lock:\n            self.operation_times[operation_type].append(duration)\n            self.cas_attempts[operation_type].append(cas_attempts)\n            self.success_rates[operation_type].append(1.0 if success else 0.0)\n    \n    def get_statistics(self, operation_type: str) -> Dict[str, float]:\n        \"\"\"Get comprehensive statistics for an operation type.\"\"\"\n        with self.lock:\n            if operation_type not in self.operation_times:\n                return {}\n            \n            times = list(self.operation_times[operation_type])\n            attempts = list(self.cas_attempts[operation_type])\n            successes = list(self.success_rates[operation_type])\n            \n            if not times:\n                return {}\n            \n            return {\n                'avg_latency_ms': statistics.mean(times) * 1000,\n                'p95_latency_ms': statistics.quantiles(times, n=20)[18] * 1000,\n                'p99_latency_ms': statistics.quantiles(times, n=100)[98] * 1000,\n                'avg_cas_attempts': statistics.mean(attempts),\n                'success_rate': statistics.mean(successes),\n                'total_operations': len(times)\n            }\n    \n    def detect_contention(self, threshold: float = 0.5) -> List[str]:\n        \"\"\"Identify operations with CAS success rates below threshold.\"\"\"\n        # TODO: Calculate recent success rates for all operation types\n        # TODO: Identify operations below threshold\n        # TODO: Return list of problematic operation types\n        # TODO: Include suggested remediation strategies\n        pass\n```\n\n#### Core Logic: Stress Testing Framework\n\n```python\nimport threading\nimport time\nimport random\nfrom concurrent.futures import ThreadPoolExecutor\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Any\n\n@dataclass\nclass StressTestConfig:\n    num_threads: int\n    operations_per_thread: int\n    test_duration_seconds: Optional[float]\n    operation_mix: Dict[str, float]  # operation_type -> probability\n    contention_level: str  # 'low', 'medium', 'high'\n    backoff_strategy: str  # 'none', 'exponential', 'linear'\n\n@dataclass\nclass StressTestResults:\n    total_operations: int\n    successful_operations: int\n    failed_operations: int\n    total_duration: float\n    throughput_ops_per_sec: float\n    average_latency_ms: float\n    p95_latency_ms: float\n    p99_latency_ms: float\n    cas_success_rate: float\n    thread_results: List[Dict]\n\nclass StressTestFramework:\n    def __init__(self, data_structure: Any, config: StressTestConfig):\n        self.data_structure = data_structure\n        self.config = config\n        self.results = None\n        self.operation_times: List[float] = []\n        self.cas_attempts = 0\n        self.cas_successes = 0\n        self.results_lock = threading.Lock()\n        self.should_stop = threading.Event()\n    \n    def stress_test_data_structure(self) -> StressTestResults:\n        \"\"\"Execute comprehensive stress test and return detailed results.\"\"\"\n        # TODO: Initialize test environment and metrics collection\n        # TODO: Start worker threads with specified configuration\n        # TODO: Run test for specified duration or operation count\n        # TODO: Collect and aggregate results from all threads\n        # TODO: Calculate comprehensive statistics and return results\n        # TODO: Verify data structure invariants after test completion\n        pass\n    \n    def _worker_thread(self, thread_id: int) -> Dict:\n        \"\"\"Execute operations for a single worker thread.\"\"\"\n        # TODO: Initialize thread-local random number generator\n        # TODO: Calculate operation mix based on configuration probabilities\n        # TODO: Execute operations until completion or timeout\n        # TODO: Record timing and success metrics for each operation\n        # TODO: Return thread-specific results dictionary\n        pass\n    \n    def _execute_operation(self, operation_type: str, rng: random.Random, \n                          thread_id: int) -> Tuple[bool, float, int]:\n        \"\"\"Execute a single operation on the data structure.\"\"\"\n        # TODO: Generate operation parameters based on type and contention level\n        # TODO: Record operation start time\n        # TODO: Execute the operation with CAS attempt counting\n        # TODO: Record operation completion and calculate duration\n        # TODO: Return success status, duration, and CAS attempt count\n        pass\n```\n\n#### Core Logic: Debugging Instrumentation Hooks\n\n```python\nclass DebugInstrumentation:\n    \"\"\"Provides debugging hooks for integration into lock-free data structures.\"\"\"\n    \n    def __init__(self):\n        self.operation_recorder = LinearizabilityChecker()\n        self.performance_monitor = PerformanceMonitor()\n        self.enabled = True\n    \n    def record_cas_attempt(self, variable_name: str, expected: Any, \n                          new_value: Any, observed: Any, success: bool):\n        \"\"\"Record CAS attempt details for ABA and livelock detection.\"\"\"\n        # TODO: Log CAS attempt with all values for later analysis\n        # TODO: Detect potential ABA patterns (observed != expected)\n        # TODO: Track consecutive failures for livelock detection\n        # TODO: Update performance counters for success rate calculation\n        pass\n    \n    def validate_structure_invariants(self, structure_type: str, \n                                    validation_function) -> Tuple[bool, str]:\n        \"\"\"Check data structure invariants and report violations.\"\"\"\n        # TODO: Execute structure-specific validation function\n        # TODO: Catch and report any invariant violations\n        # TODO: Generate detailed violation report with structure state\n        # TODO: Return validation status and violation description\n        pass\n    \n    def instrument_operation(self, operation_name: str):\n        \"\"\"Decorator to add debugging instrumentation to operations.\"\"\"\n        def decorator(func):\n            def wrapper(*args, **kwargs):\n                # TODO: Record operation invocation\n                # TODO: Execute original function with timing\n                # TODO: Record operation response and performance metrics\n                # TODO: Validate post-condition invariants\n                # TODO: Return original function result\n                pass\n            return wrapper\n        return decorator\n```\n\n#### Milestone Checkpoint: Debugging Infrastructure Validation\n\nAfter implementing the debugging infrastructure, verify its functionality with these checkpoints:\n\n**1. Operation Recording Verification:**\nRun a simple concurrent test with 2-3 threads performing stack operations. The `LinearizabilityChecker` should capture all operations with precise timestamps. Verify that the timeline report shows operations in chronological order with clear invocation/response pairing.\n\n**2. Performance Monitoring Validation:**\nExecute a brief stress test and verify that `PerformanceMonitor` captures realistic metrics. CAS success rates should be between 10-90% under moderate contention. Latency percentiles should show reasonable distributions without obvious outliers.\n\n**3. Invariant Checking Integration:**\nIntegrate debugging hooks into one data structure (start with `TreiberStack`). Verify that invariant violations are detected when you intentionally introduce bugs (e.g., skip CAS operations or corrupt pointers).\n\n**Expected behavior:** Debugging tools should capture detailed execution information without significantly impacting performance (less than 10% overhead). Reports should be human-readable and highlight potential issues clearly.\n\n**Signs of problems:** If debugging overhead exceeds 20%, optimize the logging infrastructure. If operation recording misses events or shows incorrect timestamps, check thread-safety of the recording mechanism. If performance monitoring shows impossible values (negative latencies, >100% success rates), verify metric calculation logic.\n\n\n## Future Extensions\n\n> **Milestone(s):** All milestones (this section explores advanced directions and optimizations that build upon the foundational lock-free data structures implemented throughout the project)\n\nThe lock-free data structure foundation we've built represents just the beginning of a rich ecosystem of concurrent programming possibilities. While our core implementations of atomic operations, Treiber stacks, Michael-Scott queues, hazard pointers, and split-ordered hash maps provide solid building blocks, numerous advanced algorithms, performance optimizations, and integration opportunities await exploration. This section outlines the most promising directions for extending our lock-free library, organized into three categories: advanced algorithmic improvements that push beyond lock-free to wait-free guarantees, performance optimizations that squeeze every cycle from modern hardware, and ecosystem integrations that make these data structures production-ready.\n\nThe progression from our current lock-free implementations to these advanced extensions follows a natural evolution. Our atomic operations foundation supports more sophisticated coordination protocols. The memory reclamation expertise gained from hazard pointers transfers directly to more complex schemes like epochs and RCU. The linearizability verification techniques we developed scale to validate more intricate data structures. Most importantly, the debugging and testing methodologies we've established become even more critical when dealing with the increased complexity of wait-free algorithms and hardware-specific optimizations.\n\n### Advanced Lock-free Algorithms\n\n**Mental Model: From Traffic Lights to Highway Overpasses**\n\nThink of our current lock-free algorithms as a well-designed traffic intersection with smart traffic lights. Most of the time, traffic flows smoothly with minimal delays. However, during peak hours or when one direction is particularly busy, some cars might wait through multiple light cycles before proceeding. Wait-free algorithms, by contrast, are like a highway system with overpasses and dedicated lanes - every vehicle is guaranteed to reach its destination within a predictable time, regardless of how busy other routes become.\n\nThis analogy captures the fundamental difference between lock-free and wait-free progress guarantees. Lock-free algorithms ensure that at least one thread makes progress within a finite number of steps, but individual threads might face delays if they repeatedly lose CAS races to more fortunate threads. Wait-free algorithms provide the stronger guarantee that every thread completes its operation within a bounded number of its own steps, regardless of interference from other threads.\n\n**Wait-free Algorithm Foundations**\n\nThe transition from lock-free to wait-free algorithms requires fundamentally different design approaches. Where lock-free algorithms rely on retrying CAS operations until success, wait-free algorithms must ensure that every thread can complete its work within a predetermined step bound. This typically involves more sophisticated helping mechanisms where threads not only assist stuck operations but actively contribute to every other thread's progress.\n\nThe universal construction technique provides one approach to building wait-free implementations from lock-free foundations. Each thread maintains a private copy of its intended operation, and a global consensus mechanism ensures that all operations appear to execute in some sequential order. While theoretically elegant, universal constructions often exhibit high overhead that makes them impractical for performance-sensitive applications.\n\nMore practical wait-free algorithms exploit specific properties of individual data structures. For example, a wait-free queue might reserve space for each thread's operations in advance, eliminating the contention that causes retry loops in the Michael-Scott algorithm. Similarly, wait-free hash maps can use combining techniques where threads batch their operations together, reducing per-operation overhead while guaranteeing bounded completion times.\n\n| Wait-free Algorithm | Key Technique | Step Bound | Trade-offs |\n|---------------------|---------------|------------|-----------|\n| Universal Construction | Private operation logs + consensus | O(N²) per operation | Simple but high overhead |\n| Wait-free Queue | Pre-allocated slots per thread | O(N) per operation | Memory overhead scales with threads |\n| Combining Tree | Hierarchical operation batching | O(log N) per operation | Complex helping protocols |\n| Fast Path/Slow Path | Common case optimization + fallback | O(1) typical, O(N) worst | Best of both worlds approach |\n\n**Advanced Data Structure Extensions**\n\nBeyond improving progress guarantees, numerous sophisticated data structures build upon our lock-free foundations. Lock-free trees present particular challenges due to their hierarchical nature and the need for multi-step operations that maintain tree invariants. B-trees and balanced trees require careful coordination between threads performing rotations, splits, and merges while maintaining both structural and ordering properties.\n\nPriority queues represent another fertile ground for lock-free innovation. Unlike simple FIFO queues, priority queues must maintain heap or ordering properties while allowing concurrent insertions and deletions. Skip lists provide one approach, offering probabilistic balancing that eliminates the complex coordination required for deterministic tree structures. Lock-free skip lists use forward pointers at multiple levels, allowing concurrent traversals while using CAS operations to maintain linking invariants across levels.\n\nGraph data structures pose even greater challenges due to their complex connectivity patterns. Lock-free graph algorithms must handle cycles, multiple incoming edges, and operations that span multiple nodes. Techniques like edge marking and logical deletion become essential for maintaining consistency during concurrent modifications.\n\n> **Critical Insight**: Advanced data structures often require abandoning the simple single-CAS operations that characterize basic lock-free algorithms. Instead, they employ multi-step protocols with careful ordering and helping mechanisms to maintain complex invariants across multiple memory locations.\n\n**Specialized Optimization Techniques**\n\nSeveral algorithmic optimizations can dramatically improve performance for specific use cases. Flat combining represents a hybrid approach where threads occasionally acquire exclusive access to perform batched operations on behalf of multiple threads. While not strictly lock-free due to the exclusive access periods, flat combining can achieve better cache locality and reduced CAS contention for workloads with high operation rates.\n\nRead-Copy-Update (RCU) provides another powerful technique for read-heavy workloads. RCU allows unlimited concurrent readers by ensuring that updates create new versions rather than modifying existing data. Readers access consistent snapshots without any atomic operations, while writers coordinate through quiescence detection to safely reclaim old versions.\n\nEpoch-based memory reclamation offers an alternative to hazard pointers with different trade-offs. Instead of tracking individual pointer accesses, epoch-based schemes divide time into periods and defer reclamation until all threads have moved past the epoch where nodes were retired. This approach reduces the per-access overhead of hazard pointers but may delay reclamation longer in scenarios with irregular thread scheduling.\n\n| Technique | Best Use Case | Overhead | Complexity |\n|-----------|---------------|----------|------------|\n| Flat Combining | High contention, batched operations | Low amortized | Medium |\n| RCU | Read-heavy workloads | Minimal read, higher write | Medium |\n| Epoch Reclamation | Regular thread scheduling patterns | Lower than hazard pointers | Low |\n| Hardware Transactional Memory | Short, contended critical sections | Variable (hardware dependent) | Low (when supported) |\n\n### Performance Optimizations\n\n**Mental Model: From City Streets to Formula 1 Circuits**\n\nImagine our current lock-free algorithms as efficient city street navigation - they avoid the worst traffic jams and generally get you where you need to go in reasonable time. Performance optimization transforms these implementations into Formula 1 racing circuits: every turn is banked for maximum speed, every surface is optimized for grip, and every detail is tuned for the specific characteristics of the vehicles using the track. Just as F1 circuits exploit physics and aerodynamics that don't matter for everyday driving, these optimizations exploit hardware characteristics and usage patterns that general-purpose algorithms must ignore.\n\n**Hardware-Aware Memory Layout Optimization**\n\nModern hardware presents a complex memory hierarchy where data placement dramatically affects performance. NUMA (Non-Uniform Memory Access) architectures mean that memory access costs depend on which CPU socket allocated the memory and which socket is accessing it. Our lock-free data structures can be optimized to consider NUMA topology when allocating nodes and distributing work across threads.\n\nNUMA-aware allocation strategies place frequently accessed shared data structures on memory banks close to the threads most likely to access them. For data structures with read-heavy access patterns, replicating read-only metadata across NUMA nodes can eliminate cross-socket memory traffic. Write-heavy structures might use per-NUMA-node pools for node allocation, reducing remote memory pressure during frequent allocations.\n\nCache line optimization represents another critical performance dimension. False sharing occurs when unrelated variables share cache lines, causing expensive cache coherence traffic when different threads modify their respective variables. Our atomic reference types can be padded to ensure each occupies its own cache line, eliminating false sharing at the cost of increased memory usage.\n\nCache-conscious data structure layout goes beyond avoiding false sharing to actively promoting cache efficiency. Arranging related fields to fit within single cache lines reduces memory bandwidth requirements. For linked structures like our queue and stack implementations, node prefetching can hide memory latency by speculatively loading nodes likely to be accessed in future operations.\n\n| Optimization | Impact | Memory Cost | Implementation Complexity |\n|--------------|--------|-------------|---------------------------|\n| NUMA-aware allocation | 2-4x improvement on multi-socket | Minimal | Medium (topology detection) |\n| Cache line padding | 10-50% improvement with contention | 8x memory usage for padded fields | Low |\n| Node prefetching | 20-40% improvement for traversals | None | Medium (hardware intrinsics) |\n| Data structure splitting | Variable (workload dependent) | Moderate | High (algorithmic changes) |\n\n**CPU Architecture Specialization**\n\nDifferent CPU architectures provide varying atomic operation capabilities and performance characteristics. x86-64 processors offer strong memory ordering guarantees that can eliminate memory barriers in certain algorithms, while ARM architectures require explicit ordering constraints but provide more flexible atomic primitives. Our atomic operations layer can include architecture-specific optimizations that exploit these differences.\n\nWeak memory model optimizations become particularly important on ARM and RISC-V architectures where explicit memory barriers are required for ordering guarantees. Careful placement of acquire and release barriers can maintain correctness while minimizing performance overhead. Some algorithms can be restructured to reduce the number of required barriers by changing the order of operations or using different atomic primitives.\n\nHardware transactional memory (HTM) support on newer Intel and IBM processors provides another optimization opportunity. Short, contended critical sections that would normally require complex lock-free protocols can sometimes be replaced with simple transactional code. While HTM has capacity limitations and fallback requirements, it can significantly simplify implementation while maintaining good performance for workloads that fit within hardware constraints.\n\nVectorization opportunities exist for certain bulk operations on our data structures. SIMD instructions can accelerate parallel searches, batch comparisons, or parallel updates to multiple independent elements. While vectorization typically applies to compute-intensive algorithms rather than pointer-chasing data structures, careful algorithm design can expose vectorization opportunities.\n\n**Adaptive and Self-Tuning Mechanisms**\n\nStatic optimization approaches assume fixed workload characteristics, but real applications exhibit varying contention patterns, thread counts, and operation mixes over time. Adaptive algorithms adjust their behavior based on runtime observations to optimize for current conditions rather than worst-case scenarios.\n\nContention-adaptive backoff strategies monitor CAS failure rates and adjust retry delays accordingly. During low contention periods, immediate retries minimize latency. As contention increases, exponential backoff reduces wasted CPU cycles and cache coherence traffic. Advanced backoff schemes can even detect contention patterns and predict optimal delay strategies for different operation types.\n\nLoad-adaptive data structure sizing automatically adjusts capacity based on observed usage patterns. Our hash map implementation could monitor load factors and resize frequency to find optimal bucket counts for current workloads. Similarly, memory reclamation schemes could adjust scan thresholds based on allocation rates and memory pressure.\n\nOperation-mix adaptation optimizes for the specific balance of read and write operations observed at runtime. Data structures can maintain separate fast paths for read-heavy and write-heavy workloads, switching between optimization strategies as usage patterns change.\n\n| Adaptive Technique | Monitoring Overhead | Adaptation Speed | Benefit Range |\n|--------------------|-------------------|------------------|---------------|\n| Contention-aware backoff | Very low | Immediate | 20-80% latency reduction |\n| Load-adaptive sizing | Low | Minutes to hours | 2-10x throughput improvement |\n| Operation-mix optimization | Medium | Seconds to minutes | Highly workload dependent |\n| Thread-count scaling | Low | Seconds | 50-300% improvement during scaling |\n\n### Integration and Ecosystem\n\n**Mental Model: From Prototype Workshop to Production Factory**\n\nThink of our current lock-free implementations as carefully crafted prototypes built in a specialized workshop. Each component works beautifully and demonstrates important principles, but transforming these prototypes into production-ready systems requires the same kind of systematic engineering that turns a hand-built concept car into a mass-production vehicle. This involves tooling for manufacturing (build systems and testing frameworks), quality control processes (monitoring and debugging), integration with existing systems (language bindings and framework adapters), and operational procedures (deployment, monitoring, and maintenance).\n\n**Language Ecosystem Integration**\n\nWhile our primary implementation targets Python, the algorithms and techniques we've developed translate naturally to other languages with different trade-offs and integration opportunities. Each language ecosystem presents unique challenges and opportunities for lock-free data structure integration.\n\nC and C++ implementations can leverage stronger memory model guarantees and direct access to hardware atomic primitives. The C11 and C++11 atomic standards provide portable abstractions over platform-specific atomic operations, allowing optimized implementations that exploit architecture-specific features while maintaining compatibility. C++ template metaprogramming can generate specialized versions of data structures for different data types, eliminating the boxing overhead required in Python implementations.\n\nRust's ownership system provides natural integration opportunities with memory reclamation schemes. Rust's lifetime tracking can statically verify many safety properties that our hazard pointer implementation enforces dynamically. The `Arc` and `Rc` reference counting types provide alternatives to hazard pointers for certain use cases, though they require atomic reference count updates that can become contention bottlenecks.\n\nGo's runtime scheduler and garbage collector present both opportunities and challenges for lock-free integration. The garbage collector eliminates manual memory reclamation concerns but introduces stop-the-world pauses that can affect linearizability timing. Go's channel-based concurrency model provides a higher-level alternative to lock-free programming for many use cases, but direct atomic operations remain important for performance-critical components.\n\nJava Virtual Machine integration opens opportunities for dynamic optimization and just-in-time compilation of lock-free algorithms. The JVM's escape analysis can eliminate atomic operations for thread-local objects, while profile-guided optimization can specialize hot paths based on observed usage patterns. Integration with Java's `java.util.concurrent` package ensures compatibility with existing concurrent programming patterns.\n\n| Language | Key Advantages | Integration Challenges | Recommended Use Cases |\n|----------|----------------|----------------------|----------------------|\n| C/C++ | Direct hardware access, zero-overhead abstractions | Manual memory management complexity | High-performance systems, embedded |\n| Rust | Memory safety, zero-cost abstractions | Learning curve, ownership model constraints | Systems programming, safe concurrency |\n| Go | Simple syntax, runtime integration | GC pauses, limited atomic primitives | Web services, distributed systems |\n| Java | JVM optimizations, ecosystem maturity | GC overhead, platform abstraction | Enterprise applications, frameworks |\n\n**Framework and Library Integration**\n\nModern software development relies heavily on frameworks and libraries that provide higher-level abstractions over low-level concurrency primitives. Integrating our lock-free data structures with popular frameworks requires careful attention to threading models, lifecycle management, and error handling conventions.\n\nWeb framework integration presents unique challenges due to request-scoped concurrency patterns and connection pooling. Frameworks like Django, Flask, FastAPI in Python, or Express.js in Node.js typically handle each request in separate threads or async tasks. Our lock-free data structures can provide shared state across requests without blocking, but integration requires careful attention to lifecycle management and cleanup during application shutdown.\n\nDatabase integration opportunities exist for both embedded and networked database systems. Lock-free data structures can serve as in-memory indexes, caches, or buffer pool management systems within database engines. The ACID properties required by database transactions interact interestingly with the linearizability guarantees provided by lock-free algorithms.\n\nMessage queue and event streaming integration provides another natural application domain. Systems like Apache Kafka, RabbitMQ, or Redis can benefit from lock-free internal data structures for managing queues, routing tables, and subscription lists. The ordered delivery guarantees required by messaging systems align well with the sequential consistency properties of our implementations.\n\nContainer orchestration and cloud-native integration involves packaging our implementations for deployment in Kubernetes, Docker, and serverless environments. This requires attention to resource limits, health checking, metrics collection, and graceful shutdown procedures that properly clean up lock-free data structures without losing data.\n\n**Production Deployment Considerations**\n\nMoving from development prototypes to production deployment requires systematic attention to operational concerns that don't affect correctness but critically impact reliability and maintainability. Monitoring and observability become essential for understanding behavior in production workloads that differ from development test scenarios.\n\nPerformance monitoring should track not just throughput and latency metrics, but lock-free specific indicators like CAS success rates, retry loop iterations, and memory reclamation effectiveness. High CAS failure rates indicate contention hotspots that might benefit from algorithmic changes or workload balancing. Memory reclamation monitoring helps detect leaks or inefficient cleanup that could lead to resource exhaustion.\n\nConfiguration management for lock-free systems involves tuning parameters that significantly affect performance but have non-obvious optimal values. Backoff strategies, memory reclamation thresholds, and data structure sizing parameters often require workload-specific tuning. Providing reasonable defaults while allowing runtime configuration adjustments helps balance ease of deployment with performance optimization opportunities.\n\nHealth checking and failure detection must account for the unique failure modes of lock-free systems. Traditional deadlock detection doesn't apply, but livelock detection becomes important. Progress monitoring can detect scenarios where threads are active but making no forward progress due to pathological contention patterns.\n\nDeployment automation should include verification steps that validate lock-free algorithm correctness under realistic load conditions. Stress testing with production-like workloads can expose race conditions or performance degradation that doesn't appear in unit tests. Canary deployment strategies help identify problems before full rollout.\n\n| Operational Area | Key Metrics | Alerting Thresholds | Remediation Actions |\n|------------------|-------------|-------------------|-------------------|\n| Performance | CAS success rate, operation latency | <80% success rate, >10x latency increase | Load balancing, algorithm tuning |\n| Memory Usage | Hazard pointer efficiency, node allocation rate | >10% reclamation backlog | Threshold adjustment, leak investigation |\n| Progress Guarantees | Operation completion rate, retry loop bounds | Operations stalling >1 second | Contention analysis, workload adjustment |\n| Correctness | Linearizability violations, invariant checks | Any detected violations | Immediate rollback, root cause analysis |\n\n### Implementation Guidance\n\nThe extensions described in this section represent advanced topics that build upon the solid foundation established through our core milestones. While implementing these extensions requires deep expertise in concurrent programming and hardware optimization, the systematic approach we've developed provides a pathway for gradual advancement from lock-free foundations to cutting-edge concurrent algorithms.\n\n#### Advanced Algorithm Implementation Strategy\n\nThe progression toward wait-free algorithms should begin with understanding the theoretical foundations before attempting implementations. The key insight is that wait-free algorithms trade implementation complexity and resource usage for stronger progress guarantees. Start by implementing simple wait-free counters using fetch-and-add operations, then progress to more complex structures.\n\nFor wait-free data structures, the helping mechanism becomes central to the design. Each thread must be able to assist any other thread's operation, which requires careful protocol design. Begin with consensus-based approaches where threads agree on operation ordering, then explore more specialized techniques like pre-allocation and combining trees.\n\n#### Technology Recommendations for Extensions\n\n| Extension Category | Recommended Technologies | Implementation Notes |\n|-------------------|-------------------------|---------------------|\n| Wait-free Algorithms | C++ with custom allocators | Need precise control over memory layout |\n| NUMA Optimization | libnuma, hwloc libraries | Hardware topology detection essential |\n| Hardware Transactional Memory | Intel TSX intrinsics | Requires fallback paths for unsupported hardware |\n| Language Bindings | SWIG, Cython, PyO3 | Choose based on target language ecosystem |\n| Performance Monitoring | Prometheus, StatsD | Lock-free compatible metrics collection |\n\n#### Development Roadmap for Extensions\n\nThe implementation of these extensions should follow a structured progression that builds expertise gradually:\n\n**Phase 1: Performance Optimization Foundations**\nBegin with cache line optimization and false sharing elimination. These changes provide immediate benefits while requiring minimal algorithmic changes. Implement cache line padding for atomic references and measure the performance impact under different contention scenarios.\n\n**Phase 2: Hardware-Specific Optimization**\nAdd architecture-specific atomic operation optimizations and memory ordering improvements. This phase requires platform detection and conditional compilation but provides significant performance gains on supported hardware.\n\n**Phase 3: Adaptive Mechanisms**\nImplement contention-adaptive backoff and load-adaptive sizing. These features improve performance across varying workload conditions and provide valuable experience with runtime performance monitoring.\n\n**Phase 4: Advanced Algorithms**\nProgress to wait-free algorithm implementations and specialized data structures. This phase represents the most challenging implementations but provides the deepest understanding of advanced concurrent programming techniques.\n\n**Phase 5: Production Integration**\nFocus on language bindings, framework integration, and operational tooling. This final phase transforms research implementations into production-ready systems.\n\n#### Recommended Project Structure for Extensions\n\n```\nlock-free-extensions/\n  core/                           ← Core implementations from main project\n    atomic_ops.py\n    treiber_stack.py\n    michael_scott_queue.py\n    hazard_pointers.py\n    split_ordered_hashmap.py\n  \n  advanced/                       ← Wait-free and specialized algorithms\n    wait_free/\n      universal_construction.py\n      wait_free_queue.py\n      combining_tree.py\n    specialized/\n      lock_free_tree.py\n      priority_queue.py\n      graph_structures.py\n  \n  optimizations/                  ← Performance optimizations\n    numa_aware.py\n    cache_optimized.py\n    adaptive_backoff.py\n    hardware_specific/\n      x86_atomics.py\n      arm_atomics.py\n  \n  integrations/                   ← Language and framework bindings\n    python/\n      cython_bindings.pyx\n    rust/\n      python_rust_bridge.rs\n    go/\n      cgo_bindings.go\n    frameworks/\n      django_integration.py\n      fastapi_middleware.py\n  \n  monitoring/                     ← Observability and debugging\n    performance_monitor.py\n    linearizability_checker.py\n    contention_analyzer.py\n  \n  benchmarks/                     ← Performance testing and validation\n    comparative_benchmarks.py\n    scalability_tests.py\n    correctness_verification.py\n  \n  deployment/                     ← Production deployment tooling\n    docker/\n      Dockerfile\n    kubernetes/\n      deployment.yaml\n    monitoring/\n      prometheus_config.yaml\n```\n\n#### Extension Development Guidelines\n\nEach extension should maintain the same rigorous standards for correctness and testability established in the core implementation. This means comprehensive unit tests, stress testing under high contention, and formal verification of correctness properties where possible.\n\nDocumentation for extensions must clearly explain the trade-offs involved and provide guidance on when each optimization or advanced algorithm is appropriate. Not every application benefits from wait-free guarantees or hardware-specific optimizations, and the added complexity must be justified by measurable improvements.\n\nPerformance benchmarking becomes even more critical for extensions, as the benefits often depend on specific hardware characteristics and workload patterns. Establish baseline measurements using the core implementations, then demonstrate clear improvements under relevant conditions.\n\nThe integration testing strategy should validate not just correctness but also the claimed performance benefits. Automated benchmark suites should run across different hardware configurations and workload patterns to ensure optimizations provide benefits in realistic scenarios.\n\nFinally, maintain backward compatibility with the core implementations wherever possible. Applications should be able to upgrade from basic lock-free algorithms to optimized versions without changing their usage patterns, allowing gradual adoption of more advanced techniques as requirements evolve.\n\n\n## Glossary\n\n> **Milestone(s):** All milestones (this section provides essential definitions and terminology that support understanding across the entire lock-free data structure implementation journey)\n\nUnderstanding lock-free programming requires mastery of specialized terminology that spans hardware concepts, memory models, algorithmic techniques, and verification approaches. This glossary serves as a comprehensive reference for all technical terms, acronyms, and domain-specific vocabulary used throughout the lock-free data structures design document.\n\n### Mental Model: The Technical Dictionary\n\nThink of this glossary as a specialized technical dictionary for a foreign language - the language of lock-free concurrent programming. Just as learning a new spoken language requires understanding not just individual words but also cultural context and subtle meanings, mastering lock-free programming requires understanding how terms relate to each other and the specific nuances they carry in this domain. Each term represents a carefully defined concept that has precise meaning within the broader ecosystem of concurrent programming theory and practice.\n\n### Core Atomic Operations and Memory Model Terms\n\nThe foundation of lock-free programming rests on atomic operations and memory ordering semantics that directly interact with hardware-level guarantees. These concepts form the building blocks upon which all higher-level algorithms depend.\n\n| Term | Definition | Context and Usage |\n|------|------------|-------------------|\n| **ABA problem** | A race condition where a compare-and-swap operation incorrectly succeeds because a memory location has been changed from value A to B and back to A between the initial read and the CAS attempt | Critical concern in pointer-based data structures where nodes may be deallocated and reallocated at the same address |\n| **Acquire memory ordering** | Memory ordering constraint ensuring that no memory operations can be reordered before an acquire operation, establishing synchronization with corresponding release operations | Used in loads to create happens-before relationships and ensure visibility of all writes that occurred before a matching release |\n| **Atomic operation** | An operation that appears to execute instantaneously and indivisibly from the perspective of all other threads, with guaranteed consistency across concurrent access | Forms the foundation of all lock-free algorithms by providing thread-safe primitive operations without locks |\n| **Compare-and-swap (CAS)** | Fundamental atomic read-modify-write operation that updates a memory location only if it currently contains an expected value, returning both success status and the observed value | The primary building block for implementing lock-free data structures, used in retry loops until successful update |\n| **Fetch-and-add** | Atomic operation that increments a memory location by a specified amount and returns the previous value before the increment | Commonly used for atomic counters and sequence number generation in lock-free algorithms |\n| **Memory barrier** | Hardware or compiler instruction that prevents certain types of memory operation reordering, ensuring specific ordering constraints are maintained | Essential for implementing memory ordering semantics and maintaining cache coherence in multi-core systems |\n| **Memory ordering** | Set of rules governing the visibility and ordering of memory operations across different threads, ranging from no constraints (relaxed) to full sequential consistency | Determines performance-correctness trade-offs in atomic operations, with stronger ordering providing more guarantees at higher cost |\n| **Relaxed memory ordering** | Weakest memory ordering constraint that provides atomicity guarantees but allows maximum reordering flexibility for performance | Used when only the atomicity of individual operations matters, not their ordering relative to other memory accesses |\n| **Release memory ordering** | Memory ordering constraint ensuring that no memory operations can be reordered after a release operation, making all prior writes visible to threads performing acquire operations | Used in stores to publish completed work and establish happens-before relationships with corresponding acquires |\n| **Sequential consistency** | Strongest memory ordering model where operations appear to execute in some global sequential order consistent with program order on each thread | Provides intuitive semantics but may limit performance optimizations, used when correctness is prioritized over maximum performance |\n| **Tagged pointer** | Pointer augmented with additional bits (typically version counter or generation number) to detect ABA problems by identifying when a pointer has been reused | Solves ABA problem by ensuring that even if a pointer value is reused, the tag portion will differ |\n\n### Lock-free Algorithm Classifications and Progress Guarantees\n\nLock-free programming encompasses different levels of progress guarantees, each providing specific assurances about forward progress under concurrent access patterns.\n\n| Term | Definition | Implications |\n|------|------------|--------------|\n| **Lock-free** | Non-blocking progress guarantee ensuring that at least one thread among all competing threads will complete its operation within a finite number of steps | Stronger than obstruction-free but weaker than wait-free, prevents deadlock and livelock at the system level |\n| **Wait-free** | Strongest progress guarantee where every thread completes its operation within a bounded number of steps regardless of other thread scheduling | Most expensive to implement but provides predictable latency bounds for real-time systems |\n| **Obstruction-free** | Weakest non-blocking guarantee where a thread can complete its operation if it runs in isolation for sufficiently long | Easier to implement than lock-free but provides weaker progress guarantees under contention |\n| **Linearizability** | Correctness condition requiring that concurrent operations appear to take effect atomically at some point between their invocation and response | Gold standard for correctness in concurrent data structures, enabling reasoning about concurrent execution as sequential |\n| **Linearization point** | Specific instant during an operation's execution where the operation appears to take effect atomically from the perspective of all other threads | Critical for proving correctness and understanding the precise moment when concurrent operations become visible |\n\n### Data Structure Specific Algorithms\n\nEach lock-free data structure employs specialized algorithms and techniques tailored to its specific access patterns and consistency requirements.\n\n![Data Structure Relationships](./diagrams/data-model-relationships.svg)\n\n| Term | Definition | Application Context |\n|------|------------|-------------------|\n| **Treiber stack** | Lock-free stack algorithm using single-word compare-and-swap on the top-of-stack pointer to implement push and pop operations | Simplest lock-free data structure, excellent introduction to CAS-based algorithms and ABA problem solutions |\n| **Michael-Scott queue** | Lock-free FIFO queue algorithm using separate head and tail pointers with a helping mechanism to ensure progress guarantees | Standard algorithm for lock-free queues, demonstrates two-pointer design and cooperative thread behavior |\n| **Split-ordered list** | Hash map implementation technique that maintains logical hash ordering in a physical linked list structure across bucket boundaries | Enables lock-free hash maps with incremental resizing by preserving insertion order during bucket splits |\n| **Helping mechanism** | Protocol where threads assist each other to complete operations that may have been interrupted by scheduling, ensuring system-wide progress | Essential for progress guarantees in complex algorithms like Michael-Scott queue and split-ordered hash maps |\n| **Sentinel node** | Permanent marker nodes in data structures that define boundaries and simplify edge case handling by ensuring certain pointers are never null | Simplifies queue and hash map implementations by eliminating special cases for empty structures |\n| **Dummy node** | Placeholder node used to separate operations and reduce contention, particularly in queue implementations | Michael-Scott queue uses dummy head node to separate enqueue and dequeue operations on different memory locations |\n\n### Memory Reclamation and Safety\n\nSafe memory management in lock-free programming requires sophisticated techniques to prevent use-after-free errors while maintaining non-blocking properties.\n\n| Term | Definition | Safety Guarantee |\n|------|------------|------------------|\n| **Hazard pointers** | Memory reclamation scheme where threads announce their intent to access shared nodes, preventing premature deallocation of nodes still in use | Prevents use-after-free errors by ensuring nodes remain allocated while any thread holds a hazard pointer to them |\n| **Retirement list** | Per-thread queue of nodes that have been removed from data structures but cannot yet be safely deallocated due to potential concurrent access | Enables deferred deletion until global scan confirms no thread holds hazard pointers to retired nodes |\n| **Protect-then-verify pattern** | Safe sequence for loading and protecting pointers using hazard pointers: load pointer, set hazard pointer, verify pointer is unchanged | Prevents race conditions between loading a pointer and establishing protection against its deallocation |\n| **Scan threshold** | Retirement list size that triggers a global scan of all thread hazard pointers to identify nodes safe for reclamation | Balances memory usage against scan overhead by batching reclamation operations |\n| **Thread registry** | Global data structure maintaining a list of all active threads and their hazard pointer slots for scanning during reclamation | Enables the memory reclamation system to find all hazard pointers across all threads during scan operations |\n| **Epoch-based reclamation** | Alternative memory reclamation scheme using global time periods instead of individual pointer tracking | Simpler than hazard pointers but may delay reclamation longer, trades memory efficiency for implementation simplicity |\n\n### Hash Map Specific Terminology\n\nLock-free hash maps introduce specialized concepts related to concurrent bucket management and incremental resizing operations.\n\n| Term | Definition | Purpose |\n|------|------------|---------|\n| **Bucket initialization** | Lazy creation of hash bucket sentinel nodes when first accessed, using compare-and-swap to ensure only one thread initializes each bucket | Reduces memory usage and initialization overhead by creating buckets only when needed |\n| **Incremental resizing** | Hash map expansion technique that gradually migrates entries to a larger bucket array without blocking concurrent operations | Maintains lock-free properties during resize by spreading migration work across multiple operations |\n| **Reverse bit ordering** | Technique of reversing the bit pattern of hash values to determine insertion order in split-ordered lists | Ensures that bucket splitting preserves correct ordering when hash table size doubles |\n| **Load factor threshold** | Ratio of entries to buckets that triggers resize operation to maintain acceptable performance characteristics | Balances memory usage against access performance by expanding before chains become too long |\n| **Logical deletion** | Marking nodes as deleted without immediately removing them from the linked list structure | Enables atomic deletion in the presence of concurrent traversals by allowing physical removal as separate step |\n| **Physical deletion** | Actually unlinking deleted nodes from the list structure after logical deletion has been performed | Second phase of deletion that can be performed by any thread, often combined with helping mechanism |\n\n### Concurrent Testing and Verification\n\nVerifying correctness of lock-free algorithms requires specialized testing approaches that can expose subtle race conditions and ordering violations.\n\n| Term | Definition | Testing Application |\n|------|------------|-------------------|\n| **Stress testing** | Testing approach that subjects data structures to high concurrent contention to expose race conditions and correctness violations | Primary method for validating lock-free implementations under realistic concurrent workloads |\n| **Model checking** | Exhaustive exploration of all possible thread interleavings within bounded parameters to verify correctness properties | Formal verification technique that can prove correctness but limited by state space explosion |\n| **Randomized testing** | Property-based testing using random operation sequences to discover corner cases and invariant violations | Complements stress testing by exploring diverse operation patterns that may not occur in structured tests |\n| **Linearizability checking** | Runtime verification that concurrent operations have a valid sequential ordering consistent with their real-time ordering | Gold standard for correctness verification, often implemented by recording operation history and post-processing |\n| **Race condition detection** | Techniques for identifying data races, ordering violations, and other concurrency bugs during testing | Essential for catching subtle bugs that may not manifest consistently due to timing dependencies |\n\n### Performance and Optimization Terms\n\nLock-free programming involves specific performance considerations and optimization techniques unique to concurrent algorithms.\n\n| Term | Definition | Performance Impact |\n|------|------------|-------------------|\n| **Contention hotspot** | Memory location accessed by multiple threads simultaneously, causing cache coherence traffic and performance degradation | Major source of scalability bottlenecks in concurrent data structures, requires careful algorithm design to minimize |\n| **False sharing** | Performance problem where unrelated variables sharing the same cache line cause unnecessary coherence traffic when accessed by different threads | Can severely degrade performance even when variables are not logically shared, requires careful memory layout |\n| **Exponential backoff** | Delay strategy that increases wait time after each failure, reducing contention when many threads compete for the same resource | Balances progress against wasted CPU cycles by reducing retry frequency under high contention |\n| **CAS loop spinning** | Continuous retry pattern where threads repeatedly attempt compare-and-swap operations until successful | Can waste CPU resources under high contention but necessary for lock-free progress guarantees |\n| **Cache line alignment** | Memory layout technique ensuring that frequently accessed data structures align with processor cache line boundaries | Improves performance by reducing cache misses and false sharing between unrelated data |\n| **NUMA awareness** | Optimization technique that considers Non-Uniform Memory Access costs when allocating and accessing data structures | Increasingly important on large multi-socket systems where memory access costs vary by location |\n\n### Error Handling and Debugging\n\nLock-free programming introduces unique failure modes and debugging challenges that require specialized approaches and terminology.\n\n| Term | Definition | Debugging Context |\n|------|------------|-------------------|\n| **Livelock** | Condition where threads are actively making progress individually but collectively make no forward progress due to mutual interference | Difficult to detect as threads appear active, requires analysis of global progress rather than individual thread activity |\n| **Memory reclamation race** | Timing-dependent failure where memory is deallocated while other threads still hold references, causing use-after-free errors | Particularly challenging to debug as symptoms may appear far from the actual cause in time and code location |\n| **Progress guarantee violation** | Failure of a lock-free algorithm to ensure that at least one thread makes progress within bounded steps | Indicates fundamental algorithm correctness issues rather than simple implementation bugs |\n| **Invariant violation** | Condition where data structure consistency properties are temporarily or permanently broken due to concurrent access races | Requires continuous monitoring during testing as violations may be transient and timing-dependent |\n| **ABA manifestation** | Actual occurrence of ABA problem causing incorrect algorithm behavior due to pointer reuse between read and update operations | Often appears as mysterious data corruption or lost updates that seem impossible given the algorithm logic |\n\n### Implementation and System Integration\n\nTerms related to practical implementation concerns and integration of lock-free data structures into larger systems.\n\n| Term | Definition | Integration Concern |\n|------|------------|-------------------|\n| **Graceful degradation** | System behavior that maintains correctness and basic functionality even when performance optimization fail or extreme conditions occur | Critical for production systems where lock-free algorithms must handle unexpected load patterns or failure modes |\n| **Hybrid approach** | Combining lock-free algorithms with traditional locking or other concurrency techniques where appropriate for different use cases | Recognizes that lock-free is not always optimal and allows mixing approaches based on specific requirements |\n| **Universal construction** | Theoretical technique for building wait-free implementations of any data structure from simpler atomic primitives | Primarily of academic interest but provides foundation for understanding what is theoretically possible |\n| **Flat combining** | Hybrid approach where threads occasionally take exclusive access to perform batched operations, combining benefits of locking and lock-free approaches | Practical technique for workloads where batching provides significant benefits over pure lock-free approaches |\n\n### Advanced Concepts and Future Directions\n\nTerminology related to cutting-edge research and advanced techniques in lock-free programming.\n\n| Term | Definition | Research Context |\n|------|------------|------------------|\n| **Hardware transactional memory** | CPU support for atomic execution of code blocks, providing transactional semantics at the hardware level | Emerging technology that may simplify lock-free programming by providing stronger atomic operation primitives |\n| **Wait-free universal construction** | Theoretical framework for building wait-free implementations of arbitrary data structures using consensus objects | Demonstrates theoretical limits of what can be achieved but often impractical for real implementations |\n| **Obstruction-free universal construction** | More practical approach for building non-blocking implementations using simpler primitives than full wait-free constructions | Bridge between theory and practice, providing stronger guarantees than locks with more reasonable implementation complexity |\n| **Read-copy-update (RCU)** | Synchronization mechanism allowing readers to access data structures without synchronization while writers create new versions | Specialized technique particularly effective for read-heavy workloads with infrequent updates |\n\n### Implementation Guidance\n\nThe terminology presented in this glossary provides the vocabulary necessary to understand and discuss lock-free programming concepts precisely. When implementing lock-free data structures, developers should refer to these definitions to ensure consistent understanding and communication.\n\n#### Terminology Usage Guidelines\n\n| Context | Recommended Terms | Terms to Avoid |\n|---------|------------------|----------------|\n| **Algorithm Description** | Use \"compare-and-swap\", \"linearization point\", \"helping mechanism\" | Avoid \"atomic update\", \"synchronization point\", \"cooperation\" |\n| **Memory Management** | Use \"hazard pointers\", \"retirement list\", \"protect-then-verify pattern\" | Avoid \"garbage collection\", \"reference counting\", \"smart pointers\" |\n| **Performance Discussion** | Use \"contention hotspot\", \"false sharing\", \"exponential backoff\" | Avoid \"bottleneck\", \"cache miss\", \"retry delay\" |\n| **Correctness Verification** | Use \"linearizability\", \"stress testing\", \"invariant violation\" | Avoid \"thread safety\", \"load testing\", \"assertion failure\" |\n\n#### Common Terminology Mistakes\n\n⚠️ **Pitfall: Confusing Memory Ordering Terms**\nMany developers incorrectly use \"memory barrier\" and \"memory fence\" interchangeably with \"memory ordering\". Memory ordering refers to the semantic guarantees, while barriers and fences are the mechanisms that implement those guarantees. Use \"acquire ordering\" or \"release ordering\" when discussing semantics, and \"memory barrier\" when discussing implementation mechanisms.\n\n⚠️ **Pitfall: Misusing Progress Guarantee Terminology**\nThe terms \"lock-free\", \"wait-free\", and \"obstruction-free\" have precise technical meanings that are often misused. \"Lock-free\" does not simply mean \"without locks\" - it specifies a particular progress guarantee. Always use these terms according to their formal definitions rather than intuitive interpretations.\n\n⚠️ **Pitfall: Conflating ABA Problem with General Race Conditions**\nThe ABA problem is a specific type of race condition related to pointer reuse in compare-and-swap operations. Not all race conditions are ABA problems, and not all pointer-related bugs are ABA manifestations. Use \"ABA problem\" only when specifically referring to the scenario where CAS succeeds incorrectly due to pointer reuse.\n\n#### Glossary Maintenance and Evolution\n\nThis glossary represents the current state of terminology in lock-free programming. As the field evolves and new techniques emerge, terminology may be refined or extended. When encountering new terms or concepts, developers should:\n\n1. **Verify formal definitions** from authoritative sources such as academic papers or standards documents\n2. **Understand context-specific meanings** that may differ from general usage in other domains  \n3. **Maintain consistency** with established terminology when describing algorithms or implementations\n4. **Document new terms** that emerge from practical experience or research developments\n\nThe precision of terminology directly impacts the ability to reason about correctness, communicate design decisions, and maintain implementations over time. Mastery of this vocabulary is essential for anyone working with lock-free data structures.\n"}