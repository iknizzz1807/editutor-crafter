{"html":"<h1 id=\"log-aggregation-system-design-document\">Log Aggregation System: Design Document</h1>\n<h2 id=\"overview\">Overview</h2>\n<p>A distributed log aggregation system that collects, indexes, and queries log data at scale, similar to Grafana Loki. The key architectural challenge is efficiently ingesting high-volume log streams while maintaining fast query performance through smart indexing and storage strategies.</p>\n<blockquote>\n<p>This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.</p>\n</blockquote>\n<h2 id=\"context-and-problem-statement\">Context and Problem Statement</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section provides foundational understanding that applies to all milestones (1-5), establishing the problem space and design constraints.</p>\n</blockquote>\n<h3 id=\"mental-model-the-library-analogy\">Mental Model: The Library Analogy</h3>\n<p>Before diving into the technical complexities of log aggregation, imagine you&#39;re tasked with organizing a vast, ever-growing library that receives thousands of new books every minute, 24 hours a day. These books come in different languages, formats, and subjects, and researchers need to find specific information quickly across millions of volumes.</p>\n<p>Traditional libraries solve this through <strong>card catalogs</strong> — organized indexes that tell you exactly which shelf holds the book you need. But imagine if researchers also needed to search for specific phrases within books, or find all books mentioning certain topics during particular time periods. You&#39;d need not just location indexes, but also content indexes, subject indexes, and temporal organization systems.</p>\n<p>Now scale this analogy: instead of thousands of books daily, imagine millions arriving every second. Instead of human librarians manually cataloging each book, you need automated systems that can instantly categorize, index, and shelve new arrivals while simultaneously serving hundreds of researchers performing complex searches. The books never stop coming, researchers demand sub-second response times, and you can&#39;t afford to lose a single volume.</p>\n<p>This is precisely the challenge of <strong>log aggregation</strong>. Each log entry is like a book arriving at the library. The &quot;subjects&quot; are the labels and structured fields (service name, log level, timestamp). The &quot;content search&quot; is full-text search across log messages. The &quot;temporal organization&quot; is time-based partitioning. The &quot;card catalog&quot; is your inverted index, and the &quot;quick negative lookup&quot; (knowing a book definitely isn&#39;t in a certain section) is provided by bloom filters.</p>\n<p>The fundamental insight from this analogy is that log aggregation systems must solve two seemingly conflicting requirements:</p>\n<ol>\n<li><strong>Continuous high-speed ingestion</strong> — like books arriving faster than you can manually process them</li>\n<li><strong>Interactive query performance</strong> — like researchers expecting instant answers to complex questions</li>\n</ol>\n<blockquote>\n<p>The core architectural challenge is building a system that can ingest logs at write-optimized speeds while serving queries at read-optimized speeds, despite these two access patterns having fundamentally different performance characteristics.</p>\n</blockquote>\n<h3 id=\"existing-approaches-comparison\">Existing Approaches Comparison</h3>\n<p>The log aggregation space has evolved through several architectural generations, each making different trade-offs between ingestion performance, query flexibility, storage efficiency, and operational complexity. Understanding these trade-offs is crucial for appreciating why building a Loki-style system presents unique challenges.</p>\n<h4 id=\"elk-stack-elasticsearch-logstash-kibana\">ELK Stack (Elasticsearch, Logstash, Kibana)</h4>\n<p>The <strong>ELK Stack</strong> represents the &quot;index everything&quot; approach to log aggregation. Elasticsearch creates full inverted indexes on every field of every log entry, enabling extremely flexible and fast queries at the cost of significant storage overhead and indexing complexity.</p>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>ELK Approach</th>\n<th>Trade-offs</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Indexing Strategy</strong></td>\n<td>Full-text index on all fields</td>\n<td>Maximum query flexibility but 3-5x storage overhead</td>\n</tr>\n<tr>\n<td><strong>Ingestion Path</strong></td>\n<td>Logstash parsing → ES indexing</td>\n<td>Rich transformation capabilities but complex pipeline</td>\n</tr>\n<tr>\n<td><strong>Query Performance</strong></td>\n<td>Sub-second for most queries</td>\n<td>Excellent for ad-hoc exploration but expensive for high cardinality</td>\n</tr>\n<tr>\n<td><strong>Storage Model</strong></td>\n<td>Document-oriented with replicas</td>\n<td>Easy horizontal scaling but storage costs grow quickly</td>\n</tr>\n<tr>\n<td><strong>Operational Complexity</strong></td>\n<td>Multiple components to manage</td>\n<td>Powerful but requires specialized Elasticsearch expertise</td>\n</tr>\n</tbody></table>\n<p>The ELK approach excels when you need maximum query flexibility and have budget for storage costs. However, it struggles with <strong>high-cardinality labels</strong> (like user IDs or request IDs) because indexing every unique value creates massive indexes. A single label with millions of unique values can make the index larger than the original log data.</p>\n<h4 id=\"splunk\">Splunk</h4>\n<p><strong>Splunk</strong> takes a &quot;store raw, index selectively&quot; approach. It stores log data in compressed raw format and builds indexes only on selected fields, using a proprietary search language (SPL) for queries.</p>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>Splunk Approach</th>\n<th>Trade-offs</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Indexing Strategy</strong></td>\n<td>Selective indexing with raw storage</td>\n<td>Balanced flexibility and cost but requires index planning</td>\n</tr>\n<tr>\n<td><strong>Ingestion Path</strong></td>\n<td>Universal forwarders → indexers</td>\n<td>Robust ingestion but proprietary agent deployment</td>\n</tr>\n<tr>\n<td><strong>Query Performance</strong></td>\n<td>Fast for indexed fields, slower for full-text</td>\n<td>Predictable performance but requires careful index design</td>\n</tr>\n<tr>\n<td><strong>Storage Model</strong></td>\n<td>Time-based buckets with compression</td>\n<td>Excellent compression but vendor lock-in</td>\n</tr>\n<tr>\n<td><strong>Operational Complexity</strong></td>\n<td>Integrated platform</td>\n<td>Easier operations but expensive licensing</td>\n</tr>\n</tbody></table>\n<p>Splunk&#39;s strength is its mature ecosystem and enterprise features. However, its licensing model based on daily ingestion volume makes it prohibitively expensive for high-volume environments, and its proprietary nature limits customization options.</p>\n<h4 id=\"grafana-loki\">Grafana Loki</h4>\n<p><strong>Loki</strong> pioneered the &quot;index only metadata&quot; approach, inspired by Prometheus&#39;s success with metrics. Instead of indexing log content, it indexes only labels (key-value pairs) and stores log content in compressed chunks. This dramatically reduces index size while maintaining reasonable query performance.</p>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>Loki Approach</th>\n<th>Trade-offs</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Indexing Strategy</strong></td>\n<td>Labels only, not log content</td>\n<td>Minimal storage overhead but requires structured labels</td>\n</tr>\n<tr>\n<td><strong>Ingestion Path</strong></td>\n<td>Label extraction → chunk storage</td>\n<td>Simple pipeline but labels must be designed upfront</td>\n</tr>\n<tr>\n<td><strong>Query Performance</strong></td>\n<td>Fast for label queries, uses grep for content</td>\n<td>Efficient for structured logs but slower full-text search</td>\n</tr>\n<tr>\n<td><strong>Storage Model</strong></td>\n<td>Compressed chunks by time and labels</td>\n<td>Excellent compression but query performance depends on label design</td>\n</tr>\n<tr>\n<td><strong>Operational Complexity</strong></td>\n<td>Fewer moving parts</td>\n<td>Simpler than ELK but requires understanding of label cardinality</td>\n</tr>\n</tbody></table>\n<p>Loki&#39;s innovation is recognizing that most log queries follow predictable patterns: filtering by service, environment, or log level, then searching within that filtered set. By indexing only these &quot;dimensions&quot; and using efficient grep-like search on compressed chunks, Loki achieves 90% of query use cases with 10% of the storage cost.</p>\n<blockquote>\n<p><strong>Decision: Label-Only Indexing Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to balance query performance with storage efficiency for high-volume log ingestion</li>\n<li><strong>Options Considered</strong>: Full-text indexing (ELK approach), selective field indexing (Splunk approach), label-only indexing (Loki approach)</li>\n<li><strong>Decision</strong>: Implement label-only indexing similar to Loki</li>\n<li><strong>Rationale</strong>: Full-text indexing creates unsustainable storage overhead for high-volume environments (3-5x data size), while label-only indexing provides sufficient query performance for most use cases with minimal storage overhead</li>\n<li><strong>Consequences</strong>: Enables cost-effective scaling but requires careful label design and slower full-text search performance</li>\n</ul>\n</blockquote>\n<h4 id=\"architectural-trade-off-analysis\">Architectural Trade-off Analysis</h4>\n<table>\n<thead>\n<tr>\n<th>System</th>\n<th>Storage Overhead</th>\n<th>Query Flexibility</th>\n<th>Ingestion Rate</th>\n<th>Operational Complexity</th>\n<th>Cost at Scale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>ELK Stack</strong></td>\n<td>3-5x original data</td>\n<td>Maximum</td>\n<td>Medium</td>\n<td>High</td>\n<td>Very High</td>\n</tr>\n<tr>\n<td><strong>Splunk</strong></td>\n<td>1.5-2x original data</td>\n<td>High</td>\n<td>High</td>\n<td>Medium</td>\n<td>High</td>\n</tr>\n<tr>\n<td><strong>Loki</strong></td>\n<td>1.1-1.3x original data</td>\n<td>Medium</td>\n<td>Very High</td>\n<td>Low</td>\n<td>Low</td>\n</tr>\n</tbody></table>\n<p>The table reveals why Loki-style systems are attractive for modern cloud environments: they provide the best cost/performance ratio for typical log aggregation workloads. However, this comes with the significant challenge of making label-only indexing work well in practice.</p>\n<h3 id=\"core-technical-challenges\">Core Technical Challenges</h3>\n<p>Building a Loki-style log aggregation system presents several interconnected technical challenges that must be solved simultaneously. These challenges are what make this project compelling — each one requires careful design decisions with non-obvious trade-offs.</p>\n<h4 id=\"challenge-1-high-velocity-log-ingestion\">Challenge 1: High-Velocity Log Ingestion</h4>\n<p>Modern distributed systems generate logs at unprecedented rates. A typical microservices application might produce 100,000+ log entries per second across all services, with burst rates reaching 1 million entries per second during peak traffic or incident scenarios.</p>\n<p><strong>Volume Characteristics:</strong></p>\n<ul>\n<li><strong>Sustained rates</strong>: 10,000-100,000 entries/second typical for medium-scale systems</li>\n<li><strong>Burst rates</strong>: 10x-100x sustained rates during traffic spikes or cascading failures</li>\n<li><strong>Entry sizes</strong>: 500 bytes to 10KB per entry, with JSON formatting adding 30-50% overhead</li>\n<li><strong>Total throughput</strong>: 50MB/second to 1GB/second of raw log data</li>\n</ul>\n<p>The ingestion challenge has multiple dimensions:</p>\n<p><strong>Protocol Handling Complexity</strong>: Logs arrive via multiple protocols (HTTP POST, TCP syslog, UDP syslog, file tailing), each with different reliability and performance characteristics. HTTP provides reliability but higher per-request overhead. UDP provides minimal overhead but no delivery guarantees. TCP syslog balances reliability and performance but requires connection management.</p>\n<p><strong>Parse-Time Pressure</strong>: Each log entry must be parsed to extract labels and structured fields during ingestion, not at query time. This parsing must happen at wire speed without becoming the bottleneck. Regular expressions for unstructured log parsing can consume significant CPU, while JSON parsing must handle malformed inputs gracefully.</p>\n<p><strong>Buffering Strategy Complexity</strong>: The system must buffer incoming logs to smooth out burst rates and handle downstream component failures. Memory buffering provides speed but risks data loss on crashes. Disk buffering provides durability but adds latency and I/O pressure. The system needs hybrid buffering strategies that adapt to load conditions.</p>\n<blockquote>\n<p>The critical insight for ingestion is that you cannot optimize for the average case — the system must gracefully handle burst rates that are 10x-100x the sustained rate, because these bursts often occur during incidents when log data is most critical.</p>\n</blockquote>\n<h4 id=\"challenge-2-efficient-label-based-indexing\">Challenge 2: Efficient Label-Based Indexing</h4>\n<p>The label-only indexing approach creates a complex indexing challenge: building indexes that enable fast label-based filtering while avoiding the cardinality explosion that plagues traditional full-text indexing.</p>\n<p><strong>Label Cardinality Problems:</strong></p>\n<ul>\n<li><strong>Low cardinality labels</strong> (service, environment, log_level) are ideal for indexing</li>\n<li><strong>High cardinality labels</strong> (user_id, request_id, session_id) create massive indexes</li>\n<li><strong>Cardinality explosion</strong> occurs when label combinations create millions of unique label sets</li>\n<li><strong>Index size blowup</strong> happens when index metadata becomes larger than the original log data</li>\n</ul>\n<p><strong>Bloom Filter Integration Complexity</strong>: Bloom filters provide fast negative lookups (&quot;this label combination definitely doesn&#39;t exist in this time range&quot;) but introduce probabilistic behavior. False positives mean the system must verify bloom filter hits against actual data. False negative rates must be tuned carefully — too high and the bloom filters become useless, too low and they consume excessive memory.</p>\n<p><strong>Time-Based Partitioning Strategy</strong>: Log data must be partitioned by time to enable efficient time-range queries, but the partitioning granularity affects both query performance and index management complexity. Hourly partitions provide good query selectivity but create many small indexes to manage. Daily partitions reduce management overhead but may scan unnecessary data for short time-range queries.</p>\n<p><strong>Index Compaction Requirements</strong>: As log volume grows, small index segments must be merged into larger ones to maintain query performance. This compaction process must happen continuously without blocking ingestion or queries, requiring careful coordination between read and write operations.</p>\n<blockquote>\n<p><strong>Decision: Hierarchical Label Indexing with Bloom Filters</strong></p>\n<ul>\n<li><strong>Context</strong>: Need fast label-based filtering while avoiding cardinality explosion from high-cardinality labels</li>\n<li><strong>Options Considered</strong>: Flat label indexing, hierarchical indexing, label value sampling, bloom filter pre-filtering</li>\n<li><strong>Decision</strong>: Use hierarchical indexing with bloom filters for negative lookups and label cardinality limits</li>\n<li><strong>Rationale</strong>: Hierarchical structure allows efficient range queries while bloom filters eliminate unnecessary disk reads, and cardinality limits prevent index explosion</li>\n<li><strong>Consequences</strong>: Enables sub-second label queries but requires careful bloom filter tuning and label design guidelines</li>\n</ul>\n</blockquote>\n<h4 id=\"challenge-3-query-performance-optimization\">Challenge 3: Query Performance Optimization</h4>\n<p>The label-only indexing approach shifts complexity from ingestion time to query time. Queries must efficiently combine label-based filtering with full-text search across compressed log data.</p>\n<p><strong>Query Planning Complexity</strong>: A typical LogQL query like <code>{service=&quot;api&quot;, level=&quot;error&quot;} |= &quot;timeout&quot;</code> requires:</p>\n<ol>\n<li><strong>Label filtering</strong>: Find all log chunks where service=api AND level=error</li>\n<li><strong>Time range filtering</strong>: Narrow to chunks within the query time range</li>\n<li><strong>Bloom filter checking</strong>: Use bloom filters to eliminate chunks that definitely don&#39;t contain &quot;timeout&quot;</li>\n<li><strong>Chunk decompression</strong>: Decompress remaining chunks and search for &quot;timeout&quot;</li>\n<li><strong>Result aggregation</strong>: Combine results from multiple chunks and return in time order</li>\n</ol>\n<p>Each step must be optimized, and the query planner must decide the most efficient execution order based on label selectivity and time range size.</p>\n<p><strong>Full-Text Search Performance</strong>: Unlike traditional full-text indexes, the system must perform grep-like searches across compressed chunks. This requires:</p>\n<ul>\n<li><strong>Efficient decompression</strong>: Chunks must decompress quickly enough to maintain interactive query speeds</li>\n<li><strong>Streaming search</strong>: Large chunks must be searched without loading entirely into memory</li>\n<li><strong>Regular expression optimization</strong>: Complex regex patterns must be compiled and executed efficiently</li>\n<li><strong>Result streaming</strong>: Query results must start returning before all chunks are processed</li>\n</ul>\n<p><strong>Concurrent Query Handling</strong>: The system must serve multiple concurrent queries efficiently, sharing decompressed chunk data between queries when possible and managing memory usage to prevent resource exhaustion.</p>\n<p><strong>Pagination and Sorting Challenges</strong>: Log queries often return millions of results that must be paginated efficiently. Time-based sorting is natural for logs, but other sort orders (relevance, label values) require additional processing.</p>\n<blockquote>\n<p>The query performance challenge is fundamentally about making grep-scale performance feel interactive. Users expect sub-second response times even when searching terabytes of log data, which requires aggressive optimization at every level.</p>\n</blockquote>\n<h4 id=\"challenge-4-storage-efficiency-and-durability\">Challenge 4: Storage Efficiency and Durability</h4>\n<p>The storage layer must provide both efficiency (high compression ratios, fast access) and durability (no data loss, quick recovery) while supporting the access patterns of both ingestion and querying.</p>\n<p><strong>Compression Strategy Complexity</strong>: Log data is highly compressible (typical ratios of 5:1 to 10:1), but compression choice affects both storage efficiency and query performance. Fast compression algorithms (LZ4, Snappy) enable quick decompression during queries but provide lower compression ratios. High-efficiency algorithms (zstd, gzip) provide better compression but slower decompression.</p>\n<p><strong>Write-Ahead Log Design</strong>: The WAL must ensure durability without becoming a performance bottleneck. It must handle:</p>\n<ul>\n<li><strong>High write rates</strong>: Thousands of log entries per second must be persisted durably</li>\n<li><strong>Batch optimization</strong>: Small writes must be batched for efficiency without adding excessive latency</li>\n<li><strong>Recovery performance</strong>: After crashes, WAL replay must be fast enough to minimize downtime</li>\n<li><strong>WAL cleanup</strong>: Processed entries must be cleaned up without interrupting ongoing writes</li>\n</ul>\n<p><strong>Chunk Organization Strategy</strong>: Log data must be organized into chunks that balance:</p>\n<ul>\n<li><strong>Query efficiency</strong>: Chunks should align with common query patterns (time ranges, label combinations)</li>\n<li><strong>Storage efficiency</strong>: Chunks should be large enough for good compression but small enough for efficient partial reads</li>\n<li><strong>Ingestion performance</strong>: New data must be written efficiently without fragmenting storage</li>\n</ul>\n<p><strong>Retention Policy Implementation</strong>: Old log data must be automatically deleted based on configurable policies, but retention cleanup must not interfere with ongoing queries or create storage consistency issues.</p>\n<blockquote>\n<p><strong>Decision: Time-Windowed Chunks with Adaptive Compression</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to balance storage efficiency, query performance, and ingestion speed</li>\n<li><strong>Options Considered</strong>: Fixed-size chunks, time-based chunks, label-based chunks, hybrid approaches</li>\n<li><strong>Decision</strong>: Use time-windowed chunks (1-hour windows) with compression algorithm selection based on chunk age</li>\n<li><strong>Rationale</strong>: Time-based chunking aligns with common query patterns, while adaptive compression uses fast algorithms for recent data (likely to be queried) and high-efficiency algorithms for older data</li>\n<li><strong>Consequences</strong>: Provides good query performance and storage efficiency but adds complexity in compression management</li>\n</ul>\n</blockquote>\n<h4 id=\"challenge-5-system-coordination-and-consistency\">Challenge 5: System Coordination and Consistency</h4>\n<p>A log aggregation system involves multiple concurrent processes (ingestion, indexing, compaction, querying) that must coordinate without creating bottlenecks or inconsistencies.</p>\n<p><strong>Concurrent Access Management</strong>: Multiple processes need different types of access to the same data:</p>\n<ul>\n<li><strong>Ingestion processes</strong>: Need exclusive write access to active chunks</li>\n<li><strong>Query processes</strong>: Need concurrent read access to stable chunks  </li>\n<li><strong>Compaction processes</strong>: Need exclusive access to merge small chunks into larger ones</li>\n<li><strong>Retention processes</strong>: Need exclusive access to delete old chunks</li>\n</ul>\n<p><strong>Index Consistency Maintenance</strong>: The inverted indexes must remain consistent with the stored log data even as chunks are being written, compacted, and deleted. Index updates must be atomic — either fully applied or not applied at all.</p>\n<p><strong>Failure Recovery Coordination</strong>: When components fail and restart, they must coordinate with other running components to avoid conflicts or data corruption. A compaction process that crashes mid-operation must not leave partially merged chunks that confuse the query engine.</p>\n<p><strong>Backpressure Propagation</strong>: When downstream components (indexing, storage) cannot keep up with ingestion rates, backpressure must propagate back to ingestion sources without causing data loss or cascading failures.</p>\n<blockquote>\n<p>The coordination challenge is what transforms this from a simple storage system into a distributed systems problem. Even a single-node implementation must solve these coordination problems between different concurrent processes.</p>\n</blockquote>\n<p>These five core challenges are interconnected — decisions made to solve one challenge directly impact the others. For example, choosing smaller chunk sizes improves query selectivity but increases index management complexity. Using more aggressive compression improves storage efficiency but slows query performance. The art of building a log aggregation system lies in finding the sweet spots that balance all these competing requirements.</p>\n<p>⚠️ <strong>Pitfall: Underestimating Label Cardinality Impact</strong>\nLabel cardinality is the most common cause of performance problems in label-only indexing systems. A single high-cardinality label (like <code>user_id</code> with millions of unique values) can create an index larger than the original log data. This happens because the index must store metadata for every unique label combination, and the combinations grow exponentially with cardinality. To avoid this, establish cardinality limits (e.g., maximum 10,000 unique values per label) and provide clear guidelines to developers about which fields should become labels versus which should remain in log message content.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Burst Rate Requirements</strong>\nMany systems are designed for average ingestion rates but fail catastrophically during burst rates that occur during incidents. Since incidents are precisely when log data becomes most critical, the system must handle burst rates of 10x-100x the sustained rate. This requires over-provisioning buffers, implementing circuit breakers, and having clear degradation strategies (e.g., sampling log entries during extreme bursts rather than dropping them entirely).</p>\n<p>⚠️ <strong>Pitfall: Treating Compression as an Afterthought</strong>\nCompression choice has profound impacts on both storage costs and query performance. Many implementations default to general-purpose compression (gzip) without considering that log data has specific characteristics that benefit from different approaches. Recent chunks (likely to be queried frequently) should use fast decompression algorithms like LZ4, while older chunks can use high-efficiency algorithms like zstd. Additionally, chunk size dramatically affects compression ratio — chunks smaller than 1MB typically compress poorly, while chunks larger than 100MB create query performance problems.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides concrete technology recommendations and architectural patterns for implementing the concepts discussed above. The focus is on Go-based implementations that provide good performance characteristics for log aggregation workloads.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>HTTP Ingestion</strong></td>\n<td><code>net/http</code> with <code>encoding/json</code></td>\n<td><code>fiber/v2</code> with <code>sonic</code> JSON</td>\n<td>net/http provides reliability; fiber+sonic for high performance</td>\n</tr>\n<tr>\n<td><strong>TCP/UDP Syslog</strong></td>\n<td><code>net</code> package with custom parsing</td>\n<td><code>go-syslog</code> library</td>\n<td>Custom parsing for learning; library for production robustness</td>\n</tr>\n<tr>\n<td><strong>Storage Backend</strong></td>\n<td>Local filesystem with <code>os</code> package</td>\n<td><code>badger</code> embedded database</td>\n<td>Filesystem for simplicity; badger for advanced features</td>\n</tr>\n<tr>\n<td><strong>Compression</strong></td>\n<td><code>compress/gzip</code> standard library</td>\n<td><code>klauspost/compress</code> optimized</td>\n<td>Standard gzip for start; optimized library for performance</td>\n</tr>\n<tr>\n<td><strong>Serialization</strong></td>\n<td><code>encoding/json</code> for simplicity</td>\n<td><code>msgpack</code> for efficiency</td>\n<td>JSON for development ease; msgpack for production efficiency</td>\n</tr>\n<tr>\n<td><strong>Bloom Filters</strong></td>\n<td><code>bits.OnesCount</code> bit operations</td>\n<td><code>willf/bloom</code> optimized library</td>\n<td>Bit operations for learning; library for production</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-project-structure\">Recommended Project Structure</h4>\n<p>Organizing the codebase properly from the start prevents the common mistake of cramming everything into a single large file. This structure supports the incremental development approach across milestones:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>log-aggregator/\n├── cmd/\n│   ├── server/main.go              ← HTTP/TCP/UDP server entry point\n│   ├── ingest/main.go              ← Log ingestion CLI tool\n│   └── query/main.go               ← Query CLI tool\n├── internal/\n│   ├── ingestion/                  ← Milestone 1: Log Ingestion\n│   │   ├── http_handler.go         ← HTTP POST endpoint\n│   │   ├── syslog_handler.go       ← TCP/UDP syslog receiver\n│   │   ├── file_tail.go            ← File watching and tailing\n│   │   ├── parser.go               ← JSON/syslog/regex parsing\n│   │   ├── buffer.go               ← Memory/disk buffering\n│   │   └── ingestion_test.go       ← Integration tests\n│   ├── indexing/                   ← Milestone 2: Log Index\n│   │   ├── inverted_index.go       ← Term-to-document mapping\n│   │   ├── bloom_filter.go         ← Negative lookup optimization\n│   │   ├── partitions.go           ← Time-based partitioning\n│   │   ├── compaction.go           ← Index segment merging\n│   │   └── indexing_test.go        ← Index behavior tests\n│   ├── query/                      ← Milestone 3: Query Engine\n│   │   ├── parser.go               ← LogQL parsing and AST\n│   │   ├── planner.go              ← Query execution planning\n│   │   ├── executor.go             ← Search execution\n│   │   ├── results.go              ← Result processing and pagination\n│   │   └── query_test.go           ← Query correctness tests\n│   ├── storage/                    ← Milestone 4: Storage &amp; Compression\n│   │   ├── chunks.go               ← Chunk-based storage\n│   │   ├── compression.go          ← Compression strategies\n│   │   ├── wal.go                  ← Write-ahead logging\n│   │   ├── retention.go            ← Cleanup policies\n│   │   └── storage_test.go         ← Storage durability tests\n│   ├── tenancy/                    ← Milestone 5: Multi-Tenant &amp; Alerting\n│   │   ├── isolation.go            ← Tenant separation\n│   │   ├── ratelimit.go            ← Per-tenant limits\n│   │   ├── alerting.go             ← Pattern-based alerts\n│   │   └── tenancy_test.go         ← Multi-tenant behavior tests\n│   └── shared/                     ← Common types and utilities\n│       ├── types.go                ← LogEntry, Labels, common structs\n│       ├── config.go               ← Configuration management\n│       └── metrics.go              ← Prometheus metrics\n├── pkg/                            ← Public APIs (if exposing libraries)\n│   └── client/                     ← Query client library\n├── configs/\n│   ├── server.yaml                 ← Server configuration\n│   └── docker-compose.yaml         ← Local development setup\n├── scripts/\n│   ├── generate-logs.sh            ← Test data generation\n│   └── benchmark.sh                ← Performance testing\n├── docs/\n│   ├── api.md                      ← HTTP API documentation\n│   └── logql.md                    ← Query language reference\n├── go.mod\n├── go.sum\n├── Makefile                        ← Build and test automation\n└── README.md</code></pre></div>\n\n<p>This structure allows you to work on one milestone at a time while maintaining clear separation of concerns. Each milestone maps to a specific <code>internal/</code> directory, making it easy to focus on one component without getting overwhelmed by the full system complexity.</p>\n<h4 id=\"core-data-types-shared-foundation\">Core Data Types (Shared Foundation)</h4>\n<p>The foundation for all components starts with these core types in <code>internal/shared/types.go</code>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> shared</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LogEntry represents a single log entry with labels and content</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LogEntry</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Define fields for Timestamp, Labels, Message, SourceInfo</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use time.Time for timestamps, map[string]string for labels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Consider: What metadata do you need for querying and storage?</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Labels represents the key-value pairs used for indexing and filtering</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Labels</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LogStream represents a sequence of log entries with the same label set</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LogStream</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Define fields for Labels, Entries, metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: This groups entries by label combination for efficient storage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TimeRange represents a time window for queries and partitioning</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TimeRange</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Define Start and End time fields</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Consider: How will this be used in query planning and chunk selection?</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p>To help you focus on the core learning objectives rather than getting stuck on infrastructure details, here&#39;s complete starter code for common utilities:</p>\n<p><strong>Configuration Management (<code>internal/shared/config.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> shared</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strconv</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Config holds all system configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Config</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    HTTPPort     </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TCPPort      </span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    UDPPort      </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StoragePath  </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BufferSize   </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ChunkSize    </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RetentionDays </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadConfig loads configuration from environment variables with defaults</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> LoadConfig</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        HTTPPort:     </span><span style=\"color:#B392F0\">getEnvInt</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"HTTP_PORT\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">8080</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TCPPort:      </span><span style=\"color:#B392F0\">getEnvInt</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"TCP_PORT\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1514</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        UDPPort:      </span><span style=\"color:#B392F0\">getEnvInt</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"UDP_PORT\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1514</span><span style=\"color:#E1E4E8\">), </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        StoragePath:  </span><span style=\"color:#B392F0\">getEnv</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"STORAGE_PATH\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"./data\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        BufferSize:   </span><span style=\"color:#B392F0\">getEnvInt</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"BUFFER_SIZE\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">10000</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ChunkSize:    </span><span style=\"color:#B392F0\">getEnvInt</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"CHUNK_SIZE\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#E1E4E8\">), </span><span style=\"color:#6A737D\">// 1MB default</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        RetentionDays: </span><span style=\"color:#B392F0\">getEnvInt</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"RETENTION_DAYS\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> getEnv</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">defaultValue</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> value </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">Getenv</span><span style=\"color:#E1E4E8\">(key); value </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> defaultValue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> getEnvInt</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">defaultValue</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> value </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">Getenv</span><span style=\"color:#E1E4E8\">(key); value </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> parsed, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> strconv.</span><span style=\"color:#B392F0\">Atoi</span><span style=\"color:#E1E4E8\">(value); err </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> parsed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> defaultValue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Simple Metrics Collection (<code>internal/shared/metrics.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> shared</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync/atomic</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Metrics holds system performance counters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Metrics</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LogsIngested    </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LogsIndexed     </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    QueriesExecuted </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BytesStored     </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LastActivity    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Global metrics instance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">var</span><span style=\"color:#E1E4E8\"> GlobalMetrics </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Metrics</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// IncrementLogsIngested safely increments the ingestion counter</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Metrics</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">IncrementLogsIngested</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    atomic.</span><span style=\"color:#B392F0\">AddInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">m.LogsIngested, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.LastActivity </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// IncrementQueriesExecuted safely increments the query counter  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Metrics</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">IncrementQueriesExecuted</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    atomic.</span><span style=\"color:#B392F0\">AddInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">m.QueriesExecuted, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.LastActivity </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetStats returns current metric values safely</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Metrics</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetStats</span><span style=\"color:#E1E4E8\">() (</span><span style=\"color:#FFAB70\">logs</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">queries</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> atomic.</span><span style=\"color:#B392F0\">LoadInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">m.LogsIngested), atomic.</span><span style=\"color:#B392F0\">LoadInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">m.QueriesExecuted)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint-guidelines\">Milestone Checkpoint Guidelines</h4>\n<p>Each milestone should have clear, testable acceptance criteria. Here&#39;s how to verify your implementation at each stage:</p>\n<p><strong>Milestone 1 Checkpoint - Log Ingestion:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Start your server</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/server/main.go</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test HTTP ingestion</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/v1/push</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -H</span><span style=\"color:#9ECBFF\"> \"Content-Type: application/json\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -d</span><span style=\"color:#9ECBFF\"> '{\"timestamp\":\"2024-01-01T12:00:00Z\",\"level\":\"info\",\"service\":\"api\",\"message\":\"test log\"}'</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: HTTP 200 response, log entry stored/buffered</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Verify: Check logs show successful ingestion, storage directory has data</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test TCP syslog ingestion  </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">echo</span><span style=\"color:#9ECBFF\"> '&#x3C;14>2024-01-01T12:00:00Z myhost myservice: test syslog message'</span><span style=\"color:#F97583\"> |</span><span style=\"color:#B392F0\"> nc</span><span style=\"color:#9ECBFF\"> localhost</span><span style=\"color:#79B8FF\"> 1514</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: TCP connection accepted, syslog parsed correctly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Verify: Check parsed fields match RFC format</span></span></code></pre></div>\n\n<p><strong>Performance Verification:</strong>\nYour ingestion pipeline should handle at least 10,000 entries/second on a standard development machine. Test with:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Generate high-volume test data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#B392F0\">1..10000}</span><span style=\"color:#E1E4E8\">; </span><span style=\"color:#F97583\">do</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">  curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/v1/push</span><span style=\"color:#79B8FF\"> -d</span><span style=\"color:#9ECBFF\"> \"{</span><span style=\"color:#79B8FF\">\\\"</span><span style=\"color:#9ECBFF\">timestamp</span><span style=\"color:#79B8FF\">\\\"</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">\\\"</span><span style=\"color:#9ECBFF\">$(</span><span style=\"color:#B392F0\">date</span><span style=\"color:#79B8FF\"> -Iseconds</span><span style=\"color:#9ECBFF\">)</span><span style=\"color:#79B8FF\">\\\"</span><span style=\"color:#9ECBFF\">,</span><span style=\"color:#79B8FF\">\\\"</span><span style=\"color:#9ECBFF\">level</span><span style=\"color:#79B8FF\">\\\"</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">\\\"</span><span style=\"color:#9ECBFF\">info</span><span style=\"color:#79B8FF\">\\\"</span><span style=\"color:#9ECBFF\">,</span><span style=\"color:#79B8FF\">\\\"</span><span style=\"color:#9ECBFF\">message</span><span style=\"color:#79B8FF\">\\\"</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">\\\"</span><span style=\"color:#9ECBFF\">test </span><span style=\"color:#E1E4E8\">$i</span><span style=\"color:#79B8FF\">\\\"</span><span style=\"color:#9ECBFF\">}\"</span><span style=\"color:#E1E4E8\"> &#x26;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">done</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">wait</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Check metrics endpoint for ingestion rate</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> http://localhost:8080/metrics</span></span></code></pre></div>\n\n<p>If ingestion rate falls below targets, common issues include:</p>\n<ul>\n<li><strong>Unbuffered I/O</strong>: Ensure you&#39;re batching writes to storage</li>\n<li><strong>JSON parsing overhead</strong>: Consider switching to faster JSON libraries</li>\n<li><strong>Lock contention</strong>: Use channels instead of shared memory where possible</li>\n</ul>\n<h4 id=\"language-specific-implementation-hints\">Language-Specific Implementation Hints</h4>\n<p><strong>JSON Parsing Performance:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Use json.Decoder for streaming parsing instead of json.Unmarshal</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">decoder </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> json.</span><span style=\"color:#B392F0\">NewDecoder</span><span style=\"color:#E1E4E8\">(request.Body)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">var</span><span style=\"color:#E1E4E8\"> logEntry </span><span style=\"color:#B392F0\">LogEntry</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> decoder.</span><span style=\"color:#B392F0\">Decode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">logEntry); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Handle parsing error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Efficient String Operations:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Use strings.Builder for constructing large strings</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">var</span><span style=\"color:#E1E4E8\"> builder </span><span style=\"color:#B392F0\">strings</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Builder</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">builder.</span><span style=\"color:#B392F0\">WriteString</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"log content\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">builder.</span><span style=\"color:#B392F0\">WriteString</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\" additional data\"</span><span style=\"color:#E1E4E8\">) </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">result </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> builder.</span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">()</span></span></code></pre></div>\n\n<p><strong>File I/O Optimization:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Use bufio.Writer for batched writes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">file, _ </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">OpenFile</span><span style=\"color:#E1E4E8\">(filename, os.O_APPEND</span><span style=\"color:#F97583\">|</span><span style=\"color:#E1E4E8\">os.O_CREATE</span><span style=\"color:#F97583\">|</span><span style=\"color:#E1E4E8\">os.O_WRONLY, </span><span style=\"color:#79B8FF\">0644</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">writer </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> bufio.</span><span style=\"color:#B392F0\">NewWriter</span><span style=\"color:#E1E4E8\">(file)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">writer.</span><span style=\"color:#B392F0\">WriteString</span><span style=\"color:#E1E4E8\">(logData)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">writer.</span><span style=\"color:#B392F0\">Flush</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#6A737D\">// Don't forget to flush!</span></span></code></pre></div>\n\n<p><strong>Memory Pool Usage:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Reuse byte slices to reduce GC pressure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">var</span><span style=\"color:#E1E4E8\"> bufferPool </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Pool</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    New: </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{} {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">buffer </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> bufferPool.</span><span style=\"color:#B392F0\">Get</span><span style=\"color:#E1E4E8\">().([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">defer</span><span style=\"color:#E1E4E8\"> bufferPool.</span><span style=\"color:#B392F0\">Put</span><span style=\"color:#E1E4E8\">(buffer)</span></span></code></pre></div>\n\n<p>These implementation guidelines provide a solid foundation for tackling each milestone while learning the core concepts. The key is to start simple with the infrastructure code provided, then gradually optimize performance as you understand the bottlenecks in your specific implementation.</p>\n<h2 id=\"goals-and-non-goals\">Goals and Non-Goals</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section provides foundational understanding that applies to all milestones (1-5), establishing clear boundaries and success criteria for the entire system.</p>\n</blockquote>\n<h3 id=\"mental-model-the-mission-statement\">Mental Model: The Mission Statement</h3>\n<p>Think of this section as the <strong>mission statement</strong> for our log aggregation system project. Just as a company&#39;s mission statement defines what it will accomplish, what principles guide its decisions, and what it explicitly won&#39;t pursue, our goals and non-goals section serves as the north star for every design decision we&#39;ll make throughout the five milestones.</p>\n<p>Imagine you&#39;re leading a team of engineers who will spend months building this system. Without clear goals, one engineer might optimize for maximum ingestion throughput while another focuses on minimal storage costs, leading to conflicting design decisions. The goals section acts like a <strong>constitutional document</strong> - when facing trade-offs between competing requirements, we can refer back to these clearly defined priorities to make consistent decisions.</p>\n<p>The non-goals are equally critical - they&#39;re the <strong>explicit boundaries</strong> that prevent scope creep and feature bloat. Like a ship&#39;s captain who must resist the temptation to chase every interesting island, we must resist adding features that sound useful but don&#39;t serve our core mission. This discipline is what separates successful systems from over-engineered ones that never ship.</p>\n<h3 id=\"functional-goals\">Functional Goals</h3>\n<p>The functional goals define the <strong>core capabilities</strong> our log aggregation system must deliver. These represent the fundamental value proposition - what users will actually do with our system and what workflows it must support.</p>\n<h4 id=\"primary-log-ingestion-capabilities\">Primary Log Ingestion Capabilities</h4>\n<p>Our system must accept log data from diverse sources through multiple protocols, handling the reality that modern infrastructure generates logs in various formats and delivery mechanisms. The <strong>primary ingestion goal</strong> is to provide universal log acceptance - any system that generates logs should be able to send them to our aggregation system without requiring complex client-side modifications.</p>\n<table>\n<thead>\n<tr>\n<th>Ingestion Method</th>\n<th>Protocol</th>\n<th>Format Support</th>\n<th>Throughput Target</th>\n<th>Buffering Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP REST API</td>\n<td>HTTP/HTTPS</td>\n<td>JSON, plain text</td>\n<td>10,000 msgs/sec</td>\n<td>In-memory with disk overflow</td>\n</tr>\n<tr>\n<td>Syslog Receiver</td>\n<td>TCP/UDP</td>\n<td>RFC 5424, RFC 3164</td>\n<td>15,000 msgs/sec</td>\n<td>Stream-based buffering</td>\n</tr>\n<tr>\n<td>File Tail Agent</td>\n<td>File I/O</td>\n<td>Any text format</td>\n<td>5,000 lines/sec</td>\n<td>Inode-based tracking</td>\n</tr>\n<tr>\n<td>Structured Logs</td>\n<td>HTTP POST</td>\n<td>JSON, logfmt</td>\n<td>8,000 msgs/sec</td>\n<td>Batch accumulation</td>\n</tr>\n</tbody></table>\n<p>The system must handle <strong>burst traffic</strong> gracefully - log volume often spikes during incidents when logs are most critical. Our buffering strategy ensures that temporary downstream slowdowns don&#39;t result in log loss. When the ingestion rate exceeds processing capacity, logs accumulate in memory buffers up to a configured limit, then overflow to disk-based buffers, and finally apply backpressure to prevent memory exhaustion.</p>\n<p><strong>Log parsing</strong> must extract structured fields from unstructured log messages using configurable patterns. Many legacy applications emit logs as free-form text, but effective querying requires extracting structured elements like timestamps, log levels, service names, and request IDs. Our parsing pipeline applies regex patterns, JSON extraction, and key-value pair recognition to transform unstructured logs into queryable structured data.</p>\n<blockquote>\n<p><strong>Key Insight</strong>: The ingestion layer is the <strong>trust boundary</strong> of our system. Once a log enters our ingestion pipeline, users expect it to be stored durably and made available for querying. This means ingestion must be more reliable than any upstream system - we cannot afford to lose logs due to parsing errors, buffer overflows, or downstream failures.</p>\n</blockquote>\n<h4 id=\"efficient-log-indexing-and-storage\">Efficient Log Indexing and Storage</h4>\n<p>The system must build <strong>inverted indexes</strong> that enable fast label-based queries across massive log volumes. Unlike traditional databases that index all fields, our indexing strategy focuses on labels - the key-value pairs that categorize and organize log streams. This approach, inspired by Prometheus and Loki, provides excellent query performance while keeping index sizes manageable.</p>\n<table>\n<thead>\n<tr>\n<th>Index Type</th>\n<th>Purpose</th>\n<th>Storage Format</th>\n<th>Update Frequency</th>\n<th>Compaction Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Label Index</td>\n<td>Maps label values to log streams</td>\n<td>Hash table + sorted lists</td>\n<td>Real-time during ingestion</td>\n<td>Hourly merge of small segments</td>\n</tr>\n<tr>\n<td>Term Index</td>\n<td>Full-text search within messages</td>\n<td>Inverted posting lists</td>\n<td>Batch every 1000 entries</td>\n<td>Daily compaction with bloom filters</td>\n</tr>\n<tr>\n<td>Time Index</td>\n<td>Temporal range queries</td>\n<td>Time-partitioned segments</td>\n<td>Per chunk write</td>\n<td>Weekly partition merging</td>\n</tr>\n<tr>\n<td>Bloom Filters</td>\n<td>Negative lookup optimization</td>\n<td>Bit arrays per chunk</td>\n<td>Once per chunk seal</td>\n<td>Read-only after creation</td>\n</tr>\n</tbody></table>\n<p><strong>Time-based partitioning</strong> organizes both indexes and data into time windows, typically hourly or daily segments. This temporal organization enables efficient time-range queries - when a user searches for logs from the last hour, the system only scans the relevant partition rather than the entire dataset. Partition boundaries align with common query patterns: recent logs for debugging, daily logs for analysis, and historical logs for compliance.</p>\n<p>The <strong>bloom filter</strong> implementation provides probabilistic negative lookups - if a bloom filter indicates a term is not present in a chunk, the system can skip that chunk entirely during query execution. Bloom filters have a configurable false positive rate (typically 1-5%) but never produce false negatives, making them perfect for eliminating chunks that definitely don&#39;t contain query terms.</p>\n<h4 id=\"powerful-query-language-and-execution\">Powerful Query Language and Execution</h4>\n<p>Our query engine implements a <strong>LogQL-style query language</strong> that combines the simplicity of grep with the power of structured field filtering. The query language must feel natural to developers who are accustomed to command-line log analysis tools while providing the precision needed for large-scale log analysis.</p>\n<table>\n<thead>\n<tr>\n<th>Query Type</th>\n<th>Syntax Example</th>\n<th>Index Usage</th>\n<th>Performance Characteristics</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Label Filter</td>\n<td><code>{service=&quot;api&quot;, level=&quot;error&quot;}</code></td>\n<td>Label index lookup</td>\n<td>O(1) stream identification</td>\n</tr>\n<tr>\n<td>Text Search</td>\n<td><code>|= &quot;database connection failed&quot;</code></td>\n<td>Term index scan</td>\n<td>O(log n) with bloom filters</td>\n</tr>\n<tr>\n<td>Regex Match</td>\n<td><code>|~ &quot;user_id=\\d+&quot;</code></td>\n<td>Full message scan</td>\n<td>O(n) within matching streams</td>\n</tr>\n<tr>\n<td>JSON Extraction</td>\n<td><code>| json | status_code &gt; 400</code></td>\n<td>Runtime field extraction</td>\n<td>O(m) where m = result size</td>\n</tr>\n<tr>\n<td>Aggregation</td>\n<td><code>count_over_time(5m)</code></td>\n<td>Time window processing</td>\n<td>O(k) where k = time buckets</td>\n</tr>\n</tbody></table>\n<p><strong>Query optimization</strong> applies several strategies to minimize the data scanned during query execution. The query planner pushes label filters down to the index layer, eliminating entire log streams before message-level processing begins. Time range filters are applied at the partition level, skipping historical data when users query recent logs. Regular expressions and text searches leverage bloom filters to eliminate chunks that cannot contain matching terms.</p>\n<p>The query engine supports <strong>streaming results</strong> for large queries that return thousands of log entries. Rather than buffering all results in memory, the system streams matching log entries back to the client as they&#39;re found. This approach provides faster time-to-first-result and prevents memory exhaustion on large result sets.</p>\n<h4 id=\"robust-storage-and-retention-management\">Robust Storage and Retention Management</h4>\n<p>The storage layer organizes logs into <strong>compressed chunks</strong> that balance query performance with storage efficiency. Each chunk contains logs from a specific time window and label combination, typically spanning 1-4 hours and compressed using algorithms optimized for text data. The chunk-based organization enables parallel query execution and efficient compression ratios.</p>\n<table>\n<thead>\n<tr>\n<th>Storage Component</th>\n<th>Purpose</th>\n<th>Configuration Options</th>\n<th>Default Settings</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Write-Ahead Log</td>\n<td>Durability guarantee</td>\n<td>Sync frequency, batch size</td>\n<td>fsync every 1000 entries</td>\n</tr>\n<tr>\n<td>Chunk Store</td>\n<td>Compressed log storage</td>\n<td>Compression algorithm, chunk size</td>\n<td>gzip compression, 1MB chunks</td>\n</tr>\n<tr>\n<td>Index Store</td>\n<td>Metadata and indexes</td>\n<td>Compaction frequency, bloom filter size</td>\n<td>Daily compaction, 1% false positive rate</td>\n</tr>\n<tr>\n<td>Retention Engine</td>\n<td>Automatic cleanup</td>\n<td>Time-based, size-based rules</td>\n<td>30 days retention</td>\n</tr>\n</tbody></table>\n<p><strong>Compression strategy</strong> significantly impacts both storage costs and query performance. We evaluate multiple compression algorithms during system design, measuring compression ratios and decompression speeds for typical log data. The choice affects the trade-off between storage efficiency (higher compression saves disk space) and query latency (faster decompression reduces query response time).</p>\n<p><strong>Retention policies</strong> automatically delete old log data based on configurable rules. Time-based retention removes logs older than a specified age (e.g., 30 days), while size-based retention maintains a maximum storage footprint by removing the oldest data when storage limits are exceeded. Per-stream retention rules allow different log sources to have different retention periods - security logs might be kept for compliance while debug logs are deleted after a few days.</p>\n<blockquote>\n<p><strong>Key Insight</strong>: Storage decisions made early in the design have long-term consequences. Once you choose a chunk format and compression scheme, changing them requires migrating existing data. The retention policy engine must be extremely reliable - accidentally deleting logs due to bugs in retention logic is often irreversible.</p>\n</blockquote>\n<h3 id=\"non-functional-goals\">Non-Functional Goals</h3>\n<p>Non-functional goals define the <strong>quality attributes</strong> our system must exhibit - the performance, reliability, and operational characteristics that determine whether users will trust and adopt our log aggregation system in production environments.</p>\n<h4 id=\"performance-and-scalability-requirements\">Performance and Scalability Requirements</h4>\n<p>The system must handle <strong>production-scale workloads</strong> without requiring excessive hardware resources. These performance targets reflect real-world usage patterns where log volume correlates with system activity - higher during business hours and incidents, lower during maintenance windows and off-peak periods.</p>\n<table>\n<thead>\n<tr>\n<th>Performance Metric</th>\n<th>Target</th>\n<th>Measurement Method</th>\n<th>Degradation Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Ingestion Throughput</td>\n<td>50,000 logs/second sustained</td>\n<td>Rate limiting with burst allowance</td>\n<td>Graceful backpressure, no data loss</td>\n</tr>\n<tr>\n<td>Query Response Time</td>\n<td>95th percentile &lt; 2 seconds</td>\n<td>Histogram metrics per query type</td>\n<td>Timeout after 30 seconds</td>\n</tr>\n<tr>\n<td>Index Update Latency</td>\n<td>New logs searchable within 30 seconds</td>\n<td>End-to-end ingestion-to-query test</td>\n<td>Batch processing during high load</td>\n</tr>\n<tr>\n<td>Storage Efficiency</td>\n<td>10:1 compression ratio minimum</td>\n<td>Compare raw vs compressed sizes</td>\n<td>Alert if compression ratio drops</td>\n</tr>\n<tr>\n<td>Memory Usage</td>\n<td>&lt; 2GB resident during normal operation</td>\n<td>Process memory monitoring</td>\n<td>Garbage collection tuning</td>\n</tr>\n</tbody></table>\n<p><strong>Horizontal scalability</strong> allows the system to handle growing log volumes by adding more servers rather than requiring larger individual machines. While our intermediate-level system runs on a single server, the architecture must not preclude future horizontal scaling. This means avoiding architectural decisions that would require fundamental redesigns when scaling beyond single-server capacity.</p>\n<p><strong>Burst handling</strong> recognizes that log traffic patterns are highly variable. Application deployments, incidents, and batch jobs create temporary spikes that can exceed normal capacity by 5-10x. Our buffering strategy absorbs these bursts without dropping logs, and our indexing pipeline batches updates during high-load periods to maintain system stability.</p>\n<h4 id=\"reliability-and-durability-guarantees\">Reliability and Durability Guarantees</h4>\n<p><strong>Data durability</strong> is paramount - losing logs during critical incidents undermines the entire value proposition of log aggregation. Our durability guarantees ensure that once a log is acknowledged by the ingestion endpoint, it will be available for querying even if the system experiences crashes, disk failures, or other infrastructure problems.</p>\n<table>\n<thead>\n<tr>\n<th>Reliability Component</th>\n<th>Guarantee Level</th>\n<th>Implementation Strategy</th>\n<th>Recovery Time Objective</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Write-Ahead Logging</td>\n<td>Zero data loss after ingestion</td>\n<td>fsync before ACK response</td>\n<td>Automatic on restart</td>\n</tr>\n<tr>\n<td>Index Corruption Recovery</td>\n<td>Rebuild from stored logs</td>\n<td>Checksum validation on read</td>\n<td>&lt; 1 hour for 1TB dataset</td>\n</tr>\n<tr>\n<td>Disk Failure Handling</td>\n<td>Graceful degradation</td>\n<td>Health checks with alerting</td>\n<td>Manual intervention required</td>\n</tr>\n<tr>\n<td>Memory Exhaustion</td>\n<td>No data loss, reduced performance</td>\n<td>Disk overflow buffers</td>\n<td>Automatic recovery when memory available</td>\n</tr>\n</tbody></table>\n<p><strong>Consistency guarantees</strong> define what users can expect when logs are ingested and immediately queried. Our system provides <strong>eventual consistency</strong> - logs are guaranteed to appear in query results within 30 seconds of ingestion under normal conditions. During high load periods, this window may extend to several minutes, but logs are never lost.</p>\n<p><strong>Crash recovery</strong> must restore the system to a consistent state without human intervention. The write-ahead log captures all ingestion operations before they&#39;re acknowledged, allowing the system to replay any operations that were in progress during a crash. Index recovery rebuilds any corrupted index structures from the durable log data, ensuring queries return accurate results after recovery completes.</p>\n<h4 id=\"operational-and-monitoring-requirements\">Operational and Monitoring Requirements</h4>\n<p>The system must provide <strong>comprehensive observability</strong> to support production operations. Operations teams need visibility into ingestion rates, query performance, storage usage, and error conditions to maintain system health and plan capacity upgrades.</p>\n<table>\n<thead>\n<tr>\n<th>Monitoring Category</th>\n<th>Key Metrics</th>\n<th>Alert Conditions</th>\n<th>Dashboard Views</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Ingestion Health</td>\n<td>Logs/second, error rate, buffer depth</td>\n<td>Error rate &gt; 1%, buffer &gt; 80% full</td>\n<td>Real-time ingestion dashboard</td>\n</tr>\n<tr>\n<td>Query Performance</td>\n<td>Request rate, latency distribution, timeout rate</td>\n<td>95th percentile &gt; 5 seconds</td>\n<td>Query performance trends</td>\n</tr>\n<tr>\n<td>Storage Utilization</td>\n<td>Disk usage, compression ratio, retention execution</td>\n<td>Disk &gt; 85% full</td>\n<td>Capacity planning dashboard</td>\n</tr>\n<tr>\n<td>System Resources</td>\n<td>CPU, memory, file descriptors, network</td>\n<td>Memory &gt; 90%, file descriptor exhaustion</td>\n<td>Infrastructure health view</td>\n</tr>\n</tbody></table>\n<p><strong>Graceful degradation</strong> maintains core functionality even when the system operates under stress or partial failure conditions. When memory buffers fill up, the system switches to disk-based buffering with reduced performance but no data loss. When query load exceeds capacity, the system applies rate limiting to protect core ingestion functionality. When storage approaches capacity limits, the system accelerates retention cleanup to maintain operational headroom.</p>\n<p><strong>Configuration management</strong> allows operators to tune system behavior for their specific environments and usage patterns. Critical configuration parameters include buffer sizes, retention policies, compression settings, and performance thresholds. Configuration changes should take effect without requiring system restarts where possible, and all configuration changes should be logged for audit and troubleshooting purposes.</p>\n<blockquote>\n<p><strong>Key Insight</strong>: Non-functional requirements often conflict with each other - higher compression ratios increase CPU usage, faster ingestion requires more memory, longer retention needs more storage. The system design must find the right balance points and make trade-offs explicit through configuration options.</p>\n</blockquote>\n<h3 id=\"explicit-non-goals\">Explicit Non-Goals</h3>\n<p>Non-goals define the <strong>boundaries</strong> of our project scope - features and capabilities that we explicitly choose not to implement. These boundaries prevent scope creep and help maintain focus on the core log aggregation functionality. Understanding what we won&#39;t build is as important as understanding what we will build.</p>\n<h4 id=\"real-time-analytics-and-complex-processing\">Real-Time Analytics and Complex Processing</h4>\n<p>Our system <strong>does not</strong> provide real-time analytics, machine learning capabilities, or complex event processing. While some log aggregation platforms include these features, they represent significant additional complexity that would distract from our core goal of building a solid ingestion, indexing, and querying foundation.</p>\n<table>\n<thead>\n<tr>\n<th>Excluded Feature</th>\n<th>Rationale</th>\n<th>Alternative Approach</th>\n<th>Future Consideration</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Stream Processing</td>\n<td>Adds significant complexity to query engine</td>\n<td>Use dedicated stream processing tools</td>\n<td>Could be added in future milestones</td>\n</tr>\n<tr>\n<td>Machine Learning</td>\n<td>Requires specialized algorithms and training data</td>\n<td>Export logs to ML platforms</td>\n<td>Pattern detection might be valuable later</td>\n</tr>\n<tr>\n<td>Real-time Dashboards</td>\n<td>Requires WebSocket connections and live updates</td>\n<td>Use polling-based dashboard tools</td>\n<td>Simple metrics endpoint is sufficient</td>\n</tr>\n<tr>\n<td>Complex Aggregations</td>\n<td>Window functions, joins, statistical operations</td>\n<td>Export to analytical databases</td>\n<td>Basic counting operations are adequate</td>\n</tr>\n</tbody></table>\n<p><strong>Log transformation</strong> beyond basic field extraction is not supported. While some systems provide rich transformation pipelines that can parse, enrich, and modify logs during ingestion, these features add complexity to the ingestion path and can become performance bottlenecks. Our system focuses on accepting logs as-is and making them efficiently searchable.</p>\n<p><strong>Alerting and notification</strong> systems are excluded from the core implementation. While log-based alerting is valuable, it requires additional infrastructure for notification delivery, escalation policies, and alert management. Users can implement alerting by periodically querying our system and processing the results externally.</p>\n<h4 id=\"advanced-distributed-system-features\">Advanced Distributed System Features</h4>\n<p>Our intermediate-level system <strong>does not</strong> implement advanced distributed system features like automatic failover, data replication, or cluster management. These features require significant additional complexity in areas like consensus protocols, network partition handling, and distributed state management.</p>\n<table>\n<thead>\n<tr>\n<th>Distributed Feature</th>\n<th>Complexity Reasons</th>\n<th>Single-Server Alternative</th>\n<th>Migration Path</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Automatic Failover</td>\n<td>Requires leader election and state synchronization</td>\n<td>Manual backup/restore procedures</td>\n<td>Design allows future clustering</td>\n</tr>\n<tr>\n<td>Data Replication</td>\n<td>Complex consistency guarantees and conflict resolution</td>\n<td>Backup to external storage systems</td>\n<td>Chunk format supports replication</td>\n</tr>\n<tr>\n<td>Load Balancing</td>\n<td>Client-side discovery and connection management</td>\n<td>Single endpoint with high availability</td>\n<td>HTTP load balancer compatible</td>\n</tr>\n<tr>\n<td>Sharding</td>\n<td>Automatic data placement and query federation</td>\n<td>Vertical scaling and retention management</td>\n<td>Architecture supports future sharding</td>\n</tr>\n</tbody></table>\n<p><strong>Cross-datacenter replication</strong> is not supported due to the complexity of handling network partitions, latency variations, and consistency guarantees across geographic regions. Organizations requiring multi-region log aggregation can deploy independent instances and use external tools for data synchronization.</p>\n<p><strong>Automatic scaling</strong> based on load metrics is not implemented. While cloud-native applications often include auto-scaling capabilities, they require integration with orchestration platforms and complex resource management logic. Our system provides the metrics needed for external auto-scaling systems to make scaling decisions.</p>\n<h4 id=\"enterprise-integration-and-security-features\">Enterprise Integration and Security Features</h4>\n<p>Advanced <strong>authentication and authorization</strong> systems are not included. While production log aggregation systems require robust security, implementing features like LDAP integration, role-based access control, and audit logging would significantly expand the project scope beyond the core technical challenges.</p>\n<table>\n<thead>\n<tr>\n<th>Security Feature</th>\n<th>Implementation Complexity</th>\n<th>Basic Alternative</th>\n<th>Production Requirements</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Multi-factor Authentication</td>\n<td>Integration with identity providers</td>\n<td>API key authentication</td>\n<td>External authentication proxy</td>\n</tr>\n<tr>\n<td>Fine-grained Authorization</td>\n<td>Role-based access control with permissions</td>\n<td>Tenant-based isolation</td>\n<td>Authorization service integration</td>\n</tr>\n<tr>\n<td>Audit Logging</td>\n<td>Separate audit trail with compliance features</td>\n<td>Basic operation logging</td>\n<td>Dedicated audit system</td>\n</tr>\n<tr>\n<td>Encryption at Rest</td>\n<td>Key management and performance impact</td>\n<td>File system encryption</td>\n<td>Hardware security modules</td>\n</tr>\n</tbody></table>\n<p><strong>Compliance features</strong> like data residency controls, legal hold capabilities, and privacy controls are not implemented. These features require deep integration with organizational policies and legal requirements that vary significantly across different use cases and jurisdictions.</p>\n<p><strong>Enterprise integration</strong> features like single sign-on, corporate directory integration, and configuration management system integration are excluded. These integrations are highly specific to organizational infrastructure and would require extensive configuration options and compatibility testing.</p>\n<h4 id=\"performance-optimization-beyond-core-requirements\">Performance Optimization Beyond Core Requirements</h4>\n<p>We <strong>do not</strong> optimize for extreme performance scenarios that would require specialized hardware or complex performance tuning. While our system meets production performance requirements, it does not target use cases requiring specialized optimizations.</p>\n<table>\n<thead>\n<tr>\n<th>Performance Area</th>\n<th>Not Optimized For</th>\n<th>Reason</th>\n<th>Alternative Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Ultra-low Latency</td>\n<td>Sub-millisecond query responses</td>\n<td>Requires in-memory indexes and complex caching</td>\n<td>Dedicated time-series databases</td>\n</tr>\n<tr>\n<td>Extreme Throughput</td>\n<td>&gt;1M logs/second single node</td>\n<td>Requires specialized networking and storage</td>\n<td>Distributed ingestion systems</td>\n</tr>\n<tr>\n<td>Minimal Resource Usage</td>\n<td>&lt;100MB memory footprint</td>\n<td>Conflicts with performance and functionality goals</td>\n<td>Embedded log libraries</td>\n</tr>\n<tr>\n<td>Custom Hardware</td>\n<td>GPU acceleration, FPGA compression</td>\n<td>Adds deployment complexity</td>\n<td>Specialized analytical systems</td>\n</tr>\n</tbody></table>\n<p><strong>Memory optimization</strong> beyond reasonable limits is not pursued. While our system operates within reasonable memory bounds (target: 2GB), we do not optimize for extremely memory-constrained environments where every megabyte matters. Such optimization would require complex data structure choices that sacrifice maintainability.</p>\n<p><strong>Storage optimization</strong> beyond standard compression techniques is not implemented. Advanced techniques like dictionary compression, column storage, or specialized text compression algorithms would improve storage efficiency but add significant complexity to the storage layer.</p>\n<blockquote>\n<p><strong>Key Design Principle</strong>: By clearly defining non-goals, we create space to excel at our core functionality. Every feature we exclude allows us to make the included features more robust, better tested, and easier to understand. The best systems do fewer things exceptionally well rather than many things adequately.</p>\n</blockquote>\n<h3 id=\"architecture-decision-records\">Architecture Decision Records</h3>\n<p>The goals and non-goals drive several foundational architecture decisions that influence the entire system design. These decisions establish the technical foundation that supports our functional goals while respecting the boundaries established by our non-goals.</p>\n<blockquote>\n<p><strong>Decision: Single-Server Architecture for Intermediate Implementation</strong></p>\n<ul>\n<li><strong>Context</strong>: Must balance system complexity with educational value for intermediate developers while supporting future scalability.</li>\n<li><strong>Options Considered</strong>: Microservices architecture, distributed system from start, single-server with scalable design</li>\n<li><strong>Decision</strong>: Single-server implementation with architecture that supports future distribution</li>\n<li><strong>Rationale</strong>: Reduces operational complexity, simplifies debugging, and allows focus on core log aggregation challenges while maintaining expansion paths</li>\n<li><strong>Consequences</strong>: Enables rapid development and testing but requires eventual redesign for horizontal scaling. Trade-off is appropriate for learning objectives.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Architecture Option</th>\n<th>Complexity Level</th>\n<th>Learning Value</th>\n<th>Production Readiness</th>\n<th>Chosen</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Microservices</td>\n<td>High</td>\n<td>High</td>\n<td>High</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>Distributed System</td>\n<td>Very High</td>\n<td>Very High</td>\n<td>Very High</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>Single Server</td>\n<td>Medium</td>\n<td>High</td>\n<td>Medium</td>\n<td>✅</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: LogQL-Style Query Language</strong></p>\n<ul>\n<li><strong>Context</strong>: Need query interface that balances ease of use with powerful filtering capabilities</li>\n<li><strong>Options Considered</strong>: SQL-like syntax, GraphQL-based queries, LogQL-inspired syntax</li>\n<li><strong>Decision</strong>: LogQL-inspired query language with label filters and pipeline operations</li>\n<li><strong>Rationale</strong>: Familiar to developers using Grafana/Loki, optimizes for common log analysis patterns, and maps well to our label-based indexing strategy</li>\n<li><strong>Consequences</strong>: Requires custom parser implementation but provides excellent user experience for log-specific operations</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Query Language Option</th>\n<th>Learning Curve</th>\n<th>Implementation Complexity</th>\n<th>Query Power</th>\n<th>Chosen</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>SQL-like</td>\n<td>Low</td>\n<td>Very High</td>\n<td>Very High</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>GraphQL</td>\n<td>Medium</td>\n<td>High</td>\n<td>Medium</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>LogQL-inspired</td>\n<td>Medium</td>\n<td>Medium</td>\n<td>High</td>\n<td>✅</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Write-Ahead Log for Durability</strong></p>\n<ul>\n<li><strong>Context</strong>: Must guarantee no data loss after ingestion acknowledgment while maintaining reasonable performance</li>\n<li><strong>Options Considered</strong>: Synchronous disk writes, asynchronous batching, write-ahead log with batching</li>\n<li><strong>Decision</strong>: Write-ahead log with configurable sync frequency</li>\n<li><strong>Rationale</strong>: Provides tunable durability guarantees, enables crash recovery, and allows performance optimization through batching</li>\n<li><strong>Consequences</strong>: Adds complexity to ingestion path but provides essential durability guarantees required for production usage</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Durability Strategy</th>\n<th>Performance Impact</th>\n<th>Complexity</th>\n<th>Data Safety</th>\n<th>Chosen</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Synchronous Writes</td>\n<td>High</td>\n<td>Low</td>\n<td>Highest</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>Async Batching</td>\n<td>Low</td>\n<td>Low</td>\n<td>Lowest</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>Write-Ahead Log</td>\n<td>Medium</td>\n<td>Medium</td>\n<td>High</td>\n<td>✅</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section bridges the abstract goals and constraints with concrete technology choices and project organization that support building a production-ready log aggregation system.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<p>The following technology stack balances simplicity for learning with production-readiness for real-world deployment:</p>\n<table>\n<thead>\n<tr>\n<th>Component Category</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Recommended Choice</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Server</td>\n<td><code>net/http</code> with JSON</td>\n<td>gRPC with Protocol Buffers</td>\n<td><code>net/http</code> - simpler debugging</td>\n</tr>\n<tr>\n<td>Serialization</td>\n<td>JSON for all formats</td>\n<td>Protocol Buffers + JSON hybrid</td>\n<td>JSON - universal compatibility</td>\n</tr>\n<tr>\n<td>Storage Backend</td>\n<td>Local filesystem with atomic writes</td>\n<td>S3-compatible object storage</td>\n<td>Filesystem - reduces dependencies</td>\n</tr>\n<tr>\n<td>Compression</td>\n<td>gzip standard library</td>\n<td>LZ4 or Zstandard</td>\n<td>gzip - good balance of ratio/speed</td>\n</tr>\n<tr>\n<td>Indexing</td>\n<td>Hash maps with sorted arrays</td>\n<td>B-trees with buffer pools</td>\n<td>Hash maps - simpler implementation</td>\n</tr>\n<tr>\n<td>Logging</td>\n<td>Standard <code>log</code> package</td>\n<td>Structured logging (logrus/zap)</td>\n<td>Standard <code>log</code> - meta-irony avoided</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-project-structure\">Recommended Project Structure</h4>\n<p>Organize the codebase to reflect the major functional components while maintaining clear separation of concerns:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>log-aggregator/\n├── cmd/\n│   ├── server/main.go              ← Main server entry point\n│   └── tools/                      ← Utilities (index rebuild, etc.)\n├── internal/\n│   ├── ingestion/                  ← Milestone 1: Log Ingestion\n│   │   ├── http_receiver.go\n│   │   ├── syslog_receiver.go\n│   │   ├── file_tailer.go\n│   │   └── buffer_manager.go\n│   ├── parser/                     ← Log parsing and field extraction\n│   │   ├── json_parser.go\n│   │   ├── syslog_parser.go\n│   │   └── regex_parser.go\n│   ├── index/                      ← Milestone 2: Log Index\n│   │   ├── inverted_index.go\n│   │   ├── bloom_filter.go\n│   │   ├── partition_manager.go\n│   │   └── compaction.go\n│   ├── storage/                    ← Milestone 4: Storage &amp; Compression\n│   │   ├── chunk_store.go\n│   │   ├── wal.go\n│   │   ├── compression.go\n│   │   └── retention.go\n│   ├── query/                      ← Milestone 3: Query Engine\n│   │   ├── parser.go\n│   │   ├── planner.go\n│   │   ├── executor.go\n│   │   └── logql/                  ← LogQL language implementation\n│   ├── tenant/                     ← Milestone 5: Multi-Tenancy\n│   │   ├── isolation.go\n│   │   ├── rate_limiter.go\n│   │   └── alerting.go\n│   └── common/                     ← Shared types and utilities\n│       ├── types.go                ← LogEntry, Labels, etc.\n│       ├── config.go\n│       └── metrics.go\n├── pkg/                           ← Public APIs (if needed)\n├── test/                          ← Integration tests\n├── configs/                       ← Configuration examples\n└── docs/                         ← Documentation</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Configuration Management</strong> (complete implementation):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/common/config.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> common</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strconv</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Config</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    HTTPPort     </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TCPPort      </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    UDPPort      </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StoragePath  </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BufferSize   </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ChunkSize    </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RetentionDays </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    WALSyncFreq  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadConfig loads configuration from environment with defaults</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> LoadConfig</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        HTTPPort:      </span><span style=\"color:#B392F0\">getEnvInt</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"HTTP_PORT\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">8080</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TCPPort:       </span><span style=\"color:#B392F0\">getEnvInt</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"TCP_PORT\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1514</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        UDPPort:       </span><span style=\"color:#B392F0\">getEnvInt</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"UDP_PORT\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1514</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        StoragePath:   </span><span style=\"color:#B392F0\">getEnv</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"STORAGE_PATH\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"./data\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        BufferSize:    </span><span style=\"color:#B392F0\">getEnvInt</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"BUFFER_SIZE\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">10000</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ChunkSize:     </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">getEnvInt</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"CHUNK_SIZE\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#E1E4E8\">)), </span><span style=\"color:#6A737D\">// 1MB</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        RetentionDays: </span><span style=\"color:#B392F0\">getEnvInt</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"RETENTION_DAYS\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        WALSyncFreq:   time.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">getEnvInt</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"WAL_SYNC_FREQ_MS\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">)) </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> time.Millisecond,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> getEnv</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">defaultValue</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> value </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">Getenv</span><span style=\"color:#E1E4E8\">(key); value </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> defaultValue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> getEnvInt</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">defaultValue</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> value </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">Getenv</span><span style=\"color:#E1E4E8\">(key); value </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> intValue, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> strconv.</span><span style=\"color:#B392F0\">Atoi</span><span style=\"color:#E1E4E8\">(value); err </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> intValue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> defaultValue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Core Data Types</strong> (complete implementation):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/common/types.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> common</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync/atomic</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Labels represents key-value pairs for log categorization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Labels</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LogEntry represents a single log record with metadata</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LogEntry</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Labels    </span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#9ECBFF\">    `json:\"labels\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Message   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"message\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LogStream represents a sequence of logs with the same label set</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LogStream</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Labels  </span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#9ECBFF\">     `json:\"labels\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Entries []</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#9ECBFF\"> `json:\"entries\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TimeRange represents a time window for queries</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TimeRange</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Start </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"start\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    End   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"end\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Metrics provides thread-safe performance counters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Metrics</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logsIngested </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queriesExecuted </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bytesStored </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// IncrementLogsIngested safely increments the ingestion counter</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Metrics</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">IncrementLogsIngested</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    atomic.</span><span style=\"color:#B392F0\">AddInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">m.logsIngested, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetStats returns current log and query counts safely</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Metrics</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetStats</span><span style=\"color:#E1E4E8\">() (</span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logs </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> atomic.</span><span style=\"color:#B392F0\">LoadInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">m.logsIngested)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queries </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> atomic.</span><span style=\"color:#B392F0\">LoadInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">m.queriesExecuted)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> logs, queries</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Global metrics instance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">var</span><span style=\"color:#E1E4E8\"> GlobalMetrics </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Metrics</span><span style=\"color:#E1E4E8\">{}</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeletons\">Core Logic Skeletons</h4>\n<p><strong>Log Ingestion Buffer Manager</strong> (signature + TODOs):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/ingestion/buffer_manager.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> ingestion</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/yourorg/log-aggregator/internal/common</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> BufferManager</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    memoryBuffer </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">common</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">LogEntry</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    diskBuffer   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DiskBuffer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config       </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">common</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Config</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewBufferManager creates a new buffer manager with configured capacity</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewBufferManager</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">common</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">BufferManager</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create memory buffer channel with BUFFER_SIZE capacity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Initialize disk buffer for overflow handling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Start background goroutine for buffer flushing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use buffered channel for memory buffer to provide backpressure</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AddLogEntry adds a log entry to the buffer with overflow handling</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">bm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">BufferManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">AddLogEntry</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">entry</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">common</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Try to add entry to memory buffer (non-blocking)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: If memory buffer is full, write to disk buffer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Update metrics counter for ingested logs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return error if both buffers are full (apply backpressure)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use select with default case for non-blocking channel send</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Index Builder</strong> (signature + TODOs):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/index/inverted_index.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> index</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/yourorg/log-aggregator/internal/common</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> InvertedIndex</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    termToEntries </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">uint64</span><span style=\"color:#6A737D\">  // term -> log entry IDs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    labelToStreams </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">uint32</span><span style=\"color:#6A737D\"> // label value -> stream IDs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bloomFilters  </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">BloomFilter</span><span style=\"color:#6A737D\"> // per-partition bloom filters</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AddLogEntry adds a log entry to the inverted index</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">idx </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">InvertedIndex</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">AddLogEntry</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">entryID</span><span style=\"color:#F97583\"> uint64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">entry</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">common</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Extract terms from log message (split on whitespace, normalize case)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Add each term to the inverted index pointing to this entryID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Index each label key-value pair for label-based queries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update the appropriate bloom filter with extracted terms</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: If index segment exceeds size threshold, trigger compaction</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use append() to add entryID to existing term lists</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// QueryTerms finds log entries containing all specified terms</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">idx </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">InvertedIndex</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">QueryTerms</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">terms</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">uint64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: For each term, check bloom filter first (negative lookup optimization)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Get posting list for each term from inverted index</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Compute intersection of all posting lists (entries containing ALL terms)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Sort result by entry ID for efficient storage access</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Empty result for any term means empty intersection</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"language-specific-implementation-hints\">Language-Specific Implementation Hints</h4>\n<p><strong>Go-Specific Best Practices:</strong></p>\n<ul>\n<li>Use <code>sync.RWMutex</code> for read-heavy data structures like indexes - multiple readers can access simultaneously</li>\n<li>Implement <code>io.Closer</code> interface on components that hold resources (files, network connections)</li>\n<li>Use <code>context.Context</code> for cancellation in long-running operations like query execution</li>\n<li>Leverage <code>encoding/json</code> for log parsing but consider streaming decoder for large payloads</li>\n<li>Use <code>os.File.Sync()</code> for WAL durability - it&#39;s equivalent to fsync system call</li>\n</ul>\n<p><strong>Error Handling Patterns:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Wrap errors with context for debugging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> idx.</span><span style=\"color:#B392F0\">AddLogEntry</span><span style=\"color:#E1E4E8\">(entryID, entry); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to index entry </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, entryID, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Use sentinel errors for expected failure modes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">var</span><span style=\"color:#E1E4E8\"> ErrBufferFull </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"buffer capacity exceeded\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Check for specific error types</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">Is</span><span style=\"color:#E1E4E8\">(err, ErrBufferFull) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Apply backpressure</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Concurrency Patterns:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Worker pool for parallel log processing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">workers </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> runtime.</span><span style=\"color:#B392F0\">NumCPU</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">logChan </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">; i </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> workers; i</span><span style=\"color:#F97583\">++</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    go</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> entry </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> logChan {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">            processLogEntry</span><span style=\"color:#E1E4E8\">(entry)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint-verification\">Milestone Checkpoint Verification</h4>\n<p>After implementing the goals and architectural foundation:</p>\n<p><strong>What Should Work:</strong></p>\n<ol>\n<li><code>go run cmd/server/main.go</code> starts the server without errors</li>\n<li>Configuration loads from environment variables with sensible defaults</li>\n<li>Basic HTTP endpoint responds to health checks</li>\n<li>Memory usage remains stable under no-load conditions</li>\n</ol>\n<p><strong>How to Verify:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Start the server</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/server/main.go</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Check health endpoint</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> http://localhost:8080/health</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Monitor memory usage</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">ps</span><span style=\"color:#9ECBFF\"> aux</span><span style=\"color:#F97583\"> |</span><span style=\"color:#B392F0\"> grep</span><span style=\"color:#9ECBFF\"> log-aggregator</span></span></code></pre></div>\n\n<p><strong>Expected Output:</strong></p>\n<ul>\n<li>Server starts and binds to configured ports</li>\n<li>Health endpoint returns JSON status</li>\n<li>Memory usage stabilizes around 50-100MB baseline</li>\n<li>No goroutine leaks (use <code>go tool pprof</code> to verify)</li>\n</ul>\n<p><strong>Common Issues and Fixes:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnostic Steps</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Server won&#39;t start</td>\n<td>Port already in use</td>\n<td><code>netstat -tulpn | grep :8080</code></td>\n<td>Change HTTP_PORT environment variable</td>\n</tr>\n<tr>\n<td>High memory usage</td>\n<td>Buffer not being flushed</td>\n<td>Check background goroutine status</td>\n<td>Implement proper buffer flushing logic</td>\n</tr>\n<tr>\n<td>Configuration ignored</td>\n<td>Environment variables not set</td>\n<td>Print config values at startup</td>\n<td>Export variables or use .env file</td>\n</tr>\n<tr>\n<td>Import errors</td>\n<td>Incorrect module path</td>\n<td>Check go.mod file</td>\n<td>Update import paths to match module name</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: Goroutine Leaks in Server Components</strong>\nMany beginners start background goroutines for buffer flushing or maintenance tasks but forget to implement proper shutdown. This causes goroutine leaks during testing when servers start and stop repeatedly. Always implement context-based cancellation and wait for goroutines to finish in shutdown handlers.</p>\n<p>⚠️ <strong>Pitfall: Configuration Validation Missing</strong>\nLoading configuration from environment variables without validation can cause runtime panics later. For example, setting BUFFER_SIZE to zero or negative values will cause channel creation to panic. Always validate configuration values and provide clear error messages for invalid settings.</p>\n<h2 id=\"high-level-architecture\">High-Level Architecture</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section provides foundational understanding that applies to all milestones (1-5), establishing the overall system design and component relationships that guide implementation throughout the project.</p>\n</blockquote>\n<p>The log aggregation system follows a <strong>pipeline architecture</strong> where log data flows through distinct processing stages, each optimized for a specific concern. Think of it like a modern manufacturing assembly line - raw materials (log messages) enter at one end and move through specialized stations (components) that each add value before the finished product (indexed, queryable logs) reaches the warehouse (storage).</p>\n<p><img src=\"/api/project/log-aggregator/architecture-doc/asset?path=diagrams%2Fsystem-architecture.svg\" alt=\"System Architecture Overview\"></p>\n<p>This architectural approach provides clear separation of concerns, allowing each component to be optimized independently while maintaining predictable data flow. The pipeline design also enables natural scaling points - if ingestion becomes a bottleneck, we can add more ingestion workers without affecting the indexing or query components.</p>\n<blockquote>\n<p><strong>Key Design Principle</strong>: Each component owns its data and exposes well-defined interfaces to other components. This ownership model prevents tight coupling and allows components to evolve independently as requirements change.</p>\n</blockquote>\n<p>The system implements a <strong>write-heavy, read-optimized</strong> design pattern common in observability systems. Log ingestion happens continuously at high volume, but queries are relatively infrequent and typically focus on recent data. This asymmetry drives many architectural decisions throughout the system.</p>\n<h3 id=\"component-overview\">Component Overview</h3>\n<p>The log aggregation system consists of five core components that work together to transform raw log streams into queryable, indexed data. Each component has distinct responsibilities and interfaces with its neighbors through well-defined APIs and data structures.</p>\n<h4 id=\"ingestion-engine\">Ingestion Engine</h4>\n<p>The <strong>Ingestion Engine</strong> serves as the system&#39;s front door, accepting log data from multiple sources and protocols. Think of it as a busy restaurant&#39;s host station - it must handle many concurrent arrivals, validate reservations (log formats), and route guests (log entries) to appropriate tables (processing queues) without creating bottlenecks or losing anyone in the chaos.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Ingestion Engine</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Primary Responsibility</strong></td>\n<td>Accept and buffer incoming log data from multiple protocols</td>\n</tr>\n<tr>\n<td><strong>Input Sources</strong></td>\n<td>HTTP POST requests, TCP/UDP syslog streams, file tail agents</td>\n</tr>\n<tr>\n<td><strong>Output</strong></td>\n<td>Structured <code>LogEntry</code> objects in processing buffers</td>\n</tr>\n<tr>\n<td><strong>Key Interfaces</strong></td>\n<td><code>HTTPHandler</code>, <code>SyslogReceiver</code>, <code>FileTailer</code></td>\n</tr>\n<tr>\n<td><strong>Failure Mode</strong></td>\n<td>Log loss during overload or downstream outages</td>\n</tr>\n<tr>\n<td><strong>Scaling Bottleneck</strong></td>\n<td>Network I/O and parsing CPU cycles</td>\n</tr>\n</tbody></table>\n<p>The Ingestion Engine implements <strong>backpressure management</strong> to prevent memory exhaustion when downstream components cannot keep up. When buffers approach capacity, it applies increasingly aggressive rate limiting, ultimately rejecting new connections to preserve system stability. This graceful degradation ensures the system remains responsive to queries even during ingestion overload.</p>\n<blockquote>\n<p><strong>Architecture Insight</strong>: The Ingestion Engine prioritizes <strong>availability over consistency</strong> - it&#39;s better to drop some logs during extreme overload than to crash and lose all logs. This trade-off aligns with observability requirements where partial data is better than no data.</p>\n</blockquote>\n<h4 id=\"parser-engine\">Parser Engine</h4>\n<p>The <strong>Parser Engine</strong> transforms unstructured log text into structured data that can be efficiently indexed and queried. Imagine a skilled translator at the United Nations who can understand multiple languages (log formats) and convert them all into a common structured language that everyone can understand and work with.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Parser Engine</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Primary Responsibility</strong></td>\n<td>Extract structured fields and labels from raw log messages</td>\n</tr>\n<tr>\n<td><strong>Input</strong></td>\n<td>Raw log strings from various formats (JSON, syslog, custom patterns)</td>\n</tr>\n<tr>\n<td><strong>Output</strong></td>\n<td>Enriched <code>LogEntry</code> objects with extracted <code>Labels</code></td>\n</tr>\n<tr>\n<td><strong>Key Interfaces</strong></td>\n<td><code>JSONParser</code>, <code>SyslogParser</code>, <code>RegexParser</code></td>\n</tr>\n<tr>\n<td><strong>Failure Mode</strong></td>\n<td>Parse errors leading to lost log content or malformed entries</td>\n</tr>\n<tr>\n<td><strong>Scaling Bottleneck</strong></td>\n<td>Regular expression complexity and label extraction CPU usage</td>\n</tr>\n</tbody></table>\n<p>The Parser Engine employs a <strong>plugin architecture</strong> where each log format has its own parser implementation sharing a common interface. This design allows adding new log formats without modifying the core parsing logic. Each parser extracts timestamp, message content, and structured labels that will drive query performance.</p>\n<p><strong>Label extraction</strong> is the most critical parser function because labels become the primary query interface. The parser applies configurable extraction rules to pull structured data from log messages - service names, log levels, request IDs, and other metadata that users will want to filter on. High-quality label extraction dramatically improves query performance by enabling efficient index lookups.</p>\n<h4 id=\"index-engine\">Index Engine</h4>\n<p>The <strong>Index Engine</strong> builds and maintains data structures that enable fast log searches across large datasets. Think of it as a librarian who creates multiple card catalogs - one sorted by author, another by subject, another by publication date - so researchers can quickly find relevant books without scanning every shelf.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Index Engine</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Primary Responsibility</strong></td>\n<td>Build inverted indexes and bloom filters for fast log lookups</td>\n</tr>\n<tr>\n<td><strong>Input</strong></td>\n<td>Structured <code>LogEntry</code> objects with extracted labels</td>\n</tr>\n<tr>\n<td><strong>Output</strong></td>\n<td>Inverted index mappings and bloom filter bit arrays</td>\n</tr>\n<tr>\n<td><strong>Key Interfaces</strong></td>\n<td><code>InvertedIndex</code>, <code>BloomFilter</code>, <code>TimePartition</code></td>\n</tr>\n<tr>\n<td><strong>Failure Mode</strong></td>\n<td>Index corruption leading to missing query results</td>\n</tr>\n<tr>\n<td><strong>Scaling Bottleneck</strong></td>\n<td>Label cardinality explosion and index memory usage</td>\n</tr>\n</tbody></table>\n<p>The Index Engine implements <strong>time-based partitioning</strong> where indexes are segmented by time windows (typically hourly or daily). This partitioning strategy allows queries with time ranges to scan only relevant partitions rather than the entire dataset. Each partition maintains its own inverted index and bloom filter, keeping memory usage bounded and enabling parallel query processing.</p>\n<p><strong>Bloom filters</strong> provide a crucial optimization for negative lookups - when a query searches for logs that don&#39;t exist, the bloom filter can definitively say &quot;not here&quot; without scanning the actual index. This prevents expensive full-index scans for non-existent data, which is common when users search for rare error messages or specific request IDs.</p>\n<blockquote>\n<p><strong>Critical Trade-off</strong>: The Index Engine balances query performance against storage cost. More detailed indexes enable faster queries but consume significant memory and disk space. Label cardinality directly drives index size - each unique label combination creates new index entries.</p>\n</blockquote>\n<h4 id=\"storage-engine\">Storage Engine</h4>\n<p>The <strong>Storage Engine</strong> manages efficient, durable persistence of log data with compression and retention policies. Picture it as a sophisticated warehouse management system that organizes inventory (log data) into compact, labeled boxes (chunks), tracks what&#39;s stored where, and automatically removes old inventory according to business rules.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Storage Engine</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Primary Responsibility</strong></td>\n<td>Persist log data in compressed chunks with durability guarantees</td>\n</tr>\n<tr>\n<td><strong>Input</strong></td>\n<td>Batches of indexed <code>LogEntry</code> objects ready for storage</td>\n</tr>\n<tr>\n<td><strong>Output</strong></td>\n<td>Compressed chunks written to disk with metadata</td>\n</tr>\n<tr>\n<td><strong>Key Interfaces</strong></td>\n<td><code>ChunkWriter</code>, <code>WALManager</code>, <code>RetentionPolicy</code></td>\n</tr>\n<tr>\n<td><strong>Failure Mode</strong></td>\n<td>Data corruption or loss during writes</td>\n</tr>\n<tr>\n<td><strong>Scaling Bottleneck</strong></td>\n<td>Disk I/O bandwidth and compression CPU overhead</td>\n</tr>\n</tbody></table>\n<p>The Storage Engine organizes logs into <strong>time-windowed chunks</strong> - compressed blocks containing logs from specific time periods. This chunking strategy aligns with query patterns (most queries focus on recent time ranges) and enables efficient compression since logs from the same time period often share common patterns and vocabulary.</p>\n<p><strong>Write-ahead logging (WAL)</strong> ensures durability even during system crashes. Before committing log data to compressed chunks, the Storage Engine writes entries to an append-only WAL that survives crashes. During recovery, it replays the WAL to restore any uncommitted data, ensuring no log loss.</p>\n<p>The <strong>retention policy engine</strong> automatically removes old log data according to configurable rules. It tracks chunk metadata and asynchronously deletes chunks that exceed retention thresholds, preventing unbounded storage growth. Retention policies can be configured per log stream based on labels, allowing different retention periods for different services.</p>\n<h4 id=\"query-engine\">Query Engine</h4>\n<p>The <strong>Query Engine</strong> processes search requests and returns matching log entries by orchestrating index lookups and storage access. Think of it as a skilled research assistant who understands your research question, knows exactly which card catalogs and archives to check, and efficiently gathers all relevant information while avoiding unnecessary work.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Query Engine</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Primary Responsibility</strong></td>\n<td>Execute LogQL queries and return matching log entries</td>\n</tr>\n<tr>\n<td><strong>Input</strong></td>\n<td>LogQL query strings with time ranges and filter criteria</td>\n</tr>\n<tr>\n<td><strong>Output</strong></td>\n<td>Ordered streams of matching <code>LogEntry</code> objects</td>\n</tr>\n<tr>\n<td><strong>Key Interfaces</strong></td>\n<td><code>QueryParser</code>, <code>ExecutionPlanner</code>, <code>ResultStreamer</code></td>\n</tr>\n<tr>\n<td><strong>Failure Mode</strong></td>\n<td>Slow or failing queries due to unbounded scans</td>\n</tr>\n<tr>\n<td><strong>Scaling Bottleneck</strong></td>\n<td>Complex query execution and large result set processing</td>\n</tr>\n</tbody></table>\n<p>The Query Engine implements a <strong>three-phase execution model</strong>: parsing queries into abstract syntax trees, optimizing execution plans to minimize data scanning, and streaming results back to clients. Query optimization focuses on <strong>filter pushdown</strong> - applying the most selective filters first to minimize data processing in subsequent stages.</p>\n<p><strong>Result streaming</strong> prevents memory exhaustion when queries match large numbers of log entries. Instead of buffering all results in memory, the Query Engine streams matching entries back to clients as they&#39;re found, using cursor-based pagination to handle result sets larger than memory.</p>\n<blockquote>\n<p><strong>Performance Strategy</strong>: The Query Engine prioritizes <strong>time-bounded queries</strong> over exhaustive searches. Queries without time limits are rejected or automatically constrained to prevent full-dataset scans that could impact system performance for all users.</p>\n</blockquote>\n<h3 id=\"data-flow-architecture\">Data Flow Architecture</h3>\n<p>The system processes log data through a <strong>unidirectional pipeline</strong> where each stage adds structure and indexing to enable efficient queries. Understanding this data flow is crucial for debugging performance issues and planning capacity.</p>\n<p><img src=\"/api/project/log-aggregator/architecture-doc/asset?path=diagrams%2Fingestion-flow.svg\" alt=\"Log Ingestion Sequence\"></p>\n<h4 id=\"ingestion-to-storage-flow\">Ingestion to Storage Flow</h4>\n<p>Log data enters the system through multiple ingestion protocols and follows a consistent processing path regardless of source:</p>\n<ol>\n<li><strong>Protocol Reception</strong>: HTTP, TCP, UDP, or file-tail agents deliver raw log strings to protocol-specific handlers in the Ingestion Engine</li>\n<li><strong>Format Detection</strong>: The system identifies log format (JSON, syslog RFC 3164/5424, or custom patterns) based on content analysis and source configuration</li>\n<li><strong>Buffering</strong>: Raw log strings are placed in memory buffers with overflow to disk during backpressure situations</li>\n<li><strong>Parsing</strong>: The Parser Engine extracts structured fields including timestamp, message content, and labels from raw strings</li>\n<li><strong>Validation</strong>: Parsed entries are validated for required fields (timestamp, message) and label format compliance</li>\n<li><strong>Indexing</strong>: The Index Engine updates inverted indexes and bloom filters with extracted terms and labels</li>\n<li><strong>Batching</strong>: Validated entries accumulate in storage batches organized by time windows</li>\n<li><strong>WAL Writing</strong>: Complete batches are written to the write-ahead log for durability</li>\n<li><strong>Compression</strong>: Batches are compressed using configured algorithms (gzip, snappy, or zstd)</li>\n<li><strong>Chunk Storage</strong>: Compressed batches become chunks stored on disk with metadata for retrieval</li>\n</ol>\n<p>This pipeline implements <strong>at-least-once delivery semantics</strong> where log entries may be processed multiple times during failures, but will never be lost once successfully written to the WAL. Duplicate detection during recovery prevents multiple copies of the same log entry in final storage.</p>\n<blockquote>\n<p><strong>Backpressure Propagation</strong>: When any downstream stage becomes overloaded, backpressure flows upstream through the pipeline. Storage pressure causes batching delays, which fill index queues, which eventually cause ingestion buffering and finally connection rejection.</p>\n</blockquote>\n<h4 id=\"query-processing-flow\">Query Processing Flow</h4>\n<p>Query execution follows a different path optimized for fast data retrieval:</p>\n<p><img src=\"/api/project/log-aggregator/architecture-doc/asset?path=diagrams%2Fquery-flow.svg\" alt=\"Query Processing Sequence\"></p>\n<ol>\n<li><strong>Query Parsing</strong>: LogQL query strings are parsed into abstract syntax trees with validation for syntax errors</li>\n<li><strong>Time Range Extraction</strong>: Query time bounds determine which index partitions and storage chunks to examine</li>\n<li><strong>Execution Planning</strong>: The query planner generates an optimized execution sequence with filter pushdown</li>\n<li><strong>Index Consultation</strong>: Bloom filters provide fast negative lookups, while inverted indexes identify candidate chunks</li>\n<li><strong>Chunk Loading</strong>: Only chunks likely to contain matches are loaded from storage and decompressed</li>\n<li><strong>Entry Filtering</strong>: Log entries from loaded chunks are tested against query filters</li>\n<li><strong>Result Ordering</strong>: Matching entries are sorted by timestamp for consistent result ordering</li>\n<li><strong>Streaming Response</strong>: Results stream back to clients using cursor-based pagination</li>\n</ol>\n<p>The query path implements <strong>short-circuit evaluation</strong> where subsequent processing stages are skipped when earlier stages determine no matches are possible. Bloom filter negative responses eliminate chunk loads, while time range validation skips entire partitions.</p>\n<p><strong>Query optimization</strong> focuses on minimizing I/O operations since storage access dominates query latency. The execution planner reorders filters to apply the most selective constraints first, reducing the amount of data that must be decompressed and evaluated.</p>\n<h3 id=\"recommended-project-structure\">Recommended Project Structure</h3>\n<p>The codebase organization reflects the component architecture with clear module boundaries and shared infrastructure. This structure supports independent development of components while maintaining clean interfaces.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>logaggr/\n├── cmd/\n│   ├── server/                    ← Main server entry point\n│   │   └── main.go\n│   ├── ingester/                  ← Standalone ingestion service\n│   │   └── main.go\n│   └── query/                     ← Query-only service for scaling\n│       └── main.go\n├── internal/                      ← Private implementation packages\n│   ├── ingestion/                 ← Ingestion Engine (Milestone 1)\n│   │   ├── http.go               ← HTTP log receiver\n│   │   ├── syslog.go             ← TCP/UDP syslog receiver\n│   │   ├── filetail.go           ← File watching agent\n│   │   ├── buffer.go             ← Memory/disk buffering\n│   │   └── backpressure.go       ← Flow control logic\n│   ├── parser/                    ← Parser Engine (Milestone 1)\n│   │   ├── json.go               ← JSON log parser\n│   │   ├── syslog.go             ← Syslog format parser\n│   │   ├── regex.go              ← Pattern-based parser\n│   │   └── labels.go             ← Label extraction logic\n│   ├── index/                     ← Index Engine (Milestone 2)\n│   │   ├── inverted.go           ← Inverted index implementation\n│   │   ├── bloom.go              ← Bloom filter implementation\n│   │   ├── partition.go          ← Time-based partitioning\n│   │   └── compaction.go         ← Index maintenance\n│   ├── storage/                   ← Storage Engine (Milestone 4)\n│   │   ├── chunks.go             ← Chunk management\n│   │   ├── wal.go                ← Write-ahead logging\n│   │   ├── compression.go        ← Compression algorithms\n│   │   └── retention.go          ← Retention policy engine\n│   ├── query/                     ← Query Engine (Milestone 3)\n│   │   ├── parser.go             ← LogQL parser\n│   │   ├── planner.go            ← Query optimization\n│   │   ├── executor.go           ← Query execution\n│   │   └── streaming.go          ← Result streaming\n│   ├── tenant/                    ← Multi-tenancy (Milestone 5)\n│   │   ├── isolation.go          ← Tenant data separation\n│   │   ├── ratelimit.go          ← Per-tenant rate limiting\n│   │   └── auth.go               ← Authentication/authorization\n│   └── alerting/                  ← Log-based alerting (Milestone 5)\n│       ├── rules.go              ← Alert rule evaluation\n│       ├── notification.go       ← Alert delivery\n│       └── dedup.go              ← Alert deduplication\n├── pkg/                          ← Public API packages\n│   ├── types/                    ← Shared data types\n│   │   ├── log.go               ← LogEntry, Labels, LogStream\n│   │   ├── config.go            ← Config, Metrics\n│   │   └── query.go             ← TimeRange, query types\n│   └── client/                   ← Client library\n│       └── client.go            ← HTTP client for log submission\n├── api/                         ← API definitions\n│   ├── http/                    ← HTTP API handlers\n│   │   ├── ingest.go           ← Log ingestion endpoints\n│   │   └── query.go            ← Query API endpoints\n│   └── grpc/                   ← gRPC definitions (future)\n├── configs/                    ← Configuration files\n│   ├── local.yaml             ← Local development config\n│   └── production.yaml        ← Production configuration\n├── deployments/               ← Deployment configurations\n│   ├── docker/               ← Docker configurations\n│   └── kubernetes/           ← K8s manifests\n├── docs/                     ← Documentation\n└── scripts/                  ← Build and utility scripts\n    ├── build.sh\n    └── test.sh</code></pre></div>\n\n<p>This structure provides <strong>clear separation of concerns</strong> where each internal package has a single responsibility. The <code>pkg/</code> directory contains types and interfaces that multiple components share, while <code>internal/</code> packages implement component-specific logic that shouldn&#39;t be imported by external projects.</p>\n<blockquote>\n<p><strong>Development Strategy</strong>: Start by implementing the shared types in <code>pkg/types/</code>, then build components in milestone order. Each milestone adds new packages without modifying existing ones, supporting incremental development and testing.</p>\n</blockquote>\n<p>The <strong>three-tier organization</strong> (cmd/internal/pkg) follows Go community conventions where:</p>\n<ul>\n<li><code>cmd/</code> contains executable entry points with minimal logic</li>\n<li><code>internal/</code> holds private implementation details</li>\n<li><code>pkg/</code> exposes public APIs that external projects could import</li>\n</ul>\n<p><strong>Configuration management</strong> centralizes all system settings in the <code>configs/</code> directory with environment-specific files. This approach supports different configurations for development, testing, and production without code changes.</p>\n<blockquote>\n<p><strong>Architecture Decision: Monorepo vs. Multi-repo</strong></p>\n<ul>\n<li><strong>Context</strong>: The system has five distinct components that could be separate services or combined into a monolithic deployment</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Separate repositories for each component with independent deployment</li>\n<li>Single repository with multiple deployment targets</li>\n<li>Hybrid approach with shared types repository and separate service repositories</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Single repository with flexible deployment options</li>\n<li><strong>Rationale</strong>: Shared data types and interfaces create tight coupling between components. Separate repositories would require complex dependency management and version synchronization. A monorepo simplifies development while still supporting separate service deployment through multiple <code>cmd/</code> entry points.</li>\n<li><strong>Consequences</strong>: Enables rapid development and consistent interfaces, but requires disciplined module boundaries to prevent tight coupling in the codebase</li>\n</ul>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The log aggregation system requires careful technology choices that balance development simplicity with production performance. The following recommendations provide a clear development path from prototype to production-ready system.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>HTTP Server</strong></td>\n<td><code>net/http</code> with <code>gorilla/mux</code></td>\n<td><code>gin-gonic/gin</code> with middleware</td>\n<td>Standard library provides sufficient performance; advanced option adds convenience</td>\n</tr>\n<tr>\n<td><strong>TCP/UDP Handling</strong></td>\n<td><code>net.Listen</code> with worker pools</td>\n<td><code>fasthttp</code> with connection pooling</td>\n<td>Standard library handles syslog protocols well; optimization can come later</td>\n</tr>\n<tr>\n<td><strong>JSON Parsing</strong></td>\n<td><code>encoding/json</code> with struct tags</td>\n<td><code>json-iterator/go</code> for performance</td>\n<td>Standard library sufficient for most throughput; advanced option for high-scale</td>\n</tr>\n<tr>\n<td><strong>Storage Backend</strong></td>\n<td>Local filesystem with <code>os</code> package</td>\n<td>S3-compatible with <code>minio-go</code></td>\n<td>Local storage simplifies development; object storage scales better</td>\n</tr>\n<tr>\n<td><strong>Compression</strong></td>\n<td><code>compress/gzip</code> standard library</td>\n<td><code>klauspost/compress</code> optimized versions</td>\n<td>Standard library provides good compression; advanced option offers speed</td>\n</tr>\n<tr>\n<td><strong>Concurrency</strong></td>\n<td><code>sync</code> package with worker pools</td>\n<td><code>ants</code> goroutine pool library</td>\n<td>Manual pool management teaches fundamentals; library provides production features</td>\n</tr>\n</tbody></table>\n<h4 id=\"file-structure-foundation\">File Structure Foundation</h4>\n<p>Start development with this minimal structure that expands as you implement each milestone:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// pkg/types/log.go - Core data types used throughout the system</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> types</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LogEntry represents a single log message with metadata</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LogEntry</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#6A737D\">         // When the log was created</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Labels    </span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#6A737D\">           // Key-value pairs for filtering  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Message   </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\">           // The actual log content</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Labels provides structured metadata for log entries</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Labels</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LogStream groups related log entries by label set</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LogStream</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Labels  </span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#6A737D\">      // Common labels for all entries</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Entries []</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#6A737D\">  // Time-ordered log entries</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TimeRange specifies query time boundaries</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TimeRange</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Start </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#6A737D\">    // Inclusive start time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    End   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#6A737D\">    // Exclusive end time  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Config holds all system configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Config</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    HTTPPort      </span><span style=\"color:#F97583\">int</span><span style=\"color:#6A737D\">    // HTTP ingestion port</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TCPPort       </span><span style=\"color:#F97583\">int</span><span style=\"color:#6A737D\">    // TCP syslog port</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    UDPPort       </span><span style=\"color:#F97583\">int</span><span style=\"color:#6A737D\">    // UDP syslog port  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StoragePath   </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\"> // Local storage directory</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BufferSize    </span><span style=\"color:#F97583\">int</span><span style=\"color:#6A737D\">    // In-memory buffer size</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ChunkSize     </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">  // Storage chunk size in bytes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RetentionDays </span><span style=\"color:#F97583\">int</span><span style=\"color:#6A737D\">    // Log retention period</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Metrics tracks system performance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Metrics</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logsIngested    </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">  // Total logs processed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queriesExecuted </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">  // Total queries handled</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bytesStored     </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">  // Total storage used</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// pkg/types/config.go - Configuration management</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> types</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strconv</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Configuration constants</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HTTP_PORT</span><span style=\"color:#F97583\">      =</span><span style=\"color:#79B8FF\"> 8080</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TCP_PORT</span><span style=\"color:#F97583\">       =</span><span style=\"color:#79B8FF\"> 1514</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UDP_PORT</span><span style=\"color:#F97583\">       =</span><span style=\"color:#79B8FF\"> 1514</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STORAGE_PATH</span><span style=\"color:#F97583\">   =</span><span style=\"color:#9ECBFF\"> \"./data\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    BUFFER_SIZE</span><span style=\"color:#F97583\">    =</span><span style=\"color:#79B8FF\"> 10000</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CHUNK_SIZE</span><span style=\"color:#F97583\">     =</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#6A737D\"> // 1MB</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RETENTION_DAYS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 30</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadConfig reads configuration from environment variables with defaults</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> LoadConfig</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        HTTPPort:      </span><span style=\"color:#B392F0\">getEnvInt</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"HTTP_PORT\"</span><span style=\"color:#E1E4E8\">, HTTP_PORT),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TCPPort:       </span><span style=\"color:#B392F0\">getEnvInt</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"TCP_PORT\"</span><span style=\"color:#E1E4E8\">, TCP_PORT), </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        UDPPort:       </span><span style=\"color:#B392F0\">getEnvInt</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"UDP_PORT\"</span><span style=\"color:#E1E4E8\">, UDP_PORT),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        StoragePath:   </span><span style=\"color:#B392F0\">getEnvString</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"STORAGE_PATH\"</span><span style=\"color:#E1E4E8\">, STORAGE_PATH),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        BufferSize:    </span><span style=\"color:#B392F0\">getEnvInt</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"BUFFER_SIZE\"</span><span style=\"color:#E1E4E8\">, BUFFER_SIZE),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ChunkSize:     </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">getEnvInt</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"CHUNK_SIZE\"</span><span style=\"color:#E1E4E8\">, CHUNK_SIZE)),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        RetentionDays: </span><span style=\"color:#B392F0\">getEnvInt</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"RETENTION_DAYS\"</span><span style=\"color:#E1E4E8\">, RETENTION_DAYS),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Create storage directory if it doesn't exist</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    os.</span><span style=\"color:#B392F0\">MkdirAll</span><span style=\"color:#E1E4E8\">(config.StoragePath, </span><span style=\"color:#79B8FF\">0755</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Helper functions for environment variable parsing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> getEnvInt</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">defaultVal</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> val </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">Getenv</span><span style=\"color:#E1E4E8\">(key); val </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> parsed, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> strconv.</span><span style=\"color:#B392F0\">Atoi</span><span style=\"color:#E1E4E8\">(val); err </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> parsed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> defaultVal</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> getEnvString</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">defaultVal</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> val </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">Getenv</span><span style=\"color:#E1E4E8\">(key); val </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> val</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> defaultVal</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// pkg/types/metrics.go - Thread-safe metrics tracking</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> types</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#B392F0\">sync/atomic</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// IncrementLogsIngested atomically increments the ingestion counter</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Metrics</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">IncrementLogsIngested</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    atomic.</span><span style=\"color:#B392F0\">AddInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">m.logsIngested, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// IncrementQueriesExecuted atomically increments the query counter  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Metrics</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">IncrementQueriesExecuted</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    atomic.</span><span style=\"color:#B392F0\">AddInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">m.queriesExecuted, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AddBytesStored atomically adds to the storage counter</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Metrics</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">AddBytesStored</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">bytes</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    atomic.</span><span style=\"color:#B392F0\">AddInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">m.bytesStored, bytes)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetStats returns current counters safely</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Metrics</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetStats</span><span style=\"color:#E1E4E8\">() (</span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logs </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> atomic.</span><span style=\"color:#B392F0\">LoadInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">m.logsIngested)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queries </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> atomic.</span><span style=\"color:#B392F0\">LoadInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">m.queriesExecuted) </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> logs, queries</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p>The following utilities handle cross-cutting concerns that aren&#39;t the primary learning focus:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/common/buffer.go - Ring buffer for log batching</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> common</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/yourname/logaggr/pkg/types</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RingBuffer provides thread-safe circular buffering for log entries</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RingBuffer</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    entries []</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">LogEntry</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    head    </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tail    </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    size    </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    maxSize </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex   </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    notEmpty </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Cond</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    notFull  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Cond</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewRingBuffer creates a bounded buffer with the specified capacity</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewRingBuffer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">maxSize</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RingBuffer</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rb </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">RingBuffer</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        entries: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#E1E4E8\">, maxSize),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        maxSize: maxSize,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rb.notEmpty </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sync.</span><span style=\"color:#B392F0\">NewCond</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">rb.mutex)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rb.notFull </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sync.</span><span style=\"color:#B392F0\">NewCond</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">rb.mutex)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> rb</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Put adds an entry to the buffer, blocking if full</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rb </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RingBuffer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Put</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">entry</span><span style=\"color:#B392F0\"> types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rb.mutex.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> rb.mutex.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Wait for space</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> rb.size </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> rb.maxSize {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        rb.notFull.</span><span style=\"color:#B392F0\">Wait</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rb.entries[rb.tail] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> entry</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rb.tail </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (rb.tail </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">%</span><span style=\"color:#E1E4E8\"> rb.maxSize</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rb.size</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rb.notEmpty.</span><span style=\"color:#B392F0\">Signal</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Take removes and returns an entry, blocking if empty</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rb </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RingBuffer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Take</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rb.mutex.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> rb.mutex.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Wait for data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> rb.size </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        rb.notEmpty.</span><span style=\"color:#B392F0\">Wait</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    entry </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> rb.entries[rb.head]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rb.head </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (rb.head </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">%</span><span style=\"color:#E1E4E8\"> rb.maxSize</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rb.size</span><span style=\"color:#F97583\">--</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rb.notFull.</span><span style=\"color:#B392F0\">Signal</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> entry</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeletons\">Core Logic Skeletons</h4>\n<p>These function signatures map to the detailed algorithms described in each component section:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// cmd/server/main.go - Main application entry point</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> main</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">log</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os/signal</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">syscall</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/yourname/logaggr/pkg/types</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> main</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Load configuration using LoadConfig()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Initialize metrics tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Start ingestion engine with configured ports</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Start index engine background workers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Start storage engine with WAL recovery</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Start query engine HTTP server</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Setup graceful shutdown on SIGTERM/SIGINT</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Wait for shutdown signal and cleanup resources</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use context.WithCancel for coordinated shutdown</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Start each component in its own goroutine</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use sync.WaitGroup to wait for clean shutdown</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"language-specific-implementation-hints\">Language-Specific Implementation Hints</h4>\n<p><strong>Go-Specific Optimization Tips:</strong></p>\n<ul>\n<li>Use <code>sync.Pool</code> for frequently allocated objects like <code>LogEntry</code> structs to reduce GC pressure</li>\n<li>Implement <code>io.WriterTo</code> interface on log chunks for efficient disk writes with <code>io.Copy</code></li>\n<li>Use <code>unsafe.Pointer</code> carefully in bloom filter bit manipulation for performance (advanced)</li>\n<li>Prefer <code>[]byte</code> over <code>string</code> in parsing hot paths to avoid allocations</li>\n<li>Use <code>sync/atomic</code> for lock-free counters in metrics collection</li>\n</ul>\n<p><strong>Error Handling Patterns:</strong></p>\n<ul>\n<li>Wrap errors with context using <code>fmt.Errorf(&quot;operation failed: %w&quot;, err)</code> for debugging</li>\n<li>Define custom error types for component-specific failures (ParseError, IndexError)  </li>\n<li>Use structured logging with levels (ERROR, WARN, INFO, DEBUG) for operational visibility</li>\n<li>Implement circuit breakers for external dependencies like storage backends</li>\n</ul>\n<p><strong>Concurrency Guidelines:</strong></p>\n<ul>\n<li>Use buffered channels for producer-consumer patterns between components</li>\n<li>Implement worker pools with configurable sizes for CPU-bound operations</li>\n<li>Use <code>context.Context</code> for request cancellation and timeouts throughout the system</li>\n<li>Apply read-write mutexes (<code>sync.RWMutex</code>) for data structures with frequent reads</li>\n</ul>\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>After Milestone 1 (Ingestion):</strong></p>\n<ul>\n<li>Run <code>curl -X POST localhost:8080/ingest -d &#39;{&quot;timestamp&quot;:&quot;2023-01-01T10:00:00Z&quot;,&quot;level&quot;:&quot;INFO&quot;,&quot;message&quot;:&quot;test log&quot;}&#39;</code></li>\n<li>Verify response: <code>{&quot;status&quot;:&quot;accepted&quot;,&quot;entries&quot;:1}</code></li>\n<li>Check logs directory contains new entries: <code>ls -la ./data/</code></li>\n<li>Send 100 concurrent requests: should handle without errors or memory leaks</li>\n</ul>\n<p><strong>After Milestone 2 (Indexing):</strong></p>\n<ul>\n<li>Submit logs with various labels: <code>level=ERROR</code>, <code>service=api</code>, <code>host=server1</code></li>\n<li>Verify index files created: <code>ls -la ./data/indexes/</code></li>\n<li>Check index contains expected terms: implement debug endpoint showing index contents</li>\n<li>Test bloom filter efficiency: query for non-existent terms should return quickly</li>\n</ul>\n<p><strong>After Milestone 3 (Querying):</strong></p>\n<ul>\n<li>Execute basic query: <code>curl &quot;localhost:8080/query?q=level=ERROR&amp;start=1h&quot;</code></li>\n<li>Verify results contain only ERROR level logs with proper JSON formatting</li>\n<li>Test regex queries: <code>q=message~&quot;database.*timeout&quot;</code> should match pattern</li>\n<li>Performance test: queries over large datasets should complete within 5 seconds</li>\n</ul>\n<p><strong>After Milestone 4 (Storage):</strong></p>\n<ul>\n<li>Verify chunk compression: storage files should be significantly smaller than raw logs</li>\n<li>Test WAL recovery: kill process during ingestion, restart, verify no data loss</li>\n<li>Check retention: configure 1-day retention, verify old chunks are deleted</li>\n<li>Monitor storage growth: should be bounded by retention policies</li>\n</ul>\n<p><strong>After Milestone 5 (Multi-tenancy):</strong>  </p>\n<ul>\n<li>Submit logs with different tenant headers: <code>X-Tenant-ID: tenant1</code></li>\n<li>Verify tenant isolation: tenant1 queries should not return tenant2 logs</li>\n<li>Test rate limiting: exceed tenant limits should return 429 status</li>\n<li>Configure alert rules: error rate spikes should trigger notifications</li>\n</ul>\n<h2 id=\"data-model\">Data Model</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section provides foundational data structures used across all milestones (1-5), with core types introduced in Milestone 1 (Log Ingestion), index structures in Milestone 2 (Log Index), and storage formats in Milestone 4 (Log Storage &amp; Compression).</p>\n</blockquote>\n<h3 id=\"mental-model-the-postal-service-data-system\">Mental Model: The Postal Service Data System</h3>\n<p>Before diving into the technical data structures, think of our log aggregation system like a comprehensive postal service data management system. When the postal service processes mail, they need several types of data structures:</p>\n<p><strong>Letters (Log Entries)</strong>: Each piece of mail has a timestamp (postmark), addressing information (labels), and content (message). The postal service doesn&#39;t modify the letter content, but they add metadata like routing stamps and sorting codes.</p>\n<p><strong>Address Books (Labels)</strong>: Every letter has addressing information - sender, recipient, postal codes, delivery routes. This information is structured as key-value pairs (street=Main, city=Springfield, zip=12345) and is used for routing and organization.</p>\n<p><strong>Card Catalogs (Indexes)</strong>: The postal service maintains catalogs that let them quickly find &quot;all mail going to zip code 12345&quot; or &quot;all express mail from yesterday.&quot; These catalogs don&#39;t contain the actual letters - they contain pointers to where letters can be found.</p>\n<p><strong>Storage Boxes (Chunks)</strong>: Letters are bundled into containers organized by time and destination. Each container is compressed to save space, labeled with metadata about its contents, and stored in warehouses with retention policies.</p>\n<p><strong>Filing Systems (Serialization)</strong>: Everything must be written down in standardized formats so different postal workers can read and process the information consistently, even after shifts change or systems restart.</p>\n<p>This mental model helps us understand why our data structures are designed the way they are - we need efficient ways to represent, organize, find, and store log data at scale.</p>\n<p><img src=\"/api/project/log-aggregator/architecture-doc/asset?path=diagrams%2Fdata-model.svg\" alt=\"Data Model Relationships\"></p>\n<h3 id=\"core-data-types\">Core Data Types</h3>\n<p>The foundation of our log aggregation system rests on three primary data types that represent the fundamental units of information. These types are designed to be simple, efficient, and composable, allowing them to flow through our ingestion, indexing, and query pipelines without unnecessary transformations.</p>\n<h4 id=\"logentry-structure\">LogEntry Structure</h4>\n<p>The <code>LogEntry</code> represents a single log message with its associated metadata. This is the atomic unit of data in our system - every log line that enters our system becomes a <code>LogEntry</code>, and every query result returns collections of <code>LogEntry</code> instances.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Timestamp</td>\n<td>time.Time</td>\n<td>Precise moment when log event occurred, stored in UTC with nanosecond precision for ordering</td>\n</tr>\n<tr>\n<td>Labels</td>\n<td>Labels</td>\n<td>Key-value pairs providing structured metadata about the log source and context</td>\n</tr>\n<tr>\n<td>Message</td>\n<td>string</td>\n<td>Raw log content exactly as received, with no parsing or modification applied</td>\n</tr>\n</tbody></table>\n<p>The design of <code>LogEntry</code> reflects several important decisions. The timestamp uses Go&#39;s <code>time.Time</code> type, which provides nanosecond precision and handles timezone conversions automatically. This precision is crucial for maintaining log ordering, especially in high-throughput systems where multiple logs might arrive within the same millisecond. The labels field contains structured metadata that enables efficient querying and filtering. The message field preserves the original log content unchanged, ensuring we never lose information during ingestion.</p>\n<blockquote>\n<p><strong>Decision: Immutable LogEntry Design</strong></p>\n<ul>\n<li><strong>Context</strong>: Log entries could be mutable (allowing post-ingestion modifications) or immutable (fixed after creation)</li>\n<li><strong>Options Considered</strong>: Mutable entries (allows correction/enrichment), immutable entries (ensures data integrity), hybrid approach (some fields mutable)</li>\n<li><strong>Decision</strong>: Completely immutable <code>LogEntry</code> instances</li>\n<li><strong>Rationale</strong>: Immutability prevents accidental data corruption, enables safe concurrent access without locks, simplifies reasoning about data flow, and ensures audit trails remain intact</li>\n<li><strong>Consequences</strong>: Any enrichment or correction requires creating new entries rather than modifying existing ones, which increases storage slightly but dramatically improves system reliability</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Design Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Mutable LogEntry</td>\n<td>Can fix/enrich data post-ingestion, Lower memory usage</td>\n<td>Race conditions, Data corruption risk, Complex synchronization</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Immutable LogEntry</td>\n<td>Thread-safe, Predictable behavior, Audit integrity</td>\n<td>Slight storage overhead, Cannot correct errors in-place</td>\n<td>Yes ✅</td>\n</tr>\n<tr>\n<td>Hybrid Approach</td>\n<td>Flexibility with safety for critical fields</td>\n<td>Complex rules, Partial safety only</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<h4 id=\"labels-structure\">Labels Structure</h4>\n<p>Labels provide the structured metadata that makes log entries queryable and filterable. The <code>Labels</code> type is implemented as a simple map, but its usage patterns and constraints are carefully designed to balance query performance with storage efficiency.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Labels</td>\n<td>map[string]string</td>\n<td>Key-value pairs where keys are label names and values are label values, both stored as UTF-8 strings</td>\n</tr>\n</tbody></table>\n<p>Labels serve as the primary mechanism for log organization and querying. Common label keys include <code>service</code>, <code>level</code>, <code>host</code>, <code>environment</code>, and <code>request_id</code>. The values should be relatively low-cardinality - for example, <code>level</code> might have values like <code>error</code>, <code>warn</code>, <code>info</code>, <code>debug</code>, but should not contain unique values like timestamps or request IDs unless specifically needed for correlation.</p>\n<blockquote>\n<p><strong>Decision: String-Only Label Values</strong></p>\n<ul>\n<li><strong>Context</strong>: Labels could support multiple data types (strings, numbers, booleans) or be string-only</li>\n<li><strong>Options Considered</strong>: Typed labels (native int/bool/float support), string-only labels, JSON-encoded complex types</li>\n<li><strong>Decision</strong>: All label values stored as strings</li>\n<li><strong>Rationale</strong>: Simplifies indexing logic, avoids type conversion errors, matches common logging practice where structured data is string-formatted, reduces serialization complexity</li>\n<li><strong>Consequences</strong>: Numeric comparisons require string parsing, but this matches how most log systems work and keeps the implementation straightforward</li>\n</ul>\n</blockquote>\n<p>The label system must carefully manage cardinality to prevent index explosion. High-cardinality labels (those with many unique values) can dramatically increase index size and query time. Our design includes monitoring and alerting for cardinality growth.</p>\n<table>\n<thead>\n<tr>\n<th>Label Cardinality</th>\n<th>Index Impact</th>\n<th>Query Performance</th>\n<th>Storage Impact</th>\n<th>Recommended Use</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Low (&lt; 100 values)</td>\n<td>Minimal index growth</td>\n<td>Fast lookups</td>\n<td>Efficient</td>\n<td>Primary filtering (service, level, env)</td>\n</tr>\n<tr>\n<td>Medium (100-1000)</td>\n<td>Moderate index size</td>\n<td>Good performance</td>\n<td>Acceptable</td>\n<td>Secondary attributes (host, version)</td>\n</tr>\n<tr>\n<td>High (&gt; 1000)</td>\n<td>Large index growth</td>\n<td>Slower queries</td>\n<td>Storage intensive</td>\n<td>Avoid or use sparingly (trace_id)</td>\n</tr>\n<tr>\n<td>Unbounded</td>\n<td>Index explosion</td>\n<td>Query timeouts</td>\n<td>Unsustainable</td>\n<td>Never use (timestamp, unique_id)</td>\n</tr>\n</tbody></table>\n<h4 id=\"timerange-structure\">TimeRange Structure</h4>\n<p>Time-based querying is fundamental to log analysis, and the <code>TimeRange</code> structure provides a standardized way to specify temporal boundaries for queries and storage operations.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Start</td>\n<td>time.Time</td>\n<td>Inclusive beginning of the time range, logs at exactly this timestamp are included</td>\n</tr>\n<tr>\n<td>End</td>\n<td>time.Time</td>\n<td>Exclusive end of the time range, logs at exactly this timestamp are excluded</td>\n</tr>\n</tbody></table>\n<p>The <code>TimeRange</code> uses half-open interval semantics [Start, End) which aligns with standard programming practices and prevents double-counting when adjacent time ranges are processed. This structure is used throughout the system for query filtering, chunk organization, retention policy application, and index partitioning.</p>\n<h4 id=\"logstream-structure\">LogStream Structure</h4>\n<p>When working with collections of related log entries, the <code>LogStream</code> structure groups entries that share the same labels, representing a continuous stream of logs from a specific source.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Labels</td>\n<td>Labels</td>\n<td>Common labels shared by all entries in this stream, used for stream identification</td>\n</tr>\n<tr>\n<td>Entries</td>\n<td>[]LogEntry</td>\n<td>Ordered collection of log entries, sorted by timestamp in ascending order</td>\n</tr>\n</tbody></table>\n<p>Log streams are the unit of organization for storage and compression. All entries within a stream share identical labels, which allows for efficient storage since the labels only need to be stored once per stream rather than duplicated for each entry. Streams are also the granularity at which retention policies are applied.</p>\n<blockquote>\n<p>The critical insight with log streams is that grouping by identical labels creates natural boundaries for storage optimization. Within a stream, we only store timestamps and messages since labels are constant, reducing storage overhead by 30-50% in typical deployments.</p>\n</blockquote>\n<h3 id=\"index-data-structures\">Index Data Structures</h3>\n<p>The indexing layer transforms our simple log entries into queryable data structures. These indexes must support fast lookups across millions or billions of log entries while remaining compact enough to fit in memory or be quickly loaded from disk.</p>\n<h4 id=\"inverted-index-structure\">Inverted Index Structure</h4>\n<p>The inverted index maps terms (words or label values) to the set of log entries containing those terms. This is the core data structure enabling fast text search and label filtering.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>TermIndex</td>\n<td>map[string]*PostingsList</td>\n<td>Maps each unique term to a list of log entries containing that term</td>\n</tr>\n<tr>\n<td>PostingsList</td>\n<td>[]EntryReference</td>\n<td>Ordered list of references to log entries, sorted by timestamp for efficient range queries</td>\n</tr>\n<tr>\n<td>EntryReference</td>\n<td>struct</td>\n<td>Compact reference to a log entry containing chunk ID and offset within chunk</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th>Field in EntryReference</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ChunkID</td>\n<td>string</td>\n<td>Unique identifier of the storage chunk containing this log entry</td>\n</tr>\n<tr>\n<td>Offset</td>\n<td>uint32</td>\n<td>Byte offset within the chunk where this log entry begins</td>\n</tr>\n<tr>\n<td>Timestamp</td>\n<td>time.Time</td>\n<td>Copy of entry timestamp for sorting and filtering without chunk access</td>\n</tr>\n</tbody></table>\n<p>The inverted index design balances memory usage with query performance. Terms are extracted from both the log message text (split on whitespace and punctuation) and label values. Each posting list is kept sorted by timestamp, enabling efficient time-range queries through binary search.</p>\n<blockquote>\n<p><strong>Decision: Separate Term Extraction for Messages vs Labels</strong></p>\n<ul>\n<li><strong>Context</strong>: Text indexing could treat message content and label values identically or differently</li>\n<li><strong>Options Considered</strong>: Unified term extraction, separate message/label indexing, no message text indexing</li>\n<li><strong>Decision</strong>: Separate indexing strategies for message text vs label values</li>\n<li><strong>Rationale</strong>: Label values are structured and should be indexed exactly (service=&quot;api&quot;), while message text benefits from tokenization and stemming for full-text search</li>\n<li><strong>Consequences</strong>: More complex indexing logic but much better query performance for both structured queries (level=error) and text search (message contains &quot;timeout&quot;)</li>\n</ul>\n</blockquote>\n<p>The term extraction process follows different rules for different content types:</p>\n<table>\n<thead>\n<tr>\n<th>Content Type</th>\n<th>Extraction Method</th>\n<th>Example Input</th>\n<th>Extracted Terms</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Label Values</td>\n<td>Exact value indexing</td>\n<td>service=&quot;user-api&quot;</td>\n<td>[&quot;user-api&quot;]</td>\n</tr>\n<tr>\n<td>Message Text</td>\n<td>Tokenization + normalization</td>\n<td>&quot;Request timeout after 30s&quot;</td>\n<td>[&quot;request&quot;, &quot;timeout&quot;, &quot;after&quot;, &quot;30s&quot;]</td>\n</tr>\n<tr>\n<td>JSON Fields</td>\n<td>Key-value extraction</td>\n<td>{&quot;user_id&quot;: 12345}</td>\n<td>[&quot;user_id:12345&quot;]</td>\n</tr>\n<tr>\n<td>Structured Logs</td>\n<td>Field-aware parsing</td>\n<td>timestamp=... level=ERROR msg=...</td>\n<td>[&quot;ERROR&quot;, extracted message terms]</td>\n</tr>\n</tbody></table>\n<h4 id=\"bloom-filter-implementation\">Bloom Filter Implementation</h4>\n<p>Bloom filters provide fast negative lookups, allowing the system to quickly determine that a term definitely does NOT exist in a chunk without accessing the chunk data. This dramatically reduces disk I/O for queries.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>BitArray</td>\n<td>[]uint64</td>\n<td>Packed bit array storing the bloom filter bits, sized for target false positive rate</td>\n</tr>\n<tr>\n<td>HashFunctions</td>\n<td>[]hash.Hash</td>\n<td>Set of independent hash functions used for bit setting and testing</td>\n</tr>\n<tr>\n<td>Parameters</td>\n<td>BloomParams</td>\n<td>Configuration controlling filter size and hash count for desired accuracy</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th>Field in BloomParams</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ExpectedElements</td>\n<td>uint32</td>\n<td>Estimated number of unique terms this filter will store</td>\n</tr>\n<tr>\n<td>FalsePositiveRate</td>\n<td>float64</td>\n<td>Target probability of false positives (typically 0.01 = 1%)</td>\n</tr>\n<tr>\n<td>BitArraySize</td>\n<td>uint32</td>\n<td>Calculated size of bit array needed for target accuracy</td>\n</tr>\n<tr>\n<td>HashCount</td>\n<td>uint32</td>\n<td>Number of hash functions to use (calculated from other parameters)</td>\n</tr>\n</tbody></table>\n<p>The bloom filter sizing calculations ensure optimal performance for our expected workload:</p>\n<ol>\n<li><strong>Bit Array Sizing</strong>: For <code>n</code> expected elements and false positive rate <code>p</code>, we need <code>m = -n * ln(p) / (ln(2)^2)</code> bits</li>\n<li><strong>Hash Function Count</strong>: Optimal number is <code>k = (m/n) * ln(2)</code>, typically 3-5 functions for our parameters  </li>\n<li><strong>Memory Usage</strong>: Each filter uses approximately 1.44 bytes per expected unique term</li>\n<li><strong>Update Protocol</strong>: Filters are immutable once created; chunk compaction creates new filters</li>\n</ol>\n<blockquote>\n<p><strong>Decision: Per-Chunk Bloom Filters</strong></p>\n<ul>\n<li><strong>Context</strong>: Bloom filters could be global (one per index), per-time-partition, or per-chunk</li>\n<li><strong>Options Considered</strong>: Single global filter, time-based filters, chunk-based filters</li>\n<li><strong>Decision</strong>: One bloom filter per storage chunk</li>\n<li><strong>Rationale</strong>: Chunk-based filters have optimal selectivity (can eliminate entire chunks from queries), reasonable memory overhead (filters stay in memory), and align with storage/retention boundaries</li>\n<li><strong>Consequences</strong>: More bloom filters to manage but much better query performance since entire chunks can be skipped</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Filter Scope</th>\n<th>Memory Usage</th>\n<th>Query Selectivity</th>\n<th>Management Complexity</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Global Filter</td>\n<td>Low memory</td>\n<td>Poor selectivity</td>\n<td>Simple management</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Time-Based Filters</td>\n<td>Medium memory</td>\n<td>Good for time queries</td>\n<td>Medium complexity</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Per-Chunk Filters</td>\n<td>Higher memory</td>\n<td>Excellent selectivity</td>\n<td>More complex</td>\n<td>Yes ✅</td>\n</tr>\n</tbody></table>\n<h4 id=\"time-based-partitioning-metadata\">Time-Based Partitioning Metadata</h4>\n<p>The index is partitioned by time to enable efficient time-range queries and support retention policies. Each partition contains metadata describing its temporal boundaries and contents.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>PartitionID</td>\n<td>string</td>\n<td>Unique identifier combining time window and partition sequence</td>\n</tr>\n<tr>\n<td>TimeRange</td>\n<td>TimeRange</td>\n<td>Exact time boundaries covered by this partition</td>\n</tr>\n<tr>\n<td>ChunkReferences</td>\n<td>[]string</td>\n<td>List of chunk IDs containing data for this time range</td>\n</tr>\n<tr>\n<td>IndexSegments</td>\n<td>[]IndexSegment</td>\n<td>Individual index segments within this partition</td>\n</tr>\n<tr>\n<td>BloomFilters</td>\n<td>map[string]*BloomFilter</td>\n<td>Bloom filters keyed by chunk ID for negative lookups</td>\n</tr>\n<tr>\n<td>Statistics</td>\n<td>PartitionStats</td>\n<td>Metrics about partition size, entry count, and query performance</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th>Field in IndexSegment</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>SegmentID</td>\n<td>string</td>\n<td>Unique identifier for this index segment</td>\n</tr>\n<tr>\n<td>Terms</td>\n<td>map[string]*PostingsList</td>\n<td>Subset of the inverted index covering specific chunks</td>\n</tr>\n<tr>\n<td>CreatedAt</td>\n<td>time.Time</td>\n<td>When this segment was created, used for compaction scheduling</td>\n</tr>\n<tr>\n<td>ChunkIDs</td>\n<td>[]string</td>\n<td>Which chunks are indexed by this segment</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th>Field in PartitionStats</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>EntryCount</td>\n<td>int64</td>\n<td>Total number of log entries in this partition</td>\n</tr>\n<tr>\n<td>UniqueTerms</td>\n<td>int64</td>\n<td>Number of distinct terms across all segments</td>\n</tr>\n<tr>\n<td>DiskSize</td>\n<td>int64</td>\n<td>Total bytes used by index data on disk</td>\n</tr>\n<tr>\n<td>AvgQueryTime</td>\n<td>time.Duration</td>\n<td>Moving average of query response times</td>\n</tr>\n<tr>\n<td>LastAccessed</td>\n<td>time.Time</td>\n<td>Most recent query timestamp, used for hot/cold storage decisions</td>\n</tr>\n</tbody></table>\n<p>The partitioning strategy creates natural boundaries for index maintenance:</p>\n<ol>\n<li><strong>Hourly Partitions</strong>: Primary partitioning unit balancing granularity with overhead</li>\n<li><strong>Daily Rollups</strong>: Hourly partitions are merged into daily partitions for older data  </li>\n<li><strong>Monthly Archives</strong>: Daily partitions are further consolidated for long-term retention</li>\n<li><strong>Automatic Cleanup</strong>: Partitions beyond retention period are deleted atomically</li>\n</ol>\n<h3 id=\"storage-and-serialization-formats\">Storage and Serialization Formats</h3>\n<p>The storage layer must efficiently persist log data while supporting fast retrieval, compression, and retention management. Our storage format is designed around compressed chunks with metadata enabling selective decompression and efficient querying.</p>\n<p><img src=\"/api/project/log-aggregator/architecture-doc/asset?path=diagrams%2Fstorage-layout.svg\" alt=\"Storage Layout and Chunks\"></p>\n<h4 id=\"chunk-storage-format\">Chunk Storage Format</h4>\n<p>Chunks are the fundamental storage unit, containing a time-ordered collection of log entries with associated metadata and compression. Each chunk is designed to be independently readable and compressible.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Size</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ChunkHeader</td>\n<td>256 bytes</td>\n<td>Fixed-size header with metadata and compression info</td>\n</tr>\n<tr>\n<td>StreamHeaders</td>\n<td>Variable</td>\n<td>Metadata for each log stream contained in this chunk</td>\n</tr>\n<tr>\n<td>CompressedData</td>\n<td>Variable</td>\n<td>Compressed log entry data using specified compression algorithm</td>\n</tr>\n<tr>\n<td>IndexFooter</td>\n<td>Variable</td>\n<td>Offset table enabling random access within compressed data</td>\n</tr>\n<tr>\n<td>Checksum</td>\n<td>32 bytes</td>\n<td>SHA-256 hash of entire chunk for corruption detection</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th>Field in ChunkHeader</th>\n<th>Type</th>\n<th>Size</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Magic</td>\n<td>[4]byte</td>\n<td>4</td>\n<td>File format identifier &quot;LOGS&quot;</td>\n</tr>\n<tr>\n<td>Version</td>\n<td>uint16</td>\n<td>2</td>\n<td>Chunk format version for backward compatibility</td>\n</tr>\n<tr>\n<td>CompressionType</td>\n<td>uint8</td>\n<td>1</td>\n<td>Compression algorithm used (0=none, 1=gzip, 2=lz4, 3=zstd)</td>\n</tr>\n<tr>\n<td>StreamCount</td>\n<td>uint32</td>\n<td>4</td>\n<td>Number of log streams in this chunk</td>\n</tr>\n<tr>\n<td>EntryCount</td>\n<td>uint64</td>\n<td>8</td>\n<td>Total number of log entries across all streams</td>\n</tr>\n<tr>\n<td>UncompressedSize</td>\n<td>uint64</td>\n<td>8</td>\n<td>Size of data before compression</td>\n</tr>\n<tr>\n<td>CompressedSize</td>\n<td>uint64</td>\n<td>8</td>\n<td>Size of compressed data section</td>\n</tr>\n<tr>\n<td>TimeRange</td>\n<td>TimeRange</td>\n<td>32</td>\n<td>Earliest and latest timestamps in chunk</td>\n</tr>\n<tr>\n<td>CreatedAt</td>\n<td>time.Time</td>\n<td>16</td>\n<td>When chunk was created</td>\n</tr>\n<tr>\n<td>Reserved</td>\n<td>[169]byte</td>\n<td>169</td>\n<td>Reserved space for future extensions</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th>Field in StreamHeader</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>StreamID</td>\n<td>string</td>\n<td>Unique identifier for this log stream</td>\n</tr>\n<tr>\n<td>Labels</td>\n<td>Labels</td>\n<td>Common labels shared by all entries in this stream</td>\n</tr>\n<tr>\n<td>EntryCount</td>\n<td>uint32</td>\n<td>Number of entries for this stream within the chunk</td>\n</tr>\n<tr>\n<td>CompressedOffset</td>\n<td>uint64</td>\n<td>Byte offset where this stream&#39;s data begins in compressed section</td>\n</tr>\n<tr>\n<td>CompressedSize</td>\n<td>uint64</td>\n<td>Size of this stream&#39;s compressed data</td>\n</tr>\n</tbody></table>\n<p>The chunk format enables several important optimizations:</p>\n<ol>\n<li><strong>Selective Decompression</strong>: Streams can be decompressed individually using offset information</li>\n<li><strong>Query Filtering</strong>: Time range and label information in headers avoid unnecessary decompression  </li>\n<li><strong>Corruption Detection</strong>: Checksums ensure data integrity with fast validation</li>\n<li><strong>Format Evolution</strong>: Version field and reserved space support future enhancements</li>\n</ol>\n<blockquote>\n<p><strong>Decision: Stream-Level Compression Granularity</strong></p>\n<ul>\n<li><strong>Context</strong>: Compression could be applied at chunk level (all data together) or stream level (each stream separately)</li>\n<li><strong>Options Considered</strong>: Chunk-level compression, stream-level compression, entry-level compression</li>\n<li><strong>Decision</strong>: Stream-level compression within chunks</li>\n<li><strong>Rationale</strong>: Stream-level compression allows selective decompression for queries targeting specific label combinations, while maintaining good compression ratios since entries within a stream are highly similar</li>\n<li><strong>Consequences</strong>: Slightly more complex compression logic but much better query performance when filtering by labels</li>\n</ul>\n</blockquote>\n<h4 id=\"compression-strategy-analysis\">Compression Strategy Analysis</h4>\n<p>Different compression algorithms offer trade-offs between compression ratio, compression speed, decompression speed, and memory usage. Our system must handle high ingestion rates while maintaining fast query response times.</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th>Compression Ratio</th>\n<th>Compression Speed</th>\n<th>Decompression Speed</th>\n<th>Memory Usage</th>\n<th>Best Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>None</td>\n<td>1.0x (baseline)</td>\n<td>Instant</td>\n<td>Instant</td>\n<td>Minimal</td>\n<td>Development/debugging</td>\n</tr>\n<tr>\n<td>Gzip</td>\n<td>4-6x</td>\n<td>Slow (20 MB/s)</td>\n<td>Medium (200 MB/s)</td>\n<td>Low</td>\n<td>Cold storage, bandwidth limited</td>\n</tr>\n<tr>\n<td>LZ4</td>\n<td>2-3x</td>\n<td>Very Fast (300 MB/s)</td>\n<td>Very Fast (800 MB/s)</td>\n<td>Low</td>\n<td>Hot data, query-heavy workloads</td>\n</tr>\n<tr>\n<td>Zstandard</td>\n<td>3-5x</td>\n<td>Fast (100 MB/s)</td>\n<td>Fast (400 MB/s)</td>\n<td>Medium</td>\n<td>Balanced performance, warm storage</td>\n</tr>\n</tbody></table>\n<p>The compression choice significantly impacts system performance:</p>\n<ol>\n<li><strong>Ingestion Performance</strong>: LZ4&#39;s fast compression keeps up with high ingestion rates without buffering delays</li>\n<li><strong>Query Performance</strong>: Fast decompression enables responsive queries even on compressed historical data</li>\n<li><strong>Storage Efficiency</strong>: Better compression reduces disk usage and network transfer for distributed deployments</li>\n<li><strong>CPU Resources</strong>: Compression/decompression CPU usage must be balanced against I/O savings</li>\n</ol>\n<blockquote>\n<p><strong>Decision: Adaptive Compression Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: System could use single compression algorithm or adapt based on data age and access patterns</li>\n<li><strong>Options Considered</strong>: Fixed LZ4 everywhere, fixed Zstd everywhere, adaptive strategy based on data age</li>\n<li><strong>Decision</strong>: Adaptive compression - LZ4 for recent data, Zstd for data older than 24 hours</li>\n<li><strong>Rationale</strong>: Recent data is queried frequently and benefits from fast decompression; older data is accessed less often and benefits from better compression ratios</li>\n<li><strong>Consequences</strong>: More complex storage management but optimal performance for both ingestion and long-term storage efficiency</li>\n</ul>\n</blockquote>\n<h4 id=\"write-ahead-log-format\">Write-Ahead Log Format</h4>\n<p>The Write-Ahead Log (WAL) ensures durability by recording all operations before they are applied to the main storage. The WAL format must support fast appends, reliable recovery, and efficient cleanup.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>WAL Header</td>\n<td>Fixed header with WAL metadata and recovery information</td>\n</tr>\n<tr>\n<td>Log Records</td>\n<td>Sequential records of operations, each with its own header and data</td>\n</tr>\n<tr>\n<td>Checkpoints</td>\n<td>Periodic markers indicating safe truncation points</td>\n</tr>\n<tr>\n<td>Recovery Index</td>\n<td>Optional index for faster recovery after crashes</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th>Field in WAL Header</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Magic</td>\n<td>[4]byte</td>\n<td>WAL format identifier &quot;WLOG&quot;</td>\n</tr>\n<tr>\n<td>Version</td>\n<td>uint16</td>\n<td>WAL format version</td>\n</tr>\n<tr>\n<td>SequenceStart</td>\n<td>uint64</td>\n<td>First sequence number in this WAL file</td>\n</tr>\n<tr>\n<td>CreatedAt</td>\n<td>time.Time</td>\n<td>WAL creation timestamp</td>\n</tr>\n<tr>\n<td>NodeID</td>\n<td>string</td>\n<td>Identifier of node that created this WAL</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th>Field in LogRecord</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>RecordType</td>\n<td>uint8</td>\n<td>Type of operation (1=IngestBatch, 2=IndexUpdate, 3=Checkpoint)</td>\n</tr>\n<tr>\n<td>SequenceNumber</td>\n<td>uint64</td>\n<td>Monotonically increasing sequence number</td>\n</tr>\n<tr>\n<td>Timestamp</td>\n<td>time.Time</td>\n<td>When operation was initiated</td>\n</tr>\n<tr>\n<td>DataSize</td>\n<td>uint32</td>\n<td>Size of operation data following this header</td>\n</tr>\n<tr>\n<td>Checksum</td>\n<td>uint32</td>\n<td>CRC32 checksum of record header and data</td>\n</tr>\n<tr>\n<td>Data</td>\n<td>[]byte</td>\n<td>Serialized operation data, format depends on RecordType</td>\n</tr>\n</tbody></table>\n<p>The WAL recovery process follows a deterministic algorithm:</p>\n<ol>\n<li><strong>WAL Discovery</strong>: Scan storage directory for all WAL files, sort by sequence number ranges</li>\n<li><strong>Integrity Check</strong>: Validate checksums for all records, identify any corruption boundaries  </li>\n<li><strong>Operation Replay</strong>: Re-execute all operations after the last checkpoint in sequence order</li>\n<li><strong>State Validation</strong>: Verify final system state matches expected state from WAL operations</li>\n<li><strong>Cleanup</strong>: Truncate WAL at first checkpoint after successful state verification</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Recovery Scenario</th>\n<th>Detection Method</th>\n<th>Recovery Action</th>\n<th>Data Loss Risk</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Clean Shutdown</td>\n<td>WAL ends with checkpoint record</td>\n<td>No recovery needed</td>\n<td>None</td>\n</tr>\n<tr>\n<td>Crash During Ingestion</td>\n<td>WAL ends mid-operation</td>\n<td>Replay from last checkpoint</td>\n<td>Partial batch only</td>\n</tr>\n<tr>\n<td>WAL Corruption</td>\n<td>Checksum validation fails</td>\n<td>Replay up to corruption point</td>\n<td>Records after corruption</td>\n</tr>\n<tr>\n<td>Multiple WAL Files</td>\n<td>Sequence number gaps</td>\n<td>Replay all valid files in order</td>\n<td>Gap data lost</td>\n</tr>\n</tbody></table>\n<h4 id=\"serialization-protocols\">Serialization Protocols</h4>\n<p>All data structures must be serialized for persistence and network transmission. Our serialization strategy balances performance, compatibility, and debuggability.</p>\n<table>\n<thead>\n<tr>\n<th>Data Type</th>\n<th>Serialization Format</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>LogEntry</td>\n<td>MessagePack</td>\n<td>Compact binary format, faster than JSON, good language support</td>\n</tr>\n<tr>\n<td>Labels</td>\n<td>JSON</td>\n<td>Human-readable, debuggable, standard format</td>\n</tr>\n<tr>\n<td>Index Data</td>\n<td>Custom Binary</td>\n<td>Maximum performance for frequently accessed structures</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td>YAML</td>\n<td>Human-editable, comments supported, clear structure</td>\n</tr>\n<tr>\n<td>API Messages</td>\n<td>Protocol Buffers</td>\n<td>Schema evolution, efficient network protocol</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th>Format</th>\n<th>Serialization Speed</th>\n<th>Size Efficiency</th>\n<th>Human Readable</th>\n<th>Schema Evolution</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>JSON</td>\n<td>Medium</td>\n<td>Poor</td>\n<td>Yes</td>\n<td>Limited</td>\n</tr>\n<tr>\n<td>MessagePack</td>\n<td>Fast</td>\n<td>Good</td>\n<td>No</td>\n<td>None</td>\n</tr>\n<tr>\n<td>Protocol Buffers</td>\n<td>Fast</td>\n<td>Excellent</td>\n<td>No</td>\n<td>Excellent</td>\n</tr>\n<tr>\n<td>Custom Binary</td>\n<td>Fastest</td>\n<td>Best</td>\n<td>No</td>\n<td>Manual</td>\n</tr>\n</tbody></table>\n<p>The serialization choice affects multiple aspects of system performance:</p>\n<ol>\n<li><strong>Network Bandwidth</strong>: Efficient serialization reduces data transfer costs in distributed deployments</li>\n<li><strong>CPU Overhead</strong>: Fast serialization/deserialization improves ingestion and query throughput</li>\n<li><strong>Storage Space</strong>: Compact formats reduce disk usage, especially for metadata structures</li>\n<li><strong>Debuggability</strong>: Human-readable formats aid troubleshooting but consume more resources</li>\n<li><strong>Compatibility</strong>: Standard formats ease integration with external tools and monitoring systems</li>\n</ol>\n<blockquote>\n<p>⚠️ <strong>Pitfall: Inconsistent Timestamp Serialization</strong>\nWhen serializing timestamps, different formats handle precision and timezone information differently. Always use UTC timestamps with consistent precision (nanoseconds) and explicit timezone information. Avoid Unix timestamps which lose precision and timezone context.</p>\n</blockquote>\n<blockquote>\n<p>⚠️ <strong>Pitfall: Label Key Ordering Assumptions</strong>\nNever assume that label keys will be serialized or deserialized in any particular order. Always use stable sorting when label ordering matters for indexing or comparison operations. Map iteration order is not guaranteed in most languages.</p>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Serialization</td>\n<td>JSON with encoding/json</td>\n<td>MessagePack with vmihailenco/msgpack</td>\n</tr>\n<tr>\n<td>Time Handling</td>\n<td>time.Time with RFC3339</td>\n<td>Custom timestamp with nanosecond precision</td>\n</tr>\n<tr>\n<td>Compression</td>\n<td>gzip with compress/gzip</td>\n<td>LZ4 with pierrec/lz4 or Zstd with klauspost/compress</td>\n</tr>\n<tr>\n<td>Hashing (Bloom Filters)</td>\n<td>hash/fnv and crypto/sha256</td>\n<td>Custom hash with xxhash</td>\n</tr>\n<tr>\n<td>Storage Backend</td>\n<td>Local filesystem with os.File</td>\n<td>Pluggable interface supporting S3/GCS</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/\n  models/\n    log_entry.go              ← Core data types (LogEntry, Labels, TimeRange)\n    log_stream.go             ← LogStream and stream management\n    config.go                 ← Configuration structures and validation\n    metrics.go                ← Metrics and statistics types\n  index/\n    inverted_index.go         ← Inverted index structures and operations\n    bloom_filter.go           ← Bloom filter implementation\n    partition.go              ← Time-based partitioning metadata\n  storage/\n    chunk.go                  ← Chunk format and serialization\n    compression.go            ← Compression strategy implementation\n    wal.go                    ← Write-ahead log structures\n  serialization/\n    formats.go                ← Serialization format implementations\n    timestamps.go             ← Timestamp handling utilities</code></pre></div>\n\n<h4 id=\"core-data-types-implementation\">Core Data Types Implementation</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> models</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sort</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LogEntry represents a single log message with metadata</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LogEntry</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"timestamp\" msgpack:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Labels    </span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#9ECBFF\">    `json:\"labels\" msgpack:\"labels\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Message   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"message\" msgpack:\"message\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Labels represents key-value metadata for log entries</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Labels</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewLogEntry creates a new LogEntry with validation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewLogEntry</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">timestamp</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">labels</span><span style=\"color:#B392F0\"> Labels</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">message</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate timestamp is not zero and not too far in future</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate labels map is not nil and contains valid UTF-8</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Validate message is not empty and contains valid UTF-8</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Normalize timestamp to UTC if needed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return validated LogEntry instance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// String creates a canonical string representation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">le </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Format timestamp as RFC3339 with nanosecond precision</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Sort labels by key for consistent output</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Format as: timestamp [labels] message</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Escape any special characters in message</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Equal checks if two LogEntry instances are identical</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">le </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Equal</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">other</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Compare timestamps with nanosecond precision</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Compare message strings exactly  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Compare labels maps (order-independent)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return true only if all fields match</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Clone creates a deep copy of the LogEntry</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">le </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Clone</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create new LogEntry with same timestamp and message</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create new Labels map and copy all key-value pairs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Return the cloned instance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: This ensures immutability when passing LogEntries between goroutines</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"labels-management-utilities\">Labels Management Utilities</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// String creates a sorted string representation of labels</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Extract all keys from the map into a slice</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Sort keys alphabetically for consistent output</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Build string as key1=value1,key2=value2</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Escape any special characters in keys or values</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Hash creates a consistent hash of the labels</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Hash</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">uint64</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Sort labels by key for consistent hashing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Concatenate all key=value pairs with separator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Use a fast hash function (like xxhash or fnv)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return the hash value for indexing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: This hash is used for stream identification and partitioning</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Validate checks if labels contain valid keys and values</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Validate</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check each key is non-empty and valid UTF-8</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check each value is valid UTF-8 (empty values allowed)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Check for reserved key names (like __internal__)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Check total serialized size doesn't exceed limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return error describing any validation failures</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Merge creates a new Labels map combining this and other</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Merge</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">other</span><span style=\"color:#B392F0\"> Labels</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create new Labels map with capacity for both</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Copy all key-value pairs from current labels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Copy all key-value pairs from other labels (overwrites conflicts)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return the merged labels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Other labels take precedence in case of key conflicts</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"timerange-operations\">TimeRange Operations</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> models</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TimeRange represents a time interval with half-open semantics [Start, End)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TimeRange</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Start </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"start\" msgpack:\"start\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    End   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"end\" msgpack:\"end\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewTimeRange creates a validated TimeRange</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewTimeRange</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">start</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">end</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TimeRange</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Ensure start is before end</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Normalize both timestamps to UTC</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Validate timestamps are not zero values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return TimeRange or validation error</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Contains checks if a timestamp falls within this range</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tr </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TimeRange</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Contains</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">timestamp</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check timestamp >= Start (inclusive)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check timestamp &#x3C; End (exclusive)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Return true only if both conditions met</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Half-open interval prevents double-counting at boundaries</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Overlaps checks if this range overlaps with another</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tr </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TimeRange</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Overlaps</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">other</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">TimeRange</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check if other.Start &#x3C; tr.End (other starts before this ends)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check if tr.Start &#x3C; other.End (this starts before other ends)  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Return true only if both conditions met</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Duration returns the length of this time range</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tr </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TimeRange</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Calculate End - Start</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Handle case where End &#x3C;= Start (return 0)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>After implementing core data types:</strong></p>\n<ul>\n<li>Run <code>go test ./internal/models/...</code> - all tests should pass</li>\n<li>Create a LogEntry with labels and verify it serializes to JSON correctly</li>\n<li>Test Labels.Hash() produces consistent results for same label sets</li>\n<li>Verify TimeRange.Contains() works correctly with boundary conditions</li>\n</ul>\n<p><strong>What should work:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Create test log entry</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">entry</span><span style=\"color:#9ECBFF\"> :=</span><span style=\"color:#E1E4E8\"> &#x26;</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#9ECBFF\">{</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Timestamp:</span><span style=\"color:#9ECBFF\"> time.Now</span><span style=\"color:#E1E4E8\">()</span><span style=\"color:#9ECBFF\">,</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Labels:</span><span style=\"color:#9ECBFF\"> Labels{\"service\":</span><span style=\"color:#9ECBFF\"> \"api\",</span><span style=\"color:#9ECBFF\"> \"level\":</span><span style=\"color:#9ECBFF\"> \"error\"},</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Message:</span><span style=\"color:#9ECBFF\"> \"Request timeout\",</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should serialize/deserialize without data loss</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">data,</span><span style=\"color:#9ECBFF\"> _</span><span style=\"color:#9ECBFF\"> :=</span><span style=\"color:#9ECBFF\"> json.Marshal</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">entry</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">var</span><span style=\"color:#9ECBFF\"> decoded</span><span style=\"color:#9ECBFF\"> LogEntry</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">json.Unmarshal(data,</span><span style=\"color:#E1E4E8\"> &#x26;</span><span style=\"color:#B392F0\">decoded</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">assert.Equal(t,</span><span style=\"color:#9ECBFF\"> entry,</span><span style=\"color:#E1E4E8\"> &#x26;</span><span style=\"color:#B392F0\">decoded</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Common debugging issues:</strong></p>\n<ul>\n<li>Timestamp precision loss during JSON serialization - use RFC3339Nano format</li>\n<li>Labels map iteration order causing hash inconsistencies - always sort keys</li>\n<li>Memory leaks from not cloning Labels when sharing between goroutines</li>\n<li>TimeRange boundary errors from using closed intervals instead of half-open</li>\n</ul>\n<h2 id=\"log-ingestion-engine\">Log Ingestion Engine</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section corresponds to Milestone 1 (Log Ingestion), covering the foundational capability to receive, parse, buffer, and route log data from multiple sources into the aggregation system.</p>\n</blockquote>\n<h3 id=\"mental-model-the-mail-sorting-facility\">Mental Model: The Mail Sorting Facility</h3>\n<p>Think of the log ingestion engine as a large mail sorting facility at a major postal hub. Just as the postal facility receives mail through multiple channels—trucks arriving at loading docks, mail drops from local routes, and direct deliveries—our log ingestion engine receives log data through multiple protocols: HTTP endpoints, TCP connections, UDP packets, and file monitoring agents.</p>\n<p>The postal facility has different processing workflows for different types of mail. Priority mail gets expedited handling, while bulk mail goes through standard processing. Similarly, our ingestion engine has different parsing pipelines for different log formats: structured JSON logs get fast-path processing, while unstructured syslog messages require more complex parsing to extract meaningful fields.</p>\n<p>The facility uses staging areas and conveyor belts to handle varying volumes of incoming mail without dropping packages, even during peak holiday seasons. Our ingestion engine employs memory buffers and disk-based queues to handle burst traffic and maintain durability when downstream components are temporarily unavailable. Just as the postal facility can temporarily store mail in warehouses during processing delays, our system buffers logs to disk when the indexing or storage engines fall behind.</p>\n<p>The sorting facility routes mail based on addresses and delivery requirements, ensuring each piece reaches its correct destination. Our ingestion engine routes parsed log entries to appropriate storage partitions based on labels, timestamps, and retention policies. Both systems must handle malformed inputs gracefully—unreadable addresses in mail, unparseable log messages in our system—without disrupting the processing of valid entries.</p>\n<p>This mental model helps us understand that ingestion is fundamentally about <strong>reliable reception, intelligent parsing, temporary buffering, and accurate routing</strong> of high-volume, heterogeneous data streams.</p>\n<p><img src=\"/api/project/log-aggregator/architecture-doc/asset?path=diagrams%2Fingestion-flow.svg\" alt=\"Log Ingestion Sequence\"></p>\n<h3 id=\"protocol-handlers\">Protocol Handlers</h3>\n<p>The ingestion engine supports multiple protocols to accommodate different log sources and operational requirements. Each protocol serves specific use cases and comes with distinct trade-offs in terms of reliability, performance, and operational complexity.</p>\n<h4 id=\"http-log-receiver\">HTTP Log Receiver</h4>\n<p>The HTTP endpoint provides the most flexible and widely supported ingestion method. Web applications, microservices, and cloud-native workloads naturally emit logs via HTTP APIs. The receiver accepts JSON-formatted log entries via POST requests to <code>/api/v1/logs</code>, providing immediate response codes that indicate successful ingestion or specific error conditions.</p>\n<p>HTTP offers several advantages for log ingestion. Applications can batch multiple log entries in a single request, reducing network overhead. The protocol&#39;s request-response nature provides immediate feedback about ingestion success or failure, enabling applications to implement retry logic or failover to alternative log destinations. HTTP&#39;s stateless nature simplifies load balancing across multiple ingestion endpoints.</p>\n<p>However, HTTP introduces higher per-message overhead compared to streaming protocols. Each log entry carries HTTP headers, and the request-response cycle adds latency. For high-frequency logging from latency-sensitive applications, this overhead can become significant.</p>\n<p>The HTTP handler implements several reliability mechanisms. It validates Content-Type headers to ensure JSON payloads, performs request size limits to prevent memory exhaustion, and returns structured error responses that help clients diagnose problems. The handler supports both single log entries and arrays of entries, with atomic processing—either all entries in a batch succeed or all fail together.</p>\n<h4 id=\"tcp-syslog-receiver\">TCP Syslog Receiver</h4>\n<p>The TCP receiver handles RFC 5424 and RFC 3164 syslog messages, providing compatibility with traditional Unix systems, network equipment, and legacy applications. TCP&#39;s connection-oriented nature offers reliable delivery semantics that UDP lacks, making it suitable for critical log data where loss is unacceptable.</p>\n<p>TCP syslog connections maintain state between client and server, enabling the receiver to detect client disconnections and clean up resources promptly. The protocol handles partial message transmission gracefully—syslog messages can be fragmented across multiple TCP segments, and the receiver reconstructs complete messages before parsing.</p>\n<p>The TCP handler implements connection pooling to support many concurrent log sources efficiently. Each connection runs in a separate goroutine, reading messages in a loop until the client disconnects or an error occurs. The handler implements read timeouts to detect stalled connections and prevent resource leaks from clients that connect but never send data.</p>\n<p>TCP syslog parsing must handle the complexity of two different RFC formats. RFC 3164 uses a simpler format with implicit assumptions about timestamp and hostname formatting, while RFC 5424 provides structured data fields and explicit encoding. The parser detects the format based on message structure and applies appropriate parsing rules.</p>\n<h4 id=\"udp-syslog-receiver\">UDP Syslog Receiver</h4>\n<p>The UDP receiver offers the lowest latency and highest throughput option for log ingestion, making it ideal for high-frequency logging from network devices, embedded systems, and performance-critical applications. UDP&#39;s fire-and-forget semantics eliminate connection management overhead and reduce memory footprint on both client and server sides.</p>\n<p>UDP excels in environments where losing occasional log messages is acceptable in exchange for minimal performance impact on the log-generating application. Network equipment often prefers UDP syslog because maintaining TCP connections can strain limited embedded system resources.</p>\n<p>The UDP handler manages packet reception through a single socket with a large receive buffer, preventing packet drops during traffic bursts. Unlike TCP, UDP messages arrive as complete, independent packets—there&#39;s no message fragmentation to handle. This simplifies the parsing logic but requires careful validation since malformed messages can&#39;t be recovered through retransmission.</p>\n<p>The receiver implements several strategies to maximize UDP packet reception rates. It uses multiple worker goroutines reading from the same socket, distributing packet processing load across CPU cores. The large socket receive buffer accommodates temporary processing delays without dropping packets at the kernel level.</p>\n<h4 id=\"file-tail-agent\">File Tail Agent</h4>\n<p>The file monitoring component watches log files for new content and forwards entries to the ingestion pipeline, providing compatibility with applications that write logs directly to files rather than sending them over the network. This approach is common in traditional server environments and containerized applications that mount log directories as volumes.</p>\n<p>File tailing presents unique challenges compared to network protocols. The agent must detect file rotations, handle temporary file unavailability during log rotation, and maintain position tracking to avoid reprocessing entries after restarts. Modern log rotation tools create complex scenarios: files may be moved, compressed, or deleted while new files appear with the same names.</p>\n<p>The file tail implementation uses filesystem notification APIs (inotify on Linux) to detect file changes efficiently without continuous polling. When files change, the agent reads new content incrementally, parsing complete lines and forwarding them to the ingestion pipeline with synthetic metadata like file path and modification timestamp.</p>\n<p>Position tracking ensures exactly-once processing across agent restarts. The agent periodically checkpoints its read position for each monitored file, storing offsets in a persistent state file. After restarts, the agent resumes reading from the last checkpointed position, avoiding duplicate processing of previously forwarded log entries.</p>\n<p>The following table compares the trade-offs between different protocol handlers:</p>\n<table>\n<thead>\n<tr>\n<th>Protocol</th>\n<th>Throughput</th>\n<th>Reliability</th>\n<th>Latency</th>\n<th>Resource Usage</th>\n<th>Operational Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP</td>\n<td>Medium</td>\n<td>High</td>\n<td>Medium</td>\n<td>Medium</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>TCP Syslog</td>\n<td>High</td>\n<td>High</td>\n<td>Low</td>\n<td>Medium</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>UDP Syslog</td>\n<td>Very High</td>\n<td>Medium</td>\n<td>Very Low</td>\n<td>Low</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>File Tail</td>\n<td>Medium</td>\n<td>Very High</td>\n<td>High</td>\n<td>Low</td>\n<td>High</td>\n</tr>\n</tbody></table>\n<h3 id=\"log-parsing-pipeline\">Log Parsing Pipeline</h3>\n<p>The parsing pipeline transforms raw log data from various formats into the standardized <code>LogEntry</code> structure used throughout the system. This transformation process must handle format diversity, extract structured fields, normalize timestamps, and validate label correctness while maintaining high throughput and reliability.</p>\n<h4 id=\"json-log-parsing\">JSON Log Parsing</h4>\n<p>JSON represents the most straightforward parsing path since the format is self-describing and maps naturally to structured data. Applications sending JSON logs typically provide explicit field mappings for timestamp, message content, severity level, and custom labels.</p>\n<p>The JSON parser expects a specific schema for optimal processing, though it handles variations gracefully. The preferred format includes top-level fields for <code>timestamp</code>, <code>message</code>, <code>level</code>, and <code>labels</code>, with the labels field containing key-value pairs that become the log entry&#39;s label set. Additional fields are preserved as structured content within the message or converted to labels based on configuration rules.</p>\n<p>Timestamp parsing requires special attention since JSON doesn&#39;t mandate a specific timestamp format. The parser attempts multiple formats in priority order: ISO 8601 with timezone, Unix timestamps (both seconds and milliseconds), and common application-specific formats. When timestamp parsing fails, the parser falls back to the ingestion time, but flags the entry for potential reordering issues.</p>\n<p>Label extraction from JSON follows configurable rules that determine which top-level fields become labels versus message content. Fields like <code>service</code>, <code>host</code>, <code>environment</code>, and <code>level</code> typically become labels for efficient querying, while application-specific data remains in the message field. The parser validates label keys and values against naming conventions, rejecting entries with invalid characters or excessive label counts that could cause cardinality explosion.</p>\n<h4 id=\"syslog-message-parsing\">Syslog Message Parsing</h4>\n<p>Syslog parsing handles two distinct formats with different complexity levels. RFC 3164 (traditional syslog) uses a fixed format with implicit field positions, while RFC 5424 (structured syslog) provides explicit field delimiters and optional structured data.</p>\n<p>RFC 3164 parsing extracts facility and severity from the priority field, timestamp from a fixed position (though format varies by sender), hostname from the next space-delimited token, and treats the remainder as the message content. The parser must handle timestamp format variations since different Unix systems format dates differently, particularly around timezone representation.</p>\n<p>RFC 5424 parsing follows a more structured approach with explicit field separators. The format includes version numbers, structured data fields, and standardized timestamp formats. The structured data section contains key-value pairs that the parser extracts as labels, providing richer metadata than traditional syslog.</p>\n<p>Both formats require careful handling of priority field calculation. The priority combines facility (message source type) and severity (importance level) in a single integer value. The parser extracts both components using mathematical operations: facility = priority / 8, severity = priority % 8. These values become labels that enable facility-based and severity-based log filtering.</p>\n<p>Hostname extraction presents challenges since syslog senders may provide IP addresses, short hostnames, or fully qualified domain names. The parser normalizes hostname representations to support consistent querying while preserving the original value as a separate label for troubleshooting.</p>\n<h4 id=\"regex-based-pattern-extraction\">Regex-Based Pattern Extraction</h4>\n<p>For unstructured log formats, the parsing pipeline supports configurable regular expressions that extract structured fields from free-form text. This capability handles legacy applications, proprietary log formats, and complex multi-line log entries that don&#39;t conform to standard formats.</p>\n<p>Regex patterns use named capture groups to identify fields for extraction. A web server access log pattern might capture IP addresses, timestamps, HTTP methods, URLs, status codes, and response sizes as separate fields that become labels or structured message content. The parser compiles configured patterns at startup for optimal runtime performance.</p>\n<p>Pattern matching follows a priority order when multiple patterns could apply to a single log line. The parser attempts patterns from most specific to most general, using the first successful match. When no patterns match, the entire line becomes the message content with minimal metadata extracted from the ingestion context.</p>\n<p>Multi-line log handling requires stateful parsing that accumulates related lines into single log entries. Java stack traces, SQL query logs, and application debug output often span multiple lines that should be treated as atomic units. The parser uses continuation patterns that identify line relationships and buffer partial entries until complete patterns emerge.</p>\n<p>Performance optimization becomes critical for regex parsing since complex patterns can consume significant CPU resources. The parser implements pattern caching, compiled regex reuse, and timeout mechanisms that prevent pathological backtracking from blocking the ingestion pipeline.</p>\n<p>The following table describes the key components of each parsing pipeline:</p>\n<table>\n<thead>\n<tr>\n<th>Parser Type</th>\n<th>Input Format</th>\n<th>Extracted Fields</th>\n<th>Performance</th>\n<th>Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>JSON</td>\n<td>Structured JSON objects</td>\n<td>All fields available</td>\n<td>High</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>RFC 5424 Syslog</td>\n<td><code>&lt;priority&gt;version timestamp hostname app-name proc-id msg-id [structured-data] message</code></td>\n<td>Priority, timestamp, hostname, app-name, structured data</td>\n<td>High</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>RFC 3164 Syslog</td>\n<td><code>&lt;priority&gt;timestamp hostname tag: message</code></td>\n<td>Priority, timestamp, hostname, tag</td>\n<td>High</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Regex Patterns</td>\n<td>Custom text formats</td>\n<td>Named capture groups</td>\n<td>Medium</td>\n<td>High</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Architecture Decision: Streaming vs. Batch Parsing</strong></p>\n<ul>\n<li><strong>Context</strong>: Log parsing can process entries individually as they arrive or collect entries into batches for group processing</li>\n<li><strong>Options Considered</strong>: <ul>\n<li>Streaming: Parse each log entry immediately upon receipt</li>\n<li>Micro-batching: Collect 10-100 entries before parsing</li>\n<li>Large batching: Accumulate entries for seconds before parsing</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Implement streaming parsing with optional micro-batching for specific sources</li>\n<li><strong>Rationale</strong>: Streaming provides the lowest latency for log queries and alerting, which is crucial for operational visibility. Micro-batching can be enabled for high-volume sources where parsing CPU becomes a bottleneck, but most sources benefit from immediate processing.</li>\n<li><strong>Consequences</strong>: Higher CPU usage due to frequent parsing calls, but significantly better query freshness and alerting response times</li>\n</ul>\n</blockquote>\n<h3 id=\"buffering-and-backpressure\">Buffering and Backpressure</h3>\n<p>The ingestion engine implements sophisticated buffering strategies to handle traffic bursts, downstream outages, and varying processing speeds across pipeline stages. Effective buffering ensures no log loss during temporary overload conditions while maintaining reasonable memory usage and providing clear backpressure signals to upstream sources.</p>\n<h4 id=\"memory-buffer-management\">Memory Buffer Management</h4>\n<p>The primary buffering layer uses in-memory circular buffers that provide fast insertion and removal operations for normal traffic patterns. Each protocol handler feeds parsed log entries into dedicated memory buffers sized according to expected traffic volume and downstream processing capacity.</p>\n<p>Memory buffers operate as ring buffers with separate read and write positions. Writers advance the write position after inserting entries, while readers advance the read position after processing entries. When the write position approaches the read position, the buffer is near capacity and triggers backpressure mechanisms before dropping data.</p>\n<p>Buffer sizing calculations consider several factors: expected peak ingestion rate, downstream processing capacity, and acceptable memory usage limits. A buffer sized for 10,000 entries with an average entry size of 1KB consumes approximately 10MB of memory. Production deployments typically configure buffers to handle 30-60 seconds of peak traffic, allowing time for temporary downstream slowdowns to resolve.</p>\n<p>The memory buffer implements atomic operations for thread-safe access from multiple producers and consumers. Protocol handlers write entries concurrently while the indexing engine reads entries for processing. Lock-free ring buffer algorithms avoid mutex contention that could limit ingestion throughput.</p>\n<p>Memory buffer monitoring tracks utilization levels and triggers increasingly aggressive backpressure as capacity approaches. At 70% utilization, the buffer begins logging warnings. At 85% utilization, it starts rejecting new HTTP requests with 503 status codes. At 95% utilization, it begins dropping UDP packets and disconnecting idle TCP connections to preserve capacity for critical traffic.</p>\n<h4 id=\"disk-based-overflow-queues\">Disk-Based Overflow Queues</h4>\n<p>When memory buffers reach capacity, the ingestion engine spills entries to disk-based queues that provide virtually unlimited capacity at the cost of increased latency and I/O overhead. Disk queues ensure no log loss during extended downstream outages or traffic spikes that exceed memory buffer capacity.</p>\n<p>The disk queue implementation uses append-only files with simple binary encoding for fast write operations. Each queue entry contains a length prefix, timestamp, serialized labels, and message content. Sequential writes to append-only files provide optimal disk I/O performance while maintaining data durability.</p>\n<p>Queue files rotate based on size limits (typically 64MB-256MB) to prevent individual files from becoming unwieldy. The queue manager maintains metadata about active write files, read positions within files, and files eligible for deletion after processing completion. File rotation ensures that disk space consumption remains bounded even during extended queueing periods.</p>\n<p>Disk queue reading uses sequential I/O patterns that leverage operating system read-ahead caching effectively. The queue reader maintains persistent checkpoint information that records processing positions within files, enabling recovery to the correct position after restart without reprocessing completed entries.</p>\n<p>Write-ahead logging principles ensure queue durability during system failures. Queue writes use <code>fsync()</code> operations to guarantee data reaches persistent storage before acknowledging successful buffering. This durability comes at a performance cost, so disk queuing only activates when memory buffers overflow.</p>\n<h4 id=\"backpressure-propagation\">Backpressure Propagation</h4>\n<p>Effective backpressure mechanisms communicate capacity constraints to log sources before data loss occurs, enabling upstream systems to implement appropriate flow control responses. Different protocols support different backpressure signals, requiring protocol-specific implementations.</p>\n<p>HTTP backpressure uses standard status codes to communicate capacity constraints. When buffers approach capacity, the HTTP handler returns 503 Service Unavailable responses with Retry-After headers suggesting appropriate wait times. Well-behaved HTTP clients implement exponential backoff retry logic that reduces load automatically during capacity constraints.</p>\n<p>TCP backpressure leverages the protocol&#39;s built-in flow control mechanisms. When processing falls behind, the TCP receiver stops reading from connection sockets, causing TCP window sizes to shrink and ultimately blocking senders at the socket level. This provides automatic backpressure without requiring application-level protocol changes.</p>\n<p>UDP backpressure cannot rely on protocol-level mechanisms since UDP provides no delivery guarantees or flow control. The UDP receiver monitors processing queue depths and begins dropping packets when capacity thresholds are exceeded. Packet dropping follows priority rules that preserve higher-priority log entries when possible.</p>\n<p>File tail backpressure pauses file reading when downstream capacity is constrained. The file monitor stops advancing read positions and relies on operating system filesystem buffers to preserve new log data until processing capacity recovers. This approach prevents memory exhaustion while ensuring no log data loss.</p>\n<p>The following table details backpressure mechanisms for each protocol:</p>\n<table>\n<thead>\n<tr>\n<th>Protocol</th>\n<th>Backpressure Method</th>\n<th>Response Time</th>\n<th>Upstream Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP</td>\n<td>503 status codes with Retry-After</td>\n<td>Immediate</td>\n<td>Client retry with backoff</td>\n</tr>\n<tr>\n<td>TCP</td>\n<td>Socket read blocking</td>\n<td>Automatic</td>\n<td>Sender blocking at socket level</td>\n</tr>\n<tr>\n<td>UDP</td>\n<td>Packet dropping</td>\n<td>Immediate</td>\n<td>No automatic response - relies on sender logic</td>\n</tr>\n<tr>\n<td>File Tail</td>\n<td>Read position pause</td>\n<td>Delayed</td>\n<td>File system buffering until processing resumes</td>\n</tr>\n</tbody></table>\n<h4 id=\"buffer-health-monitoring\">Buffer Health Monitoring</h4>\n<p>Comprehensive monitoring of buffer states enables proactive capacity management and early detection of downstream bottlenecks. The ingestion engine exposes detailed metrics about buffer utilization, processing rates, and backpressure activation frequency.</p>\n<p>Buffer utilization metrics track current entry counts, percentage capacity usage, and rate of change in buffer levels. Rising buffer levels indicate that ingestion rates exceed processing rates, suggesting either traffic increases or downstream performance degradation. Declining buffer levels after periods of high utilization indicate that processing has caught up with ingestion.</p>\n<p>Processing rate metrics measure entries per second flowing through each pipeline stage: parsing, buffer insertion, buffer extraction, and downstream forwarding. Rate comparisons identify pipeline bottlenecks and help size buffer capacities appropriately. Sustained rate imbalances indicate architectural issues requiring investigation.</p>\n<p>Backpressure activation metrics count the frequency and duration of capacity-constrained operations: HTTP 503 responses, TCP connection rejections, UDP packet drops, and file tail read pauses. High backpressure frequencies suggest systematic under-provisioning rather than temporary traffic spikes.</p>\n<p>Queue depth monitoring tracks both memory buffer utilization and disk queue accumulation. Disk queue growth indicates serious capacity constraints that require immediate attention to prevent operational impact. Queue age metrics measure how long entries remain buffered before processing, providing insight into end-to-end log processing latency.</p>\n<blockquote>\n<p><strong>Design Insight: Why Separate Memory and Disk Buffers</strong></p>\n<p>The two-tier buffering approach optimizes for the common case (normal traffic patterns) while providing safety for the exceptional case (traffic spikes or downstream outages). Memory buffers provide sub-millisecond latency for typical operations, while disk buffers ensure durability during extended problems. Alternative single-tier approaches either sacrifice performance (always use disk) or reliability (only use memory).</p>\n</blockquote>\n<h3 id=\"architecture-decision-records\">Architecture Decision Records</h3>\n<p>Several critical design decisions shape the ingestion engine architecture. Each decision involves trade-offs between performance, reliability, operational complexity, and compatibility requirements.</p>\n<blockquote>\n<p><strong>Decision: Multi-Protocol Support vs. Single Protocol</strong></p>\n<ul>\n<li><strong>Context</strong>: Log sources use different protocols based on their runtime environment, operational requirements, and historical conventions</li>\n<li><strong>Options Considered</strong>: <ul>\n<li>HTTP-only ingestion with protocol translation proxies</li>\n<li>Native support for HTTP, TCP, UDP, and file monitoring</li>\n<li>Plugin architecture for extensible protocol support</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Native support for four core protocols (HTTP, TCP syslog, UDP syslog, file tail)</li>\n<li><strong>Rationale</strong>: Native support provides optimal performance and eliminates proxy deployment complexity. The four chosen protocols cover 95% of log source requirements in typical environments. Plugin architecture adds significant complexity without clear benefits for the core use cases.</li>\n<li><strong>Consequences</strong>: Higher code complexity due to multiple protocol handlers, but better performance and operational simplicity. Limited extensibility for unusual protocols, but this can be addressed in future versions if needed.</li>\n</ul>\n</blockquote>\n<p>The following table compares the protocol approach options:</p>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>Performance</th>\n<th>Compatibility</th>\n<th>Complexity</th>\n<th>Extensibility</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP-only + Proxies</td>\n<td>Medium</td>\n<td>Medium</td>\n<td>Low</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Native Multi-Protocol</td>\n<td>High</td>\n<td>High</td>\n<td>High</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Plugin Architecture</td>\n<td>Medium</td>\n<td>High</td>\n<td>Very High</td>\n<td>Very High</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Streaming vs. Batch Processing</strong></p>\n<ul>\n<li><strong>Context</strong>: Log entries can be processed individually or accumulated into batches for group operations</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Pure streaming: Process each entry immediately</li>\n<li>Micro-batching: Accumulate 10-100 entries before processing</li>\n<li>Time-windowed batching: Process entries every 1-5 seconds</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Streaming processing with optional micro-batching configuration</li>\n<li><strong>Rationale</strong>: Streaming provides optimal latency for alerting and real-time queries. Optional batching allows performance tuning for high-volume sources without sacrificing latency for typical workloads.</li>\n<li><strong>Consequences</strong>: Higher CPU overhead due to frequent processing calls, but significantly better query freshness. More complex configuration due to batching options, but better adaptability to diverse workloads.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Push vs. Pull Ingestion Model</strong></p>\n<ul>\n<li><strong>Context</strong>: Log data can be sent to the aggregation system (push) or retrieved by the system (pull)</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Push-only: Sources send logs to ingestion endpoints</li>\n<li>Pull-only: Ingestion system queries sources for new logs</li>\n<li>Hybrid: Support both push and pull based on source capabilities</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Primarily push-based with file tail as the only pull mechanism</li>\n<li><strong>Rationale</strong>: Push provides lower latency and better scalability since sources control timing. Pull requires complex scheduling and state management for many sources. File tail is inherently pull-based due to filesystem semantics.</li>\n<li><strong>Consequences</strong>: Better performance and simpler architecture, but requires sources to implement retry logic and handle ingestion endpoint failures.</li>\n</ul>\n</blockquote>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>Log ingestion implementation presents several subtle challenges that can cause data loss, performance degradation, or operational difficulties. Understanding these pitfalls helps avoid common mistakes during development and deployment.</p>\n<p>⚠️ <strong>Pitfall: Unbounded Memory Growth from Buffer Overflow</strong></p>\n<p>When downstream processing slows down but memory buffers continue accepting entries, memory usage can grow without bounds until the system runs out of RAM and crashes. This typically happens when the indexing engine falls behind during traffic spikes or when storage systems experience temporary outages.</p>\n<p>The problem occurs because many naive buffer implementations use dynamic arrays or linked lists that grow automatically as new entries arrive. Without explicit size limits and backpressure mechanisms, these buffers consume all available memory before triggering any protective measures.</p>\n<p><strong>Fix</strong>: Implement fixed-size ring buffers with explicit capacity limits and backpressure activation at configurable utilization thresholds. Monitor buffer utilization continuously and activate disk spillover or upstream backpressure before memory exhaustion occurs. Set process memory limits using container resources or systemd settings to prevent system-wide impact from memory leaks.</p>\n<p>⚠️ <strong>Pitfall: Timestamp Handling Causing Query Ordering Issues</strong></p>\n<p>Incorrect timestamp parsing and normalization can cause log entries to appear out of chronological order in query results, making troubleshooting and correlation extremely difficult. This commonly occurs when mixing log sources with different timestamp formats, timezone assumptions, or clock synchronization issues.</p>\n<p>The problem manifests when some entries use local timestamps while others use UTC, when millisecond precision is lost during parsing, or when the ingestion system substitutes current time for unparseable timestamps without clear indication. Query results show events happening in impossible orders, breaking troubleshooting workflows.</p>\n<p><strong>Fix</strong>: Implement comprehensive timestamp normalization that converts all timestamps to UTC with consistent precision (typically milliseconds). When timestamp parsing fails, preserve the original timestamp string as metadata while using ingestion time for ordering. Log timezone conversion decisions and timestamp parsing failures for debugging. Validate clock synchronization across log sources during system deployment.</p>\n<p>⚠️ <strong>Pitfall: Label Cardinality Explosion from Unvalidated Input</strong></p>\n<p>Allowing unlimited unique values in log labels can cause index sizes to grow exponentially, eventually consuming all disk space and making queries extremely slow. This happens when applications accidentally include unique identifiers like request IDs, user IDs, or timestamps as label values instead of keeping them in message content.</p>\n<p>The problem typically emerges gradually as applications add new logging and unique label values accumulate over time. Index sizes grow from megabytes to gigabytes, query performance degrades significantly, and disk usage becomes unpredictable. The issue becomes critical when label combinations reach millions of unique values.</p>\n<p><strong>Fix</strong>: Implement strict label validation that rejects entries with excessive unique label values or suspicious label patterns (UUIDs, timestamps, sequential numbers). Configure per-label cardinality limits and monitor label value distributions continuously. Provide clear documentation about appropriate label usage patterns versus message content. Consider implementing label value normalization for high-cardinality fields like user agents or URLs.</p>\n<p>⚠️ <strong>Pitfall: TCP Connection Resource Leaks</strong></p>\n<p>TCP syslog receivers that don&#39;t properly manage connection lifecycles can accumulate thousands of idle connections, eventually hitting operating system file descriptor limits and preventing new connections. This commonly occurs when clients disconnect abruptly or when connection timeout handling is incorrect.</p>\n<p>The problem develops over time as connection counts slowly increase. Initially, performance remains acceptable, but eventually new connections start failing with &quot;too many open files&quot; errors. The issue often appears during traffic spikes when many clients connect simultaneously.</p>\n<p><strong>Fix</strong>: Implement proper connection lifecycle management with read timeouts, idle connection detection, and explicit resource cleanup. Use connection pooling with configurable limits on concurrent connections per client IP. Monitor active connection counts and set appropriate operating system limits for file descriptors. Implement graceful connection draining during shutdown.</p>\n<p>⚠️ <strong>Pitfall: UDP Packet Loss Due to Insufficient Socket Buffers</strong></p>\n<p>UDP syslog receivers with default socket buffer sizes often drop packets during traffic bursts without any indication of data loss. The kernel discards packets when the receive buffer fills up, but applications don&#39;t receive notification about dropped packets.</p>\n<p>This issue is particularly problematic because UDP provides no delivery guarantees, so packet loss appears as missing log entries without error messages. During troubleshooting, missing logs can mask important information about system behavior.</p>\n<p><strong>Fix</strong>: Configure UDP socket receive buffers to handle expected burst traffic patterns. Use <code>SO_RCVBUF</code> socket options to increase buffer sizes from default (typically 64KB) to several megabytes. Monitor socket buffer utilization using system metrics and increase buffer sizes when drops occur. Consider multiple UDP receivers with load balancing for very high throughput requirements.</p>\n<p>⚠️ <strong>Pitfall: Incomplete WAL Recovery Leading to Data Loss</strong></p>\n<p>Write-ahead log implementations that don&#39;t properly handle partial writes or corrupted entries can lose data during recovery after crashes. This often occurs when the system crashes during WAL writes, leaving incomplete records that the recovery process cannot parse correctly.</p>\n<p>The problem appears as missing log entries after system restarts, particularly affecting the most recent entries before crashes. In some cases, WAL corruption can prevent the system from starting at all if recovery logic doesn&#39;t handle malformed records gracefully.</p>\n<p><strong>Fix</strong>: Implement robust WAL record formatting with checksums and length prefixes that enable recovery to skip corrupted records safely. Use atomic write operations with proper <code>fsync()</code> calls to ensure record durability before acknowledging successful ingestion. Test recovery logic extensively with simulated crashes at various points during write operations.</p>\n<p>The following table summarizes common symptoms and their diagnostic approaches:</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnostic Steps</th>\n<th>Resolution</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Memory usage grows continuously</td>\n<td>Unbounded buffer growth</td>\n<td>Monitor buffer sizes and downstream processing rates</td>\n<td>Implement backpressure and buffer size limits</td>\n</tr>\n<tr>\n<td>Log entries appear out of order</td>\n<td>Timestamp parsing issues</td>\n<td>Check timezone handling and precision loss</td>\n<td>Normalize all timestamps to UTC with consistent precision</td>\n</tr>\n<tr>\n<td>Index size grows unexpectedly</td>\n<td>Label cardinality explosion</td>\n<td>Analyze unique label value counts</td>\n<td>Implement label validation and cardinality limits</td>\n</tr>\n<tr>\n<td>Connection failures after hours of operation</td>\n<td>TCP resource leaks</td>\n<td>Monitor open file descriptors and connection counts</td>\n<td>Add connection timeouts and resource cleanup</td>\n</tr>\n<tr>\n<td>Missing log entries with no error messages</td>\n<td>UDP packet drops</td>\n<td>Check kernel socket buffer statistics</td>\n<td>Increase socket buffer sizes and monitor utilization</td>\n</tr>\n<tr>\n<td>Data loss after system restarts</td>\n<td>WAL recovery failures</td>\n<td>Examine WAL contents and recovery logs</td>\n<td>Implement robust record formatting with checksums</td>\n</tr>\n</tbody></table>\n<p><img src=\"/api/project/log-aggregator/architecture-doc/asset?path=diagrams%2Fingestion-states.svg\" alt=\"Ingestion Pipeline State Machine\"></p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides practical guidance for implementing the log ingestion engine, including technology recommendations, file organization, and complete starter code for infrastructure components.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Server</td>\n<td>net/http with gorilla/mux routing</td>\n<td>Fiber or Gin web framework</td>\n</tr>\n<tr>\n<td>TCP/UDP Handling</td>\n<td>Standard net package</td>\n<td>Custom connection pooling</td>\n</tr>\n<tr>\n<td>JSON Parsing</td>\n<td>encoding/json</td>\n<td>jsoniter for high performance</td>\n</tr>\n<tr>\n<td>Syslog Parsing</td>\n<td>Custom regex parsing</td>\n<td>go-syslog library</td>\n</tr>\n<tr>\n<td>File Monitoring</td>\n<td>fsnotify package</td>\n<td>Custom inotify wrapper</td>\n</tr>\n<tr>\n<td>Buffer Implementation</td>\n<td>Channel-based queues</td>\n<td>Lock-free ring buffers</td>\n</tr>\n<tr>\n<td>Disk Persistence</td>\n<td>Standard file I/O</td>\n<td>Memory-mapped files</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td>Standard flag package</td>\n<td>Viper configuration library</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>cmd/\n  logaggregator/\n    main.go                          ← Application entry point\ninternal/\n  ingestion/\n    server.go                        ← HTTP ingestion server\n    server_test.go\n    tcp_handler.go                   ← TCP syslog receiver\n    tcp_handler_test.go\n    udp_handler.go                   ← UDP syslog receiver  \n    udp_handler_test.go\n    file_tailer.go                   ← File monitoring agent\n    file_tailer_test.go\n  parsing/\n    parser.go                        ← Common parsing interface\n    parser_test.go\n    json_parser.go                   ← JSON log parsing\n    json_parser_test.go\n    syslog_parser.go                 ← RFC 3164/5424 syslog parsing\n    syslog_parser_test.go\n    regex_parser.go                  ← Custom regex patterns\n    regex_parser_test.go\n  buffer/\n    memory_buffer.go                 ← Ring buffer implementation\n    memory_buffer_test.go\n    disk_queue.go                    ← Disk overflow queues\n    disk_queue_test.go\n    backpressure.go                  ← Backpressure management\n    backpressure_test.go\n  types/\n    log_entry.go                     ← Core data structures\n    config.go                        ← Configuration types\n    metrics.go                       ← Ingestion metrics\nconfigs/\n  config.yaml                        ← Default configuration\n  syslog_patterns.yaml              ← Syslog parsing patterns</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Complete HTTP Server Foundation</strong> (<code>internal/ingestion/server.go</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> ingestion</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/gorilla/mux</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">your-project/internal/buffer</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">your-project/internal/parsing</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">your-project/internal/types</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HTTPServer</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Config</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parser     </span><span style=\"color:#B392F0\">parsing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Parser</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    buffer     </span><span style=\"color:#B392F0\">buffer</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Buffer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    server     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Server</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metrics    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    shutdown   </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    wg         </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">WaitGroup</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewHTTPServer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">parser</span><span style=\"color:#B392F0\"> parsing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Parser</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">buffer</span><span style=\"color:#B392F0\"> buffer</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Buffer</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">metrics</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Metrics</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPServer</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">HTTPServer</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config:   config,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        parser:   parser,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        buffer:   buffer,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metrics:  metrics,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        shutdown: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    router </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> mux.</span><span style=\"color:#B392F0\">NewRouter</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    router.</span><span style=\"color:#B392F0\">HandleFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/api/v1/logs\"</span><span style=\"color:#E1E4E8\">, s.handleLogIngestion).</span><span style=\"color:#B392F0\">Methods</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"POST\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    router.</span><span style=\"color:#B392F0\">HandleFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/health\"</span><span style=\"color:#E1E4E8\">, s.handleHealthCheck).</span><span style=\"color:#B392F0\">Methods</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"GET\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    router.</span><span style=\"color:#B392F0\">HandleFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/metrics\"</span><span style=\"color:#E1E4E8\">, s.handleMetrics).</span><span style=\"color:#B392F0\">Methods</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"GET\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.server </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Addr:         fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\":</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, s.config.HTTPPort),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Handler:      router,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ReadTimeout:  </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        WriteTimeout: </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        IdleTimeout:  </span><span style=\"color:#79B8FF\">120</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.wg.</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    go</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        defer</span><span style=\"color:#E1E4E8\"> s.wg.</span><span style=\"color:#B392F0\">Done</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.server.</span><span style=\"color:#B392F0\">ListenAndServe</span><span style=\"color:#E1E4E8\">(); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#F97583\"> &#x26;&#x26;</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> http.ErrServerClosed {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            fmt.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"HTTP server error: </span><span style=\"color:#79B8FF\">%v\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    close</span><span style=\"color:#E1E4E8\">(s.shutdown)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctx, cancel </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithTimeout</span><span style=\"color:#E1E4E8\">(context.</span><span style=\"color:#B392F0\">Background</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">time.Second)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#B392F0\"> cancel</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.server.</span><span style=\"color:#B392F0\">Shutdown</span><span style=\"color:#E1E4E8\">(ctx); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.wg.</span><span style=\"color:#B392F0\">Wait</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">handleLogIngestion</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Implement log ingestion endpoint - this is where you'll add the core logic</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">handleHealthCheck</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.</span><span style=\"color:#B392F0\">WriteHeader</span><span style=\"color:#E1E4E8\">(http.StatusOK)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    json.</span><span style=\"color:#B392F0\">NewEncoder</span><span style=\"color:#E1E4E8\">(w).</span><span style=\"color:#B392F0\">Encode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"status\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"healthy\"</span><span style=\"color:#E1E4E8\">})</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">handleMetrics</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ingested, queries </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.metrics.</span><span style=\"color:#B392F0\">GetStats</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    json.</span><span style=\"color:#B392F0\">NewEncoder</span><span style=\"color:#E1E4E8\">(w).</span><span style=\"color:#B392F0\">Encode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"logs_ingested\"</span><span style=\"color:#E1E4E8\">: ingested,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"queries_executed\"</span><span style=\"color:#E1E4E8\">: queries,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Complete Ring Buffer Implementation</strong> (<code>internal/buffer/memory_buffer.go</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> buffer</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">errors</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync/atomic</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">your-project/internal/types</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MemoryBuffer</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    entries    []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">LogEntry</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    writePos   </span><span style=\"color:#F97583\">uint64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    readPos    </span><span style=\"color:#F97583\">uint64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    capacity   </span><span style=\"color:#F97583\">uint64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mask       </span><span style=\"color:#F97583\">uint64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu         </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    notEmpty   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Cond</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    closed     </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewMemoryBuffer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">capacity</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Buffer</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Ensure capacity is power of 2 for efficient masking</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    actualCapacity </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> nextPowerOfTwo</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">uint64</span><span style=\"color:#E1E4E8\">(capacity))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    b </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">MemoryBuffer</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        entries:  </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#E1E4E8\">, actualCapacity),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        capacity: actualCapacity,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        mask:     actualCapacity </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    b.notEmpty </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sync.</span><span style=\"color:#B392F0\">NewCond</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">b.mu)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> b</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">b </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MemoryBuffer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">entry</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> atomic.</span><span style=\"color:#B392F0\">LoadInt32</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">b.closed) </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"buffer closed\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    b.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> b.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Check if buffer is full</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> (b.writePos</span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\">b.readPos) </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> b.capacity {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"buffer full\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    b.entries[b.writePos</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">b.mask] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> entry</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    b.writePos</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    b.notEmpty.</span><span style=\"color:#B392F0\">Signal</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">b </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MemoryBuffer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Read</span><span style=\"color:#E1E4E8\">() (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    b.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> b.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> b.writePos </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> b.readPos {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> atomic.</span><span style=\"color:#B392F0\">LoadInt32</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">b.closed) </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"buffer closed\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        b.notEmpty.</span><span style=\"color:#B392F0\">Wait</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    entry </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> b.entries[b.readPos</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">b.mask]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    b.entries[b.readPos</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">b.mask] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#6A737D\"> // Prevent memory leaks</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    b.readPos</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> entry, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">b </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MemoryBuffer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    atomic.</span><span style=\"color:#B392F0\">StoreInt32</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">b.closed, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    b.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    b.notEmpty.</span><span style=\"color:#B392F0\">Broadcast</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    b.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> nextPowerOfTwo</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">n</span><span style=\"color:#F97583\"> uint64</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">uint64</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> n </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    n</span><span style=\"color:#F97583\">--</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    n </span><span style=\"color:#F97583\">|=</span><span style=\"color:#E1E4E8\"> n </span><span style=\"color:#F97583\">>></span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    n </span><span style=\"color:#F97583\">|=</span><span style=\"color:#E1E4E8\"> n </span><span style=\"color:#F97583\">>></span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    n </span><span style=\"color:#F97583\">|=</span><span style=\"color:#E1E4E8\"> n </span><span style=\"color:#F97583\">>></span><span style=\"color:#79B8FF\"> 4</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    n </span><span style=\"color:#F97583\">|=</span><span style=\"color:#E1E4E8\"> n </span><span style=\"color:#F97583\">>></span><span style=\"color:#79B8FF\"> 8</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    n </span><span style=\"color:#F97583\">|=</span><span style=\"color:#E1E4E8\"> n </span><span style=\"color:#F97583\">>></span><span style=\"color:#79B8FF\"> 16</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    n </span><span style=\"color:#F97583\">|=</span><span style=\"color:#E1E4E8\"> n </span><span style=\"color:#F97583\">>></span><span style=\"color:#79B8FF\"> 32</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> n </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>HTTP Log Ingestion Handler</strong> (add to <code>server.go</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">handleLogIngestion</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate Content-Type header is application/json</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Limit request body size to prevent memory exhaustion (e.g., 1MB max)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Parse JSON body into raw map[string]interface{} or []map[string]interface{}</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Handle both single log entry and array of entries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: For each entry, call s.parser.Parse() to convert to LogEntry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Validate parsed LogEntry using entry.Validate()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Attempt to write entry to buffer using s.buffer.Write()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: If buffer write fails (backpressure), return 503 Service Unavailable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: Increment ingestion counter using s.metrics.IncrementLogsIngested()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 10: Return appropriate HTTP status code and response body</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use defer to ensure metrics are updated even if processing fails partially</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Syslog TCP Handler</strong> (<code>internal/ingestion/tcp_handler.go</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TCPHandler</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">handleConnection</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">conn</span><span style=\"color:#B392F0\"> net</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Conn</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> conn.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Set read timeout on connection to detect stalled clients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create buffered reader for efficient line reading</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Loop reading lines until connection closes or errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: For each line, attempt to parse as syslog message</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Detect RFC 3164 vs RFC 5424 format based on message structure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Extract priority, timestamp, hostname, and message content</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Convert parsed fields into LogEntry structure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Write LogEntry to buffer, handling backpressure appropriately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: Log connection statistics on disconnect</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use bufio.Scanner for line-by-line reading with size limits</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>JSON Parser Implementation</strong> (<code>internal/parsing/json_parser.go</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">p </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">JSONParser</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Parse</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Parse JSON data into map[string]interface{}</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Extract timestamp field and attempt multiple format parsing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Fall back to current time if timestamp parsing fails</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Extract message field (required) or use entire JSON as message</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Extract labels from configured field names (level, service, host, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Validate label keys and values meet requirements</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Create LogEntry with parsed timestamp, labels, and message</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Return validation errors for malformed entries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use time.Parse() with multiple layouts for timestamp flexibility</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints\">Language-Specific Hints</h4>\n<p><strong>Go-Specific Implementation Tips:</strong></p>\n<ul>\n<li>Use <code>sync.Pool</code> for reusing parser objects and reducing allocation overhead during high throughput</li>\n<li>Implement <code>context.Context</code> cancellation in long-running goroutines for graceful shutdown</li>\n<li>Use <code>atomic</code> package operations for metrics counters to avoid mutex overhead</li>\n<li>Configure <code>GOMAXPROCS</code> appropriately for container environments with CPU limits</li>\n<li>Use build tags to enable debug logging and metrics collection selectively</li>\n</ul>\n<p><strong>Buffer Management:</strong></p>\n<ul>\n<li>Use <code>runtime.ReadMemStats()</code> to monitor memory usage and trigger disk spillover</li>\n<li>Implement buffer sizing as a function of available system memory</li>\n<li>Use <code>os.File.Sync()</code> for write-ahead log durability guarantees</li>\n<li>Consider <code>mmap</code> for disk queues if random access patterns emerge</li>\n</ul>\n<p><strong>Network Protocol Handling:</strong></p>\n<ul>\n<li>Use <code>net.ListenConfig</code> with <code>Control</code> function to set socket options like <code>SO_REUSEPORT</code></li>\n<li>Configure TCP keep-alive settings to detect dead connections promptly  </li>\n<li>Set UDP receive buffer sizes using <code>syscall.SetsockoptInt()</code> for high throughput</li>\n<li>Use separate goroutine pools for different protocols to prevent head-of-line blocking</li>\n</ul>\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing Milestone 1, verify the following behaviors:</p>\n<p><strong>Test Commands:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Start the ingestion server</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/logaggregator/main.go</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test HTTP ingestion</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/v1/logs</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -H</span><span style=\"color:#9ECBFF\"> \"Content-Type: application/json\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -d</span><span style=\"color:#9ECBFF\"> '{\"timestamp\": \"2024-01-15T10:30:00Z\", \"level\": \"INFO\", \"service\": \"web\", \"message\": \"Request processed\"}'</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test batch HTTP ingestion</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/v1/logs</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -H</span><span style=\"color:#9ECBFF\"> \"Content-Type: application/json\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -d</span><span style=\"color:#9ECBFF\"> '[</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    {\"timestamp\": \"2024-01-15T10:30:00Z\", \"level\": \"INFO\", \"service\": \"web\", \"message\": \"Request 1\"},</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    {\"timestamp\": \"2024-01-15T10:30:01Z\", \"level\": \"ERROR\", \"service\": \"web\", \"message\": \"Request 2 failed\"}</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">  ]'</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test syslog ingestion</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">echo</span><span style=\"color:#9ECBFF\"> \"&#x3C;134>Jan 15 10:30:00 webserver nginx: GET /api/status 200\"</span><span style=\"color:#F97583\"> |</span><span style=\"color:#B392F0\"> nc</span><span style=\"color:#9ECBFF\"> localhost</span><span style=\"color:#79B8FF\"> 1514</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Check health and metrics</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> http://localhost:8080/health</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> http://localhost:8080/metrics</span></span></code></pre></div>\n\n<p><strong>Expected Outputs:</strong></p>\n<ul>\n<li>HTTP requests return 200 OK for valid entries, 400 Bad Request for malformed JSON</li>\n<li>TCP syslog connections accept messages and parse facility/severity correctly</li>\n<li>Health endpoint returns <code>{&quot;status&quot;: &quot;healthy&quot;}</code></li>\n<li>Metrics endpoint shows increasing <code>logs_ingested</code> counter</li>\n<li>Server logs show parsed log entries with extracted labels and normalized timestamps</li>\n</ul>\n<p><strong>Verification Steps:</strong></p>\n<ol>\n<li>Monitor memory usage during sustained load - should remain bounded</li>\n<li>Test backpressure by overwhelming the system - should return 503 responses gracefully</li>\n<li>Verify timestamp parsing handles multiple formats correctly</li>\n<li>Check label extraction produces expected key-value pairs</li>\n<li>Test malformed input handling - server should remain stable</li>\n<li>Verify file tail agent detects new log lines promptly</li>\n</ol>\n<p><strong>Troubleshooting Common Issues:</strong></p>\n<ul>\n<li>&quot;Connection refused&quot; - Check if ports are already in use by other processes</li>\n<li>JSON parsing errors - Verify Content-Type header and valid JSON formatting  </li>\n<li>Memory growth - Implement buffer size limits and backpressure mechanisms</li>\n<li>TCP connection buildup - Add connection timeouts and proper cleanup</li>\n<li>Missing log entries - Check UDP socket buffer sizes and packet drop statistics</li>\n</ul>\n<h2 id=\"log-indexing-engine\">Log Indexing Engine</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section corresponds to Milestone 2 (Log Index), where we build inverted indexes with bloom filters for efficient label-based queries. This milestone depends on the log ingestion capabilities from Milestone 1 and provides the foundation for the query engine in Milestone 3.</p>\n</blockquote>\n<h3 id=\"mental-model-the-card-catalog-system\">Mental Model: The Card Catalog System</h3>\n<p>Think of our log indexing engine as a sophisticated library card catalog system from the pre-digital era. In a traditional library, finding a specific book about &quot;distributed systems&quot; required consulting multiple card catalogs: one organized by author, another by subject, and perhaps a third by title. Each card contained enough information to locate the actual book on the shelves.</p>\n<p>Our log indexing engine operates on the same principle. When millions of log entries pour into our system, we need multiple &quot;card catalogs&quot; (inverted indexes) to quickly locate relevant entries without scanning every single log message. Just as a librarian might organize cards by subject (&quot;Computer Science&quot;), author (&quot;Martin Kleppmann&quot;), or publication year (&quot;2017&quot;), we organize our indexes by log labels (<code>service=api</code>), message content (<code>ERROR</code>), and time ranges (<code>2024-01-15 14:30-15:00</code>).</p>\n<p>The bloom filter acts like a librarian&#39;s preliminary screening system. Before consulting the detailed card catalog, the librarian might quickly check: &quot;Do we even have any books published in 1995?&quot; If the answer is definitively &quot;no,&quot; there&#39;s no need to dig through the detailed catalog. The bloom filter gives us this same quick negative confirmation: &quot;Are there any logs with <code>level=DEBUG</code> in this time window?&quot; If not, we skip expensive index lookups entirely.</p>\n<p>Time-based partitioning is like organizing the library into sections by decade. When someone asks for books about &quot;World War II,&quot; the librarian doesn&#39;t search the entire library—they go straight to the 1940s section. Similarly, when querying logs from &quot;last Tuesday,&quot; we only search the index partitions covering that time range.</p>\n<p>The key insight is that indexing isn&#39;t about storing logs—it&#39;s about creating multiple efficient pathways to find them. Just as a good library has many ways to locate the same book, our indexing engine provides multiple access patterns (by time, by label, by content) to the same log data.</p>\n<h3 id=\"inverted-index-design\">Inverted Index Design</h3>\n<p>The <strong>inverted index</strong> forms the backbone of our log search capabilities, transforming the fundamental question from &quot;what&#39;s in this log entry?&quot; to &quot;which log entries contain this term?&quot; This inversion—hence the name—enables logarithmic-time lookups instead of linear scans across millions of log messages.</p>\n<p><img src=\"/api/project/log-aggregator/architecture-doc/asset?path=diagrams%2Findex-structure.svg\" alt=\"Index Architecture\"></p>\n<p>Our inverted index maintains a mapping from every unique term (extracted from log labels and message content) to a <strong>PostingsList</strong> containing references to all log entries that contain that term. The design prioritizes both query speed and storage efficiency, since a single high-traffic service can generate millions of unique terms across billions of log entries.</p>\n<p><strong>Core Index Data Structures</strong></p>\n<p>The primary index structures work together to provide fast term-to-document resolution:</p>\n<table>\n<thead>\n<tr>\n<th>Structure</th>\n<th>Type</th>\n<th>Purpose</th>\n<th>Storage Location</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>IndexSegment</code></td>\n<td>Primary container</td>\n<td>Groups terms and postings lists for a time window</td>\n<td>Memory + Disk</td>\n</tr>\n<tr>\n<td>Terms Map</td>\n<td><code>map[string]*PostingsList</code></td>\n<td>Maps each unique term to its postings list</td>\n<td>Memory with disk backing</td>\n</tr>\n<tr>\n<td><code>PostingsList</code></td>\n<td><code>[]EntryReference</code></td>\n<td>Ordered list of log entries containing the term</td>\n<td>Compressed on disk</td>\n</tr>\n<tr>\n<td><code>EntryReference</code></td>\n<td>Struct</td>\n<td>Points to specific log entry location</td>\n<td>Inline in postings list</td>\n</tr>\n<tr>\n<td>Bloom Filter</td>\n<td><code>BloomFilter</code></td>\n<td>Fast negative lookup for terms not in segment</td>\n<td>Memory</td>\n</tr>\n</tbody></table>\n<p>Each <code>IndexSegment</code> represents a bounded time window (typically 1-24 hours) and contains all terms discovered in log entries during that period. Segments remain immutable after creation, enabling safe concurrent reads and predictable storage patterns.</p>\n<p><strong>Term Extraction and Normalization</strong></p>\n<p>The indexing pipeline extracts searchable terms from multiple sources within each <code>LogEntry</code>. This multi-faceted approach ensures users can find logs regardless of their search strategy:</p>\n<ol>\n<li><p><strong>Label Terms</strong>: Every label key-value pair becomes two terms: the key (<code>service</code>) and the key-value combination (<code>service=api</code>). This enables both existence queries (&quot;show me all logs with a <code>service</code> label&quot;) and specific value queries (&quot;show me logs where <code>service=api</code>&quot;).</p>\n</li>\n<li><p><strong>Message Content Terms</strong>: Log message text undergoes tokenization and normalization. We split on whitespace and common delimiters, convert to lowercase, and optionally remove stop words. Structured message content (JSON within the message) receives special handling to extract nested field values as searchable terms.</p>\n</li>\n<li><p><strong>Timestamp Terms</strong>: Time-based terms enable temporal queries without consulting the time-range index. We generate terms like <code>hour=14</code>, <code>day=15</code>, <code>month=01</code> to support time-focused searches.</p>\n</li>\n<li><p><strong>Derived Terms</strong>: Computed values like message length ranges (<code>msglen_100_1000</code> for messages between 100-1000 characters) or pattern matches (<code>contains_ip_address</code>) provide additional search dimensions.</p>\n</li>\n</ol>\n<p><strong>PostingsList Organization and Compression</strong></p>\n<p>Each <code>PostingsList</code> maintains <code>EntryReference</code> entries sorted by timestamp, enabling efficient time-range filtering during query execution. The reference structure contains just enough information to locate the full log entry without storing redundant data:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Purpose</th>\n<th>Size</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ChunkID</code></td>\n<td><code>string</code></td>\n<td>Identifies storage chunk containing the log entry</td>\n<td>16 bytes</td>\n</tr>\n<tr>\n<td><code>Offset</code></td>\n<td><code>uint32</code></td>\n<td>Byte offset within the decompressed chunk</td>\n<td>4 bytes</td>\n</tr>\n<tr>\n<td><code>Timestamp</code></td>\n<td><code>time.Time</code></td>\n<td>Entry timestamp for time-range filtering</td>\n<td>8 bytes</td>\n</tr>\n</tbody></table>\n<p>Postings lists use delta compression to reduce storage overhead. Since entries are timestamp-ordered, we store only the difference between consecutive timestamps and offsets. For frequently occurring terms (like <code>level=INFO</code>), this compression can reduce storage by 60-80%.</p>\n<p>Large postings lists employ skip list structures to accelerate time-range searches. Instead of scanning through thousands of entries to find a specific time window, skip pointers allow logarithmic-time jumps to the approximate time range, followed by linear scanning within the narrow window.</p>\n<p><strong>Index Persistence and Memory Management</strong></p>\n<p>The index engine maintains a hybrid memory-disk architecture optimized for read-heavy workloads. Frequently accessed index segments remain fully memory-resident, while older segments live on disk with selective caching of hot terms.</p>\n<p>Memory management follows a tiered approach:</p>\n<ul>\n<li><strong>Hot Tier</strong>: Recent segments (last 1-2 hours) with complete in-memory terms maps and postings lists</li>\n<li><strong>Warm Tier</strong>: Recent segments (last 24 hours) with in-memory terms maps but disk-resident postings lists</li>\n<li><strong>Cold Tier</strong>: Historical segments with disk-resident terms maps and postings lists, cached on demand</li>\n</ul>\n<p>This tiering ensures that common queries against recent data achieve sub-millisecond response times, while historical queries remain reasonably fast through intelligent caching.</p>\n<blockquote>\n<p><strong>Critical Design Insight</strong>: The inverted index optimizes for query patterns where users search for specific terms across large time ranges. If your primary query pattern involves full log reconstruction for specific services, a different index design might be more appropriate.</p>\n</blockquote>\n<p><strong>Architecture Decision Records</strong></p>\n<blockquote>\n<p><strong>Decision: HashMap vs B-Tree for Terms Storage</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to store millions of unique terms per index segment with fast lookup and reasonable memory overhead</li>\n<li><strong>Options Considered</strong>: <ul>\n<li>In-memory HashMap (<code>map[string]*PostingsList</code>)</li>\n<li>B-Tree index with disk backing</li>\n<li>Trie structure for prefix matching</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: In-memory HashMap for hot/warm tiers, B-Tree for cold tier</li>\n<li><strong>Rationale</strong>: HashMap provides O(1) lookup for exact term matches, which represents 95% of our query patterns. B-Tree enables range scans and uses less memory for infrequently accessed terms.</li>\n<li><strong>Consequences</strong>: Fast common-case performance but requires more memory for active segments. Prefix queries require full map iteration.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Per-Segment vs Global Index Structure</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to balance query performance, update efficiency, and storage overhead as log volume grows</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Single global index updated continuously</li>\n<li>Time-based segments with periodic merging</li>\n<li>Service-based sharding with cross-service queries</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Time-based segments (1-hour windows) with background compaction</li>\n<li><strong>Rationale</strong>: Segments enable immutable index structures (no concurrent modification), natural time-range filtering, and bounded memory usage per segment.</li>\n<li><strong>Consequences</strong>: Queries spanning multiple segments require index merging, but time-range queries become extremely efficient.</li>\n</ul>\n</blockquote>\n<h3 id=\"bloom-filter-implementation\">Bloom Filter Implementation</h3>\n<p><strong>Bloom filters</strong> serve as the first line of defense against expensive index lookups, providing probabilistic &quot;definitely not present&quot; guarantees that eliminate unnecessary disk I/O and computation. In our log aggregation system, bloom filters typically eliminate 70-90% of negative lookups, dramatically reducing query latency for exploratory searches.</p>\n<p>The bloom filter operates on a simple principle: before consulting the detailed inverted index, we check whether a term might exist in the segment. A negative result (&quot;definitely not present&quot;) allows us to skip that segment entirely. A positive result (&quot;might be present&quot;) requires consulting the actual index, where we may discover the term doesn&#39;t exist after all (false positive).</p>\n<p><strong>Bloom Filter Data Structure and Parameters</strong></p>\n<p>Our <code>BloomFilter</code> implementation uses a configurable bit array with multiple independent hash functions to minimize false positive rates while maintaining memory efficiency:</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Type</th>\n<th>Purpose</th>\n<th>Configuration</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>BitArray</code></td>\n<td><code>[]uint64</code></td>\n<td>Stores the bloom filter bits</td>\n<td>Size calculated from expected elements</td>\n</tr>\n<tr>\n<td><code>HashFunctions</code></td>\n<td><code>[]hash.Hash</code></td>\n<td>Independent hash functions for bit positioning</td>\n<td>Count derived from target false positive rate</td>\n</tr>\n<tr>\n<td><code>Parameters</code></td>\n<td><code>BloomParams</code></td>\n<td>Runtime configuration and sizing</td>\n<td>Set per index segment based on expected terms</td>\n</tr>\n</tbody></table>\n<p>The <code>BloomParams</code> structure encapsulates all sizing decisions for a specific bloom filter instance:</p>\n<table>\n<thead>\n<tr>\n<th>Parameter</th>\n<th>Type</th>\n<th>Purpose</th>\n<th>Typical Range</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ExpectedElements</code></td>\n<td><code>uint32</code></td>\n<td>Number of unique terms expected in the segment</td>\n<td>10,000 - 1,000,000</td>\n</tr>\n<tr>\n<td><code>FalsePositiveRate</code></td>\n<td><code>float64</code></td>\n<td>Target probability of false positives</td>\n<td>0.01 - 0.05 (1-5%)</td>\n</tr>\n<tr>\n<td><code>BitArraySize</code></td>\n<td><code>uint32</code></td>\n<td>Calculated size of the bit array in bits</td>\n<td>Derived from above parameters</td>\n</tr>\n<tr>\n<td><code>HashCount</code></td>\n<td><code>uint32</code></td>\n<td>Number of independent hash functions to use</td>\n<td>Usually 3-8 functions</td>\n</tr>\n</tbody></table>\n<p><strong>Bloom Filter Sizing and Mathematical Foundation</strong></p>\n<p>Proper bloom filter sizing requires balancing false positive rates against memory consumption. The mathematical relationships guide our parameter selection:</p>\n<p>The optimal bit array size (in bits) follows: <code>m = -n * ln(p) / (ln(2)^2)</code> where <code>n</code> is expected elements and <code>p</code> is target false positive rate. The optimal number of hash functions is: <code>k = (m/n) * ln(2)</code>.</p>\n<p>For a concrete example: expecting 100,000 unique terms with a 2% false positive rate requires approximately 730,000 bits (91 KB) and 6 hash functions. This represents excellent space efficiency—less than 1 byte per indexed term.</p>\n<p><strong>Hash Function Selection and Distribution</strong></p>\n<p>The bloom filter&#39;s effectiveness depends critically on using truly independent hash functions that distribute values uniformly across the bit array. We implement this using a single strong hash function (like FNV-1a or xxHash) with different seed values to generate multiple independent hash values.</p>\n<p>Our implementation avoids the common pitfall of using linear combinations of two hash functions, which can introduce subtle correlations that increase false positive rates beyond theoretical expectations. Instead, we maintain separate hash instances:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Each hash function uses a different seed for independence</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">hashFunctions </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">hash</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Hash</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fnv.</span><span style=\"color:#B392F0\">New64a</span><span style=\"color:#E1E4E8\">(),           </span><span style=\"color:#6A737D\">// seed = 0 (default)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    NewSeededFNV64a</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1337</span><span style=\"color:#E1E4E8\">),  </span><span style=\"color:#6A737D\">// seed = 1337</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    NewSeededFNV64a</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">7919</span><span style=\"color:#E1E4E8\">),  </span><span style=\"color:#6A737D\">// seed = 7919 (prime number)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // ... additional functions as needed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Integration with Index Segments</strong></p>\n<p>Each <code>IndexSegment</code> maintains its own bloom filter containing all terms present in that segment. This per-segment approach provides several advantages:</p>\n<ol>\n<li><strong>Bounded Memory Growth</strong>: Bloom filter size depends only on the number of terms in one segment, not the entire system</li>\n<li><strong>Time-Range Optimization</strong>: Queries can eliminate entire time windows based on bloom filter checks</li>\n<li><strong>Independent Tuning</strong>: Different time periods might have different term densities, allowing per-segment optimization</li>\n</ol>\n<p>During index construction, we perform a two-pass process:</p>\n<ol>\n<li><strong>First Pass</strong>: Count unique terms to size the bloom filter appropriately</li>\n<li><strong>Second Pass</strong>: Add all terms to both the bloom filter and the inverted index</li>\n</ol>\n<p>This approach prevents bloom filter overflow, which would degrade false positive rates beyond acceptable levels.</p>\n<p><strong>False Positive Handling and Query Integration</strong></p>\n<p>The query engine integrates bloom filters as an optimization layer, not a correctness mechanism. The typical query flow follows this pattern:</p>\n<ol>\n<li><strong>Bloom Filter Check</strong>: For each relevant index segment, check if the search term might be present</li>\n<li><strong>Index Consultation</strong>: For segments with positive bloom filter results, consult the actual inverted index</li>\n<li><strong>Result Verification</strong>: Confirm term presence and retrieve postings list</li>\n<li><strong>False Positive Handling</strong>: If the term isn&#39;t found in the index despite a positive bloom filter result, continue to the next segment</li>\n</ol>\n<p>This design ensures that false positives impact only performance (wasted index lookups), never correctness. False negatives are mathematically impossible with bloom filters, guaranteeing we never miss relevant log entries.</p>\n<p><strong>Bloom Filter Persistence and Loading</strong></p>\n<p>Bloom filters persist alongside their corresponding index segments, typically as a header section in the segment file. During system startup, bloom filters load into memory before the detailed index structures, enabling fast segment elimination even when the full index remains disk-resident.</p>\n<p>The persistence format optimizes for quick loading:</p>\n<ul>\n<li><strong>Parameters Section</strong>: <code>BloomParams</code> serialized as fixed-size binary structure</li>\n<li><strong>Bit Array Section</strong>: Raw bit array written as contiguous bytes</li>\n<li><strong>Checksum</strong>: Integrity verification to detect corruption</li>\n</ul>\n<blockquote>\n<p><strong>Critical Performance Insight</strong>: Bloom filter memory usage should remain under 10% of total index memory consumption. If bloom filters consume significantly more memory, either the false positive rate is too aggressive, or the segment size is too large.</p>\n</blockquote>\n<p><strong>Architecture Decision Records</strong></p>\n<blockquote>\n<p><strong>Decision: Per-Segment vs Global Bloom Filters</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to decide whether to maintain one bloom filter per index segment or a single global bloom filter for all terms</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Single global bloom filter updated continuously</li>\n<li>Per-segment bloom filters created once during segment creation</li>\n<li>Hierarchical bloom filters (global + per-segment)</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Per-segment bloom filters with no global filter</li>\n<li><strong>Rationale</strong>: Per-segment filters enable time-range optimization (eliminate entire time windows) and bound memory growth. Global filters would grow unboundedly and provide no time-range benefits.</li>\n<li><strong>Consequences</strong>: Multi-segment queries require checking multiple bloom filters, but each check is fast and provides time-locality benefits.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Target False Positive Rate Selection</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to balance bloom filter memory consumption against false positive query overhead</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Aggressive 1% false positive rate (higher memory usage)</li>\n<li>Conservative 5% false positive rate (lower memory usage)</li>\n<li>Adaptive rate based on segment term density</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Fixed 2% false positive rate for all segments</li>\n<li><strong>Rationale</strong>: 2% provides good memory efficiency (8 bits per element) while keeping false positive overhead low. Most terms have low query frequency, so false positives rarely impact user-visible queries.</li>\n<li><strong>Consequences</strong>: Predictable memory usage and good query performance. May be suboptimal for segments with very high or very low query rates.</li>\n</ul>\n</blockquote>\n<h3 id=\"time-based-partitioning\">Time-Based Partitioning</h3>\n<p><strong>Time-based partitioning</strong> serves as the primary strategy for making log queries tractable across large time spans and data volumes. Rather than maintaining monolithic indexes covering all historical data, we segment our indexes by time windows, enabling queries to access only the data relevant to their time range.</p>\n<p>This approach transforms queries spanning multiple time windows from &quot;search everything&quot; operations into &quot;search specific segments and merge results&quot; operations. For typical log analysis workflows—which focus heavily on recent events or specific incident time ranges—this partitioning provides dramatic performance improvements.</p>\n<p><strong>Partition Window Sizing Strategy</strong></p>\n<p>Selecting appropriate partition window sizes requires balancing query performance, storage efficiency, and operational complexity. Different window sizes optimize for different access patterns:</p>\n<table>\n<thead>\n<tr>\n<th>Window Size</th>\n<th>Query Patterns</th>\n<th>Index Count (30 days)</th>\n<th>Merge Overhead</th>\n<th>Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1 Hour</td>\n<td>Precise time ranges, incident investigation</td>\n<td>720 segments</td>\n<td>Low</td>\n<td>High-frequency services</td>\n</tr>\n<tr>\n<td>6 Hours</td>\n<td>Shift-based analysis, trend monitoring</td>\n<td>120 segments</td>\n<td>Medium</td>\n<td>Medium-frequency services</td>\n</tr>\n<tr>\n<td>24 Hours</td>\n<td>Daily reporting, long-term trends</td>\n<td>30 segments</td>\n<td>High</td>\n<td>Low-frequency services</td>\n</tr>\n</tbody></table>\n<p>Our default 1-hour partitioning balances these concerns for typical production environments. Most log queries focus on the recent 1-6 hours, hitting only 1-6 segments. Historical queries spanning days or weeks require merging many segments, but these represent a minority of the query workload.</p>\n<p>The partitioning strategy adapts to observed query patterns through configurable window sizes per log stream. High-volume services with frequent queries benefit from smaller windows (15-30 minutes), while low-volume services can use larger windows (6-24 hours) to reduce index overhead.</p>\n<p><strong>Partition Boundary Alignment and Clock Synchronization</strong></p>\n<p>Partition boundaries align with wall-clock time boundaries (hour boundaries at :00 minutes) rather than system uptime or log arrival time. This alignment ensures that queries for &quot;logs between 2:00 PM and 4:00 PM&quot; map cleanly to specific partitions without requiring complex boundary calculations.</p>\n<p>Clock synchronization considerations become critical for multi-source log aggregation. Log entries arriving with timestamps spanning partition boundaries require careful handling:</p>\n<ol>\n<li><strong>Late-Arriving Logs</strong>: Entries arriving after their partition window has closed go into a &quot;late arrival&quot; segment associated with the original time window</li>\n<li><strong>Clock Skew</strong>: Entries with timestamps slightly outside their expected partition due to clock skew are accepted within a configurable tolerance (typically 5-10 minutes)</li>\n<li><strong>Future Timestamps</strong>: Entries with timestamps significantly in the future are either rejected or placed in a special &quot;future events&quot; partition for manual review</li>\n</ol>\n<p><strong>Partition Metadata and Catalog Management</strong></p>\n<p>Each partition maintains metadata describing its contents, query performance characteristics, and storage details. This metadata enables the query planner to make informed decisions about which partitions to search and how to optimize the search strategy.</p>\n<table>\n<thead>\n<tr>\n<th>Metadata Field</th>\n<th>Type</th>\n<th>Purpose</th>\n<th>Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Time Range</td>\n<td><code>TimeRange</code></td>\n<td>Start and end timestamps for the partition</td>\n<td>Query planning and routing</td>\n</tr>\n<tr>\n<td>Entry Count</td>\n<td><code>int64</code></td>\n<td>Total number of log entries in partition</td>\n<td>Cost estimation</td>\n</tr>\n<tr>\n<td>Unique Terms</td>\n<td><code>int64</code></td>\n<td>Number of distinct terms in inverted index</td>\n<td>Memory planning</td>\n</tr>\n<tr>\n<td>Disk Size</td>\n<td><code>int64</code></td>\n<td>Total storage consumed by partition</td>\n<td>Storage planning</td>\n</tr>\n<tr>\n<td>Last Accessed</td>\n<td><code>time.Time</code></td>\n<td>Most recent query accessing this partition</td>\n<td>Cache eviction decisions</td>\n</tr>\n</tbody></table>\n<p>The partition catalog serves as the central registry for all time-based partitions, providing fast lookup capabilities for query planning. During query processing, the catalog determines which partitions overlap with the requested time range and estimates the cost of searching each partition.</p>\n<p><strong>Cross-Partition Query Execution</strong></p>\n<p>Queries spanning multiple partitions require coordination to merge results while maintaining correct ordering and avoiding duplicates. The query engine implements a streaming merge algorithm that processes results from multiple partitions simultaneously:</p>\n<ol>\n<li><strong>Partition Selection</strong>: Query planner identifies all partitions overlapping the requested time range</li>\n<li><strong>Parallel Execution</strong>: Query executes against selected partitions in parallel, each returning a timestamp-ordered stream</li>\n<li><strong>Streaming Merge</strong>: Results from multiple streams merge using a priority queue ordered by timestamp</li>\n<li><strong>Deduplication</strong>: Adjacent entries with identical content and timestamps are deduplicated during merging</li>\n</ol>\n<p>This approach provides significant parallelization benefits for large time ranges while ensuring that results maintain temporal ordering for user presentation.</p>\n<p><strong>Partition Lifecycle and Transition Handling</strong></p>\n<p>Partition creation occurs continuously as new log data arrives. The transition from &quot;current&quot; to &quot;historical&quot; partitions involves several state changes that affect query performance and storage characteristics:</p>\n<p><strong>Active Partition State Transitions</strong>:</p>\n<ol>\n<li><strong>Write-Active</strong>: Currently accepting new log entries and updating indexes</li>\n<li><strong>Write-Sealed</strong>: No longer accepting new entries but may still have in-flight index updates</li>\n<li><strong>Read-Only</strong>: Fully sealed with immutable indexes, eligible for optimization</li>\n<li><strong>Archived</strong>: Moved to cold storage with different access patterns</li>\n</ol>\n<p>During the transition from Write-Active to Write-Sealed, a new partition begins accepting entries while the previous partition completes its final index updates. This overlap period prevents data loss during partition boundaries but requires careful handling of duplicate detection.</p>\n<p><strong>Architecture Decision Records</strong></p>\n<blockquote>\n<p><strong>Decision: Fixed vs Variable Partition Window Sizes</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to decide whether all partitions use the same time window size or allow variable sizes based on data volume</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Fixed 1-hour windows for all partitions</li>\n<li>Variable windows based on entry count (close partition after 100K entries)</li>\n<li>Hybrid approach with minimum time (1 hour) and maximum entries (500K)</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Fixed 1-hour windows with configurable overrides per stream</li>\n<li><strong>Rationale</strong>: Fixed windows provide predictable query behavior and simplify operational reasoning. Variable windows would optimize storage but complicate query planning and time-range calculations.</li>\n<li><strong>Consequences</strong>: Some partitions may be much larger or smaller than others, but query behavior remains predictable and partition boundaries align with human time concepts.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Partition Boundary Handling for Late Arrivals</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to handle log entries arriving after their partition window has already closed and been sealed</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Reject late-arriving entries entirely</li>\n<li>Reopen closed partitions to accept late entries</li>\n<li>Create separate &quot;late arrival&quot; partitions linked to original time windows</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Create late arrival partitions with configurable acceptance window (default 1 hour)</li>\n<li><strong>Rationale</strong>: Log systems must handle late arrivals due to network delays, buffering, and clock skew. Reopening sealed partitions would complicate concurrency and caching. Separate late arrival partitions maintain immutability while preserving data.</li>\n<li><strong>Consequences</strong>: Queries must check both primary and late arrival partitions for complete results, slightly increasing query complexity.</li>\n</ul>\n</blockquote>\n<h3 id=\"index-compaction-and-maintenance\">Index Compaction and Maintenance</h3>\n<p><strong>Index compaction</strong> prevents storage fragmentation and maintains query performance as the system accumulates thousands of small index segments over time. Without compaction, query execution would degrade as it processes hundreds of tiny segments instead of a few large, optimized segments.</p>\n<p>The compaction process combines multiple small index segments into fewer, larger segments while preserving all indexing relationships and removing obsolete data. This operation runs continuously in the background, targeting segments based on size, age, and access patterns.</p>\n<p><strong>Compaction Triggers and Scheduling</strong></p>\n<p>The compaction scheduler monitors several metrics to determine when compaction provides meaningful benefits:</p>\n<table>\n<thead>\n<tr>\n<th>Trigger Condition</th>\n<th>Threshold</th>\n<th>Rationale</th>\n<th>Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Segment Count</td>\n<td>&gt; 10 segments per hour</td>\n<td>Too many segments slow queries</td>\n<td>Merge segments within time window</td>\n</tr>\n<tr>\n<td>Segment Size</td>\n<td>&lt; 10MB per segment</td>\n<td>Small segments waste overhead</td>\n<td>Combine small adjacent segments</td>\n</tr>\n<tr>\n<td>Deleted Entry Ratio</td>\n<td>&gt; 20% deleted entries</td>\n<td>Space is wasted on obsolete data</td>\n<td>Rebuild segment excluding deleted entries</td>\n</tr>\n<tr>\n<td>Access Frequency</td>\n<td>No access in 7 days</td>\n<td>Segment should move to cold storage</td>\n<td>Archive and compress segment</td>\n</tr>\n</tbody></table>\n<p>Compaction scheduling balances resource usage against query performance. The scheduler prefers to compact during low-query periods (typically night hours) and limits concurrent compaction operations to avoid overwhelming disk I/O.</p>\n<p>The system maintains separate compaction strategies for different data temperatures:</p>\n<ul>\n<li><strong>Hot Data</strong> (last 24 hours): Frequent, lightweight compaction focusing on segment count reduction</li>\n<li><strong>Warm Data</strong> (last 7 days): Periodic compaction emphasizing storage efficiency</li>\n<li><strong>Cold Data</strong> (older than 7 days): Aggressive compaction with high compression ratios and slower access times</li>\n</ul>\n<p><strong>Compaction Algorithm and Merge Strategy</strong></p>\n<p>The compaction algorithm processes segments using a multi-way merge that maintains temporal ordering while combining inverted indexes efficiently. The core algorithm follows these steps:</p>\n<ol>\n<li><strong>Segment Selection</strong>: Choose 3-8 segments for compaction based on size and adjacency in time</li>\n<li><strong>Bloom Filter Preprocessing</strong>: Estimate merged segment parameters by analyzing source bloom filters</li>\n<li><strong>Multi-Way Index Merge</strong>: Combine inverted indexes while maintaining sorted postings lists</li>\n<li><strong>Duplicate Elimination</strong>: Remove duplicate entries and resolve overwrites within the time window</li>\n<li><strong>Bloom Filter Reconstruction</strong>: Build new bloom filter optimized for the merged term set</li>\n<li><strong>Atomic Replacement</strong>: Replace source segments with merged segment using atomic file operations</li>\n</ol>\n<p>During multi-way index merging, the algorithm maintains one iterator per source segment for each term being merged. This approach ensures that the merged postings list remains sorted by timestamp while combining entries from multiple sources efficiently.</p>\n<p><strong>Storage Space Reclamation</strong></p>\n<p>Compaction serves as the primary mechanism for reclaiming storage space occupied by deleted or updated log entries. When log entries are deleted (due to retention policies or user requests), the original index segments are not immediately modified. Instead, deletions are recorded in a separate deletion log.</p>\n<p>During compaction, the algorithm consults the deletion log to exclude obsolete entries from the merged segment:</p>\n<ol>\n<li><strong>Deletion Log Consultation</strong>: For each entry reference, check if it appears in the deletion log</li>\n<li><strong>Tombstone Resolution</strong>: Handle cases where entries were updated (delete old version, keep new version)</li>\n<li><strong>Space Calculation</strong>: Track storage space reclaimed to report compaction benefits</li>\n</ol>\n<p>This deferred deletion approach maintains index immutability for concurrent reads while ensuring that storage space is eventually reclaimed through the compaction process.</p>\n<p><strong>Concurrent Access During Compaction</strong></p>\n<p>Compaction operations must not interfere with ongoing queries, requiring careful coordination between compaction workers and query executors. The system achieves this through a versioned segment approach:</p>\n<p><strong>Compaction Isolation Strategy</strong>:</p>\n<ol>\n<li><strong>Snapshot Isolation</strong>: Queries operate against a consistent snapshot of segments that remains valid throughout query execution</li>\n<li><strong>Copy-on-Write</strong>: Compaction creates new merged segments without modifying source segments</li>\n<li><strong>Atomic Switchover</strong>: Once compaction completes, the segment catalog atomically updates to reference the new merged segment</li>\n<li><strong>Delayed Cleanup</strong>: Source segments remain available for in-flight queries before being deleted</li>\n</ol>\n<p>This approach ensures that compaction never causes query failures or inconsistent results, though it temporarily increases storage usage during the compaction process.</p>\n<p><strong>Compaction Performance and Resource Management</strong></p>\n<p>Compaction operations are resource-intensive, requiring significant disk I/O, memory for merge operations, and CPU for bloom filter reconstruction. The compaction scheduler includes several resource management strategies:</p>\n<p><strong>Resource Throttling Mechanisms</strong>:</p>\n<ul>\n<li><strong>I/O Rate Limiting</strong>: Limit compaction disk I/O to prevent interference with query performance</li>\n<li><strong>Memory Budgeting</strong>: Restrict compaction memory usage to avoid impacting query caching</li>\n<li><strong>CPU Prioritization</strong>: Run compaction at lower CPU priority to yield to interactive queries</li>\n<li><strong>Concurrent Operation Limits</strong>: Limit number of simultaneous compaction operations</li>\n</ul>\n<p>The system monitors query performance metrics during compaction and can pause compaction operations if query latency exceeds acceptable thresholds.</p>\n<p><strong>Compaction Monitoring and Observability</strong></p>\n<p>Effective compaction requires comprehensive monitoring to ensure the process achieves its goals without impacting system performance:</p>\n<table>\n<thead>\n<tr>\n<th>Metric</th>\n<th>Purpose</th>\n<th>Alert Threshold</th>\n<th>Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Compaction Lag</td>\n<td>Time behind optimal compaction</td>\n<td>&gt; 6 hours</td>\n<td>Increase compaction parallelism</td>\n</tr>\n<tr>\n<td>Space Reclamation Rate</td>\n<td>Storage freed per compaction cycle</td>\n<td>&lt; 10% expected</td>\n<td>Review deletion log efficiency</td>\n</tr>\n<tr>\n<td>Compaction Duration</td>\n<td>Time to complete merge operations</td>\n<td>&gt; 2x baseline</td>\n<td>Investigate I/O bottlenecks</td>\n</tr>\n<tr>\n<td>Query Impact</td>\n<td>Latency increase during compaction</td>\n<td>&gt; 20% baseline</td>\n<td>Implement additional throttling</td>\n</tr>\n</tbody></table>\n<p>These metrics enable proactive compaction tuning and help identify when compaction strategies need adjustment based on changing workload characteristics.</p>\n<p><strong>Architecture Decision Records</strong></p>\n<blockquote>\n<p><strong>Decision: Eager vs Lazy Index Compaction</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to decide when to perform compaction—immediately when segments reach threshold sizes, or defer until query performance degrades</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Eager compaction triggered by segment count/size thresholds</li>\n<li>Lazy compaction triggered by query performance degradation</li>\n<li>Hybrid approach with both proactive and reactive triggers</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Eager compaction with segment count/size triggers, plus reactive triggers for performance issues</li>\n<li><strong>Rationale</strong>: Proactive compaction prevents query performance degradation and provides predictable resource usage. Reactive triggers handle unexpected workload changes.</li>\n<li><strong>Consequences</strong>: Higher baseline resource usage for compaction, but more consistent query performance and predictable storage growth.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: In-Place vs Copy-Based Compaction</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to decide whether to modify existing segments during compaction or create new merged segments</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>In-place compaction that modifies existing segment files</li>\n<li>Copy-based compaction that creates new merged segments</li>\n<li>Hybrid approach using in-place for small changes, copy for major restructuring</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Copy-based compaction with atomic switchover</li>\n<li><strong>Rationale</strong>: Copy-based compaction provides better concurrency (no read locks), easier rollback on failure, and cleaner separation between old and new data.</li>\n<li><strong>Consequences</strong>: Higher temporary storage usage during compaction, but better reliability and concurrency characteristics.</li>\n</ul>\n</blockquote>\n<h3 id=\"architecture-decision-records\">Architecture Decision Records</h3>\n<p>This section consolidates the key architectural decisions that shape our indexing engine&#39;s design, providing rationale and trade-off analysis for each significant choice.</p>\n<blockquote>\n<p><strong>Decision: Label Indexing Strategy - Separate vs Combined Indexes</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to decide whether to index label keys and values separately or create combined key-value indexes</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Separate indexes for keys (<code>service</code>) and values (<code>api</code>), combined at query time</li>\n<li>Combined indexes for key-value pairs (<code>service=api</code>) with separate existence indexes</li>\n<li>Hierarchical indexes with key-based partitioning and value-based sub-indexes</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Combined key-value pair indexes with separate key existence indexes</li>\n<li><strong>Rationale</strong>: Most queries filter by specific key-value combinations (<code>service=api</code>) rather than key existence alone. Combined indexes provide single-lookup query resolution. Separate key existence indexes support the minority of &quot;show me all services&quot; queries.</li>\n<li><strong>Consequences</strong>: Slightly higher storage overhead for duplicate key storage, but much faster query execution for the common case.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Index Persistence Format - Binary vs Text</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to choose serialization format for persistent index storage that balances performance, debuggability, and cross-platform compatibility</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Binary format with custom serialization for maximum performance</li>\n<li>JSON format for human readability and debugging capabilities</li>\n<li>Protocol Buffers for structured binary with schema evolution</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Custom binary format with separate JSON export capability</li>\n<li><strong>Rationale</strong>: Index files are read frequently and must load quickly. Binary format provides 5-10x faster loading than JSON. Separate JSON export enables debugging without impacting production performance.</li>\n<li><strong>Consequences</strong>: More complex serialization code and debugging requires export step, but significant performance benefits for index loading.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Memory vs Disk Index Storage Distribution</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to decide what portion of index data to keep in memory vs disk, given that full in-memory storage becomes prohibitively expensive at scale</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Full in-memory indexes with disk backup only</li>\n<li>Full disk-based indexes with memory caching</li>\n<li>Tiered approach with hot data in memory, warm data cached, cold data on disk</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Tiered storage with 24-hour hot tier in memory, 7-day warm tier with selective caching, cold tier disk-resident</li>\n<li><strong>Rationale</strong>: Query patterns show strong temporal locality—90% of queries focus on recent 24 hours. Tiered approach optimizes for common case while supporting historical queries.</li>\n<li><strong>Consequences</strong>: More complex cache management and tier transition logic, but dramatic cost savings for large deployments.</li>\n</ul>\n</blockquote>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>This section identifies the most frequent mistakes developers encounter when implementing log indexing systems, providing concrete examples and remediation strategies for each pitfall.</p>\n<p>⚠️ <strong>Pitfall: Label Cardinality Explosion</strong></p>\n<p>The most dangerous indexing pitfall occurs when label values grow without bounds, causing exponential index growth that can exhaust system memory and degrade query performance. This typically happens when developers include high-cardinality values like request IDs, user IDs, or timestamps in log labels.</p>\n<p>Consider a service that generates labels like <code>request_id=abc123</code>, <code>user_id=user456</code>, and <code>session_id=sess789</code>. If each combination appears only once, the index must track millions of unique terms with single-entry postings lists. This creates massive storage overhead and provides no query benefit—searching for a specific request ID requires knowing the exact value beforehand.</p>\n<p><strong>Detection</strong>: Monitor label cardinality metrics per key. Alert when any label key exceeds 10,000 unique values within a 24-hour period. Track index memory growth rate—exponential growth often indicates cardinality problems.</p>\n<p><strong>Prevention</strong>: Implement label value validation that rejects high-cardinality labels during ingestion. Use separate high-cardinality fields in the message content rather than labels. Design label schemas around query patterns, not data capture convenience.</p>\n<p><strong>Remediation</strong>: For existing high-cardinality labels, implement label value bucketing (group similar values) or move problematic labels to structured message fields that don&#39;t participate in indexing.</p>\n<p>⚠️ <strong>Pitfall: Inefficient Bloom Filter Sizing</strong></p>\n<p>Developers often missize bloom filters, either wasting memory with overly conservative false positive rates or degrading query performance with excessive false positives. Bloom filter parameters seem like minor details but have substantial impact on system performance.</p>\n<p>A common mistake is using the same bloom filter parameters across all index segments regardless of their term density. A segment with 10,000 terms needs different parameters than a segment with 1,000,000 terms. Using identical parameters either wastes memory on small segments or creates high false positive rates on large segments.</p>\n<p><strong>Detection</strong>: Monitor false positive rates per segment—rates significantly higher than configured targets indicate undersized filters. Monitor memory usage per segment—bloom filters consuming more than 10% of segment memory may be oversized.</p>\n<p><strong>Prevention</strong>: Calculate bloom filter parameters individually for each segment based on actual term counts. Implement parameter validation that rejects configurations with unrealistic false positive rates or memory consumption.</p>\n<p><strong>Remediation</strong>: Recompact affected segments with properly sized bloom filters. Implement segment-specific parameter calculation during index creation.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Time Zone and Clock Skew Issues</strong></p>\n<p>Time-based partitioning assumes consistent timestamp interpretation across all log sources, but production environments often involve multiple time zones, clock skew, and inconsistent timestamp formats. These issues can cause logs to appear in unexpected partitions or disappear entirely from time-range queries.</p>\n<p>A typical scenario involves logs from servers in different data centers with slightly different system clocks. Server A&#39;s clock runs 10 minutes fast, Server B&#39;s clock runs 5 minutes slow. During a 15-minute incident window, logs from the three servers might appear in different hourly partitions, making incident reconstruction difficult.</p>\n<p><strong>Detection</strong>: Monitor partition boundary violations—logs appearing significantly outside their expected time windows. Track query result completeness by comparing expected vs actual log counts for known time ranges.</p>\n<p><strong>Prevention</strong>: Implement clock skew tolerance in partition assignment (accept logs within ±10 minutes of partition boundaries). Standardize on UTC timestamps throughout the ingestion pipeline. Monitor and alert on significant clock differences across log sources.</p>\n<p><strong>Remediation</strong>: Implement late arrival partitions for out-of-window logs. Create partition overlap periods where logs near boundaries are indexed in multiple partitions.</p>\n<p>⚠️ <strong>Pitfall: Unbounded Index Memory Growth During Compaction</strong></p>\n<p>Index compaction operations can consume unbounded memory when merging large segments, potentially causing out-of-memory crashes in production systems. This occurs when developers implement compaction as a single large merge operation rather than streaming merge with bounded memory usage.</p>\n<p>The problem manifests when compacting multiple large segments simultaneously. If each source segment requires 1GB of memory to load its index, compacting 8 segments would require 8GB of memory just for source data, plus additional memory for the merge output.</p>\n<p><strong>Detection</strong>: Monitor memory usage during compaction operations. Alert when compaction memory usage exceeds configured limits or when compaction operations fail with out-of-memory errors.</p>\n<p><strong>Prevention</strong>: Implement streaming merge algorithms that process one term at a time rather than loading complete indexes. Set memory budgets for compaction operations and pause compaction when budgets are exceeded.</p>\n<p><strong>Remediation</strong>: Redesign compaction to use streaming merges with bounded memory usage. Implement compaction scheduling that limits concurrent operations based on memory availability.</p>\n<p>⚠️ <strong>Pitfall: Index Corruption Without Detection or Recovery</strong></p>\n<p>Index corruption can occur due to hardware failures, software bugs, or incomplete writes, but many implementations lack corruption detection and recovery mechanisms. Corrupted indexes may return incorrect query results or crash the system during queries.</p>\n<p>Corruption often happens during system crashes when index writes are partially completed. A power failure during bloom filter writes might leave the bit array in an inconsistent state, causing the bloom filter to report incorrect presence/absence information.</p>\n<p><strong>Detection</strong>: Implement index integrity checking using checksums for each index component (terms map, postings lists, bloom filters). Perform periodic background verification of index consistency.</p>\n<p><strong>Prevention</strong>: Use atomic writes for index updates. Implement write-ahead logging for index operations. Store checksums alongside index data to detect corruption during reading.</p>\n<p><strong>Remediation</strong>: Design index recovery procedures that can rebuild corrupted segments from source log data. Implement fallback strategies that continue operating with reduced functionality when corruption is detected.</p>\n<p>⚠️ <strong>Pitfall: Poor Query Performance Due to Inefficient Term Extraction</strong></p>\n<p>Inefficient term extraction can create indexes with poor query characteristics, either missing important searchable content or including too much noise that dilutes search effectiveness. This commonly occurs with unstructured log messages that require careful parsing to extract meaningful terms.</p>\n<p>A frequent mistake involves tokenizing log messages using simple whitespace splitting without considering log-specific patterns. JSON logs embedded in message text, stack traces, and structured data within unstructured messages require specialized parsing to extract useful search terms.</p>\n<p><strong>Detection</strong>: Monitor query result quality—low relevance scores or frequent empty result sets may indicate poor term extraction. Analyze term frequency distributions to identify noise terms that dominate the index.</p>\n<p><strong>Prevention</strong>: Design term extraction rules specific to each log format. Implement structured parsing for common log patterns (JSON, key-value pairs, stack traces). Use stop word lists to exclude noise terms from indexing.</p>\n<p><strong>Remediation</strong>: Rebuild indexes with improved term extraction rules. Implement query-time term filtering to reduce noise in existing indexes.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This subsection provides practical implementation details for building the log indexing engine, with complete starter code for infrastructure components and detailed guidance for core indexing logic.</p>\n<p><strong>Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Hash Functions</td>\n<td><code>hash/fnv</code> with multiple seeds</td>\n<td><code>github.com/cespare/xxhash</code></td>\n<td>FNV provides good distribution for simple cases; xxhash offers better performance for high-throughput</td>\n</tr>\n<tr>\n<td>Serialization</td>\n<td><code>encoding/gob</code> for index persistence</td>\n<td><code>google.golang.org/protobuf</code></td>\n<td>Gob is built-in and sufficient; protobuf enables schema evolution</td>\n</tr>\n<tr>\n<td>Compression</td>\n<td><code>compress/gzip</code> for postings lists</td>\n<td><code>github.com/klauspost/compress</code> variants</td>\n<td>Gzip balances simplicity and compression ratio; specialized libraries offer better performance</td>\n</tr>\n<tr>\n<td>Memory Management</td>\n<td>Built-in garbage collector</td>\n<td><code>sync.Pool</code> for frequent allocations</td>\n<td>GC handles most cases; object pooling reduces allocation overhead in hot paths</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/index/\n  index.go                 ← IndexSegment and core index types\n  index_test.go            ← comprehensive index functionality tests\n  bloom.go                 ← BloomFilter implementation\n  bloom_test.go            ← bloom filter correctness and performance tests\n  compaction.go            ← segment compaction and maintenance\n  compaction_test.go       ← compaction logic and concurrency tests\n  partition.go             ← time-based partitioning logic\n  partition_test.go        ← partition management and boundary tests\n  posting.go               ← PostingsList implementation with compression\n  posting_test.go          ← postings list correctness tests\n  catalog.go               ← partition catalog and metadata management\n  catalog_test.go          ← catalog functionality tests\n  \ninternal/index/storage/\n  segment_writer.go        ← index persistence and atomic writes\n  segment_reader.go        ← index loading and memory mapping\n  metadata.go              ← partition and segment metadata structures</code></pre></div>\n\n<p><strong>Infrastructure Starter Code</strong></p>\n<p>Here&#39;s complete, ready-to-use infrastructure code for bloom filters and basic index structures:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/index/bloom.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> index</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/binary</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">hash</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">hash/fnv</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">math</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// BloomParams holds configuration for bloom filter sizing and performance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> BloomParams</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ExpectedElements   </span><span style=\"color:#F97583\">uint32</span><span style=\"color:#6A737D\">  // Number of elements expected to be inserted</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    FalsePositiveRate </span><span style=\"color:#F97583\">float64</span><span style=\"color:#6A737D\">  // Target probability of false positives (0.01 = 1%)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BitArraySize      </span><span style=\"color:#F97583\">uint32</span><span style=\"color:#6A737D\">  // Calculated size of bit array in bits</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    HashCount         </span><span style=\"color:#F97583\">uint32</span><span style=\"color:#6A737D\">  // Number of hash functions to use</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewBloomParams calculates optimal bloom filter parameters for given constraints</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewBloomParams</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">expectedElements</span><span style=\"color:#F97583\"> uint32</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">falsePositiveRate</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">BloomParams</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Calculate optimal bit array size: m = -n * ln(p) / (ln(2)^2)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    n </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">(expectedElements)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    p </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> falsePositiveRate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> -</span><span style=\"color:#E1E4E8\">n </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> math.</span><span style=\"color:#B392F0\">Log</span><span style=\"color:#E1E4E8\">(p) </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (math.</span><span style=\"color:#B392F0\">Log</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> math.</span><span style=\"color:#B392F0\">Log</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Calculate optimal hash count: k = (m/n) * ln(2)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    k </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> (m </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> n) </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> math.</span><span style=\"color:#B392F0\">Log</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#B392F0\"> BloomParams</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ExpectedElements:   expectedElements,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        FalsePositiveRate: falsePositiveRate,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        BitArraySize:      </span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\">(math.</span><span style=\"color:#B392F0\">Ceil</span><span style=\"color:#E1E4E8\">(m)),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        HashCount:         </span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\">(math.</span><span style=\"color:#B392F0\">Ceil</span><span style=\"color:#E1E4E8\">(k)),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// BloomFilter implements a probabilistic set membership test</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> BloomFilter</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BitArray      []</span><span style=\"color:#F97583\">uint64</span><span style=\"color:#6A737D\">      // Packed bit array for filter storage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    HashFunctions []</span><span style=\"color:#B392F0\">hash</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Hash</span><span style=\"color:#6A737D\">   // Independent hash functions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Parameters    </span><span style=\"color:#B392F0\">BloomParams</span><span style=\"color:#6A737D\">   // Configuration used to create this filter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewBloomFilter creates an empty bloom filter with the given parameters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewBloomFilter</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">params</span><span style=\"color:#B392F0\"> BloomParams</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">BloomFilter</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Calculate number of uint64s needed for bit array</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    arraySize </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> (params.BitArraySize </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 63</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Create independent hash functions with different seeds</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hashFunctions </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#B392F0\">hash</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Hash</span><span style=\"color:#E1E4E8\">, params.HashCount)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> uint32</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">); i </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> params.HashCount; i</span><span style=\"color:#F97583\">++</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        hashFunctions[i] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> fnv.</span><span style=\"color:#B392F0\">New64a</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Seed each hash function differently</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        binary.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(hashFunctions[i], binary.LittleEndian, i</span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\">2654435761</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">BloomFilter</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        BitArray:      </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">uint64</span><span style=\"color:#E1E4E8\">, arraySize),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        HashFunctions: hashFunctions,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Parameters:    params,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Add inserts an element into the bloom filter</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">bf </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">BloomFilter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">element</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, hashFunc </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> bf.HashFunctions {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        hashFunc.</span><span style=\"color:#B392F0\">Reset</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        hashFunc.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">(element))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Get bit position from hash value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        bitPos </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> hashFunc.</span><span style=\"color:#B392F0\">Sum64</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">%</span><span style=\"color:#F97583\"> uint64</span><span style=\"color:#E1E4E8\">(bf.Parameters.BitArraySize)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Set the corresponding bit</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        arrayIndex </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> bitPos </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        bitIndex </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> bitPos </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> 64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        bf.BitArray[arrayIndex] </span><span style=\"color:#F97583\">|=</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#F97583\"> &#x3C;&#x3C;</span><span style=\"color:#E1E4E8\"> bitIndex)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MightContain returns true if the element might be in the set (with possible false positives)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Returns false if the element is definitely not in the set (no false negatives)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">bf </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">BloomFilter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">MightContain</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">element</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, hashFunc </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> bf.HashFunctions {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        hashFunc.</span><span style=\"color:#B392F0\">Reset</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        hashFunc.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">(element))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Get bit position from hash value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        bitPos </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> hashFunc.</span><span style=\"color:#B392F0\">Sum64</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">%</span><span style=\"color:#F97583\"> uint64</span><span style=\"color:#E1E4E8\">(bf.Parameters.BitArraySize)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Check if the corresponding bit is set</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        arrayIndex </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> bitPos </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        bitIndex </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> bitPos </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> 64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> (bf.BitArray[arrayIndex] </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#F97583\"> &#x3C;&#x3C;</span><span style=\"color:#E1E4E8\"> bitIndex)) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> false</span><span style=\"color:#6A737D\"> // Definitely not present</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> true</span><span style=\"color:#6A737D\"> // Might be present (could be false positive)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// EstimatedElementCount returns approximate number of elements added to the filter</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">bf </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">BloomFilter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">EstimatedElementCount</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Count number of set bits</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    setBits </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> uint32</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, word </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> bf.BitArray {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        setBits </span><span style=\"color:#F97583\">+=</span><span style=\"color:#F97583\"> uint32</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">popcount</span><span style=\"color:#E1E4E8\">(word))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Estimate elements using formula: n = -(m/k) * ln(1 - X/m)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // where X is number of set bits, m is total bits, k is hash count</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">(bf.Parameters.BitArraySize)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    k </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">(bf.Parameters.HashCount)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    x </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">(setBits)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> x </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> m {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> bf.Parameters.ExpectedElements </span><span style=\"color:#6A737D\">// Filter is saturated</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    estimated </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> -</span><span style=\"color:#E1E4E8\">(m </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> k) </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> math.</span><span style=\"color:#B392F0\">Log</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#F97583\"> -</span><span style=\"color:#E1E4E8\"> x</span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\">m)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> uint32</span><span style=\"color:#E1E4E8\">(estimated)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// popcount returns the number of set bits in a uint64</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> popcount</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">x</span><span style=\"color:#F97583\"> uint64</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Use built-in bit counting if available</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    count </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> x </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        count</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        x </span><span style=\"color:#F97583\">&#x26;=</span><span style=\"color:#E1E4E8\"> x </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\"> // Clear lowest set bit</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> count</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Core Index Structures</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/index/index.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> index</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sort</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// EntryReference points to a specific log entry within a storage chunk</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> EntryReference</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ChunkID   </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\">    // Identifier for the storage chunk</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Offset    </span><span style=\"color:#F97583\">uint32</span><span style=\"color:#6A737D\">    // Byte offset within the decompressed chunk</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#6A737D\"> // Entry timestamp for time-range filtering</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// PostingsList contains all references to log entries containing a specific term</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> PostingsList</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">EntryReference</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// IndexSegment represents an immutable index for a specific time window</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> IndexSegment</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SegmentID  </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\">                     // Unique identifier for this segment</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TimeRange  </span><span style=\"color:#B392F0\">TimeRange</span><span style=\"color:#6A737D\">                  // Time window covered by this segment</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Terms      </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PostingsList</span><span style=\"color:#6A737D\">   // Maps terms to postings lists</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BloomFilter </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">BloomFilter</span><span style=\"color:#6A737D\">              // Fast negative lookup for terms</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CreatedAt  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#6A737D\">                  // When this segment was created</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ChunkIDs   []</span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\">                   // Storage chunks referenced by this segment</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Internal state for thread safety</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu         </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span><span style=\"color:#6A737D\">               // Protects concurrent access during reads</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewIndexSegment creates a new empty index segment for the given time range</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewIndexSegment</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">segmentID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">timeRange</span><span style=\"color:#B392F0\"> TimeRange</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">expectedTerms</span><span style=\"color:#F97583\"> uint32</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">IndexSegment</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bloomParams </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> NewBloomParams</span><span style=\"color:#E1E4E8\">(expectedTerms, </span><span style=\"color:#79B8FF\">0.02</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#6A737D\">// 2% false positive rate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">IndexSegment</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        SegmentID:   segmentID,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TimeRange:   timeRange,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Terms:       </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PostingsList</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        BloomFilter: </span><span style=\"color:#B392F0\">NewBloomFilter</span><span style=\"color:#E1E4E8\">(bloomParams),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        CreatedAt:   time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ChunkIDs:    </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AddTerm adds a term->entry mapping to this index segment</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">seg </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">IndexSegment</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">AddTerm</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">term</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ref</span><span style=\"color:#B392F0\"> EntryReference</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    seg.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> seg.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Add to bloom filter for fast negative lookups</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    seg.BloomFilter.</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(term)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Get or create postings list for this term</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    postings, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> seg.Terms[term]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">exists {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        postings </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">PostingsList</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        seg.Terms[term] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> postings</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Insert entry reference maintaining timestamp ordering</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    *</span><span style=\"color:#E1E4E8\">postings </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">postings, ref)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Implement efficient sorted insertion for large postings lists</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LookupTerm returns the postings list for a term, or nil if not found</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">seg </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">IndexSegment</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">LookupTerm</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">term</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PostingsList</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    seg.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> seg.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Fast negative lookup using bloom filter</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">seg.BloomFilter.</span><span style=\"color:#B392F0\">MightContain</span><span style=\"color:#E1E4E8\">(term) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#6A737D\"> // Definitely not present</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Bloom filter says might be present, check actual index</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    postings, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> seg.Terms[term]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">exists {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#6A737D\"> // False positive from bloom filter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> postings</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Finalize prepares the segment for read-only access by sorting postings lists</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">seg </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">IndexSegment</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Finalize</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    seg.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> seg.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Sort all postings lists by timestamp for efficient time-range queries</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, postings </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> seg.Terms {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sort.</span><span style=\"color:#B392F0\">Slice</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">postings, </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">i</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">j</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">postings)[i].Timestamp.</span><span style=\"color:#B392F0\">Before</span><span style=\"color:#E1E4E8\">((</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">postings)[j].Timestamp)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Core Logic Skeletons</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// BuildIndexSegment constructs an index segment from a collection of log entries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// This is the core indexing logic that learners should implement</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> BuildIndexSegment</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">segmentID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">timeRange</span><span style=\"color:#B392F0\"> TimeRange</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">entries</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">IndexSegment</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Estimate number of unique terms by sampling entries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Sample 10% of entries and count unique terms, then extrapolate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create new index segment with estimated term count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use NewIndexSegment with the estimated term count for bloom filter sizing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Extract all terms from each log entry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Process labels (key and key=value pairs) and message content</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Consider: How to tokenize message content? What about JSON within messages?</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Add each term->entry mapping to the segment</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use AddTerm for each extracted term with appropriate EntryReference</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Finalize the segment to prepare for querying</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Call Finalize() to sort postings lists by timestamp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Track which storage chunks are referenced by this segment</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Collect unique ChunkIDs from all EntryReferences</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#6A737D\"> // Remove when implementing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CompactSegments merges multiple index segments into a single optimized segment</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// This implements the core compaction logic for storage efficiency</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> CompactSegments</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">sourceSegments</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">IndexSegment</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">outputSegmentID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">IndexSegment</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Calculate time range spanning all source segments</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Find minimum start time and maximum end time across all segments</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Estimate term count for the merged segment</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use bloom filter estimates, but account for term overlap between segments</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Create output segment with appropriate sizing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use estimated term count for bloom filter parameters</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Implement multi-way merge of terms across all source segments</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Iterate through all unique terms, merging postings lists from multiple sources</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Consider: How to handle duplicate entries? How to maintain timestamp ordering?</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Merge postings lists while maintaining temporal ordering</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use priority queue or sorted merge algorithm for combining timestamp-ordered lists</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Deduplicate entries that appear in multiple source segments</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Check for identical ChunkID+Offset combinations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Update bloom filter with all terms from merged segment</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Add each unique term to ensure bloom filter accuracy</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#6A737D\"> // Remove when implementing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// PartitionQuery determines which index segments to search for a given query</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// This implements the query planning logic for time-based partitioning</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> PartitionQuery</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">query</span><span style=\"color:#B392F0\"> Query</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">availableSegments</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">IndexSegment</span><span style=\"color:#E1E4E8\">) []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">IndexSegment</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Extract time range from the query</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Parse query for time range constraints, use default if none specified</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Filter segments that overlap with query time range</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use TimeRange.Overlaps() method to check for intersection</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Use bloom filters to eliminate segments that definitely don't contain query terms</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: For each segment, check if bloom filter indicates presence of any query terms</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Sort segments by relevance/access cost</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Prefer smaller, more recent segments for better query performance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Apply segment limits to prevent unbounded query execution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Limit to reasonable number of segments (50-100) even for broad time ranges</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#6A737D\"> // Remove when implementing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint</strong></p>\n<p>After implementing the index engine, verify correct behavior with these tests:</p>\n<p><strong>Test Index Creation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/index/...</span><span style=\"color:#79B8FF\"> -run</span><span style=\"color:#9ECBFF\"> TestIndexSegment</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should see: All postings lists properly sorted by timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should see: Bloom filter false positive rate within 5% of target</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should see: Term extraction covers both labels and message content</span></span></code></pre></div>\n\n<p><strong>Test Compaction:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/index/...</span><span style=\"color:#79B8FF\"> -run</span><span style=\"color:#9ECBFF\"> TestCompaction</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should see: Merged segments contain all terms from source segments  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should see: No duplicate entries in merged postings lists</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should see: Merged segment size smaller than sum of source segments</span></span></code></pre></div>\n\n<p><strong>Verify Query Planning:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Create test segments covering different time ranges</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Query with specific time range should only return overlapping segments</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Query with terms not in bloom filter should eliminate segments quickly</span></span></code></pre></div>\n\n<p><strong>Signs of Correct Implementation:</strong></p>\n<ul>\n<li>Index segments build in reasonable time (&lt; 1 second for 10K entries)</li>\n<li>Bloom filter eliminates 70%+ of negative lookups</li>\n<li>Compaction reduces storage size by 20-40% </li>\n<li>Time-range queries only access relevant segments</li>\n</ul>\n<p><strong>Common Implementation Issues:</strong></p>\n<ul>\n<li><p><strong>Symptom</strong>: Bloom filter false positive rate much higher than expected\n<strong>Cause</strong>: Incorrect hash function implementation or bit array sizing\n<strong>Fix</strong>: Verify hash function independence and bit array calculations</p>\n</li>\n<li><p><strong>Symptom</strong>: Queries return incomplete results\n<strong>Cause</strong>: Postings lists not properly sorted or time range filtering incorrect<br><strong>Fix</strong>: Verify timestamp ordering in postings lists and time range overlap logic</p>\n</li>\n<li><p><strong>Symptom</strong>: Memory usage grows unboundedly during indexing\n<strong>Cause</strong>: Index structures not being finalized or large strings retained\n<strong>Fix</strong>: Call Finalize() after building segments and avoid retaining large string slices</p>\n</li>\n</ul>\n<h2 id=\"query-engine\">Query Engine</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section corresponds to Milestone 3 (Log Query Engine), where we implement LogQL-style querying with full-text search, label filtering, and result processing. This milestone builds on the ingestion capabilities from Milestone 1 and indexing infrastructure from Milestone 2.</p>\n</blockquote>\n<h3 id=\"mental-model-the-research-assistant\">Mental Model: The Research Assistant</h3>\n<p>Before diving into the technical implementation of query processing, let&#39;s establish an intuitive understanding through the research assistant analogy. Imagine you&#39;re working with a highly skilled research assistant who helps you find information from a vast library of documents.</p>\n<p>When you approach your research assistant with a request like &quot;find all documents from 2023 that mention &#39;database errors&#39; and were written by the engineering team,&quot; the assistant doesn&#39;t randomly start pulling books off shelves. Instead, they follow a systematic process that mirrors how our query engine operates.</p>\n<p>First, the assistant <strong>parses your request</strong> to understand exactly what you&#39;re asking for. They break down your natural language request into structured components: time range (2023), keywords (database errors), and metadata filters (engineering team). This is analogous to our query language parser transforming a LogQL query into an Abstract Syntax Tree (AST).</p>\n<p>Next, the assistant <strong>plans the search strategy</strong>. They consider which card catalogs to check first, whether to start with the time-based filing system or the subject index, and how to combine multiple search criteria efficiently. Our query planner performs similar optimization, deciding which indexes to use and in what order to execute filters for maximum efficiency.</p>\n<p>During <strong>execution</strong>, the assistant doesn&#39;t just grab every potentially relevant document. They use the card catalog system (our inverted index) to quickly identify candidate documents, then use quick screening techniques (our bloom filters) to eliminate obvious non-matches before diving into detailed examination. They understand that some search strategies are faster than others—checking the author index first when looking for a specific writer, or using the chronological filing system when time ranges are involved.</p>\n<p>Finally, the assistant <strong>presents results</strong> in a useful format. They don&#39;t dump a pile of documents on your desk; instead, they organize findings, provide summaries, and can give you results in manageable batches if there are many matches. Similarly, our query engine handles result ranking, pagination, and streaming responses.</p>\n<p>The key insight from this analogy is that effective querying is about <strong>intelligent navigation</strong> rather than brute force searching. Just as a good research assistant leverages library organization systems and applies domain knowledge to find information efficiently, our query engine must understand log data patterns, utilize index structures effectively, and execute searches in an order that minimizes unnecessary work.</p>\n<h3 id=\"query-language-parser\">Query Language Parser</h3>\n<p>The query language parser serves as the entry point for all query processing, responsible for transforming user-supplied LogQL queries into structured representations that our execution engine can process efficiently. LogQL, inspired by Grafana Loki&#39;s query language, provides a balance between expressiveness and simplicity that makes it accessible to operations teams while remaining powerful enough for complex log analysis tasks.</p>\n<p>The parser architecture follows a traditional compiler design pattern with distinct lexical analysis, parsing, and validation phases. The <strong>lexical analyzer</strong> (tokenizer) breaks the input string into meaningful tokens, identifying operators, identifiers, string literals, regular expressions, and keywords. The <strong>parser</strong> consumes these tokens to build an Abstract Syntax Tree (AST) that represents the query&#39;s logical structure. Finally, the <strong>validator</strong> performs semantic analysis to ensure the query is well-formed and can be executed given our system&#39;s capabilities.</p>\n<p>LogQL queries follow a pipe-based syntax that mirrors the mental model of data flowing through transformation stages. A typical query might look like <code>{service=&quot;api&quot;} |= &quot;error&quot; | json | level=&quot;ERROR&quot; | line_format &quot;{{.timestamp}} {{.message}}&quot;</code>. This query demonstrates the key components: label selectors in curly braces, line filters using operators like <code>|=</code>, parsed extractors like <code>json</code>, and formatting functions.</p>\n<p>The <strong>label selector</strong> component appears at the beginning of every query and specifies which log streams to consider. Label selectors support exact matching (<code>service=&quot;api&quot;</code>), regular expression matching (<code>service=~&quot;api.*&quot;</code>), negative matching (<code>service!=&quot;debug&quot;</code>), and complex boolean combinations using comma (AND) and pipe (OR) operators. The parser must handle proper precedence and grouping while validating that label names conform to our naming conventions.</p>\n<p><strong>Line filters</strong> operate on the actual log message content and support several operators. The <code>|=</code> operator performs substring matching, <code>!~</code> applies regular expression matching with negation, <code>|~</code> provides positive regex matching, and <code>!=</code> excludes lines containing specific substrings. The parser must correctly identify these operators and handle proper escaping of special characters in string literals and regular expressions.</p>\n<p><strong>Log parsers</strong> like <code>json</code>, <code>logfmt</code>, and <code>regexp</code> extract structured data from unstructured log messages. The JSON parser automatically extracts all key-value pairs from JSON-formatted log lines, making them available as labels for subsequent filtering. The logfmt parser handles the structured logging format used by many Go applications. The regexp parser allows custom field extraction using named capture groups. Each parser type requires different validation rules and parameter handling.</p>\n<p><strong>Aggregation functions</strong> enable statistical analysis over log data, including <code>count_over_time()</code>, <code>rate()</code>, <code>sum(rate())</code>, and <code>avg_over_time()</code>. These functions operate over time windows and can be grouped by extracted labels. The parser must validate function signatures, ensure proper time window syntax, and verify that grouping expressions reference valid label names.</p>\n<blockquote>\n<p><strong>Decision: LogQL Syntax Choice</strong></p>\n<ul>\n<li><strong>Context</strong>: We needed to choose a query language syntax that balances usability with implementation complexity while providing sufficient expressiveness for log analysis tasks.</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>SQL-like syntax with traditional SELECT/FROM/WHERE clauses</li>\n<li>Elasticsearch Query DSL with nested JSON structures  </li>\n<li>LogQL pipe-based syntax similar to shell command chaining</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement LogQL pipe-based syntax with modifications for our specific use cases</li>\n<li><strong>Rationale</strong>: The pipe-based approach matches operations teams&#39; mental model of data transformation pipelines, reduces cognitive load compared to complex JSON structures, and provides clear execution semantics. SQL syntax, while familiar, doesn&#39;t map naturally to log streaming concepts and would require significant semantic extensions.</li>\n<li><strong>Consequences</strong>: This choice simplifies incremental query building and debugging but requires custom parser implementation and limits compatibility with existing SQL tooling.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Parser Component</th>\n<th>Input Format</th>\n<th>Output Structure</th>\n<th>Validation Rules</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Label Selector</td>\n<td><code>{service=&quot;api&quot;, level!=&quot;debug&quot;}</code></td>\n<td><code>LabelMatcher[]</code></td>\n<td>Label names must be valid identifiers, values properly quoted</td>\n</tr>\n<tr>\n<td>Line Filter</td>\n<td><code>|= &quot;error message&quot;</code></td>\n<td><code>LineFilter{Operator, Pattern}</code></td>\n<td>Regex patterns must compile, string escaping handled correctly</td>\n</tr>\n<tr>\n<td>JSON Parser</td>\n<td><code>| json</code></td>\n<td><code>JSONExtractor{Fields}</code></td>\n<td>No parameters required, validates JSON structure at execution</td>\n</tr>\n<tr>\n<td>LogFmt Parser</td>\n<td><code>| logfmt</code></td>\n<td><code>LogFmtExtractor{}</code></td>\n<td>No parameters, handles key=value parsing</td>\n</tr>\n<tr>\n<td>Regex Parser</td>\n<td><code>| regexp &quot;(?P&lt;level&gt;\\\\w+)&quot;</code></td>\n<td><code>RegexExtractor{Pattern, Groups}</code></td>\n<td>Pattern must compile, named groups required for extraction</td>\n</tr>\n<tr>\n<td>Range Query</td>\n<td><code>[5m]</code></td>\n<td><code>TimeWindow{Duration}</code></td>\n<td>Duration must be positive, supports s/m/h/d units</td>\n</tr>\n<tr>\n<td>Aggregation</td>\n<td><code>sum(rate({app=&quot;web&quot;}[5m]))</code></td>\n<td><code>AggregateExpr{Func, Expr, Grouping}</code></td>\n<td>Function exists, expression type compatible</td>\n</tr>\n</tbody></table>\n<p>The Abstract Syntax Tree representation uses a visitor pattern to enable different processing phases (validation, optimization, execution) to traverse the same structure without tight coupling. Each AST node implements a common <code>QueryNode</code> interface with methods for type identification, child enumeration, and visitor acceptance.</p>\n<p><strong>Error handling</strong> in the parser focuses on providing actionable feedback to users. Common syntax errors include unmatched braces in label selectors, invalid regular expressions in filters, and malformed time duration specifications. The parser maintains position information for all tokens, enabling precise error location reporting. When encountering syntax errors, the parser attempts error recovery to identify multiple issues in a single parse pass rather than failing on the first error encountered.</p>\n<p><strong>Query validation</strong> occurs after successful parsing and verifies semantic correctness. The validator checks that all referenced label names exist in our label catalog, ensures aggregation functions receive compatible input types, validates that time ranges are reasonable (not negative, not extending beyond available data retention), and confirms that regular expressions compile successfully. This validation phase prevents runtime errors and provides early feedback about query correctness.</p>\n<p>The parser implementation must handle <strong>precedence and associativity</strong> correctly, particularly in complex label selectors with mixed AND/OR operations and in mathematical expressions within aggregation functions. Left-to-right evaluation within pipe stages provides predictable semantics, while proper operator precedence in boolean expressions prevents surprising query behavior.</p>\n<h3 id=\"query-planning-and-optimization\">Query Planning and Optimization</h3>\n<p>Query planning transforms the validated AST into an executable query plan that minimizes resource consumption while ensuring correct results. The planner&#39;s primary responsibilities include determining optimal execution order, identifying opportunities for predicate pushdown, selecting appropriate indexes, and estimating resource requirements to prevent runaway queries.</p>\n<p>The <strong>cost-based optimization</strong> approach relies on statistics about our data distribution, index selectivity, and historical query performance. The planner maintains metadata about label cardinality (how many unique values exist for each label), time-based data distribution, and index effectiveness for different query patterns. This statistical foundation enables intelligent decisions about execution strategy rather than relying on hard-coded heuristics.</p>\n<p><strong>Predicate pushdown</strong> represents one of the most critical optimizations in log query processing. The goal is to apply the most selective filters as early as possible in the execution pipeline, reducing the volume of data that subsequent operations must process. Label-based filters typically offer the highest selectivity and can leverage our inverted indexes, so they should execute before line filters that require reading actual log content. Regular expression filters are computationally expensive and should be delayed until after cheaper substring filters have reduced the candidate set.</p>\n<p>The planner analyzes the AST to identify <strong>pushdown opportunities</strong>. Label selectors naturally push down to the index lookup phase, where they can dramatically reduce the set of log streams under consideration. Time range filters integrate with our time-based partitioning to eliminate entire chunks from consideration. Line filters containing simple substring matches can sometimes leverage bloom filters for negative lookups, while complex regular expressions must wait until log content retrieval.</p>\n<p><strong>Index selection</strong> involves choosing which indexes to use and in what order to apply them. Our system maintains several index types: the primary inverted index mapping terms to log entries, label-specific indexes for high-cardinality labels, and time-based partition metadata. The planner must decide whether to start with label-based lookup and intersect with text search results, or begin with text search and filter by labels afterward. This decision depends on the estimated selectivity of each filter component.</p>\n<p>For queries involving multiple label filters, the planner determines the optimal join order by estimating the cardinality of intermediate results. Starting with the most selective label filter minimizes the working set size for subsequent operations. The planner uses histogram data about label value distributions to estimate filter selectivity, preferring filters that match fewer streams over those that match many.</p>\n<p><strong>Time-based optimization</strong> leverages our partitioned storage architecture to minimize data access. Queries with explicit time ranges can skip entire partitions that fall outside the specified window. The planner can also apply time-based optimizations to queries without explicit time constraints by limiting the search to recent partitions first, allowing for early termination when sufficient results are found.</p>\n<blockquote>\n<p><strong>Decision: Cost-Based vs Rule-Based Optimization</strong></p>\n<ul>\n<li><strong>Context</strong>: Query optimization requires deciding between rule-based heuristics (always apply certain optimizations) versus cost-based analysis (estimate costs and choose optimal plan).</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Simple rule-based system with fixed optimization patterns</li>\n<li>Cost-based optimizer using statistics and cardinality estimation</li>\n<li>Hybrid approach with rules for common cases and cost-based for complex queries</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement cost-based optimization with fallback rules for cases with insufficient statistics</li>\n<li><strong>Rationale</strong>: Log data characteristics vary dramatically between deployments (some have high label cardinality, others have mostly unstructured text). Cost-based optimization adapts to actual data patterns rather than assuming universal characteristics. The statistical foundation also enables automatic performance improvement as the system learns from query patterns.</li>\n<li><strong>Consequences</strong>: Requires maintaining statistics collection and storage, increasing system complexity. However, this investment pays dividends in query performance, especially for complex queries over large datasets.</li>\n</ul>\n</blockquote>\n<p>The <strong>resource estimation</strong> component predicts memory and CPU requirements for query execution, enabling proactive rejection of queries that would consume excessive resources. The planner estimates the working set size based on expected intermediate result cardinality, accounting for memory requirements of sorting, aggregation, and result buffering operations. Queries exceeding configurable resource limits are rejected with explanatory error messages.</p>\n<p><strong>Query plan representation</strong> uses a directed acyclic graph (DAG) structure that captures dependencies between operations while enabling parallel execution where possible. Each node in the plan represents a processing operation (index lookup, text filtering, aggregation) along with estimated costs and resource requirements. Edges represent data flow between operations, with annotations indicating expected data volume and selectivity.</p>\n<table>\n<thead>\n<tr>\n<th>Plan Node Type</th>\n<th>Function</th>\n<th>Input Requirements</th>\n<th>Output Characteristics</th>\n<th>Cost Factors</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>IndexLookup</td>\n<td>Find log entries matching labels</td>\n<td>Label selector expression</td>\n<td>Ordered list of entry references</td>\n<td>Index size, label cardinality</td>\n</tr>\n<tr>\n<td>TimeFilter</td>\n<td>Restrict results to time range</td>\n<td>Entry references, time bounds</td>\n<td>Filtered entry references</td>\n<td>Partition scan cost, time selectivity</td>\n</tr>\n<tr>\n<td>TextFilter</td>\n<td>Apply line filters to log content</td>\n<td>Entry references, filter patterns</td>\n<td>Matching entries with content</td>\n<td>Regex complexity, data volume</td>\n</tr>\n<tr>\n<td>JSONExtractor</td>\n<td>Parse JSON and extract fields</td>\n<td>Log entries with JSON content</td>\n<td>Entries with extracted labels</td>\n<td>JSON parsing overhead, field count</td>\n</tr>\n<tr>\n<td>Aggregator</td>\n<td>Compute metrics over time windows</td>\n<td>Timestamped entries, grouping spec</td>\n<td>Aggregated time series</td>\n<td>Memory for intermediate state, group cardinality</td>\n</tr>\n<tr>\n<td>Sorter</td>\n<td>Order results by timestamp/relevance</td>\n<td>Unordered result set</td>\n<td>Ordered result stream</td>\n<td>Memory for sorting buffer, result count</td>\n</tr>\n<tr>\n<td>Limiter</td>\n<td>Restrict result count</td>\n<td>Result stream, limit specification</td>\n<td>Truncated result stream</td>\n<td>Negligible, enables early termination</td>\n</tr>\n</tbody></table>\n<p><strong>Plan caching</strong> stores optimized plans for frequently executed queries, avoiding repeated optimization overhead. The cache uses query structure hashing to identify equivalent queries regardless of minor syntactic differences. Cached plans include freshness metadata and are invalidated when statistics changes suggest that optimization assumptions may no longer hold.</p>\n<p>The <strong>parallel execution analysis</strong> identifies operations that can run concurrently without dependencies. Index lookups for different label conditions can proceed in parallel, with results intersected afterward. Text filtering operations can be parallelized across multiple log chunks when sufficient CPU resources are available. The planner annotates the execution graph with parallelization opportunities while respecting resource constraints.</p>\n<p><strong>Statistics maintenance</strong> requires ongoing collection of data distribution metrics to keep optimization decisions accurate. The planner tracks query execution times, intermediate result sizes, and filter selectivity for different query patterns. This feedback loop enables continuous improvement of cost estimation models and helps identify opportunities for new index structures or caching strategies.</p>\n<h3 id=\"search-execution-engine\">Search Execution Engine</h3>\n<p>The search execution engine transforms optimized query plans into concrete results by coordinating index lookups, content filtering, and result aggregation. The execution engine must balance throughput with resource consumption while maintaining predictable query response times even under varying system load conditions.</p>\n<p><strong>Execution scheduling</strong> follows the dependency graph established during query planning, ensuring that each operation receives properly prepared input data. The scheduler maintains separate execution contexts for different query components, enabling isolation between concurrent queries and providing mechanisms for query cancellation and timeout enforcement. Each execution context tracks resource usage and can trigger early termination when resource limits are approached.</p>\n<p>The <strong>index lookup coordinator</strong> serves as the primary interface to our indexing infrastructure, translating label selectors and text filters into specific index operations. For label-based queries, the coordinator determines which index segments require scanning based on time ranges and label cardinality estimates. The coordinator implements smart batching to group related index operations and reduce I/O overhead when accessing disk-based index structures.</p>\n<p>Label selector execution begins by identifying the most selective label constraint and using it to seed the candidate set. Subsequent label filters are applied as intersection operations, progressively narrowing the result set. The coordinator maintains intermediate results in memory when possible, spilling to temporary storage for large intermediate sets that exceed available memory capacity.</p>\n<p><strong>Bloom filter integration</strong> provides significant performance improvements for negative lookups. When a query contains exclusion filters (label != &quot;value&quot; or line !~ &quot;pattern&quot;), the execution engine first checks bloom filters associated with relevant index segments. If the bloom filter indicates that a term is definitely not present in a segment, the entire segment can be skipped without accessing its detailed index structures.</p>\n<p>The text search component handles full-text queries and regular expression matching against log message content. For simple substring searches, the engine can often use index-based approaches when the search terms appear in our inverted index. Complex regular expression queries require retrieving log content and applying pattern matching directly, which is computationally expensive but necessary for flexible query capabilities.</p>\n<p><strong>Streaming execution</strong> processes results incrementally rather than materializing complete result sets in memory. This approach enables response streaming for large queries and provides better responsiveness for interactive use cases. The streaming model requires careful coordination between execution stages to maintain proper backpressure and avoid overwhelming downstream consumers with result data.</p>\n<p>Log content retrieval represents a critical performance bottleneck that the execution engine must manage carefully. Rather than fetching log entries individually, the engine groups retrieval requests by storage location (chunk) and batch-loads multiple entries simultaneously. This batching approach amortizes I/O costs while maintaining reasonable memory overhead for the retrieved content.</p>\n<p><strong>Regular expression optimization</strong> applies several techniques to improve pattern matching performance. The engine compiles regex patterns once and reuses compiled representations across multiple matching operations. For patterns that contain fixed string prefixes, the engine can use string searching to identify candidates before applying the full regular expression. Complex patterns that would consume excessive CPU time are subject to timeout limits and early termination.</p>\n<p>Result aggregation requires managing intermediate state for mathematical computations over time-windowed data. The aggregation engine groups log entries by specified label combinations and maintains running calculations (sums, counts, averages) within specified time windows. Memory usage is controlled through intelligent batching and periodic flushing of completed time windows to the result stream.</p>\n<blockquote>\n<p><strong>Decision: Streaming vs Batch Execution Model</strong></p>\n<ul>\n<li><strong>Context</strong>: Query execution can either process all data before returning results (batch) or stream results incrementally as they become available.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Pure batch processing with complete result materialization</li>\n<li>Pure streaming with immediate result forwarding</li>\n<li>Hybrid approach with configurable batching thresholds</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement streaming execution with intelligent batching for efficiency</li>\n<li><strong>Rationale</strong>: Streaming provides better user experience for interactive queries and enables processing datasets larger than available memory. However, pure streaming can create excessive overhead for small result sets, so intelligent batching captures efficiency benefits while maintaining streaming semantics.</li>\n<li><strong>Consequences</strong>: Increases implementation complexity due to backpressure handling and partial result management, but provides superior scalability and responsiveness characteristics.</li>\n</ul>\n</blockquote>\n<p>The <strong>memory management</strong> system tracks resource usage across all active query operations and implements priority-based eviction when memory pressure increases. Query operations are classified by resource requirements and execution priority, with long-running analytical queries yielding resources to interactive dashboard queries when necessary. The memory manager can trigger query cancellation for runaway operations that exceed configured limits.</p>\n<p><strong>Error recovery</strong> mechanisms handle various failure scenarios that can occur during query execution. Temporary I/O errors during index or storage access trigger automatic retry with exponential backoff. Malformed log entries that cannot be parsed are logged but do not terminate query execution. Resource exhaustion conditions trigger graceful degradation with partial results rather than complete failure.</p>\n<p>The execution engine implements <strong>query cancellation</strong> support to enable users to terminate long-running queries without consuming unnecessary resources. Cancellation requests propagate through the execution graph, causing individual operations to clean up their state and terminate processing. The cancellation mechanism respects transactional boundaries where applicable and ensures that partially completed operations do not leave inconsistent state.</p>\n<p><strong>Parallel execution</strong> leverages available CPU cores for operations that can be safely parallelized. Index lookups across multiple segments can proceed concurrently, with results merged using timestamp-based ordering. Text filtering operations can be distributed across worker threads, with each thread processing a subset of candidate log entries. The parallel execution system includes dynamic load balancing to ensure efficient CPU utilization even when processing non-uniform data distributions.</p>\n<p>Result ranking applies relevance scoring when queries do not specify explicit ordering requirements. The ranking algorithm considers factors such as timestamp recency, label matching quality, and text search relevance scores. For queries with explicit sorting requirements, the execution engine implements efficient external sorting algorithms that can handle result sets larger than available memory.</p>\n<h3 id=\"result-processing-and-pagination\">Result Processing and Pagination</h3>\n<p>Result processing transforms raw execution output into properly formatted, ordered, and paginated responses suitable for client consumption. This component must handle diverse output requirements while maintaining efficient resource utilization and providing consistent performance characteristics regardless of result set size.</p>\n<p>The <strong>result formatting</strong> system adapts execution engine output to match client requirements and query specifications. LogQL queries can specify output formatting through functions like <code>line_format</code> and <code>label_format</code>, which require template-based text processing applied to each result entry. The formatter supports variable substitution, conditional formatting, and basic string manipulation functions while maintaining high throughput for large result sets.</p>\n<p>JSON output formatting requires careful handling of data types and nested structures, particularly when queries extract structured data from log messages. The formatter must preserve type information for numeric values, handle special characters properly in string values, and maintain consistent field ordering for client parsing reliability. For queries that extract many fields, the formatter implements field filtering to include only requested attributes in the output.</p>\n<p><strong>Result ordering</strong> implementation varies based on query characteristics and client requirements. Time-based ordering (the default) leverages the natural time-ordering of log data and our time-partitioned storage architecture. Relevance-based ordering requires computing score values during execution and maintaining sorted intermediate results. Custom ordering based on extracted fields requires additional sorting passes that can impact query performance for large result sets.</p>\n<p>The pagination system provides efficient mechanisms for clients to retrieve large result sets in manageable chunks without requiring server-side state maintenance between requests. <strong>Cursor-based pagination</strong> uses opaque tokens that encode position information, enabling clients to resume result retrieval from specific points without server-side session storage. The cursor format includes timestamp information, chunk identifiers, and offset values needed to reconstruct query position.</p>\n<p>For time-based queries, pagination cursors encode temporal boundaries that align with our storage partitioning scheme. This alignment enables efficient seek operations when resuming pagination, as the system can jump directly to relevant storage chunks without scanning from the beginning of results. The cursor encoding includes validation information to detect attempts to use stale or manipulated cursor values.</p>\n<p><strong>Streaming response</strong> delivery enables real-time result consumption for clients that need immediate access to query results. The streaming implementation uses chunked transfer encoding for HTTP clients and maintains WebSocket connections for interactive applications. Streaming responses include proper error handling and connection management to gracefully handle client disconnections and network interruptions.</p>\n<p>Result count estimation provides clients with approximate result set sizes for pagination and progress indication purposes. The estimation algorithm uses index statistics and sampling techniques to provide reasonably accurate counts without requiring full query execution. For queries with expensive filtering operations, estimation may be less accurate but still provides useful order-of-magnitude information.</p>\n<p><strong>Result caching</strong> stores frequently accessed query results to reduce execution costs for repeated queries. The caching system uses query fingerprinting to identify equivalent queries and implements time-based invalidation to ensure result freshness. Cache entries include metadata about data freshness and can serve partial results for queries that overlap with cached content while fetching additional data for uncached portions.</p>\n<p>The result processor implements <strong>rate limiting</strong> mechanisms to prevent individual queries from overwhelming client connections or consuming excessive bandwidth. Rate limiting operates at multiple levels: per-query limits prevent runaway result generation, per-client limits ensure fair resource sharing, and global limits protect system stability. Rate limit headers inform clients about current limitations and provide guidance for retry behavior.</p>\n<table>\n<thead>\n<tr>\n<th>Result Format</th>\n<th>Use Case</th>\n<th>Performance Characteristics</th>\n<th>Special Considerations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>JSON Lines</td>\n<td>Machine processing, streaming</td>\n<td>High throughput, minimal overhead</td>\n<td>Maintains temporal ordering, one object per line</td>\n</tr>\n<tr>\n<td>Structured JSON</td>\n<td>API responses, web interfaces</td>\n<td>Moderate overhead for structure</td>\n<td>Includes metadata, pagination info, result arrays</td>\n</tr>\n<tr>\n<td>CSV</td>\n<td>Data export, analytical tools</td>\n<td>High throughput, compact size</td>\n<td>Schema consistency required, escaping for special chars</td>\n</tr>\n<tr>\n<td>LogFmt</td>\n<td>Operational dashboards</td>\n<td>Low parsing overhead</td>\n<td>Key-value format, natural for structured logs</td>\n</tr>\n<tr>\n<td>Raw Text</td>\n<td>Human readability, debugging</td>\n<td>Minimal processing required</td>\n<td>No structured data, relies on log message formatting</td>\n</tr>\n</tbody></table>\n<p><strong>Memory management</strong> for result processing requires careful attention to resource consumption patterns, as large result sets can quickly exhaust available memory if not handled properly. The result processor implements streaming patterns that maintain bounded memory usage regardless of result set size, using fixed-size buffers and flow control mechanisms to coordinate with upstream execution components.</p>\n<p>The <strong>error handling</strong> strategy for result processing focuses on partial success scenarios where some results can be delivered despite encountering errors in portions of the query execution. The processor distinguishes between fatal errors that prevent any result delivery and partial errors that affect subset of results. Partial errors are reported through error metadata included with successful results, enabling clients to understand data completeness.</p>\n<p><strong>Compression</strong> for large result sets reduces network bandwidth requirements and improves client performance for batch-oriented use cases. The result processor supports multiple compression algorithms (gzip, deflate, brotli) and automatically selects appropriate compression based on client capabilities and result characteristics. Streaming compression maintains low latency characteristics while providing bandwidth benefits.</p>\n<p>Result validation ensures that all delivered results conform to expected schemas and contain required fields based on the original query specification. The validation system checks for missing timestamps, invalid label values, and malformed extracted fields. Validation failures trigger error responses rather than delivering corrupt data to clients.</p>\n<h3 id=\"architecture-decision-records\">Architecture Decision Records</h3>\n<p>The architecture decisions for the query engine reflect careful consideration of performance, usability, and implementation complexity trade-offs. Each decision impacts multiple aspects of system behavior and requires understanding of both immediate implementation requirements and long-term scalability considerations.</p>\n<blockquote>\n<p><strong>Decision: AST-Based Query Representation</strong></p>\n<ul>\n<li><strong>Context</strong>: Query processing requires internal representation that supports optimization, validation, and execution phases while maintaining clarity and extensibility.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Direct execution from parsed tokens without intermediate representation</li>\n<li>AST-based representation with visitor pattern for different processing phases</li>\n<li>Bytecode compilation to virtual machine instructions</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement AST-based representation with visitor pattern support</li>\n<li><strong>Rationale</strong>: AST provides clear separation between parsing, optimization, and execution phases, enabling independent evolution of each component. Visitor pattern allows adding new processing phases (optimization passes, alternative execution engines) without modifying existing node types. Direct token execution would be simpler but prevents optimization opportunities, while bytecode compilation adds complexity that exceeds current performance requirements.</li>\n<li><strong>Consequences</strong>: Requires more upfront design effort and memory overhead for AST storage, but provides flexibility for future enhancements and clear debugging capabilities through tree visualization.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Push-Down Optimization Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Query performance depends critically on minimizing data access by applying filters as early as possible in the execution pipeline.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Fixed execution order based on query syntax ordering</li>\n<li>Cost-based reordering with predicate pushdown optimization</li>\n<li>Adaptive execution with runtime optimization adjustments</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement cost-based predicate pushdown with static optimization</li>\n<li><strong>Rationale</strong>: Log query performance is dominated by I/O costs for accessing log content, making early filtering crucial for acceptable response times. Cost-based optimization leverages our index statistics to make intelligent filtering decisions. Adaptive runtime optimization would provide better theoretical performance but requires significant complexity that may not be justified by practical performance gains.</li>\n<li><strong>Consequences</strong>: Requires maintaining detailed statistics about label cardinality and index selectivity, increasing system complexity. However, provides dramatic performance improvements for complex queries over large datasets.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Streaming Execution Architecture</strong></p>\n<ul>\n<li><strong>Context</strong>: Query results can range from small interactive lookups to large analytical queries that return millions of entries, requiring scalable result delivery mechanisms.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Full result materialization with batch delivery</li>\n<li>Pure streaming with immediate result forwarding</li>\n<li>Hybrid approach with intelligent buffering thresholds</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement streaming execution with configurable buffering</li>\n<li><strong>Rationale</strong>: Streaming enables processing queries larger than available memory while providing better responsiveness for interactive use cases. Configurable buffering captures efficiency benefits for small queries while maintaining streaming benefits for large ones. Full materialization would limit scalability, while pure streaming might create excessive overhead for small result sets.</li>\n<li><strong>Consequences</strong>: Increases implementation complexity due to backpressure management and error handling in streaming contexts, but provides superior scalability characteristics and better user experience.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Cursor-Based Pagination</strong></p>\n<ul>\n<li><strong>Context</strong>: Large result sets require pagination to provide manageable client experiences while avoiding server-side state maintenance overhead.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Offset-based pagination with skip/limit semantics</li>\n<li>Cursor-based pagination with opaque position tokens</li>\n<li>Time-window pagination using temporal boundaries</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement cursor-based pagination with temporal alignment</li>\n<li><strong>Rationale</strong>: Offset-based pagination becomes inefficient for large offsets and doesn&#39;t handle concurrent data changes gracefully. Cursor-based approach provides efficient seek operations and natural integration with our time-partitioned storage. Temporal alignment enables jumping directly to relevant storage partitions without scanning intermediate data.</li>\n<li><strong>Consequences</strong>: Requires more complex cursor encoding and validation logic, but provides better performance characteristics and handles dynamic datasets more gracefully than offset-based approaches.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Decision Area</th>\n<th>Chosen Approach</th>\n<th>Primary Benefit</th>\n<th>Main Trade-off</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Query Language</td>\n<td>LogQL pipe syntax</td>\n<td>Natural data flow semantics</td>\n<td>Custom parser implementation</td>\n</tr>\n<tr>\n<td>Optimization</td>\n<td>Cost-based with statistics</td>\n<td>Adaptive to actual data patterns</td>\n<td>Statistics maintenance complexity</td>\n</tr>\n<tr>\n<td>Execution</td>\n<td>Streaming with buffering</td>\n<td>Memory scalability</td>\n<td>Backpressure handling complexity</td>\n</tr>\n<tr>\n<td>Result Format</td>\n<td>Multiple format support</td>\n<td>Client flexibility</td>\n<td>Format conversion overhead</td>\n</tr>\n<tr>\n<td>Pagination</td>\n<td>Cursor-based with temporal alignment</td>\n<td>Efficient large result handling</td>\n<td>Complex cursor management</td>\n</tr>\n<tr>\n<td>Caching</td>\n<td>Query result caching</td>\n<td>Repeated query performance</td>\n<td>Cache invalidation complexity</td>\n</tr>\n<tr>\n<td>Parallelization</td>\n<td>Opportunistic with dependency analysis</td>\n<td>CPU utilization</td>\n<td>Coordination overhead</td>\n</tr>\n</tbody></table>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>Query engine implementation presents numerous opportunities for subtle bugs and performance issues that can significantly impact system reliability and user experience. Understanding these common pitfalls enables proactive design decisions that avoid problematic patterns before they manifest in production systems.</p>\n<p>⚠️ <strong>Pitfall: Unbounded Query Execution</strong></p>\n<p>One of the most dangerous pitfalls involves queries that scan enormous amounts of data without proper resource limits. A query like <code>{} |= &quot;.*&quot;</code> with no time bounds could attempt to process every log entry in the system, consuming unbounded memory and CPU resources while potentially impacting other system operations.</p>\n<p>This occurs because developers often focus on correctness during initial implementation without considering resource consumption implications. The query parser may correctly validate syntax and the execution engine may properly implement the specified operations, but without explicit bounds checking, the system becomes vulnerable to accidental or malicious resource exhaustion.</p>\n<p>The fix requires implementing multiple layers of protection. Query parsing should reject queries that lack reasonable time bounds or contain patterns likely to match excessive data. The execution engine needs resource monitoring that can terminate queries exceeding memory or CPU limits. Additionally, result pagination should be mandatory for queries that could return large result sets, preventing clients from accidentally requesting millions of log entries in a single response.</p>\n<p>⚠️ <strong>Pitfall: Regular Expression Performance Cliffs</strong></p>\n<p>Regular expression processing can exhibit dramatic performance variations that create unpredictable query response times. A seemingly innocent pattern like <code>.*error.*details.*</code> can trigger catastrophic backtracking in the regex engine, causing query execution times to increase exponentially with log message length.</p>\n<p>This pitfall emerges because regex performance characteristics are not always intuitive, and patterns that work well on small test datasets may perform poorly on production log data with longer message texts or unexpected content patterns. The problem is compounded when multiple regex operations are applied to the same log entries, multiplying performance impacts.</p>\n<p>Prevention requires several approaches: implementing regex timeout limits that terminate pattern matching operations exceeding reasonable time bounds, analyzing regex patterns during query validation to identify potentially problematic constructions, and providing query hints or warnings when expensive regex patterns are detected. The system should also maintain performance metrics for different regex patterns, enabling identification of problematic queries through monitoring.</p>\n<p>⚠️ <strong>Pitfall: Memory Leaks in Result Aggregation</strong></p>\n<p>Aggregation operations that group results by extracted labels can consume unbounded memory when label cardinality is higher than expected. A query like <code>sum(rate({}[5m])) by (request_id)</code> could create millions of aggregation groups if request IDs are highly unique, exhausting available memory.</p>\n<p>This occurs because aggregation operations naturally accumulate state for each unique grouping combination, and developers may not anticipate the cardinality characteristics of production data. Label values that appear to have reasonable cardinality during development and testing may exhibit much higher cardinality in production environments.</p>\n<p>The solution involves implementing cardinality limits that cap the number of unique groups permitted in aggregation operations, monitoring memory usage during aggregation and triggering early termination when limits are approached, and providing query analysis tools that estimate result cardinality based on historical data patterns. The system should also support approximate aggregation techniques for high-cardinality scenarios where exact results are not required.</p>\n<p>⚠️ <strong>Pitfall: Inefficient Index Usage Patterns</strong></p>\n<p>Query execution can exhibit poor performance when the query planner makes suboptimal decisions about index usage, particularly when combining multiple filter conditions. A query with both selective and non-selective filters might process filters in an order that examines unnecessary data volumes.</p>\n<p>This problem manifests when query optimization relies on outdated statistics or when the cost estimation models don&#39;t accurately reflect actual data access patterns. The query planner might choose to start with a non-selective filter that matches many log streams, then apply more selective filters afterward, resulting in excessive I/O operations.</p>\n<p>Addressing this requires maintaining current statistics about label cardinality and index selectivity, implementing query plan analysis tools that can identify inefficient execution patterns, and providing query hints or manual optimization capabilities for complex queries that don&#39;t optimize well automatically. The system should also include query performance monitoring that can identify consistently slow query patterns for further optimization.</p>\n<p>⚠️ <strong>Pitfall: Timestamp Handling and Timezone Issues</strong></p>\n<p>Time-based queries can produce incorrect results when timestamp parsing, timezone conversions, and time range comparisons are not handled consistently throughout the query processing pipeline. A query specifying a time range might miss relevant log entries or include inappropriate ones due to timezone interpretation differences.</p>\n<p>This occurs because log timestamps can arrive in various formats and timezones, while query time specifications may use different timezone assumptions. The inconsistency can cause subtle data loss or inclusion errors that are difficult to detect during testing but manifest as missing or unexpected log entries in query results.</p>\n<p>Prevention requires establishing consistent timezone handling throughout the system, with all internal timestamp storage using UTC and explicit conversion rules for input parsing and output formatting. Query validation should ensure that time range specifications are unambiguous, and the system should provide clear documentation about timezone handling behavior for users.</p>\n<p>⚠️ <strong>Pitfall: JSON Parsing Error Propagation</strong></p>\n<p>When processing log entries that contain malformed JSON, the query execution engine might silently drop entries instead of handling parsing errors gracefully. A query using <code>| json</code> extraction might miss relevant log entries if they contain invalid JSON formatting, leading to incomplete query results without clear error indication.</p>\n<p>This pitfall emerges because JSON parsing errors are often treated as exceptional conditions that terminate processing, rather than normal data variations that should be handled gracefully. The issue is particularly problematic when only a small percentage of log entries contain malformed JSON, as the error may not be immediately apparent during testing.</p>\n<p>The solution involves implementing graceful error handling that logs parsing failures without terminating query execution, providing query options that control error handling behavior (strict vs. permissive parsing), and including error statistics in query result metadata so users can understand data completeness. The system should also support partial JSON extraction that can recover valid fields from otherwise malformed JSON structures.</p>\n<p>⚠️ <strong>Pitfall: Filter Ordering Dependencies</strong></p>\n<p>Query results can vary unexpectedly when filter operations are reordered during optimization, particularly when filters have side effects or when the query contains operations that depend on specific data characteristics. A query that expects filters to execute in a particular sequence might produce different results after query optimization reorders operations.</p>\n<p>This occurs when query optimization doesn&#39;t properly account for filter dependencies or when individual filter operations have implicit assumptions about input data characteristics. The problem can manifest as missing results, incorrect aggregation values, or inconsistent query behavior between different executions of the same query.</p>\n<p>Prevention requires careful analysis of filter operation dependencies during query planning, ensuring that optimization preserves semantic equivalence even when reordering operations, and implementing regression testing that validates query result consistency across different optimization paths. The query planner should also include dependency tracking that prevents reordering when operations have implicit ordering requirements.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The query engine represents one of the most complex components in the log aggregation system, requiring careful integration of parsing, optimization, and execution concerns. This implementation guidance provides concrete starting points and detailed development approaches for building a production-capable query processing system.</p>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Parser Generator</td>\n<td>Hand-written recursive descent parser</td>\n<td>ANTLR or similar parser generator</td>\n</tr>\n<tr>\n<td>AST Representation</td>\n<td>Interface{} with type switches</td>\n<td>Strongly-typed visitor pattern</td>\n</tr>\n<tr>\n<td>Regex Engine</td>\n<td>Go&#39;s <code>regexp</code> package</td>\n<td>RE2 with custom optimizations</td>\n</tr>\n<tr>\n<td>Result Streaming</td>\n<td>HTTP chunked encoding</td>\n<td>WebSocket with flow control</td>\n</tr>\n<tr>\n<td>Query Caching</td>\n<td>In-memory LRU cache</td>\n<td>Redis with intelligent invalidation</td>\n</tr>\n<tr>\n<td>Statistics Storage</td>\n<td>JSON files with periodic updates</td>\n<td>Embedded database like BoltDB</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended Project Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/query/\n  parser/\n    lexer.go              ← tokenization and basic syntax\n    parser.go             ← AST construction and validation\n    ast.go                ← AST node definitions and visitor pattern\n    parser_test.go        ← comprehensive query parsing tests\n  planner/\n    optimizer.go          ← cost-based optimization and plan generation\n    statistics.go         ← statistics collection and maintenance\n    plan.go               ← execution plan representation\n    planner_test.go       ← optimization logic tests\n  executor/\n    engine.go             ← main execution coordination\n    index_ops.go          ← index lookup operations\n    text_ops.go           ← text filtering and regex matching\n    aggregate_ops.go      ← aggregation and mathematical operations\n    stream_ops.go         ← result streaming and pagination\n    executor_test.go      ← execution engine tests\n  api/\n    http_handler.go       ← HTTP query API endpoint\n    response_format.go    ← result formatting and serialization\n    pagination.go         ← cursor-based pagination implementation</code></pre></div>\n\n<p><strong>Query Parser Infrastructure (Complete Implementation):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> parser</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">regexp</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strconv</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strings</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TokenType represents different types of tokens in LogQL</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TokenType</span><span style=\"color:#F97583\"> int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenEOF</span><span style=\"color:#B392F0\"> TokenType</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> iota</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenError</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenLeftBrace</span><span style=\"color:#6A737D\">    // {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenRightBrace</span><span style=\"color:#6A737D\">   // }</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenComma</span><span style=\"color:#6A737D\">        // ,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenPipe</span><span style=\"color:#6A737D\">         // |</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenEqual</span><span style=\"color:#6A737D\">        // =</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenNotEqual</span><span style=\"color:#6A737D\">     // !=</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenMatch</span><span style=\"color:#6A737D\">        // =~</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenNotMatch</span><span style=\"color:#6A737D\">     // !~</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenContains</span><span style=\"color:#6A737D\">     // |=</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenNotContains</span><span style=\"color:#6A737D\">  // !=</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenString</span><span style=\"color:#6A737D\">       // \"quoted string\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenIdentifier</span><span style=\"color:#6A737D\">   // unquoted identifier</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenNumber</span><span style=\"color:#6A737D\">       // numeric literal</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenDuration</span><span style=\"color:#6A737D\">     // 5m, 1h, etc.</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenLeftParen</span><span style=\"color:#6A737D\">    // (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenRightParen</span><span style=\"color:#6A737D\">   // )</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenLeftBracket</span><span style=\"color:#6A737D\">  // [</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenRightBracket</span><span style=\"color:#6A737D\"> // ]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Token represents a lexical token with position information</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Type     </span><span style=\"color:#B392F0\">TokenType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Value    </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Position </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Line     </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Column   </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Lexer tokenizes LogQL query strings</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Lexer</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    input    </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    position </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line     </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    column   </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    current  </span><span style=\"color:#F97583\">rune</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewLexer creates a lexer for the given input string</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewLexer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">input</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Lexer</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    l </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Lexer</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        input:  input,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        line:   </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        column: </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    l.</span><span style=\"color:#B392F0\">advance</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#6A737D\">// Initialize current character</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> l</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// advance moves to the next character in the input</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Lexer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">advance</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> l.position </span><span style=\"color:#F97583\">>=</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(l.input) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        l.current </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#6A737D\"> // EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> l.current </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        l.line</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        l.column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        l.column</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    l.current </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> rune</span><span style=\"color:#E1E4E8\">(l.input[l.position])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    l.position</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NextToken returns the next token from the input stream</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Lexer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">NextToken</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">Token</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        switch</span><span style=\"color:#E1E4E8\"> l.current {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">{Type: TokenEOF, Position: l.position, Line: l.line, Column: l.column}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\t</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            l.</span><span style=\"color:#B392F0\">advance</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            continue</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            token </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">{Type: TokenLeftBrace, Value: </span><span style=\"color:#9ECBFF\">\"{\"</span><span style=\"color:#E1E4E8\">, Position: l.position </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">, Line: l.line, Column: l.column </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            l.</span><span style=\"color:#B392F0\">advance</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> token</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            token </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">{Type: TokenRightBrace, Value: </span><span style=\"color:#9ECBFF\">\"}\"</span><span style=\"color:#E1E4E8\">, Position: l.position </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">, Line: l.line, Column: l.column </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            l.</span><span style=\"color:#B392F0\">advance</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> token</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">,</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            token </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">{Type: TokenComma, Value: </span><span style=\"color:#9ECBFF\">\",\"</span><span style=\"color:#E1E4E8\">, Position: l.position </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">, Line: l.line, Column: l.column </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            l.</span><span style=\"color:#B392F0\">advance</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> token</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">|</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> l.</span><span style=\"color:#B392F0\">scanPipeOperator</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">=</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> l.</span><span style=\"color:#B392F0\">scanEqualOperator</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">!</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> l.</span><span style=\"color:#B392F0\">scanNotOperator</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\"</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> l.</span><span style=\"color:#B392F0\">scanString</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">(</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            token </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">{Type: TokenLeftParen, Value: </span><span style=\"color:#9ECBFF\">\"(\"</span><span style=\"color:#E1E4E8\">, Position: l.position </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">, Line: l.line, Column: l.column </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            l.</span><span style=\"color:#B392F0\">advance</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> token</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">)</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            token </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">{Type: TokenRightParen, Value: </span><span style=\"color:#9ECBFF\">\")\"</span><span style=\"color:#E1E4E8\">, Position: l.position </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">, Line: l.line, Column: l.column </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            l.</span><span style=\"color:#B392F0\">advance</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> token</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">[</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            token </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">{Type: TokenLeftBracket, Value: </span><span style=\"color:#9ECBFF\">\"[\"</span><span style=\"color:#E1E4E8\">, Position: l.position </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">, Line: l.line, Column: l.column </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            l.</span><span style=\"color:#B392F0\">advance</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> token</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">]</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            token </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">{Type: TokenRightBracket, Value: </span><span style=\"color:#9ECBFF\">\"]\"</span><span style=\"color:#E1E4E8\">, Position: l.position </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">, Line: l.line, Column: l.column </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            l.</span><span style=\"color:#B392F0\">advance</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> token</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        default</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#B392F0\"> isLetter</span><span style=\"color:#E1E4E8\">(l.current) </span><span style=\"color:#F97583\">||</span><span style=\"color:#E1E4E8\"> l.current </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">_</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> l.</span><span style=\"color:#B392F0\">scanIdentifier</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#B392F0\"> isDigit</span><span style=\"color:#E1E4E8\">(l.current) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> l.</span><span style=\"color:#B392F0\">scanNumber</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">{Type: TokenError, Value: fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"unexpected character: </span><span style=\"color:#79B8FF\">%c</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, l.current), Position: l.position </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">, Line: l.line, Column: l.column </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// scanPipeOperator handles | and |= operators</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Lexer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">scanPipeOperator</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">Token</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> l.position </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    l.</span><span style=\"color:#B392F0\">advance</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#6A737D\">// consume '|'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> l.current </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">=</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        l.</span><span style=\"color:#B392F0\">advance</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#6A737D\">// consume '='</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">{Type: TokenContains, Value: </span><span style=\"color:#9ECBFF\">\"|=\"</span><span style=\"color:#E1E4E8\">, Position: start, Line: l.line, Column: l.column </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">{Type: TokenPipe, Value: </span><span style=\"color:#9ECBFF\">\"|\"</span><span style=\"color:#E1E4E8\">, Position: start, Line: l.line, Column: l.column </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Implementation continues with remaining scanner methods...</span></span></code></pre></div>\n\n<p><strong>Core Query Execution Skeleton:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> executor</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">your-project/internal/query/parser</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">your-project/internal/query/planner</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">your-project/internal/storage</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">your-project/internal/index</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// QueryEngine coordinates query execution across all system components</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> QueryEngine</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    indexManager   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">index</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Manager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storageManager </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Manager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    statistics     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">planner</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Statistics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewQueryEngine creates a query engine with required dependencies</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewQueryEngine</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">indexMgr</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">index</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Manager</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">storageMgr</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Manager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryEngine</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">QueryEngine</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        indexManager:   indexMgr,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        storageManager: storageMgr,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        statistics:     planner.</span><span style=\"color:#B392F0\">NewStatistics</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ExecuteQuery processes a LogQL query and returns streaming results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">qe </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ExecuteQuery</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">queryString</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">params</span><span style=\"color:#B392F0\"> QueryParams</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ResultStream</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Parse the query string into AST using parser.NewParser(queryString).Parse()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate the AST for semantic correctness (label names exist, time ranges valid)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Create query planner with current statistics and generate optimized execution plan</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Create execution context with resource limits and timeout from params</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Initialize result stream with appropriate formatting and pagination settings</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Execute the plan using executeplan() method, handling cancellation via context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Return configured result stream for client consumption</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Each step can fail - wrap errors with context about which phase failed</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    panic</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"implement ExecuteQuery\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// executePlan coordinates execution of an optimized query plan</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">qe </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">executePlan</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">plan</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">planner</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ExecutionPlan</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ResultIterator</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create execution pipeline based on plan.Operations in dependency order  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For each IndexLookup operation, call executeIndexLookup with label selectors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: For each TextFilter operation, call executeTextFilter with regex patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: For each Aggregation operation, call executeAggregation with grouping rules</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Connect pipeline stages with proper backpressure and error propagation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Start pipeline execution and return iterator for result consumption</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Ensure all pipeline stages respect context cancellation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Pipeline stages run concurrently - use channels for communication</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    panic</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"implement executePlan\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// executeIndexLookup retrieves log entry references matching label selectors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">qe </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">executeIndexLookup</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">selectors</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">parser</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">LabelSelector</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">timeRange</span><span style=\"color:#B392F0\"> parser</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">TimeRange</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#B392F0\">index</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">EntryReference</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Determine which index segments overlap with the specified time range</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For each relevant segment, apply label selectors to get candidate entry references</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: If multiple selectors exist, compute intersection of posting lists efficiently</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Apply bloom filter checks for negative filters (!=, !~) to eliminate segments early</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Sort results by timestamp to enable efficient streaming and merging</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Return consolidated list of entry references that match all selectors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Start with most selective selector to minimize intermediate result size</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    panic</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"implement executeIndexLookup\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// executeTextFilter applies line filters to log content</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">qe </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">executeTextFilter</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">refs</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">index</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">EntryReference</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">filters</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">parser</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">LineFilter</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">&#x3C;-chan</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create result channel for streaming filtered entries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Group entry references by storage chunk to enable batch loading</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: For each chunk, load log entries and apply text filters in sequence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Compile regex patterns once and reuse across multiple entries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Apply filters in order of increasing computational cost (substring before regex)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Stream matching entries to result channel, respecting context cancellation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Close result channel when all entries processed or context cancelled</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use worker pool to parallelize text processing across multiple chunks</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    panic</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"implement executeTextFilter\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// QueryParams contains execution parameters and resource limits</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> QueryParams</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TimeRange    </span><span style=\"color:#B392F0\">parser</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">TimeRange</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Limit        </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timeout      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Format       </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxMemory    </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StreamResults </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ResultStream provides streaming access to query results with pagination</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ResultStream</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    entries   </span><span style=\"color:#F97583\">&#x3C;-chan</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">LogEntry</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    errors    </span><span style=\"color:#F97583\">&#x3C;-chan</span><span style=\"color:#F97583\"> error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cursor    </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hasMore   </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metadata  </span><span style=\"color:#B392F0\">QueryMetadata</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Next returns the next result entry or error</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rs </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ResultStream</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Next</span><span style=\"color:#E1E4E8\">() (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check if more results available using hasMore flag</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Attempt to read from entries channel with timeout handling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Check errors channel for any execution errors that occurred</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update cursor position for pagination continuation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return entry or appropriate error (EOF when stream exhausted)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use select statement to handle multiple channels simultaneously</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    panic</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"implement ResultStream.Next\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// QueryMetadata contains information about query execution</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> QueryMetadata</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ExecutionTime </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ScannedEntries </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ReturnedEntries </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ScannedBytes   </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CacheHit       </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoints:</strong></p>\n<p>After implementing the query engine, verify correct functionality through these checkpoints:</p>\n<ol>\n<li><p><strong>Basic Query Parsing</strong>: Run <code>go test ./internal/query/parser/...</code> - all lexer and parser tests should pass, demonstrating correct tokenization and AST construction for various LogQL query patterns.</p>\n</li>\n<li><p><strong>Query Optimization</strong>: Create test queries with different selectivity patterns and verify that the planner chooses efficient execution orders. Queries with highly selective labels should use index lookups first, while text-heavy queries should apply cheaper filters before expensive regex operations.</p>\n</li>\n<li><p><strong>End-to-End Execution</strong>: Test complete query flows with commands like:</p>\n</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">   curl</span><span style=\"color:#79B8FF\"> -G</span><span style=\"color:#9ECBFF\"> 'http://localhost:8080/api/v1/query'</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        --data-urlencode</span><span style=\"color:#9ECBFF\"> 'query={service=\"api\"} |= \"error\" | json | level=\"ERROR\"'</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        --data-urlencode</span><span style=\"color:#9ECBFF\"> 'start=2024-01-01T00:00:00Z'</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        --data-urlencode</span><span style=\"color:#9ECBFF\"> 'end=2024-01-02T00:00:00Z'</span></span></code></pre></div>\n<p>   Verify that results contain only matching entries with proper JSON formatting.</p>\n<ol start=\"4\">\n<li><p><strong>Performance Verification</strong>: Run queries against datasets of different sizes and measure response times. Simple label-based queries should return results in milliseconds, while complex regex queries may take longer but should complete within configured timeout limits.</p>\n</li>\n<li><p><strong>Error Handling</strong>: Test malformed queries, resource exhaustion scenarios, and system failures to ensure graceful error responses. Verify that partial failures don&#39;t crash the query engine and that error messages provide actionable information for debugging.</p>\n</li>\n</ol>\n<p><strong>Debugging Tips:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis Steps</th>\n<th>Resolution</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Queries return no results</td>\n<td>Label selector mismatch or time range issues</td>\n<td>Check index segments for time range, verify label values exist</td>\n<td>Review label selector syntax, expand time range</td>\n</tr>\n<tr>\n<td>Very slow query performance</td>\n<td>Missing index optimization or expensive regex</td>\n<td>Enable query plan logging, check filter execution order</td>\n<td>Rewrite regex patterns, add more selective label filters</td>\n</tr>\n<tr>\n<td>Memory exhaustion errors</td>\n<td>Unbounded aggregation or large result sets</td>\n<td>Monitor memory usage during execution, check group cardinality</td>\n<td>Add result limits, implement streaming aggregation</td>\n</tr>\n<tr>\n<td>Inconsistent query results</td>\n<td>Race conditions in concurrent execution</td>\n<td>Run queries multiple times, check for non-deterministic ordering</td>\n<td>Add proper synchronization, ensure deterministic result ordering</td>\n</tr>\n<tr>\n<td>Parser errors on valid syntax</td>\n<td>Lexer tokenization issues or precedence problems</td>\n<td>Test individual query components, verify token sequence</td>\n<td>Fix lexer regular expressions, adjust parser precedence rules</td>\n</tr>\n</tbody></table>\n<p><img src=\"/api/project/log-aggregator/architecture-doc/asset?path=diagrams%2Fquery-flow.svg\" alt=\"Query Processing Sequence\"></p>\n<h2 id=\"storage-engine\">Storage Engine</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section corresponds to Milestone 4 (Log Storage &amp; Compression), where we implement efficient log storage with chunk-based compression, write-ahead logging, and retention policies for long-term data management.</p>\n</blockquote>\n<h3 id=\"mental-model-the-archive-warehouse\">Mental Model: The Archive Warehouse</h3>\n<p>Think of the storage engine as a modern warehouse facility that specializes in archiving historical documents. Just as a warehouse organizes items into labeled boxes, compresses them for space efficiency, and maintains detailed records of what goes where, our storage engine organizes log entries into time-based chunks, compresses them for optimal disk usage, and maintains indexes for fast retrieval.</p>\n<p>In this warehouse analogy, <strong>chunks</strong> are like storage boxes that hold related documents from the same time period. Each box has a detailed label (metadata) describing its contents, creation date, and compression method used. The warehouse workers (storage engine) group documents by time period because researchers (queries) typically want to examine all documents from a specific era, making it efficient to store them together.</p>\n<p>The <strong>write-ahead log</strong> functions like the warehouse&#39;s receiving dock logbook. Before any document gets filed into a storage box, workers write an entry in the logbook: &quot;Received shipment #1234 containing 50 documents from Company X, intended for Archive Box 789.&quot; This ensures that even if there&#39;s a power outage or accident before the documents reach their final location, the logbook provides a complete record of what was supposed to happen, allowing workers to replay the process later.</p>\n<p><strong>Compression</strong> works like vacuum-sealed storage bags. The warehouse compresses documents into these bags to save space, choosing different compression methods based on the document type. Financial records might use one compression method optimized for tabular data, while text documents use another method that&#39;s better for natural language patterns. Log data has predictable patterns (timestamps, structured fields, repeated keywords) that compression algorithms can exploit effectively.</p>\n<p>The <strong>retention policy engine</strong> acts like the warehouse&#39;s document lifecycle manager. It maintains a calendar of when different types of documents should be destroyed according to legal requirements, storage costs, and business value. Every night, it reviews stored boxes and marks expired ones for destruction, ensuring the warehouse doesn&#39;t grow indefinitely and storage costs remain manageable.</p>\n<h3 id=\"chunk-based-storage-design\">Chunk-Based Storage Design</h3>\n<p>The chunk-based storage system organizes log entries into time-windowed containers that balance query efficiency, compression effectiveness, and operational simplicity. Each chunk represents a fixed time window (typically 1-4 hours) and contains all log entries that arrived during that period, regardless of their original timestamp. This design choice optimizes for the common query pattern of &quot;show me logs from the last hour&quot; while maintaining predictable chunk sizes.</p>\n<p>The fundamental storage unit is the <code>ChunkHeader</code> structure, which contains comprehensive metadata about the chunk&#39;s contents and organization. This header serves as both a directory for chunk contents and a summary for query planning optimization.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Magic</td>\n<td>[4]byte</td>\n<td>File format identifier for corruption detection</td>\n</tr>\n<tr>\n<td>Version</td>\n<td>uint16</td>\n<td>Chunk format version for backward compatibility</td>\n</tr>\n<tr>\n<td>CompressionType</td>\n<td>uint8</td>\n<td>Algorithm used for payload compression</td>\n</tr>\n<tr>\n<td>StreamCount</td>\n<td>uint32</td>\n<td>Number of distinct log streams in chunk</td>\n</tr>\n<tr>\n<td>EntryCount</td>\n<td>uint64</td>\n<td>Total log entries across all streams</td>\n</tr>\n<tr>\n<td>UncompressedSize</td>\n<td>uint64</td>\n<td>Original payload size before compression</td>\n</tr>\n<tr>\n<td>CompressedSize</td>\n<td>uint64</td>\n<td>Actual disk size after compression</td>\n</tr>\n<tr>\n<td>TimeRange</td>\n<td>TimeRange</td>\n<td>Earliest and latest entry timestamps</td>\n</tr>\n<tr>\n<td>CreatedAt</td>\n<td>time.Time</td>\n<td>Chunk creation timestamp for ordering</td>\n</tr>\n</tbody></table>\n<p>Each chunk contains multiple log streams, where a stream groups entries with identical label sets. The <code>StreamHeader</code> provides per-stream metadata that enables selective decompression during queries that filter by specific label combinations.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>StreamID</td>\n<td>string</td>\n<td>Unique identifier derived from sorted labels</td>\n</tr>\n<tr>\n<td>Labels</td>\n<td>Labels</td>\n<td>Complete label set defining the stream</td>\n</tr>\n<tr>\n<td>EntryCount</td>\n<td>uint32</td>\n<td>Number of entries in this stream</td>\n</tr>\n<tr>\n<td>CompressedOffset</td>\n<td>uint64</td>\n<td>Byte offset to stream data within chunk</td>\n</tr>\n<tr>\n<td>CompressedSize</td>\n<td>uint64</td>\n<td>Compressed size of stream data</td>\n</tr>\n</tbody></table>\n<p>The chunk organization follows a multi-layer structure optimized for query selectivity. The chunk header appears first, followed by an array of stream headers, then the compressed payload data. This layout allows the query engine to read metadata without decompressing the entire chunk, enabling fast query filtering and resource estimation.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: Stream-level organization within chunks is crucial for query performance. Without it, querying logs from a specific service would require decompressing the entire chunk, even if that service only contributed 1% of the entries. Stream headers enable selective decompression, reducing CPU usage and memory pressure during queries.</p>\n</blockquote>\n<p>Time-based chunk boundaries align with wall-clock time rather than log timestamps to handle out-of-order delivery gracefully. A chunk created at 14:00 contains all entries that arrived between 13:00 and 14:00, regardless of their original timestamps. This approach simplifies chunk management and prevents late-arriving logs from requiring expensive chunk reorganization.</p>\n<p>The storage engine maintains chunk metadata in a separate index structure that maps time ranges to chunk files. This metadata index enables efficient chunk discovery during query planning without requiring file system scans or chunk header reads.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ChunkID</td>\n<td>string</td>\n<td>Unique chunk identifier</td>\n</tr>\n<tr>\n<td>FilePath</td>\n<td>string</td>\n<td>Absolute path to chunk file</td>\n</tr>\n<tr>\n<td>TimeRange</td>\n<td>TimeRange</td>\n<td>Chunk time boundaries</td>\n</tr>\n<tr>\n<td>StreamCount</td>\n<td>uint32</td>\n<td>Number of distinct streams</td>\n</tr>\n<tr>\n<td>EntryCount</td>\n<td>uint64</td>\n<td>Total entries across all streams</td>\n</tr>\n<tr>\n<td>DiskSize</td>\n<td>uint64</td>\n<td>File size on disk</td>\n</tr>\n<tr>\n<td>CreatedAt</td>\n<td>time.Time</td>\n<td>Creation timestamp</td>\n</tr>\n<tr>\n<td>LastAccessed</td>\n<td>time.Time</td>\n<td>Most recent query access</td>\n</tr>\n</tbody></table>\n<h3 id=\"compression-strategy\">Compression Strategy</h3>\n<p>Log data exhibits several characteristics that compression algorithms can exploit effectively: high redundancy in structured fields (timestamps, log levels, service names), repeated text patterns in messages, and temporal locality where similar log entries cluster together. The storage engine implements multiple compression algorithms optimized for different log data patterns and query access requirements.</p>\n<p><strong>Zstandard (zstd)</strong> provides the optimal balance of compression ratio and decompression speed for most log workloads. It achieves 60-80% compression ratios on typical log data while maintaining decompression speeds of 1-2 GB/s, making it suitable for real-time query scenarios. Zstd&#39;s dictionary training capability allows the storage engine to build compression dictionaries from historical log samples, improving compression ratios by 15-25% for structured log formats.</p>\n<p><strong>LZ4</strong> prioritizes decompression speed over compression ratio, achieving 3-5 GB/s decompression with 45-60% compression ratios. This algorithm suits scenarios where query latency is more important than storage costs, particularly for frequently accessed recent log data or real-time alerting pipelines.</p>\n<p><strong>Gzip</strong> provides maximum compression ratios (70-85%) at the cost of slower decompression speeds (200-500 MB/s). The storage engine uses gzip for archival chunks that are accessed infrequently but must be retained for compliance or forensic analysis.</p>\n<p>The compression algorithm selection follows a tiered strategy based on chunk age and access patterns. Recent chunks (less than 24 hours old) use LZ4 for fast query response. Medium-age chunks (1-30 days old) use zstd for balanced performance. Archive chunks (older than 30 days) use gzip for maximum space efficiency.</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th>Compression Ratio</th>\n<th>Decompression Speed</th>\n<th>CPU Usage</th>\n<th>Best For</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>LZ4</td>\n<td>45-60%</td>\n<td>3-5 GB/s</td>\n<td>Low</td>\n<td>Recent, frequently accessed</td>\n</tr>\n<tr>\n<td>Zstd</td>\n<td>60-80%</td>\n<td>1-2 GB/s</td>\n<td>Medium</td>\n<td>General purpose, balanced</td>\n</tr>\n<tr>\n<td>Gzip</td>\n<td>70-85%</td>\n<td>200-500 MB/s</td>\n<td>High</td>\n<td>Archive, infrequent access</td>\n</tr>\n</tbody></table>\n<p>The compression process operates at the stream level within each chunk, allowing queries that filter by labels to decompress only relevant streams. This selective decompression reduces CPU usage and memory pressure during query execution, particularly important for queries that touch many chunks but filter to a small result set.</p>\n<p>Dictionary-based compression training runs as a background process, analyzing log patterns from the previous day to build optimized dictionaries for each compression algorithm. These dictionaries capture common patterns in timestamps, structured fields, and message content, significantly improving compression effectiveness for predictable log formats.</p>\n<blockquote>\n<p><strong>Critical Trade-off</strong>: Compression algorithm choice involves three competing factors: storage cost (compression ratio), query latency (decompression speed), and CPU overhead (compression/decompression processing). The storage engine&#39;s tiered approach balances these factors by matching algorithm characteristics to access patterns, but operators may need to adjust the tier boundaries based on their specific cost and performance requirements.</p>\n</blockquote>\n<h3 id=\"write-ahead-log-implementation\">Write-Ahead Log Implementation</h3>\n<p>The write-ahead log ensures data durability by recording every storage operation before it occurs, guaranteeing that no log entries are lost even if the system crashes during processing. The WAL operates as an append-only transaction log that maintains strict ordering of operations and provides recovery mechanisms for crash scenarios.</p>\n<p>The WAL record structure captures complete information about each storage operation, enabling precise replay during recovery. Each record includes operation metadata, affected data, and checksums for corruption detection.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>RecordType</td>\n<td>uint8</td>\n<td>Operation type (WRITE, COMMIT, CHECKPOINT)</td>\n</tr>\n<tr>\n<td>Timestamp</td>\n<td>time.Time</td>\n<td>Operation timestamp for ordering</td>\n</tr>\n<tr>\n<td>ChunkID</td>\n<td>string</td>\n<td>Target chunk identifier</td>\n</tr>\n<tr>\n<td>StreamID</td>\n<td>string</td>\n<td>Target stream within chunk</td>\n</tr>\n<tr>\n<td>EntryCount</td>\n<td>uint32</td>\n<td>Number of log entries in operation</td>\n</tr>\n<tr>\n<td>DataSize</td>\n<td>uint64</td>\n<td>Size of entry data in bytes</td>\n</tr>\n<tr>\n<td>Checksum</td>\n<td>uint32</td>\n<td>CRC32 checksum of record data</td>\n</tr>\n<tr>\n<td>Data</td>\n<td>[]byte</td>\n<td>Serialized log entry data</td>\n</tr>\n</tbody></table>\n<p>WAL operations follow a strict protocol that ensures atomicity and recoverability. Before writing any log entries to chunk storage, the storage engine first writes a WRITE record to the WAL containing the complete entry data. Only after the WAL record is safely persisted (fsync) does the engine proceed with chunk operations.</p>\n<p>The WAL write process follows these steps:</p>\n<ol>\n<li><p><strong>Record Construction</strong>: The storage engine serializes the log entries and constructs a WAL record with operation metadata, data payload, and integrity checksum.</p>\n</li>\n<li><p><strong>Atomic Write</strong>: The record is written to the WAL file using atomic append operations, ensuring partial writes cannot corrupt the log structure.</p>\n</li>\n<li><p><strong>Durability Guarantee</strong>: The storage engine calls fsync() to force the operating system to flush the write to physical storage, guaranteeing durability even if power fails immediately.</p>\n</li>\n<li><p><strong>Operation Proceed</strong>: Only after successful WAL persistence does the storage engine proceed with the actual chunk write operation.</p>\n</li>\n<li><p><strong>Commit Record</strong>: After successful chunk write, the engine writes a COMMIT record to the WAL, marking the operation as completed.</p>\n</li>\n</ol>\n<p>WAL recovery scans the log from the last checkpoint, identifying uncommitted operations that must be replayed. The recovery process reconstructs the exact state of in-progress operations and completes them, ensuring no data loss regardless of when the crash occurred.</p>\n<p>Recovery operation types and their handling:</p>\n<table>\n<thead>\n<tr>\n<th>Record Type</th>\n<th>Recovery Action</th>\n<th>Failure Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>WRITE (no COMMIT)</td>\n<td>Replay chunk write operation</td>\n<td>Retry with exponential backoff</td>\n</tr>\n<tr>\n<td>WRITE + COMMIT</td>\n<td>Skip (already completed)</td>\n<td>Mark as recovered</td>\n</tr>\n<tr>\n<td>Partial Record</td>\n<td>Truncate WAL at corruption</td>\n<td>Log corruption warning</td>\n</tr>\n<tr>\n<td>Invalid Checksum</td>\n<td>Skip corrupted record</td>\n<td>Alert operator, continue</td>\n</tr>\n</tbody></table>\n<p>The WAL implements automatic rotation based on size and time thresholds to prevent unbounded growth. When the current WAL file exceeds the configured size limit (default 100MB), the storage engine creates a new WAL file and marks the previous file for cleanup after the next checkpoint.</p>\n<p>Checkpoint operations create recovery points by ensuring all pending WAL operations are committed to chunk storage. During checkpoint, the storage engine:</p>\n<ol>\n<li>Flushes all in-memory buffers to disk storage</li>\n<li>Syncs all open chunk files to ensure durability</li>\n<li>Writes a CHECKPOINT record to the WAL with timestamp</li>\n<li>Rotates to a new WAL file</li>\n<li>Safely deletes old WAL files from before the checkpoint</li>\n</ol>\n<blockquote>\n<p><strong>Critical Implementation Detail</strong>: The WAL must use direct I/O or explicit fsync() calls to ensure durability guarantees. Operating system write buffering can delay actual disk writes for seconds or minutes, creating a window where committed operations exist only in volatile memory. Without proper synchronization, a crash can lose supposedly durable data, violating the fundamental WAL contract.</p>\n</blockquote>\n<h3 id=\"retention-policy-engine\">Retention Policy Engine</h3>\n<p>The retention policy engine automatically manages log lifecycle by applying configurable rules that balance storage costs, compliance requirements, and operational needs. The engine evaluates retention policies continuously, identifying expired data and orchestrating safe cleanup without disrupting active queries or system operations.</p>\n<p>Retention policies operate at multiple granularity levels to provide flexible data management. Stream-level policies apply to log entries with specific label combinations, enabling fine-grained control over different data types. Chunk-level policies provide coarse-grained cleanup for operational efficiency. Global policies set system-wide defaults and limits.</p>\n<p>The <code>RetentionPolicy</code> structure defines the rules and thresholds for automatic data cleanup:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>PolicyID</td>\n<td>string</td>\n<td>Unique identifier for the retention policy</td>\n</tr>\n<tr>\n<td>StreamSelector</td>\n<td>Labels</td>\n<td>Label pattern to match affected log streams</td>\n</tr>\n<tr>\n<td>MaxAge</td>\n<td>time.Duration</td>\n<td>Maximum age before deletion (time-based)</td>\n</tr>\n<tr>\n<td>MaxSize</td>\n<td>int64</td>\n<td>Maximum storage size before cleanup (size-based)</td>\n</tr>\n<tr>\n<td>MaxEntries</td>\n<td>int64</td>\n<td>Maximum entry count before cleanup (count-based)</td>\n</tr>\n<tr>\n<td>Priority</td>\n<td>int32</td>\n<td>Policy priority for conflict resolution</td>\n</tr>\n<tr>\n<td>GracePeriod</td>\n<td>time.Duration</td>\n<td>Delay before actual deletion for recovery</td>\n</tr>\n</tbody></table>\n<p>The retention evaluation engine runs periodically (typically every hour) to assess all stored chunks against active retention policies. The evaluation process considers multiple factors: chunk age, access patterns, storage pressure, and policy conflicts.</p>\n<p>Policy evaluation follows a multi-phase process designed to prevent accidental data loss:</p>\n<ol>\n<li><p><strong>Discovery Phase</strong>: Scan all chunk metadata to identify chunks that may be subject to retention policies based on age and content.</p>\n</li>\n<li><p><strong>Policy Matching</strong>: For each candidate chunk, evaluate all applicable retention policies based on stream selectors and label patterns.</p>\n</li>\n<li><p><strong>Conflict Resolution</strong>: When multiple policies apply to the same data, use priority ordering and most restrictive rules to determine the effective retention period.</p>\n</li>\n<li><p><strong>Grace Period Check</strong>: Ensure that chunks marked for deletion have passed their grace period, allowing time for recovery if the policy was misconfigured.</p>\n</li>\n<li><p><strong>Safety Validation</strong>: Verify that no active queries are accessing chunks marked for deletion and that backup/replication requirements are satisfied.</p>\n</li>\n<li><p><strong>Deletion Execution</strong>: Remove chunk files and update metadata indexes atomically to maintain consistency.</p>\n</li>\n</ol>\n<p>The retention engine implements multiple cleanup strategies optimized for different operational scenarios:</p>\n<table>\n<thead>\n<tr>\n<th>Strategy</th>\n<th>Description</th>\n<th>Triggers</th>\n<th>Performance Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Eager Cleanup</td>\n<td>Immediate deletion when retention exceeded</td>\n<td>High storage pressure</td>\n<td>Low, continuous</td>\n</tr>\n<tr>\n<td>Batch Cleanup</td>\n<td>Periodic bulk deletion of expired chunks</td>\n<td>Scheduled intervals</td>\n<td>Medium, bursty</td>\n</tr>\n<tr>\n<td>Lazy Cleanup</td>\n<td>Delete during query if chunk expired</td>\n<td>Query-time discovery</td>\n<td>High, unpredictable</td>\n</tr>\n</tbody></table>\n<p>Retention policy conflicts require careful resolution to prevent unintended data loss. When multiple policies apply to the same stream, the engine uses a precedence system:</p>\n<ol>\n<li><strong>Explicit stream policies</strong> (exact label match) override wildcard patterns</li>\n<li><strong>Longer retention periods</strong> override shorter ones to prevent accidental deletion</li>\n<li><strong>Higher priority values</strong> override lower priority policies</li>\n<li><strong>Compliance policies</strong> (marked with compliance flag) override operational policies</li>\n</ol>\n<p>The retention engine maintains detailed audit logs of all cleanup operations, recording which policies triggered deletions, how much data was removed, and verification that the operations completed successfully. This audit trail supports compliance reporting and provides debugging information when retention behavior appears incorrect.</p>\n<table>\n<thead>\n<tr>\n<th>Audit Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Timestamp</td>\n<td>time.Time</td>\n<td>When cleanup operation occurred</td>\n</tr>\n<tr>\n<td>PolicyID</td>\n<td>string</td>\n<td>Which retention policy triggered cleanup</td>\n</tr>\n<tr>\n<td>ChunkID</td>\n<td>string</td>\n<td>Identifier of deleted chunk</td>\n</tr>\n<tr>\n<td>EntryCount</td>\n<td>int64</td>\n<td>Number of log entries deleted</td>\n</tr>\n<tr>\n<td>DataSize</td>\n<td>int64</td>\n<td>Bytes of storage reclaimed</td>\n</tr>\n<tr>\n<td>Reason</td>\n<td>string</td>\n<td>Specific trigger (age, size, manual)</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Operational Safety</strong>: Retention policies are irreversible and can cause significant data loss if misconfigured. The grace period mechanism provides a safety net by marking chunks for deletion but delaying actual removal, allowing operators to recover from policy mistakes. However, operators must monitor retention audit logs and implement proper backup strategies for data that must be preserved beyond the retention period.</p>\n</blockquote>\n<h3 id=\"architecture-decision-records\">Architecture Decision Records</h3>\n<blockquote>\n<p><strong>Decision: Chunk Time Window Size</strong></p>\n<ul>\n<li><strong>Context</strong>: Log entries must be organized into storage chunks, but the optimal chunk duration involves trade-offs between query efficiency, compression effectiveness, and operational complexity. Smaller chunks enable fine-grained queries but increase metadata overhead. Larger chunks improve compression ratios but force queries to process more irrelevant data.</li>\n<li><strong>Options Considered</strong>: 15-minute chunks, 1-hour chunks, 4-hour chunks, daily chunks</li>\n<li><strong>Decision</strong>: 1-hour chunk windows with configurable override per stream</li>\n<li><strong>Rationale</strong>: 1-hour windows align with common query patterns (&quot;show me the last hour of logs&quot;) while maintaining manageable file sizes (typically 10-100MB compressed). This size provides good compression ratios through temporal locality while keeping decompression overhead reasonable. The configurable override allows high-volume streams to use smaller windows when needed.</li>\n<li><strong>Consequences</strong>: Query performance is optimal for time ranges that align with chunk boundaries. Queries spanning many chunks require more file I/O. Compression ratios are 10-15% better than 15-minute chunks due to larger compression windows.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Query Efficiency</th>\n<th>Compression Ratio</th>\n<th>File Count</th>\n<th>Operational Overhead</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>15-minute</td>\n<td>High (fine-grained)</td>\n<td>Good (65%)</td>\n<td>High</td>\n<td>High metadata</td>\n</tr>\n<tr>\n<td>1-hour</td>\n<td>Good (balanced)</td>\n<td>Better (72%)</td>\n<td>Moderate</td>\n<td>Balanced</td>\n</tr>\n<tr>\n<td>4-hour</td>\n<td>Moderate (coarse)</td>\n<td>Best (78%)</td>\n<td>Low</td>\n<td>Low metadata</td>\n</tr>\n<tr>\n<td>Daily</td>\n<td>Poor (very coarse)</td>\n<td>Excellent (82%)</td>\n<td>Very low</td>\n<td>Minimal</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Compression Algorithm Selection Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Different compression algorithms optimize for different metrics (compression ratio vs speed vs CPU usage), and log access patterns change over time. A single algorithm cannot optimize for all scenarios across the data lifecycle.</li>\n<li><strong>Options Considered</strong>: Single algorithm (zstd), manual per-chunk configuration, automatic tiered selection, dynamic algorithm switching</li>\n<li><strong>Decision</strong>: Automatic tiered selection based on chunk age and access patterns</li>\n<li><strong>Rationale</strong>: Recent chunks are queried frequently and need fast decompression (LZ4), medium-age chunks balance compression and speed (zstd), and old chunks prioritize storage efficiency (gzip). This approach automatically optimizes for changing access patterns without operator intervention.</li>\n<li><strong>Consequences</strong>: Storage costs are minimized for old data while query performance remains good for recent data. The system requires more complexity to manage multiple algorithms. Operator control is reduced but operational overhead is lower.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Performance Optimization</th>\n<th>Operator Effort</th>\n<th>System Complexity</th>\n<th>Storage Efficiency</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Single Algorithm</td>\n<td>Poor (one-size-fits-all)</td>\n<td>Low</td>\n<td>Low</td>\n<td>Moderate</td>\n</tr>\n<tr>\n<td>Manual Configuration</td>\n<td>Good (if configured well)</td>\n<td>High</td>\n<td>Low</td>\n<td>Variable</td>\n</tr>\n<tr>\n<td>Tiered Selection</td>\n<td>Excellent (automated)</td>\n<td>Low</td>\n<td>Medium</td>\n<td>High</td>\n</tr>\n<tr>\n<td>Dynamic Switching</td>\n<td>Excellent (adaptive)</td>\n<td>Low</td>\n<td>High</td>\n<td>Highest</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: WAL Recovery Granularity</strong></p>\n<ul>\n<li><strong>Context</strong>: WAL recovery can operate at different granularity levels: individual log entries, batches of entries, or entire chunks. Finer granularity provides better consistency guarantees but increases recovery complexity and storage overhead.</li>\n<li><strong>Options Considered</strong>: Per-entry WAL records, batch-level records, chunk-level records</li>\n<li><strong>Decision</strong>: Batch-level WAL records with configurable batch size</li>\n<li><strong>Rationale</strong>: Batch-level records provide good consistency (losing at most one batch on crash) while maintaining reasonable WAL overhead. Individual entries would create excessive WAL volume for high-throughput scenarios. Chunk-level records could lose too much data in crash scenarios.</li>\n<li><strong>Consequences</strong>: Recovery time is bounded by batch size rather than total volume. WAL overhead is 1-2% of total data volume. Maximum data loss on crash is one batch (typically 1000-10000 entries).</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Data Loss Risk</th>\n<th>WAL Overhead</th>\n<th>Recovery Time</th>\n<th>Implementation Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Per-Entry</td>\n<td>Minimal (1 entry)</td>\n<td>High (10-20%)</td>\n<td>Long</td>\n<td>High</td>\n</tr>\n<tr>\n<td>Batch-Level</td>\n<td>Low (1 batch)</td>\n<td>Low (1-2%)</td>\n<td>Medium</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Chunk-Level</td>\n<td>High (1 chunk)</td>\n<td>Very low (&lt;0.1%)</td>\n<td>Fast</td>\n<td>Low</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Retention Policy Evaluation Frequency</strong></p>\n<ul>\n<li><strong>Context</strong>: Retention policies must be evaluated regularly to free storage space, but frequent evaluation consumes CPU and I/O resources. The evaluation frequency affects how quickly storage is reclaimed versus system overhead.</li>\n<li><strong>Options Considered</strong>: Continuous evaluation, hourly evaluation, daily evaluation, on-demand evaluation</li>\n<li><strong>Decision</strong>: Hourly evaluation with emergency triggers for storage pressure</li>\n<li><strong>Rationale</strong>: Hourly evaluation provides good balance between storage reclamation speed and system overhead. Emergency triggers ensure the system can respond to unexpected storage pressure without waiting for the next scheduled evaluation.</li>\n<li><strong>Consequences</strong>: Storage is typically reclaimed within 1 hour of expiration. System overhead is predictable and bounded. Emergency situations are handled appropriately. Some storage over-consumption is possible for up to 1 hour.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Storage Reclamation</th>\n<th>CPU Overhead</th>\n<th>Predictability</th>\n<th>Emergency Response</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Continuous</td>\n<td>Immediate</td>\n<td>High</td>\n<td>Poor</td>\n<td>Excellent</td>\n</tr>\n<tr>\n<td>Hourly</td>\n<td>Good (1hr delay)</td>\n<td>Low</td>\n<td>Good</td>\n<td>Good (with triggers)</td>\n</tr>\n<tr>\n<td>Daily</td>\n<td>Poor (24hr delay)</td>\n<td>Very low</td>\n<td>Excellent</td>\n<td>Poor</td>\n</tr>\n<tr>\n<td>On-demand</td>\n<td>Variable</td>\n<td>Variable</td>\n<td>Poor</td>\n<td>Manual only</td>\n</tr>\n</tbody></table>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Compression Algorithm Choice Without Benchmarking</strong>\nMany developers choose compression algorithms based on reputation or documentation claims rather than measuring performance with their actual log data. Log data characteristics vary significantly between applications—structured JSON logs compress differently than free-form application logs, and the optimal algorithm depends on query patterns and hardware constraints.</p>\n<p><strong>Why it&#39;s wrong</strong>: Compression algorithm performance depends heavily on data patterns, CPU architecture, and access patterns. An algorithm that works well for one application may perform poorly for another. Without benchmarking, you may choose an algorithm that wastes CPU cycles or storage space.</p>\n<p><strong>How to fix</strong>: Implement a benchmarking suite that measures compression ratio, compression speed, decompression speed, and memory usage using representative samples of your actual log data. Test at least 3-4 different algorithms (LZ4, zstd, gzip, snappy) with various settings and measure performance under realistic query loads.</p>\n<p>⚠️ <strong>Pitfall: WAL Growing Without Bounds</strong>\nThe write-ahead log can grow indefinitely if the checkpoint mechanism fails or is misconfigured. Developers sometimes focus on the write path without implementing proper WAL rotation and cleanup, leading to disk space exhaustion and degraded performance as WAL files become enormous.</p>\n<p><strong>Why it&#39;s wrong</strong>: An unbounded WAL will eventually consume all available disk space and slow down recovery operations. Large WAL files also increase crash recovery time, as the entire log must be scanned during startup.</p>\n<p><strong>How to fix</strong>: Implement automatic WAL rotation based on both size thresholds (e.g., 100MB) and time intervals (e.g., 1 hour). Ensure the checkpoint process verifies that all WAL operations are committed to stable storage before deleting old WAL files. Add monitoring alerts for WAL file count and total WAL disk usage.</p>\n<p>⚠️ <strong>Pitfall: Retention Policy Race Conditions</strong>\nRetention cleanup can interfere with active queries if not properly coordinated. Developers sometimes implement retention as a simple file deletion process without considering that queries might be accessing the chunks being deleted, leading to query failures and data integrity issues.</p>\n<p><strong>Why it&#39;s wrong</strong>: Deleting chunks while queries are reading them causes query failures and can corrupt query results. The race condition is particularly dangerous because it may only manifest under specific timing conditions, making it difficult to detect during testing.</p>\n<p><strong>How to fix</strong>: Implement reference counting or cooperative deletion mechanisms. Before deleting a chunk, verify that no active queries hold references to it. Use a two-phase deletion process: first mark chunks as &quot;pending deletion&quot; and prevent new queries from accessing them, then wait for existing queries to complete before actual deletion.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Compression Decompression Memory Requirements</strong>\nCompression reduces disk storage but requires significant memory during decompression. Developers often focus on compression ratios without considering that decompressing large chunks can require 3-5x the compressed size in memory, potentially causing out-of-memory conditions during query execution.</p>\n<p><strong>Why it&#39;s wrong</strong>: Large chunks that compress well may require hundreds of megabytes or gigabytes of memory to decompress. Under concurrent query load, this can quickly exhaust available memory and cause query failures or system instability.</p>\n<p><strong>How to fix</strong>: Implement memory budgeting in the query engine that limits total decompression memory usage across concurrent queries. Set maximum chunk sizes based on available memory rather than just disk considerations. Consider streaming decompression for large chunks to reduce memory pressure.</p>\n<p>⚠️ <strong>Pitfall: Time Zone Handling in Chunk Boundaries</strong>\nChunk time boundaries can become inconsistent when dealing with time zones, daylight saving time changes, or distributed systems with clock skew. Developers sometimes use local time or naive UTC conversion without considering these edge cases, leading to chunks with overlapping time ranges or gaps.</p>\n<p><strong>Why it&#39;s wrong</strong>: Inconsistent time boundaries break query assumptions about which chunks to examine for a given time range. This can cause queries to miss data (false negatives) or scan extra chunks (performance degradation).</p>\n<p><strong>How to fix</strong>: Always use UTC for internal time boundaries and chunk organization. Store the original timezone information in log entry metadata if needed for display purposes. Implement clock skew detection and correction mechanisms for distributed ingestion scenarios.</p>\n<p>⚠️ <strong>Pitfall: Failed Cleanup Leading to Storage Leaks</strong>\nRetention cleanup operations can fail due to permission issues, disk errors, or concurrent access. Without proper error handling and retry mechanisms, failed cleanup operations may be silently ignored, leading to storage leaks where expired data accumulates indefinitely.</p>\n<p><strong>Why it&#39;s wrong</strong>: Storage leaks violate retention policies and can cause unexpected storage cost increases. In regulated environments, failing to delete data according to retention policies may create compliance violations.</p>\n<p><strong>How to fix</strong>: Implement robust error handling in cleanup operations with exponential backoff retry logic. Maintain a cleanup failure audit log and generate alerts when cleanup operations fail repeatedly. Implement manual cleanup tools that operators can use to address persistent cleanup failures.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The storage engine bridges high-performance data persistence with query efficiency requirements. Implementation focuses on managing multiple compression algorithms, ensuring WAL durability guarantees, and coordinating retention cleanup safely.</p>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Compression</td>\n<td>Standard library gzip/zlib</td>\n<td>CGO bindings to zstd/lz4</td>\n</tr>\n<tr>\n<td>File I/O</td>\n<td>os.File with explicit fsync</td>\n<td>Memory-mapped files with madvise</td>\n</tr>\n<tr>\n<td>WAL Structure</td>\n<td>Append-only binary format</td>\n<td>Structured records with checksums</td>\n</tr>\n<tr>\n<td>Metadata Index</td>\n<td>JSON files</td>\n<td>Embedded key-value store (badger)</td>\n</tr>\n<tr>\n<td>Background Tasks</td>\n<td>time.Ticker goroutines</td>\n<td>Priority-based task scheduler</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/storage/\n  engine.go                 ← main storage engine coordinator\n  engine_test.go           ← integration tests\n  chunk/\n    chunk.go               ← chunk read/write operations\n    chunk_test.go         ← chunk format tests\n    header.go             ← chunk header definitions\n    compression.go        ← compression algorithm interface\n  wal/\n    wal.go                ← write-ahead log implementation\n    wal_test.go          ← WAL recovery tests\n    recovery.go          ← crash recovery logic\n  retention/\n    policy.go            ← retention policy evaluation\n    cleanup.go           ← safe deletion coordination\n    audit.go             ← cleanup operation logging\n  metadata/\n    index.go             ← chunk metadata management\n    stats.go             ← storage statistics tracking</code></pre></div>\n\n<p><strong>Infrastructure Starter Code:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Package storage provides efficient log storage with compression and retention</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> storage</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/binary</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">hash/crc32</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">io</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">path/filepath</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StorageEngine coordinates chunk storage, WAL, and retention</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StorageEngine</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config        </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    walWriter     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WALWriter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    chunkStore    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ChunkStore</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retentionMgr  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RetentionManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metadataIndex </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MetadataIndex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metrics       </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageMetrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu           </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StorageConfig defines storage engine parameters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StorageConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StoragePath      </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ChunkDuration    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    WALSizeLimit     </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CompressionType  </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RetentionPeriod  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxMemoryUsage   </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StorageMetrics tracks storage engine performance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StorageMetrics</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ChunksWritten     </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BytesStored       </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CompressionRatio  </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    WALSize          </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RetentionDeletes </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu               </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ChunkStore handles chunk file operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ChunkStore</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    basePath    </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    compression </span><span style=\"color:#B392F0\">CompressionInterface</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu          </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CompressionInterface abstracts compression algorithms</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CompressionInterface</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Compress</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Decompress</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Name</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Level</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WALWriter provides write-ahead logging functionality</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> WALWriter</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    file     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">os</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">File</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    offset   </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu       </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Mutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WALConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WALConfig defines WAL behavior parameters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> WALConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    FilePath     </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SizeLimit    </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SyncInterval </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BufferSize   </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WALRecord represents a single WAL entry</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> WALRecord</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RecordType </span><span style=\"color:#F97583\">uint8</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ChunkID    </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StreamID   </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    EntryCount </span><span style=\"color:#F97583\">uint32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DataSize   </span><span style=\"color:#F97583\">uint64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Checksum   </span><span style=\"color:#F97583\">uint32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Data       []</span><span style=\"color:#F97583\">byte</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WAL record types</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    WALRecordWrite</span><span style=\"color:#F97583\">     uint8</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    WALRecordCommit</span><span style=\"color:#F97583\">    uint8</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    WALRecordCheckpoint</span><span style=\"color:#F97583\"> uint8</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewStorageEngine creates a configured storage engine</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewStorageEngine</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">StorageConfig</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageEngine</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Initialize WAL writer with crash recovery</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Initialize chunk store with compression setup</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Initialize retention manager with policy loading</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Initialize metadata index with persistent state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Start background maintenance goroutines</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WriteLogBatch stores a batch of log entries with WAL protection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">se </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">WriteLogBatch</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">entries</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Generate unique batch ID and chunk assignment</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Write batch to WAL with entry serialization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Fsync WAL to guarantee durability</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Group entries by stream for chunk organization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Compress and write chunk data to storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Update metadata index with chunk information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Write WAL commit record after successful storage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StartRecovery replays uncommitted WAL operations after crash</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">se </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">StartRecovery</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Scan WAL files from last checkpoint forward</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Parse WAL records and identify uncommitted operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Validate record checksums and handle corruption</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Replay write operations that lack commit records</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Update metadata index to reflect recovered state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Create new checkpoint after successful recovery</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Core Logic Skeleton:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// CompressChunkData applies the configured compression algorithm to chunk payload</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">cs </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ChunkStore</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CompressChunkData</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">streams</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">StreamData</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">algorithm</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ChunkHeader</span><span style=\"color:#E1E4E8\">, []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Serialize stream headers and entry data into binary format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Select compression algorithm based on configuration and data characteristics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Apply compression to serialized payload with error handling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Calculate compression ratio and update metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Build chunk header with metadata (sizes, timestamps, stream count)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Validate header consistency and compressed data integrity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use encoding/binary for cross-platform serialization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Store uncompressed size for decompression buffer allocation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WriteWALRecord atomically appends a record to the write-ahead log</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">w </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WALWriter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">WriteWALRecord</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">record</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">WALRecord</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> w.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Calculate CRC32 checksum of record data for corruption detection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Serialize record header and payload into binary format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Write serialized record to WAL file with atomic append</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Call fsync() to ensure data reaches physical storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Update WAL offset tracking and size metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Check if WAL rotation is needed based on size limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use binary.Write for consistent cross-platform encoding</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: fsync() is critical for durability guarantees</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// EvaluateRetentionPolicies identifies chunks eligible for cleanup</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RetentionManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">EvaluateRetentionPolicies</span><span style=\"color:#E1E4E8\">() ([]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Load all active retention policies from configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Scan chunk metadata to identify candidates based on age</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Apply policy matching logic using label selectors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Resolve conflicts between multiple applicable policies</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Check grace periods and safety constraints for each candidate</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Return list of chunk IDs ready for safe deletion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use time.Since() for age calculations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Sort policies by priority for conflict resolution</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SafeChunkDeletion removes chunks while coordinating with active queries</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RetentionManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SafeChunkDeletion</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">chunkIDs</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire retention mutex to prevent concurrent deletions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For each chunk, verify no active queries hold references</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Mark chunks as \"pending deletion\" in metadata index</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Wait for existing query references to be released</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Perform actual file deletion and update audit logs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Remove chunk entries from metadata index atomically</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use reference counting or cooperative cancellation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Log detailed audit information for compliance tracking</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Language-Specific Hints:</strong></p>\n<ul>\n<li>Use <code>os.File.Sync()</code> for fsync operations to ensure WAL durability—this forces the OS to write buffered data to physical storage</li>\n<li>Consider <code>syscall.Madvise()</code> with <code>MADV_SEQUENTIAL</code> for chunk files to optimize OS page cache behavior</li>\n<li>Use <code>encoding/binary</code> with <code>binary.LittleEndian</code> for cross-platform chunk format compatibility</li>\n<li>Implement compression interface to swap algorithms: <code>compress/gzip</code>, <code>github.com/klauspost/compress/zstd</code>, <code>github.com/pierrec/lz4</code></li>\n<li>Use <code>sync.Pool</code> for compression buffer reuse to reduce GC pressure during high-throughput scenarios</li>\n<li>Consider <code>mmap</code> for read-only chunk access to reduce memory copying: <code>golang.org/x/sys/unix.Mmap</code></li>\n</ul>\n<p><strong>Milestone Checkpoint:</strong></p>\n<p>After implementing the storage engine, verify these behaviors:</p>\n<ol>\n<li><strong>Chunk Creation</strong>: Run <code>go test ./internal/storage/chunk/...</code> and verify chunk files are created with correct headers and compression</li>\n<li><strong>WAL Durability</strong>: Kill the process during writes and restart—verify WAL recovery replays uncommitted operations</li>\n<li><strong>Retention Cleanup</strong>: Configure short retention period and verify chunks are deleted after expiration with audit logs</li>\n<li><strong>Compression Effectiveness</strong>: Store sample log data and measure compression ratios match expected values (60-80% for zstd)</li>\n<li><strong>Performance Baseline</strong>: Measure write throughput (should handle 10,000+ entries/sec) and query decompression speed</li>\n</ol>\n<p><strong>Signs of Correct Implementation:</strong></p>\n<ul>\n<li>Chunk files have valid headers and can be decompressed successfully</li>\n<li>WAL files rotate at configured size limits without unbounded growth</li>\n<li>Retention policies delete expired chunks within expected time windows</li>\n<li>Storage metrics show reasonable compression ratios and throughput numbers</li>\n<li>Recovery after simulated crashes replays all uncommitted operations correctly</li>\n</ul>\n<p><strong>Signs Something Is Wrong:</strong></p>\n<ul>\n<li>Chunk files are corrupted or cannot be decompressed → Check binary serialization and compression error handling</li>\n<li>WAL files grow indefinitely → Verify checkpoint and rotation logic is working properly  </li>\n<li>Retention policies don&#39;t delete expired data → Check policy evaluation logic and file permission issues</li>\n<li>Poor compression ratios (&lt; 50%) → Verify compression algorithm selection and data serialization format</li>\n<li>Slow write performance (&lt; 1000 entries/sec) → Profile fsync overhead and buffer management efficiency</li>\n</ul>\n<h2 id=\"multi-tenancy-and-alerting\">Multi-Tenancy and Alerting</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section corresponds to Milestone 5 (Multi-Tenant &amp; Alerting), where we add tenant isolation with per-tenant rate limits and log-based alerting rules. This builds upon all previous milestones, extending the system with enterprise features for production deployment.</p>\n</blockquote>\n<h3 id=\"mental-model-the-apartment-building\">Mental Model: The Apartment Building</h3>\n<p>Think of our log aggregation system as a large apartment building where each tenant (organization) has their own private unit. Just as apartment buildings need secure entry systems, individual mailboxes, separate utility meters, and building-wide fire safety alerts, our multi-tenant log system needs authentication barriers, isolated data storage, per-tenant resource limits, and system-wide alerting capabilities.</p>\n<p>In this apartment building analogy, the <strong>building manager</strong> represents our tenant isolation layer - checking IDs at the front door, ensuring residents can only access their own units, and tracking each unit&#39;s water and electricity usage. The <strong>mailroom</strong> represents our ingestion pipeline with per-tenant rate limiting - preventing any single resident from overwhelming the building&#39;s mail capacity. The <strong>fire alarm system</strong> represents our alerting engine - monitoring all units for dangerous patterns and notifying the appropriate parties when problems arise.</p>\n<p>Just as a poorly designed apartment building might have thin walls (data leakage), broken locks (authentication bypasses), or residents who flood the building&#39;s water system (resource exhaustion), our multi-tenant log system must carefully design isolation boundaries, access controls, and resource management to prevent one tenant&#39;s activity from affecting others.</p>\n<p>The key insight from this analogy is that multi-tenancy isn&#39;t just about storing data separately - it&#39;s about creating comprehensive isolation at every layer while efficiently sharing the underlying infrastructure. The building&#39;s foundation, plumbing, and electrical systems are shared for efficiency, but each tenant gets their own protected space with measured resource consumption.</p>\n<h3 id=\"tenant-isolation-design\">Tenant Isolation Design</h3>\n<p><strong>Multi-tenant isolation</strong> in a log aggregation system requires creating secure boundaries between different organizations or teams while maximizing infrastructure efficiency. Unlike simple access control where we trust users not to peek at each other&#39;s data, tenant isolation assumes potential adversaries and enforces separation at the architectural level.</p>\n<p><img src=\"/api/project/log-aggregator/architecture-doc/asset?path=diagrams%2Ftenant-isolation.svg\" alt=\"Multi-Tenant Architecture\"></p>\n<p>Our tenant isolation strategy operates on three fundamental principles: <strong>authentication identity</strong>, <strong>authorization boundaries</strong>, and <strong>data segregation</strong>. Every request entering the system must first establish which tenant it represents, then verify that tenant has permission for the requested operation, and finally ensure all data access respects tenant boundaries throughout the entire processing pipeline.</p>\n<p>The <strong>tenant identification</strong> process begins at the ingestion and query entry points. For HTTP endpoints, we extract tenant identity from request headers, authentication tokens, or URL paths. For TCP and UDP syslog ingestion, tenant identity comes from source IP ranges, TLS certificates, or message headers. This identification must happen before any data processing begins, as it determines how every subsequent operation behaves.</p>\n<table>\n<thead>\n<tr>\n<th>Tenant Identification Method</th>\n<th>Protocol</th>\n<th>Security Level</th>\n<th>Implementation Complexity</th>\n<th>Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Bearer Token</td>\n<td>HTTP</td>\n<td>High</td>\n<td>Medium</td>\n<td>API clients, log shippers</td>\n</tr>\n<tr>\n<td>TLS Client Certificate</td>\n<td>TCP/HTTP</td>\n<td>High</td>\n<td>High</td>\n<td>High-security environments</td>\n</tr>\n<tr>\n<td>Source IP Mapping</td>\n<td>TCP/UDP</td>\n<td>Medium</td>\n<td>Low</td>\n<td>Trusted network segments</td>\n</tr>\n<tr>\n<td>Message Header Field</td>\n<td>Syslog</td>\n<td>Low</td>\n<td>Low</td>\n<td>Development environments</td>\n</tr>\n<tr>\n<td>URL Path Prefix</td>\n<td>HTTP</td>\n<td>Low</td>\n<td>Low</td>\n<td>Simple testing setups</td>\n</tr>\n</tbody></table>\n<p><strong>Authorization boundaries</strong> define what operations each tenant can perform within their isolated environment. Our authorization model uses <strong>role-based access control (RBAC)</strong> where each tenant has roles like <code>log-writer</code>, <code>log-reader</code>, and <code>admin</code>. These roles map to specific capabilities across our system components.</p>\n<table>\n<thead>\n<tr>\n<th>Role</th>\n<th>Ingestion Rights</th>\n<th>Query Rights</th>\n<th>Admin Rights</th>\n<th>Typical Users</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>log-writer</td>\n<td>Write to tenant streams</td>\n<td>None</td>\n<td>None</td>\n<td>Application servers, log agents</td>\n</tr>\n<tr>\n<td>log-reader</td>\n<td>None</td>\n<td>Read tenant streams</td>\n<td>None</td>\n<td>Developers, monitoring tools</td>\n</tr>\n<tr>\n<td>admin</td>\n<td>Full ingestion</td>\n<td>Full queries</td>\n<td>Manage retention, alerts</td>\n<td>Operations teams</td>\n</tr>\n<tr>\n<td>audit-reader</td>\n<td>None</td>\n<td>Read audit logs only</td>\n<td>None</td>\n<td>Security teams</td>\n</tr>\n<tr>\n<td>service-account</td>\n<td>Specific streams only</td>\n<td>Specific streams only</td>\n<td>None</td>\n<td>Automated systems</td>\n</tr>\n</tbody></table>\n<p><strong>Data segregation</strong> ensures that tenant data remains completely separate throughout the entire data lifecycle. This segregation occurs at multiple levels: logical separation in our data structures, physical separation in storage chunks, and operational separation in background processes like indexing and retention cleanup.</p>\n<p>At the <strong>logical level</strong>, every <code>LogEntry</code> carries tenant context through its <code>Labels</code> map, where a reserved <code>__tenant_id__</code> label identifies ownership. This label is automatically injected during ingestion and used to filter all subsequent operations. The indexing engine creates separate <code>IndexSegment</code> instances per tenant, preventing cross-tenant term lookups even at the data structure level.</p>\n<p>At the <strong>physical level</strong>, our storage engine organizes chunks by tenant identity, creating separate directory hierarchies for each tenant&#39;s data. This physical separation provides additional security guarantees and enables tenant-specific storage policies. The write-ahead log maintains separate WAL files per tenant, ensuring that even crash recovery respects tenant boundaries.</p>\n<table>\n<thead>\n<tr>\n<th>Segregation Level</th>\n<th>Implementation</th>\n<th>Security Benefit</th>\n<th>Performance Impact</th>\n<th>Recovery Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Label-based</td>\n<td><strong>tenant_id</strong> in Labels</td>\n<td>Logical isolation</td>\n<td>Low overhead</td>\n<td>Simple filtering</td>\n</tr>\n<tr>\n<td>Index-based</td>\n<td>Separate IndexSegment per tenant</td>\n<td>Query isolation</td>\n<td>Medium overhead</td>\n<td>Per-tenant rebuild</td>\n</tr>\n<tr>\n<td>Chunk-based</td>\n<td>Tenant-specific directories</td>\n<td>Storage isolation</td>\n<td>Low overhead</td>\n<td>Independent recovery</td>\n</tr>\n<tr>\n<td>WAL-based</td>\n<td>Per-tenant WAL files</td>\n<td>Write isolation</td>\n<td>Medium overhead</td>\n<td>Parallel replay</td>\n</tr>\n</tbody></table>\n<p>The <strong>tenant context propagation</strong> mechanism ensures that tenant identity flows correctly through every processing stage. When a log entry enters the ingestion pipeline, we attach tenant context to the processing thread or goroutine. This context travels with the entry through parsing, indexing, and storage, ensuring that all operations respect tenant boundaries without requiring explicit tenant checks at every step.</p>\n<p><strong>Resource quotas</strong> prevent any single tenant from overwhelming shared system resources. Our quota system tracks multiple dimensions of resource consumption: ingestion rate (entries per second), storage usage (bytes on disk), query frequency (queries per minute), and memory consumption (active query memory). These quotas are enforced at the component level, with graceful degradation when limits are approached.</p>\n<table>\n<thead>\n<tr>\n<th>Resource Type</th>\n<th>Quota Mechanism</th>\n<th>Enforcement Point</th>\n<th>Overflow Behavior</th>\n<th>Recovery Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Ingestion Rate</td>\n<td>Token bucket per tenant</td>\n<td>HTTP/TCP handlers</td>\n<td>Return 429 status</td>\n<td>Exponential backoff</td>\n</tr>\n<tr>\n<td>Storage Size</td>\n<td>Byte counting per tenant</td>\n<td>Chunk writer</td>\n<td>Reject new writes</td>\n<td>Trigger retention cleanup</td>\n</tr>\n<tr>\n<td>Query Concurrency</td>\n<td>Active query limit</td>\n<td>Query engine</td>\n<td>Queue or reject</td>\n<td>Finish existing queries</td>\n</tr>\n<tr>\n<td>Memory Usage</td>\n<td>Per-query limits</td>\n<td>Result processing</td>\n<td>Terminate query</td>\n<td>Return partial results</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight:</strong> The key challenge in multi-tenancy is balancing security isolation with operational efficiency. Overly strict isolation (separate databases per tenant) becomes operationally complex, while insufficient isolation (shared tables with row-level security) creates security risks. Our approach uses logical isolation with physical separation at the storage layer, providing strong security guarantees while maintaining operational simplicity.</p>\n</blockquote>\n<h3 id=\"rate-limiting-and-quotas\">Rate Limiting and Quotas</h3>\n<p><strong>Rate limiting</strong> in a multi-tenant log aggregation system serves dual purposes: preventing individual tenants from overwhelming shared infrastructure and ensuring fair resource allocation across all tenants. Unlike simple throttling that just slows down requests, our rate limiting system implements sophisticated quota management with different limits for different resource types and tenant tiers.</p>\n<p>The <strong>token bucket algorithm</strong> forms the foundation of our rate limiting approach. Each tenant receives separate token buckets for different resource types: ingestion tokens (for log entries), query tokens (for search requests), and bandwidth tokens (for data transfer). These buckets refill at configurable rates, allowing burst capacity while enforcing sustained rate limits.</p>\n<p>Our token bucket implementation uses <strong>hierarchical buckets</strong> to handle different granularities of rate limiting. The top-level bucket controls the tenant&#39;s overall ingestion rate, while subsidiary buckets limit specific log streams within that tenant. This hierarchy prevents a single noisy application from consuming a tenant&#39;s entire quota while allowing legitimate burst traffic from other applications.</p>\n<table>\n<thead>\n<tr>\n<th>Bucket Level</th>\n<th>Scope</th>\n<th>Typical Limit</th>\n<th>Refill Rate</th>\n<th>Burst Capacity</th>\n<th>Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Global</td>\n<td>Entire system</td>\n<td>1M entries/sec</td>\n<td>Constant</td>\n<td>2x rate</td>\n<td>System protection</td>\n</tr>\n<tr>\n<td>Tenant</td>\n<td>Single tenant</td>\n<td>100K entries/sec</td>\n<td>Configurable</td>\n<td>5x rate</td>\n<td>Fair sharing</td>\n</tr>\n<tr>\n<td>Stream</td>\n<td>Label set</td>\n<td>10K entries/sec</td>\n<td>Proportional</td>\n<td>2x rate</td>\n<td>Application isolation</td>\n</tr>\n<tr>\n<td>Client</td>\n<td>Source IP</td>\n<td>1K entries/sec</td>\n<td>Fixed</td>\n<td>1x rate</td>\n<td>Abuse prevention</td>\n</tr>\n</tbody></table>\n<p><strong>Quota enforcement</strong> occurs at multiple stages in the ingestion pipeline to provide both early rejection and accurate accounting. The HTTP handler performs an initial quota check before accepting request bodies, preventing expensive parsing of requests that will ultimately be rejected. The buffer stage performs a second check before queuing entries, and the storage stage performs a final check before writing to disk.</p>\n<p>The <strong>ingestion rate limiting</strong> mechanism uses a combination of reject-early and queue-throttling strategies. When a tenant approaches their rate limit, we begin applying random jittered delays to their requests, encouraging clients to back off gradually. When the limit is exceeded, we return HTTP 429 responses with <code>Retry-After</code> headers indicating when the client should attempt again.</p>\n<table>\n<thead>\n<tr>\n<th>Rate Limit State</th>\n<th>Utilization</th>\n<th>Response Strategy</th>\n<th>HTTP Status</th>\n<th>Backoff Signal</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Normal</td>\n<td>0-70%</td>\n<td>Accept immediately</td>\n<td>200 OK</td>\n<td>None</td>\n</tr>\n<tr>\n<td>Warning</td>\n<td>70-90%</td>\n<td>Add jittered delay</td>\n<td>200 OK</td>\n<td>X-RateLimit-Remaining</td>\n</tr>\n<tr>\n<td>Throttling</td>\n<td>90-100%</td>\n<td>Exponential delays</td>\n<td>200 OK</td>\n<td>X-RateLimit-Reset</td>\n</tr>\n<tr>\n<td>Exceeded</td>\n<td>&gt;100%</td>\n<td>Reject request</td>\n<td>429 Too Many</td>\n<td>Retry-After</td>\n</tr>\n</tbody></table>\n<p><strong>Storage quotas</strong> prevent tenants from consuming unbounded disk space while allowing temporary bursts during high-volume periods. Our storage quota system tracks both current usage and projected future usage based on recent ingestion patterns. When a tenant approaches their storage limit, we trigger aggressive retention cleanup for their data before resorting to ingestion rejection.</p>\n<p>The <strong>quota calculation</strong> algorithm considers both absolute limits (maximum bytes per tenant) and relative limits (percentage of total system capacity). This dual approach ensures that small tenants get guaranteed minimum resources while large tenants can utilize available capacity during low-usage periods.</p>\n<p><strong>Memory quotas</strong> for query operations prevent expensive queries from exhausting system RAM. Each query receives a memory budget based on the requesting tenant&#39;s limits and current system load. The query engine monitors memory usage throughout execution and terminates queries that exceed their allocation, returning partial results with appropriate error messages.</p>\n<table>\n<thead>\n<tr>\n<th>Memory Limit Type</th>\n<th>Calculation</th>\n<th>Enforcement</th>\n<th>Overflow Action</th>\n<th>Recovery Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Per-query</td>\n<td>min(tenant_limit, system_available/active_queries)</td>\n<td>Query engine</td>\n<td>Terminate with error</td>\n<td>Retry with smaller scope</td>\n</tr>\n<tr>\n<td>Per-tenant</td>\n<td>configured_limit * tenant_tier_multiplier</td>\n<td>Admission control</td>\n<td>Queue new queries</td>\n<td>Wait for completion</td>\n</tr>\n<tr>\n<td>System-wide</td>\n<td>total_ram * 0.8</td>\n<td>Global limiter</td>\n<td>Reject all queries</td>\n<td>System load balancing</td>\n</tr>\n</tbody></table>\n<p><strong>Burst handling</strong> allows tenants to exceed sustained rates for short periods, accommodating real-world traffic patterns where applications generate logs in bursts rather than steady streams. Our burst algorithm uses a <strong>sliding window</strong> approach, tracking ingestion rates over multiple time scales (1 minute, 5 minutes, 1 hour) and allowing short-term spikes as long as longer-term averages remain within limits.</p>\n<blockquote>\n<p><strong>Design Insight:</strong> Effective rate limiting requires understanding the difference between malicious abuse and legitimate usage patterns. Log generation is inherently bursty - applications restart, batch jobs run, errors cascade. Our rate limiting system must accommodate these patterns while still protecting system stability. The key is using multiple time windows and burst allowances rather than simple per-second limits.</p>\n</blockquote>\n<h3 id=\"log-based-alerting-engine\">Log-Based Alerting Engine</h3>\n<p><strong>Log-based alerting</strong> transforms our log aggregation system from a passive storage repository into an active monitoring platform that can detect problems and notify operators in real-time. Unlike traditional metric-based alerting that relies on pre-aggregated counters, log-based alerting operates directly on the raw log stream, enabling detection of complex patterns that might not be visible in summary statistics.</p>\n<p>The <strong>alerting architecture</strong> consists of three main components: the <strong>rule engine</strong> that evaluates alert conditions, the <strong>notification dispatcher</strong> that delivers alerts through various channels, and the <strong>deduplication system</strong> that prevents alert storms. These components operate continuously on the incoming log stream, providing near real-time detection of problematic patterns.</p>\n<p>Our <strong>rule definition language</strong> extends LogQL with temporal operators and threshold functions. Alert rules specify a LogQL query, an evaluation window, and trigger conditions. For example, an alert might trigger when error logs from the payment service exceed 10 occurrences in any 5-minute window, or when any log contains specific security-related keywords.</p>\n<table>\n<thead>\n<tr>\n<th>Rule Component</th>\n<th>Purpose</th>\n<th>Example</th>\n<th>Evaluation Frequency</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Query</td>\n<td>Select relevant logs</td>\n<td>{service=&quot;payment&quot;} |= &quot;ERROR&quot;</td>\n<td>Every 30 seconds</td>\n</tr>\n<tr>\n<td>Window</td>\n<td>Time range for evaluation</td>\n<td>last 5 minutes</td>\n<td>Sliding window</td>\n</tr>\n<tr>\n<td>Condition</td>\n<td>Trigger threshold</td>\n<td>count &gt; 10</td>\n<td>Per evaluation</td>\n</tr>\n<tr>\n<td>Severity</td>\n<td>Alert importance</td>\n<td>critical, warning, info</td>\n<td>Static configuration</td>\n</tr>\n</tbody></table>\n<p><strong>Rule evaluation</strong> occurs continuously as new logs arrive, using an efficient <strong>streaming evaluation</strong> approach rather than periodic batch processing. As each log entry enters the system, the alerting engine checks it against all active rules whose query patterns it might match. This streaming approach provides faster detection than batch evaluation while using fewer system resources.</p>\n<p>The <strong>pattern matching engine</strong> uses the same inverted index infrastructure as our query system, but optimized for real-time evaluation rather than historical search. Alert rules are compiled into efficient matchers that can quickly determine whether an incoming log entry is relevant without performing expensive text processing.</p>\n<table>\n<thead>\n<tr>\n<th>Matching Strategy</th>\n<th>Use Case</th>\n<th>Performance</th>\n<th>Accuracy</th>\n<th>Resource Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Exact string match</td>\n<td>Known error messages</td>\n<td>Very fast</td>\n<td>Perfect</td>\n<td>Low CPU</td>\n</tr>\n<tr>\n<td>Regex patterns</td>\n<td>Flexible error detection</td>\n<td>Medium</td>\n<td>Good</td>\n<td>Medium CPU</td>\n</tr>\n<tr>\n<td>Label selectors</td>\n<td>Service/environment filtering</td>\n<td>Fast</td>\n<td>Perfect</td>\n<td>Low CPU</td>\n</tr>\n<tr>\n<td>Full-text search</td>\n<td>Unknown error patterns</td>\n<td>Slow</td>\n<td>Good</td>\n<td>High CPU</td>\n</tr>\n</tbody></table>\n<p><strong>State management</strong> for alert rules requires tracking evaluation windows and trigger conditions across time. Each rule maintains a sliding window of recent events, counting occurrences or tracking specific patterns. When the trigger condition is met, the rule transitions to an active state and generates an alert event.</p>\n<p><strong>Alert deduplication</strong> prevents notification fatigue by intelligently grouping related alerts and suppressing redundant notifications. Our deduplication algorithm considers multiple factors: alert content similarity, time proximity, affected services, and notification channels. Similar alerts within a configurable time window are grouped together, with notifications sent for the group rather than individual occurrences.</p>\n<table>\n<thead>\n<tr>\n<th>Deduplication Dimension</th>\n<th>Grouping Key</th>\n<th>Time Window</th>\n<th>Max Group Size</th>\n<th>Notification Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Exact message</td>\n<td>Message hash</td>\n<td>5 minutes</td>\n<td>1000 events</td>\n<td>Single notification</td>\n</tr>\n<tr>\n<td>Service errors</td>\n<td>Service label</td>\n<td>10 minutes</td>\n<td>500 events</td>\n<td>Periodic summaries</td>\n</tr>\n<tr>\n<td>Host problems</td>\n<td>Hostname</td>\n<td>15 minutes</td>\n<td>100 events</td>\n<td>Escalating frequency</td>\n</tr>\n<tr>\n<td>Security events</td>\n<td>Source IP</td>\n<td>1 hour</td>\n<td>50 events</td>\n<td>Immediate + summary</td>\n</tr>\n</tbody></table>\n<p><strong>Notification delivery</strong> supports multiple channels with different reliability guarantees and formatting options. The notification system uses a <strong>reliable delivery</strong> approach with retry logic, dead letter queues, and delivery confirmation tracking. Failed notifications are retried with exponential backoff, and persistent failures are escalated to alternative channels.</p>\n<table>\n<thead>\n<tr>\n<th>Channel Type</th>\n<th>Reliability</th>\n<th>Latency</th>\n<th>Format Options</th>\n<th>Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Webhook HTTP</td>\n<td>Medium</td>\n<td>Low</td>\n<td>JSON, custom templates</td>\n<td>ChatOps, ticketing systems</td>\n</tr>\n<tr>\n<td>Email SMTP</td>\n<td>High</td>\n<td>Medium</td>\n<td>HTML, plain text</td>\n<td>Human notifications</td>\n</tr>\n<tr>\n<td>Slack API</td>\n<td>Medium</td>\n<td>Low</td>\n<td>Rich formatting, threads</td>\n<td>Team communication</td>\n</tr>\n<tr>\n<td>PagerDuty</td>\n<td>Very High</td>\n<td>Low</td>\n<td>Structured events</td>\n<td>On-call escalation</td>\n</tr>\n<tr>\n<td>SNS/SQS</td>\n<td>High</td>\n<td>Very Low</td>\n<td>JSON events</td>\n<td>System integration</td>\n</tr>\n</tbody></table>\n<p><strong>Alert routing</strong> ensures that notifications reach the appropriate recipients based on tenant context, alert severity, and escalation policies. Each tenant configures their own routing rules, specifying which types of alerts should go to which notification channels. The routing system supports complex logic including time-based routing (different contacts for business hours vs. on-call), severity-based escalation, and service-specific routing.</p>\n<p><strong>Alert lifecycle management</strong> tracks alerts from initial trigger through resolution, providing audit trails and preventing duplicate handling. Active alerts are stored with their current state, notification history, and acknowledgment status. When the underlying condition resolves (e.g., error rate drops below threshold), the alert automatically transitions to a resolved state.</p>\n<table>\n<thead>\n<tr>\n<th>Alert State</th>\n<th>Triggers</th>\n<th>Available Actions</th>\n<th>Auto Transitions</th>\n<th>Notification Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Triggered</td>\n<td>Rule condition met</td>\n<td>Acknowledge, Escalate</td>\n<td>To Active after delay</td>\n<td>Initial notification</td>\n</tr>\n<tr>\n<td>Active</td>\n<td>Confirmation delay expires</td>\n<td>Acknowledge, Resolve</td>\n<td>To Resolved when fixed</td>\n<td>Reminder notifications</td>\n</tr>\n<tr>\n<td>Acknowledged</td>\n<td>Manual operator action</td>\n<td>Resolve, Escalate</td>\n<td>To Resolved when fixed</td>\n<td>Suppressed notifications</td>\n</tr>\n<tr>\n<td>Resolved</td>\n<td>Condition no longer met</td>\n<td>Reopen</td>\n<td>To Triggered if recurs</td>\n<td>Resolution notification</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight:</strong> The biggest challenge in log-based alerting is balancing sensitivity with noise. Too-sensitive rules generate alert fatigue, while too-conservative rules miss real problems. The key is providing sophisticated deduplication and grouping capabilities so that operators can configure sensitive detection knowing that related alerts will be intelligently grouped rather than flooding notification channels.</p>\n</blockquote>\n<h3 id=\"architecture-decision-records\">Architecture Decision Records</h3>\n<p>Our multi-tenancy and alerting implementation required several critical architectural decisions that significantly impact system behavior, security, and operational characteristics.</p>\n<blockquote>\n<p><strong>Decision: Tenant Identification Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: We needed a mechanism to identify which tenant each log entry and query belongs to while supporting multiple ingestion protocols (HTTP, TCP, UDP) and authentication methods</li>\n<li><strong>Options Considered</strong>: <ul>\n<li>URL-based tenant identification (<code>/api/v1/logs/{tenant-id}</code>)</li>\n<li>HTTP header-based identification (<code>X-Tenant-ID: tenant123</code>)</li>\n<li>Authentication token embedded tenant ID (JWT claims)</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Hybrid approach using HTTP headers as primary with token-based fallback</li>\n<li><strong>Rationale</strong>: HTTP headers provide simplicity for most clients while token-based identification supports advanced authentication flows. URL-based identification creates routing complexity and makes tenant ID visible in access logs</li>\n<li><strong>Consequences</strong>: Clients must include tenant identification in headers, but this enables flexible authentication integration and keeps tenant information secure in encrypted connections</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Security</th>\n<th>Client Simplicity</th>\n<th>Protocol Support</th>\n<th>Chosen</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>URL Path</td>\n<td>Low (visible in logs)</td>\n<td>High</td>\n<td>HTTP only</td>\n<td>No</td>\n</tr>\n<tr>\n<td>HTTP Header</td>\n<td>Medium</td>\n<td>Medium</td>\n<td>HTTP only</td>\n<td>Yes (Primary)</td>\n</tr>\n<tr>\n<td>Token Claims</td>\n<td>High</td>\n<td>Low</td>\n<td>All protocols</td>\n<td>Yes (Fallback)</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Data Isolation Level</strong></p>\n<ul>\n<li><strong>Context</strong>: Multi-tenant systems must prevent data leakage between tenants while maintaining query performance and operational simplicity</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Database-per-tenant (complete physical separation)</li>\n<li>Schema-per-tenant (logical separation with shared infrastructure)</li>\n<li>Row-level security (shared tables with access controls)</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Hybrid logical isolation with physical storage separation</li>\n<li><strong>Rationale</strong>: Complete physical separation creates operational complexity and resource waste. Pure logical separation creates security risks. Our hybrid approach uses shared indexes and query infrastructure with tenant-separated storage chunks</li>\n<li><strong>Consequences</strong>: Strong security guarantees with moderate operational complexity, but requires careful implementation to prevent logical separation bypasses</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Security Level</th>\n<th>Operational Complexity</th>\n<th>Resource Efficiency</th>\n<th>Chosen</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Database-per-tenant</td>\n<td>Very High</td>\n<td>Very High</td>\n<td>Low</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Schema-per-tenant</td>\n<td>High</td>\n<td>High</td>\n<td>Medium</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Row-level security</td>\n<td>Medium</td>\n<td>Medium</td>\n<td>High</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Hybrid isolation</td>\n<td>High</td>\n<td>Medium</td>\n<td>Medium</td>\n<td>Yes</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Rate Limiting Algorithm</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to prevent individual tenants from overwhelming shared resources while allowing legitimate burst traffic patterns common in log generation</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Fixed window rate limiting (simple counter per time window)</li>\n<li>Sliding window rate limiting (precise but memory intensive)</li>\n<li>Token bucket algorithm (burst-friendly with sustained limits)</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Token bucket with hierarchical buckets per tenant</li>\n<li><strong>Rationale</strong>: Log traffic is inherently bursty (application restarts, batch jobs, error cascades). Token bucket naturally accommodates bursts while enforcing sustained rate limits. Hierarchical buckets provide granular control</li>\n<li><strong>Consequences</strong>: More complex implementation than fixed windows, but much better user experience for legitimate traffic patterns</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Burst Handling</th>\n<th>Memory Usage</th>\n<th>Implementation Complexity</th>\n<th>Chosen</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Fixed Window</td>\n<td>Poor</td>\n<td>Low</td>\n<td>Low</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Sliding Window</td>\n<td>Good</td>\n<td>High</td>\n<td>Medium</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Token Bucket</td>\n<td>Excellent</td>\n<td>Medium</td>\n<td>Medium</td>\n<td>Yes</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Alert Rule Evaluation Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Alert rules must evaluate continuously against incoming log streams without significantly impacting ingestion performance</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Batch evaluation (periodic rule evaluation against recent logs)</li>\n<li>Streaming evaluation (check each log against relevant rules)</li>\n<li>Hybrid approach (streaming for fast rules, batch for complex queries)</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Streaming evaluation with efficient pattern matching</li>\n<li><strong>Rationale</strong>: Batch evaluation introduces latency that defeats the purpose of real-time alerting. Pure streaming provides fastest detection. Pattern matching optimization makes streaming feasible even with many rules</li>\n<li><strong>Consequences</strong>: Faster alert detection but requires sophisticated rule optimization and may impact ingestion throughput under high rule loads</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Detection Latency</th>\n<th>Ingestion Impact</th>\n<th>Rule Complexity Support</th>\n<th>Chosen</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Batch Evaluation</td>\n<td>High (1-5 minutes)</td>\n<td>Low</td>\n<td>High</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Streaming</td>\n<td>Very Low (&lt;30 sec)</td>\n<td>Medium</td>\n<td>Medium</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Hybrid</td>\n<td>Medium</td>\n<td>Medium</td>\n<td>High</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Alert Deduplication Approach</strong></p>\n<ul>\n<li><strong>Context</strong>: Log-based alerts can generate massive notification volumes during incidents, creating alert fatigue and masking important information</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Time-based deduplication (suppress identical alerts within time windows)</li>\n<li>Content-based grouping (group similar alert messages together)</li>\n<li>Machine learning clustering (automatically discover alert patterns)</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Multi-dimensional deduplication using time, content, and service context</li>\n<li><strong>Rationale</strong>: Pure time-based deduplication misses important variations. Content-only grouping doesn&#39;t account for context. ML clustering adds complexity. Multi-dimensional approach balances effectiveness with implementation simplicity</li>\n<li><strong>Consequences</strong>: Sophisticated deduplication reduces noise significantly but requires careful tuning of grouping parameters per tenant</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Noise Reduction</th>\n<th>Configuration Complexity</th>\n<th>Resource Usage</th>\n<th>Chosen</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Time-based only</td>\n<td>Low</td>\n<td>Low</td>\n<td>Low</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Content-based only</td>\n<td>Medium</td>\n<td>Medium</td>\n<td>Medium</td>\n<td>No</td>\n</tr>\n<tr>\n<td>ML Clustering</td>\n<td>High</td>\n<td>Very High</td>\n<td>High</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Multi-dimensional</td>\n<td>High</td>\n<td>Medium</td>\n<td>Medium</td>\n<td>Yes</td>\n</tr>\n</tbody></table>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>Multi-tenancy and alerting systems introduce complex security and operational challenges that can create subtle but serious problems in production deployments.</p>\n<p>⚠️ <strong>Pitfall: Tenant ID Injection Attacks</strong></p>\n<p>One of the most dangerous vulnerabilities in multi-tenant systems occurs when tenant identification can be manipulated by malicious clients. Developers often make the mistake of trusting client-provided tenant identifiers without proper validation, allowing attackers to access other tenants&#39; data by simply changing header values or token claims.</p>\n<p>This happens when the system accepts tenant IDs from user-controlled inputs (HTTP headers, query parameters, form fields) and uses them directly in database queries or file system operations. For example, if a client sends <code>X-Tenant-ID: victim-tenant</code> and the system blindly uses this value to construct file paths or database queries, the attacker can access any tenant&#39;s data.</p>\n<p>The correct approach is to derive tenant identity from authenticated and cryptographically verified sources. Authentication tokens should be validated against a trusted identity provider, and tenant membership should be verified against an authoritative directory. Never trust client-provided tenant identifiers without cryptographic proof of legitimacy.</p>\n<p>Implementation fix: Always extract tenant context from verified authentication tokens rather than client headers. Use a mapping service that validates token claims against tenant membership databases. Implement audit logging for all tenant context establishment to detect injection attempts.</p>\n<p>⚠️ <strong>Pitfall: Alert Storm Cascades</strong></p>\n<p>Log-based alerting systems can create devastating feedback loops where alerts themselves generate logs that trigger more alerts, creating exponentially growing alert volumes that overwhelm notification systems and operators. This commonly occurs when alert notification failures are logged as errors, triggering alerts about alert system failures.</p>\n<p>The cascade typically starts with a legitimate problem that triggers an alert. If the notification system fails (webhook timeouts, email server issues), the alerting system logs error messages about delivery failures. If there&#39;s an alert rule monitoring the alerting system&#39;s own logs for errors, it triggers additional alerts about the notification failures. These new alerts also fail to deliver, creating more error logs and more alerts in an exponential cascade.</p>\n<p>This problem is particularly severe in distributed systems where cascading failures cause multiple services to simultaneously generate error logs, each triggering their own alert rules, overwhelming the notification infrastructure and making it impossible for operators to identify the root cause.</p>\n<p>Prevention requires implementing alert rule hierarchies with different reliability guarantees, circuit breakers on notification delivery, and careful separation of system monitoring from application monitoring. Alert delivery failures should never trigger the same alert delivery mechanisms that are already failing.</p>\n<p>⚠️ <strong>Pitfall: Resource Leakage Between Tenants</strong></p>\n<p>Shared infrastructure in multi-tenant systems can create subtle resource leakage where one tenant&#39;s activity affects another tenant&#39;s performance. This commonly occurs in memory management, connection pooling, and background processing where resources allocated for one tenant aren&#39;t properly released or isolated.</p>\n<p>Memory leakage often happens when tenant-specific data structures (query results, parsed log entries, index segments) aren&#39;t properly garbage collected because they&#39;re referenced by shared global caches or background goroutines. A single tenant with high query volume can gradually consume all available memory, degrading performance for other tenants.</p>\n<p>Connection leakage occurs when database connections or HTTP client connections opened for one tenant&#39;s operations aren&#39;t properly returned to pools, eventually exhausting the connection limits and preventing other tenants from accessing resources.</p>\n<p>Background processing leakage happens when cleanup tasks, indexing operations, or retention processes get stuck processing one tenant&#39;s data, preventing other tenants&#39; background work from completing. This can cause query performance degradation and storage cleanup delays.</p>\n<p>Mitigation requires implementing proper resource accounting at the tenant level, with explicit quotas and cleanup procedures. Use tenant-scoped context cancellation to ensure long-running operations can be interrupted. Implement resource monitoring that can identify which tenant is consuming resources and enforce limits before system-wide impact occurs.</p>\n<p>⚠️ <strong>Pitfall: Authentication Bypass in Protocol Handlers</strong></p>\n<p>Multi-protocol ingestion systems (HTTP, TCP, UDP) often have inconsistent authentication enforcement, creating security gaps where attackers can bypass tenant isolation by choosing different ingestion protocols. Developers frequently implement robust authentication for HTTP endpoints but forget to apply the same rigor to TCP and UDP handlers.</p>\n<p>This commonly occurs when HTTP endpoints require Bearer tokens or API keys, but TCP syslog handlers only check source IP addresses or accept any connection. Attackers can discover these protocol inconsistencies and inject logs into other tenants&#39; streams by sending syslog messages to TCP ports instead of using authenticated HTTP endpoints.</p>\n<p>UDP handlers are particularly vulnerable because they&#39;re connectionless and often lack any authentication mechanism. If the system relies on source IP filtering for UDP authentication, attackers can spoof IP addresses to impersonate legitimate log sources.</p>\n<p>The solution requires implementing consistent authentication policies across all ingestion protocols. For TCP connections, use TLS client certificates or require authentication tokens in message headers. For UDP, implement cryptographic message signing or restrict UDP ingestion to trusted network segments with strong perimeter security.</p>\n<p>⚠️ <strong>Pitfall: Alert Rule Performance Degradation</strong></p>\n<p>As tenants add more alert rules, the streaming evaluation system can gradually degrade ingestion performance, especially when rules use expensive operations like regular expressions or complex LogQL queries. This degradation often goes unnoticed until ingestion latency becomes problematic during high-volume periods.</p>\n<p>The problem occurs because each incoming log entry must be evaluated against all potentially matching alert rules. With hundreds of tenants each having dozens of alert rules, a single log entry might require thousands of rule evaluations. Complex regex patterns or full-text search operations in alert rules can make this evaluation extremely expensive.</p>\n<p>Rule optimization becomes critical for system scalability. Implement rule compilation that converts alert patterns into efficient finite state machines. Use bloom filters to quickly exclude logs that couldn&#39;t possibly match rule patterns. Provide rule authoring guidance that encourages efficient patterns and warns about expensive operations.</p>\n<p>Monitor rule evaluation performance and implement automatic rule disabling for patterns that consistently exceed performance budgets. Provide tenants with rule performance analytics so they can optimize their alert configurations.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>Multi-tenancy and alerting require integrating security controls throughout the system architecture while maintaining the performance characteristics established in previous milestones.</p>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Authentication</td>\n<td>HTTP Basic Auth + static tenant mapping</td>\n<td>JWT tokens with RSA signatures</td>\n</tr>\n<tr>\n<td>Authorization</td>\n<td>Role-based access with config files</td>\n<td>Policy-based access with Open Policy Agent</td>\n</tr>\n<tr>\n<td>Rate Limiting</td>\n<td>In-memory token buckets</td>\n<td>Redis-backed distributed rate limiting</td>\n</tr>\n<tr>\n<td>Alert Storage</td>\n<td>File-based rule storage</td>\n<td>Database with rule versioning</td>\n</tr>\n<tr>\n<td>Notifications</td>\n<td>HTTP webhooks only</td>\n<td>Multi-channel with delivery tracking</td>\n</tr>\n<tr>\n<td>Deduplication</td>\n<td>Time-window based grouping</td>\n<td>Content similarity with clustering</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/\n  auth/\n    auth.go                 ← tenant authentication and context\n    middleware.go           ← HTTP middleware for tenant extraction\n    rbac.go                 ← role-based access control\n  ratelimit/\n    bucket.go               ← token bucket implementation\n    limiter.go              ← hierarchical rate limiting\n    quota.go                ← storage and resource quotas\n  alerting/\n    engine.go               ← alert rule evaluation engine\n    rules.go                ← rule definition and compilation\n    notifications.go        ← multi-channel notification delivery\n    deduplication.go        ← alert grouping and deduplication\n  tenant/\n    context.go              ← tenant context propagation\n    isolation.go            ← data isolation enforcement\n    metrics.go              ← per-tenant resource tracking</code></pre></div>\n\n<p><strong>Authentication and Authorization Infrastructure:</strong></p>\n<p>The authentication system provides the foundation for all multi-tenant operations. This complete implementation handles JWT token validation and tenant context extraction:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> auth</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">crypto/rsa</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">errors</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strings</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/golang-jwt/jwt/v5</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TenantContext represents authenticated tenant information</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TenantContext</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TenantID    </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Roles       []</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Quotas      </span><span style=\"color:#B392F0\">ResourceQuotas</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AuthMethod  </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ExpiresAt   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ResourceQuotas</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxIngestionRate   </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">  // entries per second</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxStorageBytes    </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">  // total storage limit</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxActiveQueries   </span><span style=\"color:#F97583\">int32</span><span style=\"color:#6A737D\">  // concurrent query limit</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxQueryMemoryMB   </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">  // memory per query</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AuthService handles tenant authentication and authorization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> AuthService</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    publicKey    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">rsa</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">PublicKey</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tenantConfig </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">TenantConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    defaultQuotas </span><span style=\"color:#B392F0\">ResourceQuotas</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TenantConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Name          </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Quotas        </span><span style=\"color:#B392F0\">ResourceQuotas</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AllowedRoles  []</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    IPWhitelist   []</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewAuthService</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">publicKeyPath</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">configPath</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AuthService</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Load RSA public key for JWT verification</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    publicKey, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> loadPublicKey</span><span style=\"color:#E1E4E8\">(publicKeyPath)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to load public key: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Load tenant configuration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tenantConfig, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> loadTenantConfig</span><span style=\"color:#E1E4E8\">(configPath)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to load tenant config: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">AuthService</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        publicKey:    publicKey,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tenantConfig: tenantConfig,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        defaultQuotas: </span><span style=\"color:#B392F0\">ResourceQuotas</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            MaxIngestionRate:  </span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            MaxStorageBytes:   </span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#6A737D\">// 1GB</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            MaxActiveQueries:  </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            MaxQueryMemoryMB:  </span><span style=\"color:#79B8FF\">256</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AuthenticateRequest extracts and validates tenant context from HTTP request</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AuthService</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">AuthenticateRequest</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TenantContext</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Try JWT token first</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> authHeader </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> r.Header.</span><span style=\"color:#B392F0\">Get</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Authorization\"</span><span style=\"color:#E1E4E8\">); authHeader </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> a.</span><span style=\"color:#B392F0\">authenticateJWT</span><span style=\"color:#E1E4E8\">(authHeader)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Fall back to explicit tenant header (for development)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> tenantID </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> r.Header.</span><span style=\"color:#B392F0\">Get</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"X-Tenant-ID\"</span><span style=\"color:#E1E4E8\">); tenantID </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> a.</span><span style=\"color:#B392F0\">authenticateHeaderBased</span><span style=\"color:#E1E4E8\">(tenantID, r)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"no authentication provided\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AuthService</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">authenticateJWT</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">authHeader</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TenantContext</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Extract token from \"Bearer &#x3C;token>\" format</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parts </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> strings.</span><span style=\"color:#B392F0\">SplitN</span><span style=\"color:#E1E4E8\">(authHeader, </span><span style=\"color:#9ECBFF\">\" \"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(parts) </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#F97583\"> ||</span><span style=\"color:#E1E4E8\"> parts[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> \"Bearer\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"invalid authorization header format\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Parse and validate JWT</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    token, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> jwt.</span><span style=\"color:#B392F0\">Parse</span><span style=\"color:#E1E4E8\">(parts[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">token</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">jwt</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Token</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> _, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> token.Method.(</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">jwt</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">SigningMethodRSA</span><span style=\"color:#E1E4E8\">); </span><span style=\"color:#F97583\">!</span><span style=\"color:#E1E4E8\">ok {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"unexpected signing method: </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, token.Header[</span><span style=\"color:#9ECBFF\">\"alg\"</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> a.publicKey, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to parse JWT: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">token.Valid {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"invalid JWT token\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Extract claims</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    claims, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> token.Claims.(</span><span style=\"color:#B392F0\">jwt</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">MapClaims</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">ok {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"invalid JWT claims\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tenantID, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> claims[</span><span style=\"color:#9ECBFF\">\"tenant_id\"</span><span style=\"color:#E1E4E8\">].(</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">ok {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"missing tenant_id claim\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    roles, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> claims[</span><span style=\"color:#9ECBFF\">\"roles\"</span><span style=\"color:#E1E4E8\">].([]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{})</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">ok {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"missing roles claim\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Convert roles to string slice</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    roleStrs </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(roles))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i, role </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> roles {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> roleStr, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> role.(</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">); ok {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            roleStrs[i] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> roleStr</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Get tenant configuration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> a.tenantConfig[tenantID]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">exists {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"unknown tenant: </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, tenantID)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Extract expiration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    exp, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> claims[</span><span style=\"color:#9ECBFF\">\"exp\"</span><span style=\"color:#E1E4E8\">].(</span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">ok {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"missing exp claim\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">TenantContext</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TenantID:   tenantID,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Roles:      roleStrs,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Quotas:     config.Quotas,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        AuthMethod: </span><span style=\"color:#9ECBFF\">\"jwt\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ExpiresAt:  time.</span><span style=\"color:#B392F0\">Unix</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">(exp), </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CheckPermission verifies if tenant has required role for operation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TenantContext</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CheckPermission</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">requiredRole</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, role </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> tc.Roles {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> role </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> requiredRole </span><span style=\"color:#F97583\">||</span><span style=\"color:#E1E4E8\"> role </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"admin\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"tenant </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\"> lacks required role: </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, tc.TenantID, requiredRole)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HTTP middleware for tenant authentication</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AuthService</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">TenantAuthMiddleware</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">next</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">HandlerFunc</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">HandlerFunc</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tenantCtx, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> a.</span><span style=\"color:#B392F0\">AuthenticateRequest</span><span style=\"color:#E1E4E8\">(r)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            http.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">(w, fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Authentication failed: </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err), http.StatusUnauthorized)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Add tenant context to request</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ctx </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithValue</span><span style=\"color:#E1E4E8\">(r.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#9ECBFF\">\"tenant\"</span><span style=\"color:#E1E4E8\">, tenantCtx)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        next.</span><span style=\"color:#B392F0\">ServeHTTP</span><span style=\"color:#E1E4E8\">(w, r.</span><span style=\"color:#B392F0\">WithContext</span><span style=\"color:#E1E4E8\">(ctx))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetTenantContext extracts tenant context from request context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> GetTenantContext</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TenantContext</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tenant, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> ctx.</span><span style=\"color:#B392F0\">Value</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"tenant\"</span><span style=\"color:#E1E4E8\">).(</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TenantContext</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">ok {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"no tenant context found\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> tenant, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Rate Limiting Infrastructure:</strong></p>\n<p>This token bucket implementation provides hierarchical rate limiting with burst support:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> ratelimit</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TokenBucket implements token bucket algorithm for rate limiting</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TokenBucket</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu            </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Mutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    capacity      </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">     // maximum tokens in bucket</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens        </span><span style=\"color:#F97583\">float64</span><span style=\"color:#6A737D\">   // current token count</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    refillRate    </span><span style=\"color:#F97583\">float64</span><span style=\"color:#6A737D\">   // tokens added per second</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lastRefill    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#6A737D\"> // last refill timestamp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name          </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\">    // bucket identifier for monitoring</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewTokenBucket</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">capacity</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">refillRate</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">name</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TokenBucket</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">TokenBucket</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        capacity:   capacity,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tokens:     </span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\">(capacity), </span><span style=\"color:#6A737D\">// start full</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        refillRate: refillRate,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lastRefill: time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        name:       name,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TryConsume attempts to consume specified tokens, returns true if successful</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tb </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TokenBucket</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">TryConsume</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">tokens</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tb.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> tb.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tb.</span><span style=\"color:#B392F0\">refill</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> tb.tokens </span><span style=\"color:#F97583\">>=</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">(tokens) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tb.tokens </span><span style=\"color:#F97583\">-=</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">(tokens)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// refill adds tokens based on elapsed time since last refill</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tb </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TokenBucket</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">refill</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    now </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    elapsed </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> now.</span><span style=\"color:#B392F0\">Sub</span><span style=\"color:#E1E4E8\">(tb.lastRefill).</span><span style=\"color:#B392F0\">Seconds</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> elapsed </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tokensToAdd </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> elapsed </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> tb.refillRate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tb.tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> min</span><span style=\"color:#E1E4E8\">(tb.tokens </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> tokensToAdd, </span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\">(tb.capacity))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tb.lastRefill </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> now</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetStatus returns current bucket state for monitoring</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tb </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TokenBucket</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetStatus</span><span style=\"color:#E1E4E8\">() (</span><span style=\"color:#FFAB70\">current</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">capacity</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">rate</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tb.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> tb.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tb.</span><span style=\"color:#B392F0\">refill</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> tb.tokens, tb.capacity, tb.refillRate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HierarchicalLimiter manages multiple token buckets per tenant</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HierarchicalLimiter</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tenantBuckets </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TokenBucket</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    streamBuckets </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TokenBucket</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu           </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    defaultQuotas </span><span style=\"color:#B392F0\">ResourceQuotas</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewHierarchicalLimiter</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">defaultQuotas</span><span style=\"color:#B392F0\"> ResourceQuotas</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HierarchicalLimiter</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">HierarchicalLimiter</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tenantBuckets: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TokenBucket</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        streamBuckets: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TokenBucket</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        defaultQuotas: defaultQuotas,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CheckRateLimit verifies if operation is allowed under current limits</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hl </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HierarchicalLimiter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CheckRateLimit</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">tenantID</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">streamID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">tokens</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Check tenant-level bucket</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tenantBucket </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> hl.</span><span style=\"color:#B392F0\">getTenantBucket</span><span style=\"color:#E1E4E8\">(tenantID)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">tenantBucket.</span><span style=\"color:#B392F0\">TryConsume</span><span style=\"color:#E1E4E8\">(tokens) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"tenant </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\"> exceeded rate limit\"</span><span style=\"color:#E1E4E8\">, tenantID)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Check stream-level bucket</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    streamBucket </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> hl.</span><span style=\"color:#B392F0\">getStreamBucket</span><span style=\"color:#E1E4E8\">(streamID)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">streamBucket.</span><span style=\"color:#B392F0\">TryConsume</span><span style=\"color:#E1E4E8\">(tokens) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Refund tenant tokens since stream limit hit first</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tenantBucket.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tenantBucket.tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> min</span><span style=\"color:#E1E4E8\">(tenantBucket.tokens </span><span style=\"color:#F97583\">+</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">(tokens), </span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\">(tenantBucket.capacity))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tenantBucket.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"stream </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\"> exceeded rate limit\"</span><span style=\"color:#E1E4E8\">, streamID)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hl </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HierarchicalLimiter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">getTenantBucket</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">tenantID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TokenBucket</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hl.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bucket, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> hl.tenantBuckets[tenantID]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hl.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> exists {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> bucket</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Create new bucket with write lock</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hl.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> hl.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Double-check after acquiring write lock</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> bucket, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> hl.tenantBuckets[tenantID]; exists {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> bucket</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bucket </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> NewTokenBucket</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        hl.defaultQuotas.MaxIngestionRate,</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        float64</span><span style=\"color:#E1E4E8\">(hl.defaultQuotas.MaxIngestionRate),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"tenant-</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, tenantID),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hl.tenantBuckets[tenantID] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> bucket</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> bucket</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hl </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HierarchicalLimiter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">getStreamBucket</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">streamID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TokenBucket</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Similar implementation to getTenantBucket but for streams</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Streams get 1/10th of tenant rate as default</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    streamRate </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> hl.defaultQuotas.MaxIngestionRate </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 10</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hl.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bucket, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> hl.streamBuckets[streamID]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hl.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> exists {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> bucket</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hl.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> hl.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> bucket, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> hl.streamBuckets[streamID]; exists {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> bucket</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bucket </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> NewTokenBucket</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        streamRate,</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        float64</span><span style=\"color:#E1E4E8\">(streamRate),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"stream-</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, streamID),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hl.streamBuckets[streamID] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> bucket</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> bucket</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> min</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">a</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">b</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> a </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> b {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> a</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> b</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Alert Engine Core Implementation:</strong></p>\n<p>The alerting engine evaluates rules against incoming log streams and manages notification delivery:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> alerting</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AlertEngine coordinates rule evaluation and notification delivery</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> AlertEngine</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rules           </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertRule</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rulesMu         </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    notifier        </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NotificationManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    deduplicator    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertDeduplicator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    evaluationChan  </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">LogEntry</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stopChan        </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AlertRule defines conditions that trigger notifications</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> AlertRule</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RuleID       </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TenantID     </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Name         </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Query        </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Condition    </span><span style=\"color:#B392F0\">ThresholdCondition</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Window       </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Severity     </span><span style=\"color:#B392F0\">AlertSeverity</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Enabled      </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Internal state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    matcher      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RuleMatcher</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    eventWindow  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SlidingWindow</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lastFired    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ThresholdCondition</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Type      </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\">  // \"count\", \"rate\", \"contains\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Threshold </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Operator  </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\">  // \">\", \"&#x3C;\", \">=\", \"&#x3C;=\", \"==\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> AlertSeverity</span><span style=\"color:#F97583\"> string</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SeverityInfo</span><span style=\"color:#B392F0\">     AlertSeverity</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"info\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SeverityWarning</span><span style=\"color:#B392F0\">  AlertSeverity</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"warning\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SeverityCritical</span><span style=\"color:#B392F0\"> AlertSeverity</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"critical\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewAlertEngine</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">notifier</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">NotificationManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertEngine</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">AlertEngine</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        rules:          </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertRule</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        notifier:       notifier,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        deduplicator:   </span><span style=\"color:#B392F0\">NewAlertDeduplicator</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        evaluationChan: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">10000</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        stopChan:       </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Start begins rule evaluation goroutine</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ae </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    go</span><span style=\"color:#E1E4E8\"> ae.</span><span style=\"color:#B392F0\">evaluationLoop</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// EvaluateLogEntry checks incoming log against all relevant rules</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ae </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">EvaluateLogEntry</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">entry</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    select</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> ae.evaluationChan </span><span style=\"color:#F97583\">&#x3C;-</span><span style=\"color:#E1E4E8\"> entry:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Successfully queued for evaluation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    default</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Channel full, increment dropped evaluation metric</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // In production, this should trigger an alert about alert system overload</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ae </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">evaluationLoop</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        select</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#E1E4E8\"> entry </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">ae.evaluationChan:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ae.</span><span style=\"color:#B392F0\">evaluateEntry</span><span style=\"color:#E1E4E8\">(entry)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">ae.stopChan:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ae </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">evaluateEntry</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">entry</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tenantID </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> entry.Labels[</span><span style=\"color:#9ECBFF\">\"__tenant_id__\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ae.rulesMu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> ae.rulesMu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, rule </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> ae.rules {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Skip rules for different tenants</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> rule.TenantID </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> tenantID {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            continue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">rule.Enabled {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            continue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Check if log matches rule query</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> rule.matcher.</span><span style=\"color:#B392F0\">Matches</span><span style=\"color:#E1E4E8\">(entry) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            rule.eventWindow.</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(entry.Timestamp, </span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Evaluate threshold condition</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> ae.</span><span style=\"color:#B392F0\">evaluateCondition</span><span style=\"color:#E1E4E8\">(rule) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                ae.</span><span style=\"color:#B392F0\">fireAlert</span><span style=\"color:#E1E4E8\">(rule, entry)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// evaluateCondition checks if rule threshold is met</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ae </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">evaluateCondition</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">rule</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">AlertRule</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    now </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    windowStart </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> now.</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\">rule.Window)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    value </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> rule.eventWindow.</span><span style=\"color:#B392F0\">Sum</span><span style=\"color:#E1E4E8\">(windowStart, now)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    switch</span><span style=\"color:#E1E4E8\"> rule.Condition.Operator {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#9ECBFF\"> \">\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> value </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> rule.Condition.Threshold</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#9ECBFF\"> \">=\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> value </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> rule.Condition.Threshold</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#9ECBFF\"> \"&#x3C;\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> value </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> rule.Condition.Threshold</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#9ECBFF\"> \"&#x3C;=\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> value </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#E1E4E8\"> rule.Condition.Threshold</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#9ECBFF\"> \"==\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> value </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> rule.Condition.Threshold</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    default</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// fireAlert creates and sends alert notification</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ae </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">fireAlert</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">rule</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">AlertRule</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">triggerEntry</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Prevent alert spam with minimum interval</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    now </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> now.</span><span style=\"color:#B392F0\">Sub</span><span style=\"color:#E1E4E8\">(rule.lastFired) </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> time.Minute {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rule.lastFired </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> now</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    alert </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Alert</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        AlertID:      </span><span style=\"color:#B392F0\">generateAlertID</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        RuleID:       rule.RuleID,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TenantID:     rule.TenantID,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Severity:     rule.Severity,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Title:        rule.Name,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Message:      fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Alert condition met: </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, rule.Query),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TriggerEntry: triggerEntry,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Timestamp:    now,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Status:       AlertStatusTriggered,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Apply deduplication</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> ae.deduplicator.</span><span style=\"color:#B392F0\">ShouldSuppressAlert</span><span style=\"color:#E1E4E8\">(alert) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Send notification</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ae.notifier.</span><span style=\"color:#B392F0\">SendAlert</span><span style=\"color:#E1E4E8\">(alert)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Core Alert Logic Skeleton:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// AddRule registers a new alert rule for evaluation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ae </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">AddRule</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">rule</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">AlertRule</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate rule configuration (query syntax, thresholds, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Compile rule query into efficient matcher</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Initialize sliding window for event tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Add rule to evaluation map with proper locking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Log rule addition for audit trail</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// UpdateRule modifies existing alert rule</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ae </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">UpdateRule</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ruleID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">updates</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">AlertRule</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate rule exists and belongs to correct tenant</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Preserve existing rule state (window, last fired time)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Recompile query matcher if query changed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update rule configuration atomically</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Log rule modification for audit trail</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DeleteRule removes alert rule from evaluation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ae </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">DeleteRule</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ruleID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">tenantID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Verify rule exists and tenant ownership</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Stop rule evaluation and clean up resources</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Remove rule from evaluation map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Clean up any pending alerts for this rule</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Log rule deletion for audit trail</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint:</strong></p>\n<p>After implementing multi-tenancy and alerting:</p>\n<ol>\n<li><strong>Test Tenant Isolation:</strong> Create two tenants, ingest logs for each, verify queries only return tenant-specific data</li>\n<li><strong>Test Rate Limiting:</strong> Send high-volume requests and verify 429 responses when limits exceeded</li>\n<li><strong>Test Alert Rules:</strong> Create rules that trigger on specific log patterns, verify notifications delivered</li>\n<li><strong>Test Deduplication:</strong> Generate duplicate alert conditions, verify only appropriate notifications sent</li>\n</ol>\n<p>Expected behavior: Each tenant operates in complete isolation with enforced resource limits, while alert rules provide real-time problem detection with intelligent notification management.</p>\n<h2 id=\"interactions-and-data-flow\">Interactions and Data Flow</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section integrates understanding from all milestones (1-5), showing how components work together across the complete system. It covers log ingestion flow (Milestone 1), query processing flow (Milestone 3), and background maintenance (Milestones 2, 4, 5).</p>\n</blockquote>\n<h3 id=\"mental-model-the-orchestra-performance\">Mental Model: The Orchestra Performance</h3>\n<p>Think of a log aggregation system as a symphony orchestra during a live performance. The <strong>log ingestion flow</strong> is like musicians playing their parts - each instrument (protocol handler) contributes notes (log entries) that flow through the conductor (ingestion pipeline) to create harmony (indexed, stored logs). The <strong>query processing flow</strong> resembles an audience member requesting a specific piece - the conductor (query engine) coordinates different sections (index, storage) to replay the exact musical phrases (log entries) the listener wants to hear. Meanwhile, <strong>background maintenance flows</strong> are like the stage crew working behind the scenes - tuning instruments (index compaction), replacing worn sheet music (retention cleanup), and ensuring everything stays organized for the next performance.</p>\n<p>This analogy captures the key insight that successful log aggregation requires choreographed coordination between multiple concurrent processes, each with different timing requirements and failure modes. Just as a symphony can&#39;t pause mid-performance when a violin string breaks, our system must handle component failures gracefully while maintaining the overall flow of data.</p>\n<h3 id=\"log-ingestion-flow\">Log Ingestion Flow</h3>\n<p>The log ingestion flow represents the journey of raw log data from external sources through parsing, validation, indexing, and ultimate storage. This flow must handle massive throughput while ensuring data durability and maintaining query performance through proper indexing.</p>\n<p><img src=\"/api/project/log-aggregator/architecture-doc/asset?path=diagrams%2Fingestion-flow.svg\" alt=\"Log Ingestion Sequence\"></p>\n<h4 id=\"request-reception-and-protocol-handling\">Request Reception and Protocol Handling</h4>\n<p>The ingestion process begins when external systems send log data through one of three supported protocols. The <code>HTTPServer</code> listens on port 8080 and accepts POST requests containing JSON-formatted log entries. Simultaneously, the <code>TCPHandler</code> maintains persistent connections on port 1514 for syslog clients that need reliable delivery, while UDP handlers on the same port serve clients prioritizing low latency over guaranteed delivery.</p>\n<p>Each protocol handler performs initial validation specific to its transport mechanism. HTTP handlers validate request headers, content-type, and authentication tokens. TCP handlers manage connection state and implement proper syslog framing according to RFC 5424. UDP handlers must handle potential packet loss and reordering, though they cannot provide delivery guarantees.</p>\n<p>The following table describes the message formats accepted at each ingestion endpoint:</p>\n<table>\n<thead>\n<tr>\n<th>Protocol</th>\n<th>Message Format</th>\n<th>Required Fields</th>\n<th>Optional Fields</th>\n<th>Validation Rules</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP</td>\n<td>JSON POST body</td>\n<td><code>timestamp</code>, <code>message</code></td>\n<td><code>labels</code>, <code>stream</code></td>\n<td>ISO 8601 timestamp, non-empty message</td>\n</tr>\n<tr>\n<td>TCP</td>\n<td>RFC 5424 syslog</td>\n<td><code>PRI</code>, <code>VERSION</code>, <code>TIMESTAMP</code>, <code>MSG</code></td>\n<td><code>HOSTNAME</code>, <code>APP-NAME</code>, <code>STRUCTURED-DATA</code></td>\n<td>Valid priority, parseable timestamp</td>\n</tr>\n<tr>\n<td>UDP</td>\n<td>RFC 3164 syslog</td>\n<td><code>PRI</code>, <code>TIMESTAMP</code>, <code>MSG</code></td>\n<td><code>HOSTNAME</code>, <code>TAG</code></td>\n<td>Legacy BSD syslog format support</td>\n</tr>\n<tr>\n<td>File Tail</td>\n<td>Raw log lines</td>\n<td>Detected timestamp, message content</td>\n<td>Parsed structured fields</td>\n<td>Configurable regex patterns</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Multi-Protocol Support Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Applications use diverse logging protocols based on their technology stack and operational requirements</li>\n<li><strong>Options Considered</strong>: HTTP-only (simple), Syslog-only (standard), Multi-protocol (comprehensive)</li>\n<li><strong>Decision</strong>: Support HTTP, TCP syslog, UDP syslog, and file tailing simultaneously</li>\n<li><strong>Rationale</strong>: Maximum compatibility with existing infrastructure while providing protocol-specific optimizations</li>\n<li><strong>Consequences</strong>: Increased implementation complexity but enables adoption without requiring application changes</li>\n</ul>\n</blockquote>\n<h4 id=\"log-parsing-and-normalization\">Log Parsing and Normalization</h4>\n<p>Once protocol handlers receive raw log data, the <code>JSONParser</code> converts diverse formats into standardized <code>LogEntry</code> structures. This parsing stage must handle malformed data gracefully while extracting maximum value from well-formed entries.</p>\n<p>The parsing pipeline follows these steps for each incoming message:</p>\n<ol>\n<li><strong>Format Detection</strong>: Examine message structure to identify JSON, syslog, or plain text format</li>\n<li><strong>Timestamp Extraction</strong>: Parse timestamp fields using format-specific rules, falling back to current time if missing</li>\n<li><strong>Label Extraction</strong>: Extract structured fields into the <code>Labels</code> map, including protocol-derived labels like <code>source_protocol</code> and <code>source_host</code></li>\n<li><strong>Message Normalization</strong>: Standardize message content, handling escape sequences and character encoding</li>\n<li><strong>Validation</strong>: Verify required fields are present and values fall within acceptable ranges</li>\n<li><strong>Enrichment</strong>: Add system-generated labels like <code>ingestion_time</code> and <code>entry_id</code> for tracking</li>\n</ol>\n<p>The parser handles several common data quality issues that could otherwise corrupt the index or cause query failures:</p>\n<table>\n<thead>\n<tr>\n<th>Issue Type</th>\n<th>Detection Method</th>\n<th>Handling Strategy</th>\n<th>Recovery Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Invalid Timestamp</td>\n<td>Regex validation failure</td>\n<td>Use current server time</td>\n<td>Log parsing warning</td>\n</tr>\n<tr>\n<td>Missing Message</td>\n<td>Empty or null message field</td>\n<td>Reject entry</td>\n<td>Return HTTP 400 error</td>\n</tr>\n<tr>\n<td>Label Key Conflicts</td>\n<td>Duplicate keys with different values</td>\n<td>Prefer client-provided values</td>\n<td>Merge with precedence rules</td>\n</tr>\n<tr>\n<td>Oversized Entry</td>\n<td>Message exceeds size limits</td>\n<td>Truncate message content</td>\n<td>Preserve labels and timestamp</td>\n</tr>\n<tr>\n<td>Invalid UTF-8</td>\n<td>Character encoding errors</td>\n<td>Replace invalid sequences</td>\n<td>Continue processing with substituted chars</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p>The critical insight here is that parsing failures should never halt the entire ingestion pipeline. A single malformed log entry from one application cannot be allowed to prevent other applications from successfully ingesting their logs.</p>\n</blockquote>\n<h4 id=\"buffering-and-flow-control\">Buffering and Flow Control</h4>\n<p>After successful parsing, <code>LogEntry</code> instances enter the <code>MemoryBuffer</code> - a ring buffer implementation that provides crucial isolation between ingestion and downstream processing. This buffering layer enables the system to handle traffic bursts while protecting storage and indexing components from overload.</p>\n<p>The buffer operates as follows:</p>\n<ol>\n<li><strong>Write Operation</strong>: The <code>Write(entry)</code> method attempts to add new entries to the ring buffer head</li>\n<li><strong>Capacity Check</strong>: If buffer is full, the system applies backpressure by rejecting new writes with HTTP 503 responses</li>\n<li><strong>Read Operation</strong>: Background workers continuously call <code>Read()</code> to drain entries from the buffer tail</li>\n<li><strong>Flow Control</strong>: Buffer occupancy metrics trigger adaptive rate limiting when approaching capacity limits</li>\n</ol>\n<p>Buffer management requires careful attention to memory usage and failure scenarios:</p>\n<table>\n<thead>\n<tr>\n<th>Buffer State</th>\n<th>Occupancy Level</th>\n<th>Write Behavior</th>\n<th>Read Behavior</th>\n<th>Alert Threshold</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Normal</td>\n<td>0-60% full</td>\n<td>Accept all writes</td>\n<td>Batch read 100 entries</td>\n<td>None</td>\n</tr>\n<tr>\n<td>Warning</td>\n<td>60-80% full</td>\n<td>Accept writes, emit warnings</td>\n<td>Increase batch size to 500</td>\n<td>Monitor buffer lag</td>\n</tr>\n<tr>\n<td>Critical</td>\n<td>80-95% full</td>\n<td>Apply rate limiting</td>\n<td>Maximum batch size 1000</td>\n<td>Page operations team</td>\n</tr>\n<tr>\n<td>Full</td>\n<td>95-100% full</td>\n<td>Reject writes with 503</td>\n<td>Emergency flush to disk</td>\n<td>Immediate escalation</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: Unbounded Buffer Growth</strong>\nMany implementations allow buffers to grow indefinitely when downstream processing falls behind. This leads to memory exhaustion and eventual system crashes. Instead, implement fixed-size ring buffers with explicit backpressure policies. It&#39;s better to reject new logs during overload than to crash and lose all buffered data.</p>\n<h4 id=\"index-integration-and-term-extraction\">Index Integration and Term Extraction</h4>\n<p>As buffer readers drain <code>LogEntry</code> instances, they simultaneously feed the indexing engine to maintain query performance. This integration point represents a critical system bottleneck that requires careful coordination to avoid blocking either ingestion or query processing.</p>\n<p>The indexing integration follows this sequence:</p>\n<ol>\n<li><strong>Batch Formation</strong>: Buffer readers collect entries into time-aligned batches of 1000-10000 entries</li>\n<li><strong>Term Extraction</strong>: For each entry, extract indexable terms from both message content and label values</li>\n<li><strong>Segment Assignment</strong>: Route terms to appropriate <code>IndexSegment</code> instances based on time-based partitioning</li>\n<li><strong>Posting List Updates</strong>: Add <code>EntryReference</code> instances to postings lists for each extracted term</li>\n<li><strong>Bloom Filter Updates</strong>: Insert terms into segment bloom filters for fast negative lookups</li>\n<li><strong>Memory Management</strong>: Trigger segment finalization when memory usage exceeds thresholds</li>\n</ol>\n<p>The term extraction process determines query performance and must balance comprehensiveness with processing efficiency:</p>\n<table>\n<thead>\n<tr>\n<th>Term Source</th>\n<th>Extraction Method</th>\n<th>Index Impact</th>\n<th>Storage Overhead</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Message Words</td>\n<td>Tokenization by whitespace and punctuation</td>\n<td>High query recall</td>\n<td>Moderate - common terms</td>\n</tr>\n<tr>\n<td>Label Keys</td>\n<td>Direct key indexing</td>\n<td>Fast label filtering</td>\n<td>Low - limited key set</td>\n</tr>\n<tr>\n<td>Label Values</td>\n<td>Value tokenization for high-cardinality labels</td>\n<td>Enables value search</td>\n<td>High - potentially unbounded</td>\n</tr>\n<tr>\n<td>Structured Fields</td>\n<td>JSON field path indexing</td>\n<td>Precise field queries</td>\n<td>Moderate - predictable schema</td>\n</tr>\n<tr>\n<td>Timestamp Ranges</td>\n<td>Time window quantization</td>\n<td>Efficient time filtering</td>\n<td>Low - fixed intervals</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Concurrent Index Updates</strong></p>\n<ul>\n<li><strong>Context</strong>: Index updates can create bottlenecks that slow ingestion and impact query latency</li>\n<li><strong>Options Considered</strong>: Synchronous updates (simple), Asynchronous with queuing (complex), Write-through caching (hybrid)</li>\n<li><strong>Decision</strong>: Asynchronous index updates with bounded queues and backpressure</li>\n<li><strong>Rationale</strong>: Maintains ingestion throughput while ensuring index consistency through ordered processing</li>\n<li><strong>Consequences</strong>: Slight delay between ingestion and searchability, but prevents index contention from blocking ingestion</li>\n</ul>\n</blockquote>\n<h4 id=\"storage-coordination-and-persistence\">Storage Coordination and Persistence</h4>\n<p>The final stage of log ingestion involves coordinating with the <code>StorageEngine</code> to ensure durable persistence with write-ahead logging protection. This coordination must handle both normal operations and failure recovery scenarios.</p>\n<p>Storage coordination proceeds through these phases:</p>\n<ol>\n<li><strong>Batch Preparation</strong>: Group related log entries into storage-optimized batches aligned with chunk boundaries</li>\n<li><strong>WAL Recording</strong>: Write <code>WALRecord</code> entries describing the pending storage operation before modifying any persistent state</li>\n<li><strong>Chunk Formation</strong>: Organize log entries into compressed chunks with appropriate <code>ChunkHeader</code> metadata</li>\n<li><strong>Atomic Commitment</strong>: Complete the storage operation by updating chunk indexes and marking WAL records as committed</li>\n<li><strong>Cleanup Coordination</strong>: Remove committed WAL records and update buffer positions to prevent reprocessing</li>\n</ol>\n<p>The storage engine provides several guarantees that ingestion flow depends upon:</p>\n<table>\n<thead>\n<tr>\n<th>Guarantee Type</th>\n<th>Implementation Mechanism</th>\n<th>Failure Behavior</th>\n<th>Recovery Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Durability</td>\n<td>WAL with fsync before acknowledgment</td>\n<td>No acknowledged writes lost</td>\n<td>Replay uncommitted WAL records</td>\n</tr>\n<tr>\n<td>Consistency</td>\n<td>Atomic chunk creation with metadata</td>\n<td>Partial chunks discarded</td>\n<td>Rollback incomplete operations</td>\n</tr>\n<tr>\n<td>Isolation</td>\n<td>Per-chunk locking during creation</td>\n<td>Concurrent access blocked</td>\n<td>Wait for completion or timeout</td>\n</tr>\n<tr>\n<td>Ordering</td>\n<td>Timestamp-based chunk organization</td>\n<td>Out-of-order entries detected</td>\n<td>Reorder during recovery</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: WAL Coordination Deadlock</strong>\nIncorrect coordination between ingestion buffer management and WAL operations can create deadlocks where ingestion waits for storage while storage waits for buffer space. Design buffer draining and WAL writing as independent processes that communicate through bounded queues rather than synchronous method calls.</p>\n<h3 id=\"query-processing-flow\">Query Processing Flow</h3>\n<p>Query processing transforms LogQL query strings into efficient execution plans that coordinate index lookups, storage access, and result streaming. This flow must optimize for both interactive queries requiring low latency and analytical queries processing large data volumes.</p>\n<p><img src=\"/api/project/log-aggregator/architecture-doc/asset?path=diagrams%2Fquery-flow.svg\" alt=\"Query Processing Sequence\"></p>\n<h4 id=\"query-reception-and-authentication\">Query Reception and Authentication</h4>\n<p>Query processing begins when clients submit requests to the <code>QueryEngine</code> through HTTP GET or POST endpoints. Each request undergoes authentication and authorization before query parsing begins, ensuring tenant isolation and preventing unauthorized data access.</p>\n<p>The query reception process handles the following request formats:</p>\n<table>\n<thead>\n<tr>\n<th>Request Method</th>\n<th>Content Format</th>\n<th>Required Parameters</th>\n<th>Optional Parameters</th>\n<th>Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>GET</td>\n<td>URL query parameters</td>\n<td><code>query</code>, <code>time</code> or <code>start</code>/<code>end</code></td>\n<td><code>limit</code>, <code>direction</code>, <code>format</code></td>\n<td>Simple exploratory queries</td>\n</tr>\n<tr>\n<td>POST</td>\n<td>JSON request body</td>\n<td><code>query</code>, <code>time_range</code></td>\n<td><code>limit</code>, <code>timeout</code>, <code>stream_results</code></td>\n<td>Complex analytical queries</td>\n</tr>\n<tr>\n<td>WebSocket</td>\n<td>JSON messages</td>\n<td><code>query</code>, subscription parameters</td>\n<td><code>continuous</code>, <code>filter_updates</code></td>\n<td>Real-time log streaming</td>\n</tr>\n</tbody></table>\n<p>Authentication leverages the tenant context established during ingestion to ensure queries only access logs belonging to the requesting tenant. The <code>AuthService</code> validates request tokens and populates <code>TenantContext</code> with appropriate access controls.</p>\n<p>Query authorization involves multiple validation layers:</p>\n<ol>\n<li><strong>Token Validation</strong>: Verify JWT signatures and expiration times using tenant-specific keys</li>\n<li><strong>Resource Quota Check</strong>: Ensure query doesn&#39;t exceed tenant&#39;s maximum memory or CPU limits</li>\n<li><strong>Time Range Validation</strong>: Confirm requested time window falls within tenant&#39;s data retention period</li>\n<li><strong>Rate Limit Enforcement</strong>: Apply per-tenant query frequency limits to prevent resource exhaustion</li>\n<li><strong>Query Complexity Analysis</strong>: Estimate resource requirements and reject queries exceeding tenant quotas</li>\n</ol>\n<blockquote>\n<p><strong>Decision: Query Authentication Integration Point</strong></p>\n<ul>\n<li><strong>Context</strong>: Queries must respect multi-tenant isolation while maintaining query performance</li>\n<li><strong>Options Considered</strong>: Pre-authentication (fast but brittle), Per-operation checks (secure but slow), Context propagation (balanced)</li>\n<li><strong>Decision</strong>: Authenticate once at query reception, propagate tenant context through execution pipeline</li>\n<li><strong>Rationale</strong>: Provides security without repeated authentication overhead, enables tenant-aware optimizations</li>\n<li><strong>Consequences</strong>: Requires careful context propagation design but enables efficient tenant-isolated execution</li>\n</ul>\n</blockquote>\n<h4 id=\"lexical-analysis-and-parsing\">Lexical Analysis and Parsing</h4>\n<p>Once authenticated, query strings undergo lexical analysis by the <code>Lexer</code> to break them into tokens, followed by parsing to construct an Abstract Syntax Tree (AST) representing the query structure. This parsing must handle LogQL syntax while providing clear error messages for malformed queries.</p>\n<p>The lexical analysis process identifies these token types:</p>\n<table>\n<thead>\n<tr>\n<th>Token Type</th>\n<th>Example</th>\n<th>Description</th>\n<th>Parsing Rules</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>TokenLeftBrace</code></td>\n<td><code>{</code></td>\n<td>Label selector start</td>\n<td>Must be followed by label expressions</td>\n</tr>\n<tr>\n<td><code>TokenRightBrace</code></td>\n<td><code>}</code></td>\n<td>Label selector end</td>\n<td>Must close matching left brace</td>\n</tr>\n<tr>\n<td><code>TokenIdentifier</code></td>\n<td><code>level</code>, <code>service</code></td>\n<td>Unquoted identifier</td>\n<td>Alphanumeric plus underscore</td>\n</tr>\n<tr>\n<td><code>TokenString</code></td>\n<td><code>&quot;error message&quot;</code></td>\n<td>Quoted string literal</td>\n<td>Supports escape sequences</td>\n</tr>\n<tr>\n<td><code>TokenEqual</code></td>\n<td><code>=</code></td>\n<td>Equality operator</td>\n<td>Used in label selectors</td>\n</tr>\n<tr>\n<td><code>TokenPipe</code></td>\n<td><code>|</code></td>\n<td>Pipeline operator</td>\n<td>Separates query stages</td>\n</tr>\n<tr>\n<td><code>TokenRegex</code></td>\n<td><code>\\~</code></td>\n<td>Regex match operator</td>\n<td>Followed by regex pattern</td>\n</tr>\n<tr>\n<td><code>TokenEOF</code></td>\n<td>End of input</td>\n<td>Marks query completion</td>\n<td>Triggers final parsing validation</td>\n</tr>\n</tbody></table>\n<p>The parser constructs an AST that represents query structure as a tree of operations:</p>\n<ol>\n<li><strong>Root Node</strong>: Contains the complete query with metadata like time range and limits</li>\n<li><strong>Stream Selector</strong>: Represents label-based log stream selection criteria</li>\n<li><strong>Pipeline Stages</strong>: Chain of operations applied to selected log streams</li>\n<li><strong>Filter Operations</strong>: Line filters, regex matches, and JSON extractions</li>\n<li><strong>Aggregation Functions</strong>: Count, rate, sum, and other metric computations</li>\n</ol>\n<p>⚠️ <strong>Pitfall: Parser Error Recovery</strong>\nNaive parsers abort on the first syntax error, providing poor user experience for complex queries with multiple issues. Implement error recovery that continues parsing after syntax errors to report multiple problems simultaneously. This requires careful parser state management to avoid cascading error reports.</p>\n<h4 id=\"query-planning-and-optimization\">Query Planning and Optimization</h4>\n<p>The parsed AST undergoes query planning to generate an efficient execution strategy. This planning phase determines which indexes to consult, how to order operations, and whether to use streaming or batch processing for result delivery.</p>\n<p>Query planning follows these optimization stages:</p>\n<ol>\n<li><strong>Predicate Analysis</strong>: Identify filter conditions that can be pushed down to index lookups</li>\n<li><strong>Index Selection</strong>: Choose optimal indexes based on label selectors and time ranges</li>\n<li><strong>Operation Ordering</strong>: Arrange pipeline operations to minimize data processing volume</li>\n<li><strong>Resource Estimation</strong>: Calculate memory and CPU requirements for execution plan validation</li>\n<li><strong>Execution Strategy</strong>: Choose between streaming and batch processing based on result size estimates</li>\n</ol>\n<p>The query planner applies several key optimizations:</p>\n<table>\n<thead>\n<tr>\n<th>Optimization Type</th>\n<th>Technique</th>\n<th>Benefit</th>\n<th>Application Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Predicate Pushdown</td>\n<td>Move filters before storage access</td>\n<td>Reduces I/O volume</td>\n<td>Filters compatible with index structure</td>\n</tr>\n<tr>\n<td>Index Intersection</td>\n<td>Combine multiple index lookups</td>\n<td>Narrows result set early</td>\n<td>Multiple selective label filters</td>\n</tr>\n<tr>\n<td>Time Range Pruning</td>\n<td>Skip irrelevant time partitions</td>\n<td>Eliminates unnecessary storage access</td>\n<td>Bounded time range queries</td>\n</tr>\n<tr>\n<td>Streaming Execution</td>\n<td>Process results incrementally</td>\n<td>Reduces memory usage</td>\n<td>Large result sets or real-time queries</td>\n</tr>\n<tr>\n<td>Bloom Filter Utilization</td>\n<td>Fast negative lookups</td>\n<td>Avoids expensive storage reads</td>\n<td>High-selectivity term filters</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p>The key insight for query optimization is that log data has strong temporal locality. Most queries focus on recent time windows, so optimizations that leverage time-based partitioning provide dramatic performance improvements for typical workloads.</p>\n</blockquote>\n<h4 id=\"index-consultation-and-reference-resolution\">Index Consultation and Reference Resolution</h4>\n<p>With an optimized execution plan, the query engine consults relevant indexes to identify <code>EntryReference</code> instances pointing to log entries matching the query criteria. This consultation must efficiently combine multiple index lookups while handling bloom filter false positives.</p>\n<p>Index consultation proceeds through these steps:</p>\n<ol>\n<li><strong>Partition Selection</strong>: Use time range filters to identify relevant <code>IndexSegment</code> instances</li>\n<li><strong>Term Lookup</strong>: Query inverted indexes for terms derived from label selectors and text filters</li>\n<li><strong>Posting List Intersection</strong>: Combine postings lists from multiple terms using set intersection</li>\n<li><strong>Bloom Filter Validation</strong>: Use bloom filters to eliminate segments unlikely to contain matching terms</li>\n<li><strong>Reference Collection</strong>: Gather <code>EntryReference</code> instances pointing to potentially matching log entries</li>\n<li><strong>Reference Deduplication</strong>: Remove duplicate references when multiple terms match the same entry</li>\n</ol>\n<p>The index consultation process must handle several challenging scenarios:</p>\n<table>\n<thead>\n<tr>\n<th>Challenge</th>\n<th>Cause</th>\n<th>Detection Method</th>\n<th>Handling Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Bloom Filter False Positives</td>\n<td>Probabilistic data structure limitations</td>\n<td>Storage lookup returns no matches</td>\n<td>Continue with remaining references</td>\n</tr>\n<tr>\n<td>High Cardinality Explosion</td>\n<td>Too many unique label values</td>\n<td>Posting list size exceeds thresholds</td>\n<td>Apply additional filters early</td>\n</tr>\n<tr>\n<td>Time Range Misalignment</td>\n<td>Clock skew between ingestion sources</td>\n<td>References outside query time bounds</td>\n<td>Filter references during collection</td>\n</tr>\n<tr>\n<td>Index Segment Unavailability</td>\n<td>Concurrent compaction operations</td>\n<td>Segment lock acquisition failure</td>\n<td>Retry with exponential backoff</td>\n</tr>\n<tr>\n<td>Memory Pressure</td>\n<td>Large posting list intersections</td>\n<td>Process memory usage monitoring</td>\n<td>Stream processing instead of batch loading</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: Reference Resolution Memory Explosion</strong>\nQueries matching millions of log entries can generate reference lists that exceed available memory. Instead of loading all references into memory simultaneously, implement streaming reference resolution that processes references in batches while maintaining query correctness.</p>\n<h4 id=\"storage-access-and-content-filtering\">Storage Access and Content Filtering</h4>\n<p>Armed with <code>EntryReference</code> instances from index consultation, the query engine accesses the <code>StorageEngine</code> to retrieve actual log entry content. This storage access must efficiently handle compressed data while applying line filters that couldn&#39;t be resolved through index lookups alone.</p>\n<p>Storage access coordination involves:</p>\n<ol>\n<li><strong>Chunk Loading</strong>: Group references by <code>ChunkID</code> to minimize storage I/O operations</li>\n<li><strong>Decompression</strong>: Decompress chunk data using the appropriate compression algorithm</li>\n<li><strong>Entry Extraction</strong>: Locate specific log entries within decompressed chunks using offset information</li>\n<li><strong>Content Filtering</strong>: Apply regex patterns, JSON extractions, and other content-based filters</li>\n<li><strong>Result Formatting</strong>: Convert matching entries into the requested output format</li>\n<li><strong>Memory Management</strong>: Stream results to avoid accumulating large result sets in memory</li>\n</ol>\n<p>The storage access layer provides several optimizations for query performance:</p>\n<table>\n<thead>\n<tr>\n<th>Optimization</th>\n<th>Implementation</th>\n<th>Performance Benefit</th>\n<th>Memory Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Chunk Caching</td>\n<td>LRU cache of decompressed chunks</td>\n<td>Eliminates repeated decompression</td>\n<td>High - caches full chunk content</td>\n</tr>\n<tr>\n<td>Partial Chunk Reading</td>\n<td>Read only required entry ranges</td>\n<td>Reduces I/O for selective queries</td>\n<td>Low - processes entries incrementally</td>\n</tr>\n<tr>\n<td>Compression Awareness</td>\n<td>Skip decompression for filtered chunks</td>\n<td>Saves CPU cycles</td>\n<td>None - avoids unnecessary processing</td>\n</tr>\n<tr>\n<td>Concurrent Access</td>\n<td>Parallel chunk processing</td>\n<td>Improves query throughput</td>\n<td>Moderate - temporary decompression buffers</td>\n</tr>\n<tr>\n<td>Reference Batching</td>\n<td>Group references by storage locality</td>\n<td>Optimizes disk access patterns</td>\n<td>Low - small reference metadata only</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Content Filter Application Point</strong></p>\n<ul>\n<li><strong>Context</strong>: Some filters require full log content that isn&#39;t indexed, creating tension between I/O efficiency and filter accuracy</li>\n<li><strong>Options Considered</strong>: Filter during indexing (fast but inflexible), Filter during storage access (accurate but slow), Hybrid approach (complex)</li>\n<li><strong>Decision</strong>: Apply content-based filters during storage access with aggressive caching</li>\n<li><strong>Rationale</strong>: Maintains filter accuracy while leveraging temporal locality of log queries for cache effectiveness</li>\n<li><strong>Consequences</strong>: Higher memory usage but much better query performance for typical access patterns</li>\n</ul>\n</blockquote>\n<h4 id=\"result-assembly-and-streaming\">Result Assembly and Streaming</h4>\n<p>The final stage of query processing assembles matching log entries into response formats suitable for client consumption. This assembly must handle result pagination, streaming delivery, and format conversion while maintaining consistent ordering guarantees.</p>\n<p>Result assembly follows this pipeline:</p>\n<ol>\n<li><strong>Entry Collection</strong>: Gather <code>LogEntry</code> instances from storage access operations</li>\n<li><strong>Ordering Enforcement</strong>: Sort entries by timestamp to ensure consistent query results</li>\n<li><strong>Pagination Handling</strong>: Apply limit and offset parameters while maintaining cursor positions for subsequent requests</li>\n<li><strong>Format Conversion</strong>: Convert entries to requested output format (JSON, text, CSV, etc.)</li>\n<li><strong>Streaming Delivery</strong>: Send results incrementally to reduce client wait times and memory usage</li>\n<li><strong>Metadata Calculation</strong>: Compute query statistics including execution time, scanned entries, and result counts</li>\n</ol>\n<p>The <code>ResultStream</code> provides a consistent interface for result delivery regardless of underlying processing strategy:</p>\n<table>\n<thead>\n<tr>\n<th>Stream Method</th>\n<th>Purpose</th>\n<th>Implementation</th>\n<th>Error Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Next()</code></td>\n<td>Retrieve next result entry</td>\n<td>Coordinates with storage access pipeline</td>\n<td>Returns error for processing failures</td>\n</tr>\n<tr>\n<td><code>HasMore()</code></td>\n<td>Check for additional results</td>\n<td>Examines cursor position and remaining references</td>\n<td>Always returns boolean</td>\n</tr>\n<tr>\n<td><code>Close()</code></td>\n<td>Release stream resources</td>\n<td>Cleans up storage handles and memory buffers</td>\n<td>Logs cleanup failures but doesn&#39;t propagate</td>\n</tr>\n<tr>\n<td><code>Metadata()</code></td>\n<td>Get query execution statistics</td>\n<td>Aggregates metrics from all processing stages</td>\n<td>Returns partial data on calculation errors</td>\n</tr>\n</tbody></table>\n<p>Result streaming handles several challenging requirements:</p>\n<ul>\n<li><strong>Ordering Consistency</strong>: Results must appear in timestamp order even when processed concurrently</li>\n<li><strong>Memory Bounds</strong>: Large result sets cannot be accumulated in memory before delivery</li>\n<li><strong>Error Propagation</strong>: Processing errors must be communicated without corrupting result streams</li>\n<li><strong>Cancellation Support</strong>: Clients must be able to abort expensive queries to reclaim resources</li>\n<li><strong>Progress Indication</strong>: Long-running queries should provide progress updates to prevent client timeouts</li>\n</ul>\n<h3 id=\"background-maintenance-flows\">Background Maintenance Flows</h3>\n<p>Background maintenance operations ensure system health and performance through index compaction, retention cleanup, and other housekeeping tasks. These flows must operate continuously without impacting ingestion or query performance.</p>\n<h4 id=\"index-compaction-operations\">Index Compaction Operations</h4>\n<p>Index compaction merges small <code>IndexSegment</code> instances into larger, more efficient segments that reduce query overhead and improve storage utilization. This compaction process must coordinate carefully with ongoing queries to avoid introducing consistency issues.</p>\n<p>The compaction process operates through these phases:</p>\n<ol>\n<li><strong>Compaction Candidate Selection</strong>: Identify segments eligible for merging based on size, age, and access patterns</li>\n<li><strong>Resource Availability Check</strong>: Ensure sufficient CPU and memory resources for compaction without impacting foreground operations</li>\n<li><strong>Segment Locking</strong>: Acquire exclusive locks on source segments to prevent modification during compaction</li>\n<li><strong>Merge Operation</strong>: Combine posting lists and bloom filters from multiple source segments into a new consolidated segment</li>\n<li><strong>Atomic Replacement</strong>: Replace references to source segments with references to the new merged segment</li>\n<li><strong>Cleanup</strong>: Delete source segments after confirming all queries have switched to the new segment</li>\n</ol>\n<p>Compaction scheduling uses several criteria to balance maintenance benefits with system impact:</p>\n<table>\n<thead>\n<tr>\n<th>Compaction Trigger</th>\n<th>Threshold</th>\n<th>Benefit</th>\n<th>Resource Cost</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Segment Count</td>\n<td>More than 20 segments per hour</td>\n<td>Reduces query fanout overhead</td>\n<td>High I/O during merge</td>\n</tr>\n<tr>\n<td>Average Segment Size</td>\n<td>Segments smaller than 10MB</td>\n<td>Improves storage efficiency</td>\n<td>CPU intensive posting list merging</td>\n</tr>\n<tr>\n<td>Query Performance</td>\n<td>Response times exceed SLA</td>\n<td>Directly addresses performance problems</td>\n<td>May temporarily slow queries further</td>\n</tr>\n<tr>\n<td>Storage Pressure</td>\n<td>Available disk space below 20%</td>\n<td>Prevents storage exhaustion</td>\n<td>Requires significant temporary space</td>\n</tr>\n<tr>\n<td>Scheduled Maintenance</td>\n<td>Daily during low-traffic hours</td>\n<td>Predictable maintenance windows</td>\n<td>Competes with other maintenance tasks</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Compaction Concurrency Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Index compaction requires significant resources but cannot halt query processing</li>\n<li><strong>Options Considered</strong>: Stop-the-world compaction (simple but disruptive), Concurrent compaction (complex), Incremental compaction (balanced)</li>\n<li><strong>Decision</strong>: Implement concurrent compaction with copy-on-write semantics for active segments</li>\n<li><strong>Rationale</strong>: Maintains query availability while allowing essential maintenance operations</li>\n<li><strong>Consequences</strong>: Increased implementation complexity and temporary storage overhead during compaction</li>\n</ul>\n</blockquote>\n<p>⚠️ <strong>Pitfall: Compaction Resource Starvation</strong>\nBackground compaction can consume excessive CPU and I/O resources, degrading performance for ingestion and queries. Implement adaptive resource throttling that monitors system load and reduces compaction intensity when foreground operations show signs of stress.</p>\n<h4 id=\"retention-policy-enforcement\">Retention Policy Enforcement</h4>\n<p>Retention policy enforcement automatically removes expired log data according to configurable rules, preventing unbounded storage growth while maintaining compliance requirements. This enforcement must coordinate with active queries to prevent data deletion while logs are being accessed.</p>\n<p>Retention enforcement operates through these coordinated processes:</p>\n<ol>\n<li><strong>Policy Evaluation</strong>: Scan all chunks to identify those exceeding configured retention limits</li>\n<li><strong>Query Coordination</strong>: Verify no active queries are accessing chunks marked for deletion</li>\n<li><strong>Reference Counting</strong>: Ensure all index segments referencing expired chunks are also eligible for cleanup</li>\n<li><strong>Graceful Deletion</strong>: Remove chunks and associated index entries while maintaining referential integrity</li>\n<li><strong>Cleanup Verification</strong>: Confirm successful deletion and update storage metrics</li>\n<li><strong>Audit Logging</strong>: Record all retention actions for compliance and debugging purposes</li>\n</ol>\n<p>The retention policy engine supports multiple policy types that can be applied simultaneously:</p>\n<table>\n<thead>\n<tr>\n<th>Policy Type</th>\n<th>Configuration Parameters</th>\n<th>Enforcement Method</th>\n<th>Interaction with Other Policies</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Time-Based</td>\n<td><code>MaxAge time.Duration</code></td>\n<td>Compare chunk creation time with current time</td>\n<td>Evaluated first, other policies respect time bounds</td>\n</tr>\n<tr>\n<td>Size-Based</td>\n<td><code>MaxSize int64</code></td>\n<td>Calculate cumulative storage size per stream</td>\n<td>Applied after time policy to prevent size violations</td>\n</tr>\n<tr>\n<td>Count-Based</td>\n<td><code>MaxEntries int64</code></td>\n<td>Track entry counts per stream or tenant</td>\n<td>Least frequently used, typically for debugging streams</td>\n</tr>\n<tr>\n<td>Compliance-Based</td>\n<td><code>GracePeriod time.Duration</code></td>\n<td>Legal hold overrides with explicit release dates</td>\n<td>Takes precedence over all other policies</td>\n</tr>\n</tbody></table>\n<p>Retention cleanup must handle several complex coordination scenarios:</p>\n<ul>\n<li><strong>Active Query Protection</strong>: Never delete data that might be accessed by running queries</li>\n<li><strong>Index Consistency</strong>: Ensure index segments don&#39;t contain references to deleted chunks</li>\n<li><strong>Tenant Isolation</strong>: Apply retention policies independently per tenant</li>\n<li><strong>Backup Coordination</strong>: Delay deletion until backup systems confirm successful archival</li>\n<li><strong>Audit Trail Maintenance</strong>: Preserve deletion records for compliance reporting</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Retention Race Conditions</strong>\nConcurrent retention enforcement and query processing can create race conditions where queries attempt to access recently deleted chunks. Implement reference counting with grace periods that delay physical deletion until all references are released.</p>\n<h4 id=\"write-ahead-log-maintenance\">Write-Ahead Log Maintenance</h4>\n<p>WAL maintenance ensures the write-ahead log doesn&#39;t grow unboundedly while preserving the ability to recover from system failures. This maintenance includes record cleanup, file rotation, and checkpoint creation.</p>\n<p>WAL maintenance operates through these continuous processes:</p>\n<ol>\n<li><strong>Checkpoint Creation</strong>: Periodically create snapshots of committed state to bound recovery time</li>\n<li><strong>Record Cleanup</strong>: Remove WAL records that precede the most recent checkpoint</li>\n<li><strong>File Rotation</strong>: Create new WAL files when current files exceed size limits</li>\n<li><strong>Integrity Verification</strong>: Regularly verify WAL file integrity using checksums and structural validation</li>\n<li><strong>Recovery Testing</strong>: Periodically test WAL replay capabilities to ensure recovery procedures work correctly</li>\n</ol>\n<p>The WAL maintenance system balances several competing requirements:</p>\n<table>\n<thead>\n<tr>\n<th>Requirement</th>\n<th>Implementation Strategy</th>\n<th>Trade-offs</th>\n<th>Monitoring Metrics</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Bounded Recovery Time</td>\n<td>Create checkpoints every 10 minutes</td>\n<td>More frequent I/O vs. faster recovery</td>\n<td>Recovery time during testing</td>\n</tr>\n<tr>\n<td>Storage Efficiency</td>\n<td>Clean up committed records aggressively</td>\n<td>Risk of data loss vs. disk usage</td>\n<td>WAL file size growth rate</td>\n</tr>\n<tr>\n<td>Write Performance</td>\n<td>Buffer WAL writes and batch fsync</td>\n<td>Durability guarantees vs. throughput</td>\n<td>Write latency percentiles</td>\n</tr>\n<tr>\n<td>Integrity Assurance</td>\n<td>Checksum every WAL record</td>\n<td>CPU overhead vs. corruption detection</td>\n<td>Checksum verification failures</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p>The critical insight for WAL maintenance is that it directly impacts both system reliability and performance. Too aggressive cleanup risks data loss during failures, while too conservative cleanup degrades performance and wastes storage.</p>\n</blockquote>\n<p>The WAL maintenance processes coordinate with other system components through well-defined interfaces:</p>\n<ul>\n<li><strong>Storage Engine Coordination</strong>: Checkpoint creation requires consistent snapshots of chunk metadata</li>\n<li><strong>Query Engine Integration</strong>: Active query tracking prevents cleanup of required recovery data</li>\n<li><strong>Ingestion Flow Synchronization</strong>: WAL rotation must not interrupt ongoing write operations</li>\n<li><strong>Monitoring Integration</strong>: WAL health metrics feed into overall system health dashboards</li>\n</ul>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides practical implementation details for building the interaction flows described above.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Server</td>\n<td><code>net/http</code> with <code>http.ServeMux</code></td>\n<td><code>gorilla/mux</code> or <code>chi</code> router</td>\n</tr>\n<tr>\n<td>Protocol Parsing</td>\n<td>Manual parsing with <code>bufio.Scanner</code></td>\n<td><code>gopkg.in/mcuadros/go-syslog.v2</code></td>\n</tr>\n<tr>\n<td>Concurrency</td>\n<td><code>sync.WaitGroup</code> with goroutines</td>\n<td><code>golang.org/x/sync/errgroup</code></td>\n</tr>\n<tr>\n<td>JSON Processing</td>\n<td><code>encoding/json</code> standard library</td>\n<td><code>github.com/json-iterator/go</code> for performance</td>\n</tr>\n<tr>\n<td>Context Propagation</td>\n<td><code>context.Context</code> throughout call chain</td>\n<td>Custom context with tracing integration</td>\n</tr>\n<tr>\n<td>Background Tasks</td>\n<td>Simple <code>time.Ticker</code> loops</td>\n<td><code>github.com/robfig/cron/v3</code> for scheduling</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/\n  flows/\n    ingestion.go          ← ingestion coordination\n    query.go              ← query processing coordination  \n    maintenance.go        ← background task coordination\n    ingestion_test.go     ← ingestion flow tests\n    query_test.go         ← query flow tests\n  coordinator/\n    coordinator.go        ← cross-component orchestration\n    flow_controller.go    ← backpressure and flow control\n  protocols/\n    http_server.go        ← HTTP ingestion endpoint\n    syslog_handler.go     ← TCP/UDP syslog handling\n    file_tailer.go        ← file watching and tailing\n  auth/\n    tenant_context.go     ← tenant authentication and context\n    rate_limiter.go       ← rate limiting implementation</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p>Complete HTTP server implementation for log ingestion:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> protocols</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/your-org/log-aggregator/internal/types</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HTTPServer</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Config</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parser     </span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">LogParser</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    buffer     </span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">LogBuffer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metrics    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    server     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Server</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    limiter    </span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RateLimiter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewHTTPServer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">parser</span><span style=\"color:#B392F0\"> types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">LogParser</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                   buffer</span><span style=\"color:#B392F0\"> types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">LogBuffer</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">metrics</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Metrics</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPServer</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">HTTPServer</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config:  config,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        parser:  parser,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        buffer:  buffer,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metrics: metrics,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        limiter: </span><span style=\"color:#B392F0\">NewTokenBucket</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">, time.Second), </span><span style=\"color:#6A737D\">// 1000 req/sec</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mux </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> http.</span><span style=\"color:#B392F0\">NewServeMux</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mux.</span><span style=\"color:#B392F0\">HandleFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/api/v1/push\"</span><span style=\"color:#E1E4E8\">, s.handleLogIngestion)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mux.</span><span style=\"color:#B392F0\">HandleFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/health\"</span><span style=\"color:#E1E4E8\">, s.handleHealthCheck)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.server </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Addr:         fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\":</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, s.config.HTTPPort),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Handler:      mux,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ReadTimeout:  </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        WriteTimeout: </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> s.server.</span><span style=\"color:#B392F0\">ListenAndServe</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctx, cancel </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithTimeout</span><span style=\"color:#E1E4E8\">(context.</span><span style=\"color:#B392F0\">Background</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">time.Second)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#B392F0\"> cancel</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> s.server.</span><span style=\"color:#B392F0\">Shutdown</span><span style=\"color:#E1E4E8\">(ctx)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LogPushRequest</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Streams []</span><span style=\"color:#F97583\">struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Stream  </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"stream\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Values  [][]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">       `json:\"values\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } </span><span style=\"color:#9ECBFF\">`json:\"streams\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">handleLogIngestion</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Rate limiting check</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">s.limiter.</span><span style=\"color:#B392F0\">TryConsume</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        http.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">(w, </span><span style=\"color:#9ECBFF\">\"Rate limit exceeded\"</span><span style=\"color:#E1E4E8\">, http.StatusTooManyRequests)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Parse tenant context from request headers</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tenantID </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> r.Header.</span><span style=\"color:#B392F0\">Get</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"X-Org-ID\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> tenantID </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        http.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">(w, </span><span style=\"color:#9ECBFF\">\"Missing tenant ID\"</span><span style=\"color:#E1E4E8\">, http.StatusBadRequest)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> req </span><span style=\"color:#B392F0\">LogPushRequest</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> json.</span><span style=\"color:#B392F0\">NewDecoder</span><span style=\"color:#E1E4E8\">(r.Body).</span><span style=\"color:#B392F0\">Decode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">req); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        http.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">(w, </span><span style=\"color:#9ECBFF\">\"Invalid JSON\"</span><span style=\"color:#E1E4E8\">, http.StatusBadRequest)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    entriesIngested </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, stream </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> req.Streams {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> _, values </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> stream.Values {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(values) </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                continue</span><span style=\"color:#6A737D\"> // Skip malformed entries</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Parse timestamp and message</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            timestamp, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Parse</span><span style=\"color:#E1E4E8\">(time.RFC3339Nano, values[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Create log entry with tenant isolation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            labels </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> k, v </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> stream.Stream {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                labels[k] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> v</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            labels[</span><span style=\"color:#9ECBFF\">\"tenant_id\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tenantID</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            entry, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> types.</span><span style=\"color:#B392F0\">NewLogEntry</span><span style=\"color:#E1E4E8\">(timestamp, labels, values[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                continue</span><span style=\"color:#6A737D\"> // Skip invalid entries</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Buffer the entry</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> s.buffer.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(entry); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                http.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">(w, </span><span style=\"color:#9ECBFF\">\"Buffer full\"</span><span style=\"color:#E1E4E8\">, http.StatusServiceUnavailable)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            entriesIngested</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.metrics.</span><span style=\"color:#B392F0\">IncrementLogsIngested</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">(entriesIngested))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.</span><span style=\"color:#B392F0\">WriteHeader</span><span style=\"color:#E1E4E8\">(http.StatusNoContent)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">handleHealthCheck</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.</span><span style=\"color:#B392F0\">Header</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Content-Type\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"application/json\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    json.</span><span style=\"color:#B392F0\">NewEncoder</span><span style=\"color:#E1E4E8\">(w).</span><span style=\"color:#B392F0\">Encode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"status\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"healthy\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"buffer_utilization\"</span><span style=\"color:#E1E4E8\">: s.buffer.</span><span style=\"color:#B392F0\">Utilization</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"timestamp\"</span><span style=\"color:#E1E4E8\">: time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Unix</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p>Complete query coordinator implementation:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> flows</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/your-org/log-aggregator/internal/types</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> QueryCoordinator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    indexEngine  </span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">IndexEngine</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storageEngine </span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">StorageEngine</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    authService  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">AuthService</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metrics      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewQueryCoordinator</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">indexEngine</span><span style=\"color:#B392F0\"> types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">IndexEngine</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                        storageEngine</span><span style=\"color:#B392F0\"> types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">StorageEngine</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                        authService</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">AuthService</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                        metrics</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Metrics</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryCoordinator</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">QueryCoordinator</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        indexEngine:   indexEngine,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        storageEngine: storageEngine,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        authService:   authService,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metrics:       metrics,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">qc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ExecuteQuery</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                                        queryString</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                                        params</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">QueryParams</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResultStream</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    startTime </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        qc.metrics.</span><span style=\"color:#B392F0\">RecordQueryDuration</span><span style=\"color:#E1E4E8\">(time.</span><span style=\"color:#B392F0\">Since</span><span style=\"color:#E1E4E8\">(startTime))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Parse and validate the query string using Lexer and Parser</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Extract tenant context from the request context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Validate tenant permissions for the requested time range and labels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Generate optimized execution plan with predicate pushdown</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Execute index lookups to get EntryReference instances</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Coordinate storage access to retrieve log content</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Apply content-based filters and format results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Return ResultStream with proper pagination and metadata</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use context.WithTimeout to enforce query timeouts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Check tenant quotas before starting expensive operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Stream results instead of loading everything into memory</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"query execution not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> flows</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// IngestionCoordinator manages the complete ingestion pipeline from </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// protocol reception through storage persistence</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> IngestionCoordinator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    protocols    []</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ProtocolHandler</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parser       </span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">LogParser</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    buffer       </span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">LogBuffer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    indexEngine  </span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">IndexEngine</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storageEngine </span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">StorageEngine</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metrics      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stopChan     </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ic </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">IngestionCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">StartIngestion</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Start all protocol handlers (HTTP, TCP, UDP, file tail)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Launch buffer processing goroutines</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Initialize index coordination</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Setup storage coordination</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Begin background maintenance tasks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use sync.WaitGroup to coordinate goroutine startup</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Implement graceful shutdown with context cancellation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ic </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">IngestionCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">processBufferEntries</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Continuously read entries from buffer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Batch entries for efficient processing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Extract terms for index updates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Coordinate with storage engine for persistence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Handle backpressure when downstream is slow</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Process entries in time-aligned batches for better compression</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use channel buffering to handle temporary slowdowns</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">qc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">executeIndexLookup</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                                             selectors</span><span style=\"color:#B392F0\"> types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">LabelSelectors</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                                             timeRange</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">TimeRange</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">EntryReference</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Identify relevant index segments based on time range</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Extract terms from label selectors for index queries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Perform bloom filter checks to eliminate segments</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Execute inverted index lookups for matching terms</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Intersect posting lists from multiple terms</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Return deduplicated EntryReference instances</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use bloom filters first to avoid expensive storage lookups</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Process segments in parallel but limit concurrency</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MaintenanceCoordinator handles all background maintenance operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MaintenanceCoordinator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    indexEngine   </span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">IndexEngine</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storageEngine </span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">StorageEngine</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queryTracker  </span><span style=\"color:#B392F0\">types</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ActiveQueryTracker</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scheduler     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MaintenanceScheduler</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">mc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MaintenanceCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">runIndexCompaction</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Identify segments eligible for compaction</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check resource availability for compaction work</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Acquire locks on source segments</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Merge posting lists and bloom filters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Atomically replace old segments with new merged segment</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Clean up source segments after confirming no active queries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use copy-on-write to allow concurrent queries during compaction</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Monitor system resources and throttle compaction if needed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">mc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MaintenanceCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">enforceRetentionPolicies</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Evaluate all retention policies against current chunks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Identify chunks exceeding retention limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Check for active queries accessing chunks marked for deletion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Remove chunks and update index segments</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Verify cleanup completion and update metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Record retention actions for audit trail</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use reference counting to prevent deletion of active chunks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Apply grace periods before actual deletion for safety</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints\">Language-Specific Hints</h4>\n<p><strong>Goroutine Management</strong>: Use <code>sync.WaitGroup</code> and <code>context.Context</code> for coordinating concurrent operations. Always provide cancellation mechanisms for long-running background tasks.</p>\n<p><strong>Memory Management</strong>: Be careful with slice growth in hot paths. Pre-allocate slices when the size is known, and consider using sync.Pool for frequently allocated objects.</p>\n<p><strong>Error Handling</strong>: Wrap errors with context using <code>fmt.Errorf(&quot;operation failed: %w&quot;, err)</code>. This preserves error chains for debugging while adding operation context.</p>\n<p><strong>Channel Usage</strong>: Prefer buffered channels for producer-consumer patterns, but size buffers appropriately. Unbuffered channels are better for synchronization points.</p>\n<p><strong>HTTP Timeouts</strong>: Always set read/write timeouts on HTTP servers. Use <code>context.WithTimeout</code> for outbound requests to prevent hanging operations.</p>\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>After Milestone 1 (Log Ingestion)</strong>: Run <code>curl -X POST localhost:8080/api/v1/push -H &quot;X-Org-ID: test-tenant&quot; -d &#39;{&quot;streams&quot;:[{&quot;stream&quot;:{&quot;service&quot;:&quot;test&quot;},&quot;values&quot;:[[&quot;2023-01-01T12:00:00Z&quot;,&quot;test message&quot;]]}]}&#39;</code>. Verify logs appear in storage and can be found through basic queries.</p>\n<p><strong>After Milestone 3 (Query Engine)</strong>: Execute <code>curl &quot;localhost:8080/api/v1/query?query={service=\\&quot;test\\&quot;}&amp;start=2023-01-01T11:00:00Z&amp;end=2023-01-01T13:00:00Z&quot;</code>. Confirm query returns ingested test logs in proper JSON format.</p>\n<p><strong>After Milestone 5 (Multi-tenancy)</strong>: Test tenant isolation by ingesting logs with different <code>X-Org-ID</code> headers and confirming queries only return logs for the requesting tenant.</p>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Ingestion HTTP 503 errors</td>\n<td>Buffer overflow from processing backlog</td>\n<td>Check buffer utilization metrics</td>\n<td>Increase buffer size or add more processing goroutines</td>\n</tr>\n<tr>\n<td>Queries return partial results</td>\n<td>Index segments being compacted during query</td>\n<td>Enable query/compaction coordination logging</td>\n<td>Implement proper segment locking during compaction</td>\n</tr>\n<tr>\n<td>Memory usage grows unbounded</td>\n<td>ResultStream not being closed properly</td>\n<td>Monitor goroutine counts and heap profiles</td>\n<td>Add proper resource cleanup in defer blocks</td>\n</tr>\n<tr>\n<td>Query performance degrades over time</td>\n<td>Index segments not being compacted</td>\n<td>Monitor segment count per time partition</td>\n<td>Tune compaction triggers to run more frequently</td>\n</tr>\n<tr>\n<td>WAL files growing too large</td>\n<td>Checkpoint creation failing</td>\n<td>Check WAL maintenance task logs</td>\n<td>Fix checkpoint creation bugs and add monitoring</td>\n</tr>\n</tbody></table>\n<h2 id=\"error-handling-and-edge-cases\">Error Handling and Edge Cases</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section applies to all milestones (1-5), providing comprehensive failure mode analysis and recovery strategies that ensure system reliability across log ingestion, indexing, querying, storage, and multi-tenant operations.</p>\n</blockquote>\n<h3 id=\"mental-model-the-hospital-emergency-response-system\">Mental Model: The Hospital Emergency Response System</h3>\n<p>Think of error handling in our log aggregation system like a hospital&#39;s emergency response protocols. Just as a hospital has different response procedures for different types of emergencies (heart attack vs. broken bone vs. power outage), our system needs different recovery strategies for different failure modes. The hospital has early warning systems (patient monitors), escalation procedures (calling specialists), and graceful degradation plans (backup generators). Similarly, our log aggregation system needs comprehensive monitoring, automated recovery mechanisms, and fallback modes that keep the system operational even when components fail.</p>\n<p>The key insight is that in both systems, the goal isn&#39;t to prevent all failures (impossible), but to detect them quickly, respond appropriately, and maintain critical functions. A hospital doesn&#39;t shut down when one monitor fails - it switches to backup equipment and continues treating patients. Our log aggregation system should exhibit the same resilience, continuing to ingest new logs and serve queries even when individual components experience problems.</p>\n<h2 id=\"system-failure-modes\">System Failure Modes</h2>\n<h3 id=\"network-partition-and-connectivity-failures\">Network Partition and Connectivity Failures</h3>\n<p>Network partitions represent one of the most challenging failure scenarios in distributed log aggregation systems. When network connectivity degrades or fails completely, different components can become isolated from each other, leading to split-brain scenarios and data consistency issues.</p>\n<p><strong>Client-to-Ingestion Partition</strong>: When clients cannot reach the ingestion endpoints, they may attempt to buffer logs locally or drop them entirely. The system must distinguish between permanent client failures (where buffering is wasteful) and temporary network issues (where aggressive retry is appropriate). During network splits, the ingestion layer continues operating but cannot communicate with downstream components like the index or storage engines.</p>\n<p><strong>Ingestion-to-Storage Partition</strong>: Perhaps the most critical partition scenario occurs when the ingestion layer can receive logs but cannot persist them to storage. In this case, the <code>MemoryBuffer</code> becomes the last line of defense, but it has finite capacity. The system must implement intelligent backpressure mechanisms that signal upstream clients to slow their ingestion rate while attempting to restore connectivity to storage systems.</p>\n<p><strong>Index-to-Storage Partition</strong>: When the indexing engine cannot access stored chunks, queries fail even though ingestion may continue normally. This creates a scenario where new logs arrive and get indexed, but historical queries return incomplete results. The system must track which time ranges have complete vs. partial index coverage to provide accurate query result metadata.</p>\n<table>\n<thead>\n<tr>\n<th>Partition Type</th>\n<th>Immediate Impact</th>\n<th>Data Loss Risk</th>\n<th>Recovery Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Client-to-Ingestion</td>\n<td>New logs queued at client</td>\n<td>High if client buffers overflow</td>\n<td>Low - resume ingestion when connected</td>\n</tr>\n<tr>\n<td>Ingestion-to-Storage</td>\n<td>Logs buffered in memory</td>\n<td>High if memory buffers full</td>\n<td>Medium - replay buffered logs</td>\n</tr>\n<tr>\n<td>Index-to-Storage</td>\n<td>Query degradation</td>\n<td>None - data persisted</td>\n<td>High - rebuild index state</td>\n</tr>\n<tr>\n<td>Storage-to-Query</td>\n<td>Historical queries fail</td>\n<td>None</td>\n<td>Medium - wait for connectivity</td>\n</tr>\n</tbody></table>\n<p><strong>Cross-Component Communication Failures</strong>: The system uses various protocols for internal communication. HTTP connections may timeout, TCP connections may reset, and file system operations may hang during storage issues. Each communication channel requires specific timeout configurations, retry policies, and circuit breaker implementations to prevent cascading failures.</p>\n<blockquote>\n<p><strong>Critical Design Insight</strong>: Network partitions are not just connectivity failures - they create temporal inconsistencies where different components have different views of system state. Recovery must reconcile these divergent states carefully.</p>\n</blockquote>\n<h3 id=\"disk-and-storage-failures\">Disk and Storage Failures</h3>\n<p>Storage failures manifest in multiple ways, each requiring distinct detection and recovery strategies. Understanding the failure modes of underlying storage systems is crucial for building robust error handling.</p>\n<p><strong>Disk Full Conditions</strong>: When storage volumes approach capacity, the system must gracefully degrade rather than crash. The <code>StorageEngine</code> should monitor available space continuously and implement storage pressure relief mechanisms. This includes accelerating retention policy execution, compressing older chunks more aggressively, and potentially rejecting new ingestion requests with explicit backpressure signals.</p>\n<p><strong>Write-Ahead Log Corruption</strong>: The WAL provides durability guarantees, but the WAL files themselves can become corrupted due to hardware failures, incomplete writes during system crashes, or file system issues. WAL record corruption detection relies on checksums embedded in <code>WALRecord</code> structures. When corruption is detected, the system must determine how much of the WAL remains valid and whether recent ingestion needs to be replayed from upstream sources.</p>\n<p><strong>Chunk File Corruption</strong>: Individual chunk files may become corrupted or unreadable. Since chunks contain compressed log data organized by time windows, chunk corruption affects queries for specific time ranges. The system should maintain chunk integrity checksums in <code>ChunkHeader</code> structures and implement graceful degradation where corrupted chunks are marked as unavailable rather than causing query failures.</p>\n<p><strong>Index File Corruption</strong>: Inverted index corruption is particularly problematic because it affects query performance across multiple time ranges. When <code>IndexSegment</code> files become corrupted, the system can fall back to sequential scanning of chunks, but this severely impacts query performance. Index rebuilding from stored chunks provides recovery but requires significant computational resources.</p>\n<table>\n<thead>\n<tr>\n<th>Storage Failure Type</th>\n<th>Detection Method</th>\n<th>Recovery Strategy</th>\n<th>Performance Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Disk Full</td>\n<td>Space monitoring alerts</td>\n<td>Accelerate retention, reject ingestion</td>\n<td>High - ingestion blocked</td>\n</tr>\n<tr>\n<td>WAL Corruption</td>\n<td>Checksum validation</td>\n<td>Truncate at corruption point, replay</td>\n<td>Medium - recent data loss</td>\n</tr>\n<tr>\n<td>Chunk Corruption</td>\n<td>Read verification failures</td>\n<td>Mark chunk unavailable, query degradation</td>\n<td>Low - specific time ranges</td>\n</tr>\n<tr>\n<td>Index Corruption</td>\n<td>Lookup failures, checksum errors</td>\n<td>Rebuild from chunks, sequential fallback</td>\n<td>High - query performance</td>\n</tr>\n</tbody></table>\n<p><strong>Storage Backend Failures</strong>: When using cloud storage (S3-compatible systems), the failure modes include authentication failures, rate limiting, temporary unavailability, and permanent account suspension. The system must implement exponential backoff retry policies and maintain local caching to survive temporary cloud storage outages.</p>\n<h3 id=\"memory-exhaustion-and-resource-limits\">Memory Exhaustion and Resource Limits</h3>\n<p>Memory pressure represents a gradual failure mode that can lead to sudden system crashes if not handled proactively. Different components have distinct memory usage patterns that require specialized monitoring and mitigation strategies.</p>\n<p><strong>Ingestion Buffer Overflow</strong>: The <code>MemoryBuffer</code> used for log ingestion has a fixed capacity defined by <code>BUFFER_SIZE</code>. When ingestion rates exceed processing capacity, the buffer fills up. The system must implement sophisticated buffer management that preserves high-priority logs (critical severity levels) while dropping less important entries. Buffer overflow also triggers backpressure signals to upstream clients, requesting them to reduce their ingestion rates.</p>\n<p><strong>Query Result Set Explosion</strong>: Large query result sets can exhaust available memory, particularly when clients request broad time ranges with minimal filtering. The <code>QueryEngine</code> must implement streaming execution with configurable memory limits per query. When memory thresholds are approached, queries should switch to more aggressive pagination or terminate with partial results rather than crash the system.</p>\n<p><strong>Index Memory Pressure</strong>: In-memory index structures like bloom filters and term dictionaries can grow beyond available memory, particularly in high-cardinality logging environments. The indexing engine should implement memory-aware index segment sizing and proactive compaction to control memory usage. When memory pressure is detected, the system can temporarily disable non-essential indexing features like bloom filter updates.</p>\n<p><strong>Multi-Tenant Memory Isolation</strong>: In multi-tenant deployments, individual tenants can consume excessive memory through large queries or high ingestion rates. The system must implement per-tenant memory quotas and enforce them across all components. Memory quota violations trigger tenant-specific backpressure rather than affecting other tenants.</p>\n<table>\n<thead>\n<tr>\n<th>Memory Pressure Source</th>\n<th>Warning Indicators</th>\n<th>Mitigation Strategy</th>\n<th>Fallback Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Ingestion Buffers</td>\n<td>Buffer fill percentage &gt; 80%</td>\n<td>Increase processing threads, backpressure</td>\n<td>Drop low-priority logs</td>\n</tr>\n<tr>\n<td>Query Results</td>\n<td>Query memory usage &gt; limit</td>\n<td>Switch to streaming, aggressive pagination</td>\n<td>Return partial results</td>\n</tr>\n<tr>\n<td>Index Structures</td>\n<td>Index memory growth rate</td>\n<td>Trigger compaction, bloom filter cleanup</td>\n<td>Disable bloom filters</td>\n</tr>\n<tr>\n<td>Tenant Isolation</td>\n<td>Per-tenant memory quota exceeded</td>\n<td>Apply tenant-specific limits</td>\n<td>Reject tenant requests</td>\n</tr>\n</tbody></table>\n<p><strong>Garbage Collection Pressure</strong>: In garbage-collected languages like Go, memory pressure can manifest as excessive GC overhead that impacts system throughput. The system should monitor GC metrics and implement object pooling for frequently allocated types like <code>LogEntry</code> and query result structures to reduce allocation pressure.</p>\n<h3 id=\"process-and-component-crashes\">Process and Component Crashes</h3>\n<p>Component crashes represent the most severe failure mode but are also the most predictable to handle with proper design. Each component must be designed with crash-safe state management and rapid recovery capabilities.</p>\n<p><strong>Ingestion Process Crashes</strong>: When ingestion processes crash, buffered logs in memory are lost unless they&#39;ve been persisted to WAL. The ingestion restart procedure must replay uncommitted WAL records and re-establish connections with upstream clients. Client connection state is typically lost during crashes, so clients must implement retry logic with exponential backoff to reconnect after ingestion recovery.</p>\n<p><strong>Indexing Process Crashes</strong>: Index building is a CPU and memory intensive process that&#39;s particularly susceptible to crashes during high load. Partially constructed index segments must be discarded and rebuilt from scratch, as incomplete indexes can return inconsistent query results. The indexing engine should implement checkpointing for long-running index operations to minimize work lost during crashes.</p>\n<p><strong>Query Engine Crashes</strong>: Query crashes typically occur during execution of complex queries or when processing corrupted data. Since queries are stateless operations, crashes don&#39;t cause data loss but do impact user experience. The query engine should implement per-query isolation using separate goroutines or processes to prevent one failing query from crashing the entire query service.</p>\n<p><strong>Storage Engine Crashes</strong>: Storage crashes are the most critical because they can lead to data loss if the WAL isn&#39;t properly recovered. The storage restart sequence must validate WAL integrity, replay uncommitted operations, and verify chunk consistency before accepting new write requests. During recovery, the system should reject new ingestion to prevent data interleaving with recovery operations.</p>\n<table>\n<thead>\n<tr>\n<th>Component Crash</th>\n<th>Recovery Time</th>\n<th>Data Loss Risk</th>\n<th>Restart Dependencies</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Ingestion Process</td>\n<td>&lt; 30 seconds</td>\n<td>Recent buffer contents</td>\n<td>WAL availability, client reconnection</td>\n</tr>\n<tr>\n<td>Indexing Process</td>\n<td>1-5 minutes</td>\n<td>None - rebuilds from chunks</td>\n<td>Chunk accessibility</td>\n</tr>\n<tr>\n<td>Query Engine</td>\n<td>&lt; 10 seconds</td>\n<td>None - stateless operations</td>\n<td>Index and storage availability</td>\n</tr>\n<tr>\n<td>Storage Engine</td>\n<td>30 seconds - 10 minutes</td>\n<td>WAL replay required</td>\n<td>File system, WAL integrity</td>\n</tr>\n</tbody></table>\n<h3 id=\"downstream-service-dependencies\">Downstream Service Dependencies</h3>\n<p>The log aggregation system depends on various external services that can fail independently, requiring graceful degradation strategies that maintain core functionality even when dependencies are unavailable.</p>\n<p><strong>Authentication Service Failures</strong>: When external authentication systems become unavailable, the system must decide between rejecting all requests (secure but unavailable) or allowing degraded access (available but potentially insecure). A hybrid approach uses cached authentication tokens with extended validity during auth service outages, combined with audit logging of all access during degraded mode.</p>\n<p><strong>Notification Service Failures</strong>: Alert notifications depend on external services like email servers, Slack webhooks, or SMS gateways. Notification failures shouldn&#39;t impact core log processing, but the alerting system must implement retry queues and alternative notification channels. When primary notification channels fail, the system should escalate to backup channels and log notification failures for later analysis.</p>\n<p><strong>Time Synchronization Services</strong>: Log aggregation systems depend heavily on accurate timestamps. When NTP services become unavailable, system clocks can drift, leading to timestamp inconsistencies that complicate querying and retention policies. The system should monitor clock drift and implement timestamp validation that detects and corrects minor discrepancies while flagging major timestamp anomalies.</p>\n<p><strong>Service Discovery Dependencies</strong>: In containerized deployments, the system may depend on service discovery mechanisms like Consul or Kubernetes DNS. Service discovery failures prevent components from locating each other, effectively creating network partition scenarios. Components should cache service locations and implement fallback discovery mechanisms using configuration files or environment variables.</p>\n<h2 id=\"failure-detection-and-monitoring\">Failure Detection and Monitoring</h2>\n<h3 id=\"health-check-implementation\">Health Check Implementation</h3>\n<p>Effective failure detection requires comprehensive health checks that monitor both individual component status and cross-component integration points. Health checks must be fast, reliable, and provide actionable diagnostic information.</p>\n<p><strong>Component-Level Health Checks</strong>: Each component implements standardized health check endpoints that report detailed status information. The ingestion engine health check verifies buffer availability, upstream connectivity, and downstream persistence capability. The storage engine health check validates WAL integrity, disk space availability, and chunk accessibility. These health checks should complete within strict time limits (typically 1-2 seconds) to enable rapid failure detection.</p>\n<p><strong>Cross-Component Integration Checks</strong>: Beyond individual component health, the system needs integration checks that verify end-to-end functionality. An integration health check might ingest a synthetic log entry, verify it gets indexed correctly, execute a query to retrieve it, and measure the round-trip time. These checks detect subtle integration failures that component-level checks might miss.</p>\n<p><strong>External Dependency Checks</strong>: Health checks must monitor external dependencies like authentication services, notification endpoints, and storage backends. These checks should be implemented with appropriate timeouts and circuit breaker patterns to prevent dependency failures from cascading to the health check system itself.</p>\n<table>\n<thead>\n<tr>\n<th>Health Check Type</th>\n<th>Check Frequency</th>\n<th>Timeout</th>\n<th>Failure Threshold</th>\n<th>Recovery Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Component Status</td>\n<td>Every 10 seconds</td>\n<td>2 seconds</td>\n<td>3 consecutive failures</td>\n<td>Restart component</td>\n</tr>\n<tr>\n<td>Integration End-to-End</td>\n<td>Every 60 seconds</td>\n<td>10 seconds</td>\n<td>2 consecutive failures</td>\n<td>Alert operations team</td>\n</tr>\n<tr>\n<td>External Dependencies</td>\n<td>Every 30 seconds</td>\n<td>5 seconds</td>\n<td>5 consecutive failures</td>\n<td>Enable degraded mode</td>\n</tr>\n<tr>\n<td>Resource Utilization</td>\n<td>Every 5 seconds</td>\n<td>1 second</td>\n<td>Sustained high usage</td>\n<td>Trigger scaling/throttling</td>\n</tr>\n</tbody></table>\n<p><strong>Synthetic Transaction Monitoring</strong>: The system should continuously execute synthetic transactions that represent real user workflows. A synthetic transaction might simulate client log ingestion, wait for indexing to complete, execute representative queries, and verify correct results. This provides early warning of performance degradation or functional failures before they impact real users.</p>\n<h3 id=\"metrics-collection-and-alerting\">Metrics Collection and Alerting</h3>\n<p>Comprehensive metrics collection enables proactive failure detection and performance monitoring across all system components. Metrics must be collected efficiently without impacting system performance and should provide both real-time monitoring and historical trend analysis.</p>\n<p><strong>Ingestion Metrics</strong>: The ingestion layer tracks detailed metrics about log reception rates, parsing success/failure ratios, buffer utilization, and downstream persistence latency. Buffer utilization metrics should trigger alerts before buffers fill completely, providing time for corrective action. Parsing failure rates help detect log format changes or corruption issues.</p>\n<p><strong>Storage and Persistence Metrics</strong>: Storage metrics monitor WAL size and growth rate, chunk creation frequency, disk utilization, and retention policy execution. WAL growth rate anomalies can indicate persistence problems or downstream processing delays. Chunk creation patterns help optimize storage efficiency and detect ingestion rate changes.</p>\n<p><strong>Query Performance Metrics</strong>: Query metrics track execution times, result set sizes, index hit rates, and query complexity. Degrading query performance often indicates index corruption, storage issues, or resource contention. Query metrics should be segmented by tenant and query type to enable targeted optimization.</p>\n<p><strong>Resource Utilization Metrics</strong>: System-level metrics monitor CPU usage, memory consumption, disk I/O rates, and network bandwidth utilization. Resource metrics help predict capacity needs and detect resource leaks or efficiency regressions.</p>\n<table>\n<thead>\n<tr>\n<th>Metric Category</th>\n<th>Key Indicators</th>\n<th>Alert Thresholds</th>\n<th>Diagnostic Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Ingestion Rate</td>\n<td>Logs/second, bytes/second</td>\n<td>50% above baseline</td>\n<td>Detect traffic spikes or drops</td>\n</tr>\n<tr>\n<td>Buffer Health</td>\n<td>Fill percentage, overflow rate</td>\n<td>80% utilization</td>\n<td>Prevent data loss from buffer overflow</td>\n</tr>\n<tr>\n<td>Query Performance</td>\n<td>P95 latency, timeout rate</td>\n<td>2x baseline latency</td>\n<td>Identify performance degradation</td>\n</tr>\n<tr>\n<td>Storage Health</td>\n<td>Disk usage, WAL size</td>\n<td>85% disk full</td>\n<td>Prevent storage exhaustion</td>\n</tr>\n<tr>\n<td>Error Rates</td>\n<td>Parse failures, query errors</td>\n<td>5% error rate</td>\n<td>Detect system or data issues</td>\n</tr>\n</tbody></table>\n<p><strong>Distributed Tracing Integration</strong>: For complex failure scenarios, distributed tracing helps understand how requests flow through system components and where failures occur. Each log entry and query should be associated with trace identifiers that enable end-to-end request tracking across component boundaries.</p>\n<h3 id=\"circuit-breaker-patterns\">Circuit Breaker Patterns</h3>\n<p>Circuit breakers prevent cascading failures by automatically stopping requests to failing downstream components, allowing them time to recover while protecting upstream components from overload.</p>\n<p><strong>Storage Circuit Breakers</strong>: When storage systems experience high latency or failure rates, circuit breakers prevent the ingestion system from continuing to send write requests that will fail. The circuit breaker monitors storage operation success rates and latencies, transitioning to an &quot;open&quot; state when thresholds are exceeded. During the open state, ingestion falls back to memory buffering while periodically testing storage recovery.</p>\n<p><strong>Query Circuit Breakers</strong>: Query circuit breakers protect against expensive queries that might exhaust system resources. When query execution times exceed thresholds or when the query engine experiences high load, circuit breakers can reject new queries with appropriate error messages. This prevents query storms from impacting ingestion or other critical operations.</p>\n<p><strong>External Service Circuit Breakers</strong>: Circuit breakers for authentication services, notification systems, and other external dependencies prevent external service failures from blocking internal operations. When external services fail, circuit breakers enable degraded mode operation with cached data or alternative processing paths.</p>\n<table>\n<thead>\n<tr>\n<th>Circuit Breaker Type</th>\n<th>Failure Threshold</th>\n<th>Open Duration</th>\n<th>Half-Open Test</th>\n<th>Recovery Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Storage Operations</td>\n<td>20% failure rate over 1 minute</td>\n<td>30 seconds</td>\n<td>Single test write</td>\n<td>5 successful operations</td>\n</tr>\n<tr>\n<td>Query Execution</td>\n<td>P95 latency &gt; 10 seconds</td>\n<td>60 seconds</td>\n<td>Simple test query</td>\n<td>3 fast test queries</td>\n</tr>\n<tr>\n<td>Authentication</td>\n<td>3 consecutive timeouts</td>\n<td>300 seconds</td>\n<td>Health check call</td>\n<td>Successful auth response</td>\n</tr>\n<tr>\n<td>Notifications</td>\n<td>10 failures in 5 minutes</td>\n<td>120 seconds</td>\n<td>Single test notification</td>\n<td>Successful delivery</td>\n</tr>\n</tbody></table>\n<p><strong>Circuit Breaker State Management</strong>: Circuit breakers maintain state across system restarts to prevent restart-induced failure storms. Circuit breaker states and failure statistics are persisted to disk and restored during system initialization. This prevents components from immediately overwhelming recently-failed dependencies during recovery scenarios.</p>\n<h2 id=\"recovery-and-resilience-strategies\">Recovery and Resilience Strategies</h2>\n<h3 id=\"graceful-degradation-patterns\">Graceful Degradation Patterns</h3>\n<p>Graceful degradation ensures that system components can continue operating with reduced functionality when dependencies fail, rather than failing completely. This approach maintains core functionality while temporarily disabling non-essential features.</p>\n<p><strong>Ingestion Degradation</strong>: When downstream storage or indexing components fail, the ingestion system can continue accepting logs by increasing buffer sizes, enabling memory-only operation, or forwarding logs to backup storage locations. During degraded operation, ingestion should prioritize high-severity logs and implement intelligent dropping of low-priority entries when buffers approach capacity.</p>\n<p><strong>Query Degradation</strong>: When index components fail or become unavailable, the query engine can fall back to sequential scanning of stored chunks. While this dramatically reduces query performance, it maintains query functionality for urgent debugging scenarios. Query degradation should be transparent to clients, with additional metadata indicating degraded performance and potentially incomplete results.</p>\n<p><strong>Index Degradation</strong>: When bloom filters become corrupted or unavailable, the indexing system can continue operating without probabilistic optimizations. This increases false positive rates in negative lookups but maintains correctness. Similarly, when inverted index segments fail, the system can rebuild minimal indexes from chunk data to maintain basic querying capability.</p>\n<table>\n<thead>\n<tr>\n<th>Degradation Scenario</th>\n<th>Reduced Functionality</th>\n<th>Performance Impact</th>\n<th>Recovery Path</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Storage Unavailable</td>\n<td>Memory-only buffering</td>\n<td>High - limited capacity</td>\n<td>Restore storage, flush buffers</td>\n</tr>\n<tr>\n<td>Index Unavailable</td>\n<td>Sequential chunk scanning</td>\n<td>Very High - 10-100x slower</td>\n<td>Rebuild index from chunks</td>\n</tr>\n<tr>\n<td>Auth Service Down</td>\n<td>Cached credential validation</td>\n<td>Low - stale permissions</td>\n<td>Restore auth service</td>\n</tr>\n<tr>\n<td>Alerting Failed</td>\n<td>Log alerts without delivery</td>\n<td>None - processing continues</td>\n<td>Restore notification channels</td>\n</tr>\n</tbody></table>\n<p><strong>Feature Flag Integration</strong>: The system implements feature flags that enable rapid disabling of non-essential features during degraded operation. Feature flags control bloom filter usage, advanced query optimizations, real-time alerting, and other features that can be temporarily disabled to reduce resource usage and complexity during recovery scenarios.</p>\n<h3 id=\"automatic-recovery-mechanisms\">Automatic Recovery Mechanisms</h3>\n<p>Automatic recovery reduces manual intervention requirements and enables faster restoration of full system functionality after failures. Recovery mechanisms must be designed carefully to avoid recovery loops and additional system stress.</p>\n<p><strong>WAL Replay and Recovery</strong>: The storage engine implements comprehensive WAL replay logic that reconstructs system state after crashes. WAL replay validates record checksums, ensures temporal consistency, and handles partial write scenarios. The recovery process rebuilds in-memory state from WAL records and verifies consistency with persisted chunk data before accepting new operations.</p>\n<p><strong>Index Reconstruction</strong>: When index corruption is detected, the system can automatically trigger index rebuilding from stored chunk data. Index reconstruction is resource-intensive and should be scheduled during low-traffic periods when possible. The system maintains multiple index segment versions to enable rollback if reconstruction fails.</p>\n<p><strong>Buffer Recovery and Replay</strong>: When components restart after crashes, memory buffers are lost but can be reconstructed from WAL records. The recovery process identifies uncommitted buffer contents and replays them through the normal processing pipeline. This ensures that no data is lost due to component restarts, even if the restart occurs during high ingestion rates.</p>\n<p><strong>Partition Healing</strong>: After network partitions resolve, system components must reconcile state differences that accumulated during the partition. Partition healing involves comparing timestamps, identifying missing data, and triggering replication or rebuilding of missing information. The healing process must handle conflicts carefully to maintain data consistency.</p>\n<table>\n<thead>\n<tr>\n<th>Recovery Type</th>\n<th>Trigger Condition</th>\n<th>Duration</th>\n<th>Success Criteria</th>\n<th>Fallback Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>WAL Replay</td>\n<td>Component startup after crash</td>\n<td>30 seconds - 10 minutes</td>\n<td>All records processed</td>\n<td>Manual WAL inspection</td>\n</tr>\n<tr>\n<td>Index Rebuild</td>\n<td>Corruption detection</td>\n<td>10 minutes - 2 hours</td>\n<td>Query performance restored</td>\n<td>Sequential scan fallback</td>\n</tr>\n<tr>\n<td>Buffer Replay</td>\n<td>Memory loss during restart</td>\n<td>1-5 minutes</td>\n<td>Buffer state restored</td>\n<td>Accept buffer data loss</td>\n</tr>\n<tr>\n<td>Partition Healing</td>\n<td>Network connectivity restored</td>\n<td>5-30 minutes</td>\n<td>State consistency achieved</td>\n<td>Manual reconciliation</td>\n</tr>\n</tbody></table>\n<h3 id=\"manual-intervention-procedures\">Manual Intervention Procedures</h3>\n<p>Despite comprehensive automatic recovery mechanisms, some failure scenarios require manual intervention. Manual procedures must be well-documented, tested regularly, and designed to minimize system downtime and data loss risk.</p>\n<p><strong>Emergency Shutdown Procedures</strong>: When system instability threatens data integrity, emergency shutdown procedures ensure graceful termination of all components with proper state persistence. Emergency shutdown flushes all buffers, forces WAL sync operations, and creates recovery checkpoints that enable clean restart. Shutdown procedures should complete within strict time limits to prevent external monitoring systems from forcing unclean termination.</p>\n<p><strong>Data Recovery and Restoration</strong>: When automatic recovery mechanisms fail, manual data recovery procedures provide step-by-step guidance for restoring system state from backups, WAL files, and chunk archives. Recovery procedures must handle various corruption scenarios and provide validation steps to ensure data integrity after restoration.</p>\n<p><strong>Index Repair and Validation</strong>: Manual index repair procedures address corruption scenarios that automatic rebuilding cannot handle. These procedures include index validation tools, selective segment rebuilding, and manual bloom filter reconstruction. Index repair should provide detailed progress reporting and intermediate validation checkpoints to enable recovery from repair failures.</p>\n<p><strong>Tenant Isolation Repair</strong>: In multi-tenant systems, tenant isolation failures can cause data leakage between tenants. Manual isolation repair procedures include tenant data audit tools, isolation boundary validation, and secure data migration between tenant namespaces. These procedures must maintain strict audit logs to satisfy compliance requirements.</p>\n<table>\n<thead>\n<tr>\n<th>Manual Procedure</th>\n<th>When Required</th>\n<th>Estimated Time</th>\n<th>Risk Level</th>\n<th>Required Expertise</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Emergency Shutdown</td>\n<td>System instability detected</td>\n<td>5-10 minutes</td>\n<td>Low</td>\n<td>Operations team</td>\n</tr>\n<tr>\n<td>Data Restoration</td>\n<td>Automatic recovery failed</td>\n<td>30 minutes - 4 hours</td>\n<td>High - potential data loss</td>\n<td>Senior engineers</td>\n</tr>\n<tr>\n<td>Index Repair</td>\n<td>Persistent index corruption</td>\n<td>1-8 hours</td>\n<td>Medium</td>\n<td>Log aggregation specialists</td>\n</tr>\n<tr>\n<td>Tenant Isolation Repair</td>\n<td>Cross-tenant data leakage</td>\n<td>2-24 hours</td>\n<td>Very High - security impact</td>\n<td>Security + engineering team</td>\n</tr>\n</tbody></table>\n<p><strong>Disaster Recovery Procedures</strong>: Complete system failure scenarios require comprehensive disaster recovery procedures that rebuild the entire log aggregation system from backups and archives. Disaster recovery procedures include infrastructure provisioning, data restoration sequencing, component startup ordering, and system validation testing. These procedures should be tested regularly in isolated environments to ensure effectiveness.</p>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Cascade Failure Amplification</strong>\nMany systems implement retry logic that actually amplifies failures rather than improving resilience. When a storage system is overloaded, aggressive retries from multiple components can worsen the overload condition. Instead, implement exponential backoff with jitter, circuit breakers with appropriate thresholds, and coordinated backoff across components to prevent retry storms.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Error Handling</strong>\nDifferent components handling the same error types with different strategies creates unpredictable system behavior. For example, if ingestion drops logs on storage failure while queries return errors on the same condition, users receive inconsistent experiences. Establish system-wide error handling policies that define consistent responses to common failure scenarios across all components.</p>\n<p>⚠️ <strong>Pitfall: Recovery State Corruption</strong>\nRecovery procedures that don&#39;t properly validate intermediate state can corrupt data during the recovery process itself. WAL replay that doesn&#39;t verify checksums, index rebuilding that doesn&#39;t validate segment consistency, and buffer recovery that doesn&#39;t check timestamp ordering can introduce subtle corruption that manifests later as query inconsistencies or data loss.</p>\n<p>⚠️ <strong>Pitfall: Monitoring Alert Fatigue</strong>\nImplementing too many alerts with inappropriate thresholds leads to alert fatigue where operators ignore genuine critical alerts. Focus on alerts that require immediate action, implement alert aggregation and escalation policies, and regularly review and tune alert thresholds based on operational experience. False positive rates above 10% typically indicate poorly tuned alerting systems.</p>\n<p>⚠️ <strong>Pitfall: Resource Exhaustion During Recovery</strong>\nRecovery operations often consume significant system resources, which can prevent the system from handling normal operations during recovery. Index rebuilding that monopolizes disk I/O, WAL replay that exhausts memory, or partition healing that saturates network bandwidth can extend downtime and create additional failures. Recovery operations should implement resource throttling and yield processing time to critical operations.</p>\n<p>⚠️ <strong>Pitfall: Split-Brain Resolution Data Loss</strong>\nNetwork partition scenarios can create split-brain conditions where different components have conflicting views of system state. Naive split-brain resolution that simply discards one side&#39;s changes can cause significant data loss. Implement conflict detection and resolution strategies that preserve as much data as possible while maintaining consistency guarantees.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This implementation guidance provides concrete tools and patterns for building robust error handling into your log aggregation system. The focus is on proactive failure detection, graceful degradation, and automated recovery mechanisms that maintain system reliability.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Health Checks</td>\n<td>HTTP endpoints with JSON status</td>\n<td>Kubernetes liveness/readiness probes with custom checks</td>\n</tr>\n<tr>\n<td>Metrics Collection</td>\n<td>Prometheus client library with custom metrics</td>\n<td>OpenTelemetry with distributed tracing integration</td>\n</tr>\n<tr>\n<td>Circuit Breakers</td>\n<td>Simple state machine with timeout logic</td>\n<td>Library like <code>hystrix-go</code> with statistical failure detection</td>\n</tr>\n<tr>\n<td>Alerting</td>\n<td>Webhook notifications to Slack/email</td>\n<td>PagerDuty integration with escalation policies</td>\n</tr>\n<tr>\n<td>Log Aggregation</td>\n<td>Structured logging with <code>logrus</code> or <code>zap</code></td>\n<td>ELK stack or centralized logging with correlation IDs</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  internal/monitoring/\n    health/\n      health.go                 ← health check registry and HTTP endpoints\n      checks.go                 ← component-specific health check implementations\n      integration.go            ← end-to-end integration health checks\n    metrics/\n      metrics.go                ← metrics collection and exposition\n      collectors.go             ← custom metric collectors for each component\n    alerts/\n      manager.go                ← alert routing and notification management\n      rules.go                  ← alert rule definitions and evaluation\n  internal/resilience/\n    circuit/\n      breaker.go                ← circuit breaker implementation\n      registry.go               ← circuit breaker management and configuration\n    recovery/\n      wal.go                    ← WAL replay and recovery logic\n      index.go                  ← index reconstruction procedures\n      partition.go              ← network partition healing\n    degradation/\n      features.go               ← feature flag management for graceful degradation\n      fallbacks.go              ← fallback implementations for failed components\n  pkg/errors/\n    errors.go                   ← error types and classification\n    handler.go                  ← centralized error handling policies\n  tools/recovery/\n    emergency-shutdown.go       ← emergency shutdown tooling\n    data-recovery.go           ← manual data recovery utilities\n    validation.go              ← system state validation tools</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Health Check Registry</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> health</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HealthStatus represents the overall health state of a component</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HealthStatus</span><span style=\"color:#F97583\"> string</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    StatusHealthy</span><span style=\"color:#B392F0\">   HealthStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"healthy\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    StatusDegraded</span><span style=\"color:#B392F0\">  HealthStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"degraded\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    StatusUnhealthy</span><span style=\"color:#B392F0\"> HealthStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"unhealthy\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CheckResult contains the result of a health check execution</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CheckResult</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Name      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                 `json:\"name\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Status    </span><span style=\"color:#B392F0\">HealthStatus</span><span style=\"color:#9ECBFF\">           `json:\"status\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Message   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                 `json:\"message,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">              `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Details   </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{} </span><span style=\"color:#9ECBFF\">`json:\"details,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Duration  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\">          `json:\"duration\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HealthCheck defines the interface for component health checks</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HealthCheck</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Name</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Check</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CheckResult</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Registry manages health checks and provides HTTP endpoints</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Registry</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    checks </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">HealthCheck</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex  </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewRegistry creates a new health check registry</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewRegistry</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Registry</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Registry</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        checks: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">HealthCheck</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RegisterCheck adds a health check to the registry</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">r </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Registry</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RegisterCheck</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">check</span><span style=\"color:#B392F0\"> HealthCheck</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    r.mutex.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> r.mutex.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    r.checks[check.</span><span style=\"color:#B392F0\">Name</span><span style=\"color:#E1E4E8\">()] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> check</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CheckAll executes all registered health checks</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">r </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Registry</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CheckAll</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">CheckResult</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    r.mutex.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    checks </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">HealthCheck</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(r.checks))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> name, check </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> r.checks {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        checks[name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> check</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    r.mutex.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    results </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">CheckResult</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> name, check </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> checks {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        result </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> check.</span><span style=\"color:#B392F0\">Check</span><span style=\"color:#E1E4E8\">(ctx)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        result.Duration </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Since</span><span style=\"color:#E1E4E8\">(start)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        results[name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> results</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ServeHTTP implements http.Handler for health check endpoints</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">r </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Registry</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ServeHTTP</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">req</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctx, cancel </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithTimeout</span><span style=\"color:#E1E4E8\">(req.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">time.Second)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#B392F0\"> cancel</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    results </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> r.</span><span style=\"color:#B392F0\">CheckAll</span><span style=\"color:#E1E4E8\">(ctx)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    overallStatus </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> StatusHealthy</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, result </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> results {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> result.Status </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> StatusUnhealthy {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            overallStatus </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> StatusUnhealthy</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            break</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        } </span><span style=\"color:#F97583\">else</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> result.Status </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> StatusDegraded </span><span style=\"color:#F97583\">&#x26;&#x26;</span><span style=\"color:#E1E4E8\"> overallStatus </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> StatusHealthy {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            overallStatus </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> StatusDegraded</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    response </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"status\"</span><span style=\"color:#E1E4E8\">:    overallStatus,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"timestamp\"</span><span style=\"color:#E1E4E8\">: time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"checks\"</span><span style=\"color:#E1E4E8\">:    results,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.</span><span style=\"color:#B392F0\">Header</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Content-Type\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"application/json\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> overallStatus </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> StatusHealthy {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        w.</span><span style=\"color:#B392F0\">WriteHeader</span><span style=\"color:#E1E4E8\">(http.StatusServiceUnavailable)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    json.</span><span style=\"color:#B392F0\">NewEncoder</span><span style=\"color:#E1E4E8\">(w).</span><span style=\"color:#B392F0\">Encode</span><span style=\"color:#E1E4E8\">(response)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Circuit Breaker Implementation</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> circuit</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">errors</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// State represents the current circuit breaker state</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> State</span><span style=\"color:#F97583\"> int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    StateClosed</span><span style=\"color:#B392F0\"> State</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> iota</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    StateHalfOpen</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    StateOpen</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Config defines circuit breaker configuration parameters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Config</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxFailures     </span><span style=\"color:#F97583\">int</span><span style=\"color:#6A737D\">           // Number of failures before opening</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    FailureWindow   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#6A737D\"> // Time window for failure counting</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    OpenDuration    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#6A737D\"> // How long to stay open</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    HalfOpenMaxCalls </span><span style=\"color:#F97583\">int</span><span style=\"color:#6A737D\">          // Max calls allowed in half-open state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Breaker implements a circuit breaker pattern</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Breaker</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config       </span><span style=\"color:#B392F0\">Config</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    state        </span><span style=\"color:#B392F0\">State</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    failures     </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lastFailTime </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    halfOpenCalls </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex        </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewBreaker creates a new circuit breaker with the given configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewBreaker</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#B392F0\"> Config</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Breaker</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Breaker</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config: config,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        state:  StateClosed,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Call executes the given function with circuit breaker protection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">b </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Breaker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Call</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">fn</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    state, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> b.</span><span style=\"color:#B392F0\">beforeCall</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> r </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> recover</span><span style=\"color:#E1E4E8\">(); r </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            b.</span><span style=\"color:#B392F0\">afterCall</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">false</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">            panic</span><span style=\"color:#E1E4E8\">(r)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    err </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> fn</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    b.</span><span style=\"color:#B392F0\">afterCall</span><span style=\"color:#E1E4E8\">(err </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// beforeCall checks if the call should be allowed</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">b </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Breaker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">beforeCall</span><span style=\"color:#E1E4E8\">() (</span><span style=\"color:#B392F0\">State</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    b.mutex.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> b.mutex.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    now </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    switch</span><span style=\"color:#E1E4E8\"> b.state {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> StateClosed:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Reset failure count if failure window has passed</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> now.</span><span style=\"color:#B392F0\">Sub</span><span style=\"color:#E1E4E8\">(b.lastFailTime) </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> b.config.FailureWindow {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            b.failures </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> b.state, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> StateOpen:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Check if we should transition to half-open</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> now.</span><span style=\"color:#B392F0\">Sub</span><span style=\"color:#E1E4E8\">(b.lastFailTime) </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> b.config.OpenDuration {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            b.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> StateHalfOpen</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            b.halfOpenCalls </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> b.state, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> b.state, errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"circuit breaker is open\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> StateHalfOpen:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Limit concurrent calls in half-open state</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> b.halfOpenCalls </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> b.config.HalfOpenMaxCalls {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> b.state, errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"circuit breaker is half-open with max calls\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        b.halfOpenCalls</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> b.state, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    default</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> b.state, errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"unknown circuit breaker state\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// afterCall updates circuit breaker state based on call result</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">b </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Breaker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">afterCall</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">success</span><span style=\"color:#F97583\"> bool</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    b.mutex.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> b.mutex.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> success {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        switch</span><span style=\"color:#E1E4E8\"> b.state {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#E1E4E8\"> StateClosed:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            b.failures </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#E1E4E8\"> StateHalfOpen:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            b.failures </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            b.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> StateClosed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        b.failures</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        b.lastFailTime </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        switch</span><span style=\"color:#E1E4E8\"> b.state {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#E1E4E8\"> StateClosed:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> b.failures </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> b.config.MaxFailures {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                b.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> StateOpen</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#E1E4E8\"> StateHalfOpen:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            b.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> StateOpen</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// State returns the current circuit breaker state</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">b </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Breaker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">State</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">State</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    b.mutex.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> b.mutex.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> b.state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>WAL Recovery Manager</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// RecoveryManager handles system recovery after crashes and failures</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RecoveryManager</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    walPath     </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storagePath </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">zap</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// PerformRecovery executes the complete system recovery sequence</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">r </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RecoveryManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">PerformRecovery</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate WAL file integrity using checksums</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Identify the last committed checkpoint in the WAL</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Replay all WAL records since the last checkpoint</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Verify chunk consistency with replayed operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Rebuild in-memory state from recovered data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Mark recovery as complete in a new WAL checkpoint</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use WALRecord checksum validation to detect corruption</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Group WAL records by ChunkID for efficient replay</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ValidateSystemConsistency checks data integrity after recovery</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">r </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RecoveryManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ValidateSystemConsistency</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Verify all chunks referenced in the index exist on disk</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check that chunk headers match their content checksums</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Validate that index segments have correct chunk references</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Ensure timestamp ordering within and across chunks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Report any inconsistencies found during validation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use parallel goroutines to validate multiple chunks concurrently</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Graceful Degradation Controller</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// DegradationController manages feature flags and fallback behaviors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> DegradationController</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    features    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fallbacks   </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex       </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// EnableGracefulDegradation switches the system to degraded operation mode</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DegradationController</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">EnableGracefulDegradation</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">reason</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Disable non-essential features like bloom filters and advanced indexing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Switch query engine to sequential scan mode</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Increase buffer sizes and enable memory-only operation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Activate circuit breakers for failing external dependencies</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Send degradation alerts to operations team</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use atomic operations to update feature flags safely</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Log all degradation actions with correlation IDs for debugging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RestoreNormalOperation attempts to restore full system functionality</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DegradationController</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RestoreNormalOperation</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Test connectivity to previously failed dependencies</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Gradually re-enable features starting with lowest impact</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Monitor system stability during feature restoration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Rollback to degraded mode if instability is detected</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Clear degradation alerts when restoration is complete</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use exponential backoff when testing dependency recovery</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints\">Language-Specific Hints</h4>\n<p><strong>Go-Specific Error Handling Patterns</strong>:</p>\n<ul>\n<li>Use <code>context.WithTimeout</code> for all external operations to prevent hanging</li>\n<li>Implement error wrapping with <code>fmt.Errorf(&quot;context: %w&quot;, err)</code> for better error chains</li>\n<li>Use <code>sync.Once</code> for one-time initialization of recovery procedures</li>\n<li>Leverage <code>recover()</code> in goroutines to prevent panics from crashing the main process</li>\n<li>Use buffered channels for async error reporting: <code>errorChan := make(chan error, 100)</code></li>\n</ul>\n<p><strong>Resource Management</strong>:</p>\n<ul>\n<li>Always use <code>defer</code> for cleanup operations, even in error paths</li>\n<li>Implement proper file descriptor management with explicit <code>Close()</code> calls</li>\n<li>Use <code>sync.Pool</code> for frequently allocated objects during recovery operations</li>\n<li>Monitor goroutine counts with <code>runtime.NumGoroutine()</code> to detect leaks</li>\n</ul>\n<p><strong>Concurrency Safety</strong>:</p>\n<ul>\n<li>Use <code>sync.RWMutex</code> for read-heavy data structures like health check registries</li>\n<li>Implement proper context cancellation in long-running recovery operations</li>\n<li>Use atomic operations (<code>sync/atomic</code>) for counters and simple state flags</li>\n<li>Avoid shared mutable state in recovery procedures when possible</li>\n</ul>\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>After Milestone 1 (Log Ingestion)</strong>:</p>\n<ul>\n<li>Health checks should report ingestion endpoint status and buffer utilization</li>\n<li>Circuit breakers should protect against downstream storage failures</li>\n<li>WAL recovery should restore buffered logs after process crashes</li>\n<li>Test: Restart ingestion process during high load - no logs should be lost</li>\n</ul>\n<p><strong>After Milestone 2 (Log Index)</strong>:</p>\n<ul>\n<li>Index corruption detection should trigger automatic rebuilding</li>\n<li>Bloom filter failures should gracefully degrade to direct index lookups</li>\n<li>Index compaction should handle interruption and resume correctly</li>\n<li>Test: Corrupt an index file and verify automatic recovery</li>\n</ul>\n<p><strong>After Milestone 3 (Log Query Engine)</strong>:</p>\n<ul>\n<li>Query timeouts should prevent resource exhaustion</li>\n<li>Failed queries should not impact other concurrent queries</li>\n<li>Sequential scan fallback should work when indexes are unavailable</li>\n<li>Test: Execute expensive queries and verify system stability</li>\n</ul>\n<p><strong>After Milestone 4 (Log Storage &amp; Compression)</strong>:</p>\n<ul>\n<li>WAL replay should handle partial writes and corruption</li>\n<li>Retention policy failures should not block new ingestion</li>\n<li>Chunk compression errors should fall back to uncompressed storage</li>\n<li>Test: Kill storage process during chunk write and verify recovery</li>\n</ul>\n<p><strong>After Milestone 5 (Multi-Tenancy and Alerting)</strong>:</p>\n<ul>\n<li>Tenant isolation should be maintained even during component failures</li>\n<li>Rate limiting should protect against tenant resource exhaustion</li>\n<li>Alert delivery failures should not impact log processing</li>\n<li>Test: Simulate authentication service failure and verify graceful degradation</li>\n</ul>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnostic Steps</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>System hangs during startup</td>\n<td>WAL corruption or infinite recovery loop</td>\n<td>Check WAL file size and recent modifications</td>\n<td>Truncate WAL at last valid checkpoint</td>\n</tr>\n<tr>\n<td>Memory usage continuously increases</td>\n<td>Recovery process not releasing resources</td>\n<td>Monitor goroutine count and heap profile</td>\n<td>Add explicit cleanup in recovery procedures</td>\n</tr>\n<tr>\n<td>Health checks timeout</td>\n<td>Blocking operations in check implementation</td>\n<td>Add timeout contexts to all health checks</td>\n<td>Use separate goroutines for expensive checks</td>\n</tr>\n<tr>\n<td>Circuit breakers never close</td>\n<td>Failure threshold set too low</td>\n<td>Review failure rate metrics and adjust thresholds</td>\n<td>Increase failure threshold or reduce window size</td>\n</tr>\n<tr>\n<td>Alerts fire repeatedly</td>\n<td>Alert deduplication not working</td>\n<td>Check alert fingerprinting and time windows</td>\n<td>Fix alert grouping logic or increase deduplication window</td>\n</tr>\n<tr>\n<td>Recovery takes too long</td>\n<td>Processing records sequentially</td>\n<td>Profile WAL replay performance</td>\n<td>Parallelize recovery operations by chunk</td>\n</tr>\n</tbody></table>\n<h2 id=\"testing-strategy\">Testing Strategy</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section applies to all milestones (1-5), providing comprehensive testing approaches, verification procedures, and checkpoints that ensure system reliability and correctness throughout development.</p>\n</blockquote>\n<p>Testing a log aggregation system requires a multi-layered approach that validates both individual components and their interactions under realistic load conditions. Think of testing like quality control in a manufacturing assembly line - you need inspection points at each stage of production (unit tests), integration verification between stations (integration tests), and final product validation (end-to-end tests). Unlike simpler applications, log aggregation systems must handle massive data volumes, concurrent operations, and graceful degradation scenarios that are difficult to reproduce in development environments.</p>\n<p>The testing strategy follows a pyramid structure with comprehensive unit tests at the base, focused integration tests in the middle, and targeted end-to-end scenarios at the top. Each milestone introduces new components and capabilities that require specific verification approaches. The key challenge is testing distributed system behaviors like eventual consistency, partial failures, and performance characteristics that only emerge under load.</p>\n<h3 id=\"unit-testing-approach\">Unit Testing Approach</h3>\n<p><strong>Mental Model: The Component Laboratory</strong></p>\n<p>Think of unit testing like a laboratory where each component is isolated under controlled conditions to verify its behavior. Just as a scientist tests a chemical compound&#39;s properties in isolation before mixing it with other substances, we test each log aggregation component independently before integration. Each test is an experiment with known inputs and expected outputs, allowing us to identify exactly where problems occur.</p>\n<p>Unit tests for the log aggregation system focus on verifying the correctness of individual components without external dependencies. These tests should run quickly (under 1 second per test), be deterministic (same input always produces same output), and provide clear failure messages that pinpoint the exact problem.</p>\n<h4 id=\"core-component-testing-strategies\">Core Component Testing Strategies</h4>\n<p>The <code>LogEntry</code> and <code>Labels</code> types form the foundation of all log processing, requiring comprehensive validation testing. These tests verify data structure integrity, validation rules, and edge cases that could cause downstream failures.</p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Purpose</th>\n<th>Key Scenarios</th>\n<th>Validation Points</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Structure Validation</td>\n<td>Verify LogEntry creation and manipulation</td>\n<td>Valid timestamps, empty labels, Unicode messages</td>\n<td>Field assignment, deep equality, cloning</td>\n</tr>\n<tr>\n<td>Label Operations</td>\n<td>Test Labels map operations</td>\n<td>Merge conflicts, special characters, case sensitivity</td>\n<td>Hash consistency, canonicalization</td>\n</tr>\n<tr>\n<td>Serialization</td>\n<td>Ensure data survives encoding/decoding</td>\n<td>JSON round-trip, binary formats, corrupted data</td>\n<td>Data integrity, error handling</td>\n</tr>\n<tr>\n<td>Edge Cases</td>\n<td>Handle boundary conditions</td>\n<td>Maximum message size, extreme timestamps, nil values</td>\n<td>Graceful failure, error messages</td>\n</tr>\n</tbody></table>\n<h4 id=\"ingestion-component-testing\">Ingestion Component Testing</h4>\n<p>The ingestion pipeline components require focused testing on parsing accuracy, buffer management, and protocol handling. These tests simulate various input conditions without requiring actual network connections or file system access.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Test Focus</th>\n<th>Mock Dependencies</th>\n<th>Critical Properties</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>JSONParser</td>\n<td>Parse accuracy, error handling</td>\n<td>None (pure function)</td>\n<td>Structured field extraction, malformed JSON handling</td>\n</tr>\n<tr>\n<td>HTTPServer</td>\n<td>Request handling, validation</td>\n<td>HTTP test server, mock buffer</td>\n<td>Content-type validation, request size limits</td>\n</tr>\n<tr>\n<td>MemoryBuffer</td>\n<td>Concurrency, overflow behavior</td>\n<td>Mock metrics collector</td>\n<td>Thread safety, backpressure signaling</td>\n</tr>\n<tr>\n<td>TCPHandler</td>\n<td>Protocol parsing, connection mgmt</td>\n<td>Mock network connections</td>\n<td>Syslog format compliance, connection cleanup</td>\n</tr>\n</tbody></table>\n<p><strong>Architecture Decision: Mock vs. Real Dependencies in Unit Tests</strong></p>\n<blockquote>\n<p><strong>Decision: Use dependency injection with mock interfaces for external resources</strong></p>\n<ul>\n<li><strong>Context</strong>: Unit tests need to isolate components from file systems, networks, and other services while maintaining realistic behavior validation</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Test against real dependencies (files, networks)</li>\n<li>Mock all external interfaces</li>\n<li>Hybrid approach with configurable backends</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Mock external interfaces through dependency injection</li>\n<li><strong>Rationale</strong>: Real dependencies make tests slow and flaky due to timing, permissions, and environment differences. Mocks provide deterministic behavior and precise error injection for edge case testing.</li>\n<li><strong>Consequences</strong>: Requires designing components with injectable dependencies but enables fast, reliable tests that can simulate any failure scenario.</li>\n</ul>\n</blockquote>\n<p>The <code>JSONParser</code> testing exemplifies thorough unit testing by covering all parsing scenarios including valid JSON with various field combinations, malformed JSON with specific syntax errors, and edge cases like extremely large messages or unusual Unicode characters. Each test verifies both the happy path (successful parsing) and error paths (specific failure modes with appropriate error messages).</p>\n<h4 id=\"index-component-testing\">Index Component Testing</h4>\n<p>Index components require testing both correctness and performance characteristics since they directly impact query speed. These tests focus on data structure integrity, lookup accuracy, and memory usage patterns.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Testing Approach</th>\n<th>Performance Metrics</th>\n<th>Correctness Verification</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>BloomFilter</td>\n<td>False positive rate measurement</td>\n<td>Memory per element, lookup speed</td>\n<td>No false negatives, correct probability</td>\n</tr>\n<tr>\n<td>IndexSegment</td>\n<td>Term-to-document mapping</td>\n<td>Index build time, lookup latency</td>\n<td>Complete term coverage, accurate postings</td>\n</tr>\n<tr>\n<td>PostingsList</td>\n<td>Sorted order maintenance</td>\n<td>Merge performance, memory overhead</td>\n<td>No duplicate entries, correct ordering</td>\n</tr>\n</tbody></table>\n<p>Bloom filter testing requires statistical validation since the data structure provides probabilistic guarantees. Tests generate large random datasets, measure actual false positive rates, and verify they stay within configured bounds. The testing approach includes boundary analysis (empty filters, single elements, capacity limits) and performance verification under different load factors.</p>\n<p><strong>Critical Unit Test Pattern: Property-Based Testing</strong></p>\n<p>For components like bloom filters and indexes, property-based testing generates random inputs and verifies invariants hold across all cases. This approach catches edge cases that specific example-based tests might miss.</p>\n<blockquote>\n<p>Property-based tests generate hundreds of random inputs and verify fundamental properties like &quot;if we add an element to a bloom filter, MightContain must return true for that element&quot; or &quot;if we add a term to an index segment, LookupTerm must return a non-empty postings list&quot;.</p>\n</blockquote>\n<h4 id=\"query-engine-testing\">Query Engine Testing</h4>\n<p>Query engine components require testing both parsing accuracy and execution correctness. The <code>Lexer</code> and query parser need comprehensive testing across valid and invalid query syntax, while execution components need verification of result accuracy and resource consumption.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Input Variations</th>\n<th>Error Conditions</th>\n<th>Resource Limits</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Lexer</td>\n<td>All LogQL tokens, Unicode, special chars</td>\n<td>Invalid syntax, incomplete input</td>\n<td>Maximum query length</td>\n</tr>\n<tr>\n<td>QueryEngine</td>\n<td>Simple to complex queries</td>\n<td>Syntax errors, timeout limits</td>\n<td>Memory usage per query</td>\n</tr>\n<tr>\n<td>ResultStream</td>\n<td>Various result sizes</td>\n<td>Network failures, client disconnects</td>\n<td>Streaming vs. buffering</td>\n</tr>\n</tbody></table>\n<p>Query engine testing includes fuzzing approaches where random query strings are generated to identify parsing edge cases. These tests verify the parser correctly rejects invalid syntax while providing helpful error messages that indicate the specific problem location and suggested fixes.</p>\n<h4 id=\"storage-component-testing\">Storage Component Testing</h4>\n<p>Storage components require testing durability guarantees, compression effectiveness, and recovery procedures. These tests use temporary directories and mock file systems to verify behavior without affecting the development environment.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Durability Testing</th>\n<th>Performance Testing</th>\n<th>Recovery Testing</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>StorageEngine</td>\n<td>WAL persistence, chunk integrity</td>\n<td>Write throughput, compression ratio</td>\n<td>Crash recovery, corruption detection</td>\n</tr>\n<tr>\n<td>WALRecord</td>\n<td>Serialization accuracy</td>\n<td>Record overhead</td>\n<td>Partial write detection</td>\n</tr>\n<tr>\n<td>RetentionPolicy</td>\n<td>Policy evaluation</td>\n<td>Cleanup performance</td>\n<td>Policy change handling</td>\n</tr>\n</tbody></table>\n<p>WAL testing simulates crash scenarios by interrupting write operations at various points and verifying the recovery process correctly reconstructs system state. These tests use file system hooks to inject failures at specific byte offsets, ensuring the WAL handles partial writes and corruption correctly.</p>\n<h4 id=\"multi-tenancy-component-testing\">Multi-Tenancy Component Testing</h4>\n<p>Multi-tenancy components require testing security isolation, rate limiting accuracy, and alerting rule evaluation. These tests focus on preventing data leakage and ensuring tenant quotas are enforced correctly.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Security Testing</th>\n<th>Performance Testing</th>\n<th>Isolation Testing</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>AuthService</td>\n<td>Token validation, role checking</td>\n<td>Authentication latency</td>\n<td>Tenant data separation</td>\n</tr>\n<tr>\n<td>TokenBucket</td>\n<td>Rate limit enforcement</td>\n<td>Token allocation speed</td>\n<td>Per-tenant fairness</td>\n</tr>\n<tr>\n<td>AlertEngine</td>\n<td>Rule evaluation accuracy</td>\n<td>Alert processing throughput</td>\n<td>Cross-tenant alert isolation</td>\n</tr>\n</tbody></table>\n<p>Rate limiting testing requires time-based simulation where tests advance mock clocks to verify token bucket refill rates and burst handling. These tests ensure rate limits are enforced fairly across tenants while allowing legitimate burst traffic.</p>\n<h3 id=\"integration-testing\">Integration Testing</h3>\n<p><strong>Mental Model: The Orchestra Rehearsal</strong></p>\n<p>Integration testing is like an orchestra rehearsal where individual musicians (components) who have practiced their parts (passed unit tests) now play together to create harmonious music. The conductor (test harness) verifies that components synchronize correctly, handle timing variations gracefully, and recover when individual musicians make mistakes. Unlike unit tests that focus on individual performance, integration tests verify the ensemble creates the intended result.</p>\n<p>Integration tests validate component interactions, data flow correctness, and system behavior under realistic conditions. These tests use real implementations of all components but may mock external dependencies like networks or cloud services to maintain test reliability.</p>\n<h4 id=\"end-to-end-log-processing-flow\">End-to-End Log Processing Flow</h4>\n<p>The primary integration test validates the complete log processing pipeline from ingestion through query response. This test verifies that data transformations preserve accuracy while maintaining acceptable performance characteristics.</p>\n<table>\n<thead>\n<tr>\n<th>Test Stage</th>\n<th>Component Integration</th>\n<th>Data Validation</th>\n<th>Performance Baseline</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Ingestion</td>\n<td>HTTPServer → JSONParser → MemoryBuffer</td>\n<td>Log structure preservation</td>\n<td>1000 logs/second sustained</td>\n</tr>\n<tr>\n<td>Index Building</td>\n<td>MemoryBuffer → IndexSegment → BloomFilter</td>\n<td>Term extraction accuracy</td>\n<td>Index build under 100ms</td>\n</tr>\n<tr>\n<td>Storage Write</td>\n<td>IndexSegment → StorageEngine → WAL</td>\n<td>Durability guarantee</td>\n<td>Write latency under 10ms</td>\n</tr>\n<tr>\n<td>Query Execution</td>\n<td>QueryEngine → Index lookup → Storage read</td>\n<td>Result completeness</td>\n<td>Query response under 1s</td>\n</tr>\n</tbody></table>\n<p>The integration test pipeline processes a realistic log dataset with various formats, label cardinalities, and message sizes. Each stage verifies data integrity while measuring throughput and latency to establish performance baselines for regression detection.</p>\n<p><strong>Architecture Decision: Test Data Management Strategy</strong></p>\n<blockquote>\n<p><strong>Decision: Use deterministic test datasets with controlled characteristics</strong></p>\n<ul>\n<li><strong>Context</strong>: Integration tests need realistic data that exercises edge cases while producing reproducible results across different environments</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Random data generation</li>\n<li>Production data snapshots</li>\n<li>Curated synthetic datasets</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Curated synthetic datasets with configurable properties</li>\n<li><strong>Rationale</strong>: Random data makes failures hard to reproduce. Production data contains sensitive information and varies over time. Synthetic datasets provide controlled label cardinality, message patterns, and edge cases while remaining deterministic.</li>\n<li><strong>Consequences</strong>: Requires maintaining test data generators but enables precise failure reproduction and comprehensive edge case coverage.</li>\n</ul>\n</blockquote>\n<h4 id=\"cross-protocol-ingestion-testing\">Cross-Protocol Ingestion Testing</h4>\n<p>Integration testing validates that logs ingested via different protocols (HTTP, TCP, UDP) are processed identically and maintain consistent ordering and labeling. This test ensures protocol-specific parsing differences don&#39;t affect downstream processing.</p>\n<p>The test setup establishes all three ingestion endpoints simultaneously, sends identical log content via each protocol, and verifies the resulting <code>LogEntry</code> structures are equivalent. Special attention is paid to timestamp handling, label extraction, and message formatting differences between protocols.</p>\n<table>\n<thead>\n<tr>\n<th>Protocol</th>\n<th>Message Format</th>\n<th>Timing Behavior</th>\n<th>Error Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP</td>\n<td>JSON batch requests</td>\n<td>Synchronous acknowledgment</td>\n<td>HTTP status codes</td>\n</tr>\n<tr>\n<td>TCP</td>\n<td>Syslog RFC 5424/3164</td>\n<td>Connection-based streaming</td>\n<td>Connection termination</td>\n</tr>\n<tr>\n<td>UDP</td>\n<td>Syslog datagrams</td>\n<td>Fire-and-forget delivery</td>\n<td>Silent packet loss</td>\n</tr>\n</tbody></table>\n<p>The cross-protocol test includes failure scenarios like network interruptions, malformed messages, and protocol violations to verify each ingestion path handles errors appropriately without affecting other protocols.</p>\n<h4 id=\"index-and-query-consistency-testing\">Index and Query Consistency Testing</h4>\n<p>This integration test verifies that indexed log data produces correct query results across different query patterns and time ranges. The test builds indexes from known log datasets and validates that query results match expected outputs exactly.</p>\n<table>\n<thead>\n<tr>\n<th>Query Type</th>\n<th>Index Utilization</th>\n<th>Expected Behavior</th>\n<th>Performance Target</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Label selectors</td>\n<td>Inverted index lookup</td>\n<td>Exact label matches only</td>\n<td>Sub-millisecond index scan</td>\n</tr>\n<tr>\n<td>Text search</td>\n<td>Full-text index + bloom</td>\n<td>All matching log entries</td>\n<td>Linear scan performance</td>\n</tr>\n<tr>\n<td>Time range</td>\n<td>Partition pruning</td>\n<td>Only relevant time windows</td>\n<td>Partition skip optimization</td>\n</tr>\n<tr>\n<td>Combined filters</td>\n<td>Multi-stage execution</td>\n<td>AND/OR logic correctness</td>\n<td>Filter order optimization</td>\n</tr>\n</tbody></table>\n<p>The consistency test includes edge cases like empty result sets, large result sets that exceed memory limits, and concurrent queries that access the same index partitions. Each scenario verifies result accuracy while measuring resource consumption.</p>\n<h4 id=\"storage-and-recovery-integration-testing\">Storage and Recovery Integration Testing</h4>\n<p>Recovery integration testing simulates various failure scenarios during log ingestion and verifies the system recovers to a consistent state without data loss. These tests interrupt processing at different stages and validate WAL replay functionality.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Scenario</th>\n<th>System State</th>\n<th>Recovery Action</th>\n<th>Validation Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Crash during ingestion</td>\n<td>WAL contains uncommitted entries</td>\n<td>Replay WAL records</td>\n<td>Count matches input logs</td>\n</tr>\n<tr>\n<td>Corruption in chunk</td>\n<td>Index points to bad data</td>\n<td>Mark chunk unavailable</td>\n<td>Queries skip corrupted data</td>\n</tr>\n<tr>\n<td>Disk full during write</td>\n<td>Partial chunk write</td>\n<td>Rollback to last checkpoint</td>\n<td>No partial entries visible</td>\n</tr>\n<tr>\n<td>Index corruption</td>\n<td>Query results inconsistent</td>\n<td>Rebuild index from chunks</td>\n<td>Results match original data</td>\n</tr>\n</tbody></table>\n<p>Recovery testing uses controlled failure injection where specific operations are interrupted at deterministic points. The test framework provides hooks to simulate disk failures, process crashes, and data corruption while maintaining the ability to verify recovery correctness.</p>\n<h4 id=\"multi-tenant-integration-testing\">Multi-Tenant Integration Testing</h4>\n<p>Multi-tenant integration testing verifies tenant isolation works correctly across all system components from ingestion through querying. These tests ensure tenant data and resources remain separated while validating rate limiting and quota enforcement.</p>\n<p>The test creates multiple tenant contexts with different quotas and access patterns, ingests logs for each tenant simultaneously, and verifies queries only return data for the authenticated tenant. Rate limiting validation ensures tenant quotas are enforced without affecting other tenants.</p>\n<table>\n<thead>\n<tr>\n<th>Isolation Aspect</th>\n<th>Testing Approach</th>\n<th>Verification Method</th>\n<th>Security Validation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data separation</td>\n<td>Cross-tenant queries</td>\n<td>Zero results returned</td>\n<td>Authentication bypass attempts</td>\n</tr>\n<tr>\n<td>Resource isolation</td>\n<td>Concurrent load testing</td>\n<td>Per-tenant metrics</td>\n<td>Quota enforcement accuracy</td>\n</tr>\n<tr>\n<td>Rate limiting</td>\n<td>Burst traffic simulation</td>\n<td>Throttling behavior</td>\n<td>Limit circumvention attempts</td>\n</tr>\n<tr>\n<td>Alert isolation</td>\n<td>Rule evaluation testing</td>\n<td>Tenant-specific notifications</td>\n<td>Cross-tenant alert leakage</td>\n</tr>\n</tbody></table>\n<h3 id=\"milestone-checkpoints\">Milestone Checkpoints</h3>\n<p><strong>Mental Model: The Progressive Assessment System</strong></p>\n<p>Milestone checkpoints are like a progressive assessment system where each test validates that foundational capabilities work correctly before building additional complexity. Like a driving test that verifies basic skills before allowing highway driving, each checkpoint ensures the implemented functionality is solid before adding new components that depend on it.</p>\n<p>Each milestone checkpoint includes functional verification (does it work?), performance validation (does it meet requirements?), and robustness testing (does it handle errors gracefully?). The checkpoints provide clear success criteria and diagnostic guidance when problems occur.</p>\n<h4 id=\"milestone-1-log-ingestion-checkpoint\">Milestone 1: Log Ingestion Checkpoint</h4>\n<p>After completing the log ingestion implementation, the system should successfully receive logs via all three protocols, parse them into structured <code>LogEntry</code> objects, and buffer them reliably without data loss.</p>\n<p><strong>Functional Verification Checklist:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Feature</th>\n<th>Test Command</th>\n<th>Expected Behavior</th>\n<th>Success Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Ingestion</td>\n<td><code>curl -X POST -H &quot;Content-Type: application/json&quot; -d &#39;{&quot;timestamp&quot;:&quot;2023-10-01T12:00:00Z&quot;,&quot;message&quot;:&quot;test log&quot;,&quot;labels&quot;:{&quot;level&quot;:&quot;info&quot;}}&#39; http://localhost:8080/api/v1/logs</code></td>\n<td>HTTP 200 response, log in buffer</td>\n<td>Response contains log ID</td>\n</tr>\n<tr>\n<td>TCP Syslog</td>\n<td><code>echo &#39;&lt;134&gt;Oct 1 12:00:00 host app: test message&#39; | nc localhost 1514</code></td>\n<td>Connection accepted, log parsed</td>\n<td>Log appears with correct fields</td>\n</tr>\n<tr>\n<td>UDP Syslog</td>\n<td><code>echo &#39;&lt;134&gt;Oct 1 12:00:00 host app: test message&#39; | nc -u localhost 1514</code></td>\n<td>Datagram processed, log buffered</td>\n<td>No connection errors</td>\n</tr>\n<tr>\n<td>File Tail</td>\n<td>Create log file, append lines</td>\n<td>New lines detected and ingested</td>\n<td>Tail follows file correctly</td>\n</tr>\n</tbody></table>\n<p><strong>Performance Validation:</strong></p>\n<p>The ingestion system should handle sustained load without dropping messages or excessive memory consumption. Run a load test that sends 10,000 log messages per second for 60 seconds and verify all messages are processed successfully.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Load test command example</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/load-test/main.go</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --target</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/v1/logs</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --rate</span><span style=\"color:#79B8FF\"> 10000</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --duration</span><span style=\"color:#9ECBFF\"> 60s</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --protocol</span><span style=\"color:#9ECBFF\"> http</span></span></code></pre></div>\n\n<p><strong>Expected Performance Metrics:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Metric</th>\n<th>Target Value</th>\n<th>Measurement Method</th>\n<th>Failure Threshold</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Ingestion Rate</td>\n<td>10,000 logs/sec</td>\n<td>Logs processed / elapsed time</td>\n<td>&lt; 9,500 logs/sec</td>\n</tr>\n<tr>\n<td>Memory Usage</td>\n<td>&lt; 500MB RSS</td>\n<td>Process memory monitoring</td>\n<td>&gt; 1GB RSS</td>\n</tr>\n<tr>\n<td>Buffer Utilization</td>\n<td>&lt; 80% capacity</td>\n<td>Buffer fill percentage</td>\n<td>&gt; 95% capacity</td>\n</tr>\n<tr>\n<td>Error Rate</td>\n<td>&lt; 0.1%</td>\n<td>Failed requests / total requests</td>\n<td>&gt; 1% error rate</td>\n</tr>\n</tbody></table>\n<p><strong>Troubleshooting Common Issues:</strong></p>\n<p>⚠️ <strong>Pitfall: Port Conflicts and Binding Errors</strong>\nIf the server fails to start with &quot;address already in use&quot; errors, verify no other processes are using ports 8080 (HTTP) or 1514 (TCP/UDP). Use <code>netstat -an | grep LISTEN</code> to check port usage and <code>pkill</code> to terminate conflicting processes.</p>\n<p>⚠️ <strong>Pitfall: Parser Fails on Real Syslog Data</strong>\nTest parsers often work on hand-crafted examples but fail on real syslog messages with optional fields or timezone variations. Capture actual syslog traffic using <code>tcpdump</code> and test parser against real data to identify format edge cases.</p>\n<h4 id=\"milestone-2-index-building-checkpoint\">Milestone 2: Index Building Checkpoint</h4>\n<p>After implementing the indexing engine, the system should build accurate inverted indexes from ingested logs and support efficient term lookups with bloom filter optimization.</p>\n<p><strong>Index Construction Verification:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Test Procedure</th>\n<th>Validation Criteria</th>\n<th>Performance Target</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Term Extraction</td>\n<td>Ingest logs with known terms</td>\n<td>All terms appear in index</td>\n<td>100% term coverage</td>\n</tr>\n<tr>\n<td>Inverted Index</td>\n<td>Query for indexed terms</td>\n<td>Postings lists return correct entries</td>\n<td>Zero false negatives</td>\n</tr>\n<tr>\n<td>Bloom Filter</td>\n<td>Test negative lookups</td>\n<td>False positive rate within bounds</td>\n<td>&lt; 1% false positive rate</td>\n</tr>\n<tr>\n<td>Partitioning</td>\n<td>Span multiple time windows</td>\n<td>Queries scan relevant partitions only</td>\n<td>Time-based partition pruning</td>\n</tr>\n</tbody></table>\n<p><strong>Index Quality Validation:</strong></p>\n<p>Build an index from a dataset with known characteristics and verify the index structure matches expectations:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Index verification command example</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/verify-index/main.go</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --data-path</span><span style=\"color:#9ECBFF\"> ./testdata/logs.json</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --index-path</span><span style=\"color:#9ECBFF\"> ./data/index</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --verify-completeness</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --verify-bloom-filter</span></span></code></pre></div>\n\n<p>The verification tool should report index statistics including term count, partition distribution, and bloom filter characteristics. All validation checks should pass without errors.</p>\n<p><strong>Index Performance Testing:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Operation</th>\n<th>Performance Target</th>\n<th>Test Method</th>\n<th>Baseline Measurement</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Index Build</td>\n<td>1MB logs in &lt; 5 seconds</td>\n<td>Time index construction</td>\n<td>Record actual timing</td>\n</tr>\n<tr>\n<td>Term Lookup</td>\n<td>&lt; 1ms average latency</td>\n<td>Query common terms</td>\n<td>Measure lookup distribution</td>\n</tr>\n<tr>\n<td>Bloom Filter Check</td>\n<td>&lt; 100μs per operation</td>\n<td>Test filter membership</td>\n<td>Profile filter performance</td>\n</tr>\n<tr>\n<td>Index Size</td>\n<td>&lt; 10% of raw log size</td>\n<td>Compare index to source data</td>\n<td>Calculate compression ratio</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: Bloom Filter Parameter Misconfiguration</strong>\nBloom filters with incorrect parameters either waste memory (too conservative) or produce excessive false positives (too aggressive). The <code>NewBloomParams</code> function calculates optimal parameters, but verify false positive rates match theoretical expectations through statistical testing.</p>\n<h4 id=\"milestone-3-query-engine-checkpoint\">Milestone 3: Query Engine Checkpoint</h4>\n<p>After implementing the query engine, the system should parse LogQL queries correctly, execute them efficiently against the index, and return accurate results with proper pagination.</p>\n<p><strong>Query Language Verification:</strong></p>\n<p>Test the query parser against a comprehensive set of LogQL syntax patterns to verify parsing accuracy and error handling:</p>\n<table>\n<thead>\n<tr>\n<th>Query Pattern</th>\n<th>Example Query</th>\n<th>Expected Behavior</th>\n<th>Parser Validation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Label selectors</td>\n<td><code>{service=&quot;api&quot;, level=&quot;error&quot;}</code></td>\n<td>Parse to label filter AST</td>\n<td>Correct field extraction</td>\n</tr>\n<tr>\n<td>Text search</td>\n<td><code>&quot;database connection failed&quot;</code></td>\n<td>Parse to text filter AST</td>\n<td>Proper quote handling</td>\n</tr>\n<tr>\n<td>Regex patterns</td>\n<td><code>|~ &quot;error.*timeout&quot;</code></td>\n<td>Parse to regex filter AST</td>\n<td>Valid regex compilation</td>\n</tr>\n<tr>\n<td>Combined filters</td>\n<td><code>{service=&quot;api&quot;} |= &quot;error&quot; |~ &quot;timeout&quot;</code></td>\n<td>Parse to pipeline AST</td>\n<td>Correct precedence</td>\n</tr>\n</tbody></table>\n<p><strong>Query Execution Validation:</strong></p>\n<p>Execute queries against indexed test data and verify results match expected outputs exactly:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Query testing command example</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/test-queries/main.go</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --index-path</span><span style=\"color:#9ECBFF\"> ./data/index</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --queries</span><span style=\"color:#9ECBFF\"> ./testdata/test-queries.json</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --expected-results</span><span style=\"color:#9ECBFF\"> ./testdata/expected-results.json</span></span></code></pre></div>\n\n<p>The test suite should execute each query and compare results against expected outputs, reporting any discrepancies in result content, ordering, or metadata.</p>\n<p><strong>Query Performance Testing:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Query Type</th>\n<th>Performance Target</th>\n<th>Test Dataset</th>\n<th>Success Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Simple label filter</td>\n<td>&lt; 100ms response</td>\n<td>1M log entries</td>\n<td>95th percentile under target</td>\n</tr>\n<tr>\n<td>Text search</td>\n<td>&lt; 500ms response</td>\n<td>1M log entries</td>\n<td>Index utilization confirmed</td>\n</tr>\n<tr>\n<td>Complex combined filter</td>\n<td>&lt; 1s response</td>\n<td>1M log entries</td>\n<td>Optimization applied correctly</td>\n</tr>\n<tr>\n<td>Large result set</td>\n<td>Streaming response</td>\n<td>100K matching entries</td>\n<td>Memory usage bounded</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: Query Result Pagination Edge Cases</strong>\nPagination cursors can become invalid when underlying data changes during query execution. Test pagination with concurrent ingestion to verify cursor stability and implement proper error handling for invalid cursor scenarios.</p>\n<h4 id=\"milestone-4-storage-and-compression-checkpoint\">Milestone 4: Storage and Compression Checkpoint</h4>\n<p>After implementing the storage engine, the system should persist log data reliably with compression, maintain WAL durability guarantees, and enforce retention policies correctly.</p>\n<p><strong>Storage Durability Verification:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Test Scenario</th>\n<th>Procedure</th>\n<th>Validation Method</th>\n<th>Recovery Expectation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Normal shutdown</td>\n<td>Stop process gracefully</td>\n<td>Restart and verify data</td>\n<td>All committed data present</td>\n</tr>\n<tr>\n<td>Crash simulation</td>\n<td>Kill process during write</td>\n<td>WAL replay on startup</td>\n<td>No data loss from WAL</td>\n</tr>\n<tr>\n<td>Disk full scenario</td>\n<td>Fill storage during write</td>\n<td>Monitor error handling</td>\n<td>Graceful degradation</td>\n</tr>\n<tr>\n<td>Corruption detection</td>\n<td>Corrupt chunk file</td>\n<td>Attempt to read data</td>\n<td>Error detection and reporting</td>\n</tr>\n</tbody></table>\n<p>Test storage durability by ingesting logs, terminating the process at various points, and verifying recovery restores the correct system state:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Durability testing command example</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/test-durability/main.go</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --ingest-count</span><span style=\"color:#79B8FF\"> 10000</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --crash-after</span><span style=\"color:#79B8FF\"> 5000</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --verify-recovery</span></span></code></pre></div>\n\n<p><strong>Compression Effectiveness Testing:</strong></p>\n<p>Measure compression performance across different algorithms and log patterns to verify optimal compression selection:</p>\n<table>\n<thead>\n<tr>\n<th>Log Type</th>\n<th>Uncompressed Size</th>\n<th>Compressed Size (gzip)</th>\n<th>Compressed Size (zstd)</th>\n<th>Compression Ratio</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>JSON structured logs</td>\n<td>100MB</td>\n<td>15MB</td>\n<td>12MB</td>\n<td>85-88% reduction</td>\n</tr>\n<tr>\n<td>Plain text logs</td>\n<td>100MB</td>\n<td>25MB</td>\n<td>20MB</td>\n<td>75-80% reduction</td>\n</tr>\n<tr>\n<td>Mixed format logs</td>\n<td>100MB</td>\n<td>20MB</td>\n<td>16MB</td>\n<td>80-84% reduction</td>\n</tr>\n</tbody></table>\n<p><strong>Retention Policy Testing:</strong></p>\n<p>Configure retention policies with short time windows and verify expired data is cleaned up correctly:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Retention testing command example</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/test-retention/main.go</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --retention-age</span><span style=\"color:#9ECBFF\"> 1h</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --ingest-duration</span><span style=\"color:#9ECBFF\"> 2h</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --verify-cleanup</span></span></code></pre></div>\n\n<p>The test should verify that data older than the retention window is deleted while recent data remains accessible.</p>\n<p>⚠️ <strong>Pitfall: WAL Growth Without Rotation</strong>\nWAL files that grow unbounded eventually exhaust disk space. Test WAL rotation under sustained load and verify old WAL segments are cleaned up after successful checkpointing. Monitor WAL file count and total size during extended operation.</p>\n<h4 id=\"milestone-5-multi-tenancy-and-alerting-checkpoint\">Milestone 5: Multi-Tenancy and Alerting Checkpoint</h4>\n<p>After implementing multi-tenancy and alerting, the system should enforce tenant isolation completely, respect rate limits accurately, and trigger alerts based on log patterns reliably.</p>\n<p><strong>Tenant Isolation Verification:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Isolation Test</th>\n<th>Test Procedure</th>\n<th>Expected Behavior</th>\n<th>Security Validation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data separation</td>\n<td>Query with different tenant tokens</td>\n<td>Only tenant-specific data returned</td>\n<td>Zero cross-tenant results</td>\n</tr>\n<tr>\n<td>Rate limit isolation</td>\n<td>Exceed limits for one tenant</td>\n<td>Other tenants unaffected</td>\n<td>Independent quota enforcement</td>\n</tr>\n<tr>\n<td>Alert isolation</td>\n<td>Trigger alerts for one tenant</td>\n<td>Alerts only sent to correct tenant</td>\n<td>No notification leakage</td>\n</tr>\n</tbody></table>\n<p>Test tenant isolation by creating multiple tenant contexts, ingesting data for each tenant, and verifying queries with different tenant authentication tokens only return appropriate data:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Multi-tenancy testing command example</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/test-tenancy/main.go</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --tenant-count</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --logs-per-tenant</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --verify-isolation</span></span></code></pre></div>\n\n<p><strong>Rate Limiting Accuracy Testing:</strong></p>\n<p>Configure rate limits with measurable thresholds and verify enforcement accuracy under various load patterns:</p>\n<table>\n<thead>\n<tr>\n<th>Rate Limit Type</th>\n<th>Configured Limit</th>\n<th>Test Load</th>\n<th>Expected Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Per-tenant ingestion</td>\n<td>1000 logs/min</td>\n<td>1500 logs/min</td>\n<td>500 logs rejected</td>\n</tr>\n<tr>\n<td>Per-stream ingestion</td>\n<td>100 logs/min</td>\n<td>150 logs/min</td>\n<td>50 logs rejected</td>\n</tr>\n<tr>\n<td>Query rate</td>\n<td>10 queries/min</td>\n<td>15 queries/min</td>\n<td>5 queries throttled</td>\n</tr>\n</tbody></table>\n<p><strong>Alert Engine Verification:</strong></p>\n<p>Configure alert rules with known trigger conditions and verify alerts fire correctly without false positives or negatives:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Alert testing command example  </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/test-alerts/main.go</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --rule-config</span><span style=\"color:#9ECBFF\"> ./testdata/alert-rules.yaml</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --trigger-logs</span><span style=\"color:#9ECBFF\"> ./testdata/trigger-logs.json</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --verify-notifications</span></span></code></pre></div>\n\n<p>The alert test should verify alert deduplication prevents notification spam while ensuring critical alerts are delivered reliably.</p>\n<p>⚠️ <strong>Pitfall: Token Bucket Rate Limiting Clock Skew</strong>\nRate limiting accuracy depends on consistent time measurement. Test rate limiting behavior during system clock adjustments and verify token bucket algorithms handle time skew gracefully without allowing unlimited bursts or permanent blocking.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Test Framework</td>\n<td>Go testing package + testify</td>\n<td>Ginkgo + Gomega</td>\n<td>Standard library sufficient for most cases</td>\n</tr>\n<tr>\n<td>Mock Generation</td>\n<td>Manual mocks</td>\n<td>gomock or counterfeiter</td>\n<td>Manual mocks for learning, generated for production</td>\n</tr>\n<tr>\n<td>Test Data</td>\n<td>JSON files</td>\n<td>Property-based testing with gopter</td>\n<td>Static data for deterministic results</td>\n</tr>\n<tr>\n<td>Integration Testing</td>\n<td>Docker Compose</td>\n<td>Kubernetes test environments</td>\n<td>Docker sufficient for component integration</td>\n</tr>\n<tr>\n<td>Load Testing</td>\n<td>Custom Go programs</td>\n<td>k6 or Artillery</td>\n<td>Custom tools provide precise control</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Log output + manual verification</td>\n<td>Prometheus metrics + Grafana</td>\n<td>Start simple, add observability later</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-test-structure\">Recommended Test Structure</h4>\n<p>Organize test code to support both component-level and integration testing while maintaining clear separation between test types and shared utilities:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  cmd/\n    test-ingestion/         ← milestone verification tools\n      main.go\n    test-index/\n      main.go\n    load-test/              ← performance testing tools\n      main.go\n  internal/\n    ingestion/\n      ingestion.go\n      ingestion_test.go     ← unit tests\n    index/\n      index.go\n      index_test.go\n  test/\n    integration/            ← integration test suites\n      ingestion_test.go\n      query_test.go\n      storage_test.go\n    testdata/               ← shared test datasets\n      sample-logs.json\n      test-queries.json\n    fixtures/               ← test utilities and mocks\n      mock_storage.go\n      test_helpers.go</code></pre></div>\n\n<h4 id=\"unit-test-infrastructure\">Unit Test Infrastructure</h4>\n<p><strong>Complete Mock Implementations:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// test/fixtures/mock_storage.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> fixtures</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MockStorageEngine provides in-memory storage for testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MockStorageEngine</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    chunks </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ChunkHeader</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    data   </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">byte</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    wal    []</span><span style=\"color:#B392F0\">WALRecord</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex  </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewMockStorageEngine</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockStorageEngine</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">MockStorageEngine</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        chunks: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ChunkHeader</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        data:   </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        wal:    </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#B392F0\">WALRecord</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockStorageEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">WriteLogBatch</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">entries</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Implement mock batch writing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Generate chunk ID from timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Compress entries into chunk data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Store chunk header and data in maps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Append WAL record for durability simulation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockStorageEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ReadChunk</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">chunkID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Retrieve chunk data from storage map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Decompress chunk data to log entries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Return error if chunk not found</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MockMemoryBuffer provides ring buffer simulation for testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MockMemoryBuffer</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    entries []</span><span style=\"color:#B392F0\">LogEntry</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    head    </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tail    </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    size    </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex   </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Mutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewMockMemoryBuffer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">capacity</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockMemoryBuffer</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">MockMemoryBuffer</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        entries: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#E1E4E8\">, capacity),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        size:    capacity,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockMemoryBuffer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">entry</span><span style=\"color:#B392F0\"> LogEntry</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Check if buffer is full</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Add entry at tail position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Update tail pointer with wrap-around</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Return error if buffer overflow</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockMemoryBuffer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Read</span><span style=\"color:#E1E4E8\">() (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Check if buffer is empty</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Get entry from head position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Update head pointer with wrap-around</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Return error if buffer underflow</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Test Data Generation Utilities:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// test/fixtures/test_helpers.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> fixtures</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">math/rand</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GenerateTestLogs creates deterministic log entries for testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> GenerateTestLogs</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">count</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">startTime</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">) []</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Create slice of LogEntry with specified count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Generate entries with incrementing timestamps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Include variety of log levels and services</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Add both structured and unstructured messages</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Ensure deterministic output with fixed random seed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CreateTestLabels generates label combinations with controlled cardinality</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> CreateTestLabels</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">serviceName</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">level</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">extraLabels</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Create base labels with service and level</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Add any additional labels from extraLabels map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Ensure labels pass validation rules</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadTestDataFromFile reads log entries from JSON test data files</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> LoadTestDataFromFile</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">filename</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Read JSON file from test/testdata directory</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Unmarshal JSON into LogEntry slice</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Validate all entries have required fields</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Return descriptive error for malformed data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AssertLogEntryEqual compares two LogEntry instances with detailed diff</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> AssertLogEntryEqual</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">expected</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">actual</span><span style=\"color:#B392F0\"> LogEntry</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Compare timestamps with tolerance for serialization precision</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Compare labels maps with sorted iteration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Compare message content exactly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Provide detailed diff message on mismatch</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"component-test-skeleton\">Component Test Skeleton</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/ingestion/ingestion_test.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> ingestion</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/stretchr/testify/assert</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/stretchr/testify/require</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">your-project/test/fixtures</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestJSONParser_ParseValidLog</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Create JSONParser instance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Prepare valid JSON log data with all fields</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Call Parse method with test data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Assert LogEntry fields match expected values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Verify timestamp parsing handles timezones correctly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Verify labels map contains all expected key-value pairs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestJSONParser_ParseMalformedJSON</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Create JSONParser instance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Prepare various malformed JSON strings (missing quotes, trailing commas, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Call Parse method with each malformed input</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Assert Parse returns appropriate error for each case</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Verify error messages help identify specific syntax problems</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestMemoryBuffer_ConcurrentAccess</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Create MemoryBuffer with known capacity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Start multiple goroutines writing entries concurrently</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Start multiple goroutines reading entries concurrently</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Verify no data races using go test -race</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Verify total entries written equals total entries read</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Assert buffer state remains consistent throughout test</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestBloomFilter_FalsePositiveRate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Create BloomFilter with specific false positive rate (e.g., 1%)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Add large number of known elements (e.g., 10,000 random strings)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Test large number of elements NOT in filter (e.g., 100,000 different strings)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Count false positives where MightContain returns true for non-member</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Assert measured false positive rate is within theoretical bounds</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestIndexSegment_TermLookupAccuracy</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Create IndexSegment with known capacity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Add log entries with predictable term distribution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Build inverted index from test entries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Query for each known term and verify postings list accuracy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Query for terms not in logs and verify empty results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Verify postings lists maintain correct sort order</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"integration-test-framework\">Integration Test Framework</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// test/integration/end_to_end_test.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> integration</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestCompleteLogProcessingPipeline</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Start all system components (HTTP server, index builder, storage)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Ingest test logs via HTTP endpoint</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Wait for logs to be indexed and stored</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Execute queries against indexed data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Verify query results match expected outputs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Verify logs are persisted correctly in storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Shutdown components gracefully</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestCrossProtocolIngestionConsistency</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Start HTTP, TCP, and UDP ingestion endpoints</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Send identical log content via each protocol</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Verify all protocols produce equivalent LogEntry structures</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Check that timestamps, labels, and messages are preserved accurately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Verify logs from different protocols can be queried together</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestSystemRecoveryAfterCrash</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Start system and ingest logs to establish baseline state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Simulate crash by forcibly terminating components</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Restart components and trigger WAL recovery</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Verify all committed data is restored correctly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Verify queries return same results as before crash</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Verify system can continue ingesting new logs normally</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-verification-tools\">Milestone Verification Tools</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// cmd/test-ingestion/main.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> main</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">flag</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">log</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> main</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        httpPort </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> flag.</span><span style=\"color:#B392F0\">Int</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"http-port\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">8080</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"HTTP ingestion port\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tcpPort  </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> flag.</span><span style=\"color:#B392F0\">Int</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"tcp-port\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1514</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"TCP syslog port\"</span><span style=\"color:#E1E4E8\">) </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        udpPort  </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> flag.</span><span style=\"color:#B392F0\">Int</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"udp-port\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1514</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"UDP syslog port\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        testDuration </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> flag.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"duration\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">time.Second, </span><span style=\"color:#9ECBFF\">\"Test duration\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logsPerSecond </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> flag.</span><span style=\"color:#B392F0\">Int</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"rate\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Logs per second to generate\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    flag.</span><span style=\"color:#B392F0\">Parse</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Test HTTP endpoint by sending JSON log batches</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Test TCP endpoint by sending RFC 5424 syslog messages</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Test UDP endpoint by sending RFC 3164 syslog messages</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Measure ingestion rates for each protocol</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Verify no logs are dropped during sustained load</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Report success/failure for each protocol test</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fmt.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Milestone 1 verification completed</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// cmd/test-queries/main.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> main</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">flag</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">log</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> QueryTestCase</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Name           </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"name\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Query          </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"query\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ExpectedCount  </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">    `json:\"expected_count\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ExpectedSample </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"expected_sample\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> main</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        queryFile </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> flag.</span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"queries\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"test/testdata/test-queries.json\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Query test cases\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        indexPath </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> flag.</span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"index\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"data/index\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Index directory\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    flag.</span><span style=\"color:#B392F0\">Parse</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Load query test cases from JSON file</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Initialize query engine with index path</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Execute each test query and measure response time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Compare results against expected outputs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Report any mismatches with detailed diff information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Calculate overall pass/fail statistics</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fmt.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Milestone 3 verification completed</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"language-specific-testing-hints\">Language-Specific Testing Hints</h4>\n<p><strong>Go Testing Best Practices:</strong></p>\n<ul>\n<li>Use <code>go test -race</code> to detect concurrent access issues in buffer and index components</li>\n<li>Use <code>go test -cover</code> to measure test coverage and identify untested code paths</li>\n<li>Use <code>testing.Short()</code> to skip long-running tests during development: <code>if testing.Short() { t.Skip() }</code></li>\n<li>Use <code>t.Cleanup()</code> for test resource cleanup instead of defer when setup can fail</li>\n<li>Use <code>require</code> package for assertions that should stop test execution, <code>assert</code> for continued validation</li>\n</ul>\n<p><strong>Performance Testing Techniques:</strong></p>\n<ul>\n<li>Use <code>testing.B</code> benchmarks to measure component performance: <code>go test -bench=. -benchmem</code></li>\n<li>Use <code>pprof</code> to identify performance bottlenecks: <code>go test -cpuprofile=cpu.prof</code></li>\n<li>Create memory usage baselines with <code>runtime.MemStats</code> before and after operations</li>\n<li>Test with realistic data sizes (MB chunks, thousands of labels) not toy examples</li>\n</ul>\n<p><strong>Mock and Stub Patterns:</strong></p>\n<ul>\n<li>Implement interfaces for all external dependencies (file system, network, time) to enable mocking</li>\n<li>Use dependency injection in constructors: <code>NewHTTPServer(parser Parser, buffer Buffer)</code></li>\n<li>Create test doubles that simulate specific failure conditions: <code>MockStorageEngine.SetError(error)</code></li>\n<li>Verify mock interactions with call counts and parameter validation</li>\n</ul>\n<p><strong>Debugging Test Failures:</strong></p>\n<ul>\n<li>Log actual vs expected values in assertion messages: <code>assert.Equal(t, expected, actual, &quot;term lookup failed for %s&quot;, term)</code></li>\n<li>Use <code>t.Logf()</code> to add debug output that only appears on test failure</li>\n<li>Create reproducible test data with fixed random seeds: <code>rand.Seed(42)</code></li>\n<li>Add timeouts to tests that could hang: <code>ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)</code></li>\n</ul>\n<h2 id=\"debugging-guide\">Debugging Guide</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section applies to all milestones (1-5), providing systematic debugging approaches, diagnostic procedures, and resolution strategies specific to log aggregation systems. Each milestone introduces new failure modes that require specialized debugging techniques.</p>\n</blockquote>\n<h3 id=\"mental-model-the-medical-diagnosis-process\">Mental Model: The Medical Diagnosis Process</h3>\n<p>Think of debugging a log aggregation system like being a doctor diagnosing a patient. Just as a doctor follows a systematic approach—observing symptoms, running tests, forming hypotheses, and prescribing treatments—debugging requires structured observation, measurement, hypothesis formation, and targeted fixes. The key insight is that symptoms often mask the root cause: a &quot;slow queries&quot; symptom might actually indicate index corruption, memory pressure, or retention policy failures. Like medical diagnosis, effective debugging requires understanding both the normal &quot;physiology&quot; of the system and the pathological patterns that indicate specific problems.</p>\n<p>The analogy extends to diagnostic tools: just as doctors use stethoscopes, blood tests, and X-rays, log aggregation systems need metrics, traces, and specialized debugging utilities. Each tool reveals different aspects of system health, and combining multiple signals often reveals problems invisible to any single measurement.</p>\n<h3 id=\"symptom-based-diagnosis\">Symptom-Based Diagnosis</h3>\n<p>The following diagnostic table provides a systematic approach to identifying and resolving common problems in log aggregation systems. Each entry follows the medical model: observable symptom, likely underlying causes, diagnostic procedures, and targeted treatments.</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Causes</th>\n<th>Diagnostic Steps</th>\n<th>Resolution Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>No logs arriving</strong></td>\n<td>Network connectivity, authentication failure, buffer overflow, parser errors</td>\n<td>1. Check network connectivity with <code>telnet host:port</code><br>2. Verify authentication headers in HTTP logs<br>3. Check ingestion endpoint metrics for error rates<br>4. Examine parser error logs for format mismatches<br>5. Monitor buffer fullness metrics</td>\n<td>1. Fix network configuration or firewall rules<br>2. Update authentication credentials<br>3. Increase buffer capacity or flush frequency<br>4. Adjust parser configuration for log format<br>5. Scale ingestion capacity horizontally</td>\n</tr>\n<tr>\n<td><strong>Query returns no results</strong></td>\n<td>Time range mismatch, label selector errors, index corruption, retention cleanup</td>\n<td>1. Verify time range covers ingested log timestamps<br>2. Check label exactness (case sensitivity, whitespace)<br>3. Query index directly for expected terms<br>4. Check retention policy logs for cleanup activity<br>5. Validate chunk metadata consistency</td>\n<td>1. Adjust query time range to match data<br>2. Fix label selector syntax and values<br>3. Rebuild corrupted index segments<br>4. Restore from backup if retention deleted data<br>5. Run chunk integrity verification</td>\n</tr>\n<tr>\n<td><strong>Queries extremely slow</strong></td>\n<td>Index not being used, large result sets, disk I/O bottleneck, memory pressure</td>\n<td>1. Check query execution plan for index usage<br>2. Measure result set size with EXPLAIN queries<br>3. Monitor disk I/O wait times during queries<br>4. Check memory allocation during query processing<br>5. Profile query execution with detailed timing</td>\n<td>1. Add missing indexes or rebuild corrupted ones<br>2. Add more selective filters to reduce result set<br>3. Move storage to faster disks or add read replicas<br>4. Increase query memory limits or add query caching<br>5. Implement query result streaming</td>\n</tr>\n<tr>\n<td><strong>Memory usage growing unbounded</strong></td>\n<td>Buffer not flushing, index cache not evicting, query results not streaming, WAL accumulation</td>\n<td>1. Monitor buffer flush frequency and success rate<br>2. Check index cache hit ratios and eviction policies<br>3. Measure memory allocation per active query<br>4. Check WAL file sizes and rotation frequency<br>5. Profile memory allocations by component</td>\n<td>1. Fix buffer flush triggers or increase flush frequency<br>2. Tune cache eviction policies or reduce cache sizes<br>3. Implement streaming query execution<br>4. Force WAL rotation and cleanup old files<br>5. Add memory circuit breakers</td>\n</tr>\n<tr>\n<td><strong>High CPU usage during ingestion</strong></td>\n<td>Inefficient parsing, excessive indexing, compression overhead, lock contention</td>\n<td>1. Profile CPU usage by parsing stage<br>2. Monitor index write operations and lock waits<br>3. Compare compression algorithm performance<br>4. Check goroutine/thread contention patterns<br>5. Measure parsing throughput vs CPU utilization</td>\n<td>1. Optimize regex patterns or switch to faster parsers<br>2. Batch index updates or use async indexing<br>3. Switch to faster compression (LZ4 vs gzip)<br>4. Reduce lock scope or use lock-free data structures<br>5. Scale parsing across multiple threads</td>\n</tr>\n<tr>\n<td><strong>Disk space growing rapidly</strong></td>\n<td>Compression disabled, retention not working, WAL not rotating, index bloat</td>\n<td>1. Check compression ratios in chunk headers<br>2. Verify retention policy execution logs<br>3. Monitor WAL file ages and rotation triggers<br>4. Measure index size vs log data ratio<br>5. Check for failed cleanup operations</td>\n<td>1. Enable compression or fix compression pipeline<br>2. Fix retention policy logic or schedule execution<br>3. Configure WAL rotation thresholds correctly<br>4. Rebuild indexes with better compression<br>5. Manually clean up failed operations</td>\n</tr>\n<tr>\n<td><strong>Authentication failures</strong></td>\n<td>Token expiration, tenant ID mismatches, network time skew, header formatting</td>\n<td>1. Check JWT token expiration times<br>2. Verify tenant ID extraction from requests<br>3. Compare client/server clock synchronization<br>4. Validate HTTP header formatting<br>5. Test authentication with curl commands</td>\n<td>1. Refresh expired tokens or extend validity<br>2. Fix tenant ID extraction logic<br>3. Synchronize clocks via NTP<br>4. Fix client header formatting<br>5. Add detailed authentication logging</td>\n</tr>\n<tr>\n<td><strong>Rate limiting triggering incorrectly</strong></td>\n<td>Token bucket misconfiguration, time window errors, tenant quota bugs, clock issues</td>\n<td>1. Monitor token bucket fill rates and consumption<br>2. Check time window calculations in rate limiter<br>3. Verify per-tenant quota configuration<br>4. Compare client/server timestamps<br>5. Test with synthetic traffic patterns</td>\n<td>1. Adjust token bucket parameters<br>2. Fix time window boundary calculations<br>3. Update tenant quota configuration<br>4. Synchronize system clocks<br>5. Add rate limiting metrics and alerting</td>\n</tr>\n<tr>\n<td><strong>Alerts not firing</strong></td>\n<td>Rule configuration errors, condition logic bugs, notification failures, time window issues</td>\n<td>1. Test alert rules manually with sample data<br>2. Check condition evaluation logic and thresholds<br>3. Verify notification delivery mechanisms<br>4. Check alert time window calculations<br>5. Monitor alert rule evaluation frequency</td>\n<td>1. Fix alert rule syntax or logic errors<br>2. Adjust condition thresholds for sensitivity<br>3. Fix notification webhook endpoints<br>4. Correct time window boundary handling<br>5. Increase alert evaluation frequency</td>\n</tr>\n<tr>\n<td><strong>Data corruption detected</strong></td>\n<td>Disk errors, incomplete writes, checksum mismatches, concurrent access bugs</td>\n<td>1. Check filesystem error logs<br>2. Verify WAL record checksums<br>3. Compare chunk checksums with stored values<br>4. Look for concurrent write access patterns<br>5. Run full data integrity verification</td>\n<td>1. Replace failing disk hardware<br>2. Restore from WAL or backup<br>3. Rebuild corrupted chunks from source logs<br>4. Fix concurrent access with proper locking<br>5. Implement automatic corruption detection</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight</strong>: The most effective debugging approach combines multiple diagnostic signals rather than relying on single symptoms. For example, &quot;slow queries&quot; combined with &quot;high disk I/O&quot; and &quot;low index cache hit rate&quot; points to a specific resolution strategy, while &quot;slow queries&quot; with &quot;high CPU&quot; and &quot;low memory&quot; suggests a completely different root cause.</p>\n</blockquote>\n<h3 id=\"domain-specific-debugging-techniques\">Domain-Specific Debugging Techniques</h3>\n<p>Log aggregation systems have unique debugging requirements due to their high-throughput, time-series nature and complex interaction between ingestion, indexing, storage, and querying. These specialized techniques go beyond general application debugging.</p>\n<h4 id=\"log-flow-tracing\">Log Flow Tracing</h4>\n<p>The <strong>log flow tracing</strong> technique tracks individual log entries through the entire pipeline from ingestion to storage. This helps identify where logs get lost, corrupted, or delayed. Implement trace IDs that follow specific log entries:</p>\n<p><strong>Trace ID Implementation Strategy:</strong></p>\n<ol>\n<li>Generate unique trace ID during ingestion (timestamp + source + sequence)</li>\n<li>Attach trace ID to log entry metadata throughout processing</li>\n<li>Log trace ID at each pipeline stage with timing information</li>\n<li>Provide trace lookup API for debugging specific log entry paths</li>\n<li>Include trace ID in error messages and monitoring metrics</li>\n</ol>\n<p><strong>Pipeline Stage Monitoring Points:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Stage</th>\n<th>Monitoring Point</th>\n<th>Key Metrics</th>\n<th>Failure Indicators</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>HTTP Reception</strong></td>\n<td>Request handler entry/exit</td>\n<td>Latency, error rate, payload size</td>\n<td>4xx/5xx responses, timeout errors</td>\n</tr>\n<tr>\n<td><strong>Parsing</strong></td>\n<td>Parser input/output</td>\n<td>Parse time, validation errors, format mismatches</td>\n<td>Regex failures, JSON syntax errors</td>\n</tr>\n<tr>\n<td><strong>Buffering</strong></td>\n<td>Buffer write/read operations</td>\n<td>Buffer fullness, flush frequency, backpressure</td>\n<td>Buffer overflow, flush failures</td>\n</tr>\n<tr>\n<td><strong>Indexing</strong></td>\n<td>Index term extraction/insertion</td>\n<td>Index write latency, term count, cardinality</td>\n<td>Lock contention, index corruption</td>\n</tr>\n<tr>\n<td><strong>Storage</strong></td>\n<td>Chunk write operations</td>\n<td>Compression ratio, write latency, WAL sync</td>\n<td>Disk I/O errors, checksum mismatches</td>\n</tr>\n</tbody></table>\n<h4 id=\"index-health-diagnostics\">Index Health Diagnostics</h4>\n<p>Index corruption and inefficiency are common sources of query performance problems. Specialized index debugging requires checking multiple aspects of index health:</p>\n<p><strong>Index Consistency Checks:</strong></p>\n<ol>\n<li><strong>Term-to-posting verification</strong>: For random sample of terms, verify postings lists point to valid log entries</li>\n<li><strong>Reverse reference validation</strong>: For random log entries, verify they appear in expected term postings</li>\n<li><strong>Bloom filter accuracy</strong>: Test bloom filter false positive rates against statistical expectations</li>\n<li><strong>Partition boundary integrity</strong>: Verify time-based partitions contain only logs within their time ranges</li>\n<li><strong>Compaction consistency</strong>: After index compaction, verify search results match pre-compaction results</li>\n</ol>\n<p><strong>Index Performance Analysis:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Metric</th>\n<th>Healthy Range</th>\n<th>Warning Threshold</th>\n<th>Critical Threshold</th>\n<th>Diagnostic Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Bloom filter false positive rate</strong></td>\n<td>&lt; 1%</td>\n<td>1-5%</td>\n<td>&gt; 5%</td>\n<td>Rebuild bloom filters with better parameters</td>\n</tr>\n<tr>\n<td><strong>Average postings list length</strong></td>\n<td>10-1000 entries</td>\n<td>1000-10000</td>\n<td>&gt; 10000</td>\n<td>Check for high-cardinality label explosion</td>\n</tr>\n<tr>\n<td><strong>Index cache hit rate</strong></td>\n<td>&gt; 80%</td>\n<td>60-80%</td>\n<td>&lt; 60%</td>\n<td>Increase cache size or improve cache eviction</td>\n</tr>\n<tr>\n<td><strong>Index compaction frequency</strong></td>\n<td>Daily</td>\n<td>Weekly</td>\n<td>Monthly</td>\n<td>Reduce compaction threshold or increase resources</td>\n</tr>\n<tr>\n<td><strong>Query index usage percentage</strong></td>\n<td>&gt; 90%</td>\n<td>70-90%</td>\n<td>&lt; 70%</td>\n<td>Add missing indexes or fix query patterns</td>\n</tr>\n</tbody></table>\n<h4 id=\"storage-layer-debugging\">Storage Layer Debugging</h4>\n<p>The storage layer combines WAL, chunks, compression, and retention policies, making it complex to debug. Storage debugging requires understanding data movement through multiple storage tiers:</p>\n<p><strong>WAL Health Diagnostics:</strong></p>\n<ol>\n<li><strong>WAL record integrity</strong>: Verify checksums for all WAL records, detect incomplete writes</li>\n<li><strong>WAL replay consistency</strong>: Compare system state before/after WAL replay operations</li>\n<li><strong>WAL rotation timing</strong>: Monitor WAL file sizes and rotation frequency vs configuration</li>\n<li><strong>WAL performance impact</strong>: Measure WAL fsync latency impact on ingestion throughput</li>\n<li><strong>Recovery completeness</strong>: Verify WAL recovery restores exact pre-crash state</li>\n</ol>\n<p><strong>Chunk Integrity Verification:</strong></p>\n<p>The chunk verification process ensures compressed log storage maintains data integrity across the compression, storage, and retrieval pipeline:</p>\n<ol>\n<li><strong>Header validation</strong>: Verify chunk headers contain valid metadata (magic bytes, version, checksums)</li>\n<li><strong>Compression consistency</strong>: Decompress chunks and verify entry count matches header</li>\n<li><strong>Time range compliance</strong>: Verify all entries in chunk fall within chunk&#39;s time boundaries</li>\n<li><strong>Cross-reference validation</strong>: Verify index entries point to valid chunk locations</li>\n<li><strong>Retention policy compliance</strong>: Verify chunks are cleaned up according to configured policies</li>\n</ol>\n<h4 id=\"multi-tenant-debugging\">Multi-Tenant Debugging</h4>\n<p>Multi-tenant systems require specialized debugging to isolate tenant-specific problems from system-wide issues:</p>\n<p><strong>Tenant Isolation Verification:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Verification Type</th>\n<th>Test Procedure</th>\n<th>Expected Result</th>\n<th>Failure Indication</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Data isolation</strong></td>\n<td>Query tenant A data with tenant B credentials</td>\n<td>Empty result set</td>\n<td>Cross-tenant data leak</td>\n</tr>\n<tr>\n<td><strong>Resource isolation</strong></td>\n<td>Generate high load from tenant A</td>\n<td>No impact on tenant B performance</td>\n<td>Resource bleed-through</td>\n</tr>\n<tr>\n<td><strong>Rate limit isolation</strong></td>\n<td>Exhaust tenant A rate limit</td>\n<td>Tenant B unaffected</td>\n<td>Rate limit cross-talk</td>\n</tr>\n<tr>\n<td><strong>Alert isolation</strong></td>\n<td>Trigger alerts in tenant A</td>\n<td>Tenant B receives no alerts</td>\n<td>Alert routing errors</td>\n</tr>\n<tr>\n<td><strong>Authentication bypass</strong></td>\n<td>Use malformed tenant headers</td>\n<td>Authentication failure</td>\n<td>Security vulnerability</td>\n</tr>\n</tbody></table>\n<p><strong>Per-Tenant Metrics Analysis:</strong></p>\n<p>Multi-tenant debugging requires analyzing metrics both globally and per-tenant to identify problems that affect specific tenants:</p>\n<ol>\n<li><strong>Ingestion rate per tenant</strong>: Compare against tenant quotas and historical patterns</li>\n<li><strong>Query latency by tenant</strong>: Identify tenants with consistently slow queries</li>\n<li><strong>Storage usage growth</strong>: Track tenant storage against retention policies</li>\n<li><strong>Alert frequency patterns</strong>: Identify tenants with excessive or missing alerts</li>\n<li><strong>Authentication failure rates</strong>: Monitor per-tenant auth errors for security issues</li>\n</ol>\n<blockquote>\n<p><strong>Design Insight</strong>: Multi-tenant debugging often reveals problems that only manifest under specific tenant interaction patterns. For example, a high-cardinality tenant might cause index bloat that affects all other tenants&#39; query performance, even though the symptom appears system-wide.</p>\n</blockquote>\n<h4 id=\"query-performance-profiling\">Query Performance Profiling</h4>\n<p>LogQL query debugging requires understanding both the query language semantics and the underlying execution engine performance characteristics:</p>\n<p><strong>Query Execution Analysis Framework:</strong></p>\n<ol>\n<li><strong>Parse tree validation</strong>: Verify query parses to expected AST structure</li>\n<li><strong>Execution plan inspection</strong>: Check index usage, filter ordering, and optimization decisions</li>\n<li><strong>Resource consumption profiling</strong>: Monitor memory, CPU, and I/O usage during query execution</li>\n<li><strong>Result set analysis</strong>: Verify result accuracy and completeness against expected outcomes</li>\n<li><strong>Streaming behavior verification</strong>: Ensure large result sets stream properly without memory exhaustion</li>\n</ol>\n<p><strong>Performance Regression Detection:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Performance Indicator</th>\n<th>Measurement Approach</th>\n<th>Baseline Establishment</th>\n<th>Regression Threshold</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Query latency</strong></td>\n<td>P50/P95/P99 response times</td>\n<td>Rolling 7-day average</td>\n<td>2x baseline increase</td>\n</tr>\n<tr>\n<td><strong>Memory usage</strong></td>\n<td>Peak memory per query</td>\n<td>Historical query memory</td>\n<td>50% increase from baseline</td>\n</tr>\n<tr>\n<td><strong>Index utilization</strong></td>\n<td>Index seek vs scan ratio</td>\n<td>Query plan analysis</td>\n<td>&lt; 50% index usage</td>\n</tr>\n<tr>\n<td><strong>Result streaming</strong></td>\n<td>Time to first result</td>\n<td>Streaming latency measurement</td>\n<td>5x increase in TTFR</td>\n</tr>\n<tr>\n<td><strong>Cache efficiency</strong></td>\n<td>Cache hit rates</td>\n<td>Cache performance monitoring</td>\n<td>20% hit rate decrease</td>\n</tr>\n</tbody></table>\n<h3 id=\"performance-problem-diagnosis\">Performance Problem Diagnosis</h3>\n<p>Performance problems in log aggregation systems often involve complex interactions between ingestion load, storage I/O, index efficiency, and query patterns. Systematic performance diagnosis requires understanding these interactions and measuring performance across multiple dimensions.</p>\n<h4 id=\"throughput-performance-analysis\">Throughput Performance Analysis</h4>\n<p><strong>Ingestion Throughput Diagnosis:</strong></p>\n<p>Think of ingestion throughput like water flowing through a series of pipes with different diameters. The overall flow rate is limited by the narrowest pipe, but identifying which pipe is the bottleneck requires measuring pressure at each stage.</p>\n<table>\n<thead>\n<tr>\n<th>Pipeline Stage</th>\n<th>Throughput Measurement</th>\n<th>Bottleneck Indicators</th>\n<th>Resolution Strategies</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Network Reception</strong></td>\n<td>Requests/second, bytes/second</td>\n<td>High connection queue, TCP retransmits</td>\n<td>Scale connection handlers, increase buffer sizes</td>\n</tr>\n<tr>\n<td><strong>Parsing</strong></td>\n<td>Logs parsed/second, parse CPU time</td>\n<td>High parse latency, CPU saturation</td>\n<td>Optimize regex, use faster parsers, parallel parsing</td>\n</tr>\n<tr>\n<td><strong>Validation</strong></td>\n<td>Validation time per entry</td>\n<td>Schema validation failures</td>\n<td>Cache validation rules, async validation</td>\n</tr>\n<tr>\n<td><strong>Indexing</strong></td>\n<td>Terms indexed/second, index write latency</td>\n<td>Index lock contention, high index CPU</td>\n<td>Batch index updates, async indexing, index sharding</td>\n</tr>\n<tr>\n<td><strong>Storage</strong></td>\n<td>Chunks written/second, compression throughput</td>\n<td>Disk I/O wait, compression CPU</td>\n<td>Faster compression, async writes, storage scaling</td>\n</tr>\n</tbody></table>\n<p><strong>Throughput Cascade Analysis:</strong></p>\n<p>Throughput problems often cascade through the pipeline. For example, slow storage can back up indexing, which backs up parsing, eventually causing network timeouts. The cascade analysis technique measures throughput at each stage and identifies where the backup begins:</p>\n<ol>\n<li>Start measurement at the output (storage writes/second)</li>\n<li>Work backward through pipeline measuring each stage</li>\n<li>Identify the first stage where throughput drops significantly</li>\n<li>Focus optimization efforts on the bottleneck stage</li>\n<li>Re-measure after optimization to verify improvement and identify next bottleneck</li>\n</ol>\n<h4 id=\"latency-performance-analysis\">Latency Performance Analysis</h4>\n<p><strong>Query Latency Breakdown:</strong></p>\n<p>Query latency involves multiple phases, each with different performance characteristics and optimization approaches:</p>\n<table>\n<thead>\n<tr>\n<th>Latency Phase</th>\n<th>Typical Duration</th>\n<th>Optimization Techniques</th>\n<th>Measurement Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Query parsing</strong></td>\n<td>1-10ms</td>\n<td>Cache parsed queries, optimize parser</td>\n<td>Time parse function execution</td>\n</tr>\n<tr>\n<td><strong>Index lookup</strong></td>\n<td>10-100ms</td>\n<td>Improve index caching, use bloom filters</td>\n<td>Instrument index seek operations</td>\n</tr>\n<tr>\n<td><strong>Storage retrieval</strong></td>\n<td>50-500ms</td>\n<td>Cache frequently accessed chunks</td>\n<td>Time chunk decompression</td>\n</tr>\n<tr>\n<td><strong>Result filtering</strong></td>\n<td>10-200ms</td>\n<td>Push filters to storage layer</td>\n<td>Profile filter execution</td>\n</tr>\n<tr>\n<td><strong>Result serialization</strong></td>\n<td>5-50ms</td>\n<td>Stream results, optimize JSON encoding</td>\n<td>Time response generation</td>\n</tr>\n</tbody></table>\n<p><strong>Latency Percentile Analysis:</strong></p>\n<p>Different latency percentiles reveal different types of performance problems:</p>\n<ul>\n<li><strong>P50 latency increases</strong>: Indicates general performance degradation affecting typical queries</li>\n<li><strong>P95 latency spikes</strong>: Suggests occasional expensive operations (cache misses, large queries)</li>\n<li><strong>P99 latency extremes</strong>: Points to outlier behavior (query timeouts, resource exhaustion)</li>\n<li><strong>Max latency growth</strong>: Indicates unbounded resource usage (memory leaks, runaway queries)</li>\n</ul>\n<h4 id=\"memory-performance-analysis\">Memory Performance Analysis</h4>\n<p><strong>Memory Usage Pattern Diagnosis:</strong></p>\n<p>Memory problems in log aggregation systems follow characteristic patterns that indicate specific underlying causes:</p>\n<table>\n<thead>\n<tr>\n<th>Memory Pattern</th>\n<th>Likely Cause</th>\n<th>Diagnostic Approach</th>\n<th>Resolution Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Gradual growth</strong></td>\n<td>Memory leak in long-running operations</td>\n<td>Profile allocation over time</td>\n<td>Fix leak sources, add memory limits</td>\n</tr>\n<tr>\n<td><strong>Spike during queries</strong></td>\n<td>Large result sets loading into memory</td>\n<td>Monitor per-query memory usage</td>\n<td>Implement result streaming</td>\n</tr>\n<tr>\n<td><strong>Spike during ingestion</strong></td>\n<td>Buffer overflow or batch sizing issues</td>\n<td>Monitor buffer memory usage</td>\n<td>Tune buffer sizes and flush triggers</td>\n</tr>\n<tr>\n<td><strong>Growth during indexing</strong></td>\n<td>Index cache not evicting properly</td>\n<td>Monitor cache hit rates and sizes</td>\n<td>Fix cache eviction policies</td>\n</tr>\n<tr>\n<td><strong>Growth after failures</strong></td>\n<td>Failed operations not cleaning up resources</td>\n<td>Monitor resource cleanup after errors</td>\n<td>Add proper error handling cleanup</td>\n</tr>\n</tbody></table>\n<p><strong>Garbage Collection Impact Analysis:</strong></p>\n<p>In garbage-collected languages, GC pressure can severely impact performance. GC analysis requires understanding how log aggregation workloads interact with garbage collection:</p>\n<ol>\n<li><strong>Allocation rate monitoring</strong>: Measure object allocation rate vs GC frequency</li>\n<li><strong>GC pause impact</strong>: Monitor GC pause times vs query latency spikes</li>\n<li><strong>Memory pool analysis</strong>: Track different memory pools (buffers, indexes, queries) separately</li>\n<li><strong>Collection frequency tuning</strong>: Adjust GC parameters for log aggregation workload characteristics</li>\n<li><strong>Memory pool optimization</strong>: Use object pooling for frequently allocated structures</li>\n</ol>\n<h4 id=\"disk-io-performance-analysis\">Disk I/O Performance Analysis</h4>\n<p><strong>Storage I/O Bottleneck Identification:</strong></p>\n<p>Disk I/O performance affects both ingestion (chunk writes, WAL fsync) and querying (chunk reads, index access). I/O performance diagnosis requires understanding both sequential and random access patterns:</p>\n<table>\n<thead>\n<tr>\n<th>I/O Pattern</th>\n<th>Associated Operations</th>\n<th>Performance Indicators</th>\n<th>Optimization Approaches</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Sequential writes</strong></td>\n<td>WAL appends, chunk creation</td>\n<td>Write throughput MB/s</td>\n<td>Use faster sequential storage, batch writes</td>\n</tr>\n<tr>\n<td><strong>Random writes</strong></td>\n<td>Index updates, metadata updates</td>\n<td>Write IOPS, write latency</td>\n<td>Use SSD storage, reduce write amplification</td>\n</tr>\n<tr>\n<td><strong>Sequential reads</strong></td>\n<td>Chunk scanning, WAL replay</td>\n<td>Read throughput MB/s</td>\n<td>Prefetch data, use streaming reads</td>\n</tr>\n<tr>\n<td><strong>Random reads</strong></td>\n<td>Index lookups, chunk metadata</td>\n<td>Read IOPS, cache hit rates</td>\n<td>Increase cache sizes, optimize data layout</td>\n</tr>\n</tbody></table>\n<p><strong>Storage Layer Performance Profiling:</strong></p>\n<p>Storage performance diagnosis requires measuring performance across multiple storage tiers:</p>\n<ol>\n<li><strong>WAL performance</strong>: Measure fsync latency and WAL write throughput</li>\n<li><strong>Chunk compression</strong>: Profile compression time vs compression ratio trade-offs</li>\n<li><strong>Index persistence</strong>: Monitor index write and read performance separately</li>\n<li><strong>Cache effectiveness</strong>: Measure cache hit rates for different data types (chunks, indexes, metadata)</li>\n<li><strong>Background operations</strong>: Monitor performance impact of compaction and retention cleanup</li>\n</ol>\n<h4 id=\"network-performance-analysis\">Network Performance Analysis</h4>\n<p><strong>Network Bottleneck Diagnosis:</strong></p>\n<p>Network performance affects log ingestion from remote sources and query responses to clients:</p>\n<table>\n<thead>\n<tr>\n<th>Network Metric</th>\n<th>Ingestion Impact</th>\n<th>Query Impact</th>\n<th>Diagnostic Commands</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Bandwidth utilization</strong></td>\n<td>Limits ingestion rate</td>\n<td>Affects large result transfers</td>\n<td><code>iftop</code>, <code>nload</code>, <code>bandwidthd</code></td>\n</tr>\n<tr>\n<td><strong>Connection counts</strong></td>\n<td>TCP connection limits</td>\n<td>Concurrent query limits</td>\n<td><code>netstat -an</code>, <code>ss -s</code></td>\n</tr>\n<tr>\n<td><strong>Packet loss</strong></td>\n<td>Lost log entries</td>\n<td>Query timeouts</td>\n<td><code>ping</code>, <code>mtr</code>, packet capture</td>\n</tr>\n<tr>\n<td><strong>DNS resolution</strong></td>\n<td>Service discovery failures</td>\n<td>Client connection issues</td>\n<td><code>dig</code>, <code>nslookup</code>, DNS timing</td>\n</tr>\n<tr>\n<td><strong>TLS handshake time</strong></td>\n<td>HTTPS ingestion overhead</td>\n<td>Secure query overhead</td>\n<td>TLS connection profiling</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Logging Framework</strong></td>\n<td>Go log package with structured output</td>\n<td>Zap or Logrus with contextual fields</td>\n</tr>\n<tr>\n<td><strong>Metrics Collection</strong></td>\n<td>Prometheus client library</td>\n<td>Custom metrics with OpenTelemetry</td>\n</tr>\n<tr>\n<td><strong>Profiling Tools</strong></td>\n<td>Go pprof for CPU/memory profiling</td>\n<td>Continuous profiling with Pyroscope</td>\n</tr>\n<tr>\n<td><strong>Tracing Infrastructure</strong></td>\n<td>Simple trace ID logging</td>\n<td>Distributed tracing with Jaeger</td>\n</tr>\n<tr>\n<td><strong>Health Monitoring</strong></td>\n<td>HTTP health check endpoints</td>\n<td>Comprehensive health dashboard</td>\n</tr>\n<tr>\n<td><strong>Error Tracking</strong></td>\n<td>Structured error logging</td>\n<td>Error aggregation with Sentry</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  cmd/\n    debug/                    ← debugging utilities\n      trace-log.go           ← trace specific log entries\n      index-verify.go        ← verify index consistency\n      chunk-inspect.go       ← inspect chunk contents\n      wal-replay.go          ← replay WAL operations\n  internal/\n    debug/                   ← debugging infrastructure\n      tracer/                ← log flow tracing\n        tracer.go\n        trace_id.go\n      health/                ← health monitoring\n        checks.go\n        registry.go\n        http_handler.go\n      metrics/               ← performance monitoring\n        collector.go\n        ingestion_metrics.go\n        query_metrics.go\n      profiling/             ← performance profiling\n        profiler.go\n        memory_profiler.go\n    monitoring/              ← system monitoring\n      circuit_breaker.go\n      degradation.go\n      recovery.go\n  scripts/\n    debug-queries.sh         ← query debugging scripts\n    performance-test.sh      ← load testing scripts\n    health-check.sh          ← system health verification</code></pre></div>\n\n<h4 id=\"core-debugging-infrastructure\">Core Debugging Infrastructure</h4>\n<p><strong>Distributed Tracing Implementation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// TraceContext provides request tracing across the log aggregation pipeline</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TraceContext</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TraceID     </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\">            // unique identifier for this request</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SpanID      </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\">            // current operation identifier  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ParentSpan  </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\">            // parent operation identifier</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StartTime   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#6A737D\">         // when this operation started</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Metadata    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\"> // additional context information</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Logger      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">zap</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#6A737D\">       // logger with trace context</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewTraceContext creates a new trace context for pipeline operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewTraceContext</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">operation</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">metadata</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TraceContext</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Generate unique trace ID using timestamp + random component</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Generate span ID for this specific operation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Create logger with trace ID and span ID fields</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Record start time for latency measurement</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Initialize metadata map with operation-specific context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use crypto/rand for trace ID generation to ensure uniqueness</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StartSpan creates a child span for nested operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TraceContext</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">StartSpan</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">operation</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TraceContext</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Generate new span ID for child operation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Set current span ID as parent span</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Create new logger with updated span context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Copy relevant metadata from parent context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Record span start time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RecordEvent logs a significant event within this trace span</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TraceContext</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RecordEvent</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">event</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">details</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Calculate elapsed time since span start</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Log event with trace ID, span ID, and timing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Include event details in structured format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Add event to trace timeline for debugging</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Finish completes the current span and records final metrics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TraceContext</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Finish</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">err</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Calculate total span duration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Record success or failure status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Log span completion with final metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update performance monitoring with span data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Clean up any span-specific resources</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Health Check Infrastructure:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// HealthChecker defines the interface for component health verification</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HealthChecker</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Name</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\">                                    // component name for identification</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Check</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CheckResult</span><span style=\"color:#6A737D\">         // perform health verification</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Dependencies</span><span style=\"color:#E1E4E8\">() []</span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\">                         // list component dependencies</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// IngestionHealthCheck verifies log ingestion pipeline health</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> IngestionHealthCheck</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    httpServer   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPServer</span><span style=\"color:#6A737D\">      // HTTP ingestion endpoint</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tcpHandler   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TCPHandler</span><span style=\"color:#6A737D\">      // TCP syslog handler  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    buffer       </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MemoryBuffer</span><span style=\"color:#6A737D\">    // ingestion buffer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parser       </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">JSONParser</span><span style=\"color:#6A737D\">      // log parser</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metrics      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Metrics</span><span style=\"color:#6A737D\">         // ingestion metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Check verifies all aspects of log ingestion pipeline health</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ihc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">IngestionHealthCheck</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Check</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CheckResult</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Test HTTP endpoint responsiveness with synthetic request</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Verify TCP handler is accepting connections</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Check buffer utilization and flush status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Test parser with sample log entries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Validate metrics collection is working</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Aggregate results and determine overall health status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use context timeout to prevent health checks from hanging</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StorageHealthCheck verifies storage layer functionality</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StorageHealthCheck</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storageEngine </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageEngine</span><span style=\"color:#6A737D\">  // storage coordination</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    walPath       </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\">          // WAL file location</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    chunkPath     </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\">          // chunk storage location</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Check verifies storage layer health including WAL and chunk access</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">shc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageHealthCheck</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Check</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CheckResult</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Verify WAL file accessibility and write permissions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Test chunk storage read/write operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Check storage space availability</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Validate recent WAL records for corruption</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Test chunk compression/decompression pipeline</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Verify retention policy is running properly</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Performance Monitoring Framework:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// PerformanceMonitor tracks system performance across all components</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> PerformanceMonitor</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ingestionMetrics </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">IngestionMetrics</span><span style=\"color:#6A737D\">    // ingestion performance data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queryMetrics     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryMetrics</span><span style=\"color:#6A737D\">        // query performance data  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storageMetrics   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageMetrics</span><span style=\"color:#6A737D\">      // storage performance data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    systemMetrics    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SystemMetrics</span><span style=\"color:#6A737D\">       // system resource usage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    alertThresholds  </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">float64</span><span style=\"color:#6A737D\">   // performance alert thresholds</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RecordIngestionLatency tracks latency for each ingestion pipeline stage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">pm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PerformanceMonitor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RecordIngestionLatency</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">stage</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">duration</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Record latency in appropriate histogram bucket</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Update running averages for this stage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Check if latency exceeds alert thresholds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update dashboard metrics for real-time monitoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Log performance anomalies for investigation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RecordQueryPerformance captures comprehensive query execution metrics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">pm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PerformanceMonitor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RecordQueryPerformance</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">query</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">metadata</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">QueryMetadata</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Record query latency by query complexity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Track memory usage patterns for query types</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Monitor index utilization effectiveness</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update cache hit rate statistics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Check for query performance regressions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GeneratePerformanceReport creates comprehensive performance analysis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">pm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PerformanceMonitor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GeneratePerformanceReport</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PerformanceReport</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Aggregate metrics across all components</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Calculate performance trends over time windows</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Identify performance bottlenecks and anomalies</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Generate recommendations for optimization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Format report for debugging and optimization</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Milestone 1 - Ingestion Debugging Verification:</strong></p>\n<ul>\n<li>Start the ingestion pipeline and send test logs via all protocols (HTTP, TCP, UDP)</li>\n<li>Verify trace IDs appear in logs for each ingestion path</li>\n<li>Check that buffer overflow scenarios trigger appropriate backpressure</li>\n<li>Confirm parser errors are logged with sufficient detail for debugging</li>\n<li>Expected behavior: All logs should have trace context, errors should be actionable</li>\n</ul>\n<p><strong>Milestone 2 - Index Health Verification:</strong></p>\n<ul>\n<li>Run index consistency checks on a populated index</li>\n<li>Verify bloom filter false positive rates match expected statistical ranges</li>\n<li>Test index compaction process and validate search results remain consistent</li>\n<li>Confirm index corruption detection triggers appropriate alerts</li>\n<li>Expected behavior: Index diagnostics should pass, corruption should be detected reliably</li>\n</ul>\n<p><strong>Milestone 3 - Query Performance Verification:</strong></p>\n<ul>\n<li>Profile query execution with various complexity levels</li>\n<li>Verify query execution plans use indexes appropriately</li>\n<li>Test query timeout and memory limit enforcement</li>\n<li>Confirm large result sets stream properly without memory exhaustion</li>\n<li>Expected behavior: Query performance should be predictable and bounded</li>\n</ul>\n<p><strong>Milestone 4 - Storage Debugging Verification:</strong></p>\n<ul>\n<li>Test WAL replay after simulated crash scenarios</li>\n<li>Verify chunk integrity checks detect corruption reliably</li>\n<li>Test retention policy execution and verify proper cleanup</li>\n<li>Confirm storage performance monitoring captures relevant metrics</li>\n<li>Expected behavior: Storage recovery should be automatic and complete</li>\n</ul>\n<p><strong>Milestone 5 - Multi-Tenant Debugging Verification:</strong></p>\n<ul>\n<li>Test tenant isolation verification across all data and resource boundaries</li>\n<li>Verify per-tenant performance metrics collection and analysis</li>\n<li>Test authentication debugging with various failure scenarios</li>\n<li>Confirm alert isolation prevents cross-tenant notification leakage</li>\n<li>Expected behavior: Tenant isolation should be complete and verifiable</li>\n</ul>\n<h4 id=\"language-specific-debugging-hints\">Language-Specific Debugging Hints</h4>\n<p><strong>Go-Specific Debugging Techniques:</strong></p>\n<ul>\n<li>Use <code>go tool pprof</code> for CPU and memory profiling: <code>go tool pprof http://localhost:6060/debug/pprof/heap</code></li>\n<li>Enable race detection during development: <code>go run -race main.go</code></li>\n<li>Use <code>go tool trace</code> for goroutine scheduling analysis</li>\n<li>Implement structured logging with <code>golang.org/x/exp/slog</code> for consistent log formatting</li>\n<li>Use <code>context.WithTimeout</code> for all external operations to prevent hangs</li>\n<li>Monitor goroutine counts with <code>runtime.NumGoroutine()</code> to detect goroutine leaks</li>\n<li>Use <code>sync.Pool</code> for frequent allocations to reduce GC pressure</li>\n<li>Implement graceful shutdown with signal handling using <code>os/signal</code></li>\n</ul>\n<p><strong>Memory Management Best Practices:</strong></p>\n<ul>\n<li>Use <code>make([]Type, 0, capacity)</code> to pre-allocate slices with known capacity</li>\n<li>Implement object pooling for <code>LogEntry</code> and <code>Labels</code> structs to reduce allocations</li>\n<li>Use <code>strings.Builder</code> instead of string concatenation for log formatting</li>\n<li>Monitor heap growth with <code>runtime.ReadMemStats()</code> and set appropriate GC targets</li>\n<li>Use memory-mapped files for large read-only data structures like indexes</li>\n<li>Implement backpressure mechanisms to prevent unbounded memory growth during load spikes</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Race Conditions in Concurrent Components</strong>\nMany debugging issues in log aggregation systems stem from race conditions between concurrent operations. For example, a query might read from an index while it&#39;s being compacted, or a retention cleanup might delete chunks while they&#39;re being accessed. Always use proper synchronization primitives (<code>sync.RWMutex</code> for read-heavy operations) and consider using channels for communication between goroutines instead of shared memory.</p>\n<p>⚠️ <strong>Pitfall: Context Propagation in Tracing</strong>\nFailing to properly propagate trace context through the entire pipeline makes debugging nearly impossible. Every function that processes log data should accept a <code>context.Context</code> parameter and use it for logging and tracing. Don&#39;t create new contexts unnecessarily—pass the request context through the entire pipeline to maintain trace continuity.</p>\n<p>⚠️ <strong>Pitfall: Blocking Operations in Health Checks</strong>\nHealth checks that perform blocking operations (network calls, disk I/O) without timeouts can make the entire system appear unhealthy. Always use <code>context.WithTimeout</code> in health check implementations and design checks to fail fast rather than hang indefinitely.</p>\n<p>⚠️ <strong>Pitfall: Insufficient Error Context</strong>\nGeneric error messages like &quot;parsing failed&quot; provide insufficient information for debugging in production. Always include relevant context in errors: the source of the data being parsed, the specific parsing rule that failed, and enough information to reproduce the issue. Use <code>fmt.Errorf</code> with proper error wrapping to maintain error context through the call stack.</p>\n<h2 id=\"future-extensions\">Future Extensions</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section provides a strategic roadmap for system evolution beyond the basic implementation, showing how the architecture from Milestones 1-5 can scale horizontally and accommodate advanced features while maintaining the core design principles.</p>\n</blockquote>\n<h3 id=\"mental-model-the-city-planning-system\">Mental Model: The City Planning System</h3>\n<p>Think of our log aggregation system like a growing city that starts as a small town but needs to expand into a metropolitan area. Just as urban planners must design neighborhoods that can accommodate population growth while adding new services like mass transit, shopping districts, and specialized facilities, we need to plan how our log system can scale from handling thousands of logs per second to millions, while adding sophisticated capabilities like machine learning and advanced analytics.</p>\n<p>The key insight is that good urban planning requires <strong>zoning</strong> (separating different functions into appropriate areas) and <strong>infrastructure</strong> (ensuring roads, utilities, and services can handle growth). Similarly, our system extensions require careful partitioning strategies and foundational infrastructure that can support advanced features without compromising the core logging functionality we&#39;ve built.</p>\n<h3 id=\"scalability-extensions\">Scalability Extensions</h3>\n<p>The current architecture supports vertical scaling well, but real production deployments require horizontal scaling across multiple machines. This section explores how our single-node design can evolve into a distributed system while preserving the query semantics and operational simplicity that make it valuable.</p>\n<h4 id=\"horizontal-scaling-architecture\">Horizontal Scaling Architecture</h4>\n<p>The transition from single-node to distributed deployment requires breaking our monolithic components into specialized services that can be deployed independently. The key challenge is maintaining data consistency and query performance across multiple nodes while avoiding the complexity pitfalls that plague many distributed systems.</p>\n<p><strong>Distributed Component Architecture:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Current Role</th>\n<th>Distributed Role</th>\n<th>Scaling Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Ingestion Engine</td>\n<td>Single HTTP/TCP/UDP receiver</td>\n<td>Multiple gateway nodes behind load balancer</td>\n<td>Stateless horizontal scaling with sticky sessions for TCP</td>\n</tr>\n<tr>\n<td>Index Engine</td>\n<td>Single inverted index</td>\n<td>Distributed hash table across nodes</td>\n<td>Consistent hashing by label hash with replication factor</td>\n</tr>\n<tr>\n<td>Storage Engine</td>\n<td>Local disk chunks</td>\n<td>Object storage with local cache</td>\n<td>S3-compatible backend with multi-tier caching</td>\n</tr>\n<tr>\n<td>Query Engine</td>\n<td>Single query coordinator</td>\n<td>Distributed query planning with fanout</td>\n<td>Query coordinator routes to relevant nodes based on time/labels</td>\n</tr>\n<tr>\n<td>Tenant Management</td>\n<td>In-memory tenant state</td>\n<td>Distributed tenant configuration service</td>\n<td>Replicated configuration with gossip protocol for updates</td>\n</tr>\n</tbody></table>\n<p>The distributed architecture maintains the same external APIs while internally routing operations across multiple nodes. Query processing becomes more complex because a single query might need to scatter across many nodes and gather results, similar to how map-reduce operations work in big data systems.</p>\n<blockquote>\n<p><strong>Critical Design Principle</strong>: Preserve query semantics across the distributed transition. A LogQL query should return the same results whether it runs on a single node or distributed across 100 nodes, just with different performance characteristics.</p>\n</blockquote>\n<h4 id=\"sharding-strategies\">Sharding Strategies</h4>\n<p>Effective sharding determines how data distributes across nodes and directly impacts query performance. Our system has three primary sharding dimensions: time, labels, and tenants. The optimal strategy depends on query patterns and data characteristics.</p>\n<p><strong>Time-Based Sharding:</strong></p>\n<p>Time-based sharding aligns naturally with log data&#39;s temporal nature and retention policies. Each node owns specific time ranges, making time-range queries very efficient but potentially creating hot spots during peak ingestion periods.</p>\n<table>\n<thead>\n<tr>\n<th>Sharding Approach</th>\n<th>Query Efficiency</th>\n<th>Write Distribution</th>\n<th>Operational Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Daily Shards</td>\n<td>Excellent for day-range queries</td>\n<td>Uneven during daily peaks</td>\n<td>Simple routing logic</td>\n</tr>\n<tr>\n<td>Hourly Shards</td>\n<td>Good for short-range queries</td>\n<td>Better distribution</td>\n<td>More shards to manage</td>\n</tr>\n<tr>\n<td>Rolling Windows</td>\n<td>Balanced across time ranges</td>\n<td>Even distribution</td>\n<td>Complex rebalancing</td>\n</tr>\n</tbody></table>\n<p>Time-based sharding works well when most queries target recent data (last 24 hours) and retention policies naturally expire old shards. However, queries spanning long time ranges must fan out to many nodes, potentially overwhelming the query coordinator.</p>\n<p><strong>Label-Based Sharding:</strong></p>\n<p>Label-based sharding distributes data by hashing label combinations, ensuring related log streams stay on the same node. This optimization makes label-specific queries very fast but requires careful management of label cardinality to avoid skewed distributions.</p>\n<p>The sharding key selection critically impacts performance. Using high-cardinality labels like <code>request_id</code> creates good distribution but breaks stream locality. Using low-cardinality labels like <code>service_name</code> maintains locality but risks hot spots for popular services.</p>\n<blockquote>\n<p><strong>Decision: Hybrid Time-Label Sharding</strong></p>\n<ul>\n<li><strong>Context</strong>: Pure time sharding creates hot spots, pure label sharding complicates time-range queries</li>\n<li><strong>Options Considered</strong>: Time-only sharding, label-only sharding, hybrid approach, consistent hashing</li>\n<li><strong>Decision</strong>: Hybrid sharding using time ranges as primary dimension with label hashing as secondary</li>\n<li><strong>Rationale</strong>: Time ranges naturally align with retention policies and query patterns, while label hashing within time ranges provides load distribution and query optimization</li>\n<li><strong>Consequences</strong>: Enables efficient time-range queries while preventing hot spots, but requires more complex routing logic and rebalancing procedures</li>\n</ul>\n</blockquote>\n<p><strong>Tenant-Based Sharding:</strong></p>\n<p>Multi-tenant deployments benefit from tenant-based sharding that provides natural isolation boundaries and simplified quota enforcement. Large tenants can occupy dedicated nodes while smaller tenants share resources.</p>\n<table>\n<thead>\n<tr>\n<th>Tenant Strategy</th>\n<th>Isolation Level</th>\n<th>Resource Efficiency</th>\n<th>Operational Overhead</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tenant-per-Node</td>\n<td>Perfect isolation</td>\n<td>Low for small tenants</td>\n<td>High node management</td>\n</tr>\n<tr>\n<td>Hash-Based Sharing</td>\n<td>Good isolation</td>\n<td>High efficiency</td>\n<td>Complex quota tracking</td>\n</tr>\n<tr>\n<td>Size-Based Placement</td>\n<td>Balanced approach</td>\n<td>Optimal for mixed workloads</td>\n<td>Dynamic rebalancing needed</td>\n</tr>\n</tbody></table>\n<h4 id=\"data-replication-and-consistency\">Data Replication and Consistency</h4>\n<p>Distributed deployments require replication for both availability and performance. The replication strategy affects data consistency guarantees, query performance, and operational complexity.</p>\n<p><strong>Replication Topologies:</strong></p>\n<p>The system supports multiple replication approaches depending on consistency requirements and performance goals. Asynchronous replication provides better write performance but risks data loss during node failures. Synchronous replication guarantees consistency but increases write latency.</p>\n<table>\n<thead>\n<tr>\n<th>Replication Type</th>\n<th>Consistency Guarantee</th>\n<th>Write Performance</th>\n<th>Read Performance</th>\n<th>Failure Recovery</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Async Primary-Secondary</td>\n<td>Eventually consistent</td>\n<td>High</td>\n<td>Excellent</td>\n<td>Potential data loss</td>\n</tr>\n<tr>\n<td>Sync Primary-Secondary</td>\n<td>Strong consistency</td>\n<td>Moderate</td>\n<td>Good</td>\n<td>Full recovery</td>\n</tr>\n<tr>\n<td>Multi-Primary</td>\n<td>Eventual with conflicts</td>\n<td>High</td>\n<td>Excellent</td>\n<td>Complex conflict resolution</td>\n</tr>\n<tr>\n<td>Quorum-Based</td>\n<td>Tunable consistency</td>\n<td>Configurable</td>\n<td>Configurable</td>\n<td>Automatic failover</td>\n</tr>\n</tbody></table>\n<p>For log aggregation workloads, <strong>asynchronous replication with write-ahead logging</strong> provides the best balance. Logs are immutable once written, eliminating most consistency concerns, while the WAL ensures durability even if replication lags.</p>\n<p><strong>Consensus Integration:</strong></p>\n<p>Critical system metadata (tenant configurations, shard assignments, schema definitions) requires strong consistency through a consensus protocol like Raft. This ensures all nodes have a consistent view of cluster state while allowing log data itself to use simpler replication.</p>\n<p>The consensus layer manages cluster membership, shard assignments, and configuration changes. It operates independently of the log data path to avoid becoming a bottleneck during high ingestion rates.</p>\n<h4 id=\"auto-scaling-and-load-management\">Auto-Scaling and Load Management</h4>\n<p>Production deployments need automatic scaling to handle traffic variations and node failures. The scaling system monitors cluster health and adjusts capacity based on configurable policies.</p>\n<p><strong>Scaling Metrics and Triggers:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Metric Category</th>\n<th>Key Indicators</th>\n<th>Scale-Up Triggers</th>\n<th>Scale-Down Triggers</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Ingestion Load</td>\n<td>Messages/sec, bytes/sec</td>\n<td>&gt;80% capacity for 10 minutes</td>\n<td>&lt;30% capacity for 30 minutes</td>\n</tr>\n<tr>\n<td>Query Performance</td>\n<td>P99 latency, queue depth</td>\n<td>P99 &gt;5 seconds consistently</td>\n<td>P99 &lt;1 second consistently</td>\n</tr>\n<tr>\n<td>Resource Utilization</td>\n<td>CPU, memory, disk I/O</td>\n<td>&gt;70% utilization sustained</td>\n<td>&lt;20% utilization sustained</td>\n</tr>\n<tr>\n<td>Storage Growth</td>\n<td>Disk usage rate, retention efficiency</td>\n<td>&gt;85% disk usage</td>\n<td>Retention policies sufficient</td>\n</tr>\n</tbody></table>\n<p>The auto-scaler considers multiple factors simultaneously to avoid thrashing. Scaling decisions use exponential backoff and require sustained metric violations rather than reacting to temporary spikes.</p>\n<p><strong>Node Addition and Removal:</strong></p>\n<p>Adding nodes to a running cluster requires careful orchestration to maintain availability during the transition. The system uses a gradual migration approach that moves shards incrementally while continuing to serve queries.</p>\n<ol>\n<li><strong>Node Preparation</strong>: New nodes join the cluster in &quot;joining&quot; state and receive cluster metadata</li>\n<li><strong>Shard Assignment</strong>: Cluster coordinator calculates optimal shard redistribution</li>\n<li><strong>Data Migration</strong>: Existing nodes stream relevant data to new nodes while continuing ingestion</li>\n<li><strong>Query Routing Update</strong>: Load balancers gradually shift traffic to include new nodes</li>\n<li><strong>Cleanup</strong>: Original nodes delete migrated data and update local state</li>\n</ol>\n<p>Node removal follows the reverse process, ensuring all data replicates to remaining nodes before the departing node stops serving traffic.</p>\n<h3 id=\"feature-extensions\">Feature Extensions</h3>\n<p>Beyond horizontal scaling, the system architecture can accommodate sophisticated features that transform it from a basic log storage system into an intelligent observability platform. These extensions leverage the solid foundation of ingestion, indexing, and querying while adding new capabilities.</p>\n<h4 id=\"advanced-querying-capabilities\">Advanced Querying Capabilities</h4>\n<p>The basic LogQL implementation provides foundation for much more sophisticated query operations that rival specialized analytics systems. These extensions maintain backward compatibility while enabling complex analysis workflows.</p>\n<p><strong>Aggregation and Analytics Functions:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Function Category</th>\n<th>Examples</th>\n<th>Use Cases</th>\n<th>Implementation Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Statistical Functions</td>\n<td><code>avg()</code>, <code>sum()</code>, <code>percentile()</code>, <code>stddev()</code></td>\n<td>Performance monitoring, SLA tracking</td>\n<td>Stream processing with sliding windows</td>\n</tr>\n<tr>\n<td>Time Series Analysis</td>\n<td><code>rate()</code>, <code>increase()</code>, <code>trend()</code>, <code>seasonal()</code></td>\n<td>Capacity planning, anomaly detection</td>\n<td>Time-bucketed aggregation with interpolation</td>\n</tr>\n<tr>\n<td>Text Analytics</td>\n<td><code>extract()</code>, <code>sentiment()</code>, <code>classify()</code></td>\n<td>Log content analysis, issue categorization</td>\n<td>Regex engines, ML model integration</td>\n</tr>\n<tr>\n<td>Geospatial Functions</td>\n<td><code>geo_distance()</code>, <code>geo_within()</code>, <code>geo_cluster()</code></td>\n<td>Location-based analysis, CDN optimization</td>\n<td>Geohash indexing, spatial data structures</td>\n</tr>\n</tbody></table>\n<p>Advanced aggregation functions operate on log streams using streaming algorithms that maintain accuracy while processing large volumes. The system uses approximate algorithms (HyperLogLog for cardinality, t-digest for percentiles) when exact computation would be prohibitively expensive.</p>\n<p><strong>Cross-Stream Joins and Correlations:</strong></p>\n<p>Real observability requires correlating logs across different services and systems. The query engine extends to support temporal joins that match log entries from different streams based on time windows and common attributes.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>{service=&quot;api&quot;} |= &quot;request_id=123&quot; \n  | join 5m {service=&quot;database&quot;} on request_id\n  | calculate response_time</code></pre></div>\n\n<p>This query finds API logs for request ID 123, joins them with corresponding database logs within a 5-minute window, and calculates end-to-end response time. The join operation requires careful memory management and timeout handling to avoid resource exhaustion on large result sets.</p>\n<p><strong>Saved Queries and Materialized Views:</strong></p>\n<p>Frequently-executed complex queries benefit from materialization that pre-computes results and incrementally updates them as new data arrives. This feature transforms expensive analytics queries into fast lookup operations.</p>\n<table>\n<thead>\n<tr>\n<th>Materialization Type</th>\n<th>Update Strategy</th>\n<th>Query Performance</th>\n<th>Storage Overhead</th>\n<th>Consistency Model</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Periodic Refresh</td>\n<td>Batch recalculation</td>\n<td>Fast reads</td>\n<td>Low</td>\n<td>Eventually consistent</td>\n</tr>\n<tr>\n<td>Incremental Update</td>\n<td>Stream processing</td>\n<td>Fast reads</td>\n<td>Moderate</td>\n<td>Near real-time</td>\n</tr>\n<tr>\n<td>Trigger-Based</td>\n<td>Event-driven</td>\n<td>Fast reads</td>\n<td>High</td>\n<td>Strongly consistent</td>\n</tr>\n</tbody></table>\n<h4 id=\"machine-learning-integration\">Machine Learning Integration</h4>\n<p>The log aggregation system becomes a platform for intelligent log analysis by integrating machine learning capabilities directly into the query and storage pipeline. This integration provides automated insights without requiring data export to external ML systems.</p>\n<p><strong>Anomaly Detection Pipeline:</strong></p>\n<p>Automated anomaly detection operates continuously on log streams to identify unusual patterns that might indicate system problems or security incidents. The detection pipeline uses multiple algorithms tuned for different anomaly types.</p>\n<table>\n<thead>\n<tr>\n<th>Anomaly Type</th>\n<th>Detection Algorithm</th>\n<th>Training Requirements</th>\n<th>False Positive Rate</th>\n<th>Response Time</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Volume Spikes</td>\n<td>Statistical process control</td>\n<td>7 days baseline</td>\n<td>~2%</td>\n<td>Real-time</td>\n</tr>\n<tr>\n<td>Content Anomalies</td>\n<td>Clustering/classification</td>\n<td>Labeled training set</td>\n<td>~5%</td>\n<td>Near real-time</td>\n</tr>\n<tr>\n<td>Temporal Patterns</td>\n<td>Time series decomposition</td>\n<td>30 days seasonal data</td>\n<td>~1%</td>\n<td>Minutes</td>\n</tr>\n<tr>\n<td>Cross-Service Correlations</td>\n<td>Graph neural networks</td>\n<td>Service dependency map</td>\n<td>~3%</td>\n<td>Minutes</td>\n</tr>\n</tbody></table>\n<p>The anomaly detection system learns normal patterns during a training period and then continuously scores incoming logs for deviation from expected behavior. It generates alerts through the existing alerting infrastructure while maintaining detailed model performance metrics.</p>\n<p><strong>Log Classification and Tagging:</strong></p>\n<p>Automated log classification adds semantic tags to log entries based on content analysis and pattern recognition. This capability transforms unstructured log messages into structured, searchable metadata.</p>\n<p>The classification pipeline processes log messages through multiple stages:</p>\n<ol>\n<li><strong>Preprocessing</strong>: Text normalization, tokenization, and feature extraction from log messages</li>\n<li><strong>Model Application</strong>: Pre-trained models identify log types (error, warning, security event, performance metric)</li>\n<li><strong>Confidence Scoring</strong>: Classification confidence determines whether automatic tagging applies</li>\n<li><strong>Human-in-Loop</strong>: Low-confidence classifications route to human reviewers for model improvement</li>\n<li><strong>Tag Application</strong>: High-confidence classifications automatically add structured labels to log entries</li>\n</ol>\n<p>The system supports both general-purpose models (common log patterns across many applications) and tenant-specific models trained on organization-specific log formats and terminology.</p>\n<p><strong>Predictive Analytics:</strong></p>\n<p>Machine learning models can predict future system behavior based on historical log patterns. This capability enables proactive incident response and capacity planning.</p>\n<table>\n<thead>\n<tr>\n<th>Prediction Type</th>\n<th>Input Features</th>\n<th>Prediction Horizon</th>\n<th>Model Type</th>\n<th>Update Frequency</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Service Failures</td>\n<td>Error rates, performance metrics</td>\n<td>30 minutes</td>\n<td>Random Forest</td>\n<td>Hourly</td>\n</tr>\n<tr>\n<td>Capacity Exhaustion</td>\n<td>Resource usage trends</td>\n<td>7 days</td>\n<td>LSTM Neural Network</td>\n<td>Daily</td>\n</tr>\n<tr>\n<td>Security Incidents</td>\n<td>Access patterns, anomaly scores</td>\n<td>1 hour</td>\n<td>Gradient Boosting</td>\n<td>Continuous</td>\n</tr>\n<tr>\n<td>User Experience Impact</td>\n<td>Response times, error distributions</td>\n<td>15 minutes</td>\n<td>Ensemble Model</td>\n<td>Every 5 minutes</td>\n</tr>\n</tbody></table>\n<p>Predictive models integrate with the alerting system to generate proactive notifications when predicted events exceed configured probability thresholds. This enables operations teams to address potential problems before they impact users.</p>\n<h4 id=\"real-time-analytics-dashboard\">Real-Time Analytics Dashboard</h4>\n<p>The query engine extends to support real-time dashboard queries that provide live system visibility through streaming aggregations and visualizations. This capability transforms the log system into a comprehensive observability platform.</p>\n<p><strong>Streaming Aggregation Engine:</strong></p>\n<p>Real-time dashboards require continuous aggregation of log streams to provide up-to-date metrics without overwhelming query performance. The streaming engine maintains sliding window aggregations in memory while persisting longer-term aggregations to storage.</p>\n<table>\n<thead>\n<tr>\n<th>Window Type</th>\n<th>Memory Usage</th>\n<th>Update Latency</th>\n<th>Historical Retention</th>\n<th>Query Performance</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tumbling Windows</td>\n<td>Low</td>\n<td>Batch interval</td>\n<td>Full history in storage</td>\n<td>Fast</td>\n</tr>\n<tr>\n<td>Sliding Windows</td>\n<td>High</td>\n<td>Near real-time</td>\n<td>Limited in memory</td>\n<td>Very fast</td>\n</tr>\n<tr>\n<td>Session Windows</td>\n<td>Variable</td>\n<td>Event-driven</td>\n<td>Session-based retention</td>\n<td>Fast</td>\n</tr>\n</tbody></table>\n<p>The streaming aggregation engine uses approximate algorithms to maintain efficiency at scale. HyperLogLog provides cardinality estimates, Count-Min Sketch tracks frequent items, and t-digest maintains quantile estimates, all with bounded memory usage regardless of data volume.</p>\n<p><strong>Dashboard Query Language:</strong></p>\n<p>Dashboard queries extend LogQL with real-time aggregation functions and visualization hints that guide rendering systems in presenting data effectively.</p>\n<p>Advanced dashboard queries support multi-dimensional drill-down capabilities where users can explore data by clicking on chart elements. The query engine maintains query context and generates appropriate filtered queries for the selected data slice.</p>\n<h4 id=\"data-export-and-integration-apis\">Data Export and Integration APIs</h4>\n<p>Production log systems rarely operate in isolation. The architecture supports comprehensive data export and integration capabilities that connect with existing observability tools, business intelligence systems, and compliance platforms.</p>\n<p><strong>Export Pipeline Architecture:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Export Target</th>\n<th>Data Format</th>\n<th>Delivery Method</th>\n<th>Latency</th>\n<th>Reliability Guarantees</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Warehouses</td>\n<td>Parquet, ORC</td>\n<td>Batch file transfer</td>\n<td>Hours</td>\n<td>Exactly-once delivery</td>\n</tr>\n<tr>\n<td>Stream Processors</td>\n<td>JSON, Avro</td>\n<td>Kafka, Pulsar</td>\n<td>Seconds</td>\n<td>At-least-once delivery</td>\n</tr>\n<tr>\n<td>Alerting Systems</td>\n<td>JSON, XML</td>\n<td>HTTP webhooks</td>\n<td>Real-time</td>\n<td>Best-effort delivery</td>\n</tr>\n<tr>\n<td>Compliance Archives</td>\n<td>Raw logs, JSON</td>\n<td>S3, tape backup</td>\n<td>Minutes</td>\n<td>Guaranteed delivery</td>\n</tr>\n</tbody></table>\n<p>The export pipeline operates independently of the main ingestion and query paths to avoid impacting core system performance. It maintains separate queues and processing threads while sharing the same underlying storage infrastructure.</p>\n<p><strong>API Gateway Integration:</strong></p>\n<p>RESTful APIs enable external systems to query log data programmatically while maintaining security and rate limiting controls. The API gateway provides authentication, authorization, and request transformation capabilities.</p>\n<p>API clients can subscribe to log streams using WebSocket connections that deliver matching log entries in real-time. This capability enables external monitoring systems to react immediately to critical events without polling for updates.</p>\n<p><strong>Webhook and Event Streaming:</strong></p>\n<p>The system supports outbound event streaming that publishes log events to external systems based on configurable rules. This capability enables integration with incident response platforms, notification systems, and business process automation tools.</p>\n<p>Event filtering rules use the same LogQL syntax as interactive queries, ensuring consistency between interactive analysis and automated integrations. The webhook delivery system includes retry logic, dead letter queues, and delivery confirmation to ensure reliable event delivery.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides practical guidance for implementing the scalability and feature extensions described above. The focus is on incremental evolution of the existing single-node system rather than complete architectural rewrites.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Extension Category</th>\n<th>Simple Approach</th>\n<th>Advanced Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Horizontal Scaling</td>\n<td>Docker Swarm with shared storage</td>\n<td>Kubernetes with distributed storage (Ceph/GlusterFS)</td>\n</tr>\n<tr>\n<td>Service Discovery</td>\n<td>Static configuration files</td>\n<td>Consul/etcd with health checking</td>\n</tr>\n<tr>\n<td>Load Balancing</td>\n<td>HAProxy/nginx round-robin</td>\n<td>Envoy with advanced routing and circuit breaking</td>\n</tr>\n<tr>\n<td>Message Queuing</td>\n<td>Redis pub/sub</td>\n<td>Apache Kafka with partitioning</td>\n</tr>\n<tr>\n<td>Consensus Layer</td>\n<td>Single-node SQLite</td>\n<td>Multi-node etcd cluster</td>\n</tr>\n<tr>\n<td>Machine Learning</td>\n<td>Scikit-learn batch processing</td>\n<td>TensorFlow Serving with GPU acceleration</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Prometheus + Grafana</td>\n<td>Comprehensive observability stack (Jaeger, Prometheus, Grafana, AlertManager)</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-extension-structure\">Recommended Extension Structure</h4>\n<p>The extension implementation follows a modular approach that adds new capabilities without disrupting existing functionality:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  cmd/\n    gateway/              ← ingestion gateway nodes\n    coordinator/          ← query coordination service\n    storage/              ← dedicated storage nodes\n  internal/\n    cluster/              ← distributed system coordination\n      consensus.go        ← raft consensus implementation\n      sharding.go         ← shard management and routing\n      membership.go       ← node discovery and health\n    ml/                   ← machine learning pipeline\n      anomaly/            ← anomaly detection models\n      classification/     ← log classification\n      prediction/         ← predictive analytics\n    extensions/           ← feature extension framework\n      dashboards/         ← real-time dashboard support\n      export/             ← data export pipeline\n      api/                ← external API gateway\n  deploy/\n    docker/               ← container deployment configs\n    k8s/                  ← kubernetes manifests\n    terraform/            ← infrastructure as code</code></pre></div>\n\n<h4 id=\"scalability-implementation-skeleton\">Scalability Implementation Skeleton</h4>\n<p>The horizontal scaling implementation starts with service decomposition that separates ingestion, storage, and query concerns:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// ClusterCoordinator manages distributed system state and routing decisions.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// It uses consensus to maintain consistent shard assignments across nodes.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ClusterCoordinator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Initialize consensus layer (etcd/raft) for cluster metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Implement shard assignment algorithm with replication factor</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Add node health monitoring with failure detection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Create shard migration logic for node additions/removals</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Implement query routing to appropriate shard owners</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DistributedQueryEngine coordinates queries across multiple storage nodes.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// It implements scatter-gather pattern with result streaming.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DistributedQueryEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ExecuteQuery</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">query</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ResultStream</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Parse query and extract time range and label selectors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Determine relevant shards using cluster coordinator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Generate sub-queries for each relevant storage node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Execute sub-queries in parallel with timeout handling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Merge partial results maintaining sort order and limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Handle node failures with partial result warnings</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ShardManager handles data distribution and migration across cluster nodes.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// It ensures balanced load while maintaining query performance.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ShardManager</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Implement consistent hashing for shard assignment</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Add replication factor support with rack awareness</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Create migration protocol for adding/removing nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Implement shard splitting for hotspot handling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Add metrics collection for rebalancing decisions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"machine-learning-integration-framework\">Machine Learning Integration Framework</h4>\n<p>The ML integration provides a plugin architecture that allows models to process log streams without blocking ingestion:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// MLPipeline processes log streams through machine learning models.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// It operates asynchronously to avoid impacting ingestion performance.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MLPipeline</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create model registry with versioning and rollback</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Implement stream processing framework for real-time inference</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Add batch processing for model training and evaluation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Create feedback loop for model improvement</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Implement A/B testing for model performance comparison</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AnomalyDetector identifies unusual patterns in log streams.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// It maintains baseline models and generates alerts for deviations.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AnomalyDetector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ProcessLogEntry</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">entry</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">LogEntry</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AnomalyScore</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Extract features from log entry (timestamp, content, labels)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Apply statistical models to detect volume anomalies</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Run content analysis for unusual message patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Calculate composite anomaly score with confidence intervals</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Generate alerts when score exceeds configured thresholds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Update baseline models with confirmed normal behavior</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ClassificationEngine adds semantic tags to log entries automatically.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// It uses pre-trained models and human feedback for continuous improvement.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ClassificationEngine</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Load pre-trained classification models from storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Implement text preprocessing pipeline</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Apply models with confidence scoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Route low-confidence predictions to human reviewers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Update models based on feedback and new training data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"extension-milestone-checkpoints\">Extension Milestone Checkpoints</h4>\n<p><strong>Checkpoint 1: Basic Horizontal Scaling</strong></p>\n<ul>\n<li>Deploy 3-node cluster with shared storage backend</li>\n<li>Verify log ingestion distributes across all nodes</li>\n<li>Execute queries that span multiple nodes and return correct results</li>\n<li>Test node failure scenarios with automatic failover</li>\n</ul>\n<p><strong>Checkpoint 2: Advanced Sharding</strong></p>\n<ul>\n<li>Implement hybrid time-label sharding with configurable policies</li>\n<li>Verify queries target minimal set of relevant shards</li>\n<li>Test shard rebalancing when adding/removing nodes</li>\n<li>Measure query performance improvement from shard pruning</li>\n</ul>\n<p><strong>Checkpoint 3: Machine Learning Pipeline</strong></p>\n<ul>\n<li>Deploy anomaly detection models on sample log streams</li>\n<li>Generate alerts for injected anomalous patterns</li>\n<li>Verify classification engine adds appropriate semantic tags</li>\n<li>Test model updates and A/B deployment scenarios</li>\n</ul>\n<p><strong>Checkpoint 4: Real-time Analytics</strong></p>\n<ul>\n<li>Create streaming dashboard with live log volume and error rates</li>\n<li>Verify dashboard updates within 5 seconds of log ingestion</li>\n<li>Test drill-down capabilities from dashboard visualizations</li>\n<li>Measure memory usage of streaming aggregation windows</li>\n</ul>\n<h4 id=\"common-extension-pitfalls\">Common Extension Pitfalls</h4>\n<p>⚠️ <strong>Pitfall: Premature Distribution</strong>\nAvoid implementing distributed features before understanding single-node bottlenecks. Many performance problems stem from inefficient algorithms rather than resource constraints. Profile the single-node implementation thoroughly and optimize hot paths before adding distribution complexity.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Sharding</strong>\nChanging sharding strategies after deployment requires expensive data migration. Choose sharding approaches based on actual query patterns from production workloads, not theoretical optimization. Implement shard rebalancing from the beginning rather than adding it later.</p>\n<p>⚠️ <strong>Pitfall: ML Model Drift</strong>\nMachine learning models degrade over time as log patterns change. Implement continuous model evaluation and automated retraining pipelines from the beginning. Monitor model performance metrics and alert when accuracy drops below acceptable thresholds.</p>\n<p>⚠️ <strong>Pitfall: Resource Contention</strong>\nExtension features can interfere with core log processing if they share computational resources. Use separate thread pools, memory allocation, and I/O bandwidth for extension features. Implement circuit breakers that disable extensions during resource pressure.</p>\n<p>⚠️ <strong>Pitfall: Data Export Bottlenecks</strong>\nLarge-scale data export can overwhelm storage systems and impact query performance. Implement throttling, backpressure, and priority queuing for export operations. Use separate storage replicas for export workloads when possible.</p>\n<p>The extension architecture maintains backward compatibility while enabling sophisticated capabilities. Start with simple implementations and gradually add complexity based on actual operational requirements rather than speculative feature needs.</p>\n<h2 id=\"glossary\">Glossary</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section provides comprehensive definitions for all technical terms, acronyms, and domain-specific vocabulary used throughout the design document, applying to all milestones (1-5).</p>\n</blockquote>\n<h3 id=\"mental-model-the-technical-dictionary\">Mental Model: The Technical Dictionary</h3>\n<p>Think of this glossary as the technical dictionary for our log aggregation system - just as a good dictionary doesn&#39;t just provide word definitions but explains etymology, usage examples, and relationships between concepts, this glossary serves as your comprehensive reference for understanding not just what each term means, but how it fits into the broader system architecture and why specific terminology choices matter for clear communication.</p>\n<p>Unlike a simple word list, this glossary organizes terms by their conceptual relationships and provides context about how terms are used specifically within log aggregation systems. Each definition includes not just the meaning, but the practical implications and connections to other system components.</p>\n<h3 id=\"core-system-concepts\">Core System Concepts</h3>\n<p>The foundational vocabulary that defines what we&#39;re building and why it matters.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>log aggregation</strong></td>\n<td>The process of collecting and centralizing log data from multiple distributed sources into a unified system for storage, indexing, and analysis</td>\n<td>Central purpose of our entire system - distinguishes from simple log forwarding by emphasizing centralized processing and analysis capabilities</td>\n</tr>\n<tr>\n<td><strong>LogQL</strong></td>\n<td>Log query language inspired by Grafana Loki that combines label selectors for stream identification with line filters for content matching</td>\n<td>Our query interface standard - provides familiar syntax for users coming from Prometheus/Loki ecosystem</td>\n</tr>\n<tr>\n<td><strong>ingestion pipeline</strong></td>\n<td>Sequence of processing stages that transform incoming raw log data through parsing, validation, buffering, indexing, and storage</td>\n<td>The data transformation backbone - logs enter as raw text/JSON and exit as structured, indexed, searchable entries</td>\n</tr>\n<tr>\n<td><strong>time series data</strong></td>\n<td>Data points indexed primarily by timestamp, where queries frequently filter by time ranges</td>\n<td>Logs are inherently time-ordered, making temporal indexing and partitioning critical for performance</td>\n</tr>\n<tr>\n<td><strong>structured logging</strong></td>\n<td>Log format where each entry contains well-defined fields (timestamp, level, service, message) rather than free-form text</td>\n<td>Enables efficient parsing and indexing - our system converts unstructured logs into this format</td>\n</tr>\n<tr>\n<td><strong>label</strong></td>\n<td>Key-value metadata pair attached to log entries (e.g., service=api, level=error) that enables filtering and grouping</td>\n<td>Primary indexing and querying mechanism - distinguishes our approach from full-text search engines</td>\n</tr>\n<tr>\n<td><strong>stream</strong></td>\n<td>Sequence of log entries sharing identical label sets, representing logs from a specific source configuration</td>\n<td>Fundamental unit of log organization - queries select streams first, then filter within them</td>\n</tr>\n</tbody></table>\n<h3 id=\"data-structures-and-storage\">Data Structures and Storage</h3>\n<p>Terms describing how we organize and persist log data.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>LogEntry</strong></td>\n<td>Core data structure containing timestamp, labels map, and message string representing a single log event</td>\n<td>Central data type flowing through entire system from ingestion to query response</td>\n</tr>\n<tr>\n<td><strong>Labels</strong></td>\n<td>Map[string]string containing metadata key-value pairs that identify and categorize log entries</td>\n<td>Primary mechanism for log organization and query filtering - kept separate from message content</td>\n</tr>\n<tr>\n<td><strong>LogStream</strong></td>\n<td>Collection of log entries sharing identical label sets, representing a distinct log source</td>\n<td>Organizational unit that groups related log entries for efficient storage and querying</td>\n</tr>\n<tr>\n<td><strong>chunk</strong></td>\n<td>Time-windowed compressed storage unit containing multiple log entries, typically covering 1-hour to 1-day periods</td>\n<td>Physical storage unit that balances compression efficiency with query performance</td>\n</tr>\n<tr>\n<td><strong>chunk boundaries</strong></td>\n<td>Time windows that define how logs are grouped into chunks, affecting compression ratios and query performance</td>\n<td>Critical for balancing storage efficiency (larger chunks compress better) with query latency (smaller chunks reduce scan overhead)</td>\n</tr>\n<tr>\n<td><strong>TimeRange</strong></td>\n<td>Data structure defining start and end timestamps for queries or storage operations</td>\n<td>Enables time-based query optimization and storage partitioning</td>\n</tr>\n<tr>\n<td><strong>EntryReference</strong></td>\n<td>Pointer structure containing chunk ID, offset, and timestamp for locating specific log entries within storage</td>\n<td>Enables inverted index to reference actual log content without duplicating data</td>\n</tr>\n<tr>\n<td><strong>stream-level compression</strong></td>\n<td>Technique of compressing log streams separately within chunks to improve compression ratios for similar content</td>\n<td>Exploits similarity within streams (same service generates similar log patterns) for better compression</td>\n</tr>\n<tr>\n<td><strong>retention policy</strong></td>\n<td>Configurable rules defining how long logs are stored before automatic deletion based on age, size, or other criteria</td>\n<td>Manages storage growth and compliance requirements - can be global or per-tenant/stream</td>\n</tr>\n</tbody></table>\n<h3 id=\"indexing-and-search\">Indexing and Search</h3>\n<p>Vocabulary for our search and indexing mechanisms.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>inverted index</strong></td>\n<td>Data structure mapping each unique term to the list of documents (log entries) containing that term</td>\n<td>Enables fast full-text search by avoiding sequential scans through all log content</td>\n</tr>\n<tr>\n<td><strong>PostingsList</strong></td>\n<td>Array of entry references for a specific term in the inverted index, sorted by timestamp</td>\n<td>Core index data structure that allows term lookups to return relevant log entries efficiently</td>\n</tr>\n<tr>\n<td><strong>bloom filter</strong></td>\n<td>Probabilistic data structure that can quickly determine if an element is definitely NOT in a set</td>\n<td>Optimizes negative lookups - if bloom filter says term isn&#39;t present, skip expensive index lookup</td>\n</tr>\n<tr>\n<td><strong>false positive</strong></td>\n<td>When bloom filter incorrectly reports that an element might be present in the set</td>\n<td>Acceptable trade-off - requires verification against actual index, but false positives are rare with proper tuning</td>\n</tr>\n<tr>\n<td><strong>false negative</strong></td>\n<td>Bloom filter incorrectly reporting that a present element is absent - impossible by design</td>\n<td>Mathematical impossibility for bloom filters - they never miss actual matches, only add extra candidates</td>\n</tr>\n<tr>\n<td><strong>label cardinality</strong></td>\n<td>Number of unique values for a specific label key across all log entries</td>\n<td>Critical metric for index sizing - high cardinality labels (like request_id) can cause index explosion</td>\n</tr>\n<tr>\n<td><strong>cardinality explosion</strong></td>\n<td>Exponential growth of unique label combinations that overwhelms indexing and storage systems</td>\n<td>Primary performance threat - must limit high-cardinality labels or use specialized handling</td>\n</tr>\n<tr>\n<td><strong>time-based partitioning</strong></td>\n<td>Strategy of organizing indexes and storage by time windows to enable efficient time-range queries</td>\n<td>Allows queries to scan only relevant time periods instead of entire dataset</td>\n</tr>\n<tr>\n<td><strong>IndexSegment</strong></td>\n<td>Self-contained portion of inverted index covering specific time range with its own terms map and bloom filters</td>\n<td>Unit of index management that enables parallel processing and incremental updates</td>\n</tr>\n</tbody></table>\n<h3 id=\"query-processing\">Query Processing</h3>\n<p>Terms related to how we parse, optimize, and execute queries.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>AST</strong></td>\n<td>Abstract syntax tree representing the parsed structure of a LogQL query with operators and operands</td>\n<td>Intermediate representation that enables query optimization and execution planning</td>\n</tr>\n<tr>\n<td><strong>label selector</strong></td>\n<td>LogQL component that specifies which log streams to query using label matching criteria</td>\n<td>First stage of query execution - narrows search space before applying content filters</td>\n</tr>\n<tr>\n<td><strong>line filter</strong></td>\n<td>LogQL component that matches against log message content using text search, regex, or other patterns</td>\n<td>Second stage that examines actual log content within selected streams</td>\n</tr>\n<tr>\n<td><strong>predicate pushdown</strong></td>\n<td>Query optimization technique that moves filter conditions as early as possible in the execution pipeline</td>\n<td>Reduces data processed at each stage by eliminating irrelevant entries early</td>\n</tr>\n<tr>\n<td><strong>cursor-based pagination</strong></td>\n<td>Pagination approach using opaque position tokens instead of numeric offsets for efficient large result traversal</td>\n<td>Handles large result sets without expensive offset-based scanning</td>\n</tr>\n<tr>\n<td><strong>streaming execution</strong></td>\n<td>Processing query results incrementally without materializing the complete result set in memory</td>\n<td>Enables bounded memory usage for large queries and faster time-to-first-result</td>\n</tr>\n<tr>\n<td><strong>Token</strong></td>\n<td>Lexical unit produced by query parser representing keywords, operators, strings, or identifiers</td>\n<td>Building blocks of query parsing - lexer converts input string into token sequence</td>\n</tr>\n<tr>\n<td><strong>Lexer</strong></td>\n<td>Component that breaks LogQL query strings into tokens for parsing</td>\n<td>First stage of query processing that handles syntax recognition and basic validation</td>\n</tr>\n<tr>\n<td><strong>ResultStream</strong></td>\n<td>Channel-based structure for streaming query results back to clients with metadata</td>\n<td>Enables incremental result delivery and client-controlled result consumption</td>\n</tr>\n</tbody></table>\n<h3 id=\"network-and-protocol-terms\">Network and Protocol Terms</h3>\n<p>Vocabulary for how logs enter our system.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>syslog</strong></td>\n<td>Standardized network protocol (RFC 3164/5424) for transmitting log messages over TCP/UDP</td>\n<td>Industry standard for log transmission - our system must parse both legacy and modern syslog formats</td>\n</tr>\n<tr>\n<td><strong>backpressure</strong></td>\n<td>Flow control mechanism activated when downstream components cannot keep up with incoming request rate</td>\n<td>Prevents system overload by slowing or buffering inputs when processing capacity is exceeded</td>\n</tr>\n<tr>\n<td><strong>ring buffer</strong></td>\n<td>Circular buffer data structure with fixed capacity that overwrites oldest entries when full</td>\n<td>Provides bounded memory buffering for log ingestion with predictable memory usage</td>\n</tr>\n<tr>\n<td><strong>buffering</strong></td>\n<td>Temporary storage of log entries to handle burst traffic and smooth out processing load</td>\n<td>Critical for handling traffic spikes - prevents dropping logs during temporary overload</td>\n</tr>\n<tr>\n<td><strong>protocol handler</strong></td>\n<td>Component responsible for receiving and parsing log data from specific network protocols (HTTP, TCP, UDP)</td>\n<td>Separates protocol concerns from log processing logic - enables supporting multiple ingestion methods</td>\n</tr>\n</tbody></table>\n<h3 id=\"storage-and-persistence\">Storage and Persistence</h3>\n<p>Terms describing how we durably store log data.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>write-ahead log</strong></td>\n<td>Durable transaction log that records intended operations before they&#39;re applied to main storage</td>\n<td>Ensures no data loss during crashes by allowing replay of committed but unapplied operations</td>\n</tr>\n<tr>\n<td><strong>WAL rotation</strong></td>\n<td>Process of creating new WAL files when current files exceed size limits</td>\n<td>Prevents unbounded WAL growth while maintaining crash recovery capabilities</td>\n</tr>\n<tr>\n<td><strong>WAL replay</strong></td>\n<td>Recovery process that reads WAL records after crash and reapplies any committed operations</td>\n<td>Critical for maintaining data durability guarantees across system failures</td>\n</tr>\n<tr>\n<td><strong>compression ratio</strong></td>\n<td>Percentage of storage space saved through compression algorithms</td>\n<td>Key metric for storage efficiency - log data often achieves 70-90% compression ratios</td>\n</tr>\n<tr>\n<td><strong>retention cleanup</strong></td>\n<td>Automated process that deletes expired chunks according to configured retention policies</td>\n<td>Manages storage growth by removing old data - must coordinate with active queries</td>\n</tr>\n<tr>\n<td><strong>grace period</strong></td>\n<td>Delay between retention policy triggering and actual data deletion</td>\n<td>Safety mechanism that allows recovery from accidental retention policy changes</td>\n</tr>\n<tr>\n<td><strong>reference counting</strong></td>\n<td>Technique tracking how many active queries are accessing each chunk to prevent unsafe deletion</td>\n<td>Prevents data corruption by ensuring chunks aren&#39;t deleted while being read</td>\n</tr>\n<tr>\n<td><strong>ChunkHeader</strong></td>\n<td>Metadata structure containing compression type, entry counts, time ranges, and other chunk information</td>\n<td>Enables efficient chunk processing without decompressing entire contents</td>\n</tr>\n<tr>\n<td><strong>WALRecord</strong></td>\n<td>Individual entry in write-ahead log containing operation type, timestamp, and operation data</td>\n<td>Unit of durability - each record represents one atomic operation that can be replayed</td>\n</tr>\n</tbody></table>\n<h3 id=\"multi-tenancy-and-security\">Multi-Tenancy and Security</h3>\n<p>Terms for isolating different users and organizations.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>multi-tenancy</strong></td>\n<td>Architecture pattern that securely isolates data and resources between different organizations or teams</td>\n<td>Enables SaaS deployments where multiple customers share infrastructure without data leakage</td>\n</tr>\n<tr>\n<td><strong>tenant isolation</strong></td>\n<td>Security mechanisms ensuring that one tenant cannot access another tenant&#39;s logs or affect their performance</td>\n<td>Fundamental security requirement - prevents data breaches and noisy neighbor problems</td>\n</tr>\n<tr>\n<td><strong>rate limiting</strong></td>\n<td>Mechanism controlling the frequency of requests to prevent resource exhaustion</td>\n<td>Protects system stability by preventing any single tenant from overwhelming shared resources</td>\n</tr>\n<tr>\n<td><strong>token bucket</strong></td>\n<td>Algorithm that allows burst traffic while enforcing sustained rate limits over time</td>\n<td>Balances flexibility (allowing occasional spikes) with protection (preventing sustained overload)</td>\n</tr>\n<tr>\n<td><strong>hierarchical rate limiting</strong></td>\n<td>Multi-level rate limiting applied at tenant, stream, and system levels</td>\n<td>Provides granular control over resource allocation at different organizational levels</td>\n</tr>\n<tr>\n<td><strong>TenantContext</strong></td>\n<td>Data structure containing tenant ID, permissions, quotas, and other authorization information</td>\n<td>Carries tenant information through request processing pipeline for access control decisions</td>\n</tr>\n<tr>\n<td><strong>ResourceQuotas</strong></td>\n<td>Limits on ingestion rate, storage usage, query concurrency, and other resources per tenant</td>\n<td>Prevents resource monopolization and enables predictable service levels</td>\n</tr>\n<tr>\n<td><strong>tenant ID injection</strong></td>\n<td>Security attack where malicious clients manipulate tenant identification to access other tenants&#39; data</td>\n<td>Critical vulnerability requiring careful authentication and authorization validation</td>\n</tr>\n</tbody></table>\n<h3 id=\"alerting-and-monitoring\">Alerting and Monitoring</h3>\n<p>Terms for detecting and responding to log patterns.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>log-based alerting</strong></td>\n<td>System that triggers notifications based on patterns detected in incoming log streams</td>\n<td>Enables proactive incident response by detecting problems through log analysis</td>\n</tr>\n<tr>\n<td><strong>alert deduplication</strong></td>\n<td>Process preventing multiple notifications for the same underlying issue</td>\n<td>Reduces alert fatigue by grouping related alerts and suppressing duplicates</td>\n</tr>\n<tr>\n<td><strong>ThresholdCondition</strong></td>\n<td>Configuration specifying what log patterns trigger alerts (error rate, specific messages, etc.)</td>\n<td>Defines the logic for when alerts should fire based on log content or frequency</td>\n</tr>\n<tr>\n<td><strong>AlertRule</strong></td>\n<td>Complete alert specification including query, condition, notification settings, and metadata</td>\n<td>Unit of alert configuration that can be enabled/disabled and modified independently</td>\n</tr>\n<tr>\n<td><strong>alert storm</strong></td>\n<td>Cascading alert generation that overwhelms notification systems with excessive messages</td>\n<td>Dangerous failure mode that can mask real issues - prevented through deduplication and rate limiting</td>\n</tr>\n<tr>\n<td><strong>SlidingWindow</strong></td>\n<td>Time-based buffer that tracks events over a moving time period for alert evaluation</td>\n<td>Enables time-based alert conditions like &quot;more than 10 errors in 5 minutes&quot;</td>\n</tr>\n</tbody></table>\n<h3 id=\"reliability-and-error-handling\">Reliability and Error Handling</h3>\n<p>Terms for building robust, fault-tolerant systems.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>graceful degradation</strong></td>\n<td>System behavior that maintains reduced functionality when components fail</td>\n<td>Preferred failure mode that keeps serving core functionality even with partial outages</td>\n</tr>\n<tr>\n<td><strong>circuit breaker</strong></td>\n<td>Design pattern that prevents cascading failures by failing fast when downstream services are unhealthy</td>\n<td>Protects system stability by avoiding expensive operations that are likely to fail</td>\n</tr>\n<tr>\n<td><strong>health check</strong></td>\n<td>Automated verification that a component is functioning correctly and can serve requests</td>\n<td>Enables monitoring systems to detect problems and route traffic away from failing components</td>\n</tr>\n<tr>\n<td><strong>split-brain</strong></td>\n<td>Scenario where network partitions create multiple independent views of system state</td>\n<td>Dangerous condition requiring careful design to prevent data corruption and conflicts</td>\n</tr>\n<tr>\n<td><strong>cascade failure</strong></td>\n<td>Failure propagation pattern where one component failure causes other components to fail</td>\n<td>System-level failure mode requiring circuit breakers, bulkheads, and other resilience patterns</td>\n</tr>\n<tr>\n<td><strong>HealthStatus</strong></td>\n<td>Enumeration representing component health states (healthy, degraded, unhealthy)</td>\n<td>Standardized health representation enabling consistent monitoring and alerting</td>\n</tr>\n<tr>\n<td><strong>CheckResult</strong></td>\n<td>Data structure containing health check outcomes with status, message, and timing information</td>\n<td>Provides detailed health information for diagnostics and automated response</td>\n</tr>\n</tbody></table>\n<h3 id=\"testing-and-debugging\">Testing and Debugging</h3>\n<p>Terms for verifying system correctness and diagnosing issues.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>unit testing</strong></td>\n<td>Component-level testing that verifies individual functions and classes in isolation</td>\n<td>Foundation of testing pyramid - catches bugs early and enables confident refactoring</td>\n</tr>\n<tr>\n<td><strong>integration testing</strong></td>\n<td>End-to-end testing that validates interactions between multiple components</td>\n<td>Verifies that components work together correctly beyond just individual unit correctness</td>\n</tr>\n<tr>\n<td><strong>milestone checkpoints</strong></td>\n<td>Verification points after each development phase confirming expected functionality</td>\n<td>Structured approach to incremental development with clear success criteria</td>\n</tr>\n<tr>\n<td><strong>property-based testing</strong></td>\n<td>Testing approach using randomly generated inputs to verify system invariants</td>\n<td>Finds edge cases that fixed test cases might miss - especially valuable for parsers and indexes</td>\n</tr>\n<tr>\n<td><strong>failure injection</strong></td>\n<td>Testing technique that simulates error conditions to verify recovery mechanisms</td>\n<td>Ensures system behaves correctly under failure conditions, not just happy path scenarios</td>\n</tr>\n<tr>\n<td><strong>log flow tracing</strong></td>\n<td>Debugging technique tracking individual log entries through the entire processing pipeline</td>\n<td>Essential for diagnosing data loss, corruption, or performance bottlenecks</td>\n</tr>\n<tr>\n<td><strong>index health diagnostics</strong></td>\n<td>Specialized debugging procedures for index corruption and performance issues</td>\n<td>Domain-specific debugging addressing common index problems like cardinality explosion</td>\n</tr>\n<tr>\n<td><strong>throughput cascade analysis</strong></td>\n<td>Performance debugging technique measuring throughput at each pipeline stage</td>\n<td>Identifies bottlenecks by comparing input/output rates across system components</td>\n</tr>\n<tr>\n<td><strong>latency percentile analysis</strong></td>\n<td>Performance analysis using different percentile measurements to identify performance patterns</td>\n<td>Reveals different types of performance issues - p50 vs p99 problems often have different causes</td>\n</tr>\n</tbody></table>\n<h3 id=\"performance-and-scalability\">Performance and Scalability</h3>\n<p>Terms describing system performance characteristics and optimization approaches.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>horizontal scaling</strong></td>\n<td>Approach to handling increased load by adding more machines rather than upgrading existing ones</td>\n<td>Enables unlimited scalability by distributing load across multiple nodes</td>\n</tr>\n<tr>\n<td><strong>sharding</strong></td>\n<td>Data partitioning technique that distributes data across multiple storage nodes</td>\n<td>Enables parallel processing and storage distribution for large datasets</td>\n</tr>\n<tr>\n<td><strong>scatter-gather</strong></td>\n<td>Query execution pattern that distributes work across multiple nodes and collects results</td>\n<td>Common pattern in distributed systems for parallelizing query execution</td>\n</tr>\n<tr>\n<td><strong>consensus protocol</strong></td>\n<td>Algorithm ensuring that distributed nodes agree on shared state despite failures</td>\n<td>Required for maintaining data consistency across multiple storage nodes</td>\n</tr>\n<tr>\n<td><strong>replication factor</strong></td>\n<td>Number of copies of each piece of data maintained across different nodes</td>\n<td>Balances availability (more copies survive failures) with storage cost and write complexity</td>\n</tr>\n<tr>\n<td><strong>materialized views</strong></td>\n<td>Pre-computed query results that are updated incrementally as new data arrives</td>\n<td>Optimization technique for frequently-accessed aggregate queries</td>\n</tr>\n<tr>\n<td><strong>streaming aggregation</strong></td>\n<td>Continuous computation over data streams that maintains running totals and statistics</td>\n<td>Enables real-time analytics without storing all individual events</td>\n</tr>\n</tbody></table>\n<h3 id=\"advanced-features-and-extensions\">Advanced Features and Extensions</h3>\n<p>Terms for sophisticated functionality beyond basic log aggregation.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>anomaly detection</strong></td>\n<td>Machine learning technique for identifying unusual patterns in log streams</td>\n<td>Advanced analytics capability that can detect incidents before they cause user-visible problems</td>\n</tr>\n<tr>\n<td><strong>model drift</strong></td>\n<td>Machine learning model accuracy degradation over time as data patterns change</td>\n<td>Common problem requiring model retraining and performance monitoring</td>\n</tr>\n<tr>\n<td><strong>semantic tagging</strong></td>\n<td>Automatic addition of meaningful labels based on log content analysis</td>\n<td>Uses ML to enhance log metadata beyond what&#39;s explicitly provided by log sources</td>\n</tr>\n<tr>\n<td><strong>AnomalyScore</strong></td>\n<td>Data structure containing anomaly probability, confidence level, and contributing factors</td>\n<td>Structured output from anomaly detection that enables automated response and human investigation</td>\n</tr>\n<tr>\n<td><strong>ClassificationEngine</strong></td>\n<td>Component that automatically categorizes log entries into predefined classes</td>\n<td>Enables automatic log organization and filtering based on content analysis</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-and-architecture\">Implementation and Architecture</h3>\n<p>Terms describing how the system is structured and built.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Architecture Decision Record</strong></td>\n<td>Structured documentation capturing the context, options, and rationale behind design choices</td>\n<td>Critical for maintaining design knowledge and understanding trade-offs made during development</td>\n</tr>\n<tr>\n<td><strong>component isolation</strong></td>\n<td>Design principle ensuring components have clear boundaries and minimal coupling</td>\n<td>Enables independent development, testing, and replacement of system parts</td>\n</tr>\n<tr>\n<td><strong>interface segregation</strong></td>\n<td>Principle of defining focused interfaces rather than large monolithic ones</td>\n<td>Makes components more testable and reduces coupling between system parts</td>\n</tr>\n<tr>\n<td><strong>dependency injection</strong></td>\n<td>Pattern where components receive their dependencies rather than creating them internally</td>\n<td>Enables easier testing by allowing mock dependencies and clearer dependency relationships</td>\n</tr>\n<tr>\n<td><strong>configuration management</strong></td>\n<td>System for managing application settings, connection strings, and other operational parameters</td>\n<td>Critical for deployment flexibility and environment-specific customization</td>\n</tr>\n</tbody></table>\n<h3 id=\"common-acronyms-and-abbreviations\">Common Acronyms and Abbreviations</h3>\n<p>Standard abbreviations used throughout the system documentation.</p>\n<table>\n<thead>\n<tr>\n<th>Acronym</th>\n<th>Full Form</th>\n<th>Definition</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>WAL</strong></td>\n<td>Write-Ahead Log</td>\n<td>Durable transaction log ensuring no data loss during failures</td>\n</tr>\n<tr>\n<td><strong>ADR</strong></td>\n<td>Architecture Decision Record</td>\n<td>Structured documentation of design decisions and their rationale</td>\n</tr>\n<tr>\n<td><strong>AST</strong></td>\n<td>Abstract Syntax Tree</td>\n<td>Parsed representation of query structure used for optimization and execution</td>\n</tr>\n<tr>\n<td><strong>TCP</strong></td>\n<td>Transmission Control Protocol</td>\n<td>Reliable network protocol used for syslog and other log transmission</td>\n</tr>\n<tr>\n<td><strong>UDP</strong></td>\n<td>User Datagram Protocol</td>\n<td>Unreliable but fast network protocol used for high-volume log transmission</td>\n</tr>\n<tr>\n<td><strong>HTTP</strong></td>\n<td>HyperText Transfer Protocol</td>\n<td>Web protocol used for REST API log ingestion endpoints</td>\n</tr>\n<tr>\n<td><strong>JSON</strong></td>\n<td>JavaScript Object Notation</td>\n<td>Text-based data interchange format commonly used for structured logs</td>\n</tr>\n<tr>\n<td><strong>RFC</strong></td>\n<td>Request for Comments</td>\n<td>Internet standards documents (RFC 3164/5424 define syslog formats)</td>\n</tr>\n<tr>\n<td><strong>TTL</strong></td>\n<td>Time To Live</td>\n<td>Duration data should be retained before automatic deletion</td>\n</tr>\n<tr>\n<td><strong>SLA</strong></td>\n<td>Service Level Agreement</td>\n<td>Contractual commitment to specific performance and availability levels</td>\n</tr>\n<tr>\n<td><strong>SLI</strong></td>\n<td>Service Level Indicator</td>\n<td>Measurable metric used to assess service performance</td>\n</tr>\n<tr>\n<td><strong>SLO</strong></td>\n<td>Service Level Objective</td>\n<td>Target value for service level indicators</td>\n</tr>\n<tr>\n<td><strong>MTBF</strong></td>\n<td>Mean Time Between Failures</td>\n<td>Average time between system failures</td>\n</tr>\n<tr>\n<td><strong>MTTR</strong></td>\n<td>Mean Time To Recovery</td>\n<td>Average time to restore service after failure</td>\n</tr>\n<tr>\n<td><strong>RTO</strong></td>\n<td>Recovery Time Objective</td>\n<td>Maximum acceptable downtime during disaster recovery</td>\n</tr>\n<tr>\n<td><strong>RPO</strong></td>\n<td>Recovery Point Objective</td>\n<td>Maximum acceptable data loss during disaster recovery</td>\n</tr>\n</tbody></table>\n<h3 id=\"constants-and-configuration-values\">Constants and Configuration Values</h3>\n<p>Standard values and limits used throughout the system.</p>\n<table>\n<thead>\n<tr>\n<th>Constant</th>\n<th>Value</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>HTTP_PORT</strong></td>\n<td>8080</td>\n<td>Default port for HTTP log ingestion endpoint</td>\n</tr>\n<tr>\n<td><strong>TCP_PORT</strong></td>\n<td>1514</td>\n<td>Default port for TCP syslog reception</td>\n</tr>\n<tr>\n<td><strong>UDP_PORT</strong></td>\n<td>1514</td>\n<td>Default port for UDP syslog reception</td>\n</tr>\n<tr>\n<td><strong>BUFFER_SIZE</strong></td>\n<td>10000</td>\n<td>Default size for in-memory log entry buffers</td>\n</tr>\n<tr>\n<td><strong>CHUNK_SIZE</strong></td>\n<td>1MB</td>\n<td>Target size for compressed storage chunks</td>\n</tr>\n<tr>\n<td><strong>RETENTION_DAYS</strong></td>\n<td>30</td>\n<td>Default log retention period</td>\n</tr>\n<tr>\n<td><strong>REPLICATION_FACTOR</strong></td>\n<td>3</td>\n<td>Default number of data replicas in distributed deployment</td>\n</tr>\n<tr>\n<td><strong>ML_BATCH_SIZE</strong></td>\n<td>1000</td>\n<td>Batch size for machine learning model inference</td>\n</tr>\n<tr>\n<td><strong>ANOMALY_THRESHOLD</strong></td>\n<td>0.95</td>\n<td>Default threshold for anomaly detection alerts</td>\n</tr>\n</tbody></table>\n<h3 id=\"design-patterns-and-principles\">Design Patterns and Principles</h3>\n<p>Fundamental patterns used throughout the system architecture.</p>\n<table>\n<thead>\n<tr>\n<th>Pattern</th>\n<th>Description</th>\n<th>Application</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Publisher-Subscriber</strong></td>\n<td>Decoupled communication where producers send events without knowing consumers</td>\n<td>Used for log ingestion pipeline stages and alert notification delivery</td>\n</tr>\n<tr>\n<td><strong>Command Pattern</strong></td>\n<td>Encapsulating operations as objects that can be queued, logged, and undone</td>\n<td>WAL records represent commands that can be replayed for crash recovery</td>\n</tr>\n<tr>\n<td><strong>Observer Pattern</strong></td>\n<td>Automatic notification of dependent objects when state changes</td>\n<td>Health monitoring and metrics collection throughout the system</td>\n</tr>\n<tr>\n<td><strong>Factory Pattern</strong></td>\n<td>Creating objects without specifying their exact classes</td>\n<td>Parser factory creates appropriate parsers based on log format detection</td>\n</tr>\n<tr>\n<td><strong>Strategy Pattern</strong></td>\n<td>Selecting algorithms at runtime based on context</td>\n<td>Compression algorithm selection and query optimization strategies</td>\n</tr>\n<tr>\n<td><strong>Circuit Breaker Pattern</strong></td>\n<td>Preventing cascading failures by failing fast when services are unhealthy</td>\n<td>Protecting against downstream service failures and resource exhaustion</td>\n</tr>\n<tr>\n<td><strong>Bulkhead Pattern</strong></td>\n<td>Isolating resources to prevent total system failure</td>\n<td>Tenant isolation and resource quotas prevent noisy neighbor problems</td>\n</tr>\n</tbody></table>\n<h3 id=\"performance-metrics-and-measurements\">Performance Metrics and Measurements</h3>\n<p>Key metrics for understanding system behavior and performance.</p>\n<table>\n<thead>\n<tr>\n<th>Metric Category</th>\n<th>Examples</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Throughput</strong></td>\n<td>logs/second, queries/second, bytes/second</td>\n<td>Measures system capacity and utilization</td>\n</tr>\n<tr>\n<td><strong>Latency</strong></td>\n<td>p50/p95/p99 response times, ingestion delay</td>\n<td>Measures user experience and system responsiveness</td>\n</tr>\n<tr>\n<td><strong>Resource Usage</strong></td>\n<td>CPU%, memory usage, disk I/O, network bandwidth</td>\n<td>Identifies bottlenecks and capacity planning needs</td>\n</tr>\n<tr>\n<td><strong>Error Rates</strong></td>\n<td>failed ingestions, query errors, timeout rates</td>\n<td>Measures system reliability and quality</td>\n</tr>\n<tr>\n<td><strong>Business Metrics</strong></td>\n<td>active tenants, storage growth, query complexity</td>\n<td>Tracks system adoption and resource requirements</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The glossary serves as more than just definitions - it establishes the vocabulary foundation that enables clear communication throughout the project. Understanding these terms deeply helps in several ways: they provide the conceptual framework for understanding system architecture, establish consistent naming conventions that make code more readable, enable precise communication about design decisions and trade-offs, and help identify relationships between different system components.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Documentation</td>\n<td>Markdown files in repository</td>\n<td>GitBook or similar documentation platform</td>\n</tr>\n<tr>\n<td>Term Management</td>\n<td>Manual glossary maintenance</td>\n<td>Automated glossary generation from code comments</td>\n</tr>\n<tr>\n<td>Cross-References</td>\n<td>Manual linking between terms</td>\n<td>Automated cross-reference detection and linking</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>docs/\n  glossary.md              ← this comprehensive glossary\n  architecture/\n    data-model.md          ← detailed data structure documentation\n    component-interfaces.md ← API and interface specifications\n  operations/\n    debugging-guide.md     ← troubleshooting procedures\n    performance-tuning.md  ← optimization techniques</code></pre></div>\n\n<h4 id=\"glossary-maintenance-guidelines\">Glossary Maintenance Guidelines</h4>\n<p>Maintaining a comprehensive glossary requires ongoing attention as the system evolves. New features introduce new terminology that must be clearly defined and consistently used. Existing definitions may need refinement as understanding deepens or requirements change. Cross-references between terms should be verified and updated when related concepts change.</p>\n<p>The glossary should be treated as a living document that grows with the system. Each new component or feature should contribute its key terms with clear definitions. Ambiguous or overloaded terms should be identified and disambiguated. Technical debt in terminology - where the same concept is referred to by multiple names - should be regularly cleaned up.</p>\n<h4 id=\"common-documentation-pitfalls\">Common Documentation Pitfalls</h4>\n<p>⚠️ <strong>Pitfall: Circular Definitions</strong>\nDefining terms using other undefined terms creates confusion rather than clarity. Each definition should be self-contained or reference only previously defined terms. When circular dependencies are unavoidable, provide a brief informal explanation before introducing the formal definitions.</p>\n<p>⚠️ <strong>Pitfall: Implementation-Specific Definitions</strong>\nDefining terms in ways that are tied to specific implementation choices limits reusability and understanding. Focus on the conceptual meaning rather than how something is implemented in a particular programming language or framework.</p>\n<p>⚠️ <strong>Pitfall: Missing Context</strong>\nTechnical terms often have different meanings in different domains. A &quot;stream&quot; in our log aggregation context is different from a &quot;stream&quot; in general programming or database contexts. Always provide the domain-specific context for how terms are used in this system.</p>\n<p>⚠️ <strong>Pitfall: Stale Definitions</strong>\nGlossaries that aren&#39;t maintained become misleading as systems evolve. Establish a review process that updates definitions when the underlying concepts change. Consider automated checks that flag potential inconsistencies between code and documentation.</p>\n<h4 id=\"debugging-terminology-issues\">Debugging Terminology Issues</h4>\n<p>When team members use different terms for the same concept or misunderstand established terminology, it creates communication barriers that slow development and increase bugs. Here&#39;s how to diagnose and fix terminology problems:</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Code reviews with terminology debates</td>\n<td>Missing or unclear definitions</td>\n<td>Check if disputed terms are in glossary</td>\n<td>Add missing definitions with clear examples</td>\n</tr>\n<tr>\n<td>Bug reports using inconsistent language</td>\n<td>Team members have different mental models</td>\n<td>Review related documentation and code comments</td>\n<td>Standardize terminology and update all references</td>\n</tr>\n<tr>\n<td>New team members asking same questions repeatedly</td>\n<td>Key concepts not clearly documented</td>\n<td>Track frequently asked questions</td>\n<td>Expand glossary with commonly misunderstood terms</td>\n</tr>\n<tr>\n<td>Design discussions going in circles</td>\n<td>Participants using same words for different concepts</td>\n<td>Map out what each person means by key terms</td>\n<td>Disambiguate overloaded terms with precise definitions</td>\n</tr>\n</tbody></table>\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p>After implementing each major milestone, verify that the associated terminology is clearly understood and consistently used:</p>\n<p><strong>Milestone 1 Checkpoint</strong>: Team can clearly distinguish between log entries, streams, and labels. Code reviews use consistent terminology for ingestion pipeline stages.</p>\n<p><strong>Milestone 2 Checkpoint</strong>: Discussions about indexing use precise terms for inverted indexes, bloom filters, and partitioning strategies without confusion.</p>\n<p><strong>Milestone 3 Checkpoint</strong>: Query-related conversations distinguish clearly between lexing, parsing, optimization, and execution phases.</p>\n<p><strong>Milestone 4 Checkpoint</strong>: Storage discussions precisely differentiate between chunks, WAL records, compression, and retention policies.</p>\n<p><strong>Milestone 5 Checkpoint</strong>: Multi-tenancy and alerting terminology is used consistently across security discussions and implementation.</p>\n<p>The glossary is complete when team members can communicate complex system concepts clearly and unambiguously using the established vocabulary. This creates a foundation for maintainable code, effective debugging, and successful knowledge transfer to new team members.</p>\n","toc":[{"level":1,"text":"Log Aggregation System: Design Document","id":"log-aggregation-system-design-document"},{"level":2,"text":"Overview","id":"overview"},{"level":2,"text":"Context and Problem Statement","id":"context-and-problem-statement"},{"level":3,"text":"Mental Model: The Library Analogy","id":"mental-model-the-library-analogy"},{"level":3,"text":"Existing Approaches Comparison","id":"existing-approaches-comparison"},{"level":4,"text":"ELK Stack (Elasticsearch, Logstash, Kibana)","id":"elk-stack-elasticsearch-logstash-kibana"},{"level":4,"text":"Splunk","id":"splunk"},{"level":4,"text":"Grafana Loki","id":"grafana-loki"},{"level":4,"text":"Architectural Trade-off Analysis","id":"architectural-trade-off-analysis"},{"level":3,"text":"Core Technical Challenges","id":"core-technical-challenges"},{"level":4,"text":"Challenge 1: High-Velocity Log Ingestion","id":"challenge-1-high-velocity-log-ingestion"},{"level":4,"text":"Challenge 2: Efficient Label-Based Indexing","id":"challenge-2-efficient-label-based-indexing"},{"level":4,"text":"Challenge 3: Query Performance Optimization","id":"challenge-3-query-performance-optimization"},{"level":4,"text":"Challenge 4: Storage Efficiency and Durability","id":"challenge-4-storage-efficiency-and-durability"},{"level":4,"text":"Challenge 5: System Coordination and Consistency","id":"challenge-5-system-coordination-and-consistency"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Project Structure","id":"recommended-project-structure"},{"level":4,"text":"Core Data Types (Shared Foundation)","id":"core-data-types-shared-foundation"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Milestone Checkpoint Guidelines","id":"milestone-checkpoint-guidelines"},{"level":4,"text":"Language-Specific Implementation Hints","id":"language-specific-implementation-hints"},{"level":2,"text":"Goals and Non-Goals","id":"goals-and-non-goals"},{"level":3,"text":"Mental Model: The Mission Statement","id":"mental-model-the-mission-statement"},{"level":3,"text":"Functional Goals","id":"functional-goals"},{"level":4,"text":"Primary Log Ingestion Capabilities","id":"primary-log-ingestion-capabilities"},{"level":4,"text":"Efficient Log Indexing and Storage","id":"efficient-log-indexing-and-storage"},{"level":4,"text":"Powerful Query Language and Execution","id":"powerful-query-language-and-execution"},{"level":4,"text":"Robust Storage and Retention Management","id":"robust-storage-and-retention-management"},{"level":3,"text":"Non-Functional Goals","id":"non-functional-goals"},{"level":4,"text":"Performance and Scalability Requirements","id":"performance-and-scalability-requirements"},{"level":4,"text":"Reliability and Durability Guarantees","id":"reliability-and-durability-guarantees"},{"level":4,"text":"Operational and Monitoring Requirements","id":"operational-and-monitoring-requirements"},{"level":3,"text":"Explicit Non-Goals","id":"explicit-non-goals"},{"level":4,"text":"Real-Time Analytics and Complex Processing","id":"real-time-analytics-and-complex-processing"},{"level":4,"text":"Advanced Distributed System Features","id":"advanced-distributed-system-features"},{"level":4,"text":"Enterprise Integration and Security Features","id":"enterprise-integration-and-security-features"},{"level":4,"text":"Performance Optimization Beyond Core Requirements","id":"performance-optimization-beyond-core-requirements"},{"level":3,"text":"Architecture Decision Records","id":"architecture-decision-records"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Project Structure","id":"recommended-project-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeletons","id":"core-logic-skeletons"},{"level":4,"text":"Language-Specific Implementation Hints","id":"language-specific-implementation-hints"},{"level":4,"text":"Milestone Checkpoint Verification","id":"milestone-checkpoint-verification"},{"level":2,"text":"High-Level Architecture","id":"high-level-architecture"},{"level":3,"text":"Component Overview","id":"component-overview"},{"level":4,"text":"Ingestion Engine","id":"ingestion-engine"},{"level":4,"text":"Parser Engine","id":"parser-engine"},{"level":4,"text":"Index Engine","id":"index-engine"},{"level":4,"text":"Storage Engine","id":"storage-engine"},{"level":4,"text":"Query Engine","id":"query-engine"},{"level":3,"text":"Data Flow Architecture","id":"data-flow-architecture"},{"level":4,"text":"Ingestion to Storage Flow","id":"ingestion-to-storage-flow"},{"level":4,"text":"Query Processing Flow","id":"query-processing-flow"},{"level":3,"text":"Recommended Project Structure","id":"recommended-project-structure"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"File Structure Foundation","id":"file-structure-foundation"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeletons","id":"core-logic-skeletons"},{"level":4,"text":"Language-Specific Implementation Hints","id":"language-specific-implementation-hints"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":2,"text":"Data Model","id":"data-model"},{"level":3,"text":"Mental Model: The Postal Service Data System","id":"mental-model-the-postal-service-data-system"},{"level":3,"text":"Core Data Types","id":"core-data-types"},{"level":4,"text":"LogEntry Structure","id":"logentry-structure"},{"level":4,"text":"Labels Structure","id":"labels-structure"},{"level":4,"text":"TimeRange Structure","id":"timerange-structure"},{"level":4,"text":"LogStream Structure","id":"logstream-structure"},{"level":3,"text":"Index Data Structures","id":"index-data-structures"},{"level":4,"text":"Inverted Index Structure","id":"inverted-index-structure"},{"level":4,"text":"Bloom Filter Implementation","id":"bloom-filter-implementation"},{"level":4,"text":"Time-Based Partitioning Metadata","id":"time-based-partitioning-metadata"},{"level":3,"text":"Storage and Serialization Formats","id":"storage-and-serialization-formats"},{"level":4,"text":"Chunk Storage Format","id":"chunk-storage-format"},{"level":4,"text":"Compression Strategy Analysis","id":"compression-strategy-analysis"},{"level":4,"text":"Write-Ahead Log Format","id":"write-ahead-log-format"},{"level":4,"text":"Serialization Protocols","id":"serialization-protocols"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Core Data Types Implementation","id":"core-data-types-implementation"},{"level":4,"text":"Labels Management Utilities","id":"labels-management-utilities"},{"level":4,"text":"TimeRange Operations","id":"timerange-operations"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":2,"text":"Log Ingestion Engine","id":"log-ingestion-engine"},{"level":3,"text":"Mental Model: The Mail Sorting Facility","id":"mental-model-the-mail-sorting-facility"},{"level":3,"text":"Protocol Handlers","id":"protocol-handlers"},{"level":4,"text":"HTTP Log Receiver","id":"http-log-receiver"},{"level":4,"text":"TCP Syslog Receiver","id":"tcp-syslog-receiver"},{"level":4,"text":"UDP Syslog Receiver","id":"udp-syslog-receiver"},{"level":4,"text":"File Tail Agent","id":"file-tail-agent"},{"level":3,"text":"Log Parsing Pipeline","id":"log-parsing-pipeline"},{"level":4,"text":"JSON Log Parsing","id":"json-log-parsing"},{"level":4,"text":"Syslog Message Parsing","id":"syslog-message-parsing"},{"level":4,"text":"Regex-Based Pattern Extraction","id":"regex-based-pattern-extraction"},{"level":3,"text":"Buffering and Backpressure","id":"buffering-and-backpressure"},{"level":4,"text":"Memory Buffer Management","id":"memory-buffer-management"},{"level":4,"text":"Disk-Based Overflow Queues","id":"disk-based-overflow-queues"},{"level":4,"text":"Backpressure Propagation","id":"backpressure-propagation"},{"level":4,"text":"Buffer Health Monitoring","id":"buffer-health-monitoring"},{"level":3,"text":"Architecture Decision Records","id":"architecture-decision-records"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Language-Specific Hints","id":"language-specific-hints"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":2,"text":"Log Indexing Engine","id":"log-indexing-engine"},{"level":3,"text":"Mental Model: The Card Catalog System","id":"mental-model-the-card-catalog-system"},{"level":3,"text":"Inverted Index Design","id":"inverted-index-design"},{"level":3,"text":"Bloom Filter Implementation","id":"bloom-filter-implementation"},{"level":3,"text":"Time-Based Partitioning","id":"time-based-partitioning"},{"level":3,"text":"Index Compaction and Maintenance","id":"index-compaction-and-maintenance"},{"level":3,"text":"Architecture Decision Records","id":"architecture-decision-records"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Query Engine","id":"query-engine"},{"level":3,"text":"Mental Model: The Research Assistant","id":"mental-model-the-research-assistant"},{"level":3,"text":"Query Language Parser","id":"query-language-parser"},{"level":3,"text":"Query Planning and Optimization","id":"query-planning-and-optimization"},{"level":3,"text":"Search Execution Engine","id":"search-execution-engine"},{"level":3,"text":"Result Processing and Pagination","id":"result-processing-and-pagination"},{"level":3,"text":"Architecture Decision Records","id":"architecture-decision-records"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Storage Engine","id":"storage-engine"},{"level":3,"text":"Mental Model: The Archive Warehouse","id":"mental-model-the-archive-warehouse"},{"level":3,"text":"Chunk-Based Storage Design","id":"chunk-based-storage-design"},{"level":3,"text":"Compression Strategy","id":"compression-strategy"},{"level":3,"text":"Write-Ahead Log Implementation","id":"write-ahead-log-implementation"},{"level":3,"text":"Retention Policy Engine","id":"retention-policy-engine"},{"level":3,"text":"Architecture Decision Records","id":"architecture-decision-records"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Multi-Tenancy and Alerting","id":"multi-tenancy-and-alerting"},{"level":3,"text":"Mental Model: The Apartment Building","id":"mental-model-the-apartment-building"},{"level":3,"text":"Tenant Isolation Design","id":"tenant-isolation-design"},{"level":3,"text":"Rate Limiting and Quotas","id":"rate-limiting-and-quotas"},{"level":3,"text":"Log-Based Alerting Engine","id":"log-based-alerting-engine"},{"level":3,"text":"Architecture Decision Records","id":"architecture-decision-records"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Interactions and Data Flow","id":"interactions-and-data-flow"},{"level":3,"text":"Mental Model: The Orchestra Performance","id":"mental-model-the-orchestra-performance"},{"level":3,"text":"Log Ingestion Flow","id":"log-ingestion-flow"},{"level":4,"text":"Request Reception and Protocol Handling","id":"request-reception-and-protocol-handling"},{"level":4,"text":"Log Parsing and Normalization","id":"log-parsing-and-normalization"},{"level":4,"text":"Buffering and Flow Control","id":"buffering-and-flow-control"},{"level":4,"text":"Index Integration and Term Extraction","id":"index-integration-and-term-extraction"},{"level":4,"text":"Storage Coordination and Persistence","id":"storage-coordination-and-persistence"},{"level":3,"text":"Query Processing Flow","id":"query-processing-flow"},{"level":4,"text":"Query Reception and Authentication","id":"query-reception-and-authentication"},{"level":4,"text":"Lexical Analysis and Parsing","id":"lexical-analysis-and-parsing"},{"level":4,"text":"Query Planning and Optimization","id":"query-planning-and-optimization"},{"level":4,"text":"Index Consultation and Reference Resolution","id":"index-consultation-and-reference-resolution"},{"level":4,"text":"Storage Access and Content Filtering","id":"storage-access-and-content-filtering"},{"level":4,"text":"Result Assembly and Streaming","id":"result-assembly-and-streaming"},{"level":3,"text":"Background Maintenance Flows","id":"background-maintenance-flows"},{"level":4,"text":"Index Compaction Operations","id":"index-compaction-operations"},{"level":4,"text":"Retention Policy Enforcement","id":"retention-policy-enforcement"},{"level":4,"text":"Write-Ahead Log Maintenance","id":"write-ahead-log-maintenance"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Language-Specific Hints","id":"language-specific-hints"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Error Handling and Edge Cases","id":"error-handling-and-edge-cases"},{"level":3,"text":"Mental Model: The Hospital Emergency Response System","id":"mental-model-the-hospital-emergency-response-system"},{"level":2,"text":"System Failure Modes","id":"system-failure-modes"},{"level":3,"text":"Network Partition and Connectivity Failures","id":"network-partition-and-connectivity-failures"},{"level":3,"text":"Disk and Storage Failures","id":"disk-and-storage-failures"},{"level":3,"text":"Memory Exhaustion and Resource Limits","id":"memory-exhaustion-and-resource-limits"},{"level":3,"text":"Process and Component Crashes","id":"process-and-component-crashes"},{"level":3,"text":"Downstream Service Dependencies","id":"downstream-service-dependencies"},{"level":2,"text":"Failure Detection and Monitoring","id":"failure-detection-and-monitoring"},{"level":3,"text":"Health Check Implementation","id":"health-check-implementation"},{"level":3,"text":"Metrics Collection and Alerting","id":"metrics-collection-and-alerting"},{"level":3,"text":"Circuit Breaker Patterns","id":"circuit-breaker-patterns"},{"level":2,"text":"Recovery and Resilience Strategies","id":"recovery-and-resilience-strategies"},{"level":3,"text":"Graceful Degradation Patterns","id":"graceful-degradation-patterns"},{"level":3,"text":"Automatic Recovery Mechanisms","id":"automatic-recovery-mechanisms"},{"level":3,"text":"Manual Intervention Procedures","id":"manual-intervention-procedures"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Language-Specific Hints","id":"language-specific-hints"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Testing Strategy","id":"testing-strategy"},{"level":3,"text":"Unit Testing Approach","id":"unit-testing-approach"},{"level":4,"text":"Core Component Testing Strategies","id":"core-component-testing-strategies"},{"level":4,"text":"Ingestion Component Testing","id":"ingestion-component-testing"},{"level":4,"text":"Index Component Testing","id":"index-component-testing"},{"level":4,"text":"Query Engine Testing","id":"query-engine-testing"},{"level":4,"text":"Storage Component Testing","id":"storage-component-testing"},{"level":4,"text":"Multi-Tenancy Component Testing","id":"multi-tenancy-component-testing"},{"level":3,"text":"Integration Testing","id":"integration-testing"},{"level":4,"text":"End-to-End Log Processing Flow","id":"end-to-end-log-processing-flow"},{"level":4,"text":"Cross-Protocol Ingestion Testing","id":"cross-protocol-ingestion-testing"},{"level":4,"text":"Index and Query Consistency Testing","id":"index-and-query-consistency-testing"},{"level":4,"text":"Storage and Recovery Integration Testing","id":"storage-and-recovery-integration-testing"},{"level":4,"text":"Multi-Tenant Integration Testing","id":"multi-tenant-integration-testing"},{"level":3,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Milestone 1: Log Ingestion Checkpoint","id":"milestone-1-log-ingestion-checkpoint"},{"level":4,"text":"Milestone 2: Index Building Checkpoint","id":"milestone-2-index-building-checkpoint"},{"level":4,"text":"Milestone 3: Query Engine Checkpoint","id":"milestone-3-query-engine-checkpoint"},{"level":4,"text":"Milestone 4: Storage and Compression Checkpoint","id":"milestone-4-storage-and-compression-checkpoint"},{"level":4,"text":"Milestone 5: Multi-Tenancy and Alerting Checkpoint","id":"milestone-5-multi-tenancy-and-alerting-checkpoint"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Test Structure","id":"recommended-test-structure"},{"level":4,"text":"Unit Test Infrastructure","id":"unit-test-infrastructure"},{"level":4,"text":"Component Test Skeleton","id":"component-test-skeleton"},{"level":4,"text":"Integration Test Framework","id":"integration-test-framework"},{"level":4,"text":"Milestone Verification Tools","id":"milestone-verification-tools"},{"level":4,"text":"Language-Specific Testing Hints","id":"language-specific-testing-hints"},{"level":2,"text":"Debugging Guide","id":"debugging-guide"},{"level":3,"text":"Mental Model: The Medical Diagnosis Process","id":"mental-model-the-medical-diagnosis-process"},{"level":3,"text":"Symptom-Based Diagnosis","id":"symptom-based-diagnosis"},{"level":3,"text":"Domain-Specific Debugging Techniques","id":"domain-specific-debugging-techniques"},{"level":4,"text":"Log Flow Tracing","id":"log-flow-tracing"},{"level":4,"text":"Index Health Diagnostics","id":"index-health-diagnostics"},{"level":4,"text":"Storage Layer Debugging","id":"storage-layer-debugging"},{"level":4,"text":"Multi-Tenant Debugging","id":"multi-tenant-debugging"},{"level":4,"text":"Query Performance Profiling","id":"query-performance-profiling"},{"level":3,"text":"Performance Problem Diagnosis","id":"performance-problem-diagnosis"},{"level":4,"text":"Throughput Performance Analysis","id":"throughput-performance-analysis"},{"level":4,"text":"Latency Performance Analysis","id":"latency-performance-analysis"},{"level":4,"text":"Memory Performance Analysis","id":"memory-performance-analysis"},{"level":4,"text":"Disk I/O Performance Analysis","id":"disk-io-performance-analysis"},{"level":4,"text":"Network Performance Analysis","id":"network-performance-analysis"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Core Debugging Infrastructure","id":"core-debugging-infrastructure"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Language-Specific Debugging Hints","id":"language-specific-debugging-hints"},{"level":2,"text":"Future Extensions","id":"future-extensions"},{"level":3,"text":"Mental Model: The City Planning System","id":"mental-model-the-city-planning-system"},{"level":3,"text":"Scalability Extensions","id":"scalability-extensions"},{"level":4,"text":"Horizontal Scaling Architecture","id":"horizontal-scaling-architecture"},{"level":4,"text":"Sharding Strategies","id":"sharding-strategies"},{"level":4,"text":"Data Replication and Consistency","id":"data-replication-and-consistency"},{"level":4,"text":"Auto-Scaling and Load Management","id":"auto-scaling-and-load-management"},{"level":3,"text":"Feature Extensions","id":"feature-extensions"},{"level":4,"text":"Advanced Querying Capabilities","id":"advanced-querying-capabilities"},{"level":4,"text":"Machine Learning Integration","id":"machine-learning-integration"},{"level":4,"text":"Real-Time Analytics Dashboard","id":"real-time-analytics-dashboard"},{"level":4,"text":"Data Export and Integration APIs","id":"data-export-and-integration-apis"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Extension Structure","id":"recommended-extension-structure"},{"level":4,"text":"Scalability Implementation Skeleton","id":"scalability-implementation-skeleton"},{"level":4,"text":"Machine Learning Integration Framework","id":"machine-learning-integration-framework"},{"level":4,"text":"Extension Milestone Checkpoints","id":"extension-milestone-checkpoints"},{"level":4,"text":"Common Extension Pitfalls","id":"common-extension-pitfalls"},{"level":2,"text":"Glossary","id":"glossary"},{"level":3,"text":"Mental Model: The Technical Dictionary","id":"mental-model-the-technical-dictionary"},{"level":3,"text":"Core System Concepts","id":"core-system-concepts"},{"level":3,"text":"Data Structures and Storage","id":"data-structures-and-storage"},{"level":3,"text":"Indexing and Search","id":"indexing-and-search"},{"level":3,"text":"Query Processing","id":"query-processing"},{"level":3,"text":"Network and Protocol Terms","id":"network-and-protocol-terms"},{"level":3,"text":"Storage and Persistence","id":"storage-and-persistence"},{"level":3,"text":"Multi-Tenancy and Security","id":"multi-tenancy-and-security"},{"level":3,"text":"Alerting and Monitoring","id":"alerting-and-monitoring"},{"level":3,"text":"Reliability and Error Handling","id":"reliability-and-error-handling"},{"level":3,"text":"Testing and Debugging","id":"testing-and-debugging"},{"level":3,"text":"Performance and Scalability","id":"performance-and-scalability"},{"level":3,"text":"Advanced Features and Extensions","id":"advanced-features-and-extensions"},{"level":3,"text":"Implementation and Architecture","id":"implementation-and-architecture"},{"level":3,"text":"Common Acronyms and Abbreviations","id":"common-acronyms-and-abbreviations"},{"level":3,"text":"Constants and Configuration Values","id":"constants-and-configuration-values"},{"level":3,"text":"Design Patterns and Principles","id":"design-patterns-and-principles"},{"level":3,"text":"Performance Metrics and Measurements","id":"performance-metrics-and-measurements"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Glossary Maintenance Guidelines","id":"glossary-maintenance-guidelines"},{"level":4,"text":"Common Documentation Pitfalls","id":"common-documentation-pitfalls"},{"level":4,"text":"Debugging Terminology Issues","id":"debugging-terminology-issues"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"}],"title":"Log Aggregation System: Design Document","markdown":"# Log Aggregation System: Design Document\n\n\n## Overview\n\nA distributed log aggregation system that collects, indexes, and queries log data at scale, similar to Grafana Loki. The key architectural challenge is efficiently ingesting high-volume log streams while maintaining fast query performance through smart indexing and storage strategies.\n\n\n> This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.\n\n\n## Context and Problem Statement\n\n> **Milestone(s):** This section provides foundational understanding that applies to all milestones (1-5), establishing the problem space and design constraints.\n\n### Mental Model: The Library Analogy\n\nBefore diving into the technical complexities of log aggregation, imagine you're tasked with organizing a vast, ever-growing library that receives thousands of new books every minute, 24 hours a day. These books come in different languages, formats, and subjects, and researchers need to find specific information quickly across millions of volumes.\n\nTraditional libraries solve this through **card catalogs** — organized indexes that tell you exactly which shelf holds the book you need. But imagine if researchers also needed to search for specific phrases within books, or find all books mentioning certain topics during particular time periods. You'd need not just location indexes, but also content indexes, subject indexes, and temporal organization systems.\n\nNow scale this analogy: instead of thousands of books daily, imagine millions arriving every second. Instead of human librarians manually cataloging each book, you need automated systems that can instantly categorize, index, and shelve new arrivals while simultaneously serving hundreds of researchers performing complex searches. The books never stop coming, researchers demand sub-second response times, and you can't afford to lose a single volume.\n\nThis is precisely the challenge of **log aggregation**. Each log entry is like a book arriving at the library. The \"subjects\" are the labels and structured fields (service name, log level, timestamp). The \"content search\" is full-text search across log messages. The \"temporal organization\" is time-based partitioning. The \"card catalog\" is your inverted index, and the \"quick negative lookup\" (knowing a book definitely isn't in a certain section) is provided by bloom filters.\n\nThe fundamental insight from this analogy is that log aggregation systems must solve two seemingly conflicting requirements:\n1. **Continuous high-speed ingestion** — like books arriving faster than you can manually process them\n2. **Interactive query performance** — like researchers expecting instant answers to complex questions\n\n> The core architectural challenge is building a system that can ingest logs at write-optimized speeds while serving queries at read-optimized speeds, despite these two access patterns having fundamentally different performance characteristics.\n\n### Existing Approaches Comparison\n\nThe log aggregation space has evolved through several architectural generations, each making different trade-offs between ingestion performance, query flexibility, storage efficiency, and operational complexity. Understanding these trade-offs is crucial for appreciating why building a Loki-style system presents unique challenges.\n\n#### ELK Stack (Elasticsearch, Logstash, Kibana)\n\nThe **ELK Stack** represents the \"index everything\" approach to log aggregation. Elasticsearch creates full inverted indexes on every field of every log entry, enabling extremely flexible and fast queries at the cost of significant storage overhead and indexing complexity.\n\n| Aspect | ELK Approach | Trade-offs |\n|--------|--------------|------------|\n| **Indexing Strategy** | Full-text index on all fields | Maximum query flexibility but 3-5x storage overhead |\n| **Ingestion Path** | Logstash parsing → ES indexing | Rich transformation capabilities but complex pipeline |\n| **Query Performance** | Sub-second for most queries | Excellent for ad-hoc exploration but expensive for high cardinality |\n| **Storage Model** | Document-oriented with replicas | Easy horizontal scaling but storage costs grow quickly |\n| **Operational Complexity** | Multiple components to manage | Powerful but requires specialized Elasticsearch expertise |\n\nThe ELK approach excels when you need maximum query flexibility and have budget for storage costs. However, it struggles with **high-cardinality labels** (like user IDs or request IDs) because indexing every unique value creates massive indexes. A single label with millions of unique values can make the index larger than the original log data.\n\n#### Splunk\n\n**Splunk** takes a \"store raw, index selectively\" approach. It stores log data in compressed raw format and builds indexes only on selected fields, using a proprietary search language (SPL) for queries.\n\n| Aspect | Splunk Approach | Trade-offs |\n|--------|-----------------|------------|\n| **Indexing Strategy** | Selective indexing with raw storage | Balanced flexibility and cost but requires index planning |\n| **Ingestion Path** | Universal forwarders → indexers | Robust ingestion but proprietary agent deployment |\n| **Query Performance** | Fast for indexed fields, slower for full-text | Predictable performance but requires careful index design |\n| **Storage Model** | Time-based buckets with compression | Excellent compression but vendor lock-in |\n| **Operational Complexity** | Integrated platform | Easier operations but expensive licensing |\n\nSplunk's strength is its mature ecosystem and enterprise features. However, its licensing model based on daily ingestion volume makes it prohibitively expensive for high-volume environments, and its proprietary nature limits customization options.\n\n#### Grafana Loki\n\n**Loki** pioneered the \"index only metadata\" approach, inspired by Prometheus's success with metrics. Instead of indexing log content, it indexes only labels (key-value pairs) and stores log content in compressed chunks. This dramatically reduces index size while maintaining reasonable query performance.\n\n| Aspect | Loki Approach | Trade-offs |\n|--------|---------------|------------|\n| **Indexing Strategy** | Labels only, not log content | Minimal storage overhead but requires structured labels |\n| **Ingestion Path** | Label extraction → chunk storage | Simple pipeline but labels must be designed upfront |\n| **Query Performance** | Fast for label queries, uses grep for content | Efficient for structured logs but slower full-text search |\n| **Storage Model** | Compressed chunks by time and labels | Excellent compression but query performance depends on label design |\n| **Operational Complexity** | Fewer moving parts | Simpler than ELK but requires understanding of label cardinality |\n\nLoki's innovation is recognizing that most log queries follow predictable patterns: filtering by service, environment, or log level, then searching within that filtered set. By indexing only these \"dimensions\" and using efficient grep-like search on compressed chunks, Loki achieves 90% of query use cases with 10% of the storage cost.\n\n> **Decision: Label-Only Indexing Strategy**\n> - **Context**: Need to balance query performance with storage efficiency for high-volume log ingestion\n> - **Options Considered**: Full-text indexing (ELK approach), selective field indexing (Splunk approach), label-only indexing (Loki approach)\n> - **Decision**: Implement label-only indexing similar to Loki\n> - **Rationale**: Full-text indexing creates unsustainable storage overhead for high-volume environments (3-5x data size), while label-only indexing provides sufficient query performance for most use cases with minimal storage overhead\n> - **Consequences**: Enables cost-effective scaling but requires careful label design and slower full-text search performance\n\n#### Architectural Trade-off Analysis\n\n| System | Storage Overhead | Query Flexibility | Ingestion Rate | Operational Complexity | Cost at Scale |\n|--------|------------------|-------------------|----------------|------------------------|---------------|\n| **ELK Stack** | 3-5x original data | Maximum | Medium | High | Very High |\n| **Splunk** | 1.5-2x original data | High | High | Medium | High |\n| **Loki** | 1.1-1.3x original data | Medium | Very High | Low | Low |\n\nThe table reveals why Loki-style systems are attractive for modern cloud environments: they provide the best cost/performance ratio for typical log aggregation workloads. However, this comes with the significant challenge of making label-only indexing work well in practice.\n\n### Core Technical Challenges\n\nBuilding a Loki-style log aggregation system presents several interconnected technical challenges that must be solved simultaneously. These challenges are what make this project compelling — each one requires careful design decisions with non-obvious trade-offs.\n\n#### Challenge 1: High-Velocity Log Ingestion\n\nModern distributed systems generate logs at unprecedented rates. A typical microservices application might produce 100,000+ log entries per second across all services, with burst rates reaching 1 million entries per second during peak traffic or incident scenarios.\n\n**Volume Characteristics:**\n- **Sustained rates**: 10,000-100,000 entries/second typical for medium-scale systems\n- **Burst rates**: 10x-100x sustained rates during traffic spikes or cascading failures\n- **Entry sizes**: 500 bytes to 10KB per entry, with JSON formatting adding 30-50% overhead\n- **Total throughput**: 50MB/second to 1GB/second of raw log data\n\nThe ingestion challenge has multiple dimensions:\n\n**Protocol Handling Complexity**: Logs arrive via multiple protocols (HTTP POST, TCP syslog, UDP syslog, file tailing), each with different reliability and performance characteristics. HTTP provides reliability but higher per-request overhead. UDP provides minimal overhead but no delivery guarantees. TCP syslog balances reliability and performance but requires connection management.\n\n**Parse-Time Pressure**: Each log entry must be parsed to extract labels and structured fields during ingestion, not at query time. This parsing must happen at wire speed without becoming the bottleneck. Regular expressions for unstructured log parsing can consume significant CPU, while JSON parsing must handle malformed inputs gracefully.\n\n**Buffering Strategy Complexity**: The system must buffer incoming logs to smooth out burst rates and handle downstream component failures. Memory buffering provides speed but risks data loss on crashes. Disk buffering provides durability but adds latency and I/O pressure. The system needs hybrid buffering strategies that adapt to load conditions.\n\n> The critical insight for ingestion is that you cannot optimize for the average case — the system must gracefully handle burst rates that are 10x-100x the sustained rate, because these bursts often occur during incidents when log data is most critical.\n\n#### Challenge 2: Efficient Label-Based Indexing\n\nThe label-only indexing approach creates a complex indexing challenge: building indexes that enable fast label-based filtering while avoiding the cardinality explosion that plagues traditional full-text indexing.\n\n**Label Cardinality Problems:**\n- **Low cardinality labels** (service, environment, log_level) are ideal for indexing\n- **High cardinality labels** (user_id, request_id, session_id) create massive indexes\n- **Cardinality explosion** occurs when label combinations create millions of unique label sets\n- **Index size blowup** happens when index metadata becomes larger than the original log data\n\n**Bloom Filter Integration Complexity**: Bloom filters provide fast negative lookups (\"this label combination definitely doesn't exist in this time range\") but introduce probabilistic behavior. False positives mean the system must verify bloom filter hits against actual data. False negative rates must be tuned carefully — too high and the bloom filters become useless, too low and they consume excessive memory.\n\n**Time-Based Partitioning Strategy**: Log data must be partitioned by time to enable efficient time-range queries, but the partitioning granularity affects both query performance and index management complexity. Hourly partitions provide good query selectivity but create many small indexes to manage. Daily partitions reduce management overhead but may scan unnecessary data for short time-range queries.\n\n**Index Compaction Requirements**: As log volume grows, small index segments must be merged into larger ones to maintain query performance. This compaction process must happen continuously without blocking ingestion or queries, requiring careful coordination between read and write operations.\n\n> **Decision: Hierarchical Label Indexing with Bloom Filters**\n> - **Context**: Need fast label-based filtering while avoiding cardinality explosion from high-cardinality labels\n> - **Options Considered**: Flat label indexing, hierarchical indexing, label value sampling, bloom filter pre-filtering\n> - **Decision**: Use hierarchical indexing with bloom filters for negative lookups and label cardinality limits\n> - **Rationale**: Hierarchical structure allows efficient range queries while bloom filters eliminate unnecessary disk reads, and cardinality limits prevent index explosion\n> - **Consequences**: Enables sub-second label queries but requires careful bloom filter tuning and label design guidelines\n\n#### Challenge 3: Query Performance Optimization\n\nThe label-only indexing approach shifts complexity from ingestion time to query time. Queries must efficiently combine label-based filtering with full-text search across compressed log data.\n\n**Query Planning Complexity**: A typical LogQL query like `{service=\"api\", level=\"error\"} |= \"timeout\"` requires:\n1. **Label filtering**: Find all log chunks where service=api AND level=error\n2. **Time range filtering**: Narrow to chunks within the query time range\n3. **Bloom filter checking**: Use bloom filters to eliminate chunks that definitely don't contain \"timeout\"\n4. **Chunk decompression**: Decompress remaining chunks and search for \"timeout\"\n5. **Result aggregation**: Combine results from multiple chunks and return in time order\n\nEach step must be optimized, and the query planner must decide the most efficient execution order based on label selectivity and time range size.\n\n**Full-Text Search Performance**: Unlike traditional full-text indexes, the system must perform grep-like searches across compressed chunks. This requires:\n- **Efficient decompression**: Chunks must decompress quickly enough to maintain interactive query speeds\n- **Streaming search**: Large chunks must be searched without loading entirely into memory\n- **Regular expression optimization**: Complex regex patterns must be compiled and executed efficiently\n- **Result streaming**: Query results must start returning before all chunks are processed\n\n**Concurrent Query Handling**: The system must serve multiple concurrent queries efficiently, sharing decompressed chunk data between queries when possible and managing memory usage to prevent resource exhaustion.\n\n**Pagination and Sorting Challenges**: Log queries often return millions of results that must be paginated efficiently. Time-based sorting is natural for logs, but other sort orders (relevance, label values) require additional processing.\n\n> The query performance challenge is fundamentally about making grep-scale performance feel interactive. Users expect sub-second response times even when searching terabytes of log data, which requires aggressive optimization at every level.\n\n#### Challenge 4: Storage Efficiency and Durability\n\nThe storage layer must provide both efficiency (high compression ratios, fast access) and durability (no data loss, quick recovery) while supporting the access patterns of both ingestion and querying.\n\n**Compression Strategy Complexity**: Log data is highly compressible (typical ratios of 5:1 to 10:1), but compression choice affects both storage efficiency and query performance. Fast compression algorithms (LZ4, Snappy) enable quick decompression during queries but provide lower compression ratios. High-efficiency algorithms (zstd, gzip) provide better compression but slower decompression.\n\n**Write-Ahead Log Design**: The WAL must ensure durability without becoming a performance bottleneck. It must handle:\n- **High write rates**: Thousands of log entries per second must be persisted durably\n- **Batch optimization**: Small writes must be batched for efficiency without adding excessive latency\n- **Recovery performance**: After crashes, WAL replay must be fast enough to minimize downtime\n- **WAL cleanup**: Processed entries must be cleaned up without interrupting ongoing writes\n\n**Chunk Organization Strategy**: Log data must be organized into chunks that balance:\n- **Query efficiency**: Chunks should align with common query patterns (time ranges, label combinations)\n- **Storage efficiency**: Chunks should be large enough for good compression but small enough for efficient partial reads\n- **Ingestion performance**: New data must be written efficiently without fragmenting storage\n\n**Retention Policy Implementation**: Old log data must be automatically deleted based on configurable policies, but retention cleanup must not interfere with ongoing queries or create storage consistency issues.\n\n> **Decision: Time-Windowed Chunks with Adaptive Compression**\n> - **Context**: Need to balance storage efficiency, query performance, and ingestion speed\n> - **Options Considered**: Fixed-size chunks, time-based chunks, label-based chunks, hybrid approaches\n> - **Decision**: Use time-windowed chunks (1-hour windows) with compression algorithm selection based on chunk age\n> - **Rationale**: Time-based chunking aligns with common query patterns, while adaptive compression uses fast algorithms for recent data (likely to be queried) and high-efficiency algorithms for older data\n> - **Consequences**: Provides good query performance and storage efficiency but adds complexity in compression management\n\n#### Challenge 5: System Coordination and Consistency\n\nA log aggregation system involves multiple concurrent processes (ingestion, indexing, compaction, querying) that must coordinate without creating bottlenecks or inconsistencies.\n\n**Concurrent Access Management**: Multiple processes need different types of access to the same data:\n- **Ingestion processes**: Need exclusive write access to active chunks\n- **Query processes**: Need concurrent read access to stable chunks  \n- **Compaction processes**: Need exclusive access to merge small chunks into larger ones\n- **Retention processes**: Need exclusive access to delete old chunks\n\n**Index Consistency Maintenance**: The inverted indexes must remain consistent with the stored log data even as chunks are being written, compacted, and deleted. Index updates must be atomic — either fully applied or not applied at all.\n\n**Failure Recovery Coordination**: When components fail and restart, they must coordinate with other running components to avoid conflicts or data corruption. A compaction process that crashes mid-operation must not leave partially merged chunks that confuse the query engine.\n\n**Backpressure Propagation**: When downstream components (indexing, storage) cannot keep up with ingestion rates, backpressure must propagate back to ingestion sources without causing data loss or cascading failures.\n\n> The coordination challenge is what transforms this from a simple storage system into a distributed systems problem. Even a single-node implementation must solve these coordination problems between different concurrent processes.\n\nThese five core challenges are interconnected — decisions made to solve one challenge directly impact the others. For example, choosing smaller chunk sizes improves query selectivity but increases index management complexity. Using more aggressive compression improves storage efficiency but slows query performance. The art of building a log aggregation system lies in finding the sweet spots that balance all these competing requirements.\n\n⚠️ **Pitfall: Underestimating Label Cardinality Impact**\nLabel cardinality is the most common cause of performance problems in label-only indexing systems. A single high-cardinality label (like `user_id` with millions of unique values) can create an index larger than the original log data. This happens because the index must store metadata for every unique label combination, and the combinations grow exponentially with cardinality. To avoid this, establish cardinality limits (e.g., maximum 10,000 unique values per label) and provide clear guidelines to developers about which fields should become labels versus which should remain in log message content.\n\n⚠️ **Pitfall: Ignoring Burst Rate Requirements**\nMany systems are designed for average ingestion rates but fail catastrophically during burst rates that occur during incidents. Since incidents are precisely when log data becomes most critical, the system must handle burst rates of 10x-100x the sustained rate. This requires over-provisioning buffers, implementing circuit breakers, and having clear degradation strategies (e.g., sampling log entries during extreme bursts rather than dropping them entirely).\n\n⚠️ **Pitfall: Treating Compression as an Afterthought**\nCompression choice has profound impacts on both storage costs and query performance. Many implementations default to general-purpose compression (gzip) without considering that log data has specific characteristics that benefit from different approaches. Recent chunks (likely to be queried frequently) should use fast decompression algorithms like LZ4, while older chunks can use high-efficiency algorithms like zstd. Additionally, chunk size dramatically affects compression ratio — chunks smaller than 1MB typically compress poorly, while chunks larger than 100MB create query performance problems.\n\n### Implementation Guidance\n\nThis section provides concrete technology recommendations and architectural patterns for implementing the concepts discussed above. The focus is on Go-based implementations that provide good performance characteristics for log aggregation workloads.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option | Rationale |\n|-----------|---------------|-----------------|-----------|\n| **HTTP Ingestion** | `net/http` with `encoding/json` | `fiber/v2` with `sonic` JSON | net/http provides reliability; fiber+sonic for high performance |\n| **TCP/UDP Syslog** | `net` package with custom parsing | `go-syslog` library | Custom parsing for learning; library for production robustness |\n| **Storage Backend** | Local filesystem with `os` package | `badger` embedded database | Filesystem for simplicity; badger for advanced features |\n| **Compression** | `compress/gzip` standard library | `klauspost/compress` optimized | Standard gzip for start; optimized library for performance |\n| **Serialization** | `encoding/json` for simplicity | `msgpack` for efficiency | JSON for development ease; msgpack for production efficiency |\n| **Bloom Filters** | `bits.OnesCount` bit operations | `willf/bloom` optimized library | Bit operations for learning; library for production |\n\n#### Recommended Project Structure\n\nOrganizing the codebase properly from the start prevents the common mistake of cramming everything into a single large file. This structure supports the incremental development approach across milestones:\n\n```\nlog-aggregator/\n├── cmd/\n│   ├── server/main.go              ← HTTP/TCP/UDP server entry point\n│   ├── ingest/main.go              ← Log ingestion CLI tool\n│   └── query/main.go               ← Query CLI tool\n├── internal/\n│   ├── ingestion/                  ← Milestone 1: Log Ingestion\n│   │   ├── http_handler.go         ← HTTP POST endpoint\n│   │   ├── syslog_handler.go       ← TCP/UDP syslog receiver\n│   │   ├── file_tail.go            ← File watching and tailing\n│   │   ├── parser.go               ← JSON/syslog/regex parsing\n│   │   ├── buffer.go               ← Memory/disk buffering\n│   │   └── ingestion_test.go       ← Integration tests\n│   ├── indexing/                   ← Milestone 2: Log Index\n│   │   ├── inverted_index.go       ← Term-to-document mapping\n│   │   ├── bloom_filter.go         ← Negative lookup optimization\n│   │   ├── partitions.go           ← Time-based partitioning\n│   │   ├── compaction.go           ← Index segment merging\n│   │   └── indexing_test.go        ← Index behavior tests\n│   ├── query/                      ← Milestone 3: Query Engine\n│   │   ├── parser.go               ← LogQL parsing and AST\n│   │   ├── planner.go              ← Query execution planning\n│   │   ├── executor.go             ← Search execution\n│   │   ├── results.go              ← Result processing and pagination\n│   │   └── query_test.go           ← Query correctness tests\n│   ├── storage/                    ← Milestone 4: Storage & Compression\n│   │   ├── chunks.go               ← Chunk-based storage\n│   │   ├── compression.go          ← Compression strategies\n│   │   ├── wal.go                  ← Write-ahead logging\n│   │   ├── retention.go            ← Cleanup policies\n│   │   └── storage_test.go         ← Storage durability tests\n│   ├── tenancy/                    ← Milestone 5: Multi-Tenant & Alerting\n│   │   ├── isolation.go            ← Tenant separation\n│   │   ├── ratelimit.go            ← Per-tenant limits\n│   │   ├── alerting.go             ← Pattern-based alerts\n│   │   └── tenancy_test.go         ← Multi-tenant behavior tests\n│   └── shared/                     ← Common types and utilities\n│       ├── types.go                ← LogEntry, Labels, common structs\n│       ├── config.go               ← Configuration management\n│       └── metrics.go              ← Prometheus metrics\n├── pkg/                            ← Public APIs (if exposing libraries)\n│   └── client/                     ← Query client library\n├── configs/\n│   ├── server.yaml                 ← Server configuration\n│   └── docker-compose.yaml         ← Local development setup\n├── scripts/\n│   ├── generate-logs.sh            ← Test data generation\n│   └── benchmark.sh                ← Performance testing\n├── docs/\n│   ├── api.md                      ← HTTP API documentation\n│   └── logql.md                    ← Query language reference\n├── go.mod\n├── go.sum\n├── Makefile                        ← Build and test automation\n└── README.md\n```\n\nThis structure allows you to work on one milestone at a time while maintaining clear separation of concerns. Each milestone maps to a specific `internal/` directory, making it easy to focus on one component without getting overwhelmed by the full system complexity.\n\n#### Core Data Types (Shared Foundation)\n\nThe foundation for all components starts with these core types in `internal/shared/types.go`:\n\n```go\npackage shared\n\nimport (\n    \"time\"\n)\n\n// LogEntry represents a single log entry with labels and content\ntype LogEntry struct {\n    // TODO: Define fields for Timestamp, Labels, Message, SourceInfo\n    // Hint: Use time.Time for timestamps, map[string]string for labels\n    // Consider: What metadata do you need for querying and storage?\n}\n\n// Labels represents the key-value pairs used for indexing and filtering\ntype Labels map[string]string\n\n// LogStream represents a sequence of log entries with the same label set\ntype LogStream struct {\n    // TODO: Define fields for Labels, Entries, metadata\n    // Hint: This groups entries by label combination for efficient storage\n}\n\n// TimeRange represents a time window for queries and partitioning\ntype TimeRange struct {\n    // TODO: Define Start and End time fields\n    // Consider: How will this be used in query planning and chunk selection?\n}\n```\n\n#### Infrastructure Starter Code\n\nTo help you focus on the core learning objectives rather than getting stuck on infrastructure details, here's complete starter code for common utilities:\n\n**Configuration Management (`internal/shared/config.go`):**\n```go\npackage shared\n\nimport (\n    \"fmt\"\n    \"os\"\n    \"strconv\"\n    \"time\"\n)\n\n// Config holds all system configuration\ntype Config struct {\n    HTTPPort     int\n    TCPPort      int  \n    UDPPort      int\n    StoragePath  string\n    BufferSize   int\n    ChunkSize    int\n    RetentionDays int\n}\n\n// LoadConfig loads configuration from environment variables with defaults\nfunc LoadConfig() *Config {\n    return &Config{\n        HTTPPort:     getEnvInt(\"HTTP_PORT\", 8080),\n        TCPPort:      getEnvInt(\"TCP_PORT\", 1514),\n        UDPPort:      getEnvInt(\"UDP_PORT\", 1514), \n        StoragePath:  getEnv(\"STORAGE_PATH\", \"./data\"),\n        BufferSize:   getEnvInt(\"BUFFER_SIZE\", 10000),\n        ChunkSize:    getEnvInt(\"CHUNK_SIZE\", 1024*1024), // 1MB default\n        RetentionDays: getEnvInt(\"RETENTION_DAYS\", 30),\n    }\n}\n\nfunc getEnv(key, defaultValue string) string {\n    if value := os.Getenv(key); value != \"\" {\n        return value\n    }\n    return defaultValue\n}\n\nfunc getEnvInt(key string, defaultValue int) int {\n    if value := os.Getenv(key); value != \"\" {\n        if parsed, err := strconv.Atoi(value); err == nil {\n            return parsed\n        }\n    }\n    return defaultValue\n}\n```\n\n**Simple Metrics Collection (`internal/shared/metrics.go`):**\n```go\npackage shared\n\nimport (\n    \"sync/atomic\"\n    \"time\"\n)\n\n// Metrics holds system performance counters\ntype Metrics struct {\n    LogsIngested    int64\n    LogsIndexed     int64  \n    QueriesExecuted int64\n    BytesStored     int64\n    LastActivity    time.Time\n}\n\n// Global metrics instance\nvar GlobalMetrics = &Metrics{}\n\n// IncrementLogsIngested safely increments the ingestion counter\nfunc (m *Metrics) IncrementLogsIngested() {\n    atomic.AddInt64(&m.LogsIngested, 1)\n    m.LastActivity = time.Now()\n}\n\n// IncrementQueriesExecuted safely increments the query counter  \nfunc (m *Metrics) IncrementQueriesExecuted() {\n    atomic.AddInt64(&m.QueriesExecuted, 1)\n    m.LastActivity = time.Now()\n}\n\n// GetStats returns current metric values safely\nfunc (m *Metrics) GetStats() (logs, queries int64) {\n    return atomic.LoadInt64(&m.LogsIngested), atomic.LoadInt64(&m.QueriesExecuted)\n}\n```\n\n#### Milestone Checkpoint Guidelines\n\nEach milestone should have clear, testable acceptance criteria. Here's how to verify your implementation at each stage:\n\n**Milestone 1 Checkpoint - Log Ingestion:**\n```bash\n# Start your server\ngo run cmd/server/main.go\n\n# Test HTTP ingestion\ncurl -X POST http://localhost:8080/api/v1/push \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"timestamp\":\"2024-01-01T12:00:00Z\",\"level\":\"info\",\"service\":\"api\",\"message\":\"test log\"}'\n\n# Expected: HTTP 200 response, log entry stored/buffered\n# Verify: Check logs show successful ingestion, storage directory has data\n\n# Test TCP syslog ingestion  \necho '<14>2024-01-01T12:00:00Z myhost myservice: test syslog message' | nc localhost 1514\n\n# Expected: TCP connection accepted, syslog parsed correctly\n# Verify: Check parsed fields match RFC format\n```\n\n**Performance Verification:**\nYour ingestion pipeline should handle at least 10,000 entries/second on a standard development machine. Test with:\n```bash\n# Generate high-volume test data\nfor i in {1..10000}; do\n  curl -X POST http://localhost:8080/api/v1/push -d \"{\\\"timestamp\\\":\\\"$(date -Iseconds)\\\",\\\"level\\\":\\\"info\\\",\\\"message\\\":\\\"test $i\\\"}\" &\ndone\nwait\n\n# Check metrics endpoint for ingestion rate\ncurl http://localhost:8080/metrics\n```\n\nIf ingestion rate falls below targets, common issues include:\n- **Unbuffered I/O**: Ensure you're batching writes to storage\n- **JSON parsing overhead**: Consider switching to faster JSON libraries\n- **Lock contention**: Use channels instead of shared memory where possible\n\n#### Language-Specific Implementation Hints\n\n**JSON Parsing Performance:**\n```go\n// Use json.Decoder for streaming parsing instead of json.Unmarshal\ndecoder := json.NewDecoder(request.Body)\nvar logEntry LogEntry\nif err := decoder.Decode(&logEntry); err != nil {\n    // Handle parsing error\n}\n```\n\n**Efficient String Operations:**\n```go\n// Use strings.Builder for constructing large strings\nvar builder strings.Builder\nbuilder.WriteString(\"log content\")\nbuilder.WriteString(\" additional data\") \nresult := builder.String()\n```\n\n**File I/O Optimization:**\n```go\n// Use bufio.Writer for batched writes\nfile, _ := os.OpenFile(filename, os.O_APPEND|os.O_CREATE|os.O_WRONLY, 0644)\nwriter := bufio.NewWriter(file)\nwriter.WriteString(logData)\nwriter.Flush() // Don't forget to flush!\n```\n\n**Memory Pool Usage:**\n```go\n// Reuse byte slices to reduce GC pressure\nvar bufferPool = sync.Pool{\n    New: func() interface{} {\n        return make([]byte, 1024)\n    },\n}\n\nbuffer := bufferPool.Get().([]byte)\ndefer bufferPool.Put(buffer)\n```\n\nThese implementation guidelines provide a solid foundation for tackling each milestone while learning the core concepts. The key is to start simple with the infrastructure code provided, then gradually optimize performance as you understand the bottlenecks in your specific implementation.\n\n\n## Goals and Non-Goals\n\n> **Milestone(s):** This section provides foundational understanding that applies to all milestones (1-5), establishing clear boundaries and success criteria for the entire system.\n\n### Mental Model: The Mission Statement\n\nThink of this section as the **mission statement** for our log aggregation system project. Just as a company's mission statement defines what it will accomplish, what principles guide its decisions, and what it explicitly won't pursue, our goals and non-goals section serves as the north star for every design decision we'll make throughout the five milestones.\n\nImagine you're leading a team of engineers who will spend months building this system. Without clear goals, one engineer might optimize for maximum ingestion throughput while another focuses on minimal storage costs, leading to conflicting design decisions. The goals section acts like a **constitutional document** - when facing trade-offs between competing requirements, we can refer back to these clearly defined priorities to make consistent decisions.\n\nThe non-goals are equally critical - they're the **explicit boundaries** that prevent scope creep and feature bloat. Like a ship's captain who must resist the temptation to chase every interesting island, we must resist adding features that sound useful but don't serve our core mission. This discipline is what separates successful systems from over-engineered ones that never ship.\n\n### Functional Goals\n\nThe functional goals define the **core capabilities** our log aggregation system must deliver. These represent the fundamental value proposition - what users will actually do with our system and what workflows it must support.\n\n#### Primary Log Ingestion Capabilities\n\nOur system must accept log data from diverse sources through multiple protocols, handling the reality that modern infrastructure generates logs in various formats and delivery mechanisms. The **primary ingestion goal** is to provide universal log acceptance - any system that generates logs should be able to send them to our aggregation system without requiring complex client-side modifications.\n\n| Ingestion Method | Protocol | Format Support | Throughput Target | Buffering Strategy |\n|------------------|----------|----------------|-------------------|-------------------|\n| HTTP REST API | HTTP/HTTPS | JSON, plain text | 10,000 msgs/sec | In-memory with disk overflow |\n| Syslog Receiver | TCP/UDP | RFC 5424, RFC 3164 | 15,000 msgs/sec | Stream-based buffering |\n| File Tail Agent | File I/O | Any text format | 5,000 lines/sec | Inode-based tracking |\n| Structured Logs | HTTP POST | JSON, logfmt | 8,000 msgs/sec | Batch accumulation |\n\nThe system must handle **burst traffic** gracefully - log volume often spikes during incidents when logs are most critical. Our buffering strategy ensures that temporary downstream slowdowns don't result in log loss. When the ingestion rate exceeds processing capacity, logs accumulate in memory buffers up to a configured limit, then overflow to disk-based buffers, and finally apply backpressure to prevent memory exhaustion.\n\n**Log parsing** must extract structured fields from unstructured log messages using configurable patterns. Many legacy applications emit logs as free-form text, but effective querying requires extracting structured elements like timestamps, log levels, service names, and request IDs. Our parsing pipeline applies regex patterns, JSON extraction, and key-value pair recognition to transform unstructured logs into queryable structured data.\n\n> **Key Insight**: The ingestion layer is the **trust boundary** of our system. Once a log enters our ingestion pipeline, users expect it to be stored durably and made available for querying. This means ingestion must be more reliable than any upstream system - we cannot afford to lose logs due to parsing errors, buffer overflows, or downstream failures.\n\n#### Efficient Log Indexing and Storage\n\nThe system must build **inverted indexes** that enable fast label-based queries across massive log volumes. Unlike traditional databases that index all fields, our indexing strategy focuses on labels - the key-value pairs that categorize and organize log streams. This approach, inspired by Prometheus and Loki, provides excellent query performance while keeping index sizes manageable.\n\n| Index Type | Purpose | Storage Format | Update Frequency | Compaction Strategy |\n|------------|---------|----------------|------------------|-------------------|\n| Label Index | Maps label values to log streams | Hash table + sorted lists | Real-time during ingestion | Hourly merge of small segments |\n| Term Index | Full-text search within messages | Inverted posting lists | Batch every 1000 entries | Daily compaction with bloom filters |\n| Time Index | Temporal range queries | Time-partitioned segments | Per chunk write | Weekly partition merging |\n| Bloom Filters | Negative lookup optimization | Bit arrays per chunk | Once per chunk seal | Read-only after creation |\n\n**Time-based partitioning** organizes both indexes and data into time windows, typically hourly or daily segments. This temporal organization enables efficient time-range queries - when a user searches for logs from the last hour, the system only scans the relevant partition rather than the entire dataset. Partition boundaries align with common query patterns: recent logs for debugging, daily logs for analysis, and historical logs for compliance.\n\nThe **bloom filter** implementation provides probabilistic negative lookups - if a bloom filter indicates a term is not present in a chunk, the system can skip that chunk entirely during query execution. Bloom filters have a configurable false positive rate (typically 1-5%) but never produce false negatives, making them perfect for eliminating chunks that definitely don't contain query terms.\n\n#### Powerful Query Language and Execution\n\nOur query engine implements a **LogQL-style query language** that combines the simplicity of grep with the power of structured field filtering. The query language must feel natural to developers who are accustomed to command-line log analysis tools while providing the precision needed for large-scale log analysis.\n\n| Query Type | Syntax Example | Index Usage | Performance Characteristics |\n|------------|----------------|-------------|---------------------------|\n| Label Filter | `{service=\"api\", level=\"error\"}` | Label index lookup | O(1) stream identification |\n| Text Search | `\\|= \"database connection failed\"` | Term index scan | O(log n) with bloom filters |\n| Regex Match | `\\|~ \"user_id=\\d+\"` | Full message scan | O(n) within matching streams |\n| JSON Extraction | `\\| json \\| status_code > 400` | Runtime field extraction | O(m) where m = result size |\n| Aggregation | `count_over_time(5m)` | Time window processing | O(k) where k = time buckets |\n\n**Query optimization** applies several strategies to minimize the data scanned during query execution. The query planner pushes label filters down to the index layer, eliminating entire log streams before message-level processing begins. Time range filters are applied at the partition level, skipping historical data when users query recent logs. Regular expressions and text searches leverage bloom filters to eliminate chunks that cannot contain matching terms.\n\nThe query engine supports **streaming results** for large queries that return thousands of log entries. Rather than buffering all results in memory, the system streams matching log entries back to the client as they're found. This approach provides faster time-to-first-result and prevents memory exhaustion on large result sets.\n\n#### Robust Storage and Retention Management\n\nThe storage layer organizes logs into **compressed chunks** that balance query performance with storage efficiency. Each chunk contains logs from a specific time window and label combination, typically spanning 1-4 hours and compressed using algorithms optimized for text data. The chunk-based organization enables parallel query execution and efficient compression ratios.\n\n| Storage Component | Purpose | Configuration Options | Default Settings |\n|-------------------|---------|----------------------|-----------------|\n| Write-Ahead Log | Durability guarantee | Sync frequency, batch size | fsync every 1000 entries |\n| Chunk Store | Compressed log storage | Compression algorithm, chunk size | gzip compression, 1MB chunks |\n| Index Store | Metadata and indexes | Compaction frequency, bloom filter size | Daily compaction, 1% false positive rate |\n| Retention Engine | Automatic cleanup | Time-based, size-based rules | 30 days retention |\n\n**Compression strategy** significantly impacts both storage costs and query performance. We evaluate multiple compression algorithms during system design, measuring compression ratios and decompression speeds for typical log data. The choice affects the trade-off between storage efficiency (higher compression saves disk space) and query latency (faster decompression reduces query response time).\n\n**Retention policies** automatically delete old log data based on configurable rules. Time-based retention removes logs older than a specified age (e.g., 30 days), while size-based retention maintains a maximum storage footprint by removing the oldest data when storage limits are exceeded. Per-stream retention rules allow different log sources to have different retention periods - security logs might be kept for compliance while debug logs are deleted after a few days.\n\n> **Key Insight**: Storage decisions made early in the design have long-term consequences. Once you choose a chunk format and compression scheme, changing them requires migrating existing data. The retention policy engine must be extremely reliable - accidentally deleting logs due to bugs in retention logic is often irreversible.\n\n### Non-Functional Goals\n\nNon-functional goals define the **quality attributes** our system must exhibit - the performance, reliability, and operational characteristics that determine whether users will trust and adopt our log aggregation system in production environments.\n\n#### Performance and Scalability Requirements\n\nThe system must handle **production-scale workloads** without requiring excessive hardware resources. These performance targets reflect real-world usage patterns where log volume correlates with system activity - higher during business hours and incidents, lower during maintenance windows and off-peak periods.\n\n| Performance Metric | Target | Measurement Method | Degradation Handling |\n|--------------------|--------|-------------------|---------------------|\n| Ingestion Throughput | 50,000 logs/second sustained | Rate limiting with burst allowance | Graceful backpressure, no data loss |\n| Query Response Time | 95th percentile < 2 seconds | Histogram metrics per query type | Timeout after 30 seconds |\n| Index Update Latency | New logs searchable within 30 seconds | End-to-end ingestion-to-query test | Batch processing during high load |\n| Storage Efficiency | 10:1 compression ratio minimum | Compare raw vs compressed sizes | Alert if compression ratio drops |\n| Memory Usage | < 2GB resident during normal operation | Process memory monitoring | Garbage collection tuning |\n\n**Horizontal scalability** allows the system to handle growing log volumes by adding more servers rather than requiring larger individual machines. While our intermediate-level system runs on a single server, the architecture must not preclude future horizontal scaling. This means avoiding architectural decisions that would require fundamental redesigns when scaling beyond single-server capacity.\n\n**Burst handling** recognizes that log traffic patterns are highly variable. Application deployments, incidents, and batch jobs create temporary spikes that can exceed normal capacity by 5-10x. Our buffering strategy absorbs these bursts without dropping logs, and our indexing pipeline batches updates during high-load periods to maintain system stability.\n\n#### Reliability and Durability Guarantees\n\n**Data durability** is paramount - losing logs during critical incidents undermines the entire value proposition of log aggregation. Our durability guarantees ensure that once a log is acknowledged by the ingestion endpoint, it will be available for querying even if the system experiences crashes, disk failures, or other infrastructure problems.\n\n| Reliability Component | Guarantee Level | Implementation Strategy | Recovery Time Objective |\n|----------------------|-----------------|------------------------|------------------------|\n| Write-Ahead Logging | Zero data loss after ingestion | fsync before ACK response | Automatic on restart |\n| Index Corruption Recovery | Rebuild from stored logs | Checksum validation on read | < 1 hour for 1TB dataset |\n| Disk Failure Handling | Graceful degradation | Health checks with alerting | Manual intervention required |\n| Memory Exhaustion | No data loss, reduced performance | Disk overflow buffers | Automatic recovery when memory available |\n\n**Consistency guarantees** define what users can expect when logs are ingested and immediately queried. Our system provides **eventual consistency** - logs are guaranteed to appear in query results within 30 seconds of ingestion under normal conditions. During high load periods, this window may extend to several minutes, but logs are never lost.\n\n**Crash recovery** must restore the system to a consistent state without human intervention. The write-ahead log captures all ingestion operations before they're acknowledged, allowing the system to replay any operations that were in progress during a crash. Index recovery rebuilds any corrupted index structures from the durable log data, ensuring queries return accurate results after recovery completes.\n\n#### Operational and Monitoring Requirements\n\nThe system must provide **comprehensive observability** to support production operations. Operations teams need visibility into ingestion rates, query performance, storage usage, and error conditions to maintain system health and plan capacity upgrades.\n\n| Monitoring Category | Key Metrics | Alert Conditions | Dashboard Views |\n|---------------------|-------------|------------------|----------------|\n| Ingestion Health | Logs/second, error rate, buffer depth | Error rate > 1%, buffer > 80% full | Real-time ingestion dashboard |\n| Query Performance | Request rate, latency distribution, timeout rate | 95th percentile > 5 seconds | Query performance trends |\n| Storage Utilization | Disk usage, compression ratio, retention execution | Disk > 85% full | Capacity planning dashboard |\n| System Resources | CPU, memory, file descriptors, network | Memory > 90%, file descriptor exhaustion | Infrastructure health view |\n\n**Graceful degradation** maintains core functionality even when the system operates under stress or partial failure conditions. When memory buffers fill up, the system switches to disk-based buffering with reduced performance but no data loss. When query load exceeds capacity, the system applies rate limiting to protect core ingestion functionality. When storage approaches capacity limits, the system accelerates retention cleanup to maintain operational headroom.\n\n**Configuration management** allows operators to tune system behavior for their specific environments and usage patterns. Critical configuration parameters include buffer sizes, retention policies, compression settings, and performance thresholds. Configuration changes should take effect without requiring system restarts where possible, and all configuration changes should be logged for audit and troubleshooting purposes.\n\n> **Key Insight**: Non-functional requirements often conflict with each other - higher compression ratios increase CPU usage, faster ingestion requires more memory, longer retention needs more storage. The system design must find the right balance points and make trade-offs explicit through configuration options.\n\n### Explicit Non-Goals\n\nNon-goals define the **boundaries** of our project scope - features and capabilities that we explicitly choose not to implement. These boundaries prevent scope creep and help maintain focus on the core log aggregation functionality. Understanding what we won't build is as important as understanding what we will build.\n\n#### Real-Time Analytics and Complex Processing\n\nOur system **does not** provide real-time analytics, machine learning capabilities, or complex event processing. While some log aggregation platforms include these features, they represent significant additional complexity that would distract from our core goal of building a solid ingestion, indexing, and querying foundation.\n\n| Excluded Feature | Rationale | Alternative Approach | Future Consideration |\n|------------------|-----------|----------------------|---------------------|\n| Stream Processing | Adds significant complexity to query engine | Use dedicated stream processing tools | Could be added in future milestones |\n| Machine Learning | Requires specialized algorithms and training data | Export logs to ML platforms | Pattern detection might be valuable later |\n| Real-time Dashboards | Requires WebSocket connections and live updates | Use polling-based dashboard tools | Simple metrics endpoint is sufficient |\n| Complex Aggregations | Window functions, joins, statistical operations | Export to analytical databases | Basic counting operations are adequate |\n\n**Log transformation** beyond basic field extraction is not supported. While some systems provide rich transformation pipelines that can parse, enrich, and modify logs during ingestion, these features add complexity to the ingestion path and can become performance bottlenecks. Our system focuses on accepting logs as-is and making them efficiently searchable.\n\n**Alerting and notification** systems are excluded from the core implementation. While log-based alerting is valuable, it requires additional infrastructure for notification delivery, escalation policies, and alert management. Users can implement alerting by periodically querying our system and processing the results externally.\n\n#### Advanced Distributed System Features\n\nOur intermediate-level system **does not** implement advanced distributed system features like automatic failover, data replication, or cluster management. These features require significant additional complexity in areas like consensus protocols, network partition handling, and distributed state management.\n\n| Distributed Feature | Complexity Reasons | Single-Server Alternative | Migration Path |\n|---------------------|-------------------|---------------------------|----------------|\n| Automatic Failover | Requires leader election and state synchronization | Manual backup/restore procedures | Design allows future clustering |\n| Data Replication | Complex consistency guarantees and conflict resolution | Backup to external storage systems | Chunk format supports replication |\n| Load Balancing | Client-side discovery and connection management | Single endpoint with high availability | HTTP load balancer compatible |\n| Sharding | Automatic data placement and query federation | Vertical scaling and retention management | Architecture supports future sharding |\n\n**Cross-datacenter replication** is not supported due to the complexity of handling network partitions, latency variations, and consistency guarantees across geographic regions. Organizations requiring multi-region log aggregation can deploy independent instances and use external tools for data synchronization.\n\n**Automatic scaling** based on load metrics is not implemented. While cloud-native applications often include auto-scaling capabilities, they require integration with orchestration platforms and complex resource management logic. Our system provides the metrics needed for external auto-scaling systems to make scaling decisions.\n\n#### Enterprise Integration and Security Features\n\nAdvanced **authentication and authorization** systems are not included. While production log aggregation systems require robust security, implementing features like LDAP integration, role-based access control, and audit logging would significantly expand the project scope beyond the core technical challenges.\n\n| Security Feature | Implementation Complexity | Basic Alternative | Production Requirements |\n|-------------------|---------------------------|-------------------|------------------------|\n| Multi-factor Authentication | Integration with identity providers | API key authentication | External authentication proxy |\n| Fine-grained Authorization | Role-based access control with permissions | Tenant-based isolation | Authorization service integration |\n| Audit Logging | Separate audit trail with compliance features | Basic operation logging | Dedicated audit system |\n| Encryption at Rest | Key management and performance impact | File system encryption | Hardware security modules |\n\n**Compliance features** like data residency controls, legal hold capabilities, and privacy controls are not implemented. These features require deep integration with organizational policies and legal requirements that vary significantly across different use cases and jurisdictions.\n\n**Enterprise integration** features like single sign-on, corporate directory integration, and configuration management system integration are excluded. These integrations are highly specific to organizational infrastructure and would require extensive configuration options and compatibility testing.\n\n#### Performance Optimization Beyond Core Requirements\n\nWe **do not** optimize for extreme performance scenarios that would require specialized hardware or complex performance tuning. While our system meets production performance requirements, it does not target use cases requiring specialized optimizations.\n\n| Performance Area | Not Optimized For | Reason | Alternative Approach |\n|------------------|-------------------|--------|---------------------|\n| Ultra-low Latency | Sub-millisecond query responses | Requires in-memory indexes and complex caching | Dedicated time-series databases |\n| Extreme Throughput | >1M logs/second single node | Requires specialized networking and storage | Distributed ingestion systems |\n| Minimal Resource Usage | <100MB memory footprint | Conflicts with performance and functionality goals | Embedded log libraries |\n| Custom Hardware | GPU acceleration, FPGA compression | Adds deployment complexity | Specialized analytical systems |\n\n**Memory optimization** beyond reasonable limits is not pursued. While our system operates within reasonable memory bounds (target: 2GB), we do not optimize for extremely memory-constrained environments where every megabyte matters. Such optimization would require complex data structure choices that sacrifice maintainability.\n\n**Storage optimization** beyond standard compression techniques is not implemented. Advanced techniques like dictionary compression, column storage, or specialized text compression algorithms would improve storage efficiency but add significant complexity to the storage layer.\n\n> **Key Design Principle**: By clearly defining non-goals, we create space to excel at our core functionality. Every feature we exclude allows us to make the included features more robust, better tested, and easier to understand. The best systems do fewer things exceptionally well rather than many things adequately.\n\n### Architecture Decision Records\n\nThe goals and non-goals drive several foundational architecture decisions that influence the entire system design. These decisions establish the technical foundation that supports our functional goals while respecting the boundaries established by our non-goals.\n\n> **Decision: Single-Server Architecture for Intermediate Implementation**\n> - **Context**: Must balance system complexity with educational value for intermediate developers while supporting future scalability.\n> - **Options Considered**: Microservices architecture, distributed system from start, single-server with scalable design\n> - **Decision**: Single-server implementation with architecture that supports future distribution\n> - **Rationale**: Reduces operational complexity, simplifies debugging, and allows focus on core log aggregation challenges while maintaining expansion paths\n> - **Consequences**: Enables rapid development and testing but requires eventual redesign for horizontal scaling. Trade-off is appropriate for learning objectives.\n\n| Architecture Option | Complexity Level | Learning Value | Production Readiness | Chosen |\n|---------------------|------------------|----------------|---------------------|---------|\n| Microservices | High | High | High | ❌ |\n| Distributed System | Very High | Very High | Very High | ❌ |\n| Single Server | Medium | High | Medium | ✅ |\n\n> **Decision: LogQL-Style Query Language**\n> - **Context**: Need query interface that balances ease of use with powerful filtering capabilities\n> - **Options Considered**: SQL-like syntax, GraphQL-based queries, LogQL-inspired syntax\n> - **Decision**: LogQL-inspired query language with label filters and pipeline operations\n> - **Rationale**: Familiar to developers using Grafana/Loki, optimizes for common log analysis patterns, and maps well to our label-based indexing strategy\n> - **Consequences**: Requires custom parser implementation but provides excellent user experience for log-specific operations\n\n| Query Language Option | Learning Curve | Implementation Complexity | Query Power | Chosen |\n|-----------------------|-----------------|---------------------------|-------------|---------|\n| SQL-like | Low | Very High | Very High | ❌ |\n| GraphQL | Medium | High | Medium | ❌ |\n| LogQL-inspired | Medium | Medium | High | ✅ |\n\n> **Decision: Write-Ahead Log for Durability**\n> - **Context**: Must guarantee no data loss after ingestion acknowledgment while maintaining reasonable performance\n> - **Options Considered**: Synchronous disk writes, asynchronous batching, write-ahead log with batching\n> - **Decision**: Write-ahead log with configurable sync frequency\n> - **Rationale**: Provides tunable durability guarantees, enables crash recovery, and allows performance optimization through batching\n> - **Consequences**: Adds complexity to ingestion path but provides essential durability guarantees required for production usage\n\n| Durability Strategy | Performance Impact | Complexity | Data Safety | Chosen |\n|--------------------|-------------------|------------|-------------|---------|\n| Synchronous Writes | High | Low | Highest | ❌ |\n| Async Batching | Low | Low | Lowest | ❌ |\n| Write-Ahead Log | Medium | Medium | High | ✅ |\n\n### Implementation Guidance\n\nThis section bridges the abstract goals and constraints with concrete technology choices and project organization that support building a production-ready log aggregation system.\n\n#### Technology Recommendations\n\nThe following technology stack balances simplicity for learning with production-readiness for real-world deployment:\n\n| Component Category | Simple Option | Advanced Option | Recommended Choice |\n|-------------------|---------------|-----------------|-------------------|\n| HTTP Server | `net/http` with JSON | gRPC with Protocol Buffers | `net/http` - simpler debugging |\n| Serialization | JSON for all formats | Protocol Buffers + JSON hybrid | JSON - universal compatibility |\n| Storage Backend | Local filesystem with atomic writes | S3-compatible object storage | Filesystem - reduces dependencies |\n| Compression | gzip standard library | LZ4 or Zstandard | gzip - good balance of ratio/speed |\n| Indexing | Hash maps with sorted arrays | B-trees with buffer pools | Hash maps - simpler implementation |\n| Logging | Standard `log` package | Structured logging (logrus/zap) | Standard `log` - meta-irony avoided |\n\n#### Recommended Project Structure\n\nOrganize the codebase to reflect the major functional components while maintaining clear separation of concerns:\n\n```\nlog-aggregator/\n├── cmd/\n│   ├── server/main.go              ← Main server entry point\n│   └── tools/                      ← Utilities (index rebuild, etc.)\n├── internal/\n│   ├── ingestion/                  ← Milestone 1: Log Ingestion\n│   │   ├── http_receiver.go\n│   │   ├── syslog_receiver.go\n│   │   ├── file_tailer.go\n│   │   └── buffer_manager.go\n│   ├── parser/                     ← Log parsing and field extraction\n│   │   ├── json_parser.go\n│   │   ├── syslog_parser.go\n│   │   └── regex_parser.go\n│   ├── index/                      ← Milestone 2: Log Index\n│   │   ├── inverted_index.go\n│   │   ├── bloom_filter.go\n│   │   ├── partition_manager.go\n│   │   └── compaction.go\n│   ├── storage/                    ← Milestone 4: Storage & Compression\n│   │   ├── chunk_store.go\n│   │   ├── wal.go\n│   │   ├── compression.go\n│   │   └── retention.go\n│   ├── query/                      ← Milestone 3: Query Engine\n│   │   ├── parser.go\n│   │   ├── planner.go\n│   │   ├── executor.go\n│   │   └── logql/                  ← LogQL language implementation\n│   ├── tenant/                     ← Milestone 5: Multi-Tenancy\n│   │   ├── isolation.go\n│   │   ├── rate_limiter.go\n│   │   └── alerting.go\n│   └── common/                     ← Shared types and utilities\n│       ├── types.go                ← LogEntry, Labels, etc.\n│       ├── config.go\n│       └── metrics.go\n├── pkg/                           ← Public APIs (if needed)\n├── test/                          ← Integration tests\n├── configs/                       ← Configuration examples\n└── docs/                         ← Documentation\n```\n\n#### Infrastructure Starter Code\n\n**Configuration Management** (complete implementation):\n\n```go\n// internal/common/config.go\npackage common\n\nimport (\n    \"os\"\n    \"strconv\"\n    \"time\"\n)\n\ntype Config struct {\n    HTTPPort     int\n    TCPPort      int\n    UDPPort      int\n    StoragePath  string\n    BufferSize   int\n    ChunkSize    int64\n    RetentionDays int\n    WALSyncFreq  time.Duration\n}\n\n// LoadConfig loads configuration from environment with defaults\nfunc LoadConfig() *Config {\n    return &Config{\n        HTTPPort:      getEnvInt(\"HTTP_PORT\", 8080),\n        TCPPort:       getEnvInt(\"TCP_PORT\", 1514),\n        UDPPort:       getEnvInt(\"UDP_PORT\", 1514),\n        StoragePath:   getEnv(\"STORAGE_PATH\", \"./data\"),\n        BufferSize:    getEnvInt(\"BUFFER_SIZE\", 10000),\n        ChunkSize:     int64(getEnvInt(\"CHUNK_SIZE\", 1024*1024)), // 1MB\n        RetentionDays: getEnvInt(\"RETENTION_DAYS\", 30),\n        WALSyncFreq:   time.Duration(getEnvInt(\"WAL_SYNC_FREQ_MS\", 1000)) * time.Millisecond,\n    }\n}\n\nfunc getEnv(key, defaultValue string) string {\n    if value := os.Getenv(key); value != \"\" {\n        return value\n    }\n    return defaultValue\n}\n\nfunc getEnvInt(key string, defaultValue int) int {\n    if value := os.Getenv(key); value != \"\" {\n        if intValue, err := strconv.Atoi(value); err == nil {\n            return intValue\n        }\n    }\n    return defaultValue\n}\n```\n\n**Core Data Types** (complete implementation):\n\n```go\n// internal/common/types.go\npackage common\n\nimport (\n    \"sync/atomic\"\n    \"time\"\n)\n\n// Labels represents key-value pairs for log categorization\ntype Labels map[string]string\n\n// LogEntry represents a single log record with metadata\ntype LogEntry struct {\n    Timestamp time.Time `json:\"timestamp\"`\n    Labels    Labels    `json:\"labels\"`\n    Message   string    `json:\"message\"`\n}\n\n// LogStream represents a sequence of logs with the same label set\ntype LogStream struct {\n    Labels  Labels     `json:\"labels\"`\n    Entries []LogEntry `json:\"entries\"`\n}\n\n// TimeRange represents a time window for queries\ntype TimeRange struct {\n    Start time.Time `json:\"start\"`\n    End   time.Time `json:\"end\"`\n}\n\n// Metrics provides thread-safe performance counters\ntype Metrics struct {\n    logsIngested int64\n    queriesExecuted int64\n    bytesStored int64\n}\n\n// IncrementLogsIngested safely increments the ingestion counter\nfunc (m *Metrics) IncrementLogsIngested() {\n    atomic.AddInt64(&m.logsIngested, 1)\n}\n\n// GetStats returns current log and query counts safely\nfunc (m *Metrics) GetStats() (int64, int64) {\n    logs := atomic.LoadInt64(&m.logsIngested)\n    queries := atomic.LoadInt64(&m.queriesExecuted)\n    return logs, queries\n}\n\n// Global metrics instance\nvar GlobalMetrics = &Metrics{}\n```\n\n#### Core Logic Skeletons\n\n**Log Ingestion Buffer Manager** (signature + TODOs):\n\n```go\n// internal/ingestion/buffer_manager.go\npackage ingestion\n\nimport (\n    \"context\"\n    \"github.com/yourorg/log-aggregator/internal/common\"\n)\n\ntype BufferManager struct {\n    memoryBuffer chan *common.LogEntry\n    diskBuffer   *DiskBuffer\n    config       *common.Config\n}\n\n// NewBufferManager creates a new buffer manager with configured capacity\nfunc NewBufferManager(config *common.Config) *BufferManager {\n    // TODO 1: Create memory buffer channel with BUFFER_SIZE capacity\n    // TODO 2: Initialize disk buffer for overflow handling\n    // TODO 3: Start background goroutine for buffer flushing\n    // Hint: Use buffered channel for memory buffer to provide backpressure\n}\n\n// AddLogEntry adds a log entry to the buffer with overflow handling\nfunc (bm *BufferManager) AddLogEntry(ctx context.Context, entry *common.LogEntry) error {\n    // TODO 1: Try to add entry to memory buffer (non-blocking)\n    // TODO 2: If memory buffer is full, write to disk buffer\n    // TODO 3: Update metrics counter for ingested logs\n    // TODO 4: Return error if both buffers are full (apply backpressure)\n    // Hint: Use select with default case for non-blocking channel send\n}\n```\n\n**Index Builder** (signature + TODOs):\n\n```go\n// internal/index/inverted_index.go\npackage index\n\nimport (\n    \"github.com/yourorg/log-aggregator/internal/common\"\n)\n\ntype InvertedIndex struct {\n    termToEntries map[string][]uint64  // term -> log entry IDs\n    labelToStreams map[string][]uint32 // label value -> stream IDs\n    bloomFilters  map[string]*BloomFilter // per-partition bloom filters\n}\n\n// AddLogEntry adds a log entry to the inverted index\nfunc (idx *InvertedIndex) AddLogEntry(entryID uint64, entry *common.LogEntry) error {\n    // TODO 1: Extract terms from log message (split on whitespace, normalize case)\n    // TODO 2: Add each term to the inverted index pointing to this entryID\n    // TODO 3: Index each label key-value pair for label-based queries\n    // TODO 4: Update the appropriate bloom filter with extracted terms\n    // TODO 5: If index segment exceeds size threshold, trigger compaction\n    // Hint: Use append() to add entryID to existing term lists\n}\n\n// QueryTerms finds log entries containing all specified terms\nfunc (idx *InvertedIndex) QueryTerms(terms []string) ([]uint64, error) {\n    // TODO 1: For each term, check bloom filter first (negative lookup optimization)\n    // TODO 2: Get posting list for each term from inverted index\n    // TODO 3: Compute intersection of all posting lists (entries containing ALL terms)\n    // TODO 4: Sort result by entry ID for efficient storage access\n    // Hint: Empty result for any term means empty intersection\n}\n```\n\n#### Language-Specific Implementation Hints\n\n**Go-Specific Best Practices:**\n- Use `sync.RWMutex` for read-heavy data structures like indexes - multiple readers can access simultaneously\n- Implement `io.Closer` interface on components that hold resources (files, network connections)\n- Use `context.Context` for cancellation in long-running operations like query execution\n- Leverage `encoding/json` for log parsing but consider streaming decoder for large payloads\n- Use `os.File.Sync()` for WAL durability - it's equivalent to fsync system call\n\n**Error Handling Patterns:**\n```go\n// Wrap errors with context for debugging\nif err := idx.AddLogEntry(entryID, entry); err != nil {\n    return fmt.Errorf(\"failed to index entry %d: %w\", entryID, err)\n}\n\n// Use sentinel errors for expected failure modes\nvar ErrBufferFull = errors.New(\"buffer capacity exceeded\")\n\n// Check for specific error types\nif errors.Is(err, ErrBufferFull) {\n    // Apply backpressure\n}\n```\n\n**Concurrency Patterns:**\n```go\n// Worker pool for parallel log processing\nworkers := runtime.NumCPU()\nlogChan := make(chan *LogEntry, 1000)\nfor i := 0; i < workers; i++ {\n    go func() {\n        for entry := range logChan {\n            processLogEntry(entry)\n        }\n    }()\n}\n```\n\n#### Milestone Checkpoint Verification\n\nAfter implementing the goals and architectural foundation:\n\n**What Should Work:**\n1. `go run cmd/server/main.go` starts the server without errors\n2. Configuration loads from environment variables with sensible defaults\n3. Basic HTTP endpoint responds to health checks\n4. Memory usage remains stable under no-load conditions\n\n**How to Verify:**\n```bash\n# Start the server\ngo run cmd/server/main.go\n\n# Check health endpoint\ncurl http://localhost:8080/health\n\n# Monitor memory usage\nps aux | grep log-aggregator\n```\n\n**Expected Output:**\n- Server starts and binds to configured ports\n- Health endpoint returns JSON status\n- Memory usage stabilizes around 50-100MB baseline\n- No goroutine leaks (use `go tool pprof` to verify)\n\n**Common Issues and Fixes:**\n\n| Symptom | Likely Cause | Diagnostic Steps | Fix |\n|---------|--------------|------------------|-----|\n| Server won't start | Port already in use | `netstat -tulpn \\| grep :8080` | Change HTTP_PORT environment variable |\n| High memory usage | Buffer not being flushed | Check background goroutine status | Implement proper buffer flushing logic |\n| Configuration ignored | Environment variables not set | Print config values at startup | Export variables or use .env file |\n| Import errors | Incorrect module path | Check go.mod file | Update import paths to match module name |\n\n⚠️ **Pitfall: Goroutine Leaks in Server Components**\nMany beginners start background goroutines for buffer flushing or maintenance tasks but forget to implement proper shutdown. This causes goroutine leaks during testing when servers start and stop repeatedly. Always implement context-based cancellation and wait for goroutines to finish in shutdown handlers.\n\n⚠️ **Pitfall: Configuration Validation Missing**\nLoading configuration from environment variables without validation can cause runtime panics later. For example, setting BUFFER_SIZE to zero or negative values will cause channel creation to panic. Always validate configuration values and provide clear error messages for invalid settings.\n\n\n## High-Level Architecture\n\n> **Milestone(s):** This section provides foundational understanding that applies to all milestones (1-5), establishing the overall system design and component relationships that guide implementation throughout the project.\n\nThe log aggregation system follows a **pipeline architecture** where log data flows through distinct processing stages, each optimized for a specific concern. Think of it like a modern manufacturing assembly line - raw materials (log messages) enter at one end and move through specialized stations (components) that each add value before the finished product (indexed, queryable logs) reaches the warehouse (storage).\n\n![System Architecture Overview](./diagrams/system-architecture.svg)\n\nThis architectural approach provides clear separation of concerns, allowing each component to be optimized independently while maintaining predictable data flow. The pipeline design also enables natural scaling points - if ingestion becomes a bottleneck, we can add more ingestion workers without affecting the indexing or query components.\n\n> **Key Design Principle**: Each component owns its data and exposes well-defined interfaces to other components. This ownership model prevents tight coupling and allows components to evolve independently as requirements change.\n\nThe system implements a **write-heavy, read-optimized** design pattern common in observability systems. Log ingestion happens continuously at high volume, but queries are relatively infrequent and typically focus on recent data. This asymmetry drives many architectural decisions throughout the system.\n\n### Component Overview\n\nThe log aggregation system consists of five core components that work together to transform raw log streams into queryable, indexed data. Each component has distinct responsibilities and interfaces with its neighbors through well-defined APIs and data structures.\n\n#### Ingestion Engine\n\nThe **Ingestion Engine** serves as the system's front door, accepting log data from multiple sources and protocols. Think of it as a busy restaurant's host station - it must handle many concurrent arrivals, validate reservations (log formats), and route guests (log entries) to appropriate tables (processing queues) without creating bottlenecks or losing anyone in the chaos.\n\n| Component | Ingestion Engine |\n|-----------|------------------|\n| **Primary Responsibility** | Accept and buffer incoming log data from multiple protocols |\n| **Input Sources** | HTTP POST requests, TCP/UDP syslog streams, file tail agents |\n| **Output** | Structured `LogEntry` objects in processing buffers |\n| **Key Interfaces** | `HTTPHandler`, `SyslogReceiver`, `FileTailer` |\n| **Failure Mode** | Log loss during overload or downstream outages |\n| **Scaling Bottleneck** | Network I/O and parsing CPU cycles |\n\nThe Ingestion Engine implements **backpressure management** to prevent memory exhaustion when downstream components cannot keep up. When buffers approach capacity, it applies increasingly aggressive rate limiting, ultimately rejecting new connections to preserve system stability. This graceful degradation ensures the system remains responsive to queries even during ingestion overload.\n\n> **Architecture Insight**: The Ingestion Engine prioritizes **availability over consistency** - it's better to drop some logs during extreme overload than to crash and lose all logs. This trade-off aligns with observability requirements where partial data is better than no data.\n\n#### Parser Engine\n\nThe **Parser Engine** transforms unstructured log text into structured data that can be efficiently indexed and queried. Imagine a skilled translator at the United Nations who can understand multiple languages (log formats) and convert them all into a common structured language that everyone can understand and work with.\n\n| Component | Parser Engine |\n|-----------|---------------|\n| **Primary Responsibility** | Extract structured fields and labels from raw log messages |\n| **Input** | Raw log strings from various formats (JSON, syslog, custom patterns) |\n| **Output** | Enriched `LogEntry` objects with extracted `Labels` |\n| **Key Interfaces** | `JSONParser`, `SyslogParser`, `RegexParser` |\n| **Failure Mode** | Parse errors leading to lost log content or malformed entries |\n| **Scaling Bottleneck** | Regular expression complexity and label extraction CPU usage |\n\nThe Parser Engine employs a **plugin architecture** where each log format has its own parser implementation sharing a common interface. This design allows adding new log formats without modifying the core parsing logic. Each parser extracts timestamp, message content, and structured labels that will drive query performance.\n\n**Label extraction** is the most critical parser function because labels become the primary query interface. The parser applies configurable extraction rules to pull structured data from log messages - service names, log levels, request IDs, and other metadata that users will want to filter on. High-quality label extraction dramatically improves query performance by enabling efficient index lookups.\n\n#### Index Engine\n\nThe **Index Engine** builds and maintains data structures that enable fast log searches across large datasets. Think of it as a librarian who creates multiple card catalogs - one sorted by author, another by subject, another by publication date - so researchers can quickly find relevant books without scanning every shelf.\n\n| Component | Index Engine |\n|-----------|--------------|\n| **Primary Responsibility** | Build inverted indexes and bloom filters for fast log lookups |\n| **Input** | Structured `LogEntry` objects with extracted labels |\n| **Output** | Inverted index mappings and bloom filter bit arrays |\n| **Key Interfaces** | `InvertedIndex`, `BloomFilter`, `TimePartition` |\n| **Failure Mode** | Index corruption leading to missing query results |\n| **Scaling Bottleneck** | Label cardinality explosion and index memory usage |\n\nThe Index Engine implements **time-based partitioning** where indexes are segmented by time windows (typically hourly or daily). This partitioning strategy allows queries with time ranges to scan only relevant partitions rather than the entire dataset. Each partition maintains its own inverted index and bloom filter, keeping memory usage bounded and enabling parallel query processing.\n\n**Bloom filters** provide a crucial optimization for negative lookups - when a query searches for logs that don't exist, the bloom filter can definitively say \"not here\" without scanning the actual index. This prevents expensive full-index scans for non-existent data, which is common when users search for rare error messages or specific request IDs.\n\n> **Critical Trade-off**: The Index Engine balances query performance against storage cost. More detailed indexes enable faster queries but consume significant memory and disk space. Label cardinality directly drives index size - each unique label combination creates new index entries.\n\n#### Storage Engine\n\nThe **Storage Engine** manages efficient, durable persistence of log data with compression and retention policies. Picture it as a sophisticated warehouse management system that organizes inventory (log data) into compact, labeled boxes (chunks), tracks what's stored where, and automatically removes old inventory according to business rules.\n\n| Component | Storage Engine |\n|-----------|----------------|\n| **Primary Responsibility** | Persist log data in compressed chunks with durability guarantees |\n| **Input** | Batches of indexed `LogEntry` objects ready for storage |\n| **Output** | Compressed chunks written to disk with metadata |\n| **Key Interfaces** | `ChunkWriter`, `WALManager`, `RetentionPolicy` |\n| **Failure Mode** | Data corruption or loss during writes |\n| **Scaling Bottleneck** | Disk I/O bandwidth and compression CPU overhead |\n\nThe Storage Engine organizes logs into **time-windowed chunks** - compressed blocks containing logs from specific time periods. This chunking strategy aligns with query patterns (most queries focus on recent time ranges) and enables efficient compression since logs from the same time period often share common patterns and vocabulary.\n\n**Write-ahead logging (WAL)** ensures durability even during system crashes. Before committing log data to compressed chunks, the Storage Engine writes entries to an append-only WAL that survives crashes. During recovery, it replays the WAL to restore any uncommitted data, ensuring no log loss.\n\nThe **retention policy engine** automatically removes old log data according to configurable rules. It tracks chunk metadata and asynchronously deletes chunks that exceed retention thresholds, preventing unbounded storage growth. Retention policies can be configured per log stream based on labels, allowing different retention periods for different services.\n\n#### Query Engine\n\nThe **Query Engine** processes search requests and returns matching log entries by orchestrating index lookups and storage access. Think of it as a skilled research assistant who understands your research question, knows exactly which card catalogs and archives to check, and efficiently gathers all relevant information while avoiding unnecessary work.\n\n| Component | Query Engine |\n|-----------|--------------|\n| **Primary Responsibility** | Execute LogQL queries and return matching log entries |\n| **Input** | LogQL query strings with time ranges and filter criteria |\n| **Output** | Ordered streams of matching `LogEntry` objects |\n| **Key Interfaces** | `QueryParser`, `ExecutionPlanner`, `ResultStreamer` |\n| **Failure Mode** | Slow or failing queries due to unbounded scans |\n| **Scaling Bottleneck** | Complex query execution and large result set processing |\n\nThe Query Engine implements a **three-phase execution model**: parsing queries into abstract syntax trees, optimizing execution plans to minimize data scanning, and streaming results back to clients. Query optimization focuses on **filter pushdown** - applying the most selective filters first to minimize data processing in subsequent stages.\n\n**Result streaming** prevents memory exhaustion when queries match large numbers of log entries. Instead of buffering all results in memory, the Query Engine streams matching entries back to clients as they're found, using cursor-based pagination to handle result sets larger than memory.\n\n> **Performance Strategy**: The Query Engine prioritizes **time-bounded queries** over exhaustive searches. Queries without time limits are rejected or automatically constrained to prevent full-dataset scans that could impact system performance for all users.\n\n### Data Flow Architecture\n\nThe system processes log data through a **unidirectional pipeline** where each stage adds structure and indexing to enable efficient queries. Understanding this data flow is crucial for debugging performance issues and planning capacity.\n\n![Log Ingestion Sequence](./diagrams/ingestion-flow.svg)\n\n#### Ingestion to Storage Flow\n\nLog data enters the system through multiple ingestion protocols and follows a consistent processing path regardless of source:\n\n1. **Protocol Reception**: HTTP, TCP, UDP, or file-tail agents deliver raw log strings to protocol-specific handlers in the Ingestion Engine\n2. **Format Detection**: The system identifies log format (JSON, syslog RFC 3164/5424, or custom patterns) based on content analysis and source configuration\n3. **Buffering**: Raw log strings are placed in memory buffers with overflow to disk during backpressure situations\n4. **Parsing**: The Parser Engine extracts structured fields including timestamp, message content, and labels from raw strings\n5. **Validation**: Parsed entries are validated for required fields (timestamp, message) and label format compliance\n6. **Indexing**: The Index Engine updates inverted indexes and bloom filters with extracted terms and labels\n7. **Batching**: Validated entries accumulate in storage batches organized by time windows\n8. **WAL Writing**: Complete batches are written to the write-ahead log for durability\n9. **Compression**: Batches are compressed using configured algorithms (gzip, snappy, or zstd)\n10. **Chunk Storage**: Compressed batches become chunks stored on disk with metadata for retrieval\n\nThis pipeline implements **at-least-once delivery semantics** where log entries may be processed multiple times during failures, but will never be lost once successfully written to the WAL. Duplicate detection during recovery prevents multiple copies of the same log entry in final storage.\n\n> **Backpressure Propagation**: When any downstream stage becomes overloaded, backpressure flows upstream through the pipeline. Storage pressure causes batching delays, which fill index queues, which eventually cause ingestion buffering and finally connection rejection.\n\n#### Query Processing Flow\n\nQuery execution follows a different path optimized for fast data retrieval:\n\n![Query Processing Sequence](./diagrams/query-flow.svg)\n\n1. **Query Parsing**: LogQL query strings are parsed into abstract syntax trees with validation for syntax errors\n2. **Time Range Extraction**: Query time bounds determine which index partitions and storage chunks to examine\n3. **Execution Planning**: The query planner generates an optimized execution sequence with filter pushdown\n4. **Index Consultation**: Bloom filters provide fast negative lookups, while inverted indexes identify candidate chunks\n5. **Chunk Loading**: Only chunks likely to contain matches are loaded from storage and decompressed\n6. **Entry Filtering**: Log entries from loaded chunks are tested against query filters\n7. **Result Ordering**: Matching entries are sorted by timestamp for consistent result ordering\n8. **Streaming Response**: Results stream back to clients using cursor-based pagination\n\nThe query path implements **short-circuit evaluation** where subsequent processing stages are skipped when earlier stages determine no matches are possible. Bloom filter negative responses eliminate chunk loads, while time range validation skips entire partitions.\n\n**Query optimization** focuses on minimizing I/O operations since storage access dominates query latency. The execution planner reorders filters to apply the most selective constraints first, reducing the amount of data that must be decompressed and evaluated.\n\n### Recommended Project Structure\n\nThe codebase organization reflects the component architecture with clear module boundaries and shared infrastructure. This structure supports independent development of components while maintaining clean interfaces.\n\n```\nlogaggr/\n├── cmd/\n│   ├── server/                    ← Main server entry point\n│   │   └── main.go\n│   ├── ingester/                  ← Standalone ingestion service\n│   │   └── main.go\n│   └── query/                     ← Query-only service for scaling\n│       └── main.go\n├── internal/                      ← Private implementation packages\n│   ├── ingestion/                 ← Ingestion Engine (Milestone 1)\n│   │   ├── http.go               ← HTTP log receiver\n│   │   ├── syslog.go             ← TCP/UDP syslog receiver\n│   │   ├── filetail.go           ← File watching agent\n│   │   ├── buffer.go             ← Memory/disk buffering\n│   │   └── backpressure.go       ← Flow control logic\n│   ├── parser/                    ← Parser Engine (Milestone 1)\n│   │   ├── json.go               ← JSON log parser\n│   │   ├── syslog.go             ← Syslog format parser\n│   │   ├── regex.go              ← Pattern-based parser\n│   │   └── labels.go             ← Label extraction logic\n│   ├── index/                     ← Index Engine (Milestone 2)\n│   │   ├── inverted.go           ← Inverted index implementation\n│   │   ├── bloom.go              ← Bloom filter implementation\n│   │   ├── partition.go          ← Time-based partitioning\n│   │   └── compaction.go         ← Index maintenance\n│   ├── storage/                   ← Storage Engine (Milestone 4)\n│   │   ├── chunks.go             ← Chunk management\n│   │   ├── wal.go                ← Write-ahead logging\n│   │   ├── compression.go        ← Compression algorithms\n│   │   └── retention.go          ← Retention policy engine\n│   ├── query/                     ← Query Engine (Milestone 3)\n│   │   ├── parser.go             ← LogQL parser\n│   │   ├── planner.go            ← Query optimization\n│   │   ├── executor.go           ← Query execution\n│   │   └── streaming.go          ← Result streaming\n│   ├── tenant/                    ← Multi-tenancy (Milestone 5)\n│   │   ├── isolation.go          ← Tenant data separation\n│   │   ├── ratelimit.go          ← Per-tenant rate limiting\n│   │   └── auth.go               ← Authentication/authorization\n│   └── alerting/                  ← Log-based alerting (Milestone 5)\n│       ├── rules.go              ← Alert rule evaluation\n│       ├── notification.go       ← Alert delivery\n│       └── dedup.go              ← Alert deduplication\n├── pkg/                          ← Public API packages\n│   ├── types/                    ← Shared data types\n│   │   ├── log.go               ← LogEntry, Labels, LogStream\n│   │   ├── config.go            ← Config, Metrics\n│   │   └── query.go             ← TimeRange, query types\n│   └── client/                   ← Client library\n│       └── client.go            ← HTTP client for log submission\n├── api/                         ← API definitions\n│   ├── http/                    ← HTTP API handlers\n│   │   ├── ingest.go           ← Log ingestion endpoints\n│   │   └── query.go            ← Query API endpoints\n│   └── grpc/                   ← gRPC definitions (future)\n├── configs/                    ← Configuration files\n│   ├── local.yaml             ← Local development config\n│   └── production.yaml        ← Production configuration\n├── deployments/               ← Deployment configurations\n│   ├── docker/               ← Docker configurations\n│   └── kubernetes/           ← K8s manifests\n├── docs/                     ← Documentation\n└── scripts/                  ← Build and utility scripts\n    ├── build.sh\n    └── test.sh\n```\n\nThis structure provides **clear separation of concerns** where each internal package has a single responsibility. The `pkg/` directory contains types and interfaces that multiple components share, while `internal/` packages implement component-specific logic that shouldn't be imported by external projects.\n\n> **Development Strategy**: Start by implementing the shared types in `pkg/types/`, then build components in milestone order. Each milestone adds new packages without modifying existing ones, supporting incremental development and testing.\n\nThe **three-tier organization** (cmd/internal/pkg) follows Go community conventions where:\n- `cmd/` contains executable entry points with minimal logic\n- `internal/` holds private implementation details\n- `pkg/` exposes public APIs that external projects could import\n\n**Configuration management** centralizes all system settings in the `configs/` directory with environment-specific files. This approach supports different configurations for development, testing, and production without code changes.\n\n> **Architecture Decision: Monorepo vs. Multi-repo**\n> - **Context**: The system has five distinct components that could be separate services or combined into a monolithic deployment\n> - **Options Considered**:\n>   - Separate repositories for each component with independent deployment\n>   - Single repository with multiple deployment targets\n>   - Hybrid approach with shared types repository and separate service repositories\n> - **Decision**: Single repository with flexible deployment options\n> - **Rationale**: Shared data types and interfaces create tight coupling between components. Separate repositories would require complex dependency management and version synchronization. A monorepo simplifies development while still supporting separate service deployment through multiple `cmd/` entry points.\n> - **Consequences**: Enables rapid development and consistent interfaces, but requires disciplined module boundaries to prevent tight coupling in the codebase\n\n### Implementation Guidance\n\nThe log aggregation system requires careful technology choices that balance development simplicity with production performance. The following recommendations provide a clear development path from prototype to production-ready system.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option | Rationale |\n|-----------|---------------|-----------------|-----------|\n| **HTTP Server** | `net/http` with `gorilla/mux` | `gin-gonic/gin` with middleware | Standard library provides sufficient performance; advanced option adds convenience |\n| **TCP/UDP Handling** | `net.Listen` with worker pools | `fasthttp` with connection pooling | Standard library handles syslog protocols well; optimization can come later |\n| **JSON Parsing** | `encoding/json` with struct tags | `json-iterator/go` for performance | Standard library sufficient for most throughput; advanced option for high-scale |\n| **Storage Backend** | Local filesystem with `os` package | S3-compatible with `minio-go` | Local storage simplifies development; object storage scales better |\n| **Compression** | `compress/gzip` standard library | `klauspost/compress` optimized versions | Standard library provides good compression; advanced option offers speed |\n| **Concurrency** | `sync` package with worker pools | `ants` goroutine pool library | Manual pool management teaches fundamentals; library provides production features |\n\n#### File Structure Foundation\n\nStart development with this minimal structure that expands as you implement each milestone:\n\n```go\n// pkg/types/log.go - Core data types used throughout the system\npackage types\n\nimport \"time\"\n\n// LogEntry represents a single log message with metadata\ntype LogEntry struct {\n    Timestamp time.Time         // When the log was created\n    Labels    Labels           // Key-value pairs for filtering  \n    Message   string           // The actual log content\n}\n\n// Labels provides structured metadata for log entries\ntype Labels map[string]string\n\n// LogStream groups related log entries by label set\ntype LogStream struct {\n    Labels  Labels      // Common labels for all entries\n    Entries []LogEntry  // Time-ordered log entries\n}\n\n// TimeRange specifies query time boundaries\ntype TimeRange struct {\n    Start time.Time    // Inclusive start time\n    End   time.Time    // Exclusive end time  \n}\n\n// Config holds all system configuration\ntype Config struct {\n    HTTPPort      int    // HTTP ingestion port\n    TCPPort       int    // TCP syslog port\n    UDPPort       int    // UDP syslog port  \n    StoragePath   string // Local storage directory\n    BufferSize    int    // In-memory buffer size\n    ChunkSize     int64  // Storage chunk size in bytes\n    RetentionDays int    // Log retention period\n}\n\n// Metrics tracks system performance\ntype Metrics struct {\n    logsIngested    int64  // Total logs processed\n    queriesExecuted int64  // Total queries handled\n    bytesStored     int64  // Total storage used\n}\n```\n\n```go\n// pkg/types/config.go - Configuration management\npackage types\n\nimport (\n    \"os\"\n    \"strconv\"\n)\n\n// Configuration constants\nconst (\n    HTTP_PORT      = 8080\n    TCP_PORT       = 1514  \n    UDP_PORT       = 1514\n    STORAGE_PATH   = \"./data\"\n    BUFFER_SIZE    = 10000\n    CHUNK_SIZE     = 1024 * 1024 // 1MB\n    RETENTION_DAYS = 30\n)\n\n// LoadConfig reads configuration from environment variables with defaults\nfunc LoadConfig() *Config {\n    config := &Config{\n        HTTPPort:      getEnvInt(\"HTTP_PORT\", HTTP_PORT),\n        TCPPort:       getEnvInt(\"TCP_PORT\", TCP_PORT), \n        UDPPort:       getEnvInt(\"UDP_PORT\", UDP_PORT),\n        StoragePath:   getEnvString(\"STORAGE_PATH\", STORAGE_PATH),\n        BufferSize:    getEnvInt(\"BUFFER_SIZE\", BUFFER_SIZE),\n        ChunkSize:     int64(getEnvInt(\"CHUNK_SIZE\", CHUNK_SIZE)),\n        RetentionDays: getEnvInt(\"RETENTION_DAYS\", RETENTION_DAYS),\n    }\n    \n    // Create storage directory if it doesn't exist\n    os.MkdirAll(config.StoragePath, 0755)\n    \n    return config\n}\n\n// Helper functions for environment variable parsing\nfunc getEnvInt(key string, defaultVal int) int {\n    if val := os.Getenv(key); val != \"\" {\n        if parsed, err := strconv.Atoi(val); err == nil {\n            return parsed\n        }\n    }\n    return defaultVal\n}\n\nfunc getEnvString(key, defaultVal string) string {\n    if val := os.Getenv(key); val != \"\" {\n        return val\n    }\n    return defaultVal\n}\n```\n\n```go\n// pkg/types/metrics.go - Thread-safe metrics tracking\npackage types\n\nimport \"sync/atomic\"\n\n// IncrementLogsIngested atomically increments the ingestion counter\nfunc (m *Metrics) IncrementLogsIngested() {\n    atomic.AddInt64(&m.logsIngested, 1)\n}\n\n// IncrementQueriesExecuted atomically increments the query counter  \nfunc (m *Metrics) IncrementQueriesExecuted() {\n    atomic.AddInt64(&m.queriesExecuted, 1)\n}\n\n// AddBytesStored atomically adds to the storage counter\nfunc (m *Metrics) AddBytesStored(bytes int64) {\n    atomic.AddInt64(&m.bytesStored, bytes)\n}\n\n// GetStats returns current counters safely\nfunc (m *Metrics) GetStats() (int64, int64) {\n    logs := atomic.LoadInt64(&m.logsIngested)\n    queries := atomic.LoadInt64(&m.queriesExecuted) \n    return logs, queries\n}\n```\n\n#### Infrastructure Starter Code\n\nThe following utilities handle cross-cutting concerns that aren't the primary learning focus:\n\n```go\n// internal/common/buffer.go - Ring buffer for log batching\npackage common\n\nimport (\n    \"sync\"\n    \"github.com/yourname/logaggr/pkg/types\"\n)\n\n// RingBuffer provides thread-safe circular buffering for log entries\ntype RingBuffer struct {\n    entries []types.LogEntry\n    head    int\n    tail    int\n    size    int\n    maxSize int\n    mutex   sync.RWMutex\n    notEmpty *sync.Cond\n    notFull  *sync.Cond\n}\n\n// NewRingBuffer creates a bounded buffer with the specified capacity\nfunc NewRingBuffer(maxSize int) *RingBuffer {\n    rb := &RingBuffer{\n        entries: make([]types.LogEntry, maxSize),\n        maxSize: maxSize,\n    }\n    rb.notEmpty = sync.NewCond(&rb.mutex)\n    rb.notFull = sync.NewCond(&rb.mutex)\n    return rb\n}\n\n// Put adds an entry to the buffer, blocking if full\nfunc (rb *RingBuffer) Put(entry types.LogEntry) {\n    rb.mutex.Lock()\n    defer rb.mutex.Unlock()\n    \n    // Wait for space\n    for rb.size == rb.maxSize {\n        rb.notFull.Wait()\n    }\n    \n    rb.entries[rb.tail] = entry\n    rb.tail = (rb.tail + 1) % rb.maxSize\n    rb.size++\n    \n    rb.notEmpty.Signal()\n}\n\n// Take removes and returns an entry, blocking if empty\nfunc (rb *RingBuffer) Take() types.LogEntry {\n    rb.mutex.Lock()\n    defer rb.mutex.Unlock()\n    \n    // Wait for data\n    for rb.size == 0 {\n        rb.notEmpty.Wait()\n    }\n    \n    entry := rb.entries[rb.head]\n    rb.head = (rb.head + 1) % rb.maxSize\n    rb.size--\n    \n    rb.notFull.Signal()\n    return entry\n}\n```\n\n#### Core Logic Skeletons\n\nThese function signatures map to the detailed algorithms described in each component section:\n\n```go\n// cmd/server/main.go - Main application entry point\npackage main\n\nimport (\n    \"context\"\n    \"log\"\n    \"os\"\n    \"os/signal\"\n    \"syscall\"\n    \n    \"github.com/yourname/logaggr/pkg/types\"\n)\n\nfunc main() {\n    // TODO 1: Load configuration using LoadConfig()\n    // TODO 2: Initialize metrics tracking\n    // TODO 3: Start ingestion engine with configured ports\n    // TODO 4: Start index engine background workers\n    // TODO 5: Start storage engine with WAL recovery\n    // TODO 6: Start query engine HTTP server\n    // TODO 7: Setup graceful shutdown on SIGTERM/SIGINT\n    // TODO 8: Wait for shutdown signal and cleanup resources\n    \n    // Hint: Use context.WithCancel for coordinated shutdown\n    // Hint: Start each component in its own goroutine\n    // Hint: Use sync.WaitGroup to wait for clean shutdown\n}\n```\n\n#### Language-Specific Implementation Hints\n\n**Go-Specific Optimization Tips:**\n- Use `sync.Pool` for frequently allocated objects like `LogEntry` structs to reduce GC pressure\n- Implement `io.WriterTo` interface on log chunks for efficient disk writes with `io.Copy`\n- Use `unsafe.Pointer` carefully in bloom filter bit manipulation for performance (advanced)\n- Prefer `[]byte` over `string` in parsing hot paths to avoid allocations\n- Use `sync/atomic` for lock-free counters in metrics collection\n\n**Error Handling Patterns:**\n- Wrap errors with context using `fmt.Errorf(\"operation failed: %w\", err)` for debugging\n- Define custom error types for component-specific failures (ParseError, IndexError)  \n- Use structured logging with levels (ERROR, WARN, INFO, DEBUG) for operational visibility\n- Implement circuit breakers for external dependencies like storage backends\n\n**Concurrency Guidelines:**\n- Use buffered channels for producer-consumer patterns between components\n- Implement worker pools with configurable sizes for CPU-bound operations\n- Use `context.Context` for request cancellation and timeouts throughout the system\n- Apply read-write mutexes (`sync.RWMutex`) for data structures with frequent reads\n\n#### Milestone Checkpoints\n\n**After Milestone 1 (Ingestion):**\n- Run `curl -X POST localhost:8080/ingest -d '{\"timestamp\":\"2023-01-01T10:00:00Z\",\"level\":\"INFO\",\"message\":\"test log\"}'`\n- Verify response: `{\"status\":\"accepted\",\"entries\":1}`\n- Check logs directory contains new entries: `ls -la ./data/`\n- Send 100 concurrent requests: should handle without errors or memory leaks\n\n**After Milestone 2 (Indexing):**\n- Submit logs with various labels: `level=ERROR`, `service=api`, `host=server1`\n- Verify index files created: `ls -la ./data/indexes/`\n- Check index contains expected terms: implement debug endpoint showing index contents\n- Test bloom filter efficiency: query for non-existent terms should return quickly\n\n**After Milestone 3 (Querying):**\n- Execute basic query: `curl \"localhost:8080/query?q=level=ERROR&start=1h\"`\n- Verify results contain only ERROR level logs with proper JSON formatting\n- Test regex queries: `q=message~\"database.*timeout\"` should match pattern\n- Performance test: queries over large datasets should complete within 5 seconds\n\n**After Milestone 4 (Storage):**\n- Verify chunk compression: storage files should be significantly smaller than raw logs\n- Test WAL recovery: kill process during ingestion, restart, verify no data loss\n- Check retention: configure 1-day retention, verify old chunks are deleted\n- Monitor storage growth: should be bounded by retention policies\n\n**After Milestone 5 (Multi-tenancy):**  \n- Submit logs with different tenant headers: `X-Tenant-ID: tenant1`\n- Verify tenant isolation: tenant1 queries should not return tenant2 logs\n- Test rate limiting: exceed tenant limits should return 429 status\n- Configure alert rules: error rate spikes should trigger notifications\n\n\n## Data Model\n\n> **Milestone(s):** This section provides foundational data structures used across all milestones (1-5), with core types introduced in Milestone 1 (Log Ingestion), index structures in Milestone 2 (Log Index), and storage formats in Milestone 4 (Log Storage & Compression).\n\n### Mental Model: The Postal Service Data System\n\nBefore diving into the technical data structures, think of our log aggregation system like a comprehensive postal service data management system. When the postal service processes mail, they need several types of data structures:\n\n**Letters (Log Entries)**: Each piece of mail has a timestamp (postmark), addressing information (labels), and content (message). The postal service doesn't modify the letter content, but they add metadata like routing stamps and sorting codes.\n\n**Address Books (Labels)**: Every letter has addressing information - sender, recipient, postal codes, delivery routes. This information is structured as key-value pairs (street=Main, city=Springfield, zip=12345) and is used for routing and organization.\n\n**Card Catalogs (Indexes)**: The postal service maintains catalogs that let them quickly find \"all mail going to zip code 12345\" or \"all express mail from yesterday.\" These catalogs don't contain the actual letters - they contain pointers to where letters can be found.\n\n**Storage Boxes (Chunks)**: Letters are bundled into containers organized by time and destination. Each container is compressed to save space, labeled with metadata about its contents, and stored in warehouses with retention policies.\n\n**Filing Systems (Serialization)**: Everything must be written down in standardized formats so different postal workers can read and process the information consistently, even after shifts change or systems restart.\n\nThis mental model helps us understand why our data structures are designed the way they are - we need efficient ways to represent, organize, find, and store log data at scale.\n\n![Data Model Relationships](./diagrams/data-model.svg)\n\n### Core Data Types\n\nThe foundation of our log aggregation system rests on three primary data types that represent the fundamental units of information. These types are designed to be simple, efficient, and composable, allowing them to flow through our ingestion, indexing, and query pipelines without unnecessary transformations.\n\n#### LogEntry Structure\n\nThe `LogEntry` represents a single log message with its associated metadata. This is the atomic unit of data in our system - every log line that enters our system becomes a `LogEntry`, and every query result returns collections of `LogEntry` instances.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| Timestamp | time.Time | Precise moment when log event occurred, stored in UTC with nanosecond precision for ordering |\n| Labels | Labels | Key-value pairs providing structured metadata about the log source and context |\n| Message | string | Raw log content exactly as received, with no parsing or modification applied |\n\nThe design of `LogEntry` reflects several important decisions. The timestamp uses Go's `time.Time` type, which provides nanosecond precision and handles timezone conversions automatically. This precision is crucial for maintaining log ordering, especially in high-throughput systems where multiple logs might arrive within the same millisecond. The labels field contains structured metadata that enables efficient querying and filtering. The message field preserves the original log content unchanged, ensuring we never lose information during ingestion.\n\n> **Decision: Immutable LogEntry Design**\n> - **Context**: Log entries could be mutable (allowing post-ingestion modifications) or immutable (fixed after creation)\n> - **Options Considered**: Mutable entries (allows correction/enrichment), immutable entries (ensures data integrity), hybrid approach (some fields mutable)\n> - **Decision**: Completely immutable `LogEntry` instances\n> - **Rationale**: Immutability prevents accidental data corruption, enables safe concurrent access without locks, simplifies reasoning about data flow, and ensures audit trails remain intact\n> - **Consequences**: Any enrichment or correction requires creating new entries rather than modifying existing ones, which increases storage slightly but dramatically improves system reliability\n\n| Design Option | Pros | Cons | Chosen? |\n|---------------|------|------|---------|\n| Mutable LogEntry | Can fix/enrich data post-ingestion, Lower memory usage | Race conditions, Data corruption risk, Complex synchronization | No |\n| Immutable LogEntry | Thread-safe, Predictable behavior, Audit integrity | Slight storage overhead, Cannot correct errors in-place | Yes ✅ |\n| Hybrid Approach | Flexibility with safety for critical fields | Complex rules, Partial safety only | No |\n\n#### Labels Structure\n\nLabels provide the structured metadata that makes log entries queryable and filterable. The `Labels` type is implemented as a simple map, but its usage patterns and constraints are carefully designed to balance query performance with storage efficiency.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| Labels | map[string]string | Key-value pairs where keys are label names and values are label values, both stored as UTF-8 strings |\n\nLabels serve as the primary mechanism for log organization and querying. Common label keys include `service`, `level`, `host`, `environment`, and `request_id`. The values should be relatively low-cardinality - for example, `level` might have values like `error`, `warn`, `info`, `debug`, but should not contain unique values like timestamps or request IDs unless specifically needed for correlation.\n\n> **Decision: String-Only Label Values**\n> - **Context**: Labels could support multiple data types (strings, numbers, booleans) or be string-only\n> - **Options Considered**: Typed labels (native int/bool/float support), string-only labels, JSON-encoded complex types\n> - **Decision**: All label values stored as strings\n> - **Rationale**: Simplifies indexing logic, avoids type conversion errors, matches common logging practice where structured data is string-formatted, reduces serialization complexity\n> - **Consequences**: Numeric comparisons require string parsing, but this matches how most log systems work and keeps the implementation straightforward\n\nThe label system must carefully manage cardinality to prevent index explosion. High-cardinality labels (those with many unique values) can dramatically increase index size and query time. Our design includes monitoring and alerting for cardinality growth.\n\n| Label Cardinality | Index Impact | Query Performance | Storage Impact | Recommended Use |\n|-------------------|--------------|-------------------|----------------|-----------------|\n| Low (< 100 values) | Minimal index growth | Fast lookups | Efficient | Primary filtering (service, level, env) |\n| Medium (100-1000) | Moderate index size | Good performance | Acceptable | Secondary attributes (host, version) |\n| High (> 1000) | Large index growth | Slower queries | Storage intensive | Avoid or use sparingly (trace_id) |\n| Unbounded | Index explosion | Query timeouts | Unsustainable | Never use (timestamp, unique_id) |\n\n#### TimeRange Structure\n\nTime-based querying is fundamental to log analysis, and the `TimeRange` structure provides a standardized way to specify temporal boundaries for queries and storage operations.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| Start | time.Time | Inclusive beginning of the time range, logs at exactly this timestamp are included |\n| End | time.Time | Exclusive end of the time range, logs at exactly this timestamp are excluded |\n\nThe `TimeRange` uses half-open interval semantics [Start, End) which aligns with standard programming practices and prevents double-counting when adjacent time ranges are processed. This structure is used throughout the system for query filtering, chunk organization, retention policy application, and index partitioning.\n\n#### LogStream Structure\n\nWhen working with collections of related log entries, the `LogStream` structure groups entries that share the same labels, representing a continuous stream of logs from a specific source.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| Labels | Labels | Common labels shared by all entries in this stream, used for stream identification |\n| Entries | []LogEntry | Ordered collection of log entries, sorted by timestamp in ascending order |\n\nLog streams are the unit of organization for storage and compression. All entries within a stream share identical labels, which allows for efficient storage since the labels only need to be stored once per stream rather than duplicated for each entry. Streams are also the granularity at which retention policies are applied.\n\n> The critical insight with log streams is that grouping by identical labels creates natural boundaries for storage optimization. Within a stream, we only store timestamps and messages since labels are constant, reducing storage overhead by 30-50% in typical deployments.\n\n### Index Data Structures\n\nThe indexing layer transforms our simple log entries into queryable data structures. These indexes must support fast lookups across millions or billions of log entries while remaining compact enough to fit in memory or be quickly loaded from disk.\n\n#### Inverted Index Structure\n\nThe inverted index maps terms (words or label values) to the set of log entries containing those terms. This is the core data structure enabling fast text search and label filtering.\n\n| Component | Type | Description |\n|-----------|------|-------------|\n| TermIndex | map[string]*PostingsList | Maps each unique term to a list of log entries containing that term |\n| PostingsList | []EntryReference | Ordered list of references to log entries, sorted by timestamp for efficient range queries |\n| EntryReference | struct | Compact reference to a log entry containing chunk ID and offset within chunk |\n\n| Field in EntryReference | Type | Description |\n|-------------------------|------|-------------|\n| ChunkID | string | Unique identifier of the storage chunk containing this log entry |\n| Offset | uint32 | Byte offset within the chunk where this log entry begins |\n| Timestamp | time.Time | Copy of entry timestamp for sorting and filtering without chunk access |\n\nThe inverted index design balances memory usage with query performance. Terms are extracted from both the log message text (split on whitespace and punctuation) and label values. Each posting list is kept sorted by timestamp, enabling efficient time-range queries through binary search.\n\n> **Decision: Separate Term Extraction for Messages vs Labels**\n> - **Context**: Text indexing could treat message content and label values identically or differently\n> - **Options Considered**: Unified term extraction, separate message/label indexing, no message text indexing\n> - **Decision**: Separate indexing strategies for message text vs label values\n> - **Rationale**: Label values are structured and should be indexed exactly (service=\"api\"), while message text benefits from tokenization and stemming for full-text search\n> - **Consequences**: More complex indexing logic but much better query performance for both structured queries (level=error) and text search (message contains \"timeout\")\n\nThe term extraction process follows different rules for different content types:\n\n| Content Type | Extraction Method | Example Input | Extracted Terms |\n|--------------|-------------------|---------------|-----------------|\n| Label Values | Exact value indexing | service=\"user-api\" | [\"user-api\"] |\n| Message Text | Tokenization + normalization | \"Request timeout after 30s\" | [\"request\", \"timeout\", \"after\", \"30s\"] |\n| JSON Fields | Key-value extraction | {\"user_id\": 12345} | [\"user_id:12345\"] |\n| Structured Logs | Field-aware parsing | timestamp=... level=ERROR msg=... | [\"ERROR\", extracted message terms] |\n\n#### Bloom Filter Implementation\n\nBloom filters provide fast negative lookups, allowing the system to quickly determine that a term definitely does NOT exist in a chunk without accessing the chunk data. This dramatically reduces disk I/O for queries.\n\n| Component | Type | Description |\n|-----------|------|-------------|\n| BitArray | []uint64 | Packed bit array storing the bloom filter bits, sized for target false positive rate |\n| HashFunctions | []hash.Hash | Set of independent hash functions used for bit setting and testing |\n| Parameters | BloomParams | Configuration controlling filter size and hash count for desired accuracy |\n\n| Field in BloomParams | Type | Description |\n|----------------------|------|-------------|\n| ExpectedElements | uint32 | Estimated number of unique terms this filter will store |\n| FalsePositiveRate | float64 | Target probability of false positives (typically 0.01 = 1%) |\n| BitArraySize | uint32 | Calculated size of bit array needed for target accuracy |\n| HashCount | uint32 | Number of hash functions to use (calculated from other parameters) |\n\nThe bloom filter sizing calculations ensure optimal performance for our expected workload:\n\n1. **Bit Array Sizing**: For `n` expected elements and false positive rate `p`, we need `m = -n * ln(p) / (ln(2)^2)` bits\n2. **Hash Function Count**: Optimal number is `k = (m/n) * ln(2)`, typically 3-5 functions for our parameters  \n3. **Memory Usage**: Each filter uses approximately 1.44 bytes per expected unique term\n4. **Update Protocol**: Filters are immutable once created; chunk compaction creates new filters\n\n> **Decision: Per-Chunk Bloom Filters**\n> - **Context**: Bloom filters could be global (one per index), per-time-partition, or per-chunk\n> - **Options Considered**: Single global filter, time-based filters, chunk-based filters\n> - **Decision**: One bloom filter per storage chunk\n> - **Rationale**: Chunk-based filters have optimal selectivity (can eliminate entire chunks from queries), reasonable memory overhead (filters stay in memory), and align with storage/retention boundaries\n> - **Consequences**: More bloom filters to manage but much better query performance since entire chunks can be skipped\n\n| Filter Scope | Memory Usage | Query Selectivity | Management Complexity | Chosen? |\n|--------------|--------------|-------------------|----------------------|---------|\n| Global Filter | Low memory | Poor selectivity | Simple management | No |\n| Time-Based Filters | Medium memory | Good for time queries | Medium complexity | No |\n| Per-Chunk Filters | Higher memory | Excellent selectivity | More complex | Yes ✅ |\n\n#### Time-Based Partitioning Metadata\n\nThe index is partitioned by time to enable efficient time-range queries and support retention policies. Each partition contains metadata describing its temporal boundaries and contents.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| PartitionID | string | Unique identifier combining time window and partition sequence |\n| TimeRange | TimeRange | Exact time boundaries covered by this partition |\n| ChunkReferences | []string | List of chunk IDs containing data for this time range |\n| IndexSegments | []IndexSegment | Individual index segments within this partition |\n| BloomFilters | map[string]*BloomFilter | Bloom filters keyed by chunk ID for negative lookups |\n| Statistics | PartitionStats | Metrics about partition size, entry count, and query performance |\n\n| Field in IndexSegment | Type | Description |\n|----------------------|------|-------------|\n| SegmentID | string | Unique identifier for this index segment |\n| Terms | map[string]*PostingsList | Subset of the inverted index covering specific chunks |\n| CreatedAt | time.Time | When this segment was created, used for compaction scheduling |\n| ChunkIDs | []string | Which chunks are indexed by this segment |\n\n| Field in PartitionStats | Type | Description |\n|------------------------|------|-------------|\n| EntryCount | int64 | Total number of log entries in this partition |\n| UniqueTerms | int64 | Number of distinct terms across all segments |\n| DiskSize | int64 | Total bytes used by index data on disk |\n| AvgQueryTime | time.Duration | Moving average of query response times |\n| LastAccessed | time.Time | Most recent query timestamp, used for hot/cold storage decisions |\n\nThe partitioning strategy creates natural boundaries for index maintenance:\n\n1. **Hourly Partitions**: Primary partitioning unit balancing granularity with overhead\n2. **Daily Rollups**: Hourly partitions are merged into daily partitions for older data  \n3. **Monthly Archives**: Daily partitions are further consolidated for long-term retention\n4. **Automatic Cleanup**: Partitions beyond retention period are deleted atomically\n\n### Storage and Serialization Formats\n\nThe storage layer must efficiently persist log data while supporting fast retrieval, compression, and retention management. Our storage format is designed around compressed chunks with metadata enabling selective decompression and efficient querying.\n\n![Storage Layout and Chunks](./diagrams/storage-layout.svg)\n\n#### Chunk Storage Format\n\nChunks are the fundamental storage unit, containing a time-ordered collection of log entries with associated metadata and compression. Each chunk is designed to be independently readable and compressible.\n\n| Component | Size | Description |\n|-----------|------|-------------|\n| ChunkHeader | 256 bytes | Fixed-size header with metadata and compression info |\n| StreamHeaders | Variable | Metadata for each log stream contained in this chunk |\n| CompressedData | Variable | Compressed log entry data using specified compression algorithm |\n| IndexFooter | Variable | Offset table enabling random access within compressed data |\n| Checksum | 32 bytes | SHA-256 hash of entire chunk for corruption detection |\n\n| Field in ChunkHeader | Type | Size | Description |\n|---------------------|------|------|-------------|\n| Magic | [4]byte | 4 | File format identifier \"LOGS\" |\n| Version | uint16 | 2 | Chunk format version for backward compatibility |\n| CompressionType | uint8 | 1 | Compression algorithm used (0=none, 1=gzip, 2=lz4, 3=zstd) |\n| StreamCount | uint32 | 4 | Number of log streams in this chunk |\n| EntryCount | uint64 | 8 | Total number of log entries across all streams |\n| UncompressedSize | uint64 | 8 | Size of data before compression |\n| CompressedSize | uint64 | 8 | Size of compressed data section |\n| TimeRange | TimeRange | 32 | Earliest and latest timestamps in chunk |\n| CreatedAt | time.Time | 16 | When chunk was created |\n| Reserved | [169]byte | 169 | Reserved space for future extensions |\n\n| Field in StreamHeader | Type | Description |\n|-----------------------|------|-------------|\n| StreamID | string | Unique identifier for this log stream |\n| Labels | Labels | Common labels shared by all entries in this stream |\n| EntryCount | uint32 | Number of entries for this stream within the chunk |\n| CompressedOffset | uint64 | Byte offset where this stream's data begins in compressed section |\n| CompressedSize | uint64 | Size of this stream's compressed data |\n\nThe chunk format enables several important optimizations:\n\n1. **Selective Decompression**: Streams can be decompressed individually using offset information\n2. **Query Filtering**: Time range and label information in headers avoid unnecessary decompression  \n3. **Corruption Detection**: Checksums ensure data integrity with fast validation\n4. **Format Evolution**: Version field and reserved space support future enhancements\n\n> **Decision: Stream-Level Compression Granularity**\n> - **Context**: Compression could be applied at chunk level (all data together) or stream level (each stream separately)\n> - **Options Considered**: Chunk-level compression, stream-level compression, entry-level compression\n> - **Decision**: Stream-level compression within chunks\n> - **Rationale**: Stream-level compression allows selective decompression for queries targeting specific label combinations, while maintaining good compression ratios since entries within a stream are highly similar\n> - **Consequences**: Slightly more complex compression logic but much better query performance when filtering by labels\n\n#### Compression Strategy Analysis\n\nDifferent compression algorithms offer trade-offs between compression ratio, compression speed, decompression speed, and memory usage. Our system must handle high ingestion rates while maintaining fast query response times.\n\n| Algorithm | Compression Ratio | Compression Speed | Decompression Speed | Memory Usage | Best Use Case |\n|-----------|-------------------|-------------------|---------------------|--------------|---------------|\n| None | 1.0x (baseline) | Instant | Instant | Minimal | Development/debugging |\n| Gzip | 4-6x | Slow (20 MB/s) | Medium (200 MB/s) | Low | Cold storage, bandwidth limited |\n| LZ4 | 2-3x | Very Fast (300 MB/s) | Very Fast (800 MB/s) | Low | Hot data, query-heavy workloads |\n| Zstandard | 3-5x | Fast (100 MB/s) | Fast (400 MB/s) | Medium | Balanced performance, warm storage |\n\nThe compression choice significantly impacts system performance:\n\n1. **Ingestion Performance**: LZ4's fast compression keeps up with high ingestion rates without buffering delays\n2. **Query Performance**: Fast decompression enables responsive queries even on compressed historical data\n3. **Storage Efficiency**: Better compression reduces disk usage and network transfer for distributed deployments\n4. **CPU Resources**: Compression/decompression CPU usage must be balanced against I/O savings\n\n> **Decision: Adaptive Compression Strategy**\n> - **Context**: System could use single compression algorithm or adapt based on data age and access patterns\n> - **Options Considered**: Fixed LZ4 everywhere, fixed Zstd everywhere, adaptive strategy based on data age\n> - **Decision**: Adaptive compression - LZ4 for recent data, Zstd for data older than 24 hours\n> - **Rationale**: Recent data is queried frequently and benefits from fast decompression; older data is accessed less often and benefits from better compression ratios\n> - **Consequences**: More complex storage management but optimal performance for both ingestion and long-term storage efficiency\n\n#### Write-Ahead Log Format\n\nThe Write-Ahead Log (WAL) ensures durability by recording all operations before they are applied to the main storage. The WAL format must support fast appends, reliable recovery, and efficient cleanup.\n\n| Component | Description |\n|-----------|-------------|\n| WAL Header | Fixed header with WAL metadata and recovery information |\n| Log Records | Sequential records of operations, each with its own header and data |\n| Checkpoints | Periodic markers indicating safe truncation points |\n| Recovery Index | Optional index for faster recovery after crashes |\n\n| Field in WAL Header | Type | Description |\n|---------------------|------|-------------|\n| Magic | [4]byte | WAL format identifier \"WLOG\" |\n| Version | uint16 | WAL format version |\n| SequenceStart | uint64 | First sequence number in this WAL file |\n| CreatedAt | time.Time | WAL creation timestamp |\n| NodeID | string | Identifier of node that created this WAL |\n\n| Field in LogRecord | Type | Description |\n|--------------------|------|-------------|\n| RecordType | uint8 | Type of operation (1=IngestBatch, 2=IndexUpdate, 3=Checkpoint) |\n| SequenceNumber | uint64 | Monotonically increasing sequence number |\n| Timestamp | time.Time | When operation was initiated |\n| DataSize | uint32 | Size of operation data following this header |\n| Checksum | uint32 | CRC32 checksum of record header and data |\n| Data | []byte | Serialized operation data, format depends on RecordType |\n\nThe WAL recovery process follows a deterministic algorithm:\n\n1. **WAL Discovery**: Scan storage directory for all WAL files, sort by sequence number ranges\n2. **Integrity Check**: Validate checksums for all records, identify any corruption boundaries  \n3. **Operation Replay**: Re-execute all operations after the last checkpoint in sequence order\n4. **State Validation**: Verify final system state matches expected state from WAL operations\n5. **Cleanup**: Truncate WAL at first checkpoint after successful state verification\n\n| Recovery Scenario | Detection Method | Recovery Action | Data Loss Risk |\n|-------------------|------------------|------------------|---------------|\n| Clean Shutdown | WAL ends with checkpoint record | No recovery needed | None |\n| Crash During Ingestion | WAL ends mid-operation | Replay from last checkpoint | Partial batch only |\n| WAL Corruption | Checksum validation fails | Replay up to corruption point | Records after corruption |\n| Multiple WAL Files | Sequence number gaps | Replay all valid files in order | Gap data lost |\n\n#### Serialization Protocols\n\nAll data structures must be serialized for persistence and network transmission. Our serialization strategy balances performance, compatibility, and debuggability.\n\n| Data Type | Serialization Format | Rationale |\n|-----------|---------------------|-----------|\n| LogEntry | MessagePack | Compact binary format, faster than JSON, good language support |\n| Labels | JSON | Human-readable, debuggable, standard format |\n| Index Data | Custom Binary | Maximum performance for frequently accessed structures |\n| Configuration | YAML | Human-editable, comments supported, clear structure |\n| API Messages | Protocol Buffers | Schema evolution, efficient network protocol |\n\n| Format | Serialization Speed | Size Efficiency | Human Readable | Schema Evolution |\n|--------|-------------------|-----------------|----------------|------------------|\n| JSON | Medium | Poor | Yes | Limited |\n| MessagePack | Fast | Good | No | None |\n| Protocol Buffers | Fast | Excellent | No | Excellent |\n| Custom Binary | Fastest | Best | No | Manual |\n\nThe serialization choice affects multiple aspects of system performance:\n\n1. **Network Bandwidth**: Efficient serialization reduces data transfer costs in distributed deployments\n2. **CPU Overhead**: Fast serialization/deserialization improves ingestion and query throughput\n3. **Storage Space**: Compact formats reduce disk usage, especially for metadata structures\n4. **Debuggability**: Human-readable formats aid troubleshooting but consume more resources\n5. **Compatibility**: Standard formats ease integration with external tools and monitoring systems\n\n> ⚠️ **Pitfall: Inconsistent Timestamp Serialization**\n> When serializing timestamps, different formats handle precision and timezone information differently. Always use UTC timestamps with consistent precision (nanoseconds) and explicit timezone information. Avoid Unix timestamps which lose precision and timezone context.\n\n> ⚠️ **Pitfall: Label Key Ordering Assumptions**\n> Never assume that label keys will be serialized or deserialized in any particular order. Always use stable sorting when label ordering matters for indexing or comparison operations. Map iteration order is not guaranteed in most languages.\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Serialization | JSON with encoding/json | MessagePack with vmihailenco/msgpack |\n| Time Handling | time.Time with RFC3339 | Custom timestamp with nanosecond precision |\n| Compression | gzip with compress/gzip | LZ4 with pierrec/lz4 or Zstd with klauspost/compress |\n| Hashing (Bloom Filters) | hash/fnv and crypto/sha256 | Custom hash with xxhash |\n| Storage Backend | Local filesystem with os.File | Pluggable interface supporting S3/GCS |\n\n#### Recommended File Structure\n\n```\ninternal/\n  models/\n    log_entry.go              ← Core data types (LogEntry, Labels, TimeRange)\n    log_stream.go             ← LogStream and stream management\n    config.go                 ← Configuration structures and validation\n    metrics.go                ← Metrics and statistics types\n  index/\n    inverted_index.go         ← Inverted index structures and operations\n    bloom_filter.go           ← Bloom filter implementation\n    partition.go              ← Time-based partitioning metadata\n  storage/\n    chunk.go                  ← Chunk format and serialization\n    compression.go            ← Compression strategy implementation\n    wal.go                    ← Write-ahead log structures\n  serialization/\n    formats.go                ← Serialization format implementations\n    timestamps.go             ← Timestamp handling utilities\n```\n\n#### Core Data Types Implementation\n\n```go\npackage models\n\nimport (\n    \"time\"\n    \"encoding/json\"\n    \"fmt\"\n    \"sort\"\n)\n\n// LogEntry represents a single log message with metadata\ntype LogEntry struct {\n    Timestamp time.Time `json:\"timestamp\" msgpack:\"timestamp\"`\n    Labels    Labels    `json:\"labels\" msgpack:\"labels\"`\n    Message   string    `json:\"message\" msgpack:\"message\"`\n}\n\n// Labels represents key-value metadata for log entries\ntype Labels map[string]string\n\n// NewLogEntry creates a new LogEntry with validation\nfunc NewLogEntry(timestamp time.Time, labels Labels, message string) (*LogEntry, error) {\n    // TODO 1: Validate timestamp is not zero and not too far in future\n    // TODO 2: Validate labels map is not nil and contains valid UTF-8\n    // TODO 3: Validate message is not empty and contains valid UTF-8\n    // TODO 4: Normalize timestamp to UTC if needed\n    // TODO 5: Return validated LogEntry instance\n    return nil, nil\n}\n\n// String creates a canonical string representation\nfunc (le *LogEntry) String() string {\n    // TODO 1: Format timestamp as RFC3339 with nanosecond precision\n    // TODO 2: Sort labels by key for consistent output\n    // TODO 3: Format as: timestamp [labels] message\n    // TODO 4: Escape any special characters in message\n    return \"\"\n}\n\n// Equal checks if two LogEntry instances are identical\nfunc (le *LogEntry) Equal(other *LogEntry) bool {\n    // TODO 1: Compare timestamps with nanosecond precision\n    // TODO 2: Compare message strings exactly  \n    // TODO 3: Compare labels maps (order-independent)\n    // TODO 4: Return true only if all fields match\n    return false\n}\n\n// Clone creates a deep copy of the LogEntry\nfunc (le *LogEntry) Clone() *LogEntry {\n    // TODO 1: Create new LogEntry with same timestamp and message\n    // TODO 2: Create new Labels map and copy all key-value pairs\n    // TODO 3: Return the cloned instance\n    // Hint: This ensures immutability when passing LogEntries between goroutines\n    return nil\n}\n```\n\n#### Labels Management Utilities\n\n```go\n// String creates a sorted string representation of labels\nfunc (l Labels) String() string {\n    // TODO 1: Extract all keys from the map into a slice\n    // TODO 2: Sort keys alphabetically for consistent output\n    // TODO 3: Build string as key1=value1,key2=value2\n    // TODO 4: Escape any special characters in keys or values\n    return \"\"\n}\n\n// Hash creates a consistent hash of the labels\nfunc (l Labels) Hash() uint64 {\n    // TODO 1: Sort labels by key for consistent hashing\n    // TODO 2: Concatenate all key=value pairs with separator\n    // TODO 3: Use a fast hash function (like xxhash or fnv)\n    // TODO 4: Return the hash value for indexing\n    // Hint: This hash is used for stream identification and partitioning\n    return 0\n}\n\n// Validate checks if labels contain valid keys and values\nfunc (l Labels) Validate() error {\n    // TODO 1: Check each key is non-empty and valid UTF-8\n    // TODO 2: Check each value is valid UTF-8 (empty values allowed)\n    // TODO 3: Check for reserved key names (like __internal__)\n    // TODO 4: Check total serialized size doesn't exceed limits\n    // TODO 5: Return error describing any validation failures\n    return nil\n}\n\n// Merge creates a new Labels map combining this and other\nfunc (l Labels) Merge(other Labels) Labels {\n    // TODO 1: Create new Labels map with capacity for both\n    // TODO 2: Copy all key-value pairs from current labels\n    // TODO 3: Copy all key-value pairs from other labels (overwrites conflicts)\n    // TODO 4: Return the merged labels\n    // Hint: Other labels take precedence in case of key conflicts\n    return nil\n}\n```\n\n#### TimeRange Operations\n\n```go\npackage models\n\n// TimeRange represents a time interval with half-open semantics [Start, End)\ntype TimeRange struct {\n    Start time.Time `json:\"start\" msgpack:\"start\"`\n    End   time.Time `json:\"end\" msgpack:\"end\"`\n}\n\n// NewTimeRange creates a validated TimeRange\nfunc NewTimeRange(start, end time.Time) (*TimeRange, error) {\n    // TODO 1: Ensure start is before end\n    // TODO 2: Normalize both timestamps to UTC\n    // TODO 3: Validate timestamps are not zero values\n    // TODO 4: Return TimeRange or validation error\n    return nil, nil\n}\n\n// Contains checks if a timestamp falls within this range\nfunc (tr *TimeRange) Contains(timestamp time.Time) bool {\n    // TODO 1: Check timestamp >= Start (inclusive)\n    // TODO 2: Check timestamp < End (exclusive)\n    // TODO 3: Return true only if both conditions met\n    // Hint: Half-open interval prevents double-counting at boundaries\n    return false\n}\n\n// Overlaps checks if this range overlaps with another\nfunc (tr *TimeRange) Overlaps(other *TimeRange) bool {\n    // TODO 1: Check if other.Start < tr.End (other starts before this ends)\n    // TODO 2: Check if tr.Start < other.End (this starts before other ends)  \n    // TODO 3: Return true only if both conditions met\n    return false\n}\n\n// Duration returns the length of this time range\nfunc (tr *TimeRange) Duration() time.Duration {\n    // TODO 1: Calculate End - Start\n    // TODO 2: Handle case where End <= Start (return 0)\n    return 0\n}\n```\n\n#### Milestone Checkpoints\n\n**After implementing core data types:**\n- Run `go test ./internal/models/...` - all tests should pass\n- Create a LogEntry with labels and verify it serializes to JSON correctly\n- Test Labels.Hash() produces consistent results for same label sets\n- Verify TimeRange.Contains() works correctly with boundary conditions\n\n**What should work:**\n```bash\n# Create test log entry\nentry := &LogEntry{\n    Timestamp: time.Now(),\n    Labels: Labels{\"service\": \"api\", \"level\": \"error\"},\n    Message: \"Request timeout\",\n}\n\n# Should serialize/deserialize without data loss\ndata, _ := json.Marshal(entry)\nvar decoded LogEntry\njson.Unmarshal(data, &decoded)\nassert.Equal(t, entry, &decoded)\n```\n\n**Common debugging issues:**\n- Timestamp precision loss during JSON serialization - use RFC3339Nano format\n- Labels map iteration order causing hash inconsistencies - always sort keys\n- Memory leaks from not cloning Labels when sharing between goroutines\n- TimeRange boundary errors from using closed intervals instead of half-open\n\n\n## Log Ingestion Engine\n\n> **Milestone(s):** This section corresponds to Milestone 1 (Log Ingestion), covering the foundational capability to receive, parse, buffer, and route log data from multiple sources into the aggregation system.\n\n### Mental Model: The Mail Sorting Facility\n\nThink of the log ingestion engine as a large mail sorting facility at a major postal hub. Just as the postal facility receives mail through multiple channels—trucks arriving at loading docks, mail drops from local routes, and direct deliveries—our log ingestion engine receives log data through multiple protocols: HTTP endpoints, TCP connections, UDP packets, and file monitoring agents.\n\nThe postal facility has different processing workflows for different types of mail. Priority mail gets expedited handling, while bulk mail goes through standard processing. Similarly, our ingestion engine has different parsing pipelines for different log formats: structured JSON logs get fast-path processing, while unstructured syslog messages require more complex parsing to extract meaningful fields.\n\nThe facility uses staging areas and conveyor belts to handle varying volumes of incoming mail without dropping packages, even during peak holiday seasons. Our ingestion engine employs memory buffers and disk-based queues to handle burst traffic and maintain durability when downstream components are temporarily unavailable. Just as the postal facility can temporarily store mail in warehouses during processing delays, our system buffers logs to disk when the indexing or storage engines fall behind.\n\nThe sorting facility routes mail based on addresses and delivery requirements, ensuring each piece reaches its correct destination. Our ingestion engine routes parsed log entries to appropriate storage partitions based on labels, timestamps, and retention policies. Both systems must handle malformed inputs gracefully—unreadable addresses in mail, unparseable log messages in our system—without disrupting the processing of valid entries.\n\nThis mental model helps us understand that ingestion is fundamentally about **reliable reception, intelligent parsing, temporary buffering, and accurate routing** of high-volume, heterogeneous data streams.\n\n![Log Ingestion Sequence](./diagrams/ingestion-flow.svg)\n\n### Protocol Handlers\n\nThe ingestion engine supports multiple protocols to accommodate different log sources and operational requirements. Each protocol serves specific use cases and comes with distinct trade-offs in terms of reliability, performance, and operational complexity.\n\n#### HTTP Log Receiver\n\nThe HTTP endpoint provides the most flexible and widely supported ingestion method. Web applications, microservices, and cloud-native workloads naturally emit logs via HTTP APIs. The receiver accepts JSON-formatted log entries via POST requests to `/api/v1/logs`, providing immediate response codes that indicate successful ingestion or specific error conditions.\n\nHTTP offers several advantages for log ingestion. Applications can batch multiple log entries in a single request, reducing network overhead. The protocol's request-response nature provides immediate feedback about ingestion success or failure, enabling applications to implement retry logic or failover to alternative log destinations. HTTP's stateless nature simplifies load balancing across multiple ingestion endpoints.\n\nHowever, HTTP introduces higher per-message overhead compared to streaming protocols. Each log entry carries HTTP headers, and the request-response cycle adds latency. For high-frequency logging from latency-sensitive applications, this overhead can become significant.\n\nThe HTTP handler implements several reliability mechanisms. It validates Content-Type headers to ensure JSON payloads, performs request size limits to prevent memory exhaustion, and returns structured error responses that help clients diagnose problems. The handler supports both single log entries and arrays of entries, with atomic processing—either all entries in a batch succeed or all fail together.\n\n#### TCP Syslog Receiver  \n\nThe TCP receiver handles RFC 5424 and RFC 3164 syslog messages, providing compatibility with traditional Unix systems, network equipment, and legacy applications. TCP's connection-oriented nature offers reliable delivery semantics that UDP lacks, making it suitable for critical log data where loss is unacceptable.\n\nTCP syslog connections maintain state between client and server, enabling the receiver to detect client disconnections and clean up resources promptly. The protocol handles partial message transmission gracefully—syslog messages can be fragmented across multiple TCP segments, and the receiver reconstructs complete messages before parsing.\n\nThe TCP handler implements connection pooling to support many concurrent log sources efficiently. Each connection runs in a separate goroutine, reading messages in a loop until the client disconnects or an error occurs. The handler implements read timeouts to detect stalled connections and prevent resource leaks from clients that connect but never send data.\n\nTCP syslog parsing must handle the complexity of two different RFC formats. RFC 3164 uses a simpler format with implicit assumptions about timestamp and hostname formatting, while RFC 5424 provides structured data fields and explicit encoding. The parser detects the format based on message structure and applies appropriate parsing rules.\n\n#### UDP Syslog Receiver\n\nThe UDP receiver offers the lowest latency and highest throughput option for log ingestion, making it ideal for high-frequency logging from network devices, embedded systems, and performance-critical applications. UDP's fire-and-forget semantics eliminate connection management overhead and reduce memory footprint on both client and server sides.\n\nUDP excels in environments where losing occasional log messages is acceptable in exchange for minimal performance impact on the log-generating application. Network equipment often prefers UDP syslog because maintaining TCP connections can strain limited embedded system resources.\n\nThe UDP handler manages packet reception through a single socket with a large receive buffer, preventing packet drops during traffic bursts. Unlike TCP, UDP messages arrive as complete, independent packets—there's no message fragmentation to handle. This simplifies the parsing logic but requires careful validation since malformed messages can't be recovered through retransmission.\n\nThe receiver implements several strategies to maximize UDP packet reception rates. It uses multiple worker goroutines reading from the same socket, distributing packet processing load across CPU cores. The large socket receive buffer accommodates temporary processing delays without dropping packets at the kernel level.\n\n#### File Tail Agent\n\nThe file monitoring component watches log files for new content and forwards entries to the ingestion pipeline, providing compatibility with applications that write logs directly to files rather than sending them over the network. This approach is common in traditional server environments and containerized applications that mount log directories as volumes.\n\nFile tailing presents unique challenges compared to network protocols. The agent must detect file rotations, handle temporary file unavailability during log rotation, and maintain position tracking to avoid reprocessing entries after restarts. Modern log rotation tools create complex scenarios: files may be moved, compressed, or deleted while new files appear with the same names.\n\nThe file tail implementation uses filesystem notification APIs (inotify on Linux) to detect file changes efficiently without continuous polling. When files change, the agent reads new content incrementally, parsing complete lines and forwarding them to the ingestion pipeline with synthetic metadata like file path and modification timestamp.\n\nPosition tracking ensures exactly-once processing across agent restarts. The agent periodically checkpoints its read position for each monitored file, storing offsets in a persistent state file. After restarts, the agent resumes reading from the last checkpointed position, avoiding duplicate processing of previously forwarded log entries.\n\nThe following table compares the trade-offs between different protocol handlers:\n\n| Protocol | Throughput | Reliability | Latency | Resource Usage | Operational Complexity |\n|----------|------------|-------------|---------|----------------|----------------------|\n| HTTP | Medium | High | Medium | Medium | Low |\n| TCP Syslog | High | High | Low | Medium | Medium |\n| UDP Syslog | Very High | Medium | Very Low | Low | Low |\n| File Tail | Medium | Very High | High | Low | High |\n\n### Log Parsing Pipeline\n\nThe parsing pipeline transforms raw log data from various formats into the standardized `LogEntry` structure used throughout the system. This transformation process must handle format diversity, extract structured fields, normalize timestamps, and validate label correctness while maintaining high throughput and reliability.\n\n#### JSON Log Parsing\n\nJSON represents the most straightforward parsing path since the format is self-describing and maps naturally to structured data. Applications sending JSON logs typically provide explicit field mappings for timestamp, message content, severity level, and custom labels.\n\nThe JSON parser expects a specific schema for optimal processing, though it handles variations gracefully. The preferred format includes top-level fields for `timestamp`, `message`, `level`, and `labels`, with the labels field containing key-value pairs that become the log entry's label set. Additional fields are preserved as structured content within the message or converted to labels based on configuration rules.\n\nTimestamp parsing requires special attention since JSON doesn't mandate a specific timestamp format. The parser attempts multiple formats in priority order: ISO 8601 with timezone, Unix timestamps (both seconds and milliseconds), and common application-specific formats. When timestamp parsing fails, the parser falls back to the ingestion time, but flags the entry for potential reordering issues.\n\nLabel extraction from JSON follows configurable rules that determine which top-level fields become labels versus message content. Fields like `service`, `host`, `environment`, and `level` typically become labels for efficient querying, while application-specific data remains in the message field. The parser validates label keys and values against naming conventions, rejecting entries with invalid characters or excessive label counts that could cause cardinality explosion.\n\n#### Syslog Message Parsing  \n\nSyslog parsing handles two distinct formats with different complexity levels. RFC 3164 (traditional syslog) uses a fixed format with implicit field positions, while RFC 5424 (structured syslog) provides explicit field delimiters and optional structured data.\n\nRFC 3164 parsing extracts facility and severity from the priority field, timestamp from a fixed position (though format varies by sender), hostname from the next space-delimited token, and treats the remainder as the message content. The parser must handle timestamp format variations since different Unix systems format dates differently, particularly around timezone representation.\n\nRFC 5424 parsing follows a more structured approach with explicit field separators. The format includes version numbers, structured data fields, and standardized timestamp formats. The structured data section contains key-value pairs that the parser extracts as labels, providing richer metadata than traditional syslog.\n\nBoth formats require careful handling of priority field calculation. The priority combines facility (message source type) and severity (importance level) in a single integer value. The parser extracts both components using mathematical operations: facility = priority / 8, severity = priority % 8. These values become labels that enable facility-based and severity-based log filtering.\n\nHostname extraction presents challenges since syslog senders may provide IP addresses, short hostnames, or fully qualified domain names. The parser normalizes hostname representations to support consistent querying while preserving the original value as a separate label for troubleshooting.\n\n#### Regex-Based Pattern Extraction\n\nFor unstructured log formats, the parsing pipeline supports configurable regular expressions that extract structured fields from free-form text. This capability handles legacy applications, proprietary log formats, and complex multi-line log entries that don't conform to standard formats.\n\nRegex patterns use named capture groups to identify fields for extraction. A web server access log pattern might capture IP addresses, timestamps, HTTP methods, URLs, status codes, and response sizes as separate fields that become labels or structured message content. The parser compiles configured patterns at startup for optimal runtime performance.\n\nPattern matching follows a priority order when multiple patterns could apply to a single log line. The parser attempts patterns from most specific to most general, using the first successful match. When no patterns match, the entire line becomes the message content with minimal metadata extracted from the ingestion context.\n\nMulti-line log handling requires stateful parsing that accumulates related lines into single log entries. Java stack traces, SQL query logs, and application debug output often span multiple lines that should be treated as atomic units. The parser uses continuation patterns that identify line relationships and buffer partial entries until complete patterns emerge.\n\nPerformance optimization becomes critical for regex parsing since complex patterns can consume significant CPU resources. The parser implements pattern caching, compiled regex reuse, and timeout mechanisms that prevent pathological backtracking from blocking the ingestion pipeline.\n\nThe following table describes the key components of each parsing pipeline:\n\n| Parser Type | Input Format | Extracted Fields | Performance | Complexity |\n|-------------|-------------|------------------|-------------|------------|\n| JSON | Structured JSON objects | All fields available | High | Low |\n| RFC 5424 Syslog | `<priority>version timestamp hostname app-name proc-id msg-id [structured-data] message` | Priority, timestamp, hostname, app-name, structured data | High | Medium |\n| RFC 3164 Syslog | `<priority>timestamp hostname tag: message` | Priority, timestamp, hostname, tag | High | Medium |\n| Regex Patterns | Custom text formats | Named capture groups | Medium | High |\n\n> **Architecture Decision: Streaming vs. Batch Parsing**\n> - **Context**: Log parsing can process entries individually as they arrive or collect entries into batches for group processing\n> - **Options Considered**: \n>   - Streaming: Parse each log entry immediately upon receipt\n>   - Micro-batching: Collect 10-100 entries before parsing\n>   - Large batching: Accumulate entries for seconds before parsing\n> - **Decision**: Implement streaming parsing with optional micro-batching for specific sources\n> - **Rationale**: Streaming provides the lowest latency for log queries and alerting, which is crucial for operational visibility. Micro-batching can be enabled for high-volume sources where parsing CPU becomes a bottleneck, but most sources benefit from immediate processing.\n> - **Consequences**: Higher CPU usage due to frequent parsing calls, but significantly better query freshness and alerting response times\n\n### Buffering and Backpressure\n\nThe ingestion engine implements sophisticated buffering strategies to handle traffic bursts, downstream outages, and varying processing speeds across pipeline stages. Effective buffering ensures no log loss during temporary overload conditions while maintaining reasonable memory usage and providing clear backpressure signals to upstream sources.\n\n#### Memory Buffer Management\n\nThe primary buffering layer uses in-memory circular buffers that provide fast insertion and removal operations for normal traffic patterns. Each protocol handler feeds parsed log entries into dedicated memory buffers sized according to expected traffic volume and downstream processing capacity.\n\nMemory buffers operate as ring buffers with separate read and write positions. Writers advance the write position after inserting entries, while readers advance the read position after processing entries. When the write position approaches the read position, the buffer is near capacity and triggers backpressure mechanisms before dropping data.\n\nBuffer sizing calculations consider several factors: expected peak ingestion rate, downstream processing capacity, and acceptable memory usage limits. A buffer sized for 10,000 entries with an average entry size of 1KB consumes approximately 10MB of memory. Production deployments typically configure buffers to handle 30-60 seconds of peak traffic, allowing time for temporary downstream slowdowns to resolve.\n\nThe memory buffer implements atomic operations for thread-safe access from multiple producers and consumers. Protocol handlers write entries concurrently while the indexing engine reads entries for processing. Lock-free ring buffer algorithms avoid mutex contention that could limit ingestion throughput.\n\nMemory buffer monitoring tracks utilization levels and triggers increasingly aggressive backpressure as capacity approaches. At 70% utilization, the buffer begins logging warnings. At 85% utilization, it starts rejecting new HTTP requests with 503 status codes. At 95% utilization, it begins dropping UDP packets and disconnecting idle TCP connections to preserve capacity for critical traffic.\n\n#### Disk-Based Overflow Queues\n\nWhen memory buffers reach capacity, the ingestion engine spills entries to disk-based queues that provide virtually unlimited capacity at the cost of increased latency and I/O overhead. Disk queues ensure no log loss during extended downstream outages or traffic spikes that exceed memory buffer capacity.\n\nThe disk queue implementation uses append-only files with simple binary encoding for fast write operations. Each queue entry contains a length prefix, timestamp, serialized labels, and message content. Sequential writes to append-only files provide optimal disk I/O performance while maintaining data durability.\n\nQueue files rotate based on size limits (typically 64MB-256MB) to prevent individual files from becoming unwieldy. The queue manager maintains metadata about active write files, read positions within files, and files eligible for deletion after processing completion. File rotation ensures that disk space consumption remains bounded even during extended queueing periods.\n\nDisk queue reading uses sequential I/O patterns that leverage operating system read-ahead caching effectively. The queue reader maintains persistent checkpoint information that records processing positions within files, enabling recovery to the correct position after restart without reprocessing completed entries.\n\nWrite-ahead logging principles ensure queue durability during system failures. Queue writes use `fsync()` operations to guarantee data reaches persistent storage before acknowledging successful buffering. This durability comes at a performance cost, so disk queuing only activates when memory buffers overflow.\n\n#### Backpressure Propagation\n\nEffective backpressure mechanisms communicate capacity constraints to log sources before data loss occurs, enabling upstream systems to implement appropriate flow control responses. Different protocols support different backpressure signals, requiring protocol-specific implementations.\n\nHTTP backpressure uses standard status codes to communicate capacity constraints. When buffers approach capacity, the HTTP handler returns 503 Service Unavailable responses with Retry-After headers suggesting appropriate wait times. Well-behaved HTTP clients implement exponential backoff retry logic that reduces load automatically during capacity constraints.\n\nTCP backpressure leverages the protocol's built-in flow control mechanisms. When processing falls behind, the TCP receiver stops reading from connection sockets, causing TCP window sizes to shrink and ultimately blocking senders at the socket level. This provides automatic backpressure without requiring application-level protocol changes.\n\nUDP backpressure cannot rely on protocol-level mechanisms since UDP provides no delivery guarantees or flow control. The UDP receiver monitors processing queue depths and begins dropping packets when capacity thresholds are exceeded. Packet dropping follows priority rules that preserve higher-priority log entries when possible.\n\nFile tail backpressure pauses file reading when downstream capacity is constrained. The file monitor stops advancing read positions and relies on operating system filesystem buffers to preserve new log data until processing capacity recovers. This approach prevents memory exhaustion while ensuring no log data loss.\n\nThe following table details backpressure mechanisms for each protocol:\n\n| Protocol | Backpressure Method | Response Time | Upstream Behavior |\n|----------|-------------------|---------------|-------------------|\n| HTTP | 503 status codes with Retry-After | Immediate | Client retry with backoff |\n| TCP | Socket read blocking | Automatic | Sender blocking at socket level |\n| UDP | Packet dropping | Immediate | No automatic response - relies on sender logic |\n| File Tail | Read position pause | Delayed | File system buffering until processing resumes |\n\n#### Buffer Health Monitoring\n\nComprehensive monitoring of buffer states enables proactive capacity management and early detection of downstream bottlenecks. The ingestion engine exposes detailed metrics about buffer utilization, processing rates, and backpressure activation frequency.\n\nBuffer utilization metrics track current entry counts, percentage capacity usage, and rate of change in buffer levels. Rising buffer levels indicate that ingestion rates exceed processing rates, suggesting either traffic increases or downstream performance degradation. Declining buffer levels after periods of high utilization indicate that processing has caught up with ingestion.\n\nProcessing rate metrics measure entries per second flowing through each pipeline stage: parsing, buffer insertion, buffer extraction, and downstream forwarding. Rate comparisons identify pipeline bottlenecks and help size buffer capacities appropriately. Sustained rate imbalances indicate architectural issues requiring investigation.\n\nBackpressure activation metrics count the frequency and duration of capacity-constrained operations: HTTP 503 responses, TCP connection rejections, UDP packet drops, and file tail read pauses. High backpressure frequencies suggest systematic under-provisioning rather than temporary traffic spikes.\n\nQueue depth monitoring tracks both memory buffer utilization and disk queue accumulation. Disk queue growth indicates serious capacity constraints that require immediate attention to prevent operational impact. Queue age metrics measure how long entries remain buffered before processing, providing insight into end-to-end log processing latency.\n\n> **Design Insight: Why Separate Memory and Disk Buffers**\n>\n> The two-tier buffering approach optimizes for the common case (normal traffic patterns) while providing safety for the exceptional case (traffic spikes or downstream outages). Memory buffers provide sub-millisecond latency for typical operations, while disk buffers ensure durability during extended problems. Alternative single-tier approaches either sacrifice performance (always use disk) or reliability (only use memory).\n\n### Architecture Decision Records\n\nSeveral critical design decisions shape the ingestion engine architecture. Each decision involves trade-offs between performance, reliability, operational complexity, and compatibility requirements.\n\n> **Decision: Multi-Protocol Support vs. Single Protocol**\n> - **Context**: Log sources use different protocols based on their runtime environment, operational requirements, and historical conventions\n> - **Options Considered**: \n>   - HTTP-only ingestion with protocol translation proxies\n>   - Native support for HTTP, TCP, UDP, and file monitoring\n>   - Plugin architecture for extensible protocol support\n> - **Decision**: Native support for four core protocols (HTTP, TCP syslog, UDP syslog, file tail)\n> - **Rationale**: Native support provides optimal performance and eliminates proxy deployment complexity. The four chosen protocols cover 95% of log source requirements in typical environments. Plugin architecture adds significant complexity without clear benefits for the core use cases.\n> - **Consequences**: Higher code complexity due to multiple protocol handlers, but better performance and operational simplicity. Limited extensibility for unusual protocols, but this can be addressed in future versions if needed.\n\nThe following table compares the protocol approach options:\n\n| Approach | Performance | Compatibility | Complexity | Extensibility |\n|----------|-------------|---------------|------------|---------------|\n| HTTP-only + Proxies | Medium | Medium | Low | Medium |\n| Native Multi-Protocol | High | High | High | Low |\n| Plugin Architecture | Medium | High | Very High | Very High |\n\n> **Decision: Streaming vs. Batch Processing**\n> - **Context**: Log entries can be processed individually or accumulated into batches for group operations\n> - **Options Considered**:\n>   - Pure streaming: Process each entry immediately\n>   - Micro-batching: Accumulate 10-100 entries before processing\n>   - Time-windowed batching: Process entries every 1-5 seconds\n> - **Decision**: Streaming processing with optional micro-batching configuration\n> - **Rationale**: Streaming provides optimal latency for alerting and real-time queries. Optional batching allows performance tuning for high-volume sources without sacrificing latency for typical workloads.\n> - **Consequences**: Higher CPU overhead due to frequent processing calls, but significantly better query freshness. More complex configuration due to batching options, but better adaptability to diverse workloads.\n\n> **Decision: Push vs. Pull Ingestion Model**\n> - **Context**: Log data can be sent to the aggregation system (push) or retrieved by the system (pull)\n> - **Options Considered**:\n>   - Push-only: Sources send logs to ingestion endpoints\n>   - Pull-only: Ingestion system queries sources for new logs\n>   - Hybrid: Support both push and pull based on source capabilities\n> - **Decision**: Primarily push-based with file tail as the only pull mechanism\n> - **Rationale**: Push provides lower latency and better scalability since sources control timing. Pull requires complex scheduling and state management for many sources. File tail is inherently pull-based due to filesystem semantics.\n> - **Consequences**: Better performance and simpler architecture, but requires sources to implement retry logic and handle ingestion endpoint failures.\n\n### Common Pitfalls\n\nLog ingestion implementation presents several subtle challenges that can cause data loss, performance degradation, or operational difficulties. Understanding these pitfalls helps avoid common mistakes during development and deployment.\n\n⚠️ **Pitfall: Unbounded Memory Growth from Buffer Overflow**\n\nWhen downstream processing slows down but memory buffers continue accepting entries, memory usage can grow without bounds until the system runs out of RAM and crashes. This typically happens when the indexing engine falls behind during traffic spikes or when storage systems experience temporary outages.\n\nThe problem occurs because many naive buffer implementations use dynamic arrays or linked lists that grow automatically as new entries arrive. Without explicit size limits and backpressure mechanisms, these buffers consume all available memory before triggering any protective measures.\n\n**Fix**: Implement fixed-size ring buffers with explicit capacity limits and backpressure activation at configurable utilization thresholds. Monitor buffer utilization continuously and activate disk spillover or upstream backpressure before memory exhaustion occurs. Set process memory limits using container resources or systemd settings to prevent system-wide impact from memory leaks.\n\n⚠️ **Pitfall: Timestamp Handling Causing Query Ordering Issues**\n\nIncorrect timestamp parsing and normalization can cause log entries to appear out of chronological order in query results, making troubleshooting and correlation extremely difficult. This commonly occurs when mixing log sources with different timestamp formats, timezone assumptions, or clock synchronization issues.\n\nThe problem manifests when some entries use local timestamps while others use UTC, when millisecond precision is lost during parsing, or when the ingestion system substitutes current time for unparseable timestamps without clear indication. Query results show events happening in impossible orders, breaking troubleshooting workflows.\n\n**Fix**: Implement comprehensive timestamp normalization that converts all timestamps to UTC with consistent precision (typically milliseconds). When timestamp parsing fails, preserve the original timestamp string as metadata while using ingestion time for ordering. Log timezone conversion decisions and timestamp parsing failures for debugging. Validate clock synchronization across log sources during system deployment.\n\n⚠️ **Pitfall: Label Cardinality Explosion from Unvalidated Input**\n\nAllowing unlimited unique values in log labels can cause index sizes to grow exponentially, eventually consuming all disk space and making queries extremely slow. This happens when applications accidentally include unique identifiers like request IDs, user IDs, or timestamps as label values instead of keeping them in message content.\n\nThe problem typically emerges gradually as applications add new logging and unique label values accumulate over time. Index sizes grow from megabytes to gigabytes, query performance degrades significantly, and disk usage becomes unpredictable. The issue becomes critical when label combinations reach millions of unique values.\n\n**Fix**: Implement strict label validation that rejects entries with excessive unique label values or suspicious label patterns (UUIDs, timestamps, sequential numbers). Configure per-label cardinality limits and monitor label value distributions continuously. Provide clear documentation about appropriate label usage patterns versus message content. Consider implementing label value normalization for high-cardinality fields like user agents or URLs.\n\n⚠️ **Pitfall: TCP Connection Resource Leaks**\n\nTCP syslog receivers that don't properly manage connection lifecycles can accumulate thousands of idle connections, eventually hitting operating system file descriptor limits and preventing new connections. This commonly occurs when clients disconnect abruptly or when connection timeout handling is incorrect.\n\nThe problem develops over time as connection counts slowly increase. Initially, performance remains acceptable, but eventually new connections start failing with \"too many open files\" errors. The issue often appears during traffic spikes when many clients connect simultaneously.\n\n**Fix**: Implement proper connection lifecycle management with read timeouts, idle connection detection, and explicit resource cleanup. Use connection pooling with configurable limits on concurrent connections per client IP. Monitor active connection counts and set appropriate operating system limits for file descriptors. Implement graceful connection draining during shutdown.\n\n⚠️ **Pitfall: UDP Packet Loss Due to Insufficient Socket Buffers**\n\nUDP syslog receivers with default socket buffer sizes often drop packets during traffic bursts without any indication of data loss. The kernel discards packets when the receive buffer fills up, but applications don't receive notification about dropped packets.\n\nThis issue is particularly problematic because UDP provides no delivery guarantees, so packet loss appears as missing log entries without error messages. During troubleshooting, missing logs can mask important information about system behavior.\n\n**Fix**: Configure UDP socket receive buffers to handle expected burst traffic patterns. Use `SO_RCVBUF` socket options to increase buffer sizes from default (typically 64KB) to several megabytes. Monitor socket buffer utilization using system metrics and increase buffer sizes when drops occur. Consider multiple UDP receivers with load balancing for very high throughput requirements.\n\n⚠️ **Pitfall: Incomplete WAL Recovery Leading to Data Loss**\n\nWrite-ahead log implementations that don't properly handle partial writes or corrupted entries can lose data during recovery after crashes. This often occurs when the system crashes during WAL writes, leaving incomplete records that the recovery process cannot parse correctly.\n\nThe problem appears as missing log entries after system restarts, particularly affecting the most recent entries before crashes. In some cases, WAL corruption can prevent the system from starting at all if recovery logic doesn't handle malformed records gracefully.\n\n**Fix**: Implement robust WAL record formatting with checksums and length prefixes that enable recovery to skip corrupted records safely. Use atomic write operations with proper `fsync()` calls to ensure record durability before acknowledging successful ingestion. Test recovery logic extensively with simulated crashes at various points during write operations.\n\nThe following table summarizes common symptoms and their diagnostic approaches:\n\n| Symptom | Likely Cause | Diagnostic Steps | Resolution |\n|---------|--------------|-----------------|------------|\n| Memory usage grows continuously | Unbounded buffer growth | Monitor buffer sizes and downstream processing rates | Implement backpressure and buffer size limits |\n| Log entries appear out of order | Timestamp parsing issues | Check timezone handling and precision loss | Normalize all timestamps to UTC with consistent precision |\n| Index size grows unexpectedly | Label cardinality explosion | Analyze unique label value counts | Implement label validation and cardinality limits |\n| Connection failures after hours of operation | TCP resource leaks | Monitor open file descriptors and connection counts | Add connection timeouts and resource cleanup |\n| Missing log entries with no error messages | UDP packet drops | Check kernel socket buffer statistics | Increase socket buffer sizes and monitor utilization |\n| Data loss after system restarts | WAL recovery failures | Examine WAL contents and recovery logs | Implement robust record formatting with checksums |\n\n![Ingestion Pipeline State Machine](./diagrams/ingestion-states.svg)\n\n### Implementation Guidance\n\nThis section provides practical guidance for implementing the log ingestion engine, including technology recommendations, file organization, and complete starter code for infrastructure components.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| HTTP Server | net/http with gorilla/mux routing | Fiber or Gin web framework |\n| TCP/UDP Handling | Standard net package | Custom connection pooling |\n| JSON Parsing | encoding/json | jsoniter for high performance |\n| Syslog Parsing | Custom regex parsing | go-syslog library |\n| File Monitoring | fsnotify package | Custom inotify wrapper |\n| Buffer Implementation | Channel-based queues | Lock-free ring buffers |\n| Disk Persistence | Standard file I/O | Memory-mapped files |\n| Configuration | Standard flag package | Viper configuration library |\n\n#### Recommended File Structure\n\n```\ncmd/\n  logaggregator/\n    main.go                          ← Application entry point\ninternal/\n  ingestion/\n    server.go                        ← HTTP ingestion server\n    server_test.go\n    tcp_handler.go                   ← TCP syslog receiver\n    tcp_handler_test.go\n    udp_handler.go                   ← UDP syslog receiver  \n    udp_handler_test.go\n    file_tailer.go                   ← File monitoring agent\n    file_tailer_test.go\n  parsing/\n    parser.go                        ← Common parsing interface\n    parser_test.go\n    json_parser.go                   ← JSON log parsing\n    json_parser_test.go\n    syslog_parser.go                 ← RFC 3164/5424 syslog parsing\n    syslog_parser_test.go\n    regex_parser.go                  ← Custom regex patterns\n    regex_parser_test.go\n  buffer/\n    memory_buffer.go                 ← Ring buffer implementation\n    memory_buffer_test.go\n    disk_queue.go                    ← Disk overflow queues\n    disk_queue_test.go\n    backpressure.go                  ← Backpressure management\n    backpressure_test.go\n  types/\n    log_entry.go                     ← Core data structures\n    config.go                        ← Configuration types\n    metrics.go                       ← Ingestion metrics\nconfigs/\n  config.yaml                        ← Default configuration\n  syslog_patterns.yaml              ← Syslog parsing patterns\n```\n\n#### Infrastructure Starter Code\n\n**Complete HTTP Server Foundation** (`internal/ingestion/server.go`):\n\n```go\npackage ingestion\n\nimport (\n    \"context\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"sync\"\n    \"time\"\n    \n    \"github.com/gorilla/mux\"\n    \"your-project/internal/buffer\"\n    \"your-project/internal/parsing\"\n    \"your-project/internal/types\"\n)\n\ntype HTTPServer struct {\n    config     *types.Config\n    parser     parsing.Parser\n    buffer     buffer.Buffer\n    server     *http.Server\n    metrics    *types.Metrics\n    shutdown   chan struct{}\n    wg         sync.WaitGroup\n}\n\nfunc NewHTTPServer(config *types.Config, parser parsing.Parser, buffer buffer.Buffer, metrics *types.Metrics) *HTTPServer {\n    return &HTTPServer{\n        config:   config,\n        parser:   parser,\n        buffer:   buffer,\n        metrics:  metrics,\n        shutdown: make(chan struct{}),\n    }\n}\n\nfunc (s *HTTPServer) Start() error {\n    router := mux.NewRouter()\n    router.HandleFunc(\"/api/v1/logs\", s.handleLogIngestion).Methods(\"POST\")\n    router.HandleFunc(\"/health\", s.handleHealthCheck).Methods(\"GET\")\n    router.HandleFunc(\"/metrics\", s.handleMetrics).Methods(\"GET\")\n    \n    s.server = &http.Server{\n        Addr:         fmt.Sprintf(\":%d\", s.config.HTTPPort),\n        Handler:      router,\n        ReadTimeout:  30 * time.Second,\n        WriteTimeout: 30 * time.Second,\n        IdleTimeout:  120 * time.Second,\n    }\n    \n    s.wg.Add(1)\n    go func() {\n        defer s.wg.Done()\n        if err := s.server.ListenAndServe(); err != nil && err != http.ErrServerClosed {\n            fmt.Printf(\"HTTP server error: %v\\n\", err)\n        }\n    }()\n    \n    return nil\n}\n\nfunc (s *HTTPServer) Stop() error {\n    close(s.shutdown)\n    ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)\n    defer cancel()\n    \n    if err := s.server.Shutdown(ctx); err != nil {\n        return err\n    }\n    \n    s.wg.Wait()\n    return nil\n}\n\nfunc (s *HTTPServer) handleLogIngestion(w http.ResponseWriter, r *http.Request) {\n    // TODO: Implement log ingestion endpoint - this is where you'll add the core logic\n}\n\nfunc (s *HTTPServer) handleHealthCheck(w http.ResponseWriter, r *http.Request) {\n    w.WriteHeader(http.StatusOK)\n    json.NewEncoder(w).Encode(map[string]string{\"status\": \"healthy\"})\n}\n\nfunc (s *HTTPServer) handleMetrics(w http.ResponseWriter, r *http.Request) {\n    ingested, queries := s.metrics.GetStats()\n    json.NewEncoder(w).Encode(map[string]int64{\n        \"logs_ingested\": ingested,\n        \"queries_executed\": queries,\n    })\n}\n```\n\n**Complete Ring Buffer Implementation** (`internal/buffer/memory_buffer.go`):\n\n```go\npackage buffer\n\nimport (\n    \"errors\"\n    \"sync\"\n    \"sync/atomic\"\n    \"your-project/internal/types\"\n)\n\ntype MemoryBuffer struct {\n    entries    []*types.LogEntry\n    writePos   uint64\n    readPos    uint64\n    capacity   uint64\n    mask       uint64\n    mu         sync.RWMutex\n    notEmpty   *sync.Cond\n    closed     int32\n}\n\nfunc NewMemoryBuffer(capacity int) *Buffer {\n    // Ensure capacity is power of 2 for efficient masking\n    actualCapacity := nextPowerOfTwo(uint64(capacity))\n    \n    b := &MemoryBuffer{\n        entries:  make([]*types.LogEntry, actualCapacity),\n        capacity: actualCapacity,\n        mask:     actualCapacity - 1,\n    }\n    b.notEmpty = sync.NewCond(&b.mu)\n    return b\n}\n\nfunc (b *MemoryBuffer) Write(entry *types.LogEntry) error {\n    if atomic.LoadInt32(&b.closed) != 0 {\n        return errors.New(\"buffer closed\")\n    }\n    \n    b.mu.Lock()\n    defer b.mu.Unlock()\n    \n    // Check if buffer is full\n    if (b.writePos-b.readPos) >= b.capacity {\n        return errors.New(\"buffer full\")\n    }\n    \n    b.entries[b.writePos&b.mask] = entry\n    b.writePos++\n    b.notEmpty.Signal()\n    return nil\n}\n\nfunc (b *MemoryBuffer) Read() (*types.LogEntry, error) {\n    b.mu.Lock()\n    defer b.mu.Unlock()\n    \n    for b.writePos == b.readPos {\n        if atomic.LoadInt32(&b.closed) != 0 {\n            return nil, errors.New(\"buffer closed\")\n        }\n        b.notEmpty.Wait()\n    }\n    \n    entry := b.entries[b.readPos&b.mask]\n    b.entries[b.readPos&b.mask] = nil // Prevent memory leaks\n    b.readPos++\n    return entry, nil\n}\n\nfunc (b *MemoryBuffer) Close() error {\n    atomic.StoreInt32(&b.closed, 1)\n    b.mu.Lock()\n    b.notEmpty.Broadcast()\n    b.mu.Unlock()\n    return nil\n}\n\nfunc nextPowerOfTwo(n uint64) uint64 {\n    if n == 0 {\n        return 1\n    }\n    n--\n    n |= n >> 1\n    n |= n >> 2\n    n |= n >> 4\n    n |= n >> 8\n    n |= n >> 16\n    n |= n >> 32\n    return n + 1\n}\n```\n\n#### Core Logic Skeleton Code\n\n**HTTP Log Ingestion Handler** (add to `server.go`):\n\n```go\nfunc (s *HTTPServer) handleLogIngestion(w http.ResponseWriter, r *http.Request) {\n    // TODO 1: Validate Content-Type header is application/json\n    // TODO 2: Limit request body size to prevent memory exhaustion (e.g., 1MB max)\n    // TODO 3: Parse JSON body into raw map[string]interface{} or []map[string]interface{}\n    // TODO 4: Handle both single log entry and array of entries\n    // TODO 5: For each entry, call s.parser.Parse() to convert to LogEntry\n    // TODO 6: Validate parsed LogEntry using entry.Validate()\n    // TODO 7: Attempt to write entry to buffer using s.buffer.Write()\n    // TODO 8: If buffer write fails (backpressure), return 503 Service Unavailable\n    // TODO 9: Increment ingestion counter using s.metrics.IncrementLogsIngested()\n    // TODO 10: Return appropriate HTTP status code and response body\n    // Hint: Use defer to ensure metrics are updated even if processing fails partially\n}\n```\n\n**Syslog TCP Handler** (`internal/ingestion/tcp_handler.go`):\n\n```go\nfunc (h *TCPHandler) handleConnection(conn net.Conn) {\n    defer conn.Close()\n    \n    // TODO 1: Set read timeout on connection to detect stalled clients\n    // TODO 2: Create buffered reader for efficient line reading\n    // TODO 3: Loop reading lines until connection closes or errors\n    // TODO 4: For each line, attempt to parse as syslog message\n    // TODO 5: Detect RFC 3164 vs RFC 5424 format based on message structure\n    // TODO 6: Extract priority, timestamp, hostname, and message content\n    // TODO 7: Convert parsed fields into LogEntry structure\n    // TODO 8: Write LogEntry to buffer, handling backpressure appropriately\n    // TODO 9: Log connection statistics on disconnect\n    // Hint: Use bufio.Scanner for line-by-line reading with size limits\n}\n```\n\n**JSON Parser Implementation** (`internal/parsing/json_parser.go`):\n\n```go\nfunc (p *JSONParser) Parse(data []byte) (*types.LogEntry, error) {\n    // TODO 1: Parse JSON data into map[string]interface{}\n    // TODO 2: Extract timestamp field and attempt multiple format parsing\n    // TODO 3: Fall back to current time if timestamp parsing fails\n    // TODO 4: Extract message field (required) or use entire JSON as message\n    // TODO 5: Extract labels from configured field names (level, service, host, etc.)\n    // TODO 6: Validate label keys and values meet requirements\n    // TODO 7: Create LogEntry with parsed timestamp, labels, and message\n    // TODO 8: Return validation errors for malformed entries\n    // Hint: Use time.Parse() with multiple layouts for timestamp flexibility\n}\n```\n\n#### Language-Specific Hints\n\n**Go-Specific Implementation Tips:**\n\n- Use `sync.Pool` for reusing parser objects and reducing allocation overhead during high throughput\n- Implement `context.Context` cancellation in long-running goroutines for graceful shutdown\n- Use `atomic` package operations for metrics counters to avoid mutex overhead\n- Configure `GOMAXPROCS` appropriately for container environments with CPU limits\n- Use build tags to enable debug logging and metrics collection selectively\n\n**Buffer Management:**\n- Use `runtime.ReadMemStats()` to monitor memory usage and trigger disk spillover\n- Implement buffer sizing as a function of available system memory\n- Use `os.File.Sync()` for write-ahead log durability guarantees\n- Consider `mmap` for disk queues if random access patterns emerge\n\n**Network Protocol Handling:**\n- Use `net.ListenConfig` with `Control` function to set socket options like `SO_REUSEPORT`\n- Configure TCP keep-alive settings to detect dead connections promptly  \n- Set UDP receive buffer sizes using `syscall.SetsockoptInt()` for high throughput\n- Use separate goroutine pools for different protocols to prevent head-of-line blocking\n\n#### Milestone Checkpoint\n\nAfter implementing Milestone 1, verify the following behaviors:\n\n**Test Commands:**\n```bash\n# Start the ingestion server\ngo run cmd/logaggregator/main.go\n\n# Test HTTP ingestion\ncurl -X POST http://localhost:8080/api/v1/logs \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"timestamp\": \"2024-01-15T10:30:00Z\", \"level\": \"INFO\", \"service\": \"web\", \"message\": \"Request processed\"}'\n\n# Test batch HTTP ingestion\ncurl -X POST http://localhost:8080/api/v1/logs \\\n  -H \"Content-Type: application/json\" \\\n  -d '[\n    {\"timestamp\": \"2024-01-15T10:30:00Z\", \"level\": \"INFO\", \"service\": \"web\", \"message\": \"Request 1\"},\n    {\"timestamp\": \"2024-01-15T10:30:01Z\", \"level\": \"ERROR\", \"service\": \"web\", \"message\": \"Request 2 failed\"}\n  ]'\n\n# Test syslog ingestion\necho \"<134>Jan 15 10:30:00 webserver nginx: GET /api/status 200\" | nc localhost 1514\n\n# Check health and metrics\ncurl http://localhost:8080/health\ncurl http://localhost:8080/metrics\n```\n\n**Expected Outputs:**\n- HTTP requests return 200 OK for valid entries, 400 Bad Request for malformed JSON\n- TCP syslog connections accept messages and parse facility/severity correctly\n- Health endpoint returns `{\"status\": \"healthy\"}`\n- Metrics endpoint shows increasing `logs_ingested` counter\n- Server logs show parsed log entries with extracted labels and normalized timestamps\n\n**Verification Steps:**\n1. Monitor memory usage during sustained load - should remain bounded\n2. Test backpressure by overwhelming the system - should return 503 responses gracefully\n3. Verify timestamp parsing handles multiple formats correctly\n4. Check label extraction produces expected key-value pairs\n5. Test malformed input handling - server should remain stable\n6. Verify file tail agent detects new log lines promptly\n\n**Troubleshooting Common Issues:**\n- \"Connection refused\" - Check if ports are already in use by other processes\n- JSON parsing errors - Verify Content-Type header and valid JSON formatting  \n- Memory growth - Implement buffer size limits and backpressure mechanisms\n- TCP connection buildup - Add connection timeouts and proper cleanup\n- Missing log entries - Check UDP socket buffer sizes and packet drop statistics\n\n\n## Log Indexing Engine\n\n> **Milestone(s):** This section corresponds to Milestone 2 (Log Index), where we build inverted indexes with bloom filters for efficient label-based queries. This milestone depends on the log ingestion capabilities from Milestone 1 and provides the foundation for the query engine in Milestone 3.\n\n### Mental Model: The Card Catalog System\n\nThink of our log indexing engine as a sophisticated library card catalog system from the pre-digital era. In a traditional library, finding a specific book about \"distributed systems\" required consulting multiple card catalogs: one organized by author, another by subject, and perhaps a third by title. Each card contained enough information to locate the actual book on the shelves.\n\nOur log indexing engine operates on the same principle. When millions of log entries pour into our system, we need multiple \"card catalogs\" (inverted indexes) to quickly locate relevant entries without scanning every single log message. Just as a librarian might organize cards by subject (\"Computer Science\"), author (\"Martin Kleppmann\"), or publication year (\"2017\"), we organize our indexes by log labels (`service=api`), message content (`ERROR`), and time ranges (`2024-01-15 14:30-15:00`).\n\nThe bloom filter acts like a librarian's preliminary screening system. Before consulting the detailed card catalog, the librarian might quickly check: \"Do we even have any books published in 1995?\" If the answer is definitively \"no,\" there's no need to dig through the detailed catalog. The bloom filter gives us this same quick negative confirmation: \"Are there any logs with `level=DEBUG` in this time window?\" If not, we skip expensive index lookups entirely.\n\nTime-based partitioning is like organizing the library into sections by decade. When someone asks for books about \"World War II,\" the librarian doesn't search the entire library—they go straight to the 1940s section. Similarly, when querying logs from \"last Tuesday,\" we only search the index partitions covering that time range.\n\nThe key insight is that indexing isn't about storing logs—it's about creating multiple efficient pathways to find them. Just as a good library has many ways to locate the same book, our indexing engine provides multiple access patterns (by time, by label, by content) to the same log data.\n\n### Inverted Index Design\n\nThe **inverted index** forms the backbone of our log search capabilities, transforming the fundamental question from \"what's in this log entry?\" to \"which log entries contain this term?\" This inversion—hence the name—enables logarithmic-time lookups instead of linear scans across millions of log messages.\n\n![Index Architecture](./diagrams/index-structure.svg)\n\nOur inverted index maintains a mapping from every unique term (extracted from log labels and message content) to a **PostingsList** containing references to all log entries that contain that term. The design prioritizes both query speed and storage efficiency, since a single high-traffic service can generate millions of unique terms across billions of log entries.\n\n**Core Index Data Structures**\n\nThe primary index structures work together to provide fast term-to-document resolution:\n\n| Structure | Type | Purpose | Storage Location |\n|-----------|------|---------|------------------|\n| `IndexSegment` | Primary container | Groups terms and postings lists for a time window | Memory + Disk |\n| Terms Map | `map[string]*PostingsList` | Maps each unique term to its postings list | Memory with disk backing |\n| `PostingsList` | `[]EntryReference` | Ordered list of log entries containing the term | Compressed on disk |\n| `EntryReference` | Struct | Points to specific log entry location | Inline in postings list |\n| Bloom Filter | `BloomFilter` | Fast negative lookup for terms not in segment | Memory |\n\nEach `IndexSegment` represents a bounded time window (typically 1-24 hours) and contains all terms discovered in log entries during that period. Segments remain immutable after creation, enabling safe concurrent reads and predictable storage patterns.\n\n**Term Extraction and Normalization**\n\nThe indexing pipeline extracts searchable terms from multiple sources within each `LogEntry`. This multi-faceted approach ensures users can find logs regardless of their search strategy:\n\n1. **Label Terms**: Every label key-value pair becomes two terms: the key (`service`) and the key-value combination (`service=api`). This enables both existence queries (\"show me all logs with a `service` label\") and specific value queries (\"show me logs where `service=api`\").\n\n2. **Message Content Terms**: Log message text undergoes tokenization and normalization. We split on whitespace and common delimiters, convert to lowercase, and optionally remove stop words. Structured message content (JSON within the message) receives special handling to extract nested field values as searchable terms.\n\n3. **Timestamp Terms**: Time-based terms enable temporal queries without consulting the time-range index. We generate terms like `hour=14`, `day=15`, `month=01` to support time-focused searches.\n\n4. **Derived Terms**: Computed values like message length ranges (`msglen_100_1000` for messages between 100-1000 characters) or pattern matches (`contains_ip_address`) provide additional search dimensions.\n\n**PostingsList Organization and Compression**\n\nEach `PostingsList` maintains `EntryReference` entries sorted by timestamp, enabling efficient time-range filtering during query execution. The reference structure contains just enough information to locate the full log entry without storing redundant data:\n\n| Field | Type | Purpose | Size |\n|-------|------|---------|------|\n| `ChunkID` | `string` | Identifies storage chunk containing the log entry | 16 bytes |\n| `Offset` | `uint32` | Byte offset within the decompressed chunk | 4 bytes |\n| `Timestamp` | `time.Time` | Entry timestamp for time-range filtering | 8 bytes |\n\nPostings lists use delta compression to reduce storage overhead. Since entries are timestamp-ordered, we store only the difference between consecutive timestamps and offsets. For frequently occurring terms (like `level=INFO`), this compression can reduce storage by 60-80%.\n\nLarge postings lists employ skip list structures to accelerate time-range searches. Instead of scanning through thousands of entries to find a specific time window, skip pointers allow logarithmic-time jumps to the approximate time range, followed by linear scanning within the narrow window.\n\n**Index Persistence and Memory Management**\n\nThe index engine maintains a hybrid memory-disk architecture optimized for read-heavy workloads. Frequently accessed index segments remain fully memory-resident, while older segments live on disk with selective caching of hot terms.\n\nMemory management follows a tiered approach:\n- **Hot Tier**: Recent segments (last 1-2 hours) with complete in-memory terms maps and postings lists\n- **Warm Tier**: Recent segments (last 24 hours) with in-memory terms maps but disk-resident postings lists\n- **Cold Tier**: Historical segments with disk-resident terms maps and postings lists, cached on demand\n\nThis tiering ensures that common queries against recent data achieve sub-millisecond response times, while historical queries remain reasonably fast through intelligent caching.\n\n> **Critical Design Insight**: The inverted index optimizes for query patterns where users search for specific terms across large time ranges. If your primary query pattern involves full log reconstruction for specific services, a different index design might be more appropriate.\n\n**Architecture Decision Records**\n\n> **Decision: HashMap vs B-Tree for Terms Storage**\n> - **Context**: Need to store millions of unique terms per index segment with fast lookup and reasonable memory overhead\n> - **Options Considered**: \n>   - In-memory HashMap (`map[string]*PostingsList`)\n>   - B-Tree index with disk backing\n>   - Trie structure for prefix matching\n> - **Decision**: In-memory HashMap for hot/warm tiers, B-Tree for cold tier\n> - **Rationale**: HashMap provides O(1) lookup for exact term matches, which represents 95% of our query patterns. B-Tree enables range scans and uses less memory for infrequently accessed terms.\n> - **Consequences**: Fast common-case performance but requires more memory for active segments. Prefix queries require full map iteration.\n\n> **Decision: Per-Segment vs Global Index Structure**\n> - **Context**: Need to balance query performance, update efficiency, and storage overhead as log volume grows\n> - **Options Considered**:\n>   - Single global index updated continuously\n>   - Time-based segments with periodic merging\n>   - Service-based sharding with cross-service queries\n> - **Decision**: Time-based segments (1-hour windows) with background compaction\n> - **Rationale**: Segments enable immutable index structures (no concurrent modification), natural time-range filtering, and bounded memory usage per segment.\n> - **Consequences**: Queries spanning multiple segments require index merging, but time-range queries become extremely efficient.\n\n### Bloom Filter Implementation\n\n**Bloom filters** serve as the first line of defense against expensive index lookups, providing probabilistic \"definitely not present\" guarantees that eliminate unnecessary disk I/O and computation. In our log aggregation system, bloom filters typically eliminate 70-90% of negative lookups, dramatically reducing query latency for exploratory searches.\n\nThe bloom filter operates on a simple principle: before consulting the detailed inverted index, we check whether a term might exist in the segment. A negative result (\"definitely not present\") allows us to skip that segment entirely. A positive result (\"might be present\") requires consulting the actual index, where we may discover the term doesn't exist after all (false positive).\n\n**Bloom Filter Data Structure and Parameters**\n\nOur `BloomFilter` implementation uses a configurable bit array with multiple independent hash functions to minimize false positive rates while maintaining memory efficiency:\n\n| Component | Type | Purpose | Configuration |\n|-----------|------|---------|---------------|\n| `BitArray` | `[]uint64` | Stores the bloom filter bits | Size calculated from expected elements |\n| `HashFunctions` | `[]hash.Hash` | Independent hash functions for bit positioning | Count derived from target false positive rate |\n| `Parameters` | `BloomParams` | Runtime configuration and sizing | Set per index segment based on expected terms |\n\nThe `BloomParams` structure encapsulates all sizing decisions for a specific bloom filter instance:\n\n| Parameter | Type | Purpose | Typical Range |\n|-----------|------|---------|---------------|\n| `ExpectedElements` | `uint32` | Number of unique terms expected in the segment | 10,000 - 1,000,000 |\n| `FalsePositiveRate` | `float64` | Target probability of false positives | 0.01 - 0.05 (1-5%) |\n| `BitArraySize` | `uint32` | Calculated size of the bit array in bits | Derived from above parameters |\n| `HashCount` | `uint32` | Number of independent hash functions to use | Usually 3-8 functions |\n\n**Bloom Filter Sizing and Mathematical Foundation**\n\nProper bloom filter sizing requires balancing false positive rates against memory consumption. The mathematical relationships guide our parameter selection:\n\nThe optimal bit array size (in bits) follows: `m = -n * ln(p) / (ln(2)^2)` where `n` is expected elements and `p` is target false positive rate. The optimal number of hash functions is: `k = (m/n) * ln(2)`.\n\nFor a concrete example: expecting 100,000 unique terms with a 2% false positive rate requires approximately 730,000 bits (91 KB) and 6 hash functions. This represents excellent space efficiency—less than 1 byte per indexed term.\n\n**Hash Function Selection and Distribution**\n\nThe bloom filter's effectiveness depends critically on using truly independent hash functions that distribute values uniformly across the bit array. We implement this using a single strong hash function (like FNV-1a or xxHash) with different seed values to generate multiple independent hash values.\n\nOur implementation avoids the common pitfall of using linear combinations of two hash functions, which can introduce subtle correlations that increase false positive rates beyond theoretical expectations. Instead, we maintain separate hash instances:\n\n```go\n// Each hash function uses a different seed for independence\nhashFunctions := []hash.Hash{\n    fnv.New64a(),           // seed = 0 (default)\n    NewSeededFNV64a(1337),  // seed = 1337\n    NewSeededFNV64a(7919),  // seed = 7919 (prime number)\n    // ... additional functions as needed\n}\n```\n\n**Integration with Index Segments**\n\nEach `IndexSegment` maintains its own bloom filter containing all terms present in that segment. This per-segment approach provides several advantages:\n\n1. **Bounded Memory Growth**: Bloom filter size depends only on the number of terms in one segment, not the entire system\n2. **Time-Range Optimization**: Queries can eliminate entire time windows based on bloom filter checks\n3. **Independent Tuning**: Different time periods might have different term densities, allowing per-segment optimization\n\nDuring index construction, we perform a two-pass process:\n1. **First Pass**: Count unique terms to size the bloom filter appropriately\n2. **Second Pass**: Add all terms to both the bloom filter and the inverted index\n\nThis approach prevents bloom filter overflow, which would degrade false positive rates beyond acceptable levels.\n\n**False Positive Handling and Query Integration**\n\nThe query engine integrates bloom filters as an optimization layer, not a correctness mechanism. The typical query flow follows this pattern:\n\n1. **Bloom Filter Check**: For each relevant index segment, check if the search term might be present\n2. **Index Consultation**: For segments with positive bloom filter results, consult the actual inverted index\n3. **Result Verification**: Confirm term presence and retrieve postings list\n4. **False Positive Handling**: If the term isn't found in the index despite a positive bloom filter result, continue to the next segment\n\nThis design ensures that false positives impact only performance (wasted index lookups), never correctness. False negatives are mathematically impossible with bloom filters, guaranteeing we never miss relevant log entries.\n\n**Bloom Filter Persistence and Loading**\n\nBloom filters persist alongside their corresponding index segments, typically as a header section in the segment file. During system startup, bloom filters load into memory before the detailed index structures, enabling fast segment elimination even when the full index remains disk-resident.\n\nThe persistence format optimizes for quick loading:\n- **Parameters Section**: `BloomParams` serialized as fixed-size binary structure\n- **Bit Array Section**: Raw bit array written as contiguous bytes\n- **Checksum**: Integrity verification to detect corruption\n\n> **Critical Performance Insight**: Bloom filter memory usage should remain under 10% of total index memory consumption. If bloom filters consume significantly more memory, either the false positive rate is too aggressive, or the segment size is too large.\n\n**Architecture Decision Records**\n\n> **Decision: Per-Segment vs Global Bloom Filters**\n> - **Context**: Need to decide whether to maintain one bloom filter per index segment or a single global bloom filter for all terms\n> - **Options Considered**:\n>   - Single global bloom filter updated continuously\n>   - Per-segment bloom filters created once during segment creation\n>   - Hierarchical bloom filters (global + per-segment)\n> - **Decision**: Per-segment bloom filters with no global filter\n> - **Rationale**: Per-segment filters enable time-range optimization (eliminate entire time windows) and bound memory growth. Global filters would grow unboundedly and provide no time-range benefits.\n> - **Consequences**: Multi-segment queries require checking multiple bloom filters, but each check is fast and provides time-locality benefits.\n\n> **Decision: Target False Positive Rate Selection**\n> - **Context**: Need to balance bloom filter memory consumption against false positive query overhead\n> - **Options Considered**:\n>   - Aggressive 1% false positive rate (higher memory usage)\n>   - Conservative 5% false positive rate (lower memory usage)\n>   - Adaptive rate based on segment term density\n> - **Decision**: Fixed 2% false positive rate for all segments\n> - **Rationale**: 2% provides good memory efficiency (8 bits per element) while keeping false positive overhead low. Most terms have low query frequency, so false positives rarely impact user-visible queries.\n> - **Consequences**: Predictable memory usage and good query performance. May be suboptimal for segments with very high or very low query rates.\n\n### Time-Based Partitioning\n\n**Time-based partitioning** serves as the primary strategy for making log queries tractable across large time spans and data volumes. Rather than maintaining monolithic indexes covering all historical data, we segment our indexes by time windows, enabling queries to access only the data relevant to their time range.\n\nThis approach transforms queries spanning multiple time windows from \"search everything\" operations into \"search specific segments and merge results\" operations. For typical log analysis workflows—which focus heavily on recent events or specific incident time ranges—this partitioning provides dramatic performance improvements.\n\n**Partition Window Sizing Strategy**\n\nSelecting appropriate partition window sizes requires balancing query performance, storage efficiency, and operational complexity. Different window sizes optimize for different access patterns:\n\n| Window Size | Query Patterns | Index Count (30 days) | Merge Overhead | Use Case |\n|-------------|----------------|----------------------|----------------|----------|\n| 1 Hour | Precise time ranges, incident investigation | 720 segments | Low | High-frequency services |\n| 6 Hours | Shift-based analysis, trend monitoring | 120 segments | Medium | Medium-frequency services |\n| 24 Hours | Daily reporting, long-term trends | 30 segments | High | Low-frequency services |\n\nOur default 1-hour partitioning balances these concerns for typical production environments. Most log queries focus on the recent 1-6 hours, hitting only 1-6 segments. Historical queries spanning days or weeks require merging many segments, but these represent a minority of the query workload.\n\nThe partitioning strategy adapts to observed query patterns through configurable window sizes per log stream. High-volume services with frequent queries benefit from smaller windows (15-30 minutes), while low-volume services can use larger windows (6-24 hours) to reduce index overhead.\n\n**Partition Boundary Alignment and Clock Synchronization**\n\nPartition boundaries align with wall-clock time boundaries (hour boundaries at :00 minutes) rather than system uptime or log arrival time. This alignment ensures that queries for \"logs between 2:00 PM and 4:00 PM\" map cleanly to specific partitions without requiring complex boundary calculations.\n\nClock synchronization considerations become critical for multi-source log aggregation. Log entries arriving with timestamps spanning partition boundaries require careful handling:\n\n1. **Late-Arriving Logs**: Entries arriving after their partition window has closed go into a \"late arrival\" segment associated with the original time window\n2. **Clock Skew**: Entries with timestamps slightly outside their expected partition due to clock skew are accepted within a configurable tolerance (typically 5-10 minutes)\n3. **Future Timestamps**: Entries with timestamps significantly in the future are either rejected or placed in a special \"future events\" partition for manual review\n\n**Partition Metadata and Catalog Management**\n\nEach partition maintains metadata describing its contents, query performance characteristics, and storage details. This metadata enables the query planner to make informed decisions about which partitions to search and how to optimize the search strategy.\n\n| Metadata Field | Type | Purpose | Usage |\n|-----------------|------|---------|-------|\n| Time Range | `TimeRange` | Start and end timestamps for the partition | Query planning and routing |\n| Entry Count | `int64` | Total number of log entries in partition | Cost estimation |\n| Unique Terms | `int64` | Number of distinct terms in inverted index | Memory planning |\n| Disk Size | `int64` | Total storage consumed by partition | Storage planning |\n| Last Accessed | `time.Time` | Most recent query accessing this partition | Cache eviction decisions |\n\nThe partition catalog serves as the central registry for all time-based partitions, providing fast lookup capabilities for query planning. During query processing, the catalog determines which partitions overlap with the requested time range and estimates the cost of searching each partition.\n\n**Cross-Partition Query Execution**\n\nQueries spanning multiple partitions require coordination to merge results while maintaining correct ordering and avoiding duplicates. The query engine implements a streaming merge algorithm that processes results from multiple partitions simultaneously:\n\n1. **Partition Selection**: Query planner identifies all partitions overlapping the requested time range\n2. **Parallel Execution**: Query executes against selected partitions in parallel, each returning a timestamp-ordered stream\n3. **Streaming Merge**: Results from multiple streams merge using a priority queue ordered by timestamp\n4. **Deduplication**: Adjacent entries with identical content and timestamps are deduplicated during merging\n\nThis approach provides significant parallelization benefits for large time ranges while ensuring that results maintain temporal ordering for user presentation.\n\n**Partition Lifecycle and Transition Handling**\n\nPartition creation occurs continuously as new log data arrives. The transition from \"current\" to \"historical\" partitions involves several state changes that affect query performance and storage characteristics:\n\n**Active Partition State Transitions**:\n1. **Write-Active**: Currently accepting new log entries and updating indexes\n2. **Write-Sealed**: No longer accepting new entries but may still have in-flight index updates\n3. **Read-Only**: Fully sealed with immutable indexes, eligible for optimization\n4. **Archived**: Moved to cold storage with different access patterns\n\nDuring the transition from Write-Active to Write-Sealed, a new partition begins accepting entries while the previous partition completes its final index updates. This overlap period prevents data loss during partition boundaries but requires careful handling of duplicate detection.\n\n**Architecture Decision Records**\n\n> **Decision: Fixed vs Variable Partition Window Sizes**\n> - **Context**: Need to decide whether all partitions use the same time window size or allow variable sizes based on data volume\n> - **Options Considered**:\n>   - Fixed 1-hour windows for all partitions\n>   - Variable windows based on entry count (close partition after 100K entries)\n>   - Hybrid approach with minimum time (1 hour) and maximum entries (500K)\n> - **Decision**: Fixed 1-hour windows with configurable overrides per stream\n> - **Rationale**: Fixed windows provide predictable query behavior and simplify operational reasoning. Variable windows would optimize storage but complicate query planning and time-range calculations.\n> - **Consequences**: Some partitions may be much larger or smaller than others, but query behavior remains predictable and partition boundaries align with human time concepts.\n\n> **Decision: Partition Boundary Handling for Late Arrivals**\n> - **Context**: Need to handle log entries arriving after their partition window has already closed and been sealed\n> - **Options Considered**:\n>   - Reject late-arriving entries entirely\n>   - Reopen closed partitions to accept late entries\n>   - Create separate \"late arrival\" partitions linked to original time windows\n> - **Decision**: Create late arrival partitions with configurable acceptance window (default 1 hour)\n> - **Rationale**: Log systems must handle late arrivals due to network delays, buffering, and clock skew. Reopening sealed partitions would complicate concurrency and caching. Separate late arrival partitions maintain immutability while preserving data.\n> - **Consequences**: Queries must check both primary and late arrival partitions for complete results, slightly increasing query complexity.\n\n### Index Compaction and Maintenance\n\n**Index compaction** prevents storage fragmentation and maintains query performance as the system accumulates thousands of small index segments over time. Without compaction, query execution would degrade as it processes hundreds of tiny segments instead of a few large, optimized segments.\n\nThe compaction process combines multiple small index segments into fewer, larger segments while preserving all indexing relationships and removing obsolete data. This operation runs continuously in the background, targeting segments based on size, age, and access patterns.\n\n**Compaction Triggers and Scheduling**\n\nThe compaction scheduler monitors several metrics to determine when compaction provides meaningful benefits:\n\n| Trigger Condition | Threshold | Rationale | Action |\n|-------------------|-----------|-----------|--------|\n| Segment Count | > 10 segments per hour | Too many segments slow queries | Merge segments within time window |\n| Segment Size | < 10MB per segment | Small segments waste overhead | Combine small adjacent segments |\n| Deleted Entry Ratio | > 20% deleted entries | Space is wasted on obsolete data | Rebuild segment excluding deleted entries |\n| Access Frequency | No access in 7 days | Segment should move to cold storage | Archive and compress segment |\n\nCompaction scheduling balances resource usage against query performance. The scheduler prefers to compact during low-query periods (typically night hours) and limits concurrent compaction operations to avoid overwhelming disk I/O.\n\nThe system maintains separate compaction strategies for different data temperatures:\n- **Hot Data** (last 24 hours): Frequent, lightweight compaction focusing on segment count reduction\n- **Warm Data** (last 7 days): Periodic compaction emphasizing storage efficiency\n- **Cold Data** (older than 7 days): Aggressive compaction with high compression ratios and slower access times\n\n**Compaction Algorithm and Merge Strategy**\n\nThe compaction algorithm processes segments using a multi-way merge that maintains temporal ordering while combining inverted indexes efficiently. The core algorithm follows these steps:\n\n1. **Segment Selection**: Choose 3-8 segments for compaction based on size and adjacency in time\n2. **Bloom Filter Preprocessing**: Estimate merged segment parameters by analyzing source bloom filters\n3. **Multi-Way Index Merge**: Combine inverted indexes while maintaining sorted postings lists\n4. **Duplicate Elimination**: Remove duplicate entries and resolve overwrites within the time window\n5. **Bloom Filter Reconstruction**: Build new bloom filter optimized for the merged term set\n6. **Atomic Replacement**: Replace source segments with merged segment using atomic file operations\n\nDuring multi-way index merging, the algorithm maintains one iterator per source segment for each term being merged. This approach ensures that the merged postings list remains sorted by timestamp while combining entries from multiple sources efficiently.\n\n**Storage Space Reclamation**\n\nCompaction serves as the primary mechanism for reclaiming storage space occupied by deleted or updated log entries. When log entries are deleted (due to retention policies or user requests), the original index segments are not immediately modified. Instead, deletions are recorded in a separate deletion log.\n\nDuring compaction, the algorithm consults the deletion log to exclude obsolete entries from the merged segment:\n\n1. **Deletion Log Consultation**: For each entry reference, check if it appears in the deletion log\n2. **Tombstone Resolution**: Handle cases where entries were updated (delete old version, keep new version)\n3. **Space Calculation**: Track storage space reclaimed to report compaction benefits\n\nThis deferred deletion approach maintains index immutability for concurrent reads while ensuring that storage space is eventually reclaimed through the compaction process.\n\n**Concurrent Access During Compaction**\n\nCompaction operations must not interfere with ongoing queries, requiring careful coordination between compaction workers and query executors. The system achieves this through a versioned segment approach:\n\n**Compaction Isolation Strategy**:\n1. **Snapshot Isolation**: Queries operate against a consistent snapshot of segments that remains valid throughout query execution\n2. **Copy-on-Write**: Compaction creates new merged segments without modifying source segments\n3. **Atomic Switchover**: Once compaction completes, the segment catalog atomically updates to reference the new merged segment\n4. **Delayed Cleanup**: Source segments remain available for in-flight queries before being deleted\n\nThis approach ensures that compaction never causes query failures or inconsistent results, though it temporarily increases storage usage during the compaction process.\n\n**Compaction Performance and Resource Management**\n\nCompaction operations are resource-intensive, requiring significant disk I/O, memory for merge operations, and CPU for bloom filter reconstruction. The compaction scheduler includes several resource management strategies:\n\n**Resource Throttling Mechanisms**:\n- **I/O Rate Limiting**: Limit compaction disk I/O to prevent interference with query performance\n- **Memory Budgeting**: Restrict compaction memory usage to avoid impacting query caching\n- **CPU Prioritization**: Run compaction at lower CPU priority to yield to interactive queries\n- **Concurrent Operation Limits**: Limit number of simultaneous compaction operations\n\nThe system monitors query performance metrics during compaction and can pause compaction operations if query latency exceeds acceptable thresholds.\n\n**Compaction Monitoring and Observability**\n\nEffective compaction requires comprehensive monitoring to ensure the process achieves its goals without impacting system performance:\n\n| Metric | Purpose | Alert Threshold | Action |\n|--------|---------|-----------------|--------|\n| Compaction Lag | Time behind optimal compaction | > 6 hours | Increase compaction parallelism |\n| Space Reclamation Rate | Storage freed per compaction cycle | < 10% expected | Review deletion log efficiency |\n| Compaction Duration | Time to complete merge operations | > 2x baseline | Investigate I/O bottlenecks |\n| Query Impact | Latency increase during compaction | > 20% baseline | Implement additional throttling |\n\nThese metrics enable proactive compaction tuning and help identify when compaction strategies need adjustment based on changing workload characteristics.\n\n**Architecture Decision Records**\n\n> **Decision: Eager vs Lazy Index Compaction**\n> - **Context**: Need to decide when to perform compaction—immediately when segments reach threshold sizes, or defer until query performance degrades\n> - **Options Considered**:\n>   - Eager compaction triggered by segment count/size thresholds\n>   - Lazy compaction triggered by query performance degradation\n>   - Hybrid approach with both proactive and reactive triggers\n> - **Decision**: Eager compaction with segment count/size triggers, plus reactive triggers for performance issues\n> - **Rationale**: Proactive compaction prevents query performance degradation and provides predictable resource usage. Reactive triggers handle unexpected workload changes.\n> - **Consequences**: Higher baseline resource usage for compaction, but more consistent query performance and predictable storage growth.\n\n> **Decision: In-Place vs Copy-Based Compaction**\n> - **Context**: Need to decide whether to modify existing segments during compaction or create new merged segments\n> - **Options Considered**:\n>   - In-place compaction that modifies existing segment files\n>   - Copy-based compaction that creates new merged segments\n>   - Hybrid approach using in-place for small changes, copy for major restructuring\n> - **Decision**: Copy-based compaction with atomic switchover\n> - **Rationale**: Copy-based compaction provides better concurrency (no read locks), easier rollback on failure, and cleaner separation between old and new data.\n> - **Consequences**: Higher temporary storage usage during compaction, but better reliability and concurrency characteristics.\n\n### Architecture Decision Records\n\nThis section consolidates the key architectural decisions that shape our indexing engine's design, providing rationale and trade-off analysis for each significant choice.\n\n> **Decision: Label Indexing Strategy - Separate vs Combined Indexes**\n> - **Context**: Need to decide whether to index label keys and values separately or create combined key-value indexes\n> - **Options Considered**:\n>   - Separate indexes for keys (`service`) and values (`api`), combined at query time\n>   - Combined indexes for key-value pairs (`service=api`) with separate existence indexes\n>   - Hierarchical indexes with key-based partitioning and value-based sub-indexes\n> - **Decision**: Combined key-value pair indexes with separate key existence indexes\n> - **Rationale**: Most queries filter by specific key-value combinations (`service=api`) rather than key existence alone. Combined indexes provide single-lookup query resolution. Separate key existence indexes support the minority of \"show me all services\" queries.\n> - **Consequences**: Slightly higher storage overhead for duplicate key storage, but much faster query execution for the common case.\n\n> **Decision: Index Persistence Format - Binary vs Text**\n> - **Context**: Need to choose serialization format for persistent index storage that balances performance, debuggability, and cross-platform compatibility\n> - **Options Considered**:\n>   - Binary format with custom serialization for maximum performance\n>   - JSON format for human readability and debugging capabilities\n>   - Protocol Buffers for structured binary with schema evolution\n> - **Decision**: Custom binary format with separate JSON export capability\n> - **Rationale**: Index files are read frequently and must load quickly. Binary format provides 5-10x faster loading than JSON. Separate JSON export enables debugging without impacting production performance.\n> - **Consequences**: More complex serialization code and debugging requires export step, but significant performance benefits for index loading.\n\n> **Decision: Memory vs Disk Index Storage Distribution**\n> - **Context**: Need to decide what portion of index data to keep in memory vs disk, given that full in-memory storage becomes prohibitively expensive at scale\n> - **Options Considered**:\n>   - Full in-memory indexes with disk backup only\n>   - Full disk-based indexes with memory caching\n>   - Tiered approach with hot data in memory, warm data cached, cold data on disk\n> - **Decision**: Tiered storage with 24-hour hot tier in memory, 7-day warm tier with selective caching, cold tier disk-resident\n> - **Rationale**: Query patterns show strong temporal locality—90% of queries focus on recent 24 hours. Tiered approach optimizes for common case while supporting historical queries.\n> - **Consequences**: More complex cache management and tier transition logic, but dramatic cost savings for large deployments.\n\n### Common Pitfalls\n\nThis section identifies the most frequent mistakes developers encounter when implementing log indexing systems, providing concrete examples and remediation strategies for each pitfall.\n\n⚠️ **Pitfall: Label Cardinality Explosion**\n\nThe most dangerous indexing pitfall occurs when label values grow without bounds, causing exponential index growth that can exhaust system memory and degrade query performance. This typically happens when developers include high-cardinality values like request IDs, user IDs, or timestamps in log labels.\n\nConsider a service that generates labels like `request_id=abc123`, `user_id=user456`, and `session_id=sess789`. If each combination appears only once, the index must track millions of unique terms with single-entry postings lists. This creates massive storage overhead and provides no query benefit—searching for a specific request ID requires knowing the exact value beforehand.\n\n**Detection**: Monitor label cardinality metrics per key. Alert when any label key exceeds 10,000 unique values within a 24-hour period. Track index memory growth rate—exponential growth often indicates cardinality problems.\n\n**Prevention**: Implement label value validation that rejects high-cardinality labels during ingestion. Use separate high-cardinality fields in the message content rather than labels. Design label schemas around query patterns, not data capture convenience.\n\n**Remediation**: For existing high-cardinality labels, implement label value bucketing (group similar values) or move problematic labels to structured message fields that don't participate in indexing.\n\n⚠️ **Pitfall: Inefficient Bloom Filter Sizing**\n\nDevelopers often missize bloom filters, either wasting memory with overly conservative false positive rates or degrading query performance with excessive false positives. Bloom filter parameters seem like minor details but have substantial impact on system performance.\n\nA common mistake is using the same bloom filter parameters across all index segments regardless of their term density. A segment with 10,000 terms needs different parameters than a segment with 1,000,000 terms. Using identical parameters either wastes memory on small segments or creates high false positive rates on large segments.\n\n**Detection**: Monitor false positive rates per segment—rates significantly higher than configured targets indicate undersized filters. Monitor memory usage per segment—bloom filters consuming more than 10% of segment memory may be oversized.\n\n**Prevention**: Calculate bloom filter parameters individually for each segment based on actual term counts. Implement parameter validation that rejects configurations with unrealistic false positive rates or memory consumption.\n\n**Remediation**: Recompact affected segments with properly sized bloom filters. Implement segment-specific parameter calculation during index creation.\n\n⚠️ **Pitfall: Ignoring Time Zone and Clock Skew Issues**\n\nTime-based partitioning assumes consistent timestamp interpretation across all log sources, but production environments often involve multiple time zones, clock skew, and inconsistent timestamp formats. These issues can cause logs to appear in unexpected partitions or disappear entirely from time-range queries.\n\nA typical scenario involves logs from servers in different data centers with slightly different system clocks. Server A's clock runs 10 minutes fast, Server B's clock runs 5 minutes slow. During a 15-minute incident window, logs from the three servers might appear in different hourly partitions, making incident reconstruction difficult.\n\n**Detection**: Monitor partition boundary violations—logs appearing significantly outside their expected time windows. Track query result completeness by comparing expected vs actual log counts for known time ranges.\n\n**Prevention**: Implement clock skew tolerance in partition assignment (accept logs within ±10 minutes of partition boundaries). Standardize on UTC timestamps throughout the ingestion pipeline. Monitor and alert on significant clock differences across log sources.\n\n**Remediation**: Implement late arrival partitions for out-of-window logs. Create partition overlap periods where logs near boundaries are indexed in multiple partitions.\n\n⚠️ **Pitfall: Unbounded Index Memory Growth During Compaction**\n\nIndex compaction operations can consume unbounded memory when merging large segments, potentially causing out-of-memory crashes in production systems. This occurs when developers implement compaction as a single large merge operation rather than streaming merge with bounded memory usage.\n\nThe problem manifests when compacting multiple large segments simultaneously. If each source segment requires 1GB of memory to load its index, compacting 8 segments would require 8GB of memory just for source data, plus additional memory for the merge output.\n\n**Detection**: Monitor memory usage during compaction operations. Alert when compaction memory usage exceeds configured limits or when compaction operations fail with out-of-memory errors.\n\n**Prevention**: Implement streaming merge algorithms that process one term at a time rather than loading complete indexes. Set memory budgets for compaction operations and pause compaction when budgets are exceeded.\n\n**Remediation**: Redesign compaction to use streaming merges with bounded memory usage. Implement compaction scheduling that limits concurrent operations based on memory availability.\n\n⚠️ **Pitfall: Index Corruption Without Detection or Recovery**\n\nIndex corruption can occur due to hardware failures, software bugs, or incomplete writes, but many implementations lack corruption detection and recovery mechanisms. Corrupted indexes may return incorrect query results or crash the system during queries.\n\nCorruption often happens during system crashes when index writes are partially completed. A power failure during bloom filter writes might leave the bit array in an inconsistent state, causing the bloom filter to report incorrect presence/absence information.\n\n**Detection**: Implement index integrity checking using checksums for each index component (terms map, postings lists, bloom filters). Perform periodic background verification of index consistency.\n\n**Prevention**: Use atomic writes for index updates. Implement write-ahead logging for index operations. Store checksums alongside index data to detect corruption during reading.\n\n**Remediation**: Design index recovery procedures that can rebuild corrupted segments from source log data. Implement fallback strategies that continue operating with reduced functionality when corruption is detected.\n\n⚠️ **Pitfall: Poor Query Performance Due to Inefficient Term Extraction**\n\nInefficient term extraction can create indexes with poor query characteristics, either missing important searchable content or including too much noise that dilutes search effectiveness. This commonly occurs with unstructured log messages that require careful parsing to extract meaningful terms.\n\nA frequent mistake involves tokenizing log messages using simple whitespace splitting without considering log-specific patterns. JSON logs embedded in message text, stack traces, and structured data within unstructured messages require specialized parsing to extract useful search terms.\n\n**Detection**: Monitor query result quality—low relevance scores or frequent empty result sets may indicate poor term extraction. Analyze term frequency distributions to identify noise terms that dominate the index.\n\n**Prevention**: Design term extraction rules specific to each log format. Implement structured parsing for common log patterns (JSON, key-value pairs, stack traces). Use stop word lists to exclude noise terms from indexing.\n\n**Remediation**: Rebuild indexes with improved term extraction rules. Implement query-time term filtering to reduce noise in existing indexes.\n\n### Implementation Guidance\n\nThis subsection provides practical implementation details for building the log indexing engine, with complete starter code for infrastructure components and detailed guidance for core indexing logic.\n\n**Technology Recommendations**\n\n| Component | Simple Option | Advanced Option | Rationale |\n|-----------|---------------|-----------------|-----------|\n| Hash Functions | `hash/fnv` with multiple seeds | `github.com/cespare/xxhash` | FNV provides good distribution for simple cases; xxhash offers better performance for high-throughput |\n| Serialization | `encoding/gob` for index persistence | `google.golang.org/protobuf` | Gob is built-in and sufficient; protobuf enables schema evolution |\n| Compression | `compress/gzip` for postings lists | `github.com/klauspost/compress` variants | Gzip balances simplicity and compression ratio; specialized libraries offer better performance |\n| Memory Management | Built-in garbage collector | `sync.Pool` for frequent allocations | GC handles most cases; object pooling reduces allocation overhead in hot paths |\n\n**Recommended File Structure**\n\n```\ninternal/index/\n  index.go                 ← IndexSegment and core index types\n  index_test.go            ← comprehensive index functionality tests\n  bloom.go                 ← BloomFilter implementation\n  bloom_test.go            ← bloom filter correctness and performance tests\n  compaction.go            ← segment compaction and maintenance\n  compaction_test.go       ← compaction logic and concurrency tests\n  partition.go             ← time-based partitioning logic\n  partition_test.go        ← partition management and boundary tests\n  posting.go               ← PostingsList implementation with compression\n  posting_test.go          ← postings list correctness tests\n  catalog.go               ← partition catalog and metadata management\n  catalog_test.go          ← catalog functionality tests\n  \ninternal/index/storage/\n  segment_writer.go        ← index persistence and atomic writes\n  segment_reader.go        ← index loading and memory mapping\n  metadata.go              ← partition and segment metadata structures\n```\n\n**Infrastructure Starter Code**\n\nHere's complete, ready-to-use infrastructure code for bloom filters and basic index structures:\n\n```go\n// internal/index/bloom.go\npackage index\n\nimport (\n    \"encoding/binary\"\n    \"hash\"\n    \"hash/fnv\"\n    \"math\"\n)\n\n// BloomParams holds configuration for bloom filter sizing and performance\ntype BloomParams struct {\n    ExpectedElements   uint32  // Number of elements expected to be inserted\n    FalsePositiveRate float64  // Target probability of false positives (0.01 = 1%)\n    BitArraySize      uint32  // Calculated size of bit array in bits\n    HashCount         uint32  // Number of hash functions to use\n}\n\n// NewBloomParams calculates optimal bloom filter parameters for given constraints\nfunc NewBloomParams(expectedElements uint32, falsePositiveRate float64) BloomParams {\n    // Calculate optimal bit array size: m = -n * ln(p) / (ln(2)^2)\n    n := float64(expectedElements)\n    p := falsePositiveRate\n    m := -n * math.Log(p) / (math.Log(2) * math.Log(2))\n    \n    // Calculate optimal hash count: k = (m/n) * ln(2)\n    k := (m / n) * math.Log(2)\n    \n    return BloomParams{\n        ExpectedElements:   expectedElements,\n        FalsePositiveRate: falsePositiveRate,\n        BitArraySize:      uint32(math.Ceil(m)),\n        HashCount:         uint32(math.Ceil(k)),\n    }\n}\n\n// BloomFilter implements a probabilistic set membership test\ntype BloomFilter struct {\n    BitArray      []uint64      // Packed bit array for filter storage\n    HashFunctions []hash.Hash   // Independent hash functions\n    Parameters    BloomParams   // Configuration used to create this filter\n}\n\n// NewBloomFilter creates an empty bloom filter with the given parameters\nfunc NewBloomFilter(params BloomParams) *BloomFilter {\n    // Calculate number of uint64s needed for bit array\n    arraySize := (params.BitArraySize + 63) / 64\n    \n    // Create independent hash functions with different seeds\n    hashFunctions := make([]hash.Hash, params.HashCount)\n    for i := uint32(0); i < params.HashCount; i++ {\n        hashFunctions[i] = fnv.New64a()\n        // Seed each hash function differently\n        binary.Write(hashFunctions[i], binary.LittleEndian, i*2654435761)\n    }\n    \n    return &BloomFilter{\n        BitArray:      make([]uint64, arraySize),\n        HashFunctions: hashFunctions,\n        Parameters:    params,\n    }\n}\n\n// Add inserts an element into the bloom filter\nfunc (bf *BloomFilter) Add(element string) {\n    for _, hashFunc := range bf.HashFunctions {\n        hashFunc.Reset()\n        hashFunc.Write([]byte(element))\n        \n        // Get bit position from hash value\n        bitPos := hashFunc.Sum64() % uint64(bf.Parameters.BitArraySize)\n        \n        // Set the corresponding bit\n        arrayIndex := bitPos / 64\n        bitIndex := bitPos % 64\n        bf.BitArray[arrayIndex] |= (1 << bitIndex)\n    }\n}\n\n// MightContain returns true if the element might be in the set (with possible false positives)\n// Returns false if the element is definitely not in the set (no false negatives)\nfunc (bf *BloomFilter) MightContain(element string) bool {\n    for _, hashFunc := range bf.HashFunctions {\n        hashFunc.Reset()\n        hashFunc.Write([]byte(element))\n        \n        // Get bit position from hash value\n        bitPos := hashFunc.Sum64() % uint64(bf.Parameters.BitArraySize)\n        \n        // Check if the corresponding bit is set\n        arrayIndex := bitPos / 64\n        bitIndex := bitPos % 64\n        \n        if (bf.BitArray[arrayIndex] & (1 << bitIndex)) == 0 {\n            return false // Definitely not present\n        }\n    }\n    \n    return true // Might be present (could be false positive)\n}\n\n// EstimatedElementCount returns approximate number of elements added to the filter\nfunc (bf *BloomFilter) EstimatedElementCount() uint32 {\n    // Count number of set bits\n    setBits := uint32(0)\n    for _, word := range bf.BitArray {\n        setBits += uint32(popcount(word))\n    }\n    \n    // Estimate elements using formula: n = -(m/k) * ln(1 - X/m)\n    // where X is number of set bits, m is total bits, k is hash count\n    m := float64(bf.Parameters.BitArraySize)\n    k := float64(bf.Parameters.HashCount)\n    x := float64(setBits)\n    \n    if x >= m {\n        return bf.Parameters.ExpectedElements // Filter is saturated\n    }\n    \n    estimated := -(m / k) * math.Log(1.0 - x/m)\n    return uint32(estimated)\n}\n\n// popcount returns the number of set bits in a uint64\nfunc popcount(x uint64) int {\n    // Use built-in bit counting if available\n    count := 0\n    for x != 0 {\n        count++\n        x &= x - 1 // Clear lowest set bit\n    }\n    return count\n}\n```\n\n**Core Index Structures**\n\n```go\n// internal/index/index.go\npackage index\n\nimport (\n    \"sort\"\n    \"sync\"\n    \"time\"\n)\n\n// EntryReference points to a specific log entry within a storage chunk\ntype EntryReference struct {\n    ChunkID   string    // Identifier for the storage chunk\n    Offset    uint32    // Byte offset within the decompressed chunk\n    Timestamp time.Time // Entry timestamp for time-range filtering\n}\n\n// PostingsList contains all references to log entries containing a specific term\ntype PostingsList []EntryReference\n\n// IndexSegment represents an immutable index for a specific time window\ntype IndexSegment struct {\n    SegmentID  string                     // Unique identifier for this segment\n    TimeRange  TimeRange                  // Time window covered by this segment\n    Terms      map[string]*PostingsList   // Maps terms to postings lists\n    BloomFilter *BloomFilter              // Fast negative lookup for terms\n    CreatedAt  time.Time                  // When this segment was created\n    ChunkIDs   []string                   // Storage chunks referenced by this segment\n    \n    // Internal state for thread safety\n    mu         sync.RWMutex               // Protects concurrent access during reads\n}\n\n// NewIndexSegment creates a new empty index segment for the given time range\nfunc NewIndexSegment(segmentID string, timeRange TimeRange, expectedTerms uint32) *IndexSegment {\n    bloomParams := NewBloomParams(expectedTerms, 0.02) // 2% false positive rate\n    \n    return &IndexSegment{\n        SegmentID:   segmentID,\n        TimeRange:   timeRange,\n        Terms:       make(map[string]*PostingsList),\n        BloomFilter: NewBloomFilter(bloomParams),\n        CreatedAt:   time.Now(),\n        ChunkIDs:    make([]string, 0),\n    }\n}\n\n// AddTerm adds a term->entry mapping to this index segment\nfunc (seg *IndexSegment) AddTerm(term string, ref EntryReference) {\n    seg.mu.Lock()\n    defer seg.mu.Unlock()\n    \n    // Add to bloom filter for fast negative lookups\n    seg.BloomFilter.Add(term)\n    \n    // Get or create postings list for this term\n    postings, exists := seg.Terms[term]\n    if !exists {\n        postings = &PostingsList{}\n        seg.Terms[term] = postings\n    }\n    \n    // Insert entry reference maintaining timestamp ordering\n    *postings = append(*postings, ref)\n    // TODO: Implement efficient sorted insertion for large postings lists\n}\n\n// LookupTerm returns the postings list for a term, or nil if not found\nfunc (seg *IndexSegment) LookupTerm(term string) *PostingsList {\n    seg.mu.RLock()\n    defer seg.mu.RUnlock()\n    \n    // Fast negative lookup using bloom filter\n    if !seg.BloomFilter.MightContain(term) {\n        return nil // Definitely not present\n    }\n    \n    // Bloom filter says might be present, check actual index\n    postings, exists := seg.Terms[term]\n    if !exists {\n        return nil // False positive from bloom filter\n    }\n    \n    return postings\n}\n\n// Finalize prepares the segment for read-only access by sorting postings lists\nfunc (seg *IndexSegment) Finalize() {\n    seg.mu.Lock()\n    defer seg.mu.Unlock()\n    \n    // Sort all postings lists by timestamp for efficient time-range queries\n    for _, postings := range seg.Terms {\n        sort.Slice(*postings, func(i, j int) bool {\n            return (*postings)[i].Timestamp.Before((*postings)[j].Timestamp)\n        })\n    }\n}\n```\n\n**Core Logic Skeletons**\n\n```go\n// BuildIndexSegment constructs an index segment from a collection of log entries\n// This is the core indexing logic that learners should implement\nfunc BuildIndexSegment(segmentID string, timeRange TimeRange, entries []LogEntry) (*IndexSegment, error) {\n    // TODO 1: Estimate number of unique terms by sampling entries\n    // Hint: Sample 10% of entries and count unique terms, then extrapolate\n    \n    // TODO 2: Create new index segment with estimated term count\n    // Hint: Use NewIndexSegment with the estimated term count for bloom filter sizing\n    \n    // TODO 3: Extract all terms from each log entry\n    // Hint: Process labels (key and key=value pairs) and message content\n    // Consider: How to tokenize message content? What about JSON within messages?\n    \n    // TODO 4: Add each term->entry mapping to the segment\n    // Hint: Use AddTerm for each extracted term with appropriate EntryReference\n    \n    // TODO 5: Finalize the segment to prepare for querying\n    // Hint: Call Finalize() to sort postings lists by timestamp\n    \n    // TODO 6: Track which storage chunks are referenced by this segment\n    // Hint: Collect unique ChunkIDs from all EntryReferences\n    \n    return nil, nil // Remove when implementing\n}\n\n// CompactSegments merges multiple index segments into a single optimized segment\n// This implements the core compaction logic for storage efficiency\nfunc CompactSegments(sourceSegments []*IndexSegment, outputSegmentID string) (*IndexSegment, error) {\n    // TODO 1: Calculate time range spanning all source segments\n    // Hint: Find minimum start time and maximum end time across all segments\n    \n    // TODO 2: Estimate term count for the merged segment\n    // Hint: Use bloom filter estimates, but account for term overlap between segments\n    \n    // TODO 3: Create output segment with appropriate sizing\n    // Hint: Use estimated term count for bloom filter parameters\n    \n    // TODO 4: Implement multi-way merge of terms across all source segments\n    // Hint: Iterate through all unique terms, merging postings lists from multiple sources\n    // Consider: How to handle duplicate entries? How to maintain timestamp ordering?\n    \n    // TODO 5: Merge postings lists while maintaining temporal ordering\n    // Hint: Use priority queue or sorted merge algorithm for combining timestamp-ordered lists\n    \n    // TODO 6: Deduplicate entries that appear in multiple source segments\n    // Hint: Check for identical ChunkID+Offset combinations\n    \n    // TODO 7: Update bloom filter with all terms from merged segment\n    // Hint: Add each unique term to ensure bloom filter accuracy\n    \n    return nil, nil // Remove when implementing\n}\n\n// PartitionQuery determines which index segments to search for a given query\n// This implements the query planning logic for time-based partitioning\nfunc PartitionQuery(query Query, availableSegments []*IndexSegment) []*IndexSegment {\n    // TODO 1: Extract time range from the query\n    // Hint: Parse query for time range constraints, use default if none specified\n    \n    // TODO 2: Filter segments that overlap with query time range\n    // Hint: Use TimeRange.Overlaps() method to check for intersection\n    \n    // TODO 3: Use bloom filters to eliminate segments that definitely don't contain query terms\n    // Hint: For each segment, check if bloom filter indicates presence of any query terms\n    \n    // TODO 4: Sort segments by relevance/access cost\n    // Hint: Prefer smaller, more recent segments for better query performance\n    \n    // TODO 5: Apply segment limits to prevent unbounded query execution\n    // Hint: Limit to reasonable number of segments (50-100) even for broad time ranges\n    \n    return nil // Remove when implementing\n}\n```\n\n**Milestone Checkpoint**\n\nAfter implementing the index engine, verify correct behavior with these tests:\n\n**Test Index Creation:**\n```bash\ngo test ./internal/index/... -run TestIndexSegment\n# Should see: All postings lists properly sorted by timestamp\n# Should see: Bloom filter false positive rate within 5% of target\n# Should see: Term extraction covers both labels and message content\n```\n\n**Test Compaction:**\n```bash\ngo test ./internal/index/... -run TestCompaction\n# Should see: Merged segments contain all terms from source segments  \n# Should see: No duplicate entries in merged postings lists\n# Should see: Merged segment size smaller than sum of source segments\n```\n\n**Verify Query Planning:**\n```bash\n# Create test segments covering different time ranges\n# Query with specific time range should only return overlapping segments\n# Query with terms not in bloom filter should eliminate segments quickly\n```\n\n**Signs of Correct Implementation:**\n- Index segments build in reasonable time (< 1 second for 10K entries)\n- Bloom filter eliminates 70%+ of negative lookups\n- Compaction reduces storage size by 20-40% \n- Time-range queries only access relevant segments\n\n**Common Implementation Issues:**\n- **Symptom**: Bloom filter false positive rate much higher than expected\n  **Cause**: Incorrect hash function implementation or bit array sizing\n  **Fix**: Verify hash function independence and bit array calculations\n\n- **Symptom**: Queries return incomplete results\n  **Cause**: Postings lists not properly sorted or time range filtering incorrect  \n  **Fix**: Verify timestamp ordering in postings lists and time range overlap logic\n\n- **Symptom**: Memory usage grows unboundedly during indexing\n  **Cause**: Index structures not being finalized or large strings retained\n  **Fix**: Call Finalize() after building segments and avoid retaining large string slices\n\n\n## Query Engine\n\n> **Milestone(s):** This section corresponds to Milestone 3 (Log Query Engine), where we implement LogQL-style querying with full-text search, label filtering, and result processing. This milestone builds on the ingestion capabilities from Milestone 1 and indexing infrastructure from Milestone 2.\n\n### Mental Model: The Research Assistant\n\nBefore diving into the technical implementation of query processing, let's establish an intuitive understanding through the research assistant analogy. Imagine you're working with a highly skilled research assistant who helps you find information from a vast library of documents.\n\nWhen you approach your research assistant with a request like \"find all documents from 2023 that mention 'database errors' and were written by the engineering team,\" the assistant doesn't randomly start pulling books off shelves. Instead, they follow a systematic process that mirrors how our query engine operates.\n\nFirst, the assistant **parses your request** to understand exactly what you're asking for. They break down your natural language request into structured components: time range (2023), keywords (database errors), and metadata filters (engineering team). This is analogous to our query language parser transforming a LogQL query into an Abstract Syntax Tree (AST).\n\nNext, the assistant **plans the search strategy**. They consider which card catalogs to check first, whether to start with the time-based filing system or the subject index, and how to combine multiple search criteria efficiently. Our query planner performs similar optimization, deciding which indexes to use and in what order to execute filters for maximum efficiency.\n\nDuring **execution**, the assistant doesn't just grab every potentially relevant document. They use the card catalog system (our inverted index) to quickly identify candidate documents, then use quick screening techniques (our bloom filters) to eliminate obvious non-matches before diving into detailed examination. They understand that some search strategies are faster than others—checking the author index first when looking for a specific writer, or using the chronological filing system when time ranges are involved.\n\nFinally, the assistant **presents results** in a useful format. They don't dump a pile of documents on your desk; instead, they organize findings, provide summaries, and can give you results in manageable batches if there are many matches. Similarly, our query engine handles result ranking, pagination, and streaming responses.\n\nThe key insight from this analogy is that effective querying is about **intelligent navigation** rather than brute force searching. Just as a good research assistant leverages library organization systems and applies domain knowledge to find information efficiently, our query engine must understand log data patterns, utilize index structures effectively, and execute searches in an order that minimizes unnecessary work.\n\n### Query Language Parser\n\nThe query language parser serves as the entry point for all query processing, responsible for transforming user-supplied LogQL queries into structured representations that our execution engine can process efficiently. LogQL, inspired by Grafana Loki's query language, provides a balance between expressiveness and simplicity that makes it accessible to operations teams while remaining powerful enough for complex log analysis tasks.\n\nThe parser architecture follows a traditional compiler design pattern with distinct lexical analysis, parsing, and validation phases. The **lexical analyzer** (tokenizer) breaks the input string into meaningful tokens, identifying operators, identifiers, string literals, regular expressions, and keywords. The **parser** consumes these tokens to build an Abstract Syntax Tree (AST) that represents the query's logical structure. Finally, the **validator** performs semantic analysis to ensure the query is well-formed and can be executed given our system's capabilities.\n\nLogQL queries follow a pipe-based syntax that mirrors the mental model of data flowing through transformation stages. A typical query might look like `{service=\"api\"} |= \"error\" | json | level=\"ERROR\" | line_format \"{{.timestamp}} {{.message}}\"`. This query demonstrates the key components: label selectors in curly braces, line filters using operators like `|=`, parsed extractors like `json`, and formatting functions.\n\nThe **label selector** component appears at the beginning of every query and specifies which log streams to consider. Label selectors support exact matching (`service=\"api\"`), regular expression matching (`service=~\"api.*\"`), negative matching (`service!=\"debug\"`), and complex boolean combinations using comma (AND) and pipe (OR) operators. The parser must handle proper precedence and grouping while validating that label names conform to our naming conventions.\n\n**Line filters** operate on the actual log message content and support several operators. The `|=` operator performs substring matching, `!~` applies regular expression matching with negation, `|~` provides positive regex matching, and `!=` excludes lines containing specific substrings. The parser must correctly identify these operators and handle proper escaping of special characters in string literals and regular expressions.\n\n**Log parsers** like `json`, `logfmt`, and `regexp` extract structured data from unstructured log messages. The JSON parser automatically extracts all key-value pairs from JSON-formatted log lines, making them available as labels for subsequent filtering. The logfmt parser handles the structured logging format used by many Go applications. The regexp parser allows custom field extraction using named capture groups. Each parser type requires different validation rules and parameter handling.\n\n**Aggregation functions** enable statistical analysis over log data, including `count_over_time()`, `rate()`, `sum(rate())`, and `avg_over_time()`. These functions operate over time windows and can be grouped by extracted labels. The parser must validate function signatures, ensure proper time window syntax, and verify that grouping expressions reference valid label names.\n\n> **Decision: LogQL Syntax Choice**\n> - **Context**: We needed to choose a query language syntax that balances usability with implementation complexity while providing sufficient expressiveness for log analysis tasks.\n> - **Options Considered**: \n>   1. SQL-like syntax with traditional SELECT/FROM/WHERE clauses\n>   2. Elasticsearch Query DSL with nested JSON structures  \n>   3. LogQL pipe-based syntax similar to shell command chaining\n> - **Decision**: Implement LogQL pipe-based syntax with modifications for our specific use cases\n> - **Rationale**: The pipe-based approach matches operations teams' mental model of data transformation pipelines, reduces cognitive load compared to complex JSON structures, and provides clear execution semantics. SQL syntax, while familiar, doesn't map naturally to log streaming concepts and would require significant semantic extensions.\n> - **Consequences**: This choice simplifies incremental query building and debugging but requires custom parser implementation and limits compatibility with existing SQL tooling.\n\n| Parser Component | Input Format | Output Structure | Validation Rules |\n|------------------|--------------|------------------|------------------|\n| Label Selector | `{service=\"api\", level!=\"debug\"}` | `LabelMatcher[]` | Label names must be valid identifiers, values properly quoted |\n| Line Filter | `\\|= \"error message\"` | `LineFilter{Operator, Pattern}` | Regex patterns must compile, string escaping handled correctly |\n| JSON Parser | `\\| json` | `JSONExtractor{Fields}` | No parameters required, validates JSON structure at execution |\n| LogFmt Parser | `\\| logfmt` | `LogFmtExtractor{}` | No parameters, handles key=value parsing |\n| Regex Parser | `\\| regexp \"(?P<level>\\\\w+)\"` | `RegexExtractor{Pattern, Groups}` | Pattern must compile, named groups required for extraction |\n| Range Query | `[5m]` | `TimeWindow{Duration}` | Duration must be positive, supports s/m/h/d units |\n| Aggregation | `sum(rate({app=\"web\"}[5m]))` | `AggregateExpr{Func, Expr, Grouping}` | Function exists, expression type compatible |\n\nThe Abstract Syntax Tree representation uses a visitor pattern to enable different processing phases (validation, optimization, execution) to traverse the same structure without tight coupling. Each AST node implements a common `QueryNode` interface with methods for type identification, child enumeration, and visitor acceptance.\n\n**Error handling** in the parser focuses on providing actionable feedback to users. Common syntax errors include unmatched braces in label selectors, invalid regular expressions in filters, and malformed time duration specifications. The parser maintains position information for all tokens, enabling precise error location reporting. When encountering syntax errors, the parser attempts error recovery to identify multiple issues in a single parse pass rather than failing on the first error encountered.\n\n**Query validation** occurs after successful parsing and verifies semantic correctness. The validator checks that all referenced label names exist in our label catalog, ensures aggregation functions receive compatible input types, validates that time ranges are reasonable (not negative, not extending beyond available data retention), and confirms that regular expressions compile successfully. This validation phase prevents runtime errors and provides early feedback about query correctness.\n\nThe parser implementation must handle **precedence and associativity** correctly, particularly in complex label selectors with mixed AND/OR operations and in mathematical expressions within aggregation functions. Left-to-right evaluation within pipe stages provides predictable semantics, while proper operator precedence in boolean expressions prevents surprising query behavior.\n\n### Query Planning and Optimization\n\nQuery planning transforms the validated AST into an executable query plan that minimizes resource consumption while ensuring correct results. The planner's primary responsibilities include determining optimal execution order, identifying opportunities for predicate pushdown, selecting appropriate indexes, and estimating resource requirements to prevent runaway queries.\n\nThe **cost-based optimization** approach relies on statistics about our data distribution, index selectivity, and historical query performance. The planner maintains metadata about label cardinality (how many unique values exist for each label), time-based data distribution, and index effectiveness for different query patterns. This statistical foundation enables intelligent decisions about execution strategy rather than relying on hard-coded heuristics.\n\n**Predicate pushdown** represents one of the most critical optimizations in log query processing. The goal is to apply the most selective filters as early as possible in the execution pipeline, reducing the volume of data that subsequent operations must process. Label-based filters typically offer the highest selectivity and can leverage our inverted indexes, so they should execute before line filters that require reading actual log content. Regular expression filters are computationally expensive and should be delayed until after cheaper substring filters have reduced the candidate set.\n\nThe planner analyzes the AST to identify **pushdown opportunities**. Label selectors naturally push down to the index lookup phase, where they can dramatically reduce the set of log streams under consideration. Time range filters integrate with our time-based partitioning to eliminate entire chunks from consideration. Line filters containing simple substring matches can sometimes leverage bloom filters for negative lookups, while complex regular expressions must wait until log content retrieval.\n\n**Index selection** involves choosing which indexes to use and in what order to apply them. Our system maintains several index types: the primary inverted index mapping terms to log entries, label-specific indexes for high-cardinality labels, and time-based partition metadata. The planner must decide whether to start with label-based lookup and intersect with text search results, or begin with text search and filter by labels afterward. This decision depends on the estimated selectivity of each filter component.\n\nFor queries involving multiple label filters, the planner determines the optimal join order by estimating the cardinality of intermediate results. Starting with the most selective label filter minimizes the working set size for subsequent operations. The planner uses histogram data about label value distributions to estimate filter selectivity, preferring filters that match fewer streams over those that match many.\n\n**Time-based optimization** leverages our partitioned storage architecture to minimize data access. Queries with explicit time ranges can skip entire partitions that fall outside the specified window. The planner can also apply time-based optimizations to queries without explicit time constraints by limiting the search to recent partitions first, allowing for early termination when sufficient results are found.\n\n> **Decision: Cost-Based vs Rule-Based Optimization**\n> - **Context**: Query optimization requires deciding between rule-based heuristics (always apply certain optimizations) versus cost-based analysis (estimate costs and choose optimal plan).\n> - **Options Considered**:\n>   1. Simple rule-based system with fixed optimization patterns\n>   2. Cost-based optimizer using statistics and cardinality estimation\n>   3. Hybrid approach with rules for common cases and cost-based for complex queries\n> - **Decision**: Implement cost-based optimization with fallback rules for cases with insufficient statistics\n> - **Rationale**: Log data characteristics vary dramatically between deployments (some have high label cardinality, others have mostly unstructured text). Cost-based optimization adapts to actual data patterns rather than assuming universal characteristics. The statistical foundation also enables automatic performance improvement as the system learns from query patterns.\n> - **Consequences**: Requires maintaining statistics collection and storage, increasing system complexity. However, this investment pays dividends in query performance, especially for complex queries over large datasets.\n\nThe **resource estimation** component predicts memory and CPU requirements for query execution, enabling proactive rejection of queries that would consume excessive resources. The planner estimates the working set size based on expected intermediate result cardinality, accounting for memory requirements of sorting, aggregation, and result buffering operations. Queries exceeding configurable resource limits are rejected with explanatory error messages.\n\n**Query plan representation** uses a directed acyclic graph (DAG) structure that captures dependencies between operations while enabling parallel execution where possible. Each node in the plan represents a processing operation (index lookup, text filtering, aggregation) along with estimated costs and resource requirements. Edges represent data flow between operations, with annotations indicating expected data volume and selectivity.\n\n| Plan Node Type | Function | Input Requirements | Output Characteristics | Cost Factors |\n|----------------|----------|-------------------|----------------------|--------------|\n| IndexLookup | Find log entries matching labels | Label selector expression | Ordered list of entry references | Index size, label cardinality |\n| TimeFilter | Restrict results to time range | Entry references, time bounds | Filtered entry references | Partition scan cost, time selectivity |\n| TextFilter | Apply line filters to log content | Entry references, filter patterns | Matching entries with content | Regex complexity, data volume |\n| JSONExtractor | Parse JSON and extract fields | Log entries with JSON content | Entries with extracted labels | JSON parsing overhead, field count |\n| Aggregator | Compute metrics over time windows | Timestamped entries, grouping spec | Aggregated time series | Memory for intermediate state, group cardinality |\n| Sorter | Order results by timestamp/relevance | Unordered result set | Ordered result stream | Memory for sorting buffer, result count |\n| Limiter | Restrict result count | Result stream, limit specification | Truncated result stream | Negligible, enables early termination |\n\n**Plan caching** stores optimized plans for frequently executed queries, avoiding repeated optimization overhead. The cache uses query structure hashing to identify equivalent queries regardless of minor syntactic differences. Cached plans include freshness metadata and are invalidated when statistics changes suggest that optimization assumptions may no longer hold.\n\nThe **parallel execution analysis** identifies operations that can run concurrently without dependencies. Index lookups for different label conditions can proceed in parallel, with results intersected afterward. Text filtering operations can be parallelized across multiple log chunks when sufficient CPU resources are available. The planner annotates the execution graph with parallelization opportunities while respecting resource constraints.\n\n**Statistics maintenance** requires ongoing collection of data distribution metrics to keep optimization decisions accurate. The planner tracks query execution times, intermediate result sizes, and filter selectivity for different query patterns. This feedback loop enables continuous improvement of cost estimation models and helps identify opportunities for new index structures or caching strategies.\n\n### Search Execution Engine\n\nThe search execution engine transforms optimized query plans into concrete results by coordinating index lookups, content filtering, and result aggregation. The execution engine must balance throughput with resource consumption while maintaining predictable query response times even under varying system load conditions.\n\n**Execution scheduling** follows the dependency graph established during query planning, ensuring that each operation receives properly prepared input data. The scheduler maintains separate execution contexts for different query components, enabling isolation between concurrent queries and providing mechanisms for query cancellation and timeout enforcement. Each execution context tracks resource usage and can trigger early termination when resource limits are approached.\n\nThe **index lookup coordinator** serves as the primary interface to our indexing infrastructure, translating label selectors and text filters into specific index operations. For label-based queries, the coordinator determines which index segments require scanning based on time ranges and label cardinality estimates. The coordinator implements smart batching to group related index operations and reduce I/O overhead when accessing disk-based index structures.\n\nLabel selector execution begins by identifying the most selective label constraint and using it to seed the candidate set. Subsequent label filters are applied as intersection operations, progressively narrowing the result set. The coordinator maintains intermediate results in memory when possible, spilling to temporary storage for large intermediate sets that exceed available memory capacity.\n\n**Bloom filter integration** provides significant performance improvements for negative lookups. When a query contains exclusion filters (label != \"value\" or line !~ \"pattern\"), the execution engine first checks bloom filters associated with relevant index segments. If the bloom filter indicates that a term is definitely not present in a segment, the entire segment can be skipped without accessing its detailed index structures.\n\nThe text search component handles full-text queries and regular expression matching against log message content. For simple substring searches, the engine can often use index-based approaches when the search terms appear in our inverted index. Complex regular expression queries require retrieving log content and applying pattern matching directly, which is computationally expensive but necessary for flexible query capabilities.\n\n**Streaming execution** processes results incrementally rather than materializing complete result sets in memory. This approach enables response streaming for large queries and provides better responsiveness for interactive use cases. The streaming model requires careful coordination between execution stages to maintain proper backpressure and avoid overwhelming downstream consumers with result data.\n\nLog content retrieval represents a critical performance bottleneck that the execution engine must manage carefully. Rather than fetching log entries individually, the engine groups retrieval requests by storage location (chunk) and batch-loads multiple entries simultaneously. This batching approach amortizes I/O costs while maintaining reasonable memory overhead for the retrieved content.\n\n**Regular expression optimization** applies several techniques to improve pattern matching performance. The engine compiles regex patterns once and reuses compiled representations across multiple matching operations. For patterns that contain fixed string prefixes, the engine can use string searching to identify candidates before applying the full regular expression. Complex patterns that would consume excessive CPU time are subject to timeout limits and early termination.\n\nResult aggregation requires managing intermediate state for mathematical computations over time-windowed data. The aggregation engine groups log entries by specified label combinations and maintains running calculations (sums, counts, averages) within specified time windows. Memory usage is controlled through intelligent batching and periodic flushing of completed time windows to the result stream.\n\n> **Decision: Streaming vs Batch Execution Model**\n> - **Context**: Query execution can either process all data before returning results (batch) or stream results incrementally as they become available.\n> - **Options Considered**:\n>   1. Pure batch processing with complete result materialization\n>   2. Pure streaming with immediate result forwarding\n>   3. Hybrid approach with configurable batching thresholds\n> - **Decision**: Implement streaming execution with intelligent batching for efficiency\n> - **Rationale**: Streaming provides better user experience for interactive queries and enables processing datasets larger than available memory. However, pure streaming can create excessive overhead for small result sets, so intelligent batching captures efficiency benefits while maintaining streaming semantics.\n> - **Consequences**: Increases implementation complexity due to backpressure handling and partial result management, but provides superior scalability and responsiveness characteristics.\n\nThe **memory management** system tracks resource usage across all active query operations and implements priority-based eviction when memory pressure increases. Query operations are classified by resource requirements and execution priority, with long-running analytical queries yielding resources to interactive dashboard queries when necessary. The memory manager can trigger query cancellation for runaway operations that exceed configured limits.\n\n**Error recovery** mechanisms handle various failure scenarios that can occur during query execution. Temporary I/O errors during index or storage access trigger automatic retry with exponential backoff. Malformed log entries that cannot be parsed are logged but do not terminate query execution. Resource exhaustion conditions trigger graceful degradation with partial results rather than complete failure.\n\nThe execution engine implements **query cancellation** support to enable users to terminate long-running queries without consuming unnecessary resources. Cancellation requests propagate through the execution graph, causing individual operations to clean up their state and terminate processing. The cancellation mechanism respects transactional boundaries where applicable and ensures that partially completed operations do not leave inconsistent state.\n\n**Parallel execution** leverages available CPU cores for operations that can be safely parallelized. Index lookups across multiple segments can proceed concurrently, with results merged using timestamp-based ordering. Text filtering operations can be distributed across worker threads, with each thread processing a subset of candidate log entries. The parallel execution system includes dynamic load balancing to ensure efficient CPU utilization even when processing non-uniform data distributions.\n\nResult ranking applies relevance scoring when queries do not specify explicit ordering requirements. The ranking algorithm considers factors such as timestamp recency, label matching quality, and text search relevance scores. For queries with explicit sorting requirements, the execution engine implements efficient external sorting algorithms that can handle result sets larger than available memory.\n\n### Result Processing and Pagination\n\nResult processing transforms raw execution output into properly formatted, ordered, and paginated responses suitable for client consumption. This component must handle diverse output requirements while maintaining efficient resource utilization and providing consistent performance characteristics regardless of result set size.\n\nThe **result formatting** system adapts execution engine output to match client requirements and query specifications. LogQL queries can specify output formatting through functions like `line_format` and `label_format`, which require template-based text processing applied to each result entry. The formatter supports variable substitution, conditional formatting, and basic string manipulation functions while maintaining high throughput for large result sets.\n\nJSON output formatting requires careful handling of data types and nested structures, particularly when queries extract structured data from log messages. The formatter must preserve type information for numeric values, handle special characters properly in string values, and maintain consistent field ordering for client parsing reliability. For queries that extract many fields, the formatter implements field filtering to include only requested attributes in the output.\n\n**Result ordering** implementation varies based on query characteristics and client requirements. Time-based ordering (the default) leverages the natural time-ordering of log data and our time-partitioned storage architecture. Relevance-based ordering requires computing score values during execution and maintaining sorted intermediate results. Custom ordering based on extracted fields requires additional sorting passes that can impact query performance for large result sets.\n\nThe pagination system provides efficient mechanisms for clients to retrieve large result sets in manageable chunks without requiring server-side state maintenance between requests. **Cursor-based pagination** uses opaque tokens that encode position information, enabling clients to resume result retrieval from specific points without server-side session storage. The cursor format includes timestamp information, chunk identifiers, and offset values needed to reconstruct query position.\n\nFor time-based queries, pagination cursors encode temporal boundaries that align with our storage partitioning scheme. This alignment enables efficient seek operations when resuming pagination, as the system can jump directly to relevant storage chunks without scanning from the beginning of results. The cursor encoding includes validation information to detect attempts to use stale or manipulated cursor values.\n\n**Streaming response** delivery enables real-time result consumption for clients that need immediate access to query results. The streaming implementation uses chunked transfer encoding for HTTP clients and maintains WebSocket connections for interactive applications. Streaming responses include proper error handling and connection management to gracefully handle client disconnections and network interruptions.\n\nResult count estimation provides clients with approximate result set sizes for pagination and progress indication purposes. The estimation algorithm uses index statistics and sampling techniques to provide reasonably accurate counts without requiring full query execution. For queries with expensive filtering operations, estimation may be less accurate but still provides useful order-of-magnitude information.\n\n**Result caching** stores frequently accessed query results to reduce execution costs for repeated queries. The caching system uses query fingerprinting to identify equivalent queries and implements time-based invalidation to ensure result freshness. Cache entries include metadata about data freshness and can serve partial results for queries that overlap with cached content while fetching additional data for uncached portions.\n\nThe result processor implements **rate limiting** mechanisms to prevent individual queries from overwhelming client connections or consuming excessive bandwidth. Rate limiting operates at multiple levels: per-query limits prevent runaway result generation, per-client limits ensure fair resource sharing, and global limits protect system stability. Rate limit headers inform clients about current limitations and provide guidance for retry behavior.\n\n| Result Format | Use Case | Performance Characteristics | Special Considerations |\n|---------------|----------|----------------------------|----------------------|\n| JSON Lines | Machine processing, streaming | High throughput, minimal overhead | Maintains temporal ordering, one object per line |\n| Structured JSON | API responses, web interfaces | Moderate overhead for structure | Includes metadata, pagination info, result arrays |\n| CSV | Data export, analytical tools | High throughput, compact size | Schema consistency required, escaping for special chars |\n| LogFmt | Operational dashboards | Low parsing overhead | Key-value format, natural for structured logs |\n| Raw Text | Human readability, debugging | Minimal processing required | No structured data, relies on log message formatting |\n\n**Memory management** for result processing requires careful attention to resource consumption patterns, as large result sets can quickly exhaust available memory if not handled properly. The result processor implements streaming patterns that maintain bounded memory usage regardless of result set size, using fixed-size buffers and flow control mechanisms to coordinate with upstream execution components.\n\nThe **error handling** strategy for result processing focuses on partial success scenarios where some results can be delivered despite encountering errors in portions of the query execution. The processor distinguishes between fatal errors that prevent any result delivery and partial errors that affect subset of results. Partial errors are reported through error metadata included with successful results, enabling clients to understand data completeness.\n\n**Compression** for large result sets reduces network bandwidth requirements and improves client performance for batch-oriented use cases. The result processor supports multiple compression algorithms (gzip, deflate, brotli) and automatically selects appropriate compression based on client capabilities and result characteristics. Streaming compression maintains low latency characteristics while providing bandwidth benefits.\n\nResult validation ensures that all delivered results conform to expected schemas and contain required fields based on the original query specification. The validation system checks for missing timestamps, invalid label values, and malformed extracted fields. Validation failures trigger error responses rather than delivering corrupt data to clients.\n\n### Architecture Decision Records\n\nThe architecture decisions for the query engine reflect careful consideration of performance, usability, and implementation complexity trade-offs. Each decision impacts multiple aspects of system behavior and requires understanding of both immediate implementation requirements and long-term scalability considerations.\n\n> **Decision: AST-Based Query Representation**\n> - **Context**: Query processing requires internal representation that supports optimization, validation, and execution phases while maintaining clarity and extensibility.\n> - **Options Considered**:\n>   1. Direct execution from parsed tokens without intermediate representation\n>   2. AST-based representation with visitor pattern for different processing phases\n>   3. Bytecode compilation to virtual machine instructions\n> - **Decision**: Implement AST-based representation with visitor pattern support\n> - **Rationale**: AST provides clear separation between parsing, optimization, and execution phases, enabling independent evolution of each component. Visitor pattern allows adding new processing phases (optimization passes, alternative execution engines) without modifying existing node types. Direct token execution would be simpler but prevents optimization opportunities, while bytecode compilation adds complexity that exceeds current performance requirements.\n> - **Consequences**: Requires more upfront design effort and memory overhead for AST storage, but provides flexibility for future enhancements and clear debugging capabilities through tree visualization.\n\n> **Decision: Push-Down Optimization Strategy**\n> - **Context**: Query performance depends critically on minimizing data access by applying filters as early as possible in the execution pipeline.\n> - **Options Considered**:\n>   1. Fixed execution order based on query syntax ordering\n>   2. Cost-based reordering with predicate pushdown optimization\n>   3. Adaptive execution with runtime optimization adjustments\n> - **Decision**: Implement cost-based predicate pushdown with static optimization\n> - **Rationale**: Log query performance is dominated by I/O costs for accessing log content, making early filtering crucial for acceptable response times. Cost-based optimization leverages our index statistics to make intelligent filtering decisions. Adaptive runtime optimization would provide better theoretical performance but requires significant complexity that may not be justified by practical performance gains.\n> - **Consequences**: Requires maintaining detailed statistics about label cardinality and index selectivity, increasing system complexity. However, provides dramatic performance improvements for complex queries over large datasets.\n\n> **Decision: Streaming Execution Architecture**\n> - **Context**: Query results can range from small interactive lookups to large analytical queries that return millions of entries, requiring scalable result delivery mechanisms.\n> - **Options Considered**:\n>   1. Full result materialization with batch delivery\n>   2. Pure streaming with immediate result forwarding\n>   3. Hybrid approach with intelligent buffering thresholds\n> - **Decision**: Implement streaming execution with configurable buffering\n> - **Rationale**: Streaming enables processing queries larger than available memory while providing better responsiveness for interactive use cases. Configurable buffering captures efficiency benefits for small queries while maintaining streaming benefits for large ones. Full materialization would limit scalability, while pure streaming might create excessive overhead for small result sets.\n> - **Consequences**: Increases implementation complexity due to backpressure management and error handling in streaming contexts, but provides superior scalability characteristics and better user experience.\n\n> **Decision: Cursor-Based Pagination**\n> - **Context**: Large result sets require pagination to provide manageable client experiences while avoiding server-side state maintenance overhead.\n> - **Options Considered**:\n>   1. Offset-based pagination with skip/limit semantics\n>   2. Cursor-based pagination with opaque position tokens\n>   3. Time-window pagination using temporal boundaries\n> - **Decision**: Implement cursor-based pagination with temporal alignment\n> - **Rationale**: Offset-based pagination becomes inefficient for large offsets and doesn't handle concurrent data changes gracefully. Cursor-based approach provides efficient seek operations and natural integration with our time-partitioned storage. Temporal alignment enables jumping directly to relevant storage partitions without scanning intermediate data.\n> - **Consequences**: Requires more complex cursor encoding and validation logic, but provides better performance characteristics and handles dynamic datasets more gracefully than offset-based approaches.\n\n| Decision Area | Chosen Approach | Primary Benefit | Main Trade-off |\n|---------------|-----------------|------------------|----------------|\n| Query Language | LogQL pipe syntax | Natural data flow semantics | Custom parser implementation |\n| Optimization | Cost-based with statistics | Adaptive to actual data patterns | Statistics maintenance complexity |\n| Execution | Streaming with buffering | Memory scalability | Backpressure handling complexity |\n| Result Format | Multiple format support | Client flexibility | Format conversion overhead |\n| Pagination | Cursor-based with temporal alignment | Efficient large result handling | Complex cursor management |\n| Caching | Query result caching | Repeated query performance | Cache invalidation complexity |\n| Parallelization | Opportunistic with dependency analysis | CPU utilization | Coordination overhead |\n\n### Common Pitfalls\n\nQuery engine implementation presents numerous opportunities for subtle bugs and performance issues that can significantly impact system reliability and user experience. Understanding these common pitfalls enables proactive design decisions that avoid problematic patterns before they manifest in production systems.\n\n⚠️ **Pitfall: Unbounded Query Execution**\n\nOne of the most dangerous pitfalls involves queries that scan enormous amounts of data without proper resource limits. A query like `{} |= \".*\"` with no time bounds could attempt to process every log entry in the system, consuming unbounded memory and CPU resources while potentially impacting other system operations.\n\nThis occurs because developers often focus on correctness during initial implementation without considering resource consumption implications. The query parser may correctly validate syntax and the execution engine may properly implement the specified operations, but without explicit bounds checking, the system becomes vulnerable to accidental or malicious resource exhaustion.\n\nThe fix requires implementing multiple layers of protection. Query parsing should reject queries that lack reasonable time bounds or contain patterns likely to match excessive data. The execution engine needs resource monitoring that can terminate queries exceeding memory or CPU limits. Additionally, result pagination should be mandatory for queries that could return large result sets, preventing clients from accidentally requesting millions of log entries in a single response.\n\n⚠️ **Pitfall: Regular Expression Performance Cliffs**\n\nRegular expression processing can exhibit dramatic performance variations that create unpredictable query response times. A seemingly innocent pattern like `.*error.*details.*` can trigger catastrophic backtracking in the regex engine, causing query execution times to increase exponentially with log message length.\n\nThis pitfall emerges because regex performance characteristics are not always intuitive, and patterns that work well on small test datasets may perform poorly on production log data with longer message texts or unexpected content patterns. The problem is compounded when multiple regex operations are applied to the same log entries, multiplying performance impacts.\n\nPrevention requires several approaches: implementing regex timeout limits that terminate pattern matching operations exceeding reasonable time bounds, analyzing regex patterns during query validation to identify potentially problematic constructions, and providing query hints or warnings when expensive regex patterns are detected. The system should also maintain performance metrics for different regex patterns, enabling identification of problematic queries through monitoring.\n\n⚠️ **Pitfall: Memory Leaks in Result Aggregation**\n\nAggregation operations that group results by extracted labels can consume unbounded memory when label cardinality is higher than expected. A query like `sum(rate({}[5m])) by (request_id)` could create millions of aggregation groups if request IDs are highly unique, exhausting available memory.\n\nThis occurs because aggregation operations naturally accumulate state for each unique grouping combination, and developers may not anticipate the cardinality characteristics of production data. Label values that appear to have reasonable cardinality during development and testing may exhibit much higher cardinality in production environments.\n\nThe solution involves implementing cardinality limits that cap the number of unique groups permitted in aggregation operations, monitoring memory usage during aggregation and triggering early termination when limits are approached, and providing query analysis tools that estimate result cardinality based on historical data patterns. The system should also support approximate aggregation techniques for high-cardinality scenarios where exact results are not required.\n\n⚠️ **Pitfall: Inefficient Index Usage Patterns**\n\nQuery execution can exhibit poor performance when the query planner makes suboptimal decisions about index usage, particularly when combining multiple filter conditions. A query with both selective and non-selective filters might process filters in an order that examines unnecessary data volumes.\n\nThis problem manifests when query optimization relies on outdated statistics or when the cost estimation models don't accurately reflect actual data access patterns. The query planner might choose to start with a non-selective filter that matches many log streams, then apply more selective filters afterward, resulting in excessive I/O operations.\n\nAddressing this requires maintaining current statistics about label cardinality and index selectivity, implementing query plan analysis tools that can identify inefficient execution patterns, and providing query hints or manual optimization capabilities for complex queries that don't optimize well automatically. The system should also include query performance monitoring that can identify consistently slow query patterns for further optimization.\n\n⚠️ **Pitfall: Timestamp Handling and Timezone Issues**\n\nTime-based queries can produce incorrect results when timestamp parsing, timezone conversions, and time range comparisons are not handled consistently throughout the query processing pipeline. A query specifying a time range might miss relevant log entries or include inappropriate ones due to timezone interpretation differences.\n\nThis occurs because log timestamps can arrive in various formats and timezones, while query time specifications may use different timezone assumptions. The inconsistency can cause subtle data loss or inclusion errors that are difficult to detect during testing but manifest as missing or unexpected log entries in query results.\n\nPrevention requires establishing consistent timezone handling throughout the system, with all internal timestamp storage using UTC and explicit conversion rules for input parsing and output formatting. Query validation should ensure that time range specifications are unambiguous, and the system should provide clear documentation about timezone handling behavior for users.\n\n⚠️ **Pitfall: JSON Parsing Error Propagation**\n\nWhen processing log entries that contain malformed JSON, the query execution engine might silently drop entries instead of handling parsing errors gracefully. A query using `| json` extraction might miss relevant log entries if they contain invalid JSON formatting, leading to incomplete query results without clear error indication.\n\nThis pitfall emerges because JSON parsing errors are often treated as exceptional conditions that terminate processing, rather than normal data variations that should be handled gracefully. The issue is particularly problematic when only a small percentage of log entries contain malformed JSON, as the error may not be immediately apparent during testing.\n\nThe solution involves implementing graceful error handling that logs parsing failures without terminating query execution, providing query options that control error handling behavior (strict vs. permissive parsing), and including error statistics in query result metadata so users can understand data completeness. The system should also support partial JSON extraction that can recover valid fields from otherwise malformed JSON structures.\n\n⚠️ **Pitfall: Filter Ordering Dependencies**\n\nQuery results can vary unexpectedly when filter operations are reordered during optimization, particularly when filters have side effects or when the query contains operations that depend on specific data characteristics. A query that expects filters to execute in a particular sequence might produce different results after query optimization reorders operations.\n\nThis occurs when query optimization doesn't properly account for filter dependencies or when individual filter operations have implicit assumptions about input data characteristics. The problem can manifest as missing results, incorrect aggregation values, or inconsistent query behavior between different executions of the same query.\n\nPrevention requires careful analysis of filter operation dependencies during query planning, ensuring that optimization preserves semantic equivalence even when reordering operations, and implementing regression testing that validates query result consistency across different optimization paths. The query planner should also include dependency tracking that prevents reordering when operations have implicit ordering requirements.\n\n### Implementation Guidance\n\nThe query engine represents one of the most complex components in the log aggregation system, requiring careful integration of parsing, optimization, and execution concerns. This implementation guidance provides concrete starting points and detailed development approaches for building a production-capable query processing system.\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Parser Generator | Hand-written recursive descent parser | ANTLR or similar parser generator |\n| AST Representation | Interface{} with type switches | Strongly-typed visitor pattern |\n| Regex Engine | Go's `regexp` package | RE2 with custom optimizations |\n| Result Streaming | HTTP chunked encoding | WebSocket with flow control |\n| Query Caching | In-memory LRU cache | Redis with intelligent invalidation |\n| Statistics Storage | JSON files with periodic updates | Embedded database like BoltDB |\n\n**Recommended Project Structure:**\n\n```\ninternal/query/\n  parser/\n    lexer.go              ← tokenization and basic syntax\n    parser.go             ← AST construction and validation\n    ast.go                ← AST node definitions and visitor pattern\n    parser_test.go        ← comprehensive query parsing tests\n  planner/\n    optimizer.go          ← cost-based optimization and plan generation\n    statistics.go         ← statistics collection and maintenance\n    plan.go               ← execution plan representation\n    planner_test.go       ← optimization logic tests\n  executor/\n    engine.go             ← main execution coordination\n    index_ops.go          ← index lookup operations\n    text_ops.go           ← text filtering and regex matching\n    aggregate_ops.go      ← aggregation and mathematical operations\n    stream_ops.go         ← result streaming and pagination\n    executor_test.go      ← execution engine tests\n  api/\n    http_handler.go       ← HTTP query API endpoint\n    response_format.go    ← result formatting and serialization\n    pagination.go         ← cursor-based pagination implementation\n```\n\n**Query Parser Infrastructure (Complete Implementation):**\n\n```go\npackage parser\n\nimport (\n    \"fmt\"\n    \"regexp\"\n    \"strconv\"\n    \"strings\"\n    \"time\"\n)\n\n// TokenType represents different types of tokens in LogQL\ntype TokenType int\n\nconst (\n    TokenEOF TokenType = iota\n    TokenError\n    TokenLeftBrace    // {\n    TokenRightBrace   // }\n    TokenComma        // ,\n    TokenPipe         // |\n    TokenEqual        // =\n    TokenNotEqual     // !=\n    TokenMatch        // =~\n    TokenNotMatch     // !~\n    TokenContains     // |=\n    TokenNotContains  // !=\n    TokenString       // \"quoted string\"\n    TokenIdentifier   // unquoted identifier\n    TokenNumber       // numeric literal\n    TokenDuration     // 5m, 1h, etc.\n    TokenLeftParen    // (\n    TokenRightParen   // )\n    TokenLeftBracket  // [\n    TokenRightBracket // ]\n)\n\n// Token represents a lexical token with position information\ntype Token struct {\n    Type     TokenType\n    Value    string\n    Position int\n    Line     int\n    Column   int\n}\n\n// Lexer tokenizes LogQL query strings\ntype Lexer struct {\n    input    string\n    position int\n    line     int\n    column   int\n    current  rune\n}\n\n// NewLexer creates a lexer for the given input string\nfunc NewLexer(input string) *Lexer {\n    l := &Lexer{\n        input:  input,\n        line:   1,\n        column: 1,\n    }\n    l.advance() // Initialize current character\n    return l\n}\n\n// advance moves to the next character in the input\nfunc (l *Lexer) advance() {\n    if l.position >= len(l.input) {\n        l.current = 0 // EOF\n        return\n    }\n    \n    if l.current == '\\n' {\n        l.line++\n        l.column = 1\n    } else {\n        l.column++\n    }\n    \n    l.current = rune(l.input[l.position])\n    l.position++\n}\n\n// NextToken returns the next token from the input stream\nfunc (l *Lexer) NextToken() Token {\n    for {\n        switch l.current {\n        case 0:\n            return Token{Type: TokenEOF, Position: l.position, Line: l.line, Column: l.column}\n        case ' ', '\\t', '\\n', '\\r':\n            l.advance()\n            continue\n        case '{':\n            token := Token{Type: TokenLeftBrace, Value: \"{\", Position: l.position - 1, Line: l.line, Column: l.column - 1}\n            l.advance()\n            return token\n        case '}':\n            token := Token{Type: TokenRightBrace, Value: \"}\", Position: l.position - 1, Line: l.line, Column: l.column - 1}\n            l.advance()\n            return token\n        case ',':\n            token := Token{Type: TokenComma, Value: \",\", Position: l.position - 1, Line: l.line, Column: l.column - 1}\n            l.advance()\n            return token\n        case '|':\n            return l.scanPipeOperator()\n        case '=':\n            return l.scanEqualOperator()\n        case '!':\n            return l.scanNotOperator()\n        case '\"':\n            return l.scanString()\n        case '(':\n            token := Token{Type: TokenLeftParen, Value: \"(\", Position: l.position - 1, Line: l.line, Column: l.column - 1}\n            l.advance()\n            return token\n        case ')':\n            token := Token{Type: TokenRightParen, Value: \")\", Position: l.position - 1, Line: l.line, Column: l.column - 1}\n            l.advance()\n            return token\n        case '[':\n            token := Token{Type: TokenLeftBracket, Value: \"[\", Position: l.position - 1, Line: l.line, Column: l.column - 1}\n            l.advance()\n            return token\n        case ']':\n            token := Token{Type: TokenRightBracket, Value: \"]\", Position: l.position - 1, Line: l.line, Column: l.column - 1}\n            l.advance()\n            return token\n        default:\n            if isLetter(l.current) || l.current == '_' {\n                return l.scanIdentifier()\n            }\n            if isDigit(l.current) {\n                return l.scanNumber()\n            }\n            return Token{Type: TokenError, Value: fmt.Sprintf(\"unexpected character: %c\", l.current), Position: l.position - 1, Line: l.line, Column: l.column - 1}\n        }\n    }\n}\n\n// scanPipeOperator handles | and |= operators\nfunc (l *Lexer) scanPipeOperator() Token {\n    start := l.position - 1\n    l.advance() // consume '|'\n    \n    if l.current == '=' {\n        l.advance() // consume '='\n        return Token{Type: TokenContains, Value: \"|=\", Position: start, Line: l.line, Column: l.column - 2}\n    }\n    \n    return Token{Type: TokenPipe, Value: \"|\", Position: start, Line: l.line, Column: l.column - 1}\n}\n\n// Implementation continues with remaining scanner methods...\n```\n\n**Core Query Execution Skeleton:**\n\n```go\npackage executor\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"time\"\n    \n    \"your-project/internal/query/parser\"\n    \"your-project/internal/query/planner\"\n    \"your-project/internal/storage\"\n    \"your-project/internal/index\"\n)\n\n// QueryEngine coordinates query execution across all system components\ntype QueryEngine struct {\n    indexManager   *index.Manager\n    storageManager *storage.Manager\n    statistics     *planner.Statistics\n}\n\n// NewQueryEngine creates a query engine with required dependencies\nfunc NewQueryEngine(indexMgr *index.Manager, storageMgr *storage.Manager) *QueryEngine {\n    return &QueryEngine{\n        indexManager:   indexMgr,\n        storageManager: storageMgr,\n        statistics:     planner.NewStatistics(),\n    }\n}\n\n// ExecuteQuery processes a LogQL query and returns streaming results\nfunc (qe *QueryEngine) ExecuteQuery(ctx context.Context, queryString string, params QueryParams) (*ResultStream, error) {\n    // TODO 1: Parse the query string into AST using parser.NewParser(queryString).Parse()\n    // TODO 2: Validate the AST for semantic correctness (label names exist, time ranges valid)\n    // TODO 3: Create query planner with current statistics and generate optimized execution plan\n    // TODO 4: Create execution context with resource limits and timeout from params\n    // TODO 5: Initialize result stream with appropriate formatting and pagination settings\n    // TODO 6: Execute the plan using executeplan() method, handling cancellation via context\n    // TODO 7: Return configured result stream for client consumption\n    // Hint: Each step can fail - wrap errors with context about which phase failed\n    panic(\"implement ExecuteQuery\")\n}\n\n// executePlan coordinates execution of an optimized query plan\nfunc (qe *QueryEngine) executePlan(ctx context.Context, plan *planner.ExecutionPlan) (*ResultIterator, error) {\n    // TODO 1: Create execution pipeline based on plan.Operations in dependency order  \n    // TODO 2: For each IndexLookup operation, call executeIndexLookup with label selectors\n    // TODO 3: For each TextFilter operation, call executeTextFilter with regex patterns\n    // TODO 4: For each Aggregation operation, call executeAggregation with grouping rules\n    // TODO 5: Connect pipeline stages with proper backpressure and error propagation\n    // TODO 6: Start pipeline execution and return iterator for result consumption\n    // TODO 7: Ensure all pipeline stages respect context cancellation\n    // Hint: Pipeline stages run concurrently - use channels for communication\n    panic(\"implement executePlan\")\n}\n\n// executeIndexLookup retrieves log entry references matching label selectors\nfunc (qe *QueryEngine) executeIndexLookup(ctx context.Context, selectors []parser.LabelSelector, timeRange parser.TimeRange) ([]index.EntryReference, error) {\n    // TODO 1: Determine which index segments overlap with the specified time range\n    // TODO 2: For each relevant segment, apply label selectors to get candidate entry references\n    // TODO 3: If multiple selectors exist, compute intersection of posting lists efficiently\n    // TODO 4: Apply bloom filter checks for negative filters (!=, !~) to eliminate segments early\n    // TODO 5: Sort results by timestamp to enable efficient streaming and merging\n    // TODO 6: Return consolidated list of entry references that match all selectors\n    // Hint: Start with most selective selector to minimize intermediate result size\n    panic(\"implement executeIndexLookup\")\n}\n\n// executeTextFilter applies line filters to log content\nfunc (qe *QueryEngine) executeTextFilter(ctx context.Context, refs []index.EntryReference, filters []parser.LineFilter) (<-chan *LogEntry, error) {\n    // TODO 1: Create result channel for streaming filtered entries\n    // TODO 2: Group entry references by storage chunk to enable batch loading\n    // TODO 3: For each chunk, load log entries and apply text filters in sequence\n    // TODO 4: Compile regex patterns once and reuse across multiple entries\n    // TODO 5: Apply filters in order of increasing computational cost (substring before regex)\n    // TODO 6: Stream matching entries to result channel, respecting context cancellation\n    // TODO 7: Close result channel when all entries processed or context cancelled\n    // Hint: Use worker pool to parallelize text processing across multiple chunks\n    panic(\"implement executeTextFilter\")\n}\n\n// QueryParams contains execution parameters and resource limits\ntype QueryParams struct {\n    TimeRange    parser.TimeRange\n    Limit        int\n    Timeout      time.Duration\n    Format       string\n    MaxMemory    int64\n    StreamResults bool\n}\n\n// ResultStream provides streaming access to query results with pagination\ntype ResultStream struct {\n    entries   <-chan *LogEntry\n    errors    <-chan error\n    cursor    string\n    hasMore   bool\n    metadata  QueryMetadata\n}\n\n// Next returns the next result entry or error\nfunc (rs *ResultStream) Next() (*LogEntry, error) {\n    // TODO 1: Check if more results available using hasMore flag\n    // TODO 2: Attempt to read from entries channel with timeout handling\n    // TODO 3: Check errors channel for any execution errors that occurred\n    // TODO 4: Update cursor position for pagination continuation\n    // TODO 5: Return entry or appropriate error (EOF when stream exhausted)\n    // Hint: Use select statement to handle multiple channels simultaneously\n    panic(\"implement ResultStream.Next\")\n}\n\n// QueryMetadata contains information about query execution\ntype QueryMetadata struct {\n    ExecutionTime time.Duration\n    ScannedEntries int64\n    ReturnedEntries int64\n    ScannedBytes   int64\n    CacheHit       bool\n}\n```\n\n**Milestone Checkpoints:**\n\nAfter implementing the query engine, verify correct functionality through these checkpoints:\n\n1. **Basic Query Parsing**: Run `go test ./internal/query/parser/...` - all lexer and parser tests should pass, demonstrating correct tokenization and AST construction for various LogQL query patterns.\n\n2. **Query Optimization**: Create test queries with different selectivity patterns and verify that the planner chooses efficient execution orders. Queries with highly selective labels should use index lookups first, while text-heavy queries should apply cheaper filters before expensive regex operations.\n\n3. **End-to-End Execution**: Test complete query flows with commands like:\n   ```bash\n   curl -G 'http://localhost:8080/api/v1/query' \\\n        --data-urlencode 'query={service=\"api\"} |= \"error\" | json | level=\"ERROR\"' \\\n        --data-urlencode 'start=2024-01-01T00:00:00Z' \\\n        --data-urlencode 'end=2024-01-02T00:00:00Z'\n   ```\n   Verify that results contain only matching entries with proper JSON formatting.\n\n4. **Performance Verification**: Run queries against datasets of different sizes and measure response times. Simple label-based queries should return results in milliseconds, while complex regex queries may take longer but should complete within configured timeout limits.\n\n5. **Error Handling**: Test malformed queries, resource exhaustion scenarios, and system failures to ensure graceful error responses. Verify that partial failures don't crash the query engine and that error messages provide actionable information for debugging.\n\n**Debugging Tips:**\n\n| Symptom | Likely Cause | Diagnosis Steps | Resolution |\n|---------|--------------|----------------|------------|\n| Queries return no results | Label selector mismatch or time range issues | Check index segments for time range, verify label values exist | Review label selector syntax, expand time range |\n| Very slow query performance | Missing index optimization or expensive regex | Enable query plan logging, check filter execution order | Rewrite regex patterns, add more selective label filters |\n| Memory exhaustion errors | Unbounded aggregation or large result sets | Monitor memory usage during execution, check group cardinality | Add result limits, implement streaming aggregation |\n| Inconsistent query results | Race conditions in concurrent execution | Run queries multiple times, check for non-deterministic ordering | Add proper synchronization, ensure deterministic result ordering |\n| Parser errors on valid syntax | Lexer tokenization issues or precedence problems | Test individual query components, verify token sequence | Fix lexer regular expressions, adjust parser precedence rules |\n\n![Query Processing Sequence](./diagrams/query-flow.svg)\n\n\n## Storage Engine\n\n> **Milestone(s):** This section corresponds to Milestone 4 (Log Storage & Compression), where we implement efficient log storage with chunk-based compression, write-ahead logging, and retention policies for long-term data management.\n\n### Mental Model: The Archive Warehouse\n\nThink of the storage engine as a modern warehouse facility that specializes in archiving historical documents. Just as a warehouse organizes items into labeled boxes, compresses them for space efficiency, and maintains detailed records of what goes where, our storage engine organizes log entries into time-based chunks, compresses them for optimal disk usage, and maintains indexes for fast retrieval.\n\nIn this warehouse analogy, **chunks** are like storage boxes that hold related documents from the same time period. Each box has a detailed label (metadata) describing its contents, creation date, and compression method used. The warehouse workers (storage engine) group documents by time period because researchers (queries) typically want to examine all documents from a specific era, making it efficient to store them together.\n\nThe **write-ahead log** functions like the warehouse's receiving dock logbook. Before any document gets filed into a storage box, workers write an entry in the logbook: \"Received shipment #1234 containing 50 documents from Company X, intended for Archive Box 789.\" This ensures that even if there's a power outage or accident before the documents reach their final location, the logbook provides a complete record of what was supposed to happen, allowing workers to replay the process later.\n\n**Compression** works like vacuum-sealed storage bags. The warehouse compresses documents into these bags to save space, choosing different compression methods based on the document type. Financial records might use one compression method optimized for tabular data, while text documents use another method that's better for natural language patterns. Log data has predictable patterns (timestamps, structured fields, repeated keywords) that compression algorithms can exploit effectively.\n\nThe **retention policy engine** acts like the warehouse's document lifecycle manager. It maintains a calendar of when different types of documents should be destroyed according to legal requirements, storage costs, and business value. Every night, it reviews stored boxes and marks expired ones for destruction, ensuring the warehouse doesn't grow indefinitely and storage costs remain manageable.\n\n### Chunk-Based Storage Design\n\nThe chunk-based storage system organizes log entries into time-windowed containers that balance query efficiency, compression effectiveness, and operational simplicity. Each chunk represents a fixed time window (typically 1-4 hours) and contains all log entries that arrived during that period, regardless of their original timestamp. This design choice optimizes for the common query pattern of \"show me logs from the last hour\" while maintaining predictable chunk sizes.\n\nThe fundamental storage unit is the `ChunkHeader` structure, which contains comprehensive metadata about the chunk's contents and organization. This header serves as both a directory for chunk contents and a summary for query planning optimization.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| Magic | [4]byte | File format identifier for corruption detection |\n| Version | uint16 | Chunk format version for backward compatibility |\n| CompressionType | uint8 | Algorithm used for payload compression |\n| StreamCount | uint32 | Number of distinct log streams in chunk |\n| EntryCount | uint64 | Total log entries across all streams |\n| UncompressedSize | uint64 | Original payload size before compression |\n| CompressedSize | uint64 | Actual disk size after compression |\n| TimeRange | TimeRange | Earliest and latest entry timestamps |\n| CreatedAt | time.Time | Chunk creation timestamp for ordering |\n\nEach chunk contains multiple log streams, where a stream groups entries with identical label sets. The `StreamHeader` provides per-stream metadata that enables selective decompression during queries that filter by specific label combinations.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| StreamID | string | Unique identifier derived from sorted labels |\n| Labels | Labels | Complete label set defining the stream |\n| EntryCount | uint32 | Number of entries in this stream |\n| CompressedOffset | uint64 | Byte offset to stream data within chunk |\n| CompressedSize | uint64 | Compressed size of stream data |\n\nThe chunk organization follows a multi-layer structure optimized for query selectivity. The chunk header appears first, followed by an array of stream headers, then the compressed payload data. This layout allows the query engine to read metadata without decompressing the entire chunk, enabling fast query filtering and resource estimation.\n\n> **Design Insight**: Stream-level organization within chunks is crucial for query performance. Without it, querying logs from a specific service would require decompressing the entire chunk, even if that service only contributed 1% of the entries. Stream headers enable selective decompression, reducing CPU usage and memory pressure during queries.\n\nTime-based chunk boundaries align with wall-clock time rather than log timestamps to handle out-of-order delivery gracefully. A chunk created at 14:00 contains all entries that arrived between 13:00 and 14:00, regardless of their original timestamps. This approach simplifies chunk management and prevents late-arriving logs from requiring expensive chunk reorganization.\n\nThe storage engine maintains chunk metadata in a separate index structure that maps time ranges to chunk files. This metadata index enables efficient chunk discovery during query planning without requiring file system scans or chunk header reads.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| ChunkID | string | Unique chunk identifier |\n| FilePath | string | Absolute path to chunk file |\n| TimeRange | TimeRange | Chunk time boundaries |\n| StreamCount | uint32 | Number of distinct streams |\n| EntryCount | uint64 | Total entries across all streams |\n| DiskSize | uint64 | File size on disk |\n| CreatedAt | time.Time | Creation timestamp |\n| LastAccessed | time.Time | Most recent query access |\n\n### Compression Strategy\n\nLog data exhibits several characteristics that compression algorithms can exploit effectively: high redundancy in structured fields (timestamps, log levels, service names), repeated text patterns in messages, and temporal locality where similar log entries cluster together. The storage engine implements multiple compression algorithms optimized for different log data patterns and query access requirements.\n\n**Zstandard (zstd)** provides the optimal balance of compression ratio and decompression speed for most log workloads. It achieves 60-80% compression ratios on typical log data while maintaining decompression speeds of 1-2 GB/s, making it suitable for real-time query scenarios. Zstd's dictionary training capability allows the storage engine to build compression dictionaries from historical log samples, improving compression ratios by 15-25% for structured log formats.\n\n**LZ4** prioritizes decompression speed over compression ratio, achieving 3-5 GB/s decompression with 45-60% compression ratios. This algorithm suits scenarios where query latency is more important than storage costs, particularly for frequently accessed recent log data or real-time alerting pipelines.\n\n**Gzip** provides maximum compression ratios (70-85%) at the cost of slower decompression speeds (200-500 MB/s). The storage engine uses gzip for archival chunks that are accessed infrequently but must be retained for compliance or forensic analysis.\n\nThe compression algorithm selection follows a tiered strategy based on chunk age and access patterns. Recent chunks (less than 24 hours old) use LZ4 for fast query response. Medium-age chunks (1-30 days old) use zstd for balanced performance. Archive chunks (older than 30 days) use gzip for maximum space efficiency.\n\n| Algorithm | Compression Ratio | Decompression Speed | CPU Usage | Best For |\n|-----------|------------------|-------------------|-----------|----------|\n| LZ4 | 45-60% | 3-5 GB/s | Low | Recent, frequently accessed |\n| Zstd | 60-80% | 1-2 GB/s | Medium | General purpose, balanced |\n| Gzip | 70-85% | 200-500 MB/s | High | Archive, infrequent access |\n\nThe compression process operates at the stream level within each chunk, allowing queries that filter by labels to decompress only relevant streams. This selective decompression reduces CPU usage and memory pressure during query execution, particularly important for queries that touch many chunks but filter to a small result set.\n\nDictionary-based compression training runs as a background process, analyzing log patterns from the previous day to build optimized dictionaries for each compression algorithm. These dictionaries capture common patterns in timestamps, structured fields, and message content, significantly improving compression effectiveness for predictable log formats.\n\n> **Critical Trade-off**: Compression algorithm choice involves three competing factors: storage cost (compression ratio), query latency (decompression speed), and CPU overhead (compression/decompression processing). The storage engine's tiered approach balances these factors by matching algorithm characteristics to access patterns, but operators may need to adjust the tier boundaries based on their specific cost and performance requirements.\n\n### Write-Ahead Log Implementation\n\nThe write-ahead log ensures data durability by recording every storage operation before it occurs, guaranteeing that no log entries are lost even if the system crashes during processing. The WAL operates as an append-only transaction log that maintains strict ordering of operations and provides recovery mechanisms for crash scenarios.\n\nThe WAL record structure captures complete information about each storage operation, enabling precise replay during recovery. Each record includes operation metadata, affected data, and checksums for corruption detection.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| RecordType | uint8 | Operation type (WRITE, COMMIT, CHECKPOINT) |\n| Timestamp | time.Time | Operation timestamp for ordering |\n| ChunkID | string | Target chunk identifier |\n| StreamID | string | Target stream within chunk |\n| EntryCount | uint32 | Number of log entries in operation |\n| DataSize | uint64 | Size of entry data in bytes |\n| Checksum | uint32 | CRC32 checksum of record data |\n| Data | []byte | Serialized log entry data |\n\nWAL operations follow a strict protocol that ensures atomicity and recoverability. Before writing any log entries to chunk storage, the storage engine first writes a WRITE record to the WAL containing the complete entry data. Only after the WAL record is safely persisted (fsync) does the engine proceed with chunk operations.\n\nThe WAL write process follows these steps:\n\n1. **Record Construction**: The storage engine serializes the log entries and constructs a WAL record with operation metadata, data payload, and integrity checksum.\n\n2. **Atomic Write**: The record is written to the WAL file using atomic append operations, ensuring partial writes cannot corrupt the log structure.\n\n3. **Durability Guarantee**: The storage engine calls fsync() to force the operating system to flush the write to physical storage, guaranteeing durability even if power fails immediately.\n\n4. **Operation Proceed**: Only after successful WAL persistence does the storage engine proceed with the actual chunk write operation.\n\n5. **Commit Record**: After successful chunk write, the engine writes a COMMIT record to the WAL, marking the operation as completed.\n\nWAL recovery scans the log from the last checkpoint, identifying uncommitted operations that must be replayed. The recovery process reconstructs the exact state of in-progress operations and completes them, ensuring no data loss regardless of when the crash occurred.\n\nRecovery operation types and their handling:\n\n| Record Type | Recovery Action | Failure Handling |\n|-------------|-----------------|------------------|\n| WRITE (no COMMIT) | Replay chunk write operation | Retry with exponential backoff |\n| WRITE + COMMIT | Skip (already completed) | Mark as recovered |\n| Partial Record | Truncate WAL at corruption | Log corruption warning |\n| Invalid Checksum | Skip corrupted record | Alert operator, continue |\n\nThe WAL implements automatic rotation based on size and time thresholds to prevent unbounded growth. When the current WAL file exceeds the configured size limit (default 100MB), the storage engine creates a new WAL file and marks the previous file for cleanup after the next checkpoint.\n\nCheckpoint operations create recovery points by ensuring all pending WAL operations are committed to chunk storage. During checkpoint, the storage engine:\n\n1. Flushes all in-memory buffers to disk storage\n2. Syncs all open chunk files to ensure durability\n3. Writes a CHECKPOINT record to the WAL with timestamp\n4. Rotates to a new WAL file\n5. Safely deletes old WAL files from before the checkpoint\n\n> **Critical Implementation Detail**: The WAL must use direct I/O or explicit fsync() calls to ensure durability guarantees. Operating system write buffering can delay actual disk writes for seconds or minutes, creating a window where committed operations exist only in volatile memory. Without proper synchronization, a crash can lose supposedly durable data, violating the fundamental WAL contract.\n\n### Retention Policy Engine\n\nThe retention policy engine automatically manages log lifecycle by applying configurable rules that balance storage costs, compliance requirements, and operational needs. The engine evaluates retention policies continuously, identifying expired data and orchestrating safe cleanup without disrupting active queries or system operations.\n\nRetention policies operate at multiple granularity levels to provide flexible data management. Stream-level policies apply to log entries with specific label combinations, enabling fine-grained control over different data types. Chunk-level policies provide coarse-grained cleanup for operational efficiency. Global policies set system-wide defaults and limits.\n\nThe `RetentionPolicy` structure defines the rules and thresholds for automatic data cleanup:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| PolicyID | string | Unique identifier for the retention policy |\n| StreamSelector | Labels | Label pattern to match affected log streams |\n| MaxAge | time.Duration | Maximum age before deletion (time-based) |\n| MaxSize | int64 | Maximum storage size before cleanup (size-based) |\n| MaxEntries | int64 | Maximum entry count before cleanup (count-based) |\n| Priority | int32 | Policy priority for conflict resolution |\n| GracePeriod | time.Duration | Delay before actual deletion for recovery |\n\nThe retention evaluation engine runs periodically (typically every hour) to assess all stored chunks against active retention policies. The evaluation process considers multiple factors: chunk age, access patterns, storage pressure, and policy conflicts.\n\nPolicy evaluation follows a multi-phase process designed to prevent accidental data loss:\n\n1. **Discovery Phase**: Scan all chunk metadata to identify chunks that may be subject to retention policies based on age and content.\n\n2. **Policy Matching**: For each candidate chunk, evaluate all applicable retention policies based on stream selectors and label patterns.\n\n3. **Conflict Resolution**: When multiple policies apply to the same data, use priority ordering and most restrictive rules to determine the effective retention period.\n\n4. **Grace Period Check**: Ensure that chunks marked for deletion have passed their grace period, allowing time for recovery if the policy was misconfigured.\n\n5. **Safety Validation**: Verify that no active queries are accessing chunks marked for deletion and that backup/replication requirements are satisfied.\n\n6. **Deletion Execution**: Remove chunk files and update metadata indexes atomically to maintain consistency.\n\nThe retention engine implements multiple cleanup strategies optimized for different operational scenarios:\n\n| Strategy | Description | Triggers | Performance Impact |\n|----------|-------------|----------|-------------------|\n| Eager Cleanup | Immediate deletion when retention exceeded | High storage pressure | Low, continuous |\n| Batch Cleanup | Periodic bulk deletion of expired chunks | Scheduled intervals | Medium, bursty |\n| Lazy Cleanup | Delete during query if chunk expired | Query-time discovery | High, unpredictable |\n\nRetention policy conflicts require careful resolution to prevent unintended data loss. When multiple policies apply to the same stream, the engine uses a precedence system:\n\n1. **Explicit stream policies** (exact label match) override wildcard patterns\n2. **Longer retention periods** override shorter ones to prevent accidental deletion\n3. **Higher priority values** override lower priority policies\n4. **Compliance policies** (marked with compliance flag) override operational policies\n\nThe retention engine maintains detailed audit logs of all cleanup operations, recording which policies triggered deletions, how much data was removed, and verification that the operations completed successfully. This audit trail supports compliance reporting and provides debugging information when retention behavior appears incorrect.\n\n| Audit Field | Type | Description |\n|-------------|------|-------------|\n| Timestamp | time.Time | When cleanup operation occurred |\n| PolicyID | string | Which retention policy triggered cleanup |\n| ChunkID | string | Identifier of deleted chunk |\n| EntryCount | int64 | Number of log entries deleted |\n| DataSize | int64 | Bytes of storage reclaimed |\n| Reason | string | Specific trigger (age, size, manual) |\n\n> **Operational Safety**: Retention policies are irreversible and can cause significant data loss if misconfigured. The grace period mechanism provides a safety net by marking chunks for deletion but delaying actual removal, allowing operators to recover from policy mistakes. However, operators must monitor retention audit logs and implement proper backup strategies for data that must be preserved beyond the retention period.\n\n### Architecture Decision Records\n\n> **Decision: Chunk Time Window Size**\n> - **Context**: Log entries must be organized into storage chunks, but the optimal chunk duration involves trade-offs between query efficiency, compression effectiveness, and operational complexity. Smaller chunks enable fine-grained queries but increase metadata overhead. Larger chunks improve compression ratios but force queries to process more irrelevant data.\n> - **Options Considered**: 15-minute chunks, 1-hour chunks, 4-hour chunks, daily chunks\n> - **Decision**: 1-hour chunk windows with configurable override per stream\n> - **Rationale**: 1-hour windows align with common query patterns (\"show me the last hour of logs\") while maintaining manageable file sizes (typically 10-100MB compressed). This size provides good compression ratios through temporal locality while keeping decompression overhead reasonable. The configurable override allows high-volume streams to use smaller windows when needed.\n> - **Consequences**: Query performance is optimal for time ranges that align with chunk boundaries. Queries spanning many chunks require more file I/O. Compression ratios are 10-15% better than 15-minute chunks due to larger compression windows.\n\n| Option | Query Efficiency | Compression Ratio | File Count | Operational Overhead |\n|--------|------------------|-------------------|------------|---------------------|\n| 15-minute | High (fine-grained) | Good (65%) | High | High metadata |\n| 1-hour | Good (balanced) | Better (72%) | Moderate | Balanced |\n| 4-hour | Moderate (coarse) | Best (78%) | Low | Low metadata |\n| Daily | Poor (very coarse) | Excellent (82%) | Very low | Minimal |\n\n> **Decision: Compression Algorithm Selection Strategy**\n> - **Context**: Different compression algorithms optimize for different metrics (compression ratio vs speed vs CPU usage), and log access patterns change over time. A single algorithm cannot optimize for all scenarios across the data lifecycle.\n> - **Options Considered**: Single algorithm (zstd), manual per-chunk configuration, automatic tiered selection, dynamic algorithm switching\n> - **Decision**: Automatic tiered selection based on chunk age and access patterns\n> - **Rationale**: Recent chunks are queried frequently and need fast decompression (LZ4), medium-age chunks balance compression and speed (zstd), and old chunks prioritize storage efficiency (gzip). This approach automatically optimizes for changing access patterns without operator intervention.\n> - **Consequences**: Storage costs are minimized for old data while query performance remains good for recent data. The system requires more complexity to manage multiple algorithms. Operator control is reduced but operational overhead is lower.\n\n| Option | Performance Optimization | Operator Effort | System Complexity | Storage Efficiency |\n|--------|-------------------------|----------------|-------------------|-------------------|\n| Single Algorithm | Poor (one-size-fits-all) | Low | Low | Moderate |\n| Manual Configuration | Good (if configured well) | High | Low | Variable |\n| Tiered Selection | Excellent (automated) | Low | Medium | High |\n| Dynamic Switching | Excellent (adaptive) | Low | High | Highest |\n\n> **Decision: WAL Recovery Granularity**\n> - **Context**: WAL recovery can operate at different granularity levels: individual log entries, batches of entries, or entire chunks. Finer granularity provides better consistency guarantees but increases recovery complexity and storage overhead.\n> - **Options Considered**: Per-entry WAL records, batch-level records, chunk-level records\n> - **Decision**: Batch-level WAL records with configurable batch size\n> - **Rationale**: Batch-level records provide good consistency (losing at most one batch on crash) while maintaining reasonable WAL overhead. Individual entries would create excessive WAL volume for high-throughput scenarios. Chunk-level records could lose too much data in crash scenarios.\n> - **Consequences**: Recovery time is bounded by batch size rather than total volume. WAL overhead is 1-2% of total data volume. Maximum data loss on crash is one batch (typically 1000-10000 entries).\n\n| Option | Data Loss Risk | WAL Overhead | Recovery Time | Implementation Complexity |\n|--------|----------------|--------------|---------------|---------------------------|\n| Per-Entry | Minimal (1 entry) | High (10-20%) | Long | High |\n| Batch-Level | Low (1 batch) | Low (1-2%) | Medium | Medium |\n| Chunk-Level | High (1 chunk) | Very low (<0.1%) | Fast | Low |\n\n> **Decision: Retention Policy Evaluation Frequency**\n> - **Context**: Retention policies must be evaluated regularly to free storage space, but frequent evaluation consumes CPU and I/O resources. The evaluation frequency affects how quickly storage is reclaimed versus system overhead.\n> - **Options Considered**: Continuous evaluation, hourly evaluation, daily evaluation, on-demand evaluation\n> - **Decision**: Hourly evaluation with emergency triggers for storage pressure\n> - **Rationale**: Hourly evaluation provides good balance between storage reclamation speed and system overhead. Emergency triggers ensure the system can respond to unexpected storage pressure without waiting for the next scheduled evaluation.\n> - **Consequences**: Storage is typically reclaimed within 1 hour of expiration. System overhead is predictable and bounded. Emergency situations are handled appropriately. Some storage over-consumption is possible for up to 1 hour.\n\n| Option | Storage Reclamation | CPU Overhead | Predictability | Emergency Response |\n|--------|-------------------|------------|----------------|-------------------|\n| Continuous | Immediate | High | Poor | Excellent |\n| Hourly | Good (1hr delay) | Low | Good | Good (with triggers) |\n| Daily | Poor (24hr delay) | Very low | Excellent | Poor |\n| On-demand | Variable | Variable | Poor | Manual only |\n\n### Common Pitfalls\n\n⚠️ **Pitfall: Compression Algorithm Choice Without Benchmarking**\nMany developers choose compression algorithms based on reputation or documentation claims rather than measuring performance with their actual log data. Log data characteristics vary significantly between applications—structured JSON logs compress differently than free-form application logs, and the optimal algorithm depends on query patterns and hardware constraints.\n\n**Why it's wrong**: Compression algorithm performance depends heavily on data patterns, CPU architecture, and access patterns. An algorithm that works well for one application may perform poorly for another. Without benchmarking, you may choose an algorithm that wastes CPU cycles or storage space.\n\n**How to fix**: Implement a benchmarking suite that measures compression ratio, compression speed, decompression speed, and memory usage using representative samples of your actual log data. Test at least 3-4 different algorithms (LZ4, zstd, gzip, snappy) with various settings and measure performance under realistic query loads.\n\n⚠️ **Pitfall: WAL Growing Without Bounds**\nThe write-ahead log can grow indefinitely if the checkpoint mechanism fails or is misconfigured. Developers sometimes focus on the write path without implementing proper WAL rotation and cleanup, leading to disk space exhaustion and degraded performance as WAL files become enormous.\n\n**Why it's wrong**: An unbounded WAL will eventually consume all available disk space and slow down recovery operations. Large WAL files also increase crash recovery time, as the entire log must be scanned during startup.\n\n**How to fix**: Implement automatic WAL rotation based on both size thresholds (e.g., 100MB) and time intervals (e.g., 1 hour). Ensure the checkpoint process verifies that all WAL operations are committed to stable storage before deleting old WAL files. Add monitoring alerts for WAL file count and total WAL disk usage.\n\n⚠️ **Pitfall: Retention Policy Race Conditions**\nRetention cleanup can interfere with active queries if not properly coordinated. Developers sometimes implement retention as a simple file deletion process without considering that queries might be accessing the chunks being deleted, leading to query failures and data integrity issues.\n\n**Why it's wrong**: Deleting chunks while queries are reading them causes query failures and can corrupt query results. The race condition is particularly dangerous because it may only manifest under specific timing conditions, making it difficult to detect during testing.\n\n**How to fix**: Implement reference counting or cooperative deletion mechanisms. Before deleting a chunk, verify that no active queries hold references to it. Use a two-phase deletion process: first mark chunks as \"pending deletion\" and prevent new queries from accessing them, then wait for existing queries to complete before actual deletion.\n\n⚠️ **Pitfall: Ignoring Compression Decompression Memory Requirements**\nCompression reduces disk storage but requires significant memory during decompression. Developers often focus on compression ratios without considering that decompressing large chunks can require 3-5x the compressed size in memory, potentially causing out-of-memory conditions during query execution.\n\n**Why it's wrong**: Large chunks that compress well may require hundreds of megabytes or gigabytes of memory to decompress. Under concurrent query load, this can quickly exhaust available memory and cause query failures or system instability.\n\n**How to fix**: Implement memory budgeting in the query engine that limits total decompression memory usage across concurrent queries. Set maximum chunk sizes based on available memory rather than just disk considerations. Consider streaming decompression for large chunks to reduce memory pressure.\n\n⚠️ **Pitfall: Time Zone Handling in Chunk Boundaries**\nChunk time boundaries can become inconsistent when dealing with time zones, daylight saving time changes, or distributed systems with clock skew. Developers sometimes use local time or naive UTC conversion without considering these edge cases, leading to chunks with overlapping time ranges or gaps.\n\n**Why it's wrong**: Inconsistent time boundaries break query assumptions about which chunks to examine for a given time range. This can cause queries to miss data (false negatives) or scan extra chunks (performance degradation).\n\n**How to fix**: Always use UTC for internal time boundaries and chunk organization. Store the original timezone information in log entry metadata if needed for display purposes. Implement clock skew detection and correction mechanisms for distributed ingestion scenarios.\n\n⚠️ **Pitfall: Failed Cleanup Leading to Storage Leaks**\nRetention cleanup operations can fail due to permission issues, disk errors, or concurrent access. Without proper error handling and retry mechanisms, failed cleanup operations may be silently ignored, leading to storage leaks where expired data accumulates indefinitely.\n\n**Why it's wrong**: Storage leaks violate retention policies and can cause unexpected storage cost increases. In regulated environments, failing to delete data according to retention policies may create compliance violations.\n\n**How to fix**: Implement robust error handling in cleanup operations with exponential backoff retry logic. Maintain a cleanup failure audit log and generate alerts when cleanup operations fail repeatedly. Implement manual cleanup tools that operators can use to address persistent cleanup failures.\n\n### Implementation Guidance\n\nThe storage engine bridges high-performance data persistence with query efficiency requirements. Implementation focuses on managing multiple compression algorithms, ensuring WAL durability guarantees, and coordinating retention cleanup safely.\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Compression | Standard library gzip/zlib | CGO bindings to zstd/lz4 |\n| File I/O | os.File with explicit fsync | Memory-mapped files with madvise |\n| WAL Structure | Append-only binary format | Structured records with checksums |\n| Metadata Index | JSON files | Embedded key-value store (badger) |\n| Background Tasks | time.Ticker goroutines | Priority-based task scheduler |\n\n**Recommended File Structure:**\n```\ninternal/storage/\n  engine.go                 ← main storage engine coordinator\n  engine_test.go           ← integration tests\n  chunk/\n    chunk.go               ← chunk read/write operations\n    chunk_test.go         ← chunk format tests\n    header.go             ← chunk header definitions\n    compression.go        ← compression algorithm interface\n  wal/\n    wal.go                ← write-ahead log implementation\n    wal_test.go          ← WAL recovery tests\n    recovery.go          ← crash recovery logic\n  retention/\n    policy.go            ← retention policy evaluation\n    cleanup.go           ← safe deletion coordination\n    audit.go             ← cleanup operation logging\n  metadata/\n    index.go             ← chunk metadata management\n    stats.go             ← storage statistics tracking\n```\n\n**Infrastructure Starter Code:**\n\n```go\n// Package storage provides efficient log storage with compression and retention\npackage storage\n\nimport (\n    \"encoding/binary\"\n    \"encoding/json\"\n    \"fmt\"\n    \"hash/crc32\"\n    \"io\"\n    \"os\"\n    \"path/filepath\"\n    \"sync\"\n    \"time\"\n)\n\n// StorageEngine coordinates chunk storage, WAL, and retention\ntype StorageEngine struct {\n    config        *StorageConfig\n    walWriter     *WALWriter\n    chunkStore    *ChunkStore\n    retentionMgr  *RetentionManager\n    metadataIndex *MetadataIndex\n    metrics       *StorageMetrics\n    mu           sync.RWMutex\n}\n\n// StorageConfig defines storage engine parameters\ntype StorageConfig struct {\n    StoragePath      string\n    ChunkDuration    time.Duration\n    WALSizeLimit     int64\n    CompressionType  string\n    RetentionPeriod  time.Duration\n    MaxMemoryUsage   int64\n}\n\n// StorageMetrics tracks storage engine performance\ntype StorageMetrics struct {\n    ChunksWritten     int64\n    BytesStored       int64\n    CompressionRatio  float64\n    WALSize          int64\n    RetentionDeletes int64\n    mu               sync.RWMutex\n}\n\n// ChunkStore handles chunk file operations\ntype ChunkStore struct {\n    basePath    string\n    compression CompressionInterface\n    mu          sync.RWMutex\n}\n\n// CompressionInterface abstracts compression algorithms\ntype CompressionInterface interface {\n    Compress(data []byte) ([]byte, error)\n    Decompress(data []byte) ([]byte, error)\n    Name() string\n    Level() int\n}\n\n// WALWriter provides write-ahead logging functionality\ntype WALWriter struct {\n    file     *os.File\n    offset   int64\n    mu       sync.Mutex\n    config   *WALConfig\n}\n\n// WALConfig defines WAL behavior parameters\ntype WALConfig struct {\n    FilePath     string\n    SizeLimit    int64\n    SyncInterval time.Duration\n    BufferSize   int\n}\n\n// WALRecord represents a single WAL entry\ntype WALRecord struct {\n    RecordType uint8\n    Timestamp  time.Time\n    ChunkID    string\n    StreamID   string\n    EntryCount uint32\n    DataSize   uint64\n    Checksum   uint32\n    Data       []byte\n}\n\n// WAL record types\nconst (\n    WALRecordWrite     uint8 = 1\n    WALRecordCommit    uint8 = 2\n    WALRecordCheckpoint uint8 = 3\n)\n\n// NewStorageEngine creates a configured storage engine\nfunc NewStorageEngine(config *StorageConfig) (*StorageEngine, error) {\n    // TODO: Initialize WAL writer with crash recovery\n    // TODO: Initialize chunk store with compression setup\n    // TODO: Initialize retention manager with policy loading\n    // TODO: Initialize metadata index with persistent state\n    // TODO: Start background maintenance goroutines\n    return nil, fmt.Errorf(\"not implemented\")\n}\n\n// WriteLogBatch stores a batch of log entries with WAL protection\nfunc (se *StorageEngine) WriteLogBatch(entries []LogEntry) error {\n    // TODO 1: Generate unique batch ID and chunk assignment\n    // TODO 2: Write batch to WAL with entry serialization\n    // TODO 3: Fsync WAL to guarantee durability\n    // TODO 4: Group entries by stream for chunk organization\n    // TODO 5: Compress and write chunk data to storage\n    // TODO 6: Update metadata index with chunk information\n    // TODO 7: Write WAL commit record after successful storage\n    return fmt.Errorf(\"not implemented\")\n}\n\n// StartRecovery replays uncommitted WAL operations after crash\nfunc (se *StorageEngine) StartRecovery() error {\n    // TODO 1: Scan WAL files from last checkpoint forward\n    // TODO 2: Parse WAL records and identify uncommitted operations\n    // TODO 3: Validate record checksums and handle corruption\n    // TODO 4: Replay write operations that lack commit records\n    // TODO 5: Update metadata index to reflect recovered state\n    // TODO 6: Create new checkpoint after successful recovery\n    return fmt.Errorf(\"not implemented\")\n}\n```\n\n**Core Logic Skeleton:**\n\n```go\n// CompressChunkData applies the configured compression algorithm to chunk payload\nfunc (cs *ChunkStore) CompressChunkData(streams []StreamData, algorithm string) (*ChunkHeader, []byte, error) {\n    // TODO 1: Serialize stream headers and entry data into binary format\n    // TODO 2: Select compression algorithm based on configuration and data characteristics\n    // TODO 3: Apply compression to serialized payload with error handling\n    // TODO 4: Calculate compression ratio and update metrics\n    // TODO 5: Build chunk header with metadata (sizes, timestamps, stream count)\n    // TODO 6: Validate header consistency and compressed data integrity\n    // Hint: Use encoding/binary for cross-platform serialization\n    // Hint: Store uncompressed size for decompression buffer allocation\n    return nil, nil, fmt.Errorf(\"not implemented\")\n}\n\n// WriteWALRecord atomically appends a record to the write-ahead log\nfunc (w *WALWriter) WriteWALRecord(record *WALRecord) error {\n    w.mu.Lock()\n    defer w.mu.Unlock()\n    \n    // TODO 1: Calculate CRC32 checksum of record data for corruption detection\n    // TODO 2: Serialize record header and payload into binary format\n    // TODO 3: Write serialized record to WAL file with atomic append\n    // TODO 4: Call fsync() to ensure data reaches physical storage\n    // TODO 5: Update WAL offset tracking and size metrics\n    // TODO 6: Check if WAL rotation is needed based on size limits\n    // Hint: Use binary.Write for consistent cross-platform encoding\n    // Hint: fsync() is critical for durability guarantees\n    return fmt.Errorf(\"not implemented\")\n}\n\n// EvaluateRetentionPolicies identifies chunks eligible for cleanup\nfunc (rm *RetentionManager) EvaluateRetentionPolicies() ([]string, error) {\n    // TODO 1: Load all active retention policies from configuration\n    // TODO 2: Scan chunk metadata to identify candidates based on age\n    // TODO 3: Apply policy matching logic using label selectors\n    // TODO 4: Resolve conflicts between multiple applicable policies\n    // TODO 5: Check grace periods and safety constraints for each candidate\n    // TODO 6: Return list of chunk IDs ready for safe deletion\n    // Hint: Use time.Since() for age calculations\n    // Hint: Sort policies by priority for conflict resolution\n    return nil, fmt.Errorf(\"not implemented\")\n}\n\n// SafeChunkDeletion removes chunks while coordinating with active queries\nfunc (rm *RetentionManager) SafeChunkDeletion(chunkIDs []string) error {\n    // TODO 1: Acquire retention mutex to prevent concurrent deletions\n    // TODO 2: For each chunk, verify no active queries hold references\n    // TODO 3: Mark chunks as \"pending deletion\" in metadata index\n    // TODO 4: Wait for existing query references to be released\n    // TODO 5: Perform actual file deletion and update audit logs\n    // TODO 6: Remove chunk entries from metadata index atomically\n    // Hint: Use reference counting or cooperative cancellation\n    // Hint: Log detailed audit information for compliance tracking\n    return fmt.Errorf(\"not implemented\")\n}\n```\n\n**Language-Specific Hints:**\n\n- Use `os.File.Sync()` for fsync operations to ensure WAL durability—this forces the OS to write buffered data to physical storage\n- Consider `syscall.Madvise()` with `MADV_SEQUENTIAL` for chunk files to optimize OS page cache behavior\n- Use `encoding/binary` with `binary.LittleEndian` for cross-platform chunk format compatibility\n- Implement compression interface to swap algorithms: `compress/gzip`, `github.com/klauspost/compress/zstd`, `github.com/pierrec/lz4`\n- Use `sync.Pool` for compression buffer reuse to reduce GC pressure during high-throughput scenarios\n- Consider `mmap` for read-only chunk access to reduce memory copying: `golang.org/x/sys/unix.Mmap`\n\n**Milestone Checkpoint:**\n\nAfter implementing the storage engine, verify these behaviors:\n\n1. **Chunk Creation**: Run `go test ./internal/storage/chunk/...` and verify chunk files are created with correct headers and compression\n2. **WAL Durability**: Kill the process during writes and restart—verify WAL recovery replays uncommitted operations\n3. **Retention Cleanup**: Configure short retention period and verify chunks are deleted after expiration with audit logs\n4. **Compression Effectiveness**: Store sample log data and measure compression ratios match expected values (60-80% for zstd)\n5. **Performance Baseline**: Measure write throughput (should handle 10,000+ entries/sec) and query decompression speed\n\n**Signs of Correct Implementation:**\n- Chunk files have valid headers and can be decompressed successfully\n- WAL files rotate at configured size limits without unbounded growth\n- Retention policies delete expired chunks within expected time windows\n- Storage metrics show reasonable compression ratios and throughput numbers\n- Recovery after simulated crashes replays all uncommitted operations correctly\n\n**Signs Something Is Wrong:**\n- Chunk files are corrupted or cannot be decompressed → Check binary serialization and compression error handling\n- WAL files grow indefinitely → Verify checkpoint and rotation logic is working properly  \n- Retention policies don't delete expired data → Check policy evaluation logic and file permission issues\n- Poor compression ratios (< 50%) → Verify compression algorithm selection and data serialization format\n- Slow write performance (< 1000 entries/sec) → Profile fsync overhead and buffer management efficiency\n\n\n## Multi-Tenancy and Alerting\n\n> **Milestone(s):** This section corresponds to Milestone 5 (Multi-Tenant & Alerting), where we add tenant isolation with per-tenant rate limits and log-based alerting rules. This builds upon all previous milestones, extending the system with enterprise features for production deployment.\n\n### Mental Model: The Apartment Building\n\nThink of our log aggregation system as a large apartment building where each tenant (organization) has their own private unit. Just as apartment buildings need secure entry systems, individual mailboxes, separate utility meters, and building-wide fire safety alerts, our multi-tenant log system needs authentication barriers, isolated data storage, per-tenant resource limits, and system-wide alerting capabilities.\n\nIn this apartment building analogy, the **building manager** represents our tenant isolation layer - checking IDs at the front door, ensuring residents can only access their own units, and tracking each unit's water and electricity usage. The **mailroom** represents our ingestion pipeline with per-tenant rate limiting - preventing any single resident from overwhelming the building's mail capacity. The **fire alarm system** represents our alerting engine - monitoring all units for dangerous patterns and notifying the appropriate parties when problems arise.\n\nJust as a poorly designed apartment building might have thin walls (data leakage), broken locks (authentication bypasses), or residents who flood the building's water system (resource exhaustion), our multi-tenant log system must carefully design isolation boundaries, access controls, and resource management to prevent one tenant's activity from affecting others.\n\nThe key insight from this analogy is that multi-tenancy isn't just about storing data separately - it's about creating comprehensive isolation at every layer while efficiently sharing the underlying infrastructure. The building's foundation, plumbing, and electrical systems are shared for efficiency, but each tenant gets their own protected space with measured resource consumption.\n\n### Tenant Isolation Design\n\n**Multi-tenant isolation** in a log aggregation system requires creating secure boundaries between different organizations or teams while maximizing infrastructure efficiency. Unlike simple access control where we trust users not to peek at each other's data, tenant isolation assumes potential adversaries and enforces separation at the architectural level.\n\n![Multi-Tenant Architecture](./diagrams/tenant-isolation.svg)\n\nOur tenant isolation strategy operates on three fundamental principles: **authentication identity**, **authorization boundaries**, and **data segregation**. Every request entering the system must first establish which tenant it represents, then verify that tenant has permission for the requested operation, and finally ensure all data access respects tenant boundaries throughout the entire processing pipeline.\n\nThe **tenant identification** process begins at the ingestion and query entry points. For HTTP endpoints, we extract tenant identity from request headers, authentication tokens, or URL paths. For TCP and UDP syslog ingestion, tenant identity comes from source IP ranges, TLS certificates, or message headers. This identification must happen before any data processing begins, as it determines how every subsequent operation behaves.\n\n| Tenant Identification Method | Protocol | Security Level | Implementation Complexity | Use Case |\n|----------------------------|----------|----------------|-------------------------|----------|\n| HTTP Bearer Token | HTTP | High | Medium | API clients, log shippers |\n| TLS Client Certificate | TCP/HTTP | High | High | High-security environments |\n| Source IP Mapping | TCP/UDP | Medium | Low | Trusted network segments |\n| Message Header Field | Syslog | Low | Low | Development environments |\n| URL Path Prefix | HTTP | Low | Low | Simple testing setups |\n\n**Authorization boundaries** define what operations each tenant can perform within their isolated environment. Our authorization model uses **role-based access control (RBAC)** where each tenant has roles like `log-writer`, `log-reader`, and `admin`. These roles map to specific capabilities across our system components.\n\n| Role | Ingestion Rights | Query Rights | Admin Rights | Typical Users |\n|------|-----------------|--------------|--------------|---------------|\n| log-writer | Write to tenant streams | None | None | Application servers, log agents |\n| log-reader | None | Read tenant streams | None | Developers, monitoring tools |\n| admin | Full ingestion | Full queries | Manage retention, alerts | Operations teams |\n| audit-reader | None | Read audit logs only | None | Security teams |\n| service-account | Specific streams only | Specific streams only | None | Automated systems |\n\n**Data segregation** ensures that tenant data remains completely separate throughout the entire data lifecycle. This segregation occurs at multiple levels: logical separation in our data structures, physical separation in storage chunks, and operational separation in background processes like indexing and retention cleanup.\n\nAt the **logical level**, every `LogEntry` carries tenant context through its `Labels` map, where a reserved `__tenant_id__` label identifies ownership. This label is automatically injected during ingestion and used to filter all subsequent operations. The indexing engine creates separate `IndexSegment` instances per tenant, preventing cross-tenant term lookups even at the data structure level.\n\nAt the **physical level**, our storage engine organizes chunks by tenant identity, creating separate directory hierarchies for each tenant's data. This physical separation provides additional security guarantees and enables tenant-specific storage policies. The write-ahead log maintains separate WAL files per tenant, ensuring that even crash recovery respects tenant boundaries.\n\n| Segregation Level | Implementation | Security Benefit | Performance Impact | Recovery Complexity |\n|------------------|----------------|------------------|-------------------|-------------------|\n| Label-based | __tenant_id__ in Labels | Logical isolation | Low overhead | Simple filtering |\n| Index-based | Separate IndexSegment per tenant | Query isolation | Medium overhead | Per-tenant rebuild |\n| Chunk-based | Tenant-specific directories | Storage isolation | Low overhead | Independent recovery |\n| WAL-based | Per-tenant WAL files | Write isolation | Medium overhead | Parallel replay |\n\nThe **tenant context propagation** mechanism ensures that tenant identity flows correctly through every processing stage. When a log entry enters the ingestion pipeline, we attach tenant context to the processing thread or goroutine. This context travels with the entry through parsing, indexing, and storage, ensuring that all operations respect tenant boundaries without requiring explicit tenant checks at every step.\n\n**Resource quotas** prevent any single tenant from overwhelming shared system resources. Our quota system tracks multiple dimensions of resource consumption: ingestion rate (entries per second), storage usage (bytes on disk), query frequency (queries per minute), and memory consumption (active query memory). These quotas are enforced at the component level, with graceful degradation when limits are approached.\n\n| Resource Type | Quota Mechanism | Enforcement Point | Overflow Behavior | Recovery Action |\n|---------------|-----------------|-------------------|-------------------|-----------------|\n| Ingestion Rate | Token bucket per tenant | HTTP/TCP handlers | Return 429 status | Exponential backoff |\n| Storage Size | Byte counting per tenant | Chunk writer | Reject new writes | Trigger retention cleanup |\n| Query Concurrency | Active query limit | Query engine | Queue or reject | Finish existing queries |\n| Memory Usage | Per-query limits | Result processing | Terminate query | Return partial results |\n\n> **Design Insight:** The key challenge in multi-tenancy is balancing security isolation with operational efficiency. Overly strict isolation (separate databases per tenant) becomes operationally complex, while insufficient isolation (shared tables with row-level security) creates security risks. Our approach uses logical isolation with physical separation at the storage layer, providing strong security guarantees while maintaining operational simplicity.\n\n### Rate Limiting and Quotas\n\n**Rate limiting** in a multi-tenant log aggregation system serves dual purposes: preventing individual tenants from overwhelming shared infrastructure and ensuring fair resource allocation across all tenants. Unlike simple throttling that just slows down requests, our rate limiting system implements sophisticated quota management with different limits for different resource types and tenant tiers.\n\nThe **token bucket algorithm** forms the foundation of our rate limiting approach. Each tenant receives separate token buckets for different resource types: ingestion tokens (for log entries), query tokens (for search requests), and bandwidth tokens (for data transfer). These buckets refill at configurable rates, allowing burst capacity while enforcing sustained rate limits.\n\nOur token bucket implementation uses **hierarchical buckets** to handle different granularities of rate limiting. The top-level bucket controls the tenant's overall ingestion rate, while subsidiary buckets limit specific log streams within that tenant. This hierarchy prevents a single noisy application from consuming a tenant's entire quota while allowing legitimate burst traffic from other applications.\n\n| Bucket Level | Scope | Typical Limit | Refill Rate | Burst Capacity | Use Case |\n|-------------|--------|---------------|-------------|----------------|----------|\n| Global | Entire system | 1M entries/sec | Constant | 2x rate | System protection |\n| Tenant | Single tenant | 100K entries/sec | Configurable | 5x rate | Fair sharing |\n| Stream | Label set | 10K entries/sec | Proportional | 2x rate | Application isolation |\n| Client | Source IP | 1K entries/sec | Fixed | 1x rate | Abuse prevention |\n\n**Quota enforcement** occurs at multiple stages in the ingestion pipeline to provide both early rejection and accurate accounting. The HTTP handler performs an initial quota check before accepting request bodies, preventing expensive parsing of requests that will ultimately be rejected. The buffer stage performs a second check before queuing entries, and the storage stage performs a final check before writing to disk.\n\nThe **ingestion rate limiting** mechanism uses a combination of reject-early and queue-throttling strategies. When a tenant approaches their rate limit, we begin applying random jittered delays to their requests, encouraging clients to back off gradually. When the limit is exceeded, we return HTTP 429 responses with `Retry-After` headers indicating when the client should attempt again.\n\n| Rate Limit State | Utilization | Response Strategy | HTTP Status | Backoff Signal |\n|-----------------|-------------|-------------------|-------------|----------------|\n| Normal | 0-70% | Accept immediately | 200 OK | None |\n| Warning | 70-90% | Add jittered delay | 200 OK | X-RateLimit-Remaining |\n| Throttling | 90-100% | Exponential delays | 200 OK | X-RateLimit-Reset |\n| Exceeded | >100% | Reject request | 429 Too Many | Retry-After |\n\n**Storage quotas** prevent tenants from consuming unbounded disk space while allowing temporary bursts during high-volume periods. Our storage quota system tracks both current usage and projected future usage based on recent ingestion patterns. When a tenant approaches their storage limit, we trigger aggressive retention cleanup for their data before resorting to ingestion rejection.\n\nThe **quota calculation** algorithm considers both absolute limits (maximum bytes per tenant) and relative limits (percentage of total system capacity). This dual approach ensures that small tenants get guaranteed minimum resources while large tenants can utilize available capacity during low-usage periods.\n\n**Memory quotas** for query operations prevent expensive queries from exhausting system RAM. Each query receives a memory budget based on the requesting tenant's limits and current system load. The query engine monitors memory usage throughout execution and terminates queries that exceed their allocation, returning partial results with appropriate error messages.\n\n| Memory Limit Type | Calculation | Enforcement | Overflow Action | Recovery Method |\n|-------------------|-------------|-------------|-----------------|-----------------|\n| Per-query | min(tenant_limit, system_available/active_queries) | Query engine | Terminate with error | Retry with smaller scope |\n| Per-tenant | configured_limit * tenant_tier_multiplier | Admission control | Queue new queries | Wait for completion |\n| System-wide | total_ram * 0.8 | Global limiter | Reject all queries | System load balancing |\n\n**Burst handling** allows tenants to exceed sustained rates for short periods, accommodating real-world traffic patterns where applications generate logs in bursts rather than steady streams. Our burst algorithm uses a **sliding window** approach, tracking ingestion rates over multiple time scales (1 minute, 5 minutes, 1 hour) and allowing short-term spikes as long as longer-term averages remain within limits.\n\n> **Design Insight:** Effective rate limiting requires understanding the difference between malicious abuse and legitimate usage patterns. Log generation is inherently bursty - applications restart, batch jobs run, errors cascade. Our rate limiting system must accommodate these patterns while still protecting system stability. The key is using multiple time windows and burst allowances rather than simple per-second limits.\n\n### Log-Based Alerting Engine\n\n**Log-based alerting** transforms our log aggregation system from a passive storage repository into an active monitoring platform that can detect problems and notify operators in real-time. Unlike traditional metric-based alerting that relies on pre-aggregated counters, log-based alerting operates directly on the raw log stream, enabling detection of complex patterns that might not be visible in summary statistics.\n\nThe **alerting architecture** consists of three main components: the **rule engine** that evaluates alert conditions, the **notification dispatcher** that delivers alerts through various channels, and the **deduplication system** that prevents alert storms. These components operate continuously on the incoming log stream, providing near real-time detection of problematic patterns.\n\nOur **rule definition language** extends LogQL with temporal operators and threshold functions. Alert rules specify a LogQL query, an evaluation window, and trigger conditions. For example, an alert might trigger when error logs from the payment service exceed 10 occurrences in any 5-minute window, or when any log contains specific security-related keywords.\n\n| Rule Component | Purpose | Example | Evaluation Frequency |\n|---------------|---------|---------|---------------------|\n| Query | Select relevant logs | {service=\"payment\"} \\|= \"ERROR\" | Every 30 seconds |\n| Window | Time range for evaluation | last 5 minutes | Sliding window |\n| Condition | Trigger threshold | count > 10 | Per evaluation |\n| Severity | Alert importance | critical, warning, info | Static configuration |\n\n**Rule evaluation** occurs continuously as new logs arrive, using an efficient **streaming evaluation** approach rather than periodic batch processing. As each log entry enters the system, the alerting engine checks it against all active rules whose query patterns it might match. This streaming approach provides faster detection than batch evaluation while using fewer system resources.\n\nThe **pattern matching engine** uses the same inverted index infrastructure as our query system, but optimized for real-time evaluation rather than historical search. Alert rules are compiled into efficient matchers that can quickly determine whether an incoming log entry is relevant without performing expensive text processing.\n\n| Matching Strategy | Use Case | Performance | Accuracy | Resource Usage |\n|------------------|----------|-------------|----------|----------------|\n| Exact string match | Known error messages | Very fast | Perfect | Low CPU |\n| Regex patterns | Flexible error detection | Medium | Good | Medium CPU |\n| Label selectors | Service/environment filtering | Fast | Perfect | Low CPU |\n| Full-text search | Unknown error patterns | Slow | Good | High CPU |\n\n**State management** for alert rules requires tracking evaluation windows and trigger conditions across time. Each rule maintains a sliding window of recent events, counting occurrences or tracking specific patterns. When the trigger condition is met, the rule transitions to an active state and generates an alert event.\n\n**Alert deduplication** prevents notification fatigue by intelligently grouping related alerts and suppressing redundant notifications. Our deduplication algorithm considers multiple factors: alert content similarity, time proximity, affected services, and notification channels. Similar alerts within a configurable time window are grouped together, with notifications sent for the group rather than individual occurrences.\n\n| Deduplication Dimension | Grouping Key | Time Window | Max Group Size | Notification Strategy |\n|------------------------|--------------|-------------|----------------|----------------------|\n| Exact message | Message hash | 5 minutes | 1000 events | Single notification |\n| Service errors | Service label | 10 minutes | 500 events | Periodic summaries |\n| Host problems | Hostname | 15 minutes | 100 events | Escalating frequency |\n| Security events | Source IP | 1 hour | 50 events | Immediate + summary |\n\n**Notification delivery** supports multiple channels with different reliability guarantees and formatting options. The notification system uses a **reliable delivery** approach with retry logic, dead letter queues, and delivery confirmation tracking. Failed notifications are retried with exponential backoff, and persistent failures are escalated to alternative channels.\n\n| Channel Type | Reliability | Latency | Format Options | Use Case |\n|-------------|-------------|---------|----------------|----------|\n| Webhook HTTP | Medium | Low | JSON, custom templates | ChatOps, ticketing systems |\n| Email SMTP | High | Medium | HTML, plain text | Human notifications |\n| Slack API | Medium | Low | Rich formatting, threads | Team communication |\n| PagerDuty | Very High | Low | Structured events | On-call escalation |\n| SNS/SQS | High | Very Low | JSON events | System integration |\n\n**Alert routing** ensures that notifications reach the appropriate recipients based on tenant context, alert severity, and escalation policies. Each tenant configures their own routing rules, specifying which types of alerts should go to which notification channels. The routing system supports complex logic including time-based routing (different contacts for business hours vs. on-call), severity-based escalation, and service-specific routing.\n\n**Alert lifecycle management** tracks alerts from initial trigger through resolution, providing audit trails and preventing duplicate handling. Active alerts are stored with their current state, notification history, and acknowledgment status. When the underlying condition resolves (e.g., error rate drops below threshold), the alert automatically transitions to a resolved state.\n\n| Alert State | Triggers | Available Actions | Auto Transitions | Notification Behavior |\n|------------|----------|------------------|------------------|----------------------|\n| Triggered | Rule condition met | Acknowledge, Escalate | To Active after delay | Initial notification |\n| Active | Confirmation delay expires | Acknowledge, Resolve | To Resolved when fixed | Reminder notifications |\n| Acknowledged | Manual operator action | Resolve, Escalate | To Resolved when fixed | Suppressed notifications |\n| Resolved | Condition no longer met | Reopen | To Triggered if recurs | Resolution notification |\n\n> **Design Insight:** The biggest challenge in log-based alerting is balancing sensitivity with noise. Too-sensitive rules generate alert fatigue, while too-conservative rules miss real problems. The key is providing sophisticated deduplication and grouping capabilities so that operators can configure sensitive detection knowing that related alerts will be intelligently grouped rather than flooding notification channels.\n\n### Architecture Decision Records\n\nOur multi-tenancy and alerting implementation required several critical architectural decisions that significantly impact system behavior, security, and operational characteristics.\n\n> **Decision: Tenant Identification Strategy**\n> - **Context**: We needed a mechanism to identify which tenant each log entry and query belongs to while supporting multiple ingestion protocols (HTTP, TCP, UDP) and authentication methods\n> - **Options Considered**: \n>   - URL-based tenant identification (`/api/v1/logs/{tenant-id}`)\n>   - HTTP header-based identification (`X-Tenant-ID: tenant123`)\n>   - Authentication token embedded tenant ID (JWT claims)\n> - **Decision**: Hybrid approach using HTTP headers as primary with token-based fallback\n> - **Rationale**: HTTP headers provide simplicity for most clients while token-based identification supports advanced authentication flows. URL-based identification creates routing complexity and makes tenant ID visible in access logs\n> - **Consequences**: Clients must include tenant identification in headers, but this enables flexible authentication integration and keeps tenant information secure in encrypted connections\n\n| Option | Security | Client Simplicity | Protocol Support | Chosen |\n|--------|----------|------------------|------------------|--------|\n| URL Path | Low (visible in logs) | High | HTTP only | No |\n| HTTP Header | Medium | Medium | HTTP only | Yes (Primary) |\n| Token Claims | High | Low | All protocols | Yes (Fallback) |\n\n> **Decision: Data Isolation Level**\n> - **Context**: Multi-tenant systems must prevent data leakage between tenants while maintaining query performance and operational simplicity\n> - **Options Considered**:\n>   - Database-per-tenant (complete physical separation)\n>   - Schema-per-tenant (logical separation with shared infrastructure)\n>   - Row-level security (shared tables with access controls)\n> - **Decision**: Hybrid logical isolation with physical storage separation\n> - **Rationale**: Complete physical separation creates operational complexity and resource waste. Pure logical separation creates security risks. Our hybrid approach uses shared indexes and query infrastructure with tenant-separated storage chunks\n> - **Consequences**: Strong security guarantees with moderate operational complexity, but requires careful implementation to prevent logical separation bypasses\n\n| Option | Security Level | Operational Complexity | Resource Efficiency | Chosen |\n|--------|---------------|------------------------|-------------------|--------|\n| Database-per-tenant | Very High | Very High | Low | No |\n| Schema-per-tenant | High | High | Medium | No |\n| Row-level security | Medium | Medium | High | No |\n| Hybrid isolation | High | Medium | Medium | Yes |\n\n> **Decision: Rate Limiting Algorithm**\n> - **Context**: Need to prevent individual tenants from overwhelming shared resources while allowing legitimate burst traffic patterns common in log generation\n> - **Options Considered**:\n>   - Fixed window rate limiting (simple counter per time window)\n>   - Sliding window rate limiting (precise but memory intensive)\n>   - Token bucket algorithm (burst-friendly with sustained limits)\n> - **Decision**: Token bucket with hierarchical buckets per tenant\n> - **Rationale**: Log traffic is inherently bursty (application restarts, batch jobs, error cascades). Token bucket naturally accommodates bursts while enforcing sustained rate limits. Hierarchical buckets provide granular control\n> - **Consequences**: More complex implementation than fixed windows, but much better user experience for legitimate traffic patterns\n\n| Option | Burst Handling | Memory Usage | Implementation Complexity | Chosen |\n|--------|---------------|--------------|--------------------------|--------|\n| Fixed Window | Poor | Low | Low | No |\n| Sliding Window | Good | High | Medium | No |\n| Token Bucket | Excellent | Medium | Medium | Yes |\n\n> **Decision: Alert Rule Evaluation Strategy**\n> - **Context**: Alert rules must evaluate continuously against incoming log streams without significantly impacting ingestion performance\n> - **Options Considered**:\n>   - Batch evaluation (periodic rule evaluation against recent logs)\n>   - Streaming evaluation (check each log against relevant rules)\n>   - Hybrid approach (streaming for fast rules, batch for complex queries)\n> - **Decision**: Streaming evaluation with efficient pattern matching\n> - **Rationale**: Batch evaluation introduces latency that defeats the purpose of real-time alerting. Pure streaming provides fastest detection. Pattern matching optimization makes streaming feasible even with many rules\n> - **Consequences**: Faster alert detection but requires sophisticated rule optimization and may impact ingestion throughput under high rule loads\n\n| Option | Detection Latency | Ingestion Impact | Rule Complexity Support | Chosen |\n|--------|------------------|------------------|------------------------|--------|\n| Batch Evaluation | High (1-5 minutes) | Low | High | No |\n| Streaming | Very Low (<30 sec) | Medium | Medium | Yes |\n| Hybrid | Medium | Medium | High | No |\n\n> **Decision: Alert Deduplication Approach**\n> - **Context**: Log-based alerts can generate massive notification volumes during incidents, creating alert fatigue and masking important information\n> - **Options Considered**:\n>   - Time-based deduplication (suppress identical alerts within time windows)\n>   - Content-based grouping (group similar alert messages together)\n>   - Machine learning clustering (automatically discover alert patterns)\n> - **Decision**: Multi-dimensional deduplication using time, content, and service context\n> - **Rationale**: Pure time-based deduplication misses important variations. Content-only grouping doesn't account for context. ML clustering adds complexity. Multi-dimensional approach balances effectiveness with implementation simplicity\n> - **Consequences**: Sophisticated deduplication reduces noise significantly but requires careful tuning of grouping parameters per tenant\n\n| Option | Noise Reduction | Configuration Complexity | Resource Usage | Chosen |\n|--------|-----------------|-------------------------|----------------|--------|\n| Time-based only | Low | Low | Low | No |\n| Content-based only | Medium | Medium | Medium | No |\n| ML Clustering | High | Very High | High | No |\n| Multi-dimensional | High | Medium | Medium | Yes |\n\n### Common Pitfalls\n\nMulti-tenancy and alerting systems introduce complex security and operational challenges that can create subtle but serious problems in production deployments.\n\n⚠️ **Pitfall: Tenant ID Injection Attacks**\n\nOne of the most dangerous vulnerabilities in multi-tenant systems occurs when tenant identification can be manipulated by malicious clients. Developers often make the mistake of trusting client-provided tenant identifiers without proper validation, allowing attackers to access other tenants' data by simply changing header values or token claims.\n\nThis happens when the system accepts tenant IDs from user-controlled inputs (HTTP headers, query parameters, form fields) and uses them directly in database queries or file system operations. For example, if a client sends `X-Tenant-ID: victim-tenant` and the system blindly uses this value to construct file paths or database queries, the attacker can access any tenant's data.\n\nThe correct approach is to derive tenant identity from authenticated and cryptographically verified sources. Authentication tokens should be validated against a trusted identity provider, and tenant membership should be verified against an authoritative directory. Never trust client-provided tenant identifiers without cryptographic proof of legitimacy.\n\nImplementation fix: Always extract tenant context from verified authentication tokens rather than client headers. Use a mapping service that validates token claims against tenant membership databases. Implement audit logging for all tenant context establishment to detect injection attempts.\n\n⚠️ **Pitfall: Alert Storm Cascades**\n\nLog-based alerting systems can create devastating feedback loops where alerts themselves generate logs that trigger more alerts, creating exponentially growing alert volumes that overwhelm notification systems and operators. This commonly occurs when alert notification failures are logged as errors, triggering alerts about alert system failures.\n\nThe cascade typically starts with a legitimate problem that triggers an alert. If the notification system fails (webhook timeouts, email server issues), the alerting system logs error messages about delivery failures. If there's an alert rule monitoring the alerting system's own logs for errors, it triggers additional alerts about the notification failures. These new alerts also fail to deliver, creating more error logs and more alerts in an exponential cascade.\n\nThis problem is particularly severe in distributed systems where cascading failures cause multiple services to simultaneously generate error logs, each triggering their own alert rules, overwhelming the notification infrastructure and making it impossible for operators to identify the root cause.\n\nPrevention requires implementing alert rule hierarchies with different reliability guarantees, circuit breakers on notification delivery, and careful separation of system monitoring from application monitoring. Alert delivery failures should never trigger the same alert delivery mechanisms that are already failing.\n\n⚠️ **Pitfall: Resource Leakage Between Tenants**\n\nShared infrastructure in multi-tenant systems can create subtle resource leakage where one tenant's activity affects another tenant's performance. This commonly occurs in memory management, connection pooling, and background processing where resources allocated for one tenant aren't properly released or isolated.\n\nMemory leakage often happens when tenant-specific data structures (query results, parsed log entries, index segments) aren't properly garbage collected because they're referenced by shared global caches or background goroutines. A single tenant with high query volume can gradually consume all available memory, degrading performance for other tenants.\n\nConnection leakage occurs when database connections or HTTP client connections opened for one tenant's operations aren't properly returned to pools, eventually exhausting the connection limits and preventing other tenants from accessing resources.\n\nBackground processing leakage happens when cleanup tasks, indexing operations, or retention processes get stuck processing one tenant's data, preventing other tenants' background work from completing. This can cause query performance degradation and storage cleanup delays.\n\nMitigation requires implementing proper resource accounting at the tenant level, with explicit quotas and cleanup procedures. Use tenant-scoped context cancellation to ensure long-running operations can be interrupted. Implement resource monitoring that can identify which tenant is consuming resources and enforce limits before system-wide impact occurs.\n\n⚠️ **Pitfall: Authentication Bypass in Protocol Handlers**\n\nMulti-protocol ingestion systems (HTTP, TCP, UDP) often have inconsistent authentication enforcement, creating security gaps where attackers can bypass tenant isolation by choosing different ingestion protocols. Developers frequently implement robust authentication for HTTP endpoints but forget to apply the same rigor to TCP and UDP handlers.\n\nThis commonly occurs when HTTP endpoints require Bearer tokens or API keys, but TCP syslog handlers only check source IP addresses or accept any connection. Attackers can discover these protocol inconsistencies and inject logs into other tenants' streams by sending syslog messages to TCP ports instead of using authenticated HTTP endpoints.\n\nUDP handlers are particularly vulnerable because they're connectionless and often lack any authentication mechanism. If the system relies on source IP filtering for UDP authentication, attackers can spoof IP addresses to impersonate legitimate log sources.\n\nThe solution requires implementing consistent authentication policies across all ingestion protocols. For TCP connections, use TLS client certificates or require authentication tokens in message headers. For UDP, implement cryptographic message signing or restrict UDP ingestion to trusted network segments with strong perimeter security.\n\n⚠️ **Pitfall: Alert Rule Performance Degradation**\n\nAs tenants add more alert rules, the streaming evaluation system can gradually degrade ingestion performance, especially when rules use expensive operations like regular expressions or complex LogQL queries. This degradation often goes unnoticed until ingestion latency becomes problematic during high-volume periods.\n\nThe problem occurs because each incoming log entry must be evaluated against all potentially matching alert rules. With hundreds of tenants each having dozens of alert rules, a single log entry might require thousands of rule evaluations. Complex regex patterns or full-text search operations in alert rules can make this evaluation extremely expensive.\n\nRule optimization becomes critical for system scalability. Implement rule compilation that converts alert patterns into efficient finite state machines. Use bloom filters to quickly exclude logs that couldn't possibly match rule patterns. Provide rule authoring guidance that encourages efficient patterns and warns about expensive operations.\n\nMonitor rule evaluation performance and implement automatic rule disabling for patterns that consistently exceed performance budgets. Provide tenants with rule performance analytics so they can optimize their alert configurations.\n\n### Implementation Guidance\n\nMulti-tenancy and alerting require integrating security controls throughout the system architecture while maintaining the performance characteristics established in previous milestones.\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Authentication | HTTP Basic Auth + static tenant mapping | JWT tokens with RSA signatures |\n| Authorization | Role-based access with config files | Policy-based access with Open Policy Agent |\n| Rate Limiting | In-memory token buckets | Redis-backed distributed rate limiting |\n| Alert Storage | File-based rule storage | Database with rule versioning |\n| Notifications | HTTP webhooks only | Multi-channel with delivery tracking |\n| Deduplication | Time-window based grouping | Content similarity with clustering |\n\n**Recommended File Structure:**\n\n```\ninternal/\n  auth/\n    auth.go                 ← tenant authentication and context\n    middleware.go           ← HTTP middleware for tenant extraction\n    rbac.go                 ← role-based access control\n  ratelimit/\n    bucket.go               ← token bucket implementation\n    limiter.go              ← hierarchical rate limiting\n    quota.go                ← storage and resource quotas\n  alerting/\n    engine.go               ← alert rule evaluation engine\n    rules.go                ← rule definition and compilation\n    notifications.go        ← multi-channel notification delivery\n    deduplication.go        ← alert grouping and deduplication\n  tenant/\n    context.go              ← tenant context propagation\n    isolation.go            ← data isolation enforcement\n    metrics.go              ← per-tenant resource tracking\n```\n\n**Authentication and Authorization Infrastructure:**\n\nThe authentication system provides the foundation for all multi-tenant operations. This complete implementation handles JWT token validation and tenant context extraction:\n\n```go\npackage auth\n\nimport (\n    \"context\"\n    \"crypto/rsa\"\n    \"errors\"\n    \"fmt\"\n    \"net/http\"\n    \"strings\"\n    \"time\"\n    \n    \"github.com/golang-jwt/jwt/v5\"\n)\n\n// TenantContext represents authenticated tenant information\ntype TenantContext struct {\n    TenantID    string\n    Roles       []string\n    Quotas      ResourceQuotas\n    AuthMethod  string\n    ExpiresAt   time.Time\n}\n\ntype ResourceQuotas struct {\n    MaxIngestionRate   int64  // entries per second\n    MaxStorageBytes    int64  // total storage limit\n    MaxActiveQueries   int32  // concurrent query limit\n    MaxQueryMemoryMB   int64  // memory per query\n}\n\n// AuthService handles tenant authentication and authorization\ntype AuthService struct {\n    publicKey    *rsa.PublicKey\n    tenantConfig map[string]TenantConfig\n    defaultQuotas ResourceQuotas\n}\n\ntype TenantConfig struct {\n    Name          string\n    Quotas        ResourceQuotas\n    AllowedRoles  []string\n    IPWhitelist   []string\n}\n\nfunc NewAuthService(publicKeyPath string, configPath string) (*AuthService, error) {\n    // Load RSA public key for JWT verification\n    publicKey, err := loadPublicKey(publicKeyPath)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to load public key: %w\", err)\n    }\n    \n    // Load tenant configuration\n    tenantConfig, err := loadTenantConfig(configPath)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to load tenant config: %w\", err)\n    }\n    \n    return &AuthService{\n        publicKey:    publicKey,\n        tenantConfig: tenantConfig,\n        defaultQuotas: ResourceQuotas{\n            MaxIngestionRate:  1000,\n            MaxStorageBytes:   1024 * 1024 * 1024, // 1GB\n            MaxActiveQueries:  10,\n            MaxQueryMemoryMB:  256,\n        },\n    }, nil\n}\n\n// AuthenticateRequest extracts and validates tenant context from HTTP request\nfunc (a *AuthService) AuthenticateRequest(r *http.Request) (*TenantContext, error) {\n    // Try JWT token first\n    if authHeader := r.Header.Get(\"Authorization\"); authHeader != \"\" {\n        return a.authenticateJWT(authHeader)\n    }\n    \n    // Fall back to explicit tenant header (for development)\n    if tenantID := r.Header.Get(\"X-Tenant-ID\"); tenantID != \"\" {\n        return a.authenticateHeaderBased(tenantID, r)\n    }\n    \n    return nil, errors.New(\"no authentication provided\")\n}\n\nfunc (a *AuthService) authenticateJWT(authHeader string) (*TenantContext, error) {\n    // Extract token from \"Bearer <token>\" format\n    parts := strings.SplitN(authHeader, \" \", 2)\n    if len(parts) != 2 || parts[0] != \"Bearer\" {\n        return nil, errors.New(\"invalid authorization header format\")\n    }\n    \n    // Parse and validate JWT\n    token, err := jwt.Parse(parts[1], func(token *jwt.Token) (interface{}, error) {\n        if _, ok := token.Method.(*jwt.SigningMethodRSA); !ok {\n            return nil, fmt.Errorf(\"unexpected signing method: %v\", token.Header[\"alg\"])\n        }\n        return a.publicKey, nil\n    })\n    \n    if err != nil {\n        return nil, fmt.Errorf(\"failed to parse JWT: %w\", err)\n    }\n    \n    if !token.Valid {\n        return nil, errors.New(\"invalid JWT token\")\n    }\n    \n    // Extract claims\n    claims, ok := token.Claims.(jwt.MapClaims)\n    if !ok {\n        return nil, errors.New(\"invalid JWT claims\")\n    }\n    \n    tenantID, ok := claims[\"tenant_id\"].(string)\n    if !ok {\n        return nil, errors.New(\"missing tenant_id claim\")\n    }\n    \n    roles, ok := claims[\"roles\"].([]interface{})\n    if !ok {\n        return nil, errors.New(\"missing roles claim\")\n    }\n    \n    // Convert roles to string slice\n    roleStrs := make([]string, len(roles))\n    for i, role := range roles {\n        if roleStr, ok := role.(string); ok {\n            roleStrs[i] = roleStr\n        }\n    }\n    \n    // Get tenant configuration\n    config, exists := a.tenantConfig[tenantID]\n    if !exists {\n        return nil, fmt.Errorf(\"unknown tenant: %s\", tenantID)\n    }\n    \n    // Extract expiration\n    exp, ok := claims[\"exp\"].(float64)\n    if !ok {\n        return nil, errors.New(\"missing exp claim\")\n    }\n    \n    return &TenantContext{\n        TenantID:   tenantID,\n        Roles:      roleStrs,\n        Quotas:     config.Quotas,\n        AuthMethod: \"jwt\",\n        ExpiresAt:  time.Unix(int64(exp), 0),\n    }, nil\n}\n\n// CheckPermission verifies if tenant has required role for operation\nfunc (tc *TenantContext) CheckPermission(requiredRole string) error {\n    for _, role := range tc.Roles {\n        if role == requiredRole || role == \"admin\" {\n            return nil\n        }\n    }\n    return fmt.Errorf(\"tenant %s lacks required role: %s\", tc.TenantID, requiredRole)\n}\n\n// HTTP middleware for tenant authentication\nfunc (a *AuthService) TenantAuthMiddleware(next http.HandlerFunc) http.HandlerFunc {\n    return func(w http.ResponseWriter, r *http.Request) {\n        tenantCtx, err := a.AuthenticateRequest(r)\n        if err != nil {\n            http.Error(w, fmt.Sprintf(\"Authentication failed: %v\", err), http.StatusUnauthorized)\n            return\n        }\n        \n        // Add tenant context to request\n        ctx := context.WithValue(r.Context(), \"tenant\", tenantCtx)\n        next.ServeHTTP(w, r.WithContext(ctx))\n    }\n}\n\n// GetTenantContext extracts tenant context from request context\nfunc GetTenantContext(ctx context.Context) (*TenantContext, error) {\n    tenant, ok := ctx.Value(\"tenant\").(*TenantContext)\n    if !ok {\n        return nil, errors.New(\"no tenant context found\")\n    }\n    return tenant, nil\n}\n```\n\n**Rate Limiting Infrastructure:**\n\nThis token bucket implementation provides hierarchical rate limiting with burst support:\n\n```go\npackage ratelimit\n\nimport (\n    \"context\"\n    \"sync\"\n    \"time\"\n    \"fmt\"\n)\n\n// TokenBucket implements token bucket algorithm for rate limiting\ntype TokenBucket struct {\n    mu            sync.Mutex\n    capacity      int64     // maximum tokens in bucket\n    tokens        float64   // current token count\n    refillRate    float64   // tokens added per second\n    lastRefill    time.Time // last refill timestamp\n    name          string    // bucket identifier for monitoring\n}\n\nfunc NewTokenBucket(capacity int64, refillRate float64, name string) *TokenBucket {\n    return &TokenBucket{\n        capacity:   capacity,\n        tokens:     float64(capacity), // start full\n        refillRate: refillRate,\n        lastRefill: time.Now(),\n        name:       name,\n    }\n}\n\n// TryConsume attempts to consume specified tokens, returns true if successful\nfunc (tb *TokenBucket) TryConsume(tokens int64) bool {\n    tb.mu.Lock()\n    defer tb.mu.Unlock()\n    \n    tb.refill()\n    \n    if tb.tokens >= float64(tokens) {\n        tb.tokens -= float64(tokens)\n        return true\n    }\n    \n    return false\n}\n\n// refill adds tokens based on elapsed time since last refill\nfunc (tb *TokenBucket) refill() {\n    now := time.Now()\n    elapsed := now.Sub(tb.lastRefill).Seconds()\n    \n    if elapsed > 0 {\n        tokensToAdd := elapsed * tb.refillRate\n        tb.tokens = min(tb.tokens + tokensToAdd, float64(tb.capacity))\n        tb.lastRefill = now\n    }\n}\n\n// GetStatus returns current bucket state for monitoring\nfunc (tb *TokenBucket) GetStatus() (current float64, capacity int64, rate float64) {\n    tb.mu.Lock()\n    defer tb.mu.Unlock()\n    \n    tb.refill()\n    return tb.tokens, tb.capacity, tb.refillRate\n}\n\n// HierarchicalLimiter manages multiple token buckets per tenant\ntype HierarchicalLimiter struct {\n    tenantBuckets map[string]*TokenBucket\n    streamBuckets map[string]*TokenBucket\n    mu           sync.RWMutex\n    defaultQuotas ResourceQuotas\n}\n\nfunc NewHierarchicalLimiter(defaultQuotas ResourceQuotas) *HierarchicalLimiter {\n    return &HierarchicalLimiter{\n        tenantBuckets: make(map[string]*TokenBucket),\n        streamBuckets: make(map[string]*TokenBucket),\n        defaultQuotas: defaultQuotas,\n    }\n}\n\n// CheckRateLimit verifies if operation is allowed under current limits\nfunc (hl *HierarchicalLimiter) CheckRateLimit(tenantID, streamID string, tokens int64) error {\n    // Check tenant-level bucket\n    tenantBucket := hl.getTenantBucket(tenantID)\n    if !tenantBucket.TryConsume(tokens) {\n        return fmt.Errorf(\"tenant %s exceeded rate limit\", tenantID)\n    }\n    \n    // Check stream-level bucket\n    streamBucket := hl.getStreamBucket(streamID)\n    if !streamBucket.TryConsume(tokens) {\n        // Refund tenant tokens since stream limit hit first\n        tenantBucket.mu.Lock()\n        tenantBucket.tokens = min(tenantBucket.tokens + float64(tokens), float64(tenantBucket.capacity))\n        tenantBucket.mu.Unlock()\n        \n        return fmt.Errorf(\"stream %s exceeded rate limit\", streamID)\n    }\n    \n    return nil\n}\n\nfunc (hl *HierarchicalLimiter) getTenantBucket(tenantID string) *TokenBucket {\n    hl.mu.RLock()\n    bucket, exists := hl.tenantBuckets[tenantID]\n    hl.mu.RUnlock()\n    \n    if exists {\n        return bucket\n    }\n    \n    // Create new bucket with write lock\n    hl.mu.Lock()\n    defer hl.mu.Unlock()\n    \n    // Double-check after acquiring write lock\n    if bucket, exists := hl.tenantBuckets[tenantID]; exists {\n        return bucket\n    }\n    \n    bucket = NewTokenBucket(\n        hl.defaultQuotas.MaxIngestionRate,\n        float64(hl.defaultQuotas.MaxIngestionRate),\n        fmt.Sprintf(\"tenant-%s\", tenantID),\n    )\n    hl.tenantBuckets[tenantID] = bucket\n    \n    return bucket\n}\n\nfunc (hl *HierarchicalLimiter) getStreamBucket(streamID string) *TokenBucket {\n    // Similar implementation to getTenantBucket but for streams\n    // Streams get 1/10th of tenant rate as default\n    streamRate := hl.defaultQuotas.MaxIngestionRate / 10\n    \n    hl.mu.RLock()\n    bucket, exists := hl.streamBuckets[streamID]\n    hl.mu.RUnlock()\n    \n    if exists {\n        return bucket\n    }\n    \n    hl.mu.Lock()\n    defer hl.mu.Unlock()\n    \n    if bucket, exists := hl.streamBuckets[streamID]; exists {\n        return bucket\n    }\n    \n    bucket = NewTokenBucket(\n        streamRate,\n        float64(streamRate),\n        fmt.Sprintf(\"stream-%s\", streamID),\n    )\n    hl.streamBuckets[streamID] = bucket\n    \n    return bucket\n}\n\nfunc min(a, b float64) float64 {\n    if a < b {\n        return a\n    }\n    return b\n}\n```\n\n**Alert Engine Core Implementation:**\n\nThe alerting engine evaluates rules against incoming log streams and manages notification delivery:\n\n```go\npackage alerting\n\n// AlertEngine coordinates rule evaluation and notification delivery\ntype AlertEngine struct {\n    rules           map[string]*AlertRule\n    rulesMu         sync.RWMutex\n    notifier        *NotificationManager\n    deduplicator    *AlertDeduplicator\n    evaluationChan  chan *LogEntry\n    stopChan        chan struct{}\n}\n\n// AlertRule defines conditions that trigger notifications\ntype AlertRule struct {\n    RuleID       string\n    TenantID     string\n    Name         string\n    Query        string\n    Condition    ThresholdCondition\n    Window       time.Duration\n    Severity     AlertSeverity\n    Enabled      bool\n    \n    // Internal state\n    matcher      *RuleMatcher\n    eventWindow  *SlidingWindow\n    lastFired    time.Time\n}\n\ntype ThresholdCondition struct {\n    Type      string  // \"count\", \"rate\", \"contains\"\n    Threshold float64\n    Operator  string  // \">\", \"<\", \">=\", \"<=\", \"==\"\n}\n\ntype AlertSeverity string\n\nconst (\n    SeverityInfo     AlertSeverity = \"info\"\n    SeverityWarning  AlertSeverity = \"warning\" \n    SeverityCritical AlertSeverity = \"critical\"\n)\n\nfunc NewAlertEngine(notifier *NotificationManager) *AlertEngine {\n    return &AlertEngine{\n        rules:          make(map[string]*AlertRule),\n        notifier:       notifier,\n        deduplicator:   NewAlertDeduplicator(),\n        evaluationChan: make(chan *LogEntry, 10000),\n        stopChan:       make(chan struct{}),\n    }\n}\n\n// Start begins rule evaluation goroutine\nfunc (ae *AlertEngine) Start() error {\n    go ae.evaluationLoop()\n    return nil\n}\n\n// EvaluateLogEntry checks incoming log against all relevant rules\nfunc (ae *AlertEngine) EvaluateLogEntry(entry *LogEntry) {\n    select {\n    case ae.evaluationChan <- entry:\n        // Successfully queued for evaluation\n    default:\n        // Channel full, increment dropped evaluation metric\n        // In production, this should trigger an alert about alert system overload\n    }\n}\n\nfunc (ae *AlertEngine) evaluationLoop() {\n    for {\n        select {\n        case entry := <-ae.evaluationChan:\n            ae.evaluateEntry(entry)\n        case <-ae.stopChan:\n            return\n        }\n    }\n}\n\nfunc (ae *AlertEngine) evaluateEntry(entry *LogEntry) {\n    tenantID := entry.Labels[\"__tenant_id__\"]\n    \n    ae.rulesMu.RLock()\n    defer ae.rulesMu.RUnlock()\n    \n    for _, rule := range ae.rules {\n        // Skip rules for different tenants\n        if rule.TenantID != tenantID {\n            continue\n        }\n        \n        if !rule.Enabled {\n            continue\n        }\n        \n        // Check if log matches rule query\n        if rule.matcher.Matches(entry) {\n            rule.eventWindow.Add(entry.Timestamp, 1.0)\n            \n            // Evaluate threshold condition\n            if ae.evaluateCondition(rule) {\n                ae.fireAlert(rule, entry)\n            }\n        }\n    }\n}\n\n// evaluateCondition checks if rule threshold is met\nfunc (ae *AlertEngine) evaluateCondition(rule *AlertRule) bool {\n    now := time.Now()\n    windowStart := now.Add(-rule.Window)\n    \n    value := rule.eventWindow.Sum(windowStart, now)\n    \n    switch rule.Condition.Operator {\n    case \">\":\n        return value > rule.Condition.Threshold\n    case \">=\":\n        return value >= rule.Condition.Threshold\n    case \"<\":\n        return value < rule.Condition.Threshold\n    case \"<=\":\n        return value <= rule.Condition.Threshold\n    case \"==\":\n        return value == rule.Condition.Threshold\n    default:\n        return false\n    }\n}\n\n// fireAlert creates and sends alert notification\nfunc (ae *AlertEngine) fireAlert(rule *AlertRule, triggerEntry *LogEntry) {\n    // Prevent alert spam with minimum interval\n    now := time.Now()\n    if now.Sub(rule.lastFired) < time.Minute {\n        return\n    }\n    rule.lastFired = now\n    \n    alert := &Alert{\n        AlertID:      generateAlertID(),\n        RuleID:       rule.RuleID,\n        TenantID:     rule.TenantID,\n        Severity:     rule.Severity,\n        Title:        rule.Name,\n        Message:      fmt.Sprintf(\"Alert condition met: %s\", rule.Query),\n        TriggerEntry: triggerEntry,\n        Timestamp:    now,\n        Status:       AlertStatusTriggered,\n    }\n    \n    // Apply deduplication\n    if ae.deduplicator.ShouldSuppressAlert(alert) {\n        return\n    }\n    \n    // Send notification\n    ae.notifier.SendAlert(alert)\n}\n```\n\n**Core Alert Logic Skeleton:**\n\n```go\n// AddRule registers a new alert rule for evaluation\nfunc (ae *AlertEngine) AddRule(rule *AlertRule) error {\n    // TODO 1: Validate rule configuration (query syntax, thresholds, etc.)\n    // TODO 2: Compile rule query into efficient matcher\n    // TODO 3: Initialize sliding window for event tracking\n    // TODO 4: Add rule to evaluation map with proper locking\n    // TODO 5: Log rule addition for audit trail\n}\n\n// UpdateRule modifies existing alert rule\nfunc (ae *AlertEngine) UpdateRule(ruleID string, updates *AlertRule) error {\n    // TODO 1: Validate rule exists and belongs to correct tenant\n    // TODO 2: Preserve existing rule state (window, last fired time)\n    // TODO 3: Recompile query matcher if query changed\n    // TODO 4: Update rule configuration atomically\n    // TODO 5: Log rule modification for audit trail\n}\n\n// DeleteRule removes alert rule from evaluation\nfunc (ae *AlertEngine) DeleteRule(ruleID string, tenantID string) error {\n    // TODO 1: Verify rule exists and tenant ownership\n    // TODO 2: Stop rule evaluation and clean up resources\n    // TODO 3: Remove rule from evaluation map\n    // TODO 4: Clean up any pending alerts for this rule\n    // TODO 5: Log rule deletion for audit trail\n}\n```\n\n**Milestone Checkpoint:**\n\nAfter implementing multi-tenancy and alerting:\n\n1. **Test Tenant Isolation:** Create two tenants, ingest logs for each, verify queries only return tenant-specific data\n2. **Test Rate Limiting:** Send high-volume requests and verify 429 responses when limits exceeded\n3. **Test Alert Rules:** Create rules that trigger on specific log patterns, verify notifications delivered\n4. **Test Deduplication:** Generate duplicate alert conditions, verify only appropriate notifications sent\n\nExpected behavior: Each tenant operates in complete isolation with enforced resource limits, while alert rules provide real-time problem detection with intelligent notification management.\n\n\n## Interactions and Data Flow\n\n> **Milestone(s):** This section integrates understanding from all milestones (1-5), showing how components work together across the complete system. It covers log ingestion flow (Milestone 1), query processing flow (Milestone 3), and background maintenance (Milestones 2, 4, 5).\n\n### Mental Model: The Orchestra Performance\n\nThink of a log aggregation system as a symphony orchestra during a live performance. The **log ingestion flow** is like musicians playing their parts - each instrument (protocol handler) contributes notes (log entries) that flow through the conductor (ingestion pipeline) to create harmony (indexed, stored logs). The **query processing flow** resembles an audience member requesting a specific piece - the conductor (query engine) coordinates different sections (index, storage) to replay the exact musical phrases (log entries) the listener wants to hear. Meanwhile, **background maintenance flows** are like the stage crew working behind the scenes - tuning instruments (index compaction), replacing worn sheet music (retention cleanup), and ensuring everything stays organized for the next performance.\n\nThis analogy captures the key insight that successful log aggregation requires choreographed coordination between multiple concurrent processes, each with different timing requirements and failure modes. Just as a symphony can't pause mid-performance when a violin string breaks, our system must handle component failures gracefully while maintaining the overall flow of data.\n\n### Log Ingestion Flow\n\nThe log ingestion flow represents the journey of raw log data from external sources through parsing, validation, indexing, and ultimate storage. This flow must handle massive throughput while ensuring data durability and maintaining query performance through proper indexing.\n\n![Log Ingestion Sequence](./diagrams/ingestion-flow.svg)\n\n#### Request Reception and Protocol Handling\n\nThe ingestion process begins when external systems send log data through one of three supported protocols. The `HTTPServer` listens on port 8080 and accepts POST requests containing JSON-formatted log entries. Simultaneously, the `TCPHandler` maintains persistent connections on port 1514 for syslog clients that need reliable delivery, while UDP handlers on the same port serve clients prioritizing low latency over guaranteed delivery.\n\nEach protocol handler performs initial validation specific to its transport mechanism. HTTP handlers validate request headers, content-type, and authentication tokens. TCP handlers manage connection state and implement proper syslog framing according to RFC 5424. UDP handlers must handle potential packet loss and reordering, though they cannot provide delivery guarantees.\n\nThe following table describes the message formats accepted at each ingestion endpoint:\n\n| Protocol | Message Format | Required Fields | Optional Fields | Validation Rules |\n|----------|---------------|-----------------|-----------------|------------------|\n| HTTP | JSON POST body | `timestamp`, `message` | `labels`, `stream` | ISO 8601 timestamp, non-empty message |\n| TCP | RFC 5424 syslog | `PRI`, `VERSION`, `TIMESTAMP`, `MSG` | `HOSTNAME`, `APP-NAME`, `STRUCTURED-DATA` | Valid priority, parseable timestamp |\n| UDP | RFC 3164 syslog | `PRI`, `TIMESTAMP`, `MSG` | `HOSTNAME`, `TAG` | Legacy BSD syslog format support |\n| File Tail | Raw log lines | Detected timestamp, message content | Parsed structured fields | Configurable regex patterns |\n\n> **Decision: Multi-Protocol Support Strategy**\n> - **Context**: Applications use diverse logging protocols based on their technology stack and operational requirements\n> - **Options Considered**: HTTP-only (simple), Syslog-only (standard), Multi-protocol (comprehensive)\n> - **Decision**: Support HTTP, TCP syslog, UDP syslog, and file tailing simultaneously\n> - **Rationale**: Maximum compatibility with existing infrastructure while providing protocol-specific optimizations\n> - **Consequences**: Increased implementation complexity but enables adoption without requiring application changes\n\n#### Log Parsing and Normalization\n\nOnce protocol handlers receive raw log data, the `JSONParser` converts diverse formats into standardized `LogEntry` structures. This parsing stage must handle malformed data gracefully while extracting maximum value from well-formed entries.\n\nThe parsing pipeline follows these steps for each incoming message:\n\n1. **Format Detection**: Examine message structure to identify JSON, syslog, or plain text format\n2. **Timestamp Extraction**: Parse timestamp fields using format-specific rules, falling back to current time if missing\n3. **Label Extraction**: Extract structured fields into the `Labels` map, including protocol-derived labels like `source_protocol` and `source_host`\n4. **Message Normalization**: Standardize message content, handling escape sequences and character encoding\n5. **Validation**: Verify required fields are present and values fall within acceptable ranges\n6. **Enrichment**: Add system-generated labels like `ingestion_time` and `entry_id` for tracking\n\nThe parser handles several common data quality issues that could otherwise corrupt the index or cause query failures:\n\n| Issue Type | Detection Method | Handling Strategy | Recovery Action |\n|------------|------------------|-------------------|-----------------|\n| Invalid Timestamp | Regex validation failure | Use current server time | Log parsing warning |\n| Missing Message | Empty or null message field | Reject entry | Return HTTP 400 error |\n| Label Key Conflicts | Duplicate keys with different values | Prefer client-provided values | Merge with precedence rules |\n| Oversized Entry | Message exceeds size limits | Truncate message content | Preserve labels and timestamp |\n| Invalid UTF-8 | Character encoding errors | Replace invalid sequences | Continue processing with substituted chars |\n\n> The critical insight here is that parsing failures should never halt the entire ingestion pipeline. A single malformed log entry from one application cannot be allowed to prevent other applications from successfully ingesting their logs.\n\n#### Buffering and Flow Control\n\nAfter successful parsing, `LogEntry` instances enter the `MemoryBuffer` - a ring buffer implementation that provides crucial isolation between ingestion and downstream processing. This buffering layer enables the system to handle traffic bursts while protecting storage and indexing components from overload.\n\nThe buffer operates as follows:\n\n1. **Write Operation**: The `Write(entry)` method attempts to add new entries to the ring buffer head\n2. **Capacity Check**: If buffer is full, the system applies backpressure by rejecting new writes with HTTP 503 responses\n3. **Read Operation**: Background workers continuously call `Read()` to drain entries from the buffer tail\n4. **Flow Control**: Buffer occupancy metrics trigger adaptive rate limiting when approaching capacity limits\n\nBuffer management requires careful attention to memory usage and failure scenarios:\n\n| Buffer State | Occupancy Level | Write Behavior | Read Behavior | Alert Threshold |\n|--------------|----------------|----------------|---------------|-----------------|\n| Normal | 0-60% full | Accept all writes | Batch read 100 entries | None |\n| Warning | 60-80% full | Accept writes, emit warnings | Increase batch size to 500 | Monitor buffer lag |\n| Critical | 80-95% full | Apply rate limiting | Maximum batch size 1000 | Page operations team |\n| Full | 95-100% full | Reject writes with 503 | Emergency flush to disk | Immediate escalation |\n\n⚠️ **Pitfall: Unbounded Buffer Growth**\nMany implementations allow buffers to grow indefinitely when downstream processing falls behind. This leads to memory exhaustion and eventual system crashes. Instead, implement fixed-size ring buffers with explicit backpressure policies. It's better to reject new logs during overload than to crash and lose all buffered data.\n\n#### Index Integration and Term Extraction\n\nAs buffer readers drain `LogEntry` instances, they simultaneously feed the indexing engine to maintain query performance. This integration point represents a critical system bottleneck that requires careful coordination to avoid blocking either ingestion or query processing.\n\nThe indexing integration follows this sequence:\n\n1. **Batch Formation**: Buffer readers collect entries into time-aligned batches of 1000-10000 entries\n2. **Term Extraction**: For each entry, extract indexable terms from both message content and label values\n3. **Segment Assignment**: Route terms to appropriate `IndexSegment` instances based on time-based partitioning\n4. **Posting List Updates**: Add `EntryReference` instances to postings lists for each extracted term\n5. **Bloom Filter Updates**: Insert terms into segment bloom filters for fast negative lookups\n6. **Memory Management**: Trigger segment finalization when memory usage exceeds thresholds\n\nThe term extraction process determines query performance and must balance comprehensiveness with processing efficiency:\n\n| Term Source | Extraction Method | Index Impact | Storage Overhead |\n|-------------|-------------------|--------------|-------------------|\n| Message Words | Tokenization by whitespace and punctuation | High query recall | Moderate - common terms |\n| Label Keys | Direct key indexing | Fast label filtering | Low - limited key set |\n| Label Values | Value tokenization for high-cardinality labels | Enables value search | High - potentially unbounded |\n| Structured Fields | JSON field path indexing | Precise field queries | Moderate - predictable schema |\n| Timestamp Ranges | Time window quantization | Efficient time filtering | Low - fixed intervals |\n\n> **Decision: Concurrent Index Updates**\n> - **Context**: Index updates can create bottlenecks that slow ingestion and impact query latency\n> - **Options Considered**: Synchronous updates (simple), Asynchronous with queuing (complex), Write-through caching (hybrid)\n> - **Decision**: Asynchronous index updates with bounded queues and backpressure\n> - **Rationale**: Maintains ingestion throughput while ensuring index consistency through ordered processing\n> - **Consequences**: Slight delay between ingestion and searchability, but prevents index contention from blocking ingestion\n\n#### Storage Coordination and Persistence\n\nThe final stage of log ingestion involves coordinating with the `StorageEngine` to ensure durable persistence with write-ahead logging protection. This coordination must handle both normal operations and failure recovery scenarios.\n\nStorage coordination proceeds through these phases:\n\n1. **Batch Preparation**: Group related log entries into storage-optimized batches aligned with chunk boundaries\n2. **WAL Recording**: Write `WALRecord` entries describing the pending storage operation before modifying any persistent state\n3. **Chunk Formation**: Organize log entries into compressed chunks with appropriate `ChunkHeader` metadata\n4. **Atomic Commitment**: Complete the storage operation by updating chunk indexes and marking WAL records as committed\n5. **Cleanup Coordination**: Remove committed WAL records and update buffer positions to prevent reprocessing\n\nThe storage engine provides several guarantees that ingestion flow depends upon:\n\n| Guarantee Type | Implementation Mechanism | Failure Behavior | Recovery Method |\n|----------------|-------------------------|------------------|-----------------|\n| Durability | WAL with fsync before acknowledgment | No acknowledged writes lost | Replay uncommitted WAL records |\n| Consistency | Atomic chunk creation with metadata | Partial chunks discarded | Rollback incomplete operations |\n| Isolation | Per-chunk locking during creation | Concurrent access blocked | Wait for completion or timeout |\n| Ordering | Timestamp-based chunk organization | Out-of-order entries detected | Reorder during recovery |\n\n⚠️ **Pitfall: WAL Coordination Deadlock**\nIncorrect coordination between ingestion buffer management and WAL operations can create deadlocks where ingestion waits for storage while storage waits for buffer space. Design buffer draining and WAL writing as independent processes that communicate through bounded queues rather than synchronous method calls.\n\n### Query Processing Flow\n\nQuery processing transforms LogQL query strings into efficient execution plans that coordinate index lookups, storage access, and result streaming. This flow must optimize for both interactive queries requiring low latency and analytical queries processing large data volumes.\n\n![Query Processing Sequence](./diagrams/query-flow.svg)\n\n#### Query Reception and Authentication\n\nQuery processing begins when clients submit requests to the `QueryEngine` through HTTP GET or POST endpoints. Each request undergoes authentication and authorization before query parsing begins, ensuring tenant isolation and preventing unauthorized data access.\n\nThe query reception process handles the following request formats:\n\n| Request Method | Content Format | Required Parameters | Optional Parameters | Use Case |\n|----------------|----------------|-------------------|-------------------|----------|\n| GET | URL query parameters | `query`, `time` or `start`/`end` | `limit`, `direction`, `format` | Simple exploratory queries |\n| POST | JSON request body | `query`, `time_range` | `limit`, `timeout`, `stream_results` | Complex analytical queries |\n| WebSocket | JSON messages | `query`, subscription parameters | `continuous`, `filter_updates` | Real-time log streaming |\n\nAuthentication leverages the tenant context established during ingestion to ensure queries only access logs belonging to the requesting tenant. The `AuthService` validates request tokens and populates `TenantContext` with appropriate access controls.\n\nQuery authorization involves multiple validation layers:\n\n1. **Token Validation**: Verify JWT signatures and expiration times using tenant-specific keys\n2. **Resource Quota Check**: Ensure query doesn't exceed tenant's maximum memory or CPU limits\n3. **Time Range Validation**: Confirm requested time window falls within tenant's data retention period\n4. **Rate Limit Enforcement**: Apply per-tenant query frequency limits to prevent resource exhaustion\n5. **Query Complexity Analysis**: Estimate resource requirements and reject queries exceeding tenant quotas\n\n> **Decision: Query Authentication Integration Point**\n> - **Context**: Queries must respect multi-tenant isolation while maintaining query performance\n> - **Options Considered**: Pre-authentication (fast but brittle), Per-operation checks (secure but slow), Context propagation (balanced)\n> - **Decision**: Authenticate once at query reception, propagate tenant context through execution pipeline\n> - **Rationale**: Provides security without repeated authentication overhead, enables tenant-aware optimizations\n> - **Consequences**: Requires careful context propagation design but enables efficient tenant-isolated execution\n\n#### Lexical Analysis and Parsing\n\nOnce authenticated, query strings undergo lexical analysis by the `Lexer` to break them into tokens, followed by parsing to construct an Abstract Syntax Tree (AST) representing the query structure. This parsing must handle LogQL syntax while providing clear error messages for malformed queries.\n\nThe lexical analysis process identifies these token types:\n\n| Token Type | Example | Description | Parsing Rules |\n|------------|---------|-------------|---------------|\n| `TokenLeftBrace` | `{` | Label selector start | Must be followed by label expressions |\n| `TokenRightBrace` | `}` | Label selector end | Must close matching left brace |\n| `TokenIdentifier` | `level`, `service` | Unquoted identifier | Alphanumeric plus underscore |\n| `TokenString` | `\"error message\"` | Quoted string literal | Supports escape sequences |\n| `TokenEqual` | `=` | Equality operator | Used in label selectors |\n| `TokenPipe` | `\\|` | Pipeline operator | Separates query stages |\n| `TokenRegex` | `\\~` | Regex match operator | Followed by regex pattern |\n| `TokenEOF` | End of input | Marks query completion | Triggers final parsing validation |\n\nThe parser constructs an AST that represents query structure as a tree of operations:\n\n1. **Root Node**: Contains the complete query with metadata like time range and limits\n2. **Stream Selector**: Represents label-based log stream selection criteria\n3. **Pipeline Stages**: Chain of operations applied to selected log streams\n4. **Filter Operations**: Line filters, regex matches, and JSON extractions\n5. **Aggregation Functions**: Count, rate, sum, and other metric computations\n\n⚠️ **Pitfall: Parser Error Recovery**\nNaive parsers abort on the first syntax error, providing poor user experience for complex queries with multiple issues. Implement error recovery that continues parsing after syntax errors to report multiple problems simultaneously. This requires careful parser state management to avoid cascading error reports.\n\n#### Query Planning and Optimization\n\nThe parsed AST undergoes query planning to generate an efficient execution strategy. This planning phase determines which indexes to consult, how to order operations, and whether to use streaming or batch processing for result delivery.\n\nQuery planning follows these optimization stages:\n\n1. **Predicate Analysis**: Identify filter conditions that can be pushed down to index lookups\n2. **Index Selection**: Choose optimal indexes based on label selectors and time ranges\n3. **Operation Ordering**: Arrange pipeline operations to minimize data processing volume\n4. **Resource Estimation**: Calculate memory and CPU requirements for execution plan validation\n5. **Execution Strategy**: Choose between streaming and batch processing based on result size estimates\n\nThe query planner applies several key optimizations:\n\n| Optimization Type | Technique | Benefit | Application Criteria |\n|-------------------|-----------|---------|---------------------|\n| Predicate Pushdown | Move filters before storage access | Reduces I/O volume | Filters compatible with index structure |\n| Index Intersection | Combine multiple index lookups | Narrows result set early | Multiple selective label filters |\n| Time Range Pruning | Skip irrelevant time partitions | Eliminates unnecessary storage access | Bounded time range queries |\n| Streaming Execution | Process results incrementally | Reduces memory usage | Large result sets or real-time queries |\n| Bloom Filter Utilization | Fast negative lookups | Avoids expensive storage reads | High-selectivity term filters |\n\n> The key insight for query optimization is that log data has strong temporal locality. Most queries focus on recent time windows, so optimizations that leverage time-based partitioning provide dramatic performance improvements for typical workloads.\n\n#### Index Consultation and Reference Resolution\n\nWith an optimized execution plan, the query engine consults relevant indexes to identify `EntryReference` instances pointing to log entries matching the query criteria. This consultation must efficiently combine multiple index lookups while handling bloom filter false positives.\n\nIndex consultation proceeds through these steps:\n\n1. **Partition Selection**: Use time range filters to identify relevant `IndexSegment` instances\n2. **Term Lookup**: Query inverted indexes for terms derived from label selectors and text filters\n3. **Posting List Intersection**: Combine postings lists from multiple terms using set intersection\n4. **Bloom Filter Validation**: Use bloom filters to eliminate segments unlikely to contain matching terms\n5. **Reference Collection**: Gather `EntryReference` instances pointing to potentially matching log entries\n6. **Reference Deduplication**: Remove duplicate references when multiple terms match the same entry\n\nThe index consultation process must handle several challenging scenarios:\n\n| Challenge | Cause | Detection Method | Handling Strategy |\n|-----------|-------|------------------|-------------------|\n| Bloom Filter False Positives | Probabilistic data structure limitations | Storage lookup returns no matches | Continue with remaining references |\n| High Cardinality Explosion | Too many unique label values | Posting list size exceeds thresholds | Apply additional filters early |\n| Time Range Misalignment | Clock skew between ingestion sources | References outside query time bounds | Filter references during collection |\n| Index Segment Unavailability | Concurrent compaction operations | Segment lock acquisition failure | Retry with exponential backoff |\n| Memory Pressure | Large posting list intersections | Process memory usage monitoring | Stream processing instead of batch loading |\n\n⚠️ **Pitfall: Reference Resolution Memory Explosion**\nQueries matching millions of log entries can generate reference lists that exceed available memory. Instead of loading all references into memory simultaneously, implement streaming reference resolution that processes references in batches while maintaining query correctness.\n\n#### Storage Access and Content Filtering\n\nArmed with `EntryReference` instances from index consultation, the query engine accesses the `StorageEngine` to retrieve actual log entry content. This storage access must efficiently handle compressed data while applying line filters that couldn't be resolved through index lookups alone.\n\nStorage access coordination involves:\n\n1. **Chunk Loading**: Group references by `ChunkID` to minimize storage I/O operations\n2. **Decompression**: Decompress chunk data using the appropriate compression algorithm\n3. **Entry Extraction**: Locate specific log entries within decompressed chunks using offset information\n4. **Content Filtering**: Apply regex patterns, JSON extractions, and other content-based filters\n5. **Result Formatting**: Convert matching entries into the requested output format\n6. **Memory Management**: Stream results to avoid accumulating large result sets in memory\n\nThe storage access layer provides several optimizations for query performance:\n\n| Optimization | Implementation | Performance Benefit | Memory Impact |\n|--------------|---------------|-------------------|---------------|\n| Chunk Caching | LRU cache of decompressed chunks | Eliminates repeated decompression | High - caches full chunk content |\n| Partial Chunk Reading | Read only required entry ranges | Reduces I/O for selective queries | Low - processes entries incrementally |\n| Compression Awareness | Skip decompression for filtered chunks | Saves CPU cycles | None - avoids unnecessary processing |\n| Concurrent Access | Parallel chunk processing | Improves query throughput | Moderate - temporary decompression buffers |\n| Reference Batching | Group references by storage locality | Optimizes disk access patterns | Low - small reference metadata only |\n\n> **Decision: Content Filter Application Point**\n> - **Context**: Some filters require full log content that isn't indexed, creating tension between I/O efficiency and filter accuracy\n> - **Options Considered**: Filter during indexing (fast but inflexible), Filter during storage access (accurate but slow), Hybrid approach (complex)\n> - **Decision**: Apply content-based filters during storage access with aggressive caching\n> - **Rationale**: Maintains filter accuracy while leveraging temporal locality of log queries for cache effectiveness\n> - **Consequences**: Higher memory usage but much better query performance for typical access patterns\n\n#### Result Assembly and Streaming\n\nThe final stage of query processing assembles matching log entries into response formats suitable for client consumption. This assembly must handle result pagination, streaming delivery, and format conversion while maintaining consistent ordering guarantees.\n\nResult assembly follows this pipeline:\n\n1. **Entry Collection**: Gather `LogEntry` instances from storage access operations\n2. **Ordering Enforcement**: Sort entries by timestamp to ensure consistent query results\n3. **Pagination Handling**: Apply limit and offset parameters while maintaining cursor positions for subsequent requests\n4. **Format Conversion**: Convert entries to requested output format (JSON, text, CSV, etc.)\n5. **Streaming Delivery**: Send results incrementally to reduce client wait times and memory usage\n6. **Metadata Calculation**: Compute query statistics including execution time, scanned entries, and result counts\n\nThe `ResultStream` provides a consistent interface for result delivery regardless of underlying processing strategy:\n\n| Stream Method | Purpose | Implementation | Error Handling |\n|---------------|---------|----------------|----------------|\n| `Next()` | Retrieve next result entry | Coordinates with storage access pipeline | Returns error for processing failures |\n| `HasMore()` | Check for additional results | Examines cursor position and remaining references | Always returns boolean |\n| `Close()` | Release stream resources | Cleans up storage handles and memory buffers | Logs cleanup failures but doesn't propagate |\n| `Metadata()` | Get query execution statistics | Aggregates metrics from all processing stages | Returns partial data on calculation errors |\n\nResult streaming handles several challenging requirements:\n\n- **Ordering Consistency**: Results must appear in timestamp order even when processed concurrently\n- **Memory Bounds**: Large result sets cannot be accumulated in memory before delivery\n- **Error Propagation**: Processing errors must be communicated without corrupting result streams\n- **Cancellation Support**: Clients must be able to abort expensive queries to reclaim resources\n- **Progress Indication**: Long-running queries should provide progress updates to prevent client timeouts\n\n### Background Maintenance Flows\n\nBackground maintenance operations ensure system health and performance through index compaction, retention cleanup, and other housekeeping tasks. These flows must operate continuously without impacting ingestion or query performance.\n\n#### Index Compaction Operations\n\nIndex compaction merges small `IndexSegment` instances into larger, more efficient segments that reduce query overhead and improve storage utilization. This compaction process must coordinate carefully with ongoing queries to avoid introducing consistency issues.\n\nThe compaction process operates through these phases:\n\n1. **Compaction Candidate Selection**: Identify segments eligible for merging based on size, age, and access patterns\n2. **Resource Availability Check**: Ensure sufficient CPU and memory resources for compaction without impacting foreground operations\n3. **Segment Locking**: Acquire exclusive locks on source segments to prevent modification during compaction\n4. **Merge Operation**: Combine posting lists and bloom filters from multiple source segments into a new consolidated segment\n5. **Atomic Replacement**: Replace references to source segments with references to the new merged segment\n6. **Cleanup**: Delete source segments after confirming all queries have switched to the new segment\n\nCompaction scheduling uses several criteria to balance maintenance benefits with system impact:\n\n| Compaction Trigger | Threshold | Benefit | Resource Cost |\n|-------------------|-----------|---------|---------------|\n| Segment Count | More than 20 segments per hour | Reduces query fanout overhead | High I/O during merge |\n| Average Segment Size | Segments smaller than 10MB | Improves storage efficiency | CPU intensive posting list merging |\n| Query Performance | Response times exceed SLA | Directly addresses performance problems | May temporarily slow queries further |\n| Storage Pressure | Available disk space below 20% | Prevents storage exhaustion | Requires significant temporary space |\n| Scheduled Maintenance | Daily during low-traffic hours | Predictable maintenance windows | Competes with other maintenance tasks |\n\n> **Decision: Compaction Concurrency Strategy**\n> - **Context**: Index compaction requires significant resources but cannot halt query processing\n> - **Options Considered**: Stop-the-world compaction (simple but disruptive), Concurrent compaction (complex), Incremental compaction (balanced)\n> - **Decision**: Implement concurrent compaction with copy-on-write semantics for active segments\n> - **Rationale**: Maintains query availability while allowing essential maintenance operations\n> - **Consequences**: Increased implementation complexity and temporary storage overhead during compaction\n\n⚠️ **Pitfall: Compaction Resource Starvation**\nBackground compaction can consume excessive CPU and I/O resources, degrading performance for ingestion and queries. Implement adaptive resource throttling that monitors system load and reduces compaction intensity when foreground operations show signs of stress.\n\n#### Retention Policy Enforcement\n\nRetention policy enforcement automatically removes expired log data according to configurable rules, preventing unbounded storage growth while maintaining compliance requirements. This enforcement must coordinate with active queries to prevent data deletion while logs are being accessed.\n\nRetention enforcement operates through these coordinated processes:\n\n1. **Policy Evaluation**: Scan all chunks to identify those exceeding configured retention limits\n2. **Query Coordination**: Verify no active queries are accessing chunks marked for deletion\n3. **Reference Counting**: Ensure all index segments referencing expired chunks are also eligible for cleanup\n4. **Graceful Deletion**: Remove chunks and associated index entries while maintaining referential integrity\n5. **Cleanup Verification**: Confirm successful deletion and update storage metrics\n6. **Audit Logging**: Record all retention actions for compliance and debugging purposes\n\nThe retention policy engine supports multiple policy types that can be applied simultaneously:\n\n| Policy Type | Configuration Parameters | Enforcement Method | Interaction with Other Policies |\n|-------------|-------------------------|-------------------|--------------------------------|\n| Time-Based | `MaxAge time.Duration` | Compare chunk creation time with current time | Evaluated first, other policies respect time bounds |\n| Size-Based | `MaxSize int64` | Calculate cumulative storage size per stream | Applied after time policy to prevent size violations |\n| Count-Based | `MaxEntries int64` | Track entry counts per stream or tenant | Least frequently used, typically for debugging streams |\n| Compliance-Based | `GracePeriod time.Duration` | Legal hold overrides with explicit release dates | Takes precedence over all other policies |\n\nRetention cleanup must handle several complex coordination scenarios:\n\n- **Active Query Protection**: Never delete data that might be accessed by running queries\n- **Index Consistency**: Ensure index segments don't contain references to deleted chunks\n- **Tenant Isolation**: Apply retention policies independently per tenant\n- **Backup Coordination**: Delay deletion until backup systems confirm successful archival\n- **Audit Trail Maintenance**: Preserve deletion records for compliance reporting\n\n⚠️ **Pitfall: Retention Race Conditions**\nConcurrent retention enforcement and query processing can create race conditions where queries attempt to access recently deleted chunks. Implement reference counting with grace periods that delay physical deletion until all references are released.\n\n#### Write-Ahead Log Maintenance\n\nWAL maintenance ensures the write-ahead log doesn't grow unboundedly while preserving the ability to recover from system failures. This maintenance includes record cleanup, file rotation, and checkpoint creation.\n\nWAL maintenance operates through these continuous processes:\n\n1. **Checkpoint Creation**: Periodically create snapshots of committed state to bound recovery time\n2. **Record Cleanup**: Remove WAL records that precede the most recent checkpoint\n3. **File Rotation**: Create new WAL files when current files exceed size limits\n4. **Integrity Verification**: Regularly verify WAL file integrity using checksums and structural validation\n5. **Recovery Testing**: Periodically test WAL replay capabilities to ensure recovery procedures work correctly\n\nThe WAL maintenance system balances several competing requirements:\n\n| Requirement | Implementation Strategy | Trade-offs | Monitoring Metrics |\n|-------------|------------------------|------------|-------------------|\n| Bounded Recovery Time | Create checkpoints every 10 minutes | More frequent I/O vs. faster recovery | Recovery time during testing |\n| Storage Efficiency | Clean up committed records aggressively | Risk of data loss vs. disk usage | WAL file size growth rate |\n| Write Performance | Buffer WAL writes and batch fsync | Durability guarantees vs. throughput | Write latency percentiles |\n| Integrity Assurance | Checksum every WAL record | CPU overhead vs. corruption detection | Checksum verification failures |\n\n> The critical insight for WAL maintenance is that it directly impacts both system reliability and performance. Too aggressive cleanup risks data loss during failures, while too conservative cleanup degrades performance and wastes storage.\n\nThe WAL maintenance processes coordinate with other system components through well-defined interfaces:\n\n- **Storage Engine Coordination**: Checkpoint creation requires consistent snapshots of chunk metadata\n- **Query Engine Integration**: Active query tracking prevents cleanup of required recovery data\n- **Ingestion Flow Synchronization**: WAL rotation must not interrupt ongoing write operations\n- **Monitoring Integration**: WAL health metrics feed into overall system health dashboards\n\n### Implementation Guidance\n\nThis section provides practical implementation details for building the interaction flows described above.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| HTTP Server | `net/http` with `http.ServeMux` | `gorilla/mux` or `chi` router |\n| Protocol Parsing | Manual parsing with `bufio.Scanner` | `gopkg.in/mcuadros/go-syslog.v2` |\n| Concurrency | `sync.WaitGroup` with goroutines | `golang.org/x/sync/errgroup` |\n| JSON Processing | `encoding/json` standard library | `github.com/json-iterator/go` for performance |\n| Context Propagation | `context.Context` throughout call chain | Custom context with tracing integration |\n| Background Tasks | Simple `time.Ticker` loops | `github.com/robfig/cron/v3` for scheduling |\n\n#### Recommended File Structure\n\n```\ninternal/\n  flows/\n    ingestion.go          ← ingestion coordination\n    query.go              ← query processing coordination  \n    maintenance.go        ← background task coordination\n    ingestion_test.go     ← ingestion flow tests\n    query_test.go         ← query flow tests\n  coordinator/\n    coordinator.go        ← cross-component orchestration\n    flow_controller.go    ← backpressure and flow control\n  protocols/\n    http_server.go        ← HTTP ingestion endpoint\n    syslog_handler.go     ← TCP/UDP syslog handling\n    file_tailer.go        ← file watching and tailing\n  auth/\n    tenant_context.go     ← tenant authentication and context\n    rate_limiter.go       ← rate limiting implementation\n```\n\n#### Infrastructure Starter Code\n\nComplete HTTP server implementation for log ingestion:\n\n```go\npackage protocols\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n    \"context\"\n    \"github.com/your-org/log-aggregator/internal/types\"\n)\n\ntype HTTPServer struct {\n    config     *types.Config\n    parser     types.LogParser\n    buffer     types.LogBuffer\n    metrics    *types.Metrics\n    server     *http.Server\n    limiter    types.RateLimiter\n}\n\nfunc NewHTTPServer(config *types.Config, parser types.LogParser, \n                   buffer types.LogBuffer, metrics *types.Metrics) *HTTPServer {\n    return &HTTPServer{\n        config:  config,\n        parser:  parser,\n        buffer:  buffer,\n        metrics: metrics,\n        limiter: NewTokenBucket(1000, time.Second), // 1000 req/sec\n    }\n}\n\nfunc (s *HTTPServer) Start() error {\n    mux := http.NewServeMux()\n    mux.HandleFunc(\"/api/v1/push\", s.handleLogIngestion)\n    mux.HandleFunc(\"/health\", s.handleHealthCheck)\n    \n    s.server = &http.Server{\n        Addr:         fmt.Sprintf(\":%d\", s.config.HTTPPort),\n        Handler:      mux,\n        ReadTimeout:  30 * time.Second,\n        WriteTimeout: 30 * time.Second,\n    }\n    \n    return s.server.ListenAndServe()\n}\n\nfunc (s *HTTPServer) Stop() error {\n    ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)\n    defer cancel()\n    return s.server.Shutdown(ctx)\n}\n\ntype LogPushRequest struct {\n    Streams []struct {\n        Stream  map[string]string `json:\"stream\"`\n        Values  [][]string       `json:\"values\"`\n    } `json:\"streams\"`\n}\n\nfunc (s *HTTPServer) handleLogIngestion(w http.ResponseWriter, r *http.Request) {\n    // Rate limiting check\n    if !s.limiter.TryConsume(1) {\n        http.Error(w, \"Rate limit exceeded\", http.StatusTooManyRequests)\n        return\n    }\n\n    // Parse tenant context from request headers\n    tenantID := r.Header.Get(\"X-Org-ID\")\n    if tenantID == \"\" {\n        http.Error(w, \"Missing tenant ID\", http.StatusBadRequest)\n        return\n    }\n\n    var req LogPushRequest\n    if err := json.NewDecoder(r.Body).Decode(&req); err != nil {\n        http.Error(w, \"Invalid JSON\", http.StatusBadRequest)\n        return\n    }\n\n    entriesIngested := 0\n    for _, stream := range req.Streams {\n        for _, values := range stream.Values {\n            if len(values) < 2 {\n                continue // Skip malformed entries\n            }\n            \n            // Parse timestamp and message\n            timestamp, err := time.Parse(time.RFC3339Nano, values[0])\n            if err != nil {\n                timestamp = time.Now()\n            }\n            \n            // Create log entry with tenant isolation\n            labels := make(types.Labels)\n            for k, v := range stream.Stream {\n                labels[k] = v\n            }\n            labels[\"tenant_id\"] = tenantID\n            \n            entry, err := types.NewLogEntry(timestamp, labels, values[1])\n            if err != nil {\n                continue // Skip invalid entries\n            }\n            \n            // Buffer the entry\n            if err := s.buffer.Write(entry); err != nil {\n                http.Error(w, \"Buffer full\", http.StatusServiceUnavailable)\n                return\n            }\n            \n            entriesIngested++\n        }\n    }\n    \n    s.metrics.IncrementLogsIngested(int64(entriesIngested))\n    w.WriteHeader(http.StatusNoContent)\n}\n\nfunc (s *HTTPServer) handleHealthCheck(w http.ResponseWriter, r *http.Request) {\n    w.Header().Set(\"Content-Type\", \"application/json\")\n    json.NewEncoder(w).Encode(map[string]interface{}{\n        \"status\": \"healthy\",\n        \"buffer_utilization\": s.buffer.Utilization(),\n        \"timestamp\": time.Now().Unix(),\n    })\n}\n```\n\nComplete query coordinator implementation:\n\n```go\npackage flows\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"time\"\n    \"github.com/your-org/log-aggregator/internal/types\"\n)\n\ntype QueryCoordinator struct {\n    indexEngine  types.IndexEngine\n    storageEngine types.StorageEngine\n    authService  *types.AuthService\n    metrics      *types.Metrics\n}\n\nfunc NewQueryCoordinator(indexEngine types.IndexEngine, \n                        storageEngine types.StorageEngine,\n                        authService *types.AuthService,\n                        metrics *types.Metrics) *QueryCoordinator {\n    return &QueryCoordinator{\n        indexEngine:   indexEngine,\n        storageEngine: storageEngine,\n        authService:   authService,\n        metrics:       metrics,\n    }\n}\n\nfunc (qc *QueryCoordinator) ExecuteQuery(ctx context.Context, \n                                        queryString string,\n                                        params *types.QueryParams) (*types.ResultStream, error) {\n    startTime := time.Now()\n    defer func() {\n        qc.metrics.RecordQueryDuration(time.Since(startTime))\n    }()\n    \n    // TODO 1: Parse and validate the query string using Lexer and Parser\n    // TODO 2: Extract tenant context from the request context\n    // TODO 3: Validate tenant permissions for the requested time range and labels\n    // TODO 4: Generate optimized execution plan with predicate pushdown\n    // TODO 5: Execute index lookups to get EntryReference instances\n    // TODO 6: Coordinate storage access to retrieve log content\n    // TODO 7: Apply content-based filters and format results\n    // TODO 8: Return ResultStream with proper pagination and metadata\n    \n    // Hint: Use context.WithTimeout to enforce query timeouts\n    // Hint: Check tenant quotas before starting expensive operations\n    // Hint: Stream results instead of loading everything into memory\n    \n    return nil, fmt.Errorf(\"query execution not implemented\")\n}\n```\n\n#### Core Logic Skeleton Code\n\n```go\npackage flows\n\n// IngestionCoordinator manages the complete ingestion pipeline from \n// protocol reception through storage persistence\ntype IngestionCoordinator struct {\n    protocols    []types.ProtocolHandler\n    parser       types.LogParser  \n    buffer       types.LogBuffer\n    indexEngine  types.IndexEngine\n    storageEngine types.StorageEngine\n    metrics      *types.Metrics\n    stopChan     chan struct{}\n}\n\nfunc (ic *IngestionCoordinator) StartIngestion() error {\n    // TODO 1: Start all protocol handlers (HTTP, TCP, UDP, file tail)\n    // TODO 2: Launch buffer processing goroutines\n    // TODO 3: Initialize index coordination\n    // TODO 4: Setup storage coordination\n    // TODO 5: Begin background maintenance tasks\n    // Hint: Use sync.WaitGroup to coordinate goroutine startup\n    // Hint: Implement graceful shutdown with context cancellation\n}\n\nfunc (ic *IngestionCoordinator) processBufferEntries() {\n    // TODO 1: Continuously read entries from buffer\n    // TODO 2: Batch entries for efficient processing\n    // TODO 3: Extract terms for index updates\n    // TODO 4: Coordinate with storage engine for persistence\n    // TODO 5: Handle backpressure when downstream is slow\n    // Hint: Process entries in time-aligned batches for better compression\n    // Hint: Use channel buffering to handle temporary slowdowns\n}\n\nfunc (qc *QueryCoordinator) executeIndexLookup(ctx context.Context,\n                                             selectors types.LabelSelectors,\n                                             timeRange *types.TimeRange) ([]types.EntryReference, error) {\n    // TODO 1: Identify relevant index segments based on time range\n    // TODO 2: Extract terms from label selectors for index queries\n    // TODO 3: Perform bloom filter checks to eliminate segments\n    // TODO 4: Execute inverted index lookups for matching terms\n    // TODO 5: Intersect posting lists from multiple terms\n    // TODO 6: Return deduplicated EntryReference instances\n    // Hint: Use bloom filters first to avoid expensive storage lookups\n    // Hint: Process segments in parallel but limit concurrency\n}\n\n// MaintenanceCoordinator handles all background maintenance operations\ntype MaintenanceCoordinator struct {\n    indexEngine   types.IndexEngine\n    storageEngine types.StorageEngine\n    queryTracker  types.ActiveQueryTracker\n    scheduler     *MaintenanceScheduler\n}\n\nfunc (mc *MaintenanceCoordinator) runIndexCompaction() {\n    // TODO 1: Identify segments eligible for compaction\n    // TODO 2: Check resource availability for compaction work\n    // TODO 3: Acquire locks on source segments\n    // TODO 4: Merge posting lists and bloom filters\n    // TODO 5: Atomically replace old segments with new merged segment\n    // TODO 6: Clean up source segments after confirming no active queries\n    // Hint: Use copy-on-write to allow concurrent queries during compaction\n    // Hint: Monitor system resources and throttle compaction if needed\n}\n\nfunc (mc *MaintenanceCoordinator) enforceRetentionPolicies() {\n    // TODO 1: Evaluate all retention policies against current chunks\n    // TODO 2: Identify chunks exceeding retention limits\n    // TODO 3: Check for active queries accessing chunks marked for deletion\n    // TODO 4: Remove chunks and update index segments\n    // TODO 5: Verify cleanup completion and update metrics\n    // TODO 6: Record retention actions for audit trail\n    // Hint: Use reference counting to prevent deletion of active chunks\n    // Hint: Apply grace periods before actual deletion for safety\n}\n```\n\n#### Language-Specific Hints\n\n**Goroutine Management**: Use `sync.WaitGroup` and `context.Context` for coordinating concurrent operations. Always provide cancellation mechanisms for long-running background tasks.\n\n**Memory Management**: Be careful with slice growth in hot paths. Pre-allocate slices when the size is known, and consider using sync.Pool for frequently allocated objects.\n\n**Error Handling**: Wrap errors with context using `fmt.Errorf(\"operation failed: %w\", err)`. This preserves error chains for debugging while adding operation context.\n\n**Channel Usage**: Prefer buffered channels for producer-consumer patterns, but size buffers appropriately. Unbuffered channels are better for synchronization points.\n\n**HTTP Timeouts**: Always set read/write timeouts on HTTP servers. Use `context.WithTimeout` for outbound requests to prevent hanging operations.\n\n#### Milestone Checkpoints\n\n**After Milestone 1 (Log Ingestion)**: Run `curl -X POST localhost:8080/api/v1/push -H \"X-Org-ID: test-tenant\" -d '{\"streams\":[{\"stream\":{\"service\":\"test\"},\"values\":[[\"2023-01-01T12:00:00Z\",\"test message\"]]}]}'`. Verify logs appear in storage and can be found through basic queries.\n\n**After Milestone 3 (Query Engine)**: Execute `curl \"localhost:8080/api/v1/query?query={service=\\\"test\\\"}&start=2023-01-01T11:00:00Z&end=2023-01-01T13:00:00Z\"`. Confirm query returns ingested test logs in proper JSON format.\n\n**After Milestone 5 (Multi-tenancy)**: Test tenant isolation by ingesting logs with different `X-Org-ID` headers and confirming queries only return logs for the requesting tenant.\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Ingestion HTTP 503 errors | Buffer overflow from processing backlog | Check buffer utilization metrics | Increase buffer size or add more processing goroutines |\n| Queries return partial results | Index segments being compacted during query | Enable query/compaction coordination logging | Implement proper segment locking during compaction |\n| Memory usage grows unbounded | ResultStream not being closed properly | Monitor goroutine counts and heap profiles | Add proper resource cleanup in defer blocks |\n| Query performance degrades over time | Index segments not being compacted | Monitor segment count per time partition | Tune compaction triggers to run more frequently |\n| WAL files growing too large | Checkpoint creation failing | Check WAL maintenance task logs | Fix checkpoint creation bugs and add monitoring |\n\n\n## Error Handling and Edge Cases\n\n> **Milestone(s):** This section applies to all milestones (1-5), providing comprehensive failure mode analysis and recovery strategies that ensure system reliability across log ingestion, indexing, querying, storage, and multi-tenant operations.\n\n### Mental Model: The Hospital Emergency Response System\n\nThink of error handling in our log aggregation system like a hospital's emergency response protocols. Just as a hospital has different response procedures for different types of emergencies (heart attack vs. broken bone vs. power outage), our system needs different recovery strategies for different failure modes. The hospital has early warning systems (patient monitors), escalation procedures (calling specialists), and graceful degradation plans (backup generators). Similarly, our log aggregation system needs comprehensive monitoring, automated recovery mechanisms, and fallback modes that keep the system operational even when components fail.\n\nThe key insight is that in both systems, the goal isn't to prevent all failures (impossible), but to detect them quickly, respond appropriately, and maintain critical functions. A hospital doesn't shut down when one monitor fails - it switches to backup equipment and continues treating patients. Our log aggregation system should exhibit the same resilience, continuing to ingest new logs and serve queries even when individual components experience problems.\n\n## System Failure Modes\n\n### Network Partition and Connectivity Failures\n\nNetwork partitions represent one of the most challenging failure scenarios in distributed log aggregation systems. When network connectivity degrades or fails completely, different components can become isolated from each other, leading to split-brain scenarios and data consistency issues.\n\n**Client-to-Ingestion Partition**: When clients cannot reach the ingestion endpoints, they may attempt to buffer logs locally or drop them entirely. The system must distinguish between permanent client failures (where buffering is wasteful) and temporary network issues (where aggressive retry is appropriate). During network splits, the ingestion layer continues operating but cannot communicate with downstream components like the index or storage engines.\n\n**Ingestion-to-Storage Partition**: Perhaps the most critical partition scenario occurs when the ingestion layer can receive logs but cannot persist them to storage. In this case, the `MemoryBuffer` becomes the last line of defense, but it has finite capacity. The system must implement intelligent backpressure mechanisms that signal upstream clients to slow their ingestion rate while attempting to restore connectivity to storage systems.\n\n**Index-to-Storage Partition**: When the indexing engine cannot access stored chunks, queries fail even though ingestion may continue normally. This creates a scenario where new logs arrive and get indexed, but historical queries return incomplete results. The system must track which time ranges have complete vs. partial index coverage to provide accurate query result metadata.\n\n| Partition Type | Immediate Impact | Data Loss Risk | Recovery Complexity |\n|----------------|------------------|----------------|-------------------|\n| Client-to-Ingestion | New logs queued at client | High if client buffers overflow | Low - resume ingestion when connected |\n| Ingestion-to-Storage | Logs buffered in memory | High if memory buffers full | Medium - replay buffered logs |\n| Index-to-Storage | Query degradation | None - data persisted | High - rebuild index state |\n| Storage-to-Query | Historical queries fail | None | Medium - wait for connectivity |\n\n**Cross-Component Communication Failures**: The system uses various protocols for internal communication. HTTP connections may timeout, TCP connections may reset, and file system operations may hang during storage issues. Each communication channel requires specific timeout configurations, retry policies, and circuit breaker implementations to prevent cascading failures.\n\n> **Critical Design Insight**: Network partitions are not just connectivity failures - they create temporal inconsistencies where different components have different views of system state. Recovery must reconcile these divergent states carefully.\n\n### Disk and Storage Failures\n\nStorage failures manifest in multiple ways, each requiring distinct detection and recovery strategies. Understanding the failure modes of underlying storage systems is crucial for building robust error handling.\n\n**Disk Full Conditions**: When storage volumes approach capacity, the system must gracefully degrade rather than crash. The `StorageEngine` should monitor available space continuously and implement storage pressure relief mechanisms. This includes accelerating retention policy execution, compressing older chunks more aggressively, and potentially rejecting new ingestion requests with explicit backpressure signals.\n\n**Write-Ahead Log Corruption**: The WAL provides durability guarantees, but the WAL files themselves can become corrupted due to hardware failures, incomplete writes during system crashes, or file system issues. WAL record corruption detection relies on checksums embedded in `WALRecord` structures. When corruption is detected, the system must determine how much of the WAL remains valid and whether recent ingestion needs to be replayed from upstream sources.\n\n**Chunk File Corruption**: Individual chunk files may become corrupted or unreadable. Since chunks contain compressed log data organized by time windows, chunk corruption affects queries for specific time ranges. The system should maintain chunk integrity checksums in `ChunkHeader` structures and implement graceful degradation where corrupted chunks are marked as unavailable rather than causing query failures.\n\n**Index File Corruption**: Inverted index corruption is particularly problematic because it affects query performance across multiple time ranges. When `IndexSegment` files become corrupted, the system can fall back to sequential scanning of chunks, but this severely impacts query performance. Index rebuilding from stored chunks provides recovery but requires significant computational resources.\n\n| Storage Failure Type | Detection Method | Recovery Strategy | Performance Impact |\n|---------------------|------------------|-------------------|-------------------|\n| Disk Full | Space monitoring alerts | Accelerate retention, reject ingestion | High - ingestion blocked |\n| WAL Corruption | Checksum validation | Truncate at corruption point, replay | Medium - recent data loss |\n| Chunk Corruption | Read verification failures | Mark chunk unavailable, query degradation | Low - specific time ranges |\n| Index Corruption | Lookup failures, checksum errors | Rebuild from chunks, sequential fallback | High - query performance |\n\n**Storage Backend Failures**: When using cloud storage (S3-compatible systems), the failure modes include authentication failures, rate limiting, temporary unavailability, and permanent account suspension. The system must implement exponential backoff retry policies and maintain local caching to survive temporary cloud storage outages.\n\n### Memory Exhaustion and Resource Limits\n\nMemory pressure represents a gradual failure mode that can lead to sudden system crashes if not handled proactively. Different components have distinct memory usage patterns that require specialized monitoring and mitigation strategies.\n\n**Ingestion Buffer Overflow**: The `MemoryBuffer` used for log ingestion has a fixed capacity defined by `BUFFER_SIZE`. When ingestion rates exceed processing capacity, the buffer fills up. The system must implement sophisticated buffer management that preserves high-priority logs (critical severity levels) while dropping less important entries. Buffer overflow also triggers backpressure signals to upstream clients, requesting them to reduce their ingestion rates.\n\n**Query Result Set Explosion**: Large query result sets can exhaust available memory, particularly when clients request broad time ranges with minimal filtering. The `QueryEngine` must implement streaming execution with configurable memory limits per query. When memory thresholds are approached, queries should switch to more aggressive pagination or terminate with partial results rather than crash the system.\n\n**Index Memory Pressure**: In-memory index structures like bloom filters and term dictionaries can grow beyond available memory, particularly in high-cardinality logging environments. The indexing engine should implement memory-aware index segment sizing and proactive compaction to control memory usage. When memory pressure is detected, the system can temporarily disable non-essential indexing features like bloom filter updates.\n\n**Multi-Tenant Memory Isolation**: In multi-tenant deployments, individual tenants can consume excessive memory through large queries or high ingestion rates. The system must implement per-tenant memory quotas and enforce them across all components. Memory quota violations trigger tenant-specific backpressure rather than affecting other tenants.\n\n| Memory Pressure Source | Warning Indicators | Mitigation Strategy | Fallback Behavior |\n|----------------------|-------------------|-------------------|------------------|\n| Ingestion Buffers | Buffer fill percentage > 80% | Increase processing threads, backpressure | Drop low-priority logs |\n| Query Results | Query memory usage > limit | Switch to streaming, aggressive pagination | Return partial results |\n| Index Structures | Index memory growth rate | Trigger compaction, bloom filter cleanup | Disable bloom filters |\n| Tenant Isolation | Per-tenant memory quota exceeded | Apply tenant-specific limits | Reject tenant requests |\n\n**Garbage Collection Pressure**: In garbage-collected languages like Go, memory pressure can manifest as excessive GC overhead that impacts system throughput. The system should monitor GC metrics and implement object pooling for frequently allocated types like `LogEntry` and query result structures to reduce allocation pressure.\n\n### Process and Component Crashes\n\nComponent crashes represent the most severe failure mode but are also the most predictable to handle with proper design. Each component must be designed with crash-safe state management and rapid recovery capabilities.\n\n**Ingestion Process Crashes**: When ingestion processes crash, buffered logs in memory are lost unless they've been persisted to WAL. The ingestion restart procedure must replay uncommitted WAL records and re-establish connections with upstream clients. Client connection state is typically lost during crashes, so clients must implement retry logic with exponential backoff to reconnect after ingestion recovery.\n\n**Indexing Process Crashes**: Index building is a CPU and memory intensive process that's particularly susceptible to crashes during high load. Partially constructed index segments must be discarded and rebuilt from scratch, as incomplete indexes can return inconsistent query results. The indexing engine should implement checkpointing for long-running index operations to minimize work lost during crashes.\n\n**Query Engine Crashes**: Query crashes typically occur during execution of complex queries or when processing corrupted data. Since queries are stateless operations, crashes don't cause data loss but do impact user experience. The query engine should implement per-query isolation using separate goroutines or processes to prevent one failing query from crashing the entire query service.\n\n**Storage Engine Crashes**: Storage crashes are the most critical because they can lead to data loss if the WAL isn't properly recovered. The storage restart sequence must validate WAL integrity, replay uncommitted operations, and verify chunk consistency before accepting new write requests. During recovery, the system should reject new ingestion to prevent data interleaving with recovery operations.\n\n| Component Crash | Recovery Time | Data Loss Risk | Restart Dependencies |\n|------------------|---------------|----------------|-------------------|\n| Ingestion Process | < 30 seconds | Recent buffer contents | WAL availability, client reconnection |\n| Indexing Process | 1-5 minutes | None - rebuilds from chunks | Chunk accessibility |\n| Query Engine | < 10 seconds | None - stateless operations | Index and storage availability |\n| Storage Engine | 30 seconds - 10 minutes | WAL replay required | File system, WAL integrity |\n\n### Downstream Service Dependencies\n\nThe log aggregation system depends on various external services that can fail independently, requiring graceful degradation strategies that maintain core functionality even when dependencies are unavailable.\n\n**Authentication Service Failures**: When external authentication systems become unavailable, the system must decide between rejecting all requests (secure but unavailable) or allowing degraded access (available but potentially insecure). A hybrid approach uses cached authentication tokens with extended validity during auth service outages, combined with audit logging of all access during degraded mode.\n\n**Notification Service Failures**: Alert notifications depend on external services like email servers, Slack webhooks, or SMS gateways. Notification failures shouldn't impact core log processing, but the alerting system must implement retry queues and alternative notification channels. When primary notification channels fail, the system should escalate to backup channels and log notification failures for later analysis.\n\n**Time Synchronization Services**: Log aggregation systems depend heavily on accurate timestamps. When NTP services become unavailable, system clocks can drift, leading to timestamp inconsistencies that complicate querying and retention policies. The system should monitor clock drift and implement timestamp validation that detects and corrects minor discrepancies while flagging major timestamp anomalies.\n\n**Service Discovery Dependencies**: In containerized deployments, the system may depend on service discovery mechanisms like Consul or Kubernetes DNS. Service discovery failures prevent components from locating each other, effectively creating network partition scenarios. Components should cache service locations and implement fallback discovery mechanisms using configuration files or environment variables.\n\n## Failure Detection and Monitoring\n\n### Health Check Implementation\n\nEffective failure detection requires comprehensive health checks that monitor both individual component status and cross-component integration points. Health checks must be fast, reliable, and provide actionable diagnostic information.\n\n**Component-Level Health Checks**: Each component implements standardized health check endpoints that report detailed status information. The ingestion engine health check verifies buffer availability, upstream connectivity, and downstream persistence capability. The storage engine health check validates WAL integrity, disk space availability, and chunk accessibility. These health checks should complete within strict time limits (typically 1-2 seconds) to enable rapid failure detection.\n\n**Cross-Component Integration Checks**: Beyond individual component health, the system needs integration checks that verify end-to-end functionality. An integration health check might ingest a synthetic log entry, verify it gets indexed correctly, execute a query to retrieve it, and measure the round-trip time. These checks detect subtle integration failures that component-level checks might miss.\n\n**External Dependency Checks**: Health checks must monitor external dependencies like authentication services, notification endpoints, and storage backends. These checks should be implemented with appropriate timeouts and circuit breaker patterns to prevent dependency failures from cascading to the health check system itself.\n\n| Health Check Type | Check Frequency | Timeout | Failure Threshold | Recovery Action |\n|-------------------|----------------|---------|-------------------|-----------------|\n| Component Status | Every 10 seconds | 2 seconds | 3 consecutive failures | Restart component |\n| Integration End-to-End | Every 60 seconds | 10 seconds | 2 consecutive failures | Alert operations team |\n| External Dependencies | Every 30 seconds | 5 seconds | 5 consecutive failures | Enable degraded mode |\n| Resource Utilization | Every 5 seconds | 1 second | Sustained high usage | Trigger scaling/throttling |\n\n**Synthetic Transaction Monitoring**: The system should continuously execute synthetic transactions that represent real user workflows. A synthetic transaction might simulate client log ingestion, wait for indexing to complete, execute representative queries, and verify correct results. This provides early warning of performance degradation or functional failures before they impact real users.\n\n### Metrics Collection and Alerting\n\nComprehensive metrics collection enables proactive failure detection and performance monitoring across all system components. Metrics must be collected efficiently without impacting system performance and should provide both real-time monitoring and historical trend analysis.\n\n**Ingestion Metrics**: The ingestion layer tracks detailed metrics about log reception rates, parsing success/failure ratios, buffer utilization, and downstream persistence latency. Buffer utilization metrics should trigger alerts before buffers fill completely, providing time for corrective action. Parsing failure rates help detect log format changes or corruption issues.\n\n**Storage and Persistence Metrics**: Storage metrics monitor WAL size and growth rate, chunk creation frequency, disk utilization, and retention policy execution. WAL growth rate anomalies can indicate persistence problems or downstream processing delays. Chunk creation patterns help optimize storage efficiency and detect ingestion rate changes.\n\n**Query Performance Metrics**: Query metrics track execution times, result set sizes, index hit rates, and query complexity. Degrading query performance often indicates index corruption, storage issues, or resource contention. Query metrics should be segmented by tenant and query type to enable targeted optimization.\n\n**Resource Utilization Metrics**: System-level metrics monitor CPU usage, memory consumption, disk I/O rates, and network bandwidth utilization. Resource metrics help predict capacity needs and detect resource leaks or efficiency regressions.\n\n| Metric Category | Key Indicators | Alert Thresholds | Diagnostic Value |\n|-----------------|----------------|------------------|------------------|\n| Ingestion Rate | Logs/second, bytes/second | 50% above baseline | Detect traffic spikes or drops |\n| Buffer Health | Fill percentage, overflow rate | 80% utilization | Prevent data loss from buffer overflow |\n| Query Performance | P95 latency, timeout rate | 2x baseline latency | Identify performance degradation |\n| Storage Health | Disk usage, WAL size | 85% disk full | Prevent storage exhaustion |\n| Error Rates | Parse failures, query errors | 5% error rate | Detect system or data issues |\n\n**Distributed Tracing Integration**: For complex failure scenarios, distributed tracing helps understand how requests flow through system components and where failures occur. Each log entry and query should be associated with trace identifiers that enable end-to-end request tracking across component boundaries.\n\n### Circuit Breaker Patterns\n\nCircuit breakers prevent cascading failures by automatically stopping requests to failing downstream components, allowing them time to recover while protecting upstream components from overload.\n\n**Storage Circuit Breakers**: When storage systems experience high latency or failure rates, circuit breakers prevent the ingestion system from continuing to send write requests that will fail. The circuit breaker monitors storage operation success rates and latencies, transitioning to an \"open\" state when thresholds are exceeded. During the open state, ingestion falls back to memory buffering while periodically testing storage recovery.\n\n**Query Circuit Breakers**: Query circuit breakers protect against expensive queries that might exhaust system resources. When query execution times exceed thresholds or when the query engine experiences high load, circuit breakers can reject new queries with appropriate error messages. This prevents query storms from impacting ingestion or other critical operations.\n\n**External Service Circuit Breakers**: Circuit breakers for authentication services, notification systems, and other external dependencies prevent external service failures from blocking internal operations. When external services fail, circuit breakers enable degraded mode operation with cached data or alternative processing paths.\n\n| Circuit Breaker Type | Failure Threshold | Open Duration | Half-Open Test | Recovery Criteria |\n|---------------------|-------------------|---------------|----------------|-------------------|\n| Storage Operations | 20% failure rate over 1 minute | 30 seconds | Single test write | 5 successful operations |\n| Query Execution | P95 latency > 10 seconds | 60 seconds | Simple test query | 3 fast test queries |\n| Authentication | 3 consecutive timeouts | 300 seconds | Health check call | Successful auth response |\n| Notifications | 10 failures in 5 minutes | 120 seconds | Single test notification | Successful delivery |\n\n**Circuit Breaker State Management**: Circuit breakers maintain state across system restarts to prevent restart-induced failure storms. Circuit breaker states and failure statistics are persisted to disk and restored during system initialization. This prevents components from immediately overwhelming recently-failed dependencies during recovery scenarios.\n\n## Recovery and Resilience Strategies\n\n### Graceful Degradation Patterns\n\nGraceful degradation ensures that system components can continue operating with reduced functionality when dependencies fail, rather than failing completely. This approach maintains core functionality while temporarily disabling non-essential features.\n\n**Ingestion Degradation**: When downstream storage or indexing components fail, the ingestion system can continue accepting logs by increasing buffer sizes, enabling memory-only operation, or forwarding logs to backup storage locations. During degraded operation, ingestion should prioritize high-severity logs and implement intelligent dropping of low-priority entries when buffers approach capacity.\n\n**Query Degradation**: When index components fail or become unavailable, the query engine can fall back to sequential scanning of stored chunks. While this dramatically reduces query performance, it maintains query functionality for urgent debugging scenarios. Query degradation should be transparent to clients, with additional metadata indicating degraded performance and potentially incomplete results.\n\n**Index Degradation**: When bloom filters become corrupted or unavailable, the indexing system can continue operating without probabilistic optimizations. This increases false positive rates in negative lookups but maintains correctness. Similarly, when inverted index segments fail, the system can rebuild minimal indexes from chunk data to maintain basic querying capability.\n\n| Degradation Scenario | Reduced Functionality | Performance Impact | Recovery Path |\n|---------------------|----------------------|-------------------|---------------|\n| Storage Unavailable | Memory-only buffering | High - limited capacity | Restore storage, flush buffers |\n| Index Unavailable | Sequential chunk scanning | Very High - 10-100x slower | Rebuild index from chunks |\n| Auth Service Down | Cached credential validation | Low - stale permissions | Restore auth service |\n| Alerting Failed | Log alerts without delivery | None - processing continues | Restore notification channels |\n\n**Feature Flag Integration**: The system implements feature flags that enable rapid disabling of non-essential features during degraded operation. Feature flags control bloom filter usage, advanced query optimizations, real-time alerting, and other features that can be temporarily disabled to reduce resource usage and complexity during recovery scenarios.\n\n### Automatic Recovery Mechanisms\n\nAutomatic recovery reduces manual intervention requirements and enables faster restoration of full system functionality after failures. Recovery mechanisms must be designed carefully to avoid recovery loops and additional system stress.\n\n**WAL Replay and Recovery**: The storage engine implements comprehensive WAL replay logic that reconstructs system state after crashes. WAL replay validates record checksums, ensures temporal consistency, and handles partial write scenarios. The recovery process rebuilds in-memory state from WAL records and verifies consistency with persisted chunk data before accepting new operations.\n\n**Index Reconstruction**: When index corruption is detected, the system can automatically trigger index rebuilding from stored chunk data. Index reconstruction is resource-intensive and should be scheduled during low-traffic periods when possible. The system maintains multiple index segment versions to enable rollback if reconstruction fails.\n\n**Buffer Recovery and Replay**: When components restart after crashes, memory buffers are lost but can be reconstructed from WAL records. The recovery process identifies uncommitted buffer contents and replays them through the normal processing pipeline. This ensures that no data is lost due to component restarts, even if the restart occurs during high ingestion rates.\n\n**Partition Healing**: After network partitions resolve, system components must reconcile state differences that accumulated during the partition. Partition healing involves comparing timestamps, identifying missing data, and triggering replication or rebuilding of missing information. The healing process must handle conflicts carefully to maintain data consistency.\n\n| Recovery Type | Trigger Condition | Duration | Success Criteria | Fallback Action |\n|---------------|------------------|----------|------------------|-----------------|\n| WAL Replay | Component startup after crash | 30 seconds - 10 minutes | All records processed | Manual WAL inspection |\n| Index Rebuild | Corruption detection | 10 minutes - 2 hours | Query performance restored | Sequential scan fallback |\n| Buffer Replay | Memory loss during restart | 1-5 minutes | Buffer state restored | Accept buffer data loss |\n| Partition Healing | Network connectivity restored | 5-30 minutes | State consistency achieved | Manual reconciliation |\n\n### Manual Intervention Procedures\n\nDespite comprehensive automatic recovery mechanisms, some failure scenarios require manual intervention. Manual procedures must be well-documented, tested regularly, and designed to minimize system downtime and data loss risk.\n\n**Emergency Shutdown Procedures**: When system instability threatens data integrity, emergency shutdown procedures ensure graceful termination of all components with proper state persistence. Emergency shutdown flushes all buffers, forces WAL sync operations, and creates recovery checkpoints that enable clean restart. Shutdown procedures should complete within strict time limits to prevent external monitoring systems from forcing unclean termination.\n\n**Data Recovery and Restoration**: When automatic recovery mechanisms fail, manual data recovery procedures provide step-by-step guidance for restoring system state from backups, WAL files, and chunk archives. Recovery procedures must handle various corruption scenarios and provide validation steps to ensure data integrity after restoration.\n\n**Index Repair and Validation**: Manual index repair procedures address corruption scenarios that automatic rebuilding cannot handle. These procedures include index validation tools, selective segment rebuilding, and manual bloom filter reconstruction. Index repair should provide detailed progress reporting and intermediate validation checkpoints to enable recovery from repair failures.\n\n**Tenant Isolation Repair**: In multi-tenant systems, tenant isolation failures can cause data leakage between tenants. Manual isolation repair procedures include tenant data audit tools, isolation boundary validation, and secure data migration between tenant namespaces. These procedures must maintain strict audit logs to satisfy compliance requirements.\n\n| Manual Procedure | When Required | Estimated Time | Risk Level | Required Expertise |\n|------------------|---------------|----------------|------------|-------------------|\n| Emergency Shutdown | System instability detected | 5-10 minutes | Low | Operations team |\n| Data Restoration | Automatic recovery failed | 30 minutes - 4 hours | High - potential data loss | Senior engineers |\n| Index Repair | Persistent index corruption | 1-8 hours | Medium | Log aggregation specialists |\n| Tenant Isolation Repair | Cross-tenant data leakage | 2-24 hours | Very High - security impact | Security + engineering team |\n\n**Disaster Recovery Procedures**: Complete system failure scenarios require comprehensive disaster recovery procedures that rebuild the entire log aggregation system from backups and archives. Disaster recovery procedures include infrastructure provisioning, data restoration sequencing, component startup ordering, and system validation testing. These procedures should be tested regularly in isolated environments to ensure effectiveness.\n\n### Common Pitfalls\n\n⚠️ **Pitfall: Cascade Failure Amplification**\nMany systems implement retry logic that actually amplifies failures rather than improving resilience. When a storage system is overloaded, aggressive retries from multiple components can worsen the overload condition. Instead, implement exponential backoff with jitter, circuit breakers with appropriate thresholds, and coordinated backoff across components to prevent retry storms.\n\n⚠️ **Pitfall: Inconsistent Error Handling**\nDifferent components handling the same error types with different strategies creates unpredictable system behavior. For example, if ingestion drops logs on storage failure while queries return errors on the same condition, users receive inconsistent experiences. Establish system-wide error handling policies that define consistent responses to common failure scenarios across all components.\n\n⚠️ **Pitfall: Recovery State Corruption**\nRecovery procedures that don't properly validate intermediate state can corrupt data during the recovery process itself. WAL replay that doesn't verify checksums, index rebuilding that doesn't validate segment consistency, and buffer recovery that doesn't check timestamp ordering can introduce subtle corruption that manifests later as query inconsistencies or data loss.\n\n⚠️ **Pitfall: Monitoring Alert Fatigue**\nImplementing too many alerts with inappropriate thresholds leads to alert fatigue where operators ignore genuine critical alerts. Focus on alerts that require immediate action, implement alert aggregation and escalation policies, and regularly review and tune alert thresholds based on operational experience. False positive rates above 10% typically indicate poorly tuned alerting systems.\n\n⚠️ **Pitfall: Resource Exhaustion During Recovery**\nRecovery operations often consume significant system resources, which can prevent the system from handling normal operations during recovery. Index rebuilding that monopolizes disk I/O, WAL replay that exhausts memory, or partition healing that saturates network bandwidth can extend downtime and create additional failures. Recovery operations should implement resource throttling and yield processing time to critical operations.\n\n⚠️ **Pitfall: Split-Brain Resolution Data Loss**\nNetwork partition scenarios can create split-brain conditions where different components have conflicting views of system state. Naive split-brain resolution that simply discards one side's changes can cause significant data loss. Implement conflict detection and resolution strategies that preserve as much data as possible while maintaining consistency guarantees.\n\n### Implementation Guidance\n\nThis implementation guidance provides concrete tools and patterns for building robust error handling into your log aggregation system. The focus is on proactive failure detection, graceful degradation, and automated recovery mechanisms that maintain system reliability.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Health Checks | HTTP endpoints with JSON status | Kubernetes liveness/readiness probes with custom checks |\n| Metrics Collection | Prometheus client library with custom metrics | OpenTelemetry with distributed tracing integration |\n| Circuit Breakers | Simple state machine with timeout logic | Library like `hystrix-go` with statistical failure detection |\n| Alerting | Webhook notifications to Slack/email | PagerDuty integration with escalation policies |\n| Log Aggregation | Structured logging with `logrus` or `zap` | ELK stack or centralized logging with correlation IDs |\n\n#### Recommended File Structure\n\n```\nproject-root/\n  internal/monitoring/\n    health/\n      health.go                 ← health check registry and HTTP endpoints\n      checks.go                 ← component-specific health check implementations\n      integration.go            ← end-to-end integration health checks\n    metrics/\n      metrics.go                ← metrics collection and exposition\n      collectors.go             ← custom metric collectors for each component\n    alerts/\n      manager.go                ← alert routing and notification management\n      rules.go                  ← alert rule definitions and evaluation\n  internal/resilience/\n    circuit/\n      breaker.go                ← circuit breaker implementation\n      registry.go               ← circuit breaker management and configuration\n    recovery/\n      wal.go                    ← WAL replay and recovery logic\n      index.go                  ← index reconstruction procedures\n      partition.go              ← network partition healing\n    degradation/\n      features.go               ← feature flag management for graceful degradation\n      fallbacks.go              ← fallback implementations for failed components\n  pkg/errors/\n    errors.go                   ← error types and classification\n    handler.go                  ← centralized error handling policies\n  tools/recovery/\n    emergency-shutdown.go       ← emergency shutdown tooling\n    data-recovery.go           ← manual data recovery utilities\n    validation.go              ← system state validation tools\n```\n\n#### Infrastructure Starter Code\n\n**Health Check Registry**:\n```go\npackage health\n\nimport (\n    \"context\"\n    \"encoding/json\"\n    \"net/http\"\n    \"sync\"\n    \"time\"\n)\n\n// HealthStatus represents the overall health state of a component\ntype HealthStatus string\n\nconst (\n    StatusHealthy   HealthStatus = \"healthy\"\n    StatusDegraded  HealthStatus = \"degraded\"\n    StatusUnhealthy HealthStatus = \"unhealthy\"\n)\n\n// CheckResult contains the result of a health check execution\ntype CheckResult struct {\n    Name      string                 `json:\"name\"`\n    Status    HealthStatus           `json:\"status\"`\n    Message   string                 `json:\"message,omitempty\"`\n    Timestamp time.Time              `json:\"timestamp\"`\n    Details   map[string]interface{} `json:\"details,omitempty\"`\n    Duration  time.Duration          `json:\"duration\"`\n}\n\n// HealthCheck defines the interface for component health checks\ntype HealthCheck interface {\n    Name() string\n    Check(ctx context.Context) CheckResult\n}\n\n// Registry manages health checks and provides HTTP endpoints\ntype Registry struct {\n    checks map[string]HealthCheck\n    mutex  sync.RWMutex\n}\n\n// NewRegistry creates a new health check registry\nfunc NewRegistry() *Registry {\n    return &Registry{\n        checks: make(map[string]HealthCheck),\n    }\n}\n\n// RegisterCheck adds a health check to the registry\nfunc (r *Registry) RegisterCheck(check HealthCheck) {\n    r.mutex.Lock()\n    defer r.mutex.Unlock()\n    r.checks[check.Name()] = check\n}\n\n// CheckAll executes all registered health checks\nfunc (r *Registry) CheckAll(ctx context.Context) map[string]CheckResult {\n    r.mutex.RLock()\n    checks := make(map[string]HealthCheck, len(r.checks))\n    for name, check := range r.checks {\n        checks[name] = check\n    }\n    r.mutex.RUnlock()\n\n    results := make(map[string]CheckResult)\n    for name, check := range checks {\n        start := time.Now()\n        result := check.Check(ctx)\n        result.Duration = time.Since(start)\n        results[name] = result\n    }\n    return results\n}\n\n// ServeHTTP implements http.Handler for health check endpoints\nfunc (r *Registry) ServeHTTP(w http.ResponseWriter, req *http.Request) {\n    ctx, cancel := context.WithTimeout(req.Context(), 10*time.Second)\n    defer cancel()\n\n    results := r.CheckAll(ctx)\n    \n    overallStatus := StatusHealthy\n    for _, result := range results {\n        if result.Status == StatusUnhealthy {\n            overallStatus = StatusUnhealthy\n            break\n        } else if result.Status == StatusDegraded && overallStatus == StatusHealthy {\n            overallStatus = StatusDegraded\n        }\n    }\n\n    response := map[string]interface{}{\n        \"status\":    overallStatus,\n        \"timestamp\": time.Now(),\n        \"checks\":    results,\n    }\n\n    w.Header().Set(\"Content-Type\", \"application/json\")\n    if overallStatus != StatusHealthy {\n        w.WriteHeader(http.StatusServiceUnavailable)\n    }\n    \n    json.NewEncoder(w).Encode(response)\n}\n```\n\n**Circuit Breaker Implementation**:\n```go\npackage circuit\n\nimport (\n    \"context\"\n    \"errors\"\n    \"sync\"\n    \"time\"\n)\n\n// State represents the current circuit breaker state\ntype State int\n\nconst (\n    StateClosed State = iota\n    StateHalfOpen\n    StateOpen\n)\n\n// Config defines circuit breaker configuration parameters\ntype Config struct {\n    MaxFailures     int           // Number of failures before opening\n    FailureWindow   time.Duration // Time window for failure counting\n    OpenDuration    time.Duration // How long to stay open\n    HalfOpenMaxCalls int          // Max calls allowed in half-open state\n}\n\n// Breaker implements a circuit breaker pattern\ntype Breaker struct {\n    config       Config\n    state        State\n    failures     int\n    lastFailTime time.Time\n    halfOpenCalls int\n    mutex        sync.RWMutex\n}\n\n// NewBreaker creates a new circuit breaker with the given configuration\nfunc NewBreaker(config Config) *Breaker {\n    return &Breaker{\n        config: config,\n        state:  StateClosed,\n    }\n}\n\n// Call executes the given function with circuit breaker protection\nfunc (b *Breaker) Call(ctx context.Context, fn func() error) error {\n    state, err := b.beforeCall()\n    if err != nil {\n        return err\n    }\n\n    defer func() {\n        if r := recover(); r != nil {\n            b.afterCall(false)\n            panic(r)\n        }\n    }()\n\n    err = fn()\n    b.afterCall(err == nil)\n    return err\n}\n\n// beforeCall checks if the call should be allowed\nfunc (b *Breaker) beforeCall() (State, error) {\n    b.mutex.Lock()\n    defer b.mutex.Unlock()\n\n    now := time.Now()\n    \n    switch b.state {\n    case StateClosed:\n        // Reset failure count if failure window has passed\n        if now.Sub(b.lastFailTime) > b.config.FailureWindow {\n            b.failures = 0\n        }\n        return b.state, nil\n        \n    case StateOpen:\n        // Check if we should transition to half-open\n        if now.Sub(b.lastFailTime) > b.config.OpenDuration {\n            b.state = StateHalfOpen\n            b.halfOpenCalls = 0\n            return b.state, nil\n        }\n        return b.state, errors.New(\"circuit breaker is open\")\n        \n    case StateHalfOpen:\n        // Limit concurrent calls in half-open state\n        if b.halfOpenCalls >= b.config.HalfOpenMaxCalls {\n            return b.state, errors.New(\"circuit breaker is half-open with max calls\")\n        }\n        b.halfOpenCalls++\n        return b.state, nil\n        \n    default:\n        return b.state, errors.New(\"unknown circuit breaker state\")\n    }\n}\n\n// afterCall updates circuit breaker state based on call result\nfunc (b *Breaker) afterCall(success bool) {\n    b.mutex.Lock()\n    defer b.mutex.Unlock()\n\n    if success {\n        switch b.state {\n        case StateClosed:\n            b.failures = 0\n        case StateHalfOpen:\n            b.failures = 0\n            b.state = StateClosed\n        }\n    } else {\n        b.failures++\n        b.lastFailTime = time.Now()\n        \n        switch b.state {\n        case StateClosed:\n            if b.failures >= b.config.MaxFailures {\n                b.state = StateOpen\n            }\n        case StateHalfOpen:\n            b.state = StateOpen\n        }\n    }\n}\n\n// State returns the current circuit breaker state\nfunc (b *Breaker) State() State {\n    b.mutex.RLock()\n    defer b.mutex.RUnlock()\n    return b.state\n}\n```\n\n#### Core Logic Skeleton Code\n\n**WAL Recovery Manager**:\n```go\n// RecoveryManager handles system recovery after crashes and failures\ntype RecoveryManager struct {\n    walPath     string\n    storagePath string\n    logger      *zap.Logger\n}\n\n// PerformRecovery executes the complete system recovery sequence\nfunc (r *RecoveryManager) PerformRecovery(ctx context.Context) error {\n    // TODO 1: Validate WAL file integrity using checksums\n    // TODO 2: Identify the last committed checkpoint in the WAL\n    // TODO 3: Replay all WAL records since the last checkpoint\n    // TODO 4: Verify chunk consistency with replayed operations\n    // TODO 5: Rebuild in-memory state from recovered data\n    // TODO 6: Mark recovery as complete in a new WAL checkpoint\n    // Hint: Use WALRecord checksum validation to detect corruption\n    // Hint: Group WAL records by ChunkID for efficient replay\n    return nil\n}\n\n// ValidateSystemConsistency checks data integrity after recovery\nfunc (r *RecoveryManager) ValidateSystemConsistency() error {\n    // TODO 1: Verify all chunks referenced in the index exist on disk\n    // TODO 2: Check that chunk headers match their content checksums\n    // TODO 3: Validate that index segments have correct chunk references\n    // TODO 4: Ensure timestamp ordering within and across chunks\n    // TODO 5: Report any inconsistencies found during validation\n    // Hint: Use parallel goroutines to validate multiple chunks concurrently\n    return nil\n}\n```\n\n**Graceful Degradation Controller**:\n```go\n// DegradationController manages feature flags and fallback behaviors\ntype DegradationController struct {\n    features    map[string]bool\n    fallbacks   map[string]func() error\n    mutex       sync.RWMutex\n}\n\n// EnableGracefulDegradation switches the system to degraded operation mode\nfunc (d *DegradationController) EnableGracefulDegradation(reason string) error {\n    // TODO 1: Disable non-essential features like bloom filters and advanced indexing\n    // TODO 2: Switch query engine to sequential scan mode\n    // TODO 3: Increase buffer sizes and enable memory-only operation\n    // TODO 4: Activate circuit breakers for failing external dependencies\n    // TODO 5: Send degradation alerts to operations team\n    // Hint: Use atomic operations to update feature flags safely\n    // Hint: Log all degradation actions with correlation IDs for debugging\n    return nil\n}\n\n// RestoreNormalOperation attempts to restore full system functionality\nfunc (d *DegradationController) RestoreNormalOperation() error {\n    // TODO 1: Test connectivity to previously failed dependencies\n    // TODO 2: Gradually re-enable features starting with lowest impact\n    // TODO 3: Monitor system stability during feature restoration\n    // TODO 4: Rollback to degraded mode if instability is detected\n    // TODO 5: Clear degradation alerts when restoration is complete\n    // Hint: Use exponential backoff when testing dependency recovery\n    return nil\n}\n```\n\n#### Language-Specific Hints\n\n**Go-Specific Error Handling Patterns**:\n- Use `context.WithTimeout` for all external operations to prevent hanging\n- Implement error wrapping with `fmt.Errorf(\"context: %w\", err)` for better error chains\n- Use `sync.Once` for one-time initialization of recovery procedures\n- Leverage `recover()` in goroutines to prevent panics from crashing the main process\n- Use buffered channels for async error reporting: `errorChan := make(chan error, 100)`\n\n**Resource Management**:\n- Always use `defer` for cleanup operations, even in error paths\n- Implement proper file descriptor management with explicit `Close()` calls\n- Use `sync.Pool` for frequently allocated objects during recovery operations\n- Monitor goroutine counts with `runtime.NumGoroutine()` to detect leaks\n\n**Concurrency Safety**:\n- Use `sync.RWMutex` for read-heavy data structures like health check registries\n- Implement proper context cancellation in long-running recovery operations\n- Use atomic operations (`sync/atomic`) for counters and simple state flags\n- Avoid shared mutable state in recovery procedures when possible\n\n#### Milestone Checkpoints\n\n**After Milestone 1 (Log Ingestion)**:\n- Health checks should report ingestion endpoint status and buffer utilization\n- Circuit breakers should protect against downstream storage failures\n- WAL recovery should restore buffered logs after process crashes\n- Test: Restart ingestion process during high load - no logs should be lost\n\n**After Milestone 2 (Log Index)**:\n- Index corruption detection should trigger automatic rebuilding\n- Bloom filter failures should gracefully degrade to direct index lookups\n- Index compaction should handle interruption and resume correctly\n- Test: Corrupt an index file and verify automatic recovery\n\n**After Milestone 3 (Log Query Engine)**:\n- Query timeouts should prevent resource exhaustion\n- Failed queries should not impact other concurrent queries\n- Sequential scan fallback should work when indexes are unavailable\n- Test: Execute expensive queries and verify system stability\n\n**After Milestone 4 (Log Storage & Compression)**:\n- WAL replay should handle partial writes and corruption\n- Retention policy failures should not block new ingestion\n- Chunk compression errors should fall back to uncompressed storage\n- Test: Kill storage process during chunk write and verify recovery\n\n**After Milestone 5 (Multi-Tenancy and Alerting)**:\n- Tenant isolation should be maintained even during component failures\n- Rate limiting should protect against tenant resource exhaustion\n- Alert delivery failures should not impact log processing\n- Test: Simulate authentication service failure and verify graceful degradation\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | Diagnostic Steps | Fix |\n|---------|--------------|------------------|-----|\n| System hangs during startup | WAL corruption or infinite recovery loop | Check WAL file size and recent modifications | Truncate WAL at last valid checkpoint |\n| Memory usage continuously increases | Recovery process not releasing resources | Monitor goroutine count and heap profile | Add explicit cleanup in recovery procedures |\n| Health checks timeout | Blocking operations in check implementation | Add timeout contexts to all health checks | Use separate goroutines for expensive checks |\n| Circuit breakers never close | Failure threshold set too low | Review failure rate metrics and adjust thresholds | Increase failure threshold or reduce window size |\n| Alerts fire repeatedly | Alert deduplication not working | Check alert fingerprinting and time windows | Fix alert grouping logic or increase deduplication window |\n| Recovery takes too long | Processing records sequentially | Profile WAL replay performance | Parallelize recovery operations by chunk |\n\n\n## Testing Strategy\n\n> **Milestone(s):** This section applies to all milestones (1-5), providing comprehensive testing approaches, verification procedures, and checkpoints that ensure system reliability and correctness throughout development.\n\nTesting a log aggregation system requires a multi-layered approach that validates both individual components and their interactions under realistic load conditions. Think of testing like quality control in a manufacturing assembly line - you need inspection points at each stage of production (unit tests), integration verification between stations (integration tests), and final product validation (end-to-end tests). Unlike simpler applications, log aggregation systems must handle massive data volumes, concurrent operations, and graceful degradation scenarios that are difficult to reproduce in development environments.\n\nThe testing strategy follows a pyramid structure with comprehensive unit tests at the base, focused integration tests in the middle, and targeted end-to-end scenarios at the top. Each milestone introduces new components and capabilities that require specific verification approaches. The key challenge is testing distributed system behaviors like eventual consistency, partial failures, and performance characteristics that only emerge under load.\n\n### Unit Testing Approach\n\n**Mental Model: The Component Laboratory**\n\nThink of unit testing like a laboratory where each component is isolated under controlled conditions to verify its behavior. Just as a scientist tests a chemical compound's properties in isolation before mixing it with other substances, we test each log aggregation component independently before integration. Each test is an experiment with known inputs and expected outputs, allowing us to identify exactly where problems occur.\n\nUnit tests for the log aggregation system focus on verifying the correctness of individual components without external dependencies. These tests should run quickly (under 1 second per test), be deterministic (same input always produces same output), and provide clear failure messages that pinpoint the exact problem.\n\n#### Core Component Testing Strategies\n\nThe `LogEntry` and `Labels` types form the foundation of all log processing, requiring comprehensive validation testing. These tests verify data structure integrity, validation rules, and edge cases that could cause downstream failures.\n\n| Test Category | Purpose | Key Scenarios | Validation Points |\n|---------------|---------|---------------|-------------------|\n| Data Structure Validation | Verify LogEntry creation and manipulation | Valid timestamps, empty labels, Unicode messages | Field assignment, deep equality, cloning |\n| Label Operations | Test Labels map operations | Merge conflicts, special characters, case sensitivity | Hash consistency, canonicalization |\n| Serialization | Ensure data survives encoding/decoding | JSON round-trip, binary formats, corrupted data | Data integrity, error handling |\n| Edge Cases | Handle boundary conditions | Maximum message size, extreme timestamps, nil values | Graceful failure, error messages |\n\n#### Ingestion Component Testing\n\nThe ingestion pipeline components require focused testing on parsing accuracy, buffer management, and protocol handling. These tests simulate various input conditions without requiring actual network connections or file system access.\n\n| Component | Test Focus | Mock Dependencies | Critical Properties |\n|-----------|------------|-------------------|-------------------|\n| JSONParser | Parse accuracy, error handling | None (pure function) | Structured field extraction, malformed JSON handling |\n| HTTPServer | Request handling, validation | HTTP test server, mock buffer | Content-type validation, request size limits |\n| MemoryBuffer | Concurrency, overflow behavior | Mock metrics collector | Thread safety, backpressure signaling |\n| TCPHandler | Protocol parsing, connection mgmt | Mock network connections | Syslog format compliance, connection cleanup |\n\n**Architecture Decision: Mock vs. Real Dependencies in Unit Tests**\n\n> **Decision: Use dependency injection with mock interfaces for external resources**\n> - **Context**: Unit tests need to isolate components from file systems, networks, and other services while maintaining realistic behavior validation\n> - **Options Considered**: \n>   1. Test against real dependencies (files, networks)\n>   2. Mock all external interfaces\n>   3. Hybrid approach with configurable backends\n> - **Decision**: Mock external interfaces through dependency injection\n> - **Rationale**: Real dependencies make tests slow and flaky due to timing, permissions, and environment differences. Mocks provide deterministic behavior and precise error injection for edge case testing.\n> - **Consequences**: Requires designing components with injectable dependencies but enables fast, reliable tests that can simulate any failure scenario.\n\nThe `JSONParser` testing exemplifies thorough unit testing by covering all parsing scenarios including valid JSON with various field combinations, malformed JSON with specific syntax errors, and edge cases like extremely large messages or unusual Unicode characters. Each test verifies both the happy path (successful parsing) and error paths (specific failure modes with appropriate error messages).\n\n#### Index Component Testing\n\nIndex components require testing both correctness and performance characteristics since they directly impact query speed. These tests focus on data structure integrity, lookup accuracy, and memory usage patterns.\n\n| Component | Testing Approach | Performance Metrics | Correctness Verification |\n|-----------|------------------|-------------------|-------------------------|\n| BloomFilter | False positive rate measurement | Memory per element, lookup speed | No false negatives, correct probability |\n| IndexSegment | Term-to-document mapping | Index build time, lookup latency | Complete term coverage, accurate postings |\n| PostingsList | Sorted order maintenance | Merge performance, memory overhead | No duplicate entries, correct ordering |\n\nBloom filter testing requires statistical validation since the data structure provides probabilistic guarantees. Tests generate large random datasets, measure actual false positive rates, and verify they stay within configured bounds. The testing approach includes boundary analysis (empty filters, single elements, capacity limits) and performance verification under different load factors.\n\n**Critical Unit Test Pattern: Property-Based Testing**\n\nFor components like bloom filters and indexes, property-based testing generates random inputs and verifies invariants hold across all cases. This approach catches edge cases that specific example-based tests might miss.\n\n> Property-based tests generate hundreds of random inputs and verify fundamental properties like \"if we add an element to a bloom filter, MightContain must return true for that element\" or \"if we add a term to an index segment, LookupTerm must return a non-empty postings list\".\n\n#### Query Engine Testing\n\nQuery engine components require testing both parsing accuracy and execution correctness. The `Lexer` and query parser need comprehensive testing across valid and invalid query syntax, while execution components need verification of result accuracy and resource consumption.\n\n| Component | Input Variations | Error Conditions | Resource Limits |\n|-----------|------------------|------------------|-----------------|\n| Lexer | All LogQL tokens, Unicode, special chars | Invalid syntax, incomplete input | Maximum query length |\n| QueryEngine | Simple to complex queries | Syntax errors, timeout limits | Memory usage per query |\n| ResultStream | Various result sizes | Network failures, client disconnects | Streaming vs. buffering |\n\nQuery engine testing includes fuzzing approaches where random query strings are generated to identify parsing edge cases. These tests verify the parser correctly rejects invalid syntax while providing helpful error messages that indicate the specific problem location and suggested fixes.\n\n#### Storage Component Testing\n\nStorage components require testing durability guarantees, compression effectiveness, and recovery procedures. These tests use temporary directories and mock file systems to verify behavior without affecting the development environment.\n\n| Component | Durability Testing | Performance Testing | Recovery Testing |\n|-----------|-------------------|-------------------|------------------|\n| StorageEngine | WAL persistence, chunk integrity | Write throughput, compression ratio | Crash recovery, corruption detection |\n| WALRecord | Serialization accuracy | Record overhead | Partial write detection |\n| RetentionPolicy | Policy evaluation | Cleanup performance | Policy change handling |\n\nWAL testing simulates crash scenarios by interrupting write operations at various points and verifying the recovery process correctly reconstructs system state. These tests use file system hooks to inject failures at specific byte offsets, ensuring the WAL handles partial writes and corruption correctly.\n\n#### Multi-Tenancy Component Testing\n\nMulti-tenancy components require testing security isolation, rate limiting accuracy, and alerting rule evaluation. These tests focus on preventing data leakage and ensuring tenant quotas are enforced correctly.\n\n| Component | Security Testing | Performance Testing | Isolation Testing |\n|-----------|------------------|-------------------|------------------|\n| AuthService | Token validation, role checking | Authentication latency | Tenant data separation |\n| TokenBucket | Rate limit enforcement | Token allocation speed | Per-tenant fairness |\n| AlertEngine | Rule evaluation accuracy | Alert processing throughput | Cross-tenant alert isolation |\n\nRate limiting testing requires time-based simulation where tests advance mock clocks to verify token bucket refill rates and burst handling. These tests ensure rate limits are enforced fairly across tenants while allowing legitimate burst traffic.\n\n### Integration Testing\n\n**Mental Model: The Orchestra Rehearsal**\n\nIntegration testing is like an orchestra rehearsal where individual musicians (components) who have practiced their parts (passed unit tests) now play together to create harmonious music. The conductor (test harness) verifies that components synchronize correctly, handle timing variations gracefully, and recover when individual musicians make mistakes. Unlike unit tests that focus on individual performance, integration tests verify the ensemble creates the intended result.\n\nIntegration tests validate component interactions, data flow correctness, and system behavior under realistic conditions. These tests use real implementations of all components but may mock external dependencies like networks or cloud services to maintain test reliability.\n\n#### End-to-End Log Processing Flow\n\nThe primary integration test validates the complete log processing pipeline from ingestion through query response. This test verifies that data transformations preserve accuracy while maintaining acceptable performance characteristics.\n\n| Test Stage | Component Integration | Data Validation | Performance Baseline |\n|------------|----------------------|-----------------|-------------------|\n| HTTP Ingestion | HTTPServer → JSONParser → MemoryBuffer | Log structure preservation | 1000 logs/second sustained |\n| Index Building | MemoryBuffer → IndexSegment → BloomFilter | Term extraction accuracy | Index build under 100ms |\n| Storage Write | IndexSegment → StorageEngine → WAL | Durability guarantee | Write latency under 10ms |\n| Query Execution | QueryEngine → Index lookup → Storage read | Result completeness | Query response under 1s |\n\nThe integration test pipeline processes a realistic log dataset with various formats, label cardinalities, and message sizes. Each stage verifies data integrity while measuring throughput and latency to establish performance baselines for regression detection.\n\n**Architecture Decision: Test Data Management Strategy**\n\n> **Decision: Use deterministic test datasets with controlled characteristics**\n> - **Context**: Integration tests need realistic data that exercises edge cases while producing reproducible results across different environments\n> - **Options Considered**:\n>   1. Random data generation\n>   2. Production data snapshots\n>   3. Curated synthetic datasets\n> - **Decision**: Curated synthetic datasets with configurable properties\n> - **Rationale**: Random data makes failures hard to reproduce. Production data contains sensitive information and varies over time. Synthetic datasets provide controlled label cardinality, message patterns, and edge cases while remaining deterministic.\n> - **Consequences**: Requires maintaining test data generators but enables precise failure reproduction and comprehensive edge case coverage.\n\n#### Cross-Protocol Ingestion Testing\n\nIntegration testing validates that logs ingested via different protocols (HTTP, TCP, UDP) are processed identically and maintain consistent ordering and labeling. This test ensures protocol-specific parsing differences don't affect downstream processing.\n\nThe test setup establishes all three ingestion endpoints simultaneously, sends identical log content via each protocol, and verifies the resulting `LogEntry` structures are equivalent. Special attention is paid to timestamp handling, label extraction, and message formatting differences between protocols.\n\n| Protocol | Message Format | Timing Behavior | Error Handling |\n|----------|---------------|-----------------|----------------|\n| HTTP | JSON batch requests | Synchronous acknowledgment | HTTP status codes |\n| TCP | Syslog RFC 5424/3164 | Connection-based streaming | Connection termination |\n| UDP | Syslog datagrams | Fire-and-forget delivery | Silent packet loss |\n\nThe cross-protocol test includes failure scenarios like network interruptions, malformed messages, and protocol violations to verify each ingestion path handles errors appropriately without affecting other protocols.\n\n#### Index and Query Consistency Testing\n\nThis integration test verifies that indexed log data produces correct query results across different query patterns and time ranges. The test builds indexes from known log datasets and validates that query results match expected outputs exactly.\n\n| Query Type | Index Utilization | Expected Behavior | Performance Target |\n|------------|-------------------|-------------------|-------------------|\n| Label selectors | Inverted index lookup | Exact label matches only | Sub-millisecond index scan |\n| Text search | Full-text index + bloom | All matching log entries | Linear scan performance |\n| Time range | Partition pruning | Only relevant time windows | Partition skip optimization |\n| Combined filters | Multi-stage execution | AND/OR logic correctness | Filter order optimization |\n\nThe consistency test includes edge cases like empty result sets, large result sets that exceed memory limits, and concurrent queries that access the same index partitions. Each scenario verifies result accuracy while measuring resource consumption.\n\n#### Storage and Recovery Integration Testing\n\nRecovery integration testing simulates various failure scenarios during log ingestion and verifies the system recovers to a consistent state without data loss. These tests interrupt processing at different stages and validate WAL replay functionality.\n\n| Failure Scenario | System State | Recovery Action | Validation Method |\n|-------------------|--------------|-----------------|-------------------|\n| Crash during ingestion | WAL contains uncommitted entries | Replay WAL records | Count matches input logs |\n| Corruption in chunk | Index points to bad data | Mark chunk unavailable | Queries skip corrupted data |\n| Disk full during write | Partial chunk write | Rollback to last checkpoint | No partial entries visible |\n| Index corruption | Query results inconsistent | Rebuild index from chunks | Results match original data |\n\nRecovery testing uses controlled failure injection where specific operations are interrupted at deterministic points. The test framework provides hooks to simulate disk failures, process crashes, and data corruption while maintaining the ability to verify recovery correctness.\n\n#### Multi-Tenant Integration Testing\n\nMulti-tenant integration testing verifies tenant isolation works correctly across all system components from ingestion through querying. These tests ensure tenant data and resources remain separated while validating rate limiting and quota enforcement.\n\nThe test creates multiple tenant contexts with different quotas and access patterns, ingests logs for each tenant simultaneously, and verifies queries only return data for the authenticated tenant. Rate limiting validation ensures tenant quotas are enforced without affecting other tenants.\n\n| Isolation Aspect | Testing Approach | Verification Method | Security Validation |\n|-------------------|------------------|-------------------|-------------------|\n| Data separation | Cross-tenant queries | Zero results returned | Authentication bypass attempts |\n| Resource isolation | Concurrent load testing | Per-tenant metrics | Quota enforcement accuracy |\n| Rate limiting | Burst traffic simulation | Throttling behavior | Limit circumvention attempts |\n| Alert isolation | Rule evaluation testing | Tenant-specific notifications | Cross-tenant alert leakage |\n\n### Milestone Checkpoints\n\n**Mental Model: The Progressive Assessment System**\n\nMilestone checkpoints are like a progressive assessment system where each test validates that foundational capabilities work correctly before building additional complexity. Like a driving test that verifies basic skills before allowing highway driving, each checkpoint ensures the implemented functionality is solid before adding new components that depend on it.\n\nEach milestone checkpoint includes functional verification (does it work?), performance validation (does it meet requirements?), and robustness testing (does it handle errors gracefully?). The checkpoints provide clear success criteria and diagnostic guidance when problems occur.\n\n#### Milestone 1: Log Ingestion Checkpoint\n\nAfter completing the log ingestion implementation, the system should successfully receive logs via all three protocols, parse them into structured `LogEntry` objects, and buffer them reliably without data loss.\n\n**Functional Verification Checklist:**\n\n| Feature | Test Command | Expected Behavior | Success Criteria |\n|---------|--------------|-------------------|------------------|\n| HTTP Ingestion | `curl -X POST -H \"Content-Type: application/json\" -d '{\"timestamp\":\"2023-10-01T12:00:00Z\",\"message\":\"test log\",\"labels\":{\"level\":\"info\"}}' http://localhost:8080/api/v1/logs` | HTTP 200 response, log in buffer | Response contains log ID |\n| TCP Syslog | `echo '<134>Oct 1 12:00:00 host app: test message' \\| nc localhost 1514` | Connection accepted, log parsed | Log appears with correct fields |\n| UDP Syslog | `echo '<134>Oct 1 12:00:00 host app: test message' \\| nc -u localhost 1514` | Datagram processed, log buffered | No connection errors |\n| File Tail | Create log file, append lines | New lines detected and ingested | Tail follows file correctly |\n\n**Performance Validation:**\n\nThe ingestion system should handle sustained load without dropping messages or excessive memory consumption. Run a load test that sends 10,000 log messages per second for 60 seconds and verify all messages are processed successfully.\n\n```bash\n# Load test command example\ngo run cmd/load-test/main.go \\\n  --target http://localhost:8080/api/v1/logs \\\n  --rate 10000 \\\n  --duration 60s \\\n  --protocol http\n```\n\n**Expected Performance Metrics:**\n\n| Metric | Target Value | Measurement Method | Failure Threshold |\n|--------|--------------|-------------------|-------------------|\n| Ingestion Rate | 10,000 logs/sec | Logs processed / elapsed time | < 9,500 logs/sec |\n| Memory Usage | < 500MB RSS | Process memory monitoring | > 1GB RSS |\n| Buffer Utilization | < 80% capacity | Buffer fill percentage | > 95% capacity |\n| Error Rate | < 0.1% | Failed requests / total requests | > 1% error rate |\n\n**Troubleshooting Common Issues:**\n\n⚠️ **Pitfall: Port Conflicts and Binding Errors**\nIf the server fails to start with \"address already in use\" errors, verify no other processes are using ports 8080 (HTTP) or 1514 (TCP/UDP). Use `netstat -an | grep LISTEN` to check port usage and `pkill` to terminate conflicting processes.\n\n⚠️ **Pitfall: Parser Fails on Real Syslog Data**\nTest parsers often work on hand-crafted examples but fail on real syslog messages with optional fields or timezone variations. Capture actual syslog traffic using `tcpdump` and test parser against real data to identify format edge cases.\n\n#### Milestone 2: Index Building Checkpoint\n\nAfter implementing the indexing engine, the system should build accurate inverted indexes from ingested logs and support efficient term lookups with bloom filter optimization.\n\n**Index Construction Verification:**\n\n| Component | Test Procedure | Validation Criteria | Performance Target |\n|-----------|----------------|-------------------|-------------------|\n| Term Extraction | Ingest logs with known terms | All terms appear in index | 100% term coverage |\n| Inverted Index | Query for indexed terms | Postings lists return correct entries | Zero false negatives |\n| Bloom Filter | Test negative lookups | False positive rate within bounds | < 1% false positive rate |\n| Partitioning | Span multiple time windows | Queries scan relevant partitions only | Time-based partition pruning |\n\n**Index Quality Validation:**\n\nBuild an index from a dataset with known characteristics and verify the index structure matches expectations:\n\n```bash\n# Index verification command example\ngo run cmd/verify-index/main.go \\\n  --data-path ./testdata/logs.json \\\n  --index-path ./data/index \\\n  --verify-completeness \\\n  --verify-bloom-filter\n```\n\nThe verification tool should report index statistics including term count, partition distribution, and bloom filter characteristics. All validation checks should pass without errors.\n\n**Index Performance Testing:**\n\n| Operation | Performance Target | Test Method | Baseline Measurement |\n|-----------|-------------------|-------------|-------------------|\n| Index Build | 1MB logs in < 5 seconds | Time index construction | Record actual timing |\n| Term Lookup | < 1ms average latency | Query common terms | Measure lookup distribution |\n| Bloom Filter Check | < 100μs per operation | Test filter membership | Profile filter performance |\n| Index Size | < 10% of raw log size | Compare index to source data | Calculate compression ratio |\n\n⚠️ **Pitfall: Bloom Filter Parameter Misconfiguration**\nBloom filters with incorrect parameters either waste memory (too conservative) or produce excessive false positives (too aggressive). The `NewBloomParams` function calculates optimal parameters, but verify false positive rates match theoretical expectations through statistical testing.\n\n#### Milestone 3: Query Engine Checkpoint\n\nAfter implementing the query engine, the system should parse LogQL queries correctly, execute them efficiently against the index, and return accurate results with proper pagination.\n\n**Query Language Verification:**\n\nTest the query parser against a comprehensive set of LogQL syntax patterns to verify parsing accuracy and error handling:\n\n| Query Pattern | Example Query | Expected Behavior | Parser Validation |\n|---------------|---------------|-------------------|-------------------|\n| Label selectors | `{service=\"api\", level=\"error\"}` | Parse to label filter AST | Correct field extraction |\n| Text search | `\"database connection failed\"` | Parse to text filter AST | Proper quote handling |\n| Regex patterns | `\\|~ \"error.*timeout\"` | Parse to regex filter AST | Valid regex compilation |\n| Combined filters | `{service=\"api\"} \\|= \"error\" \\|~ \"timeout\"` | Parse to pipeline AST | Correct precedence |\n\n**Query Execution Validation:**\n\nExecute queries against indexed test data and verify results match expected outputs exactly:\n\n```bash\n# Query testing command example\ngo run cmd/test-queries/main.go \\\n  --index-path ./data/index \\\n  --queries ./testdata/test-queries.json \\\n  --expected-results ./testdata/expected-results.json\n```\n\nThe test suite should execute each query and compare results against expected outputs, reporting any discrepancies in result content, ordering, or metadata.\n\n**Query Performance Testing:**\n\n| Query Type | Performance Target | Test Dataset | Success Criteria |\n|------------|-------------------|--------------|------------------|\n| Simple label filter | < 100ms response | 1M log entries | 95th percentile under target |\n| Text search | < 500ms response | 1M log entries | Index utilization confirmed |\n| Complex combined filter | < 1s response | 1M log entries | Optimization applied correctly |\n| Large result set | Streaming response | 100K matching entries | Memory usage bounded |\n\n⚠️ **Pitfall: Query Result Pagination Edge Cases**\nPagination cursors can become invalid when underlying data changes during query execution. Test pagination with concurrent ingestion to verify cursor stability and implement proper error handling for invalid cursor scenarios.\n\n#### Milestone 4: Storage and Compression Checkpoint\n\nAfter implementing the storage engine, the system should persist log data reliably with compression, maintain WAL durability guarantees, and enforce retention policies correctly.\n\n**Storage Durability Verification:**\n\n| Test Scenario | Procedure | Validation Method | Recovery Expectation |\n|---------------|-----------|-------------------|-------------------|\n| Normal shutdown | Stop process gracefully | Restart and verify data | All committed data present |\n| Crash simulation | Kill process during write | WAL replay on startup | No data loss from WAL |\n| Disk full scenario | Fill storage during write | Monitor error handling | Graceful degradation |\n| Corruption detection | Corrupt chunk file | Attempt to read data | Error detection and reporting |\n\nTest storage durability by ingesting logs, terminating the process at various points, and verifying recovery restores the correct system state:\n\n```bash\n# Durability testing command example\ngo run cmd/test-durability/main.go \\\n  --ingest-count 10000 \\\n  --crash-after 5000 \\\n  --verify-recovery\n```\n\n**Compression Effectiveness Testing:**\n\nMeasure compression performance across different algorithms and log patterns to verify optimal compression selection:\n\n| Log Type | Uncompressed Size | Compressed Size (gzip) | Compressed Size (zstd) | Compression Ratio |\n|----------|------------------|----------------------|----------------------|-------------------|\n| JSON structured logs | 100MB | 15MB | 12MB | 85-88% reduction |\n| Plain text logs | 100MB | 25MB | 20MB | 75-80% reduction |\n| Mixed format logs | 100MB | 20MB | 16MB | 80-84% reduction |\n\n**Retention Policy Testing:**\n\nConfigure retention policies with short time windows and verify expired data is cleaned up correctly:\n\n```bash\n# Retention testing command example\ngo run cmd/test-retention/main.go \\\n  --retention-age 1h \\\n  --ingest-duration 2h \\\n  --verify-cleanup\n```\n\nThe test should verify that data older than the retention window is deleted while recent data remains accessible.\n\n⚠️ **Pitfall: WAL Growth Without Rotation**\nWAL files that grow unbounded eventually exhaust disk space. Test WAL rotation under sustained load and verify old WAL segments are cleaned up after successful checkpointing. Monitor WAL file count and total size during extended operation.\n\n#### Milestone 5: Multi-Tenancy and Alerting Checkpoint\n\nAfter implementing multi-tenancy and alerting, the system should enforce tenant isolation completely, respect rate limits accurately, and trigger alerts based on log patterns reliably.\n\n**Tenant Isolation Verification:**\n\n| Isolation Test | Test Procedure | Expected Behavior | Security Validation |\n|----------------|----------------|-------------------|-------------------|\n| Data separation | Query with different tenant tokens | Only tenant-specific data returned | Zero cross-tenant results |\n| Rate limit isolation | Exceed limits for one tenant | Other tenants unaffected | Independent quota enforcement |\n| Alert isolation | Trigger alerts for one tenant | Alerts only sent to correct tenant | No notification leakage |\n\nTest tenant isolation by creating multiple tenant contexts, ingesting data for each tenant, and verifying queries with different tenant authentication tokens only return appropriate data:\n\n```bash\n# Multi-tenancy testing command example\ngo run cmd/test-tenancy/main.go \\\n  --tenant-count 3 \\\n  --logs-per-tenant 1000 \\\n  --verify-isolation\n```\n\n**Rate Limiting Accuracy Testing:**\n\nConfigure rate limits with measurable thresholds and verify enforcement accuracy under various load patterns:\n\n| Rate Limit Type | Configured Limit | Test Load | Expected Behavior |\n|------------------|------------------|-----------|-------------------|\n| Per-tenant ingestion | 1000 logs/min | 1500 logs/min | 500 logs rejected |\n| Per-stream ingestion | 100 logs/min | 150 logs/min | 50 logs rejected |\n| Query rate | 10 queries/min | 15 queries/min | 5 queries throttled |\n\n**Alert Engine Verification:**\n\nConfigure alert rules with known trigger conditions and verify alerts fire correctly without false positives or negatives:\n\n```bash\n# Alert testing command example  \ngo run cmd/test-alerts/main.go \\\n  --rule-config ./testdata/alert-rules.yaml \\\n  --trigger-logs ./testdata/trigger-logs.json \\\n  --verify-notifications\n```\n\nThe alert test should verify alert deduplication prevents notification spam while ensuring critical alerts are delivered reliably.\n\n⚠️ **Pitfall: Token Bucket Rate Limiting Clock Skew**\nRate limiting accuracy depends on consistent time measurement. Test rate limiting behavior during system clock adjustments and verify token bucket algorithms handle time skew gracefully without allowing unlimited bursts or permanent blocking.\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option | Rationale |\n|-----------|---------------|-----------------|-----------|\n| Test Framework | Go testing package + testify | Ginkgo + Gomega | Standard library sufficient for most cases |\n| Mock Generation | Manual mocks | gomock or counterfeiter | Manual mocks for learning, generated for production |\n| Test Data | JSON files | Property-based testing with gopter | Static data for deterministic results |\n| Integration Testing | Docker Compose | Kubernetes test environments | Docker sufficient for component integration |\n| Load Testing | Custom Go programs | k6 or Artillery | Custom tools provide precise control |\n| Monitoring | Log output + manual verification | Prometheus metrics + Grafana | Start simple, add observability later |\n\n#### Recommended Test Structure\n\nOrganize test code to support both component-level and integration testing while maintaining clear separation between test types and shared utilities:\n\n```\nproject-root/\n  cmd/\n    test-ingestion/         ← milestone verification tools\n      main.go\n    test-index/\n      main.go\n    load-test/              ← performance testing tools\n      main.go\n  internal/\n    ingestion/\n      ingestion.go\n      ingestion_test.go     ← unit tests\n    index/\n      index.go\n      index_test.go\n  test/\n    integration/            ← integration test suites\n      ingestion_test.go\n      query_test.go\n      storage_test.go\n    testdata/               ← shared test datasets\n      sample-logs.json\n      test-queries.json\n    fixtures/               ← test utilities and mocks\n      mock_storage.go\n      test_helpers.go\n```\n\n#### Unit Test Infrastructure\n\n**Complete Mock Implementations:**\n\n```go\n// test/fixtures/mock_storage.go\npackage fixtures\n\nimport (\n    \"sync\"\n    \"time\"\n)\n\n// MockStorageEngine provides in-memory storage for testing\ntype MockStorageEngine struct {\n    chunks map[string]*ChunkHeader\n    data   map[string][]byte\n    wal    []WALRecord\n    mutex  sync.RWMutex\n}\n\nfunc NewMockStorageEngine() *MockStorageEngine {\n    return &MockStorageEngine{\n        chunks: make(map[string]*ChunkHeader),\n        data:   make(map[string][]byte),\n        wal:    make([]WALRecord, 0),\n    }\n}\n\nfunc (m *MockStorageEngine) WriteLogBatch(entries []LogEntry) error {\n    // TODO: Implement mock batch writing\n    // TODO: Generate chunk ID from timestamp\n    // TODO: Compress entries into chunk data\n    // TODO: Store chunk header and data in maps\n    // TODO: Append WAL record for durability simulation\n}\n\nfunc (m *MockStorageEngine) ReadChunk(chunkID string) ([]LogEntry, error) {\n    // TODO: Retrieve chunk data from storage map\n    // TODO: Decompress chunk data to log entries\n    // TODO: Return error if chunk not found\n}\n\n// MockMemoryBuffer provides ring buffer simulation for testing\ntype MockMemoryBuffer struct {\n    entries []LogEntry\n    head    int\n    tail    int\n    size    int\n    mutex   sync.Mutex\n}\n\nfunc NewMockMemoryBuffer(capacity int) *MockMemoryBuffer {\n    return &MockMemoryBuffer{\n        entries: make([]LogEntry, capacity),\n        size:    capacity,\n    }\n}\n\nfunc (m *MockMemoryBuffer) Write(entry LogEntry) error {\n    // TODO: Check if buffer is full\n    // TODO: Add entry at tail position\n    // TODO: Update tail pointer with wrap-around\n    // TODO: Return error if buffer overflow\n}\n\nfunc (m *MockMemoryBuffer) Read() (*LogEntry, error) {\n    // TODO: Check if buffer is empty\n    // TODO: Get entry from head position\n    // TODO: Update head pointer with wrap-around\n    // TODO: Return error if buffer underflow\n}\n```\n\n**Test Data Generation Utilities:**\n\n```go\n// test/fixtures/test_helpers.go\npackage fixtures\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"math/rand\"\n    \"time\"\n)\n\n// GenerateTestLogs creates deterministic log entries for testing\nfunc GenerateTestLogs(count int, startTime time.Time) []LogEntry {\n    // TODO: Create slice of LogEntry with specified count\n    // TODO: Generate entries with incrementing timestamps\n    // TODO: Include variety of log levels and services\n    // TODO: Add both structured and unstructured messages\n    // TODO: Ensure deterministic output with fixed random seed\n}\n\n// CreateTestLabels generates label combinations with controlled cardinality\nfunc CreateTestLabels(serviceName string, level string, extraLabels map[string]string) Labels {\n    // TODO: Create base labels with service and level\n    // TODO: Add any additional labels from extraLabels map\n    // TODO: Ensure labels pass validation rules\n}\n\n// LoadTestDataFromFile reads log entries from JSON test data files\nfunc LoadTestDataFromFile(filename string) ([]LogEntry, error) {\n    // TODO: Read JSON file from test/testdata directory\n    // TODO: Unmarshal JSON into LogEntry slice\n    // TODO: Validate all entries have required fields\n    // TODO: Return descriptive error for malformed data\n}\n\n// AssertLogEntryEqual compares two LogEntry instances with detailed diff\nfunc AssertLogEntryEqual(t *testing.T, expected, actual LogEntry) {\n    // TODO: Compare timestamps with tolerance for serialization precision\n    // TODO: Compare labels maps with sorted iteration\n    // TODO: Compare message content exactly\n    // TODO: Provide detailed diff message on mismatch\n}\n```\n\n#### Component Test Skeleton\n\n```go\n// internal/ingestion/ingestion_test.go\npackage ingestion\n\nimport (\n    \"testing\"\n    \"time\"\n    \"github.com/stretchr/testify/assert\"\n    \"github.com/stretchr/testify/require\"\n    \"your-project/test/fixtures\"\n)\n\nfunc TestJSONParser_ParseValidLog(t *testing.T) {\n    // TODO: Create JSONParser instance\n    // TODO: Prepare valid JSON log data with all fields\n    // TODO: Call Parse method with test data\n    // TODO: Assert LogEntry fields match expected values\n    // TODO: Verify timestamp parsing handles timezones correctly\n    // TODO: Verify labels map contains all expected key-value pairs\n}\n\nfunc TestJSONParser_ParseMalformedJSON(t *testing.T) {\n    // TODO: Create JSONParser instance\n    // TODO: Prepare various malformed JSON strings (missing quotes, trailing commas, etc.)\n    // TODO: Call Parse method with each malformed input\n    // TODO: Assert Parse returns appropriate error for each case\n    // TODO: Verify error messages help identify specific syntax problems\n}\n\nfunc TestMemoryBuffer_ConcurrentAccess(t *testing.T) {\n    // TODO: Create MemoryBuffer with known capacity\n    // TODO: Start multiple goroutines writing entries concurrently\n    // TODO: Start multiple goroutines reading entries concurrently\n    // TODO: Verify no data races using go test -race\n    // TODO: Verify total entries written equals total entries read\n    // TODO: Assert buffer state remains consistent throughout test\n}\n\nfunc TestBloomFilter_FalsePositiveRate(t *testing.T) {\n    // TODO: Create BloomFilter with specific false positive rate (e.g., 1%)\n    // TODO: Add large number of known elements (e.g., 10,000 random strings)\n    // TODO: Test large number of elements NOT in filter (e.g., 100,000 different strings)\n    // TODO: Count false positives where MightContain returns true for non-member\n    // TODO: Assert measured false positive rate is within theoretical bounds\n}\n\nfunc TestIndexSegment_TermLookupAccuracy(t *testing.T) {\n    // TODO: Create IndexSegment with known capacity\n    // TODO: Add log entries with predictable term distribution\n    // TODO: Build inverted index from test entries\n    // TODO: Query for each known term and verify postings list accuracy\n    // TODO: Query for terms not in logs and verify empty results\n    // TODO: Verify postings lists maintain correct sort order\n}\n```\n\n#### Integration Test Framework\n\n```go\n// test/integration/end_to_end_test.go\npackage integration\n\nimport (\n    \"context\"\n    \"net/http\"\n    \"testing\"\n    \"time\"\n)\n\nfunc TestCompleteLogProcessingPipeline(t *testing.T) {\n    // TODO: Start all system components (HTTP server, index builder, storage)\n    // TODO: Ingest test logs via HTTP endpoint\n    // TODO: Wait for logs to be indexed and stored\n    // TODO: Execute queries against indexed data\n    // TODO: Verify query results match expected outputs\n    // TODO: Verify logs are persisted correctly in storage\n    // TODO: Shutdown components gracefully\n}\n\nfunc TestCrossProtocolIngestionConsistency(t *testing.T) {\n    // TODO: Start HTTP, TCP, and UDP ingestion endpoints\n    // TODO: Send identical log content via each protocol\n    // TODO: Verify all protocols produce equivalent LogEntry structures\n    // TODO: Check that timestamps, labels, and messages are preserved accurately\n    // TODO: Verify logs from different protocols can be queried together\n}\n\nfunc TestSystemRecoveryAfterCrash(t *testing.T) {\n    // TODO: Start system and ingest logs to establish baseline state\n    // TODO: Simulate crash by forcibly terminating components\n    // TODO: Restart components and trigger WAL recovery\n    // TODO: Verify all committed data is restored correctly\n    // TODO: Verify queries return same results as before crash\n    // TODO: Verify system can continue ingesting new logs normally\n}\n```\n\n#### Milestone Verification Tools\n\n```go\n// cmd/test-ingestion/main.go\npackage main\n\nimport (\n    \"flag\"\n    \"fmt\"\n    \"log\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    var (\n        httpPort = flag.Int(\"http-port\", 8080, \"HTTP ingestion port\")\n        tcpPort  = flag.Int(\"tcp-port\", 1514, \"TCP syslog port\") \n        udpPort  = flag.Int(\"udp-port\", 1514, \"UDP syslog port\")\n        testDuration = flag.Duration(\"duration\", 30*time.Second, \"Test duration\")\n        logsPerSecond = flag.Int(\"rate\", 100, \"Logs per second to generate\")\n    )\n    flag.Parse()\n\n    // TODO: Test HTTP endpoint by sending JSON log batches\n    // TODO: Test TCP endpoint by sending RFC 5424 syslog messages\n    // TODO: Test UDP endpoint by sending RFC 3164 syslog messages\n    // TODO: Measure ingestion rates for each protocol\n    // TODO: Verify no logs are dropped during sustained load\n    // TODO: Report success/failure for each protocol test\n    \n    fmt.Printf(\"Milestone 1 verification completed\\n\")\n}\n\n// cmd/test-queries/main.go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"flag\"\n    \"fmt\"\n    \"log\"\n    \"os\"\n)\n\ntype QueryTestCase struct {\n    Name           string `json:\"name\"`\n    Query          string `json:\"query\"`\n    ExpectedCount  int    `json:\"expected_count\"`\n    ExpectedSample string `json:\"expected_sample\"`\n}\n\nfunc main() {\n    var (\n        queryFile = flag.String(\"queries\", \"test/testdata/test-queries.json\", \"Query test cases\")\n        indexPath = flag.String(\"index\", \"data/index\", \"Index directory\")\n    )\n    flag.Parse()\n\n    // TODO: Load query test cases from JSON file\n    // TODO: Initialize query engine with index path\n    // TODO: Execute each test query and measure response time\n    // TODO: Compare results against expected outputs\n    // TODO: Report any mismatches with detailed diff information\n    // TODO: Calculate overall pass/fail statistics\n\n    fmt.Printf(\"Milestone 3 verification completed\\n\")\n}\n```\n\n#### Language-Specific Testing Hints\n\n**Go Testing Best Practices:**\n- Use `go test -race` to detect concurrent access issues in buffer and index components\n- Use `go test -cover` to measure test coverage and identify untested code paths\n- Use `testing.Short()` to skip long-running tests during development: `if testing.Short() { t.Skip() }`\n- Use `t.Cleanup()` for test resource cleanup instead of defer when setup can fail\n- Use `require` package for assertions that should stop test execution, `assert` for continued validation\n\n**Performance Testing Techniques:**\n- Use `testing.B` benchmarks to measure component performance: `go test -bench=. -benchmem`\n- Use `pprof` to identify performance bottlenecks: `go test -cpuprofile=cpu.prof`\n- Create memory usage baselines with `runtime.MemStats` before and after operations\n- Test with realistic data sizes (MB chunks, thousands of labels) not toy examples\n\n**Mock and Stub Patterns:**\n- Implement interfaces for all external dependencies (file system, network, time) to enable mocking\n- Use dependency injection in constructors: `NewHTTPServer(parser Parser, buffer Buffer)`\n- Create test doubles that simulate specific failure conditions: `MockStorageEngine.SetError(error)`\n- Verify mock interactions with call counts and parameter validation\n\n**Debugging Test Failures:**\n- Log actual vs expected values in assertion messages: `assert.Equal(t, expected, actual, \"term lookup failed for %s\", term)`\n- Use `t.Logf()` to add debug output that only appears on test failure\n- Create reproducible test data with fixed random seeds: `rand.Seed(42)`\n- Add timeouts to tests that could hang: `ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)`\n\n\n## Debugging Guide\n\n> **Milestone(s):** This section applies to all milestones (1-5), providing systematic debugging approaches, diagnostic procedures, and resolution strategies specific to log aggregation systems. Each milestone introduces new failure modes that require specialized debugging techniques.\n\n### Mental Model: The Medical Diagnosis Process\n\nThink of debugging a log aggregation system like being a doctor diagnosing a patient. Just as a doctor follows a systematic approach—observing symptoms, running tests, forming hypotheses, and prescribing treatments—debugging requires structured observation, measurement, hypothesis formation, and targeted fixes. The key insight is that symptoms often mask the root cause: a \"slow queries\" symptom might actually indicate index corruption, memory pressure, or retention policy failures. Like medical diagnosis, effective debugging requires understanding both the normal \"physiology\" of the system and the pathological patterns that indicate specific problems.\n\nThe analogy extends to diagnostic tools: just as doctors use stethoscopes, blood tests, and X-rays, log aggregation systems need metrics, traces, and specialized debugging utilities. Each tool reveals different aspects of system health, and combining multiple signals often reveals problems invisible to any single measurement.\n\n### Symptom-Based Diagnosis\n\nThe following diagnostic table provides a systematic approach to identifying and resolving common problems in log aggregation systems. Each entry follows the medical model: observable symptom, likely underlying causes, diagnostic procedures, and targeted treatments.\n\n| Symptom | Likely Causes | Diagnostic Steps | Resolution Strategy |\n|---------|---------------|------------------|-------------------|\n| **No logs arriving** | Network connectivity, authentication failure, buffer overflow, parser errors | 1. Check network connectivity with `telnet host:port`<br>2. Verify authentication headers in HTTP logs<br>3. Check ingestion endpoint metrics for error rates<br>4. Examine parser error logs for format mismatches<br>5. Monitor buffer fullness metrics | 1. Fix network configuration or firewall rules<br>2. Update authentication credentials<br>3. Increase buffer capacity or flush frequency<br>4. Adjust parser configuration for log format<br>5. Scale ingestion capacity horizontally |\n| **Query returns no results** | Time range mismatch, label selector errors, index corruption, retention cleanup | 1. Verify time range covers ingested log timestamps<br>2. Check label exactness (case sensitivity, whitespace)<br>3. Query index directly for expected terms<br>4. Check retention policy logs for cleanup activity<br>5. Validate chunk metadata consistency | 1. Adjust query time range to match data<br>2. Fix label selector syntax and values<br>3. Rebuild corrupted index segments<br>4. Restore from backup if retention deleted data<br>5. Run chunk integrity verification |\n| **Queries extremely slow** | Index not being used, large result sets, disk I/O bottleneck, memory pressure | 1. Check query execution plan for index usage<br>2. Measure result set size with EXPLAIN queries<br>3. Monitor disk I/O wait times during queries<br>4. Check memory allocation during query processing<br>5. Profile query execution with detailed timing | 1. Add missing indexes or rebuild corrupted ones<br>2. Add more selective filters to reduce result set<br>3. Move storage to faster disks or add read replicas<br>4. Increase query memory limits or add query caching<br>5. Implement query result streaming |\n| **Memory usage growing unbounded** | Buffer not flushing, index cache not evicting, query results not streaming, WAL accumulation | 1. Monitor buffer flush frequency and success rate<br>2. Check index cache hit ratios and eviction policies<br>3. Measure memory allocation per active query<br>4. Check WAL file sizes and rotation frequency<br>5. Profile memory allocations by component | 1. Fix buffer flush triggers or increase flush frequency<br>2. Tune cache eviction policies or reduce cache sizes<br>3. Implement streaming query execution<br>4. Force WAL rotation and cleanup old files<br>5. Add memory circuit breakers |\n| **High CPU usage during ingestion** | Inefficient parsing, excessive indexing, compression overhead, lock contention | 1. Profile CPU usage by parsing stage<br>2. Monitor index write operations and lock waits<br>3. Compare compression algorithm performance<br>4. Check goroutine/thread contention patterns<br>5. Measure parsing throughput vs CPU utilization | 1. Optimize regex patterns or switch to faster parsers<br>2. Batch index updates or use async indexing<br>3. Switch to faster compression (LZ4 vs gzip)<br>4. Reduce lock scope or use lock-free data structures<br>5. Scale parsing across multiple threads |\n| **Disk space growing rapidly** | Compression disabled, retention not working, WAL not rotating, index bloat | 1. Check compression ratios in chunk headers<br>2. Verify retention policy execution logs<br>3. Monitor WAL file ages and rotation triggers<br>4. Measure index size vs log data ratio<br>5. Check for failed cleanup operations | 1. Enable compression or fix compression pipeline<br>2. Fix retention policy logic or schedule execution<br>3. Configure WAL rotation thresholds correctly<br>4. Rebuild indexes with better compression<br>5. Manually clean up failed operations |\n| **Authentication failures** | Token expiration, tenant ID mismatches, network time skew, header formatting | 1. Check JWT token expiration times<br>2. Verify tenant ID extraction from requests<br>3. Compare client/server clock synchronization<br>4. Validate HTTP header formatting<br>5. Test authentication with curl commands | 1. Refresh expired tokens or extend validity<br>2. Fix tenant ID extraction logic<br>3. Synchronize clocks via NTP<br>4. Fix client header formatting<br>5. Add detailed authentication logging |\n| **Rate limiting triggering incorrectly** | Token bucket misconfiguration, time window errors, tenant quota bugs, clock issues | 1. Monitor token bucket fill rates and consumption<br>2. Check time window calculations in rate limiter<br>3. Verify per-tenant quota configuration<br>4. Compare client/server timestamps<br>5. Test with synthetic traffic patterns | 1. Adjust token bucket parameters<br>2. Fix time window boundary calculations<br>3. Update tenant quota configuration<br>4. Synchronize system clocks<br>5. Add rate limiting metrics and alerting |\n| **Alerts not firing** | Rule configuration errors, condition logic bugs, notification failures, time window issues | 1. Test alert rules manually with sample data<br>2. Check condition evaluation logic and thresholds<br>3. Verify notification delivery mechanisms<br>4. Check alert time window calculations<br>5. Monitor alert rule evaluation frequency | 1. Fix alert rule syntax or logic errors<br>2. Adjust condition thresholds for sensitivity<br>3. Fix notification webhook endpoints<br>4. Correct time window boundary handling<br>5. Increase alert evaluation frequency |\n| **Data corruption detected** | Disk errors, incomplete writes, checksum mismatches, concurrent access bugs | 1. Check filesystem error logs<br>2. Verify WAL record checksums<br>3. Compare chunk checksums with stored values<br>4. Look for concurrent write access patterns<br>5. Run full data integrity verification | 1. Replace failing disk hardware<br>2. Restore from WAL or backup<br>3. Rebuild corrupted chunks from source logs<br>4. Fix concurrent access with proper locking<br>5. Implement automatic corruption detection |\n\n> **Design Insight**: The most effective debugging approach combines multiple diagnostic signals rather than relying on single symptoms. For example, \"slow queries\" combined with \"high disk I/O\" and \"low index cache hit rate\" points to a specific resolution strategy, while \"slow queries\" with \"high CPU\" and \"low memory\" suggests a completely different root cause.\n\n### Domain-Specific Debugging Techniques\n\nLog aggregation systems have unique debugging requirements due to their high-throughput, time-series nature and complex interaction between ingestion, indexing, storage, and querying. These specialized techniques go beyond general application debugging.\n\n#### Log Flow Tracing\n\nThe **log flow tracing** technique tracks individual log entries through the entire pipeline from ingestion to storage. This helps identify where logs get lost, corrupted, or delayed. Implement trace IDs that follow specific log entries:\n\n**Trace ID Implementation Strategy:**\n\n1. Generate unique trace ID during ingestion (timestamp + source + sequence)\n2. Attach trace ID to log entry metadata throughout processing\n3. Log trace ID at each pipeline stage with timing information\n4. Provide trace lookup API for debugging specific log entry paths\n5. Include trace ID in error messages and monitoring metrics\n\n**Pipeline Stage Monitoring Points:**\n\n| Stage | Monitoring Point | Key Metrics | Failure Indicators |\n|-------|------------------|-------------|-------------------|\n| **HTTP Reception** | Request handler entry/exit | Latency, error rate, payload size | 4xx/5xx responses, timeout errors |\n| **Parsing** | Parser input/output | Parse time, validation errors, format mismatches | Regex failures, JSON syntax errors |\n| **Buffering** | Buffer write/read operations | Buffer fullness, flush frequency, backpressure | Buffer overflow, flush failures |\n| **Indexing** | Index term extraction/insertion | Index write latency, term count, cardinality | Lock contention, index corruption |\n| **Storage** | Chunk write operations | Compression ratio, write latency, WAL sync | Disk I/O errors, checksum mismatches |\n\n#### Index Health Diagnostics\n\nIndex corruption and inefficiency are common sources of query performance problems. Specialized index debugging requires checking multiple aspects of index health:\n\n**Index Consistency Checks:**\n\n1. **Term-to-posting verification**: For random sample of terms, verify postings lists point to valid log entries\n2. **Reverse reference validation**: For random log entries, verify they appear in expected term postings\n3. **Bloom filter accuracy**: Test bloom filter false positive rates against statistical expectations\n4. **Partition boundary integrity**: Verify time-based partitions contain only logs within their time ranges\n5. **Compaction consistency**: After index compaction, verify search results match pre-compaction results\n\n**Index Performance Analysis:**\n\n| Metric | Healthy Range | Warning Threshold | Critical Threshold | Diagnostic Action |\n|--------|---------------|-------------------|-------------------|------------------|\n| **Bloom filter false positive rate** | < 1% | 1-5% | > 5% | Rebuild bloom filters with better parameters |\n| **Average postings list length** | 10-1000 entries | 1000-10000 | > 10000 | Check for high-cardinality label explosion |\n| **Index cache hit rate** | > 80% | 60-80% | < 60% | Increase cache size or improve cache eviction |\n| **Index compaction frequency** | Daily | Weekly | Monthly | Reduce compaction threshold or increase resources |\n| **Query index usage percentage** | > 90% | 70-90% | < 70% | Add missing indexes or fix query patterns |\n\n#### Storage Layer Debugging\n\nThe storage layer combines WAL, chunks, compression, and retention policies, making it complex to debug. Storage debugging requires understanding data movement through multiple storage tiers:\n\n**WAL Health Diagnostics:**\n\n1. **WAL record integrity**: Verify checksums for all WAL records, detect incomplete writes\n2. **WAL replay consistency**: Compare system state before/after WAL replay operations\n3. **WAL rotation timing**: Monitor WAL file sizes and rotation frequency vs configuration\n4. **WAL performance impact**: Measure WAL fsync latency impact on ingestion throughput\n5. **Recovery completeness**: Verify WAL recovery restores exact pre-crash state\n\n**Chunk Integrity Verification:**\n\nThe chunk verification process ensures compressed log storage maintains data integrity across the compression, storage, and retrieval pipeline:\n\n1. **Header validation**: Verify chunk headers contain valid metadata (magic bytes, version, checksums)\n2. **Compression consistency**: Decompress chunks and verify entry count matches header\n3. **Time range compliance**: Verify all entries in chunk fall within chunk's time boundaries\n4. **Cross-reference validation**: Verify index entries point to valid chunk locations\n5. **Retention policy compliance**: Verify chunks are cleaned up according to configured policies\n\n#### Multi-Tenant Debugging\n\nMulti-tenant systems require specialized debugging to isolate tenant-specific problems from system-wide issues:\n\n**Tenant Isolation Verification:**\n\n| Verification Type | Test Procedure | Expected Result | Failure Indication |\n|------------------|----------------|-----------------|-------------------|\n| **Data isolation** | Query tenant A data with tenant B credentials | Empty result set | Cross-tenant data leak |\n| **Resource isolation** | Generate high load from tenant A | No impact on tenant B performance | Resource bleed-through |\n| **Rate limit isolation** | Exhaust tenant A rate limit | Tenant B unaffected | Rate limit cross-talk |\n| **Alert isolation** | Trigger alerts in tenant A | Tenant B receives no alerts | Alert routing errors |\n| **Authentication bypass** | Use malformed tenant headers | Authentication failure | Security vulnerability |\n\n**Per-Tenant Metrics Analysis:**\n\nMulti-tenant debugging requires analyzing metrics both globally and per-tenant to identify problems that affect specific tenants:\n\n1. **Ingestion rate per tenant**: Compare against tenant quotas and historical patterns\n2. **Query latency by tenant**: Identify tenants with consistently slow queries\n3. **Storage usage growth**: Track tenant storage against retention policies\n4. **Alert frequency patterns**: Identify tenants with excessive or missing alerts\n5. **Authentication failure rates**: Monitor per-tenant auth errors for security issues\n\n> **Design Insight**: Multi-tenant debugging often reveals problems that only manifest under specific tenant interaction patterns. For example, a high-cardinality tenant might cause index bloat that affects all other tenants' query performance, even though the symptom appears system-wide.\n\n#### Query Performance Profiling\n\nLogQL query debugging requires understanding both the query language semantics and the underlying execution engine performance characteristics:\n\n**Query Execution Analysis Framework:**\n\n1. **Parse tree validation**: Verify query parses to expected AST structure\n2. **Execution plan inspection**: Check index usage, filter ordering, and optimization decisions\n3. **Resource consumption profiling**: Monitor memory, CPU, and I/O usage during query execution\n4. **Result set analysis**: Verify result accuracy and completeness against expected outcomes\n5. **Streaming behavior verification**: Ensure large result sets stream properly without memory exhaustion\n\n**Performance Regression Detection:**\n\n| Performance Indicator | Measurement Approach | Baseline Establishment | Regression Threshold |\n|---------------------|---------------------|----------------------|-------------------|\n| **Query latency** | P50/P95/P99 response times | Rolling 7-day average | 2x baseline increase |\n| **Memory usage** | Peak memory per query | Historical query memory | 50% increase from baseline |\n| **Index utilization** | Index seek vs scan ratio | Query plan analysis | < 50% index usage |\n| **Result streaming** | Time to first result | Streaming latency measurement | 5x increase in TTFR |\n| **Cache efficiency** | Cache hit rates | Cache performance monitoring | 20% hit rate decrease |\n\n### Performance Problem Diagnosis\n\nPerformance problems in log aggregation systems often involve complex interactions between ingestion load, storage I/O, index efficiency, and query patterns. Systematic performance diagnosis requires understanding these interactions and measuring performance across multiple dimensions.\n\n#### Throughput Performance Analysis\n\n**Ingestion Throughput Diagnosis:**\n\nThink of ingestion throughput like water flowing through a series of pipes with different diameters. The overall flow rate is limited by the narrowest pipe, but identifying which pipe is the bottleneck requires measuring pressure at each stage.\n\n| Pipeline Stage | Throughput Measurement | Bottleneck Indicators | Resolution Strategies |\n|---------------|----------------------|---------------------|----------------------|\n| **Network Reception** | Requests/second, bytes/second | High connection queue, TCP retransmits | Scale connection handlers, increase buffer sizes |\n| **Parsing** | Logs parsed/second, parse CPU time | High parse latency, CPU saturation | Optimize regex, use faster parsers, parallel parsing |\n| **Validation** | Validation time per entry | Schema validation failures | Cache validation rules, async validation |\n| **Indexing** | Terms indexed/second, index write latency | Index lock contention, high index CPU | Batch index updates, async indexing, index sharding |\n| **Storage** | Chunks written/second, compression throughput | Disk I/O wait, compression CPU | Faster compression, async writes, storage scaling |\n\n**Throughput Cascade Analysis:**\n\nThroughput problems often cascade through the pipeline. For example, slow storage can back up indexing, which backs up parsing, eventually causing network timeouts. The cascade analysis technique measures throughput at each stage and identifies where the backup begins:\n\n1. Start measurement at the output (storage writes/second)\n2. Work backward through pipeline measuring each stage\n3. Identify the first stage where throughput drops significantly\n4. Focus optimization efforts on the bottleneck stage\n5. Re-measure after optimization to verify improvement and identify next bottleneck\n\n#### Latency Performance Analysis\n\n**Query Latency Breakdown:**\n\nQuery latency involves multiple phases, each with different performance characteristics and optimization approaches:\n\n| Latency Phase | Typical Duration | Optimization Techniques | Measurement Approach |\n|--------------|------------------|------------------------|-------------------|\n| **Query parsing** | 1-10ms | Cache parsed queries, optimize parser | Time parse function execution |\n| **Index lookup** | 10-100ms | Improve index caching, use bloom filters | Instrument index seek operations |\n| **Storage retrieval** | 50-500ms | Cache frequently accessed chunks | Time chunk decompression |\n| **Result filtering** | 10-200ms | Push filters to storage layer | Profile filter execution |\n| **Result serialization** | 5-50ms | Stream results, optimize JSON encoding | Time response generation |\n\n**Latency Percentile Analysis:**\n\nDifferent latency percentiles reveal different types of performance problems:\n\n- **P50 latency increases**: Indicates general performance degradation affecting typical queries\n- **P95 latency spikes**: Suggests occasional expensive operations (cache misses, large queries)\n- **P99 latency extremes**: Points to outlier behavior (query timeouts, resource exhaustion)\n- **Max latency growth**: Indicates unbounded resource usage (memory leaks, runaway queries)\n\n#### Memory Performance Analysis\n\n**Memory Usage Pattern Diagnosis:**\n\nMemory problems in log aggregation systems follow characteristic patterns that indicate specific underlying causes:\n\n| Memory Pattern | Likely Cause | Diagnostic Approach | Resolution Strategy |\n|---------------|--------------|-------------------|-------------------|\n| **Gradual growth** | Memory leak in long-running operations | Profile allocation over time | Fix leak sources, add memory limits |\n| **Spike during queries** | Large result sets loading into memory | Monitor per-query memory usage | Implement result streaming |\n| **Spike during ingestion** | Buffer overflow or batch sizing issues | Monitor buffer memory usage | Tune buffer sizes and flush triggers |\n| **Growth during indexing** | Index cache not evicting properly | Monitor cache hit rates and sizes | Fix cache eviction policies |\n| **Growth after failures** | Failed operations not cleaning up resources | Monitor resource cleanup after errors | Add proper error handling cleanup |\n\n**Garbage Collection Impact Analysis:**\n\nIn garbage-collected languages, GC pressure can severely impact performance. GC analysis requires understanding how log aggregation workloads interact with garbage collection:\n\n1. **Allocation rate monitoring**: Measure object allocation rate vs GC frequency\n2. **GC pause impact**: Monitor GC pause times vs query latency spikes\n3. **Memory pool analysis**: Track different memory pools (buffers, indexes, queries) separately\n4. **Collection frequency tuning**: Adjust GC parameters for log aggregation workload characteristics\n5. **Memory pool optimization**: Use object pooling for frequently allocated structures\n\n#### Disk I/O Performance Analysis\n\n**Storage I/O Bottleneck Identification:**\n\nDisk I/O performance affects both ingestion (chunk writes, WAL fsync) and querying (chunk reads, index access). I/O performance diagnosis requires understanding both sequential and random access patterns:\n\n| I/O Pattern | Associated Operations | Performance Indicators | Optimization Approaches |\n|------------|---------------------|----------------------|------------------------|\n| **Sequential writes** | WAL appends, chunk creation | Write throughput MB/s | Use faster sequential storage, batch writes |\n| **Random writes** | Index updates, metadata updates | Write IOPS, write latency | Use SSD storage, reduce write amplification |\n| **Sequential reads** | Chunk scanning, WAL replay | Read throughput MB/s | Prefetch data, use streaming reads |\n| **Random reads** | Index lookups, chunk metadata | Read IOPS, cache hit rates | Increase cache sizes, optimize data layout |\n\n**Storage Layer Performance Profiling:**\n\nStorage performance diagnosis requires measuring performance across multiple storage tiers:\n\n1. **WAL performance**: Measure fsync latency and WAL write throughput\n2. **Chunk compression**: Profile compression time vs compression ratio trade-offs\n3. **Index persistence**: Monitor index write and read performance separately\n4. **Cache effectiveness**: Measure cache hit rates for different data types (chunks, indexes, metadata)\n5. **Background operations**: Monitor performance impact of compaction and retention cleanup\n\n#### Network Performance Analysis\n\n**Network Bottleneck Diagnosis:**\n\nNetwork performance affects log ingestion from remote sources and query responses to clients:\n\n| Network Metric | Ingestion Impact | Query Impact | Diagnostic Commands |\n|---------------|-----------------|--------------|-------------------|\n| **Bandwidth utilization** | Limits ingestion rate | Affects large result transfers | `iftop`, `nload`, `bandwidthd` |\n| **Connection counts** | TCP connection limits | Concurrent query limits | `netstat -an`, `ss -s` |\n| **Packet loss** | Lost log entries | Query timeouts | `ping`, `mtr`, packet capture |\n| **DNS resolution** | Service discovery failures | Client connection issues | `dig`, `nslookup`, DNS timing |\n| **TLS handshake time** | HTTPS ingestion overhead | Secure query overhead | TLS connection profiling |\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| **Logging Framework** | Go log package with structured output | Zap or Logrus with contextual fields |\n| **Metrics Collection** | Prometheus client library | Custom metrics with OpenTelemetry |\n| **Profiling Tools** | Go pprof for CPU/memory profiling | Continuous profiling with Pyroscope |\n| **Tracing Infrastructure** | Simple trace ID logging | Distributed tracing with Jaeger |\n| **Health Monitoring** | HTTP health check endpoints | Comprehensive health dashboard |\n| **Error Tracking** | Structured error logging | Error aggregation with Sentry |\n\n#### Recommended File Structure\n\n```\nproject-root/\n  cmd/\n    debug/                    ← debugging utilities\n      trace-log.go           ← trace specific log entries\n      index-verify.go        ← verify index consistency\n      chunk-inspect.go       ← inspect chunk contents\n      wal-replay.go          ← replay WAL operations\n  internal/\n    debug/                   ← debugging infrastructure\n      tracer/                ← log flow tracing\n        tracer.go\n        trace_id.go\n      health/                ← health monitoring\n        checks.go\n        registry.go\n        http_handler.go\n      metrics/               ← performance monitoring\n        collector.go\n        ingestion_metrics.go\n        query_metrics.go\n      profiling/             ← performance profiling\n        profiler.go\n        memory_profiler.go\n    monitoring/              ← system monitoring\n      circuit_breaker.go\n      degradation.go\n      recovery.go\n  scripts/\n    debug-queries.sh         ← query debugging scripts\n    performance-test.sh      ← load testing scripts\n    health-check.sh          ← system health verification\n```\n\n#### Core Debugging Infrastructure\n\n**Distributed Tracing Implementation:**\n\n```go\n// TraceContext provides request tracing across the log aggregation pipeline\ntype TraceContext struct {\n    TraceID     string            // unique identifier for this request\n    SpanID      string            // current operation identifier  \n    ParentSpan  string            // parent operation identifier\n    StartTime   time.Time         // when this operation started\n    Metadata    map[string]string // additional context information\n    Logger      *zap.Logger       // logger with trace context\n}\n\n// NewTraceContext creates a new trace context for pipeline operations\nfunc NewTraceContext(operation string, metadata map[string]string) *TraceContext {\n    // TODO 1: Generate unique trace ID using timestamp + random component\n    // TODO 2: Generate span ID for this specific operation\n    // TODO 3: Create logger with trace ID and span ID fields\n    // TODO 4: Record start time for latency measurement\n    // TODO 5: Initialize metadata map with operation-specific context\n    // Hint: Use crypto/rand for trace ID generation to ensure uniqueness\n}\n\n// StartSpan creates a child span for nested operations\nfunc (tc *TraceContext) StartSpan(operation string) *TraceContext {\n    // TODO 1: Generate new span ID for child operation\n    // TODO 2: Set current span ID as parent span\n    // TODO 3: Create new logger with updated span context\n    // TODO 4: Copy relevant metadata from parent context\n    // TODO 5: Record span start time\n}\n\n// RecordEvent logs a significant event within this trace span\nfunc (tc *TraceContext) RecordEvent(event string, details map[string]interface{}) {\n    // TODO 1: Calculate elapsed time since span start\n    // TODO 2: Log event with trace ID, span ID, and timing\n    // TODO 3: Include event details in structured format\n    // TODO 4: Add event to trace timeline for debugging\n}\n\n// Finish completes the current span and records final metrics\nfunc (tc *TraceContext) Finish(err error) {\n    // TODO 1: Calculate total span duration\n    // TODO 2: Record success or failure status\n    // TODO 3: Log span completion with final metrics\n    // TODO 4: Update performance monitoring with span data\n    // TODO 5: Clean up any span-specific resources\n}\n```\n\n**Health Check Infrastructure:**\n\n```go\n// HealthChecker defines the interface for component health verification\ntype HealthChecker interface {\n    Name() string                                    // component name for identification\n    Check(ctx context.Context) CheckResult         // perform health verification\n    Dependencies() []string                         // list component dependencies\n}\n\n// IngestionHealthCheck verifies log ingestion pipeline health\ntype IngestionHealthCheck struct {\n    httpServer   *HTTPServer      // HTTP ingestion endpoint\n    tcpHandler   *TCPHandler      // TCP syslog handler  \n    buffer       *MemoryBuffer    // ingestion buffer\n    parser       *JSONParser      // log parser\n    metrics      *Metrics         // ingestion metrics\n}\n\n// Check verifies all aspects of log ingestion pipeline health\nfunc (ihc *IngestionHealthCheck) Check(ctx context.Context) CheckResult {\n    // TODO 1: Test HTTP endpoint responsiveness with synthetic request\n    // TODO 2: Verify TCP handler is accepting connections\n    // TODO 3: Check buffer utilization and flush status\n    // TODO 4: Test parser with sample log entries\n    // TODO 5: Validate metrics collection is working\n    // TODO 6: Aggregate results and determine overall health status\n    // Hint: Use context timeout to prevent health checks from hanging\n}\n\n// StorageHealthCheck verifies storage layer functionality\ntype StorageHealthCheck struct {\n    storageEngine *StorageEngine  // storage coordination\n    walPath       string          // WAL file location\n    chunkPath     string          // chunk storage location\n}\n\n// Check verifies storage layer health including WAL and chunk access\nfunc (shc *StorageHealthCheck) Check(ctx context.Context) CheckResult {\n    // TODO 1: Verify WAL file accessibility and write permissions\n    // TODO 2: Test chunk storage read/write operations\n    // TODO 3: Check storage space availability\n    // TODO 4: Validate recent WAL records for corruption\n    // TODO 5: Test chunk compression/decompression pipeline\n    // TODO 6: Verify retention policy is running properly\n}\n```\n\n**Performance Monitoring Framework:**\n\n```go\n// PerformanceMonitor tracks system performance across all components\ntype PerformanceMonitor struct {\n    ingestionMetrics *IngestionMetrics    // ingestion performance data\n    queryMetrics     *QueryMetrics        // query performance data  \n    storageMetrics   *StorageMetrics      // storage performance data\n    systemMetrics    *SystemMetrics       // system resource usage\n    alertThresholds  map[string]float64   // performance alert thresholds\n}\n\n// RecordIngestionLatency tracks latency for each ingestion pipeline stage\nfunc (pm *PerformanceMonitor) RecordIngestionLatency(stage string, duration time.Duration) {\n    // TODO 1: Record latency in appropriate histogram bucket\n    // TODO 2: Update running averages for this stage\n    // TODO 3: Check if latency exceeds alert thresholds\n    // TODO 4: Update dashboard metrics for real-time monitoring\n    // TODO 5: Log performance anomalies for investigation\n}\n\n// RecordQueryPerformance captures comprehensive query execution metrics\nfunc (pm *PerformanceMonitor) RecordQueryPerformance(query string, metadata *QueryMetadata) {\n    // TODO 1: Record query latency by query complexity\n    // TODO 2: Track memory usage patterns for query types\n    // TODO 3: Monitor index utilization effectiveness\n    // TODO 4: Update cache hit rate statistics\n    // TODO 5: Check for query performance regressions\n}\n\n// GeneratePerformanceReport creates comprehensive performance analysis\nfunc (pm *PerformanceMonitor) GeneratePerformanceReport() *PerformanceReport {\n    // TODO 1: Aggregate metrics across all components\n    // TODO 2: Calculate performance trends over time windows\n    // TODO 3: Identify performance bottlenecks and anomalies\n    // TODO 4: Generate recommendations for optimization\n    // TODO 5: Format report for debugging and optimization\n}\n```\n\n#### Milestone Checkpoints\n\n**Milestone 1 - Ingestion Debugging Verification:**\n- Start the ingestion pipeline and send test logs via all protocols (HTTP, TCP, UDP)\n- Verify trace IDs appear in logs for each ingestion path\n- Check that buffer overflow scenarios trigger appropriate backpressure\n- Confirm parser errors are logged with sufficient detail for debugging\n- Expected behavior: All logs should have trace context, errors should be actionable\n\n**Milestone 2 - Index Health Verification:**\n- Run index consistency checks on a populated index\n- Verify bloom filter false positive rates match expected statistical ranges\n- Test index compaction process and validate search results remain consistent\n- Confirm index corruption detection triggers appropriate alerts\n- Expected behavior: Index diagnostics should pass, corruption should be detected reliably\n\n**Milestone 3 - Query Performance Verification:**\n- Profile query execution with various complexity levels\n- Verify query execution plans use indexes appropriately\n- Test query timeout and memory limit enforcement\n- Confirm large result sets stream properly without memory exhaustion\n- Expected behavior: Query performance should be predictable and bounded\n\n**Milestone 4 - Storage Debugging Verification:**\n- Test WAL replay after simulated crash scenarios\n- Verify chunk integrity checks detect corruption reliably\n- Test retention policy execution and verify proper cleanup\n- Confirm storage performance monitoring captures relevant metrics\n- Expected behavior: Storage recovery should be automatic and complete\n\n**Milestone 5 - Multi-Tenant Debugging Verification:**\n- Test tenant isolation verification across all data and resource boundaries\n- Verify per-tenant performance metrics collection and analysis\n- Test authentication debugging with various failure scenarios\n- Confirm alert isolation prevents cross-tenant notification leakage\n- Expected behavior: Tenant isolation should be complete and verifiable\n\n#### Language-Specific Debugging Hints\n\n**Go-Specific Debugging Techniques:**\n\n- Use `go tool pprof` for CPU and memory profiling: `go tool pprof http://localhost:6060/debug/pprof/heap`\n- Enable race detection during development: `go run -race main.go`\n- Use `go tool trace` for goroutine scheduling analysis\n- Implement structured logging with `golang.org/x/exp/slog` for consistent log formatting\n- Use `context.WithTimeout` for all external operations to prevent hangs\n- Monitor goroutine counts with `runtime.NumGoroutine()` to detect goroutine leaks\n- Use `sync.Pool` for frequent allocations to reduce GC pressure\n- Implement graceful shutdown with signal handling using `os/signal`\n\n**Memory Management Best Practices:**\n\n- Use `make([]Type, 0, capacity)` to pre-allocate slices with known capacity\n- Implement object pooling for `LogEntry` and `Labels` structs to reduce allocations\n- Use `strings.Builder` instead of string concatenation for log formatting\n- Monitor heap growth with `runtime.ReadMemStats()` and set appropriate GC targets\n- Use memory-mapped files for large read-only data structures like indexes\n- Implement backpressure mechanisms to prevent unbounded memory growth during load spikes\n\n⚠️ **Pitfall: Race Conditions in Concurrent Components**\nMany debugging issues in log aggregation systems stem from race conditions between concurrent operations. For example, a query might read from an index while it's being compacted, or a retention cleanup might delete chunks while they're being accessed. Always use proper synchronization primitives (`sync.RWMutex` for read-heavy operations) and consider using channels for communication between goroutines instead of shared memory.\n\n⚠️ **Pitfall: Context Propagation in Tracing**\nFailing to properly propagate trace context through the entire pipeline makes debugging nearly impossible. Every function that processes log data should accept a `context.Context` parameter and use it for logging and tracing. Don't create new contexts unnecessarily—pass the request context through the entire pipeline to maintain trace continuity.\n\n⚠️ **Pitfall: Blocking Operations in Health Checks**\nHealth checks that perform blocking operations (network calls, disk I/O) without timeouts can make the entire system appear unhealthy. Always use `context.WithTimeout` in health check implementations and design checks to fail fast rather than hang indefinitely.\n\n⚠️ **Pitfall: Insufficient Error Context**\nGeneric error messages like \"parsing failed\" provide insufficient information for debugging in production. Always include relevant context in errors: the source of the data being parsed, the specific parsing rule that failed, and enough information to reproduce the issue. Use `fmt.Errorf` with proper error wrapping to maintain error context through the call stack.\n\n\n## Future Extensions\n\n> **Milestone(s):** This section provides a strategic roadmap for system evolution beyond the basic implementation, showing how the architecture from Milestones 1-5 can scale horizontally and accommodate advanced features while maintaining the core design principles.\n\n### Mental Model: The City Planning System\n\nThink of our log aggregation system like a growing city that starts as a small town but needs to expand into a metropolitan area. Just as urban planners must design neighborhoods that can accommodate population growth while adding new services like mass transit, shopping districts, and specialized facilities, we need to plan how our log system can scale from handling thousands of logs per second to millions, while adding sophisticated capabilities like machine learning and advanced analytics.\n\nThe key insight is that good urban planning requires **zoning** (separating different functions into appropriate areas) and **infrastructure** (ensuring roads, utilities, and services can handle growth). Similarly, our system extensions require careful partitioning strategies and foundational infrastructure that can support advanced features without compromising the core logging functionality we've built.\n\n### Scalability Extensions\n\nThe current architecture supports vertical scaling well, but real production deployments require horizontal scaling across multiple machines. This section explores how our single-node design can evolve into a distributed system while preserving the query semantics and operational simplicity that make it valuable.\n\n#### Horizontal Scaling Architecture\n\nThe transition from single-node to distributed deployment requires breaking our monolithic components into specialized services that can be deployed independently. The key challenge is maintaining data consistency and query performance across multiple nodes while avoiding the complexity pitfalls that plague many distributed systems.\n\n**Distributed Component Architecture:**\n\n| Component | Current Role | Distributed Role | Scaling Strategy |\n|-----------|--------------|------------------|------------------|\n| Ingestion Engine | Single HTTP/TCP/UDP receiver | Multiple gateway nodes behind load balancer | Stateless horizontal scaling with sticky sessions for TCP |\n| Index Engine | Single inverted index | Distributed hash table across nodes | Consistent hashing by label hash with replication factor |\n| Storage Engine | Local disk chunks | Object storage with local cache | S3-compatible backend with multi-tier caching |\n| Query Engine | Single query coordinator | Distributed query planning with fanout | Query coordinator routes to relevant nodes based on time/labels |\n| Tenant Management | In-memory tenant state | Distributed tenant configuration service | Replicated configuration with gossip protocol for updates |\n\nThe distributed architecture maintains the same external APIs while internally routing operations across multiple nodes. Query processing becomes more complex because a single query might need to scatter across many nodes and gather results, similar to how map-reduce operations work in big data systems.\n\n> **Critical Design Principle**: Preserve query semantics across the distributed transition. A LogQL query should return the same results whether it runs on a single node or distributed across 100 nodes, just with different performance characteristics.\n\n#### Sharding Strategies\n\nEffective sharding determines how data distributes across nodes and directly impacts query performance. Our system has three primary sharding dimensions: time, labels, and tenants. The optimal strategy depends on query patterns and data characteristics.\n\n**Time-Based Sharding:**\n\nTime-based sharding aligns naturally with log data's temporal nature and retention policies. Each node owns specific time ranges, making time-range queries very efficient but potentially creating hot spots during peak ingestion periods.\n\n| Sharding Approach | Query Efficiency | Write Distribution | Operational Complexity |\n|-------------------|------------------|-------------------|------------------------|\n| Daily Shards | Excellent for day-range queries | Uneven during daily peaks | Simple routing logic |\n| Hourly Shards | Good for short-range queries | Better distribution | More shards to manage |\n| Rolling Windows | Balanced across time ranges | Even distribution | Complex rebalancing |\n\nTime-based sharding works well when most queries target recent data (last 24 hours) and retention policies naturally expire old shards. However, queries spanning long time ranges must fan out to many nodes, potentially overwhelming the query coordinator.\n\n**Label-Based Sharding:**\n\nLabel-based sharding distributes data by hashing label combinations, ensuring related log streams stay on the same node. This optimization makes label-specific queries very fast but requires careful management of label cardinality to avoid skewed distributions.\n\nThe sharding key selection critically impacts performance. Using high-cardinality labels like `request_id` creates good distribution but breaks stream locality. Using low-cardinality labels like `service_name` maintains locality but risks hot spots for popular services.\n\n> **Decision: Hybrid Time-Label Sharding**\n> - **Context**: Pure time sharding creates hot spots, pure label sharding complicates time-range queries\n> - **Options Considered**: Time-only sharding, label-only sharding, hybrid approach, consistent hashing\n> - **Decision**: Hybrid sharding using time ranges as primary dimension with label hashing as secondary\n> - **Rationale**: Time ranges naturally align with retention policies and query patterns, while label hashing within time ranges provides load distribution and query optimization\n> - **Consequences**: Enables efficient time-range queries while preventing hot spots, but requires more complex routing logic and rebalancing procedures\n\n**Tenant-Based Sharding:**\n\nMulti-tenant deployments benefit from tenant-based sharding that provides natural isolation boundaries and simplified quota enforcement. Large tenants can occupy dedicated nodes while smaller tenants share resources.\n\n| Tenant Strategy | Isolation Level | Resource Efficiency | Operational Overhead |\n|-----------------|-----------------|-------------------|---------------------|\n| Tenant-per-Node | Perfect isolation | Low for small tenants | High node management |\n| Hash-Based Sharing | Good isolation | High efficiency | Complex quota tracking |\n| Size-Based Placement | Balanced approach | Optimal for mixed workloads | Dynamic rebalancing needed |\n\n#### Data Replication and Consistency\n\nDistributed deployments require replication for both availability and performance. The replication strategy affects data consistency guarantees, query performance, and operational complexity.\n\n**Replication Topologies:**\n\nThe system supports multiple replication approaches depending on consistency requirements and performance goals. Asynchronous replication provides better write performance but risks data loss during node failures. Synchronous replication guarantees consistency but increases write latency.\n\n| Replication Type | Consistency Guarantee | Write Performance | Read Performance | Failure Recovery |\n|------------------|----------------------|-------------------|------------------|------------------|\n| Async Primary-Secondary | Eventually consistent | High | Excellent | Potential data loss |\n| Sync Primary-Secondary | Strong consistency | Moderate | Good | Full recovery |\n| Multi-Primary | Eventual with conflicts | High | Excellent | Complex conflict resolution |\n| Quorum-Based | Tunable consistency | Configurable | Configurable | Automatic failover |\n\nFor log aggregation workloads, **asynchronous replication with write-ahead logging** provides the best balance. Logs are immutable once written, eliminating most consistency concerns, while the WAL ensures durability even if replication lags.\n\n**Consensus Integration:**\n\nCritical system metadata (tenant configurations, shard assignments, schema definitions) requires strong consistency through a consensus protocol like Raft. This ensures all nodes have a consistent view of cluster state while allowing log data itself to use simpler replication.\n\nThe consensus layer manages cluster membership, shard assignments, and configuration changes. It operates independently of the log data path to avoid becoming a bottleneck during high ingestion rates.\n\n#### Auto-Scaling and Load Management\n\nProduction deployments need automatic scaling to handle traffic variations and node failures. The scaling system monitors cluster health and adjusts capacity based on configurable policies.\n\n**Scaling Metrics and Triggers:**\n\n| Metric Category | Key Indicators | Scale-Up Triggers | Scale-Down Triggers |\n|-----------------|---------------|-------------------|-------------------|\n| Ingestion Load | Messages/sec, bytes/sec | >80% capacity for 10 minutes | <30% capacity for 30 minutes |\n| Query Performance | P99 latency, queue depth | P99 >5 seconds consistently | P99 <1 second consistently |\n| Resource Utilization | CPU, memory, disk I/O | >70% utilization sustained | <20% utilization sustained |\n| Storage Growth | Disk usage rate, retention efficiency | >85% disk usage | Retention policies sufficient |\n\nThe auto-scaler considers multiple factors simultaneously to avoid thrashing. Scaling decisions use exponential backoff and require sustained metric violations rather than reacting to temporary spikes.\n\n**Node Addition and Removal:**\n\nAdding nodes to a running cluster requires careful orchestration to maintain availability during the transition. The system uses a gradual migration approach that moves shards incrementally while continuing to serve queries.\n\n1. **Node Preparation**: New nodes join the cluster in \"joining\" state and receive cluster metadata\n2. **Shard Assignment**: Cluster coordinator calculates optimal shard redistribution\n3. **Data Migration**: Existing nodes stream relevant data to new nodes while continuing ingestion\n4. **Query Routing Update**: Load balancers gradually shift traffic to include new nodes\n5. **Cleanup**: Original nodes delete migrated data and update local state\n\nNode removal follows the reverse process, ensuring all data replicates to remaining nodes before the departing node stops serving traffic.\n\n### Feature Extensions\n\nBeyond horizontal scaling, the system architecture can accommodate sophisticated features that transform it from a basic log storage system into an intelligent observability platform. These extensions leverage the solid foundation of ingestion, indexing, and querying while adding new capabilities.\n\n#### Advanced Querying Capabilities\n\nThe basic LogQL implementation provides foundation for much more sophisticated query operations that rival specialized analytics systems. These extensions maintain backward compatibility while enabling complex analysis workflows.\n\n**Aggregation and Analytics Functions:**\n\n| Function Category | Examples | Use Cases | Implementation Approach |\n|------------------|----------|-----------|-------------------------|\n| Statistical Functions | `avg()`, `sum()`, `percentile()`, `stddev()` | Performance monitoring, SLA tracking | Stream processing with sliding windows |\n| Time Series Analysis | `rate()`, `increase()`, `trend()`, `seasonal()` | Capacity planning, anomaly detection | Time-bucketed aggregation with interpolation |\n| Text Analytics | `extract()`, `sentiment()`, `classify()` | Log content analysis, issue categorization | Regex engines, ML model integration |\n| Geospatial Functions | `geo_distance()`, `geo_within()`, `geo_cluster()` | Location-based analysis, CDN optimization | Geohash indexing, spatial data structures |\n\nAdvanced aggregation functions operate on log streams using streaming algorithms that maintain accuracy while processing large volumes. The system uses approximate algorithms (HyperLogLog for cardinality, t-digest for percentiles) when exact computation would be prohibitively expensive.\n\n**Cross-Stream Joins and Correlations:**\n\nReal observability requires correlating logs across different services and systems. The query engine extends to support temporal joins that match log entries from different streams based on time windows and common attributes.\n\n```\n{service=\"api\"} |= \"request_id=123\" \n  | join 5m {service=\"database\"} on request_id\n  | calculate response_time\n```\n\nThis query finds API logs for request ID 123, joins them with corresponding database logs within a 5-minute window, and calculates end-to-end response time. The join operation requires careful memory management and timeout handling to avoid resource exhaustion on large result sets.\n\n**Saved Queries and Materialized Views:**\n\nFrequently-executed complex queries benefit from materialization that pre-computes results and incrementally updates them as new data arrives. This feature transforms expensive analytics queries into fast lookup operations.\n\n| Materialization Type | Update Strategy | Query Performance | Storage Overhead | Consistency Model |\n|---------------------|----------------|-------------------|------------------|-------------------|\n| Periodic Refresh | Batch recalculation | Fast reads | Low | Eventually consistent |\n| Incremental Update | Stream processing | Fast reads | Moderate | Near real-time |\n| Trigger-Based | Event-driven | Fast reads | High | Strongly consistent |\n\n#### Machine Learning Integration\n\nThe log aggregation system becomes a platform for intelligent log analysis by integrating machine learning capabilities directly into the query and storage pipeline. This integration provides automated insights without requiring data export to external ML systems.\n\n**Anomaly Detection Pipeline:**\n\nAutomated anomaly detection operates continuously on log streams to identify unusual patterns that might indicate system problems or security incidents. The detection pipeline uses multiple algorithms tuned for different anomaly types.\n\n| Anomaly Type | Detection Algorithm | Training Requirements | False Positive Rate | Response Time |\n|--------------|-------------------|----------------------|-------------------|---------------|\n| Volume Spikes | Statistical process control | 7 days baseline | ~2% | Real-time |\n| Content Anomalies | Clustering/classification | Labeled training set | ~5% | Near real-time |\n| Temporal Patterns | Time series decomposition | 30 days seasonal data | ~1% | Minutes |\n| Cross-Service Correlations | Graph neural networks | Service dependency map | ~3% | Minutes |\n\nThe anomaly detection system learns normal patterns during a training period and then continuously scores incoming logs for deviation from expected behavior. It generates alerts through the existing alerting infrastructure while maintaining detailed model performance metrics.\n\n**Log Classification and Tagging:**\n\nAutomated log classification adds semantic tags to log entries based on content analysis and pattern recognition. This capability transforms unstructured log messages into structured, searchable metadata.\n\nThe classification pipeline processes log messages through multiple stages:\n\n1. **Preprocessing**: Text normalization, tokenization, and feature extraction from log messages\n2. **Model Application**: Pre-trained models identify log types (error, warning, security event, performance metric)\n3. **Confidence Scoring**: Classification confidence determines whether automatic tagging applies\n4. **Human-in-Loop**: Low-confidence classifications route to human reviewers for model improvement\n5. **Tag Application**: High-confidence classifications automatically add structured labels to log entries\n\nThe system supports both general-purpose models (common log patterns across many applications) and tenant-specific models trained on organization-specific log formats and terminology.\n\n**Predictive Analytics:**\n\nMachine learning models can predict future system behavior based on historical log patterns. This capability enables proactive incident response and capacity planning.\n\n| Prediction Type | Input Features | Prediction Horizon | Model Type | Update Frequency |\n|-----------------|---------------|-------------------|------------|------------------|\n| Service Failures | Error rates, performance metrics | 30 minutes | Random Forest | Hourly |\n| Capacity Exhaustion | Resource usage trends | 7 days | LSTM Neural Network | Daily |\n| Security Incidents | Access patterns, anomaly scores | 1 hour | Gradient Boosting | Continuous |\n| User Experience Impact | Response times, error distributions | 15 minutes | Ensemble Model | Every 5 minutes |\n\nPredictive models integrate with the alerting system to generate proactive notifications when predicted events exceed configured probability thresholds. This enables operations teams to address potential problems before they impact users.\n\n#### Real-Time Analytics Dashboard\n\nThe query engine extends to support real-time dashboard queries that provide live system visibility through streaming aggregations and visualizations. This capability transforms the log system into a comprehensive observability platform.\n\n**Streaming Aggregation Engine:**\n\nReal-time dashboards require continuous aggregation of log streams to provide up-to-date metrics without overwhelming query performance. The streaming engine maintains sliding window aggregations in memory while persisting longer-term aggregations to storage.\n\n| Window Type | Memory Usage | Update Latency | Historical Retention | Query Performance |\n|-------------|--------------|----------------|---------------------|------------------|\n| Tumbling Windows | Low | Batch interval | Full history in storage | Fast |\n| Sliding Windows | High | Near real-time | Limited in memory | Very fast |\n| Session Windows | Variable | Event-driven | Session-based retention | Fast |\n\nThe streaming aggregation engine uses approximate algorithms to maintain efficiency at scale. HyperLogLog provides cardinality estimates, Count-Min Sketch tracks frequent items, and t-digest maintains quantile estimates, all with bounded memory usage regardless of data volume.\n\n**Dashboard Query Language:**\n\nDashboard queries extend LogQL with real-time aggregation functions and visualization hints that guide rendering systems in presenting data effectively.\n\nAdvanced dashboard queries support multi-dimensional drill-down capabilities where users can explore data by clicking on chart elements. The query engine maintains query context and generates appropriate filtered queries for the selected data slice.\n\n#### Data Export and Integration APIs\n\nProduction log systems rarely operate in isolation. The architecture supports comprehensive data export and integration capabilities that connect with existing observability tools, business intelligence systems, and compliance platforms.\n\n**Export Pipeline Architecture:**\n\n| Export Target | Data Format | Delivery Method | Latency | Reliability Guarantees |\n|---------------|-------------|-----------------|---------|----------------------|\n| Data Warehouses | Parquet, ORC | Batch file transfer | Hours | Exactly-once delivery |\n| Stream Processors | JSON, Avro | Kafka, Pulsar | Seconds | At-least-once delivery |\n| Alerting Systems | JSON, XML | HTTP webhooks | Real-time | Best-effort delivery |\n| Compliance Archives | Raw logs, JSON | S3, tape backup | Minutes | Guaranteed delivery |\n\nThe export pipeline operates independently of the main ingestion and query paths to avoid impacting core system performance. It maintains separate queues and processing threads while sharing the same underlying storage infrastructure.\n\n**API Gateway Integration:**\n\nRESTful APIs enable external systems to query log data programmatically while maintaining security and rate limiting controls. The API gateway provides authentication, authorization, and request transformation capabilities.\n\nAPI clients can subscribe to log streams using WebSocket connections that deliver matching log entries in real-time. This capability enables external monitoring systems to react immediately to critical events without polling for updates.\n\n**Webhook and Event Streaming:**\n\nThe system supports outbound event streaming that publishes log events to external systems based on configurable rules. This capability enables integration with incident response platforms, notification systems, and business process automation tools.\n\nEvent filtering rules use the same LogQL syntax as interactive queries, ensuring consistency between interactive analysis and automated integrations. The webhook delivery system includes retry logic, dead letter queues, and delivery confirmation to ensure reliable event delivery.\n\n### Implementation Guidance\n\nThis section provides practical guidance for implementing the scalability and feature extensions described above. The focus is on incremental evolution of the existing single-node system rather than complete architectural rewrites.\n\n#### Technology Recommendations\n\n| Extension Category | Simple Approach | Advanced Approach |\n|-------------------|------------------|------------------|\n| Horizontal Scaling | Docker Swarm with shared storage | Kubernetes with distributed storage (Ceph/GlusterFS) |\n| Service Discovery | Static configuration files | Consul/etcd with health checking |\n| Load Balancing | HAProxy/nginx round-robin | Envoy with advanced routing and circuit breaking |\n| Message Queuing | Redis pub/sub | Apache Kafka with partitioning |\n| Consensus Layer | Single-node SQLite | Multi-node etcd cluster |\n| Machine Learning | Scikit-learn batch processing | TensorFlow Serving with GPU acceleration |\n| Monitoring | Prometheus + Grafana | Comprehensive observability stack (Jaeger, Prometheus, Grafana, AlertManager) |\n\n#### Recommended Extension Structure\n\nThe extension implementation follows a modular approach that adds new capabilities without disrupting existing functionality:\n\n```\nproject-root/\n  cmd/\n    gateway/              ← ingestion gateway nodes\n    coordinator/          ← query coordination service\n    storage/              ← dedicated storage nodes\n  internal/\n    cluster/              ← distributed system coordination\n      consensus.go        ← raft consensus implementation\n      sharding.go         ← shard management and routing\n      membership.go       ← node discovery and health\n    ml/                   ← machine learning pipeline\n      anomaly/            ← anomaly detection models\n      classification/     ← log classification\n      prediction/         ← predictive analytics\n    extensions/           ← feature extension framework\n      dashboards/         ← real-time dashboard support\n      export/             ← data export pipeline\n      api/                ← external API gateway\n  deploy/\n    docker/               ← container deployment configs\n    k8s/                  ← kubernetes manifests\n    terraform/            ← infrastructure as code\n```\n\n#### Scalability Implementation Skeleton\n\nThe horizontal scaling implementation starts with service decomposition that separates ingestion, storage, and query concerns:\n\n```go\n// ClusterCoordinator manages distributed system state and routing decisions.\n// It uses consensus to maintain consistent shard assignments across nodes.\ntype ClusterCoordinator struct {\n    // TODO 1: Initialize consensus layer (etcd/raft) for cluster metadata\n    // TODO 2: Implement shard assignment algorithm with replication factor\n    // TODO 3: Add node health monitoring with failure detection\n    // TODO 4: Create shard migration logic for node additions/removals\n    // TODO 5: Implement query routing to appropriate shard owners\n}\n\n// DistributedQueryEngine coordinates queries across multiple storage nodes.\n// It implements scatter-gather pattern with result streaming.\nfunc (d *DistributedQueryEngine) ExecuteQuery(ctx context.Context, query string) *ResultStream {\n    // TODO 1: Parse query and extract time range and label selectors\n    // TODO 2: Determine relevant shards using cluster coordinator\n    // TODO 3: Generate sub-queries for each relevant storage node\n    // TODO 4: Execute sub-queries in parallel with timeout handling\n    // TODO 5: Merge partial results maintaining sort order and limits\n    // TODO 6: Handle node failures with partial result warnings\n}\n\n// ShardManager handles data distribution and migration across cluster nodes.\n// It ensures balanced load while maintaining query performance.\ntype ShardManager struct {\n    // TODO 1: Implement consistent hashing for shard assignment\n    // TODO 2: Add replication factor support with rack awareness\n    // TODO 3: Create migration protocol for adding/removing nodes\n    // TODO 4: Implement shard splitting for hotspot handling\n    // TODO 5: Add metrics collection for rebalancing decisions\n}\n```\n\n#### Machine Learning Integration Framework\n\nThe ML integration provides a plugin architecture that allows models to process log streams without blocking ingestion:\n\n```go\n// MLPipeline processes log streams through machine learning models.\n// It operates asynchronously to avoid impacting ingestion performance.\ntype MLPipeline struct {\n    // TODO 1: Create model registry with versioning and rollback\n    // TODO 2: Implement stream processing framework for real-time inference\n    // TODO 3: Add batch processing for model training and evaluation\n    // TODO 4: Create feedback loop for model improvement\n    // TODO 5: Implement A/B testing for model performance comparison\n}\n\n// AnomalyDetector identifies unusual patterns in log streams.\n// It maintains baseline models and generates alerts for deviations.\nfunc (a *AnomalyDetector) ProcessLogEntry(entry *LogEntry) *AnomalyScore {\n    // TODO 1: Extract features from log entry (timestamp, content, labels)\n    // TODO 2: Apply statistical models to detect volume anomalies\n    // TODO 3: Run content analysis for unusual message patterns\n    // TODO 4: Calculate composite anomaly score with confidence intervals\n    // TODO 5: Generate alerts when score exceeds configured thresholds\n    // TODO 6: Update baseline models with confirmed normal behavior\n}\n\n// ClassificationEngine adds semantic tags to log entries automatically.\n// It uses pre-trained models and human feedback for continuous improvement.\ntype ClassificationEngine struct {\n    // TODO 1: Load pre-trained classification models from storage\n    // TODO 2: Implement text preprocessing pipeline\n    // TODO 3: Apply models with confidence scoring\n    // TODO 4: Route low-confidence predictions to human reviewers\n    // TODO 5: Update models based on feedback and new training data\n}\n```\n\n#### Extension Milestone Checkpoints\n\n**Checkpoint 1: Basic Horizontal Scaling**\n- Deploy 3-node cluster with shared storage backend\n- Verify log ingestion distributes across all nodes\n- Execute queries that span multiple nodes and return correct results\n- Test node failure scenarios with automatic failover\n\n**Checkpoint 2: Advanced Sharding**\n- Implement hybrid time-label sharding with configurable policies\n- Verify queries target minimal set of relevant shards\n- Test shard rebalancing when adding/removing nodes\n- Measure query performance improvement from shard pruning\n\n**Checkpoint 3: Machine Learning Pipeline**\n- Deploy anomaly detection models on sample log streams\n- Generate alerts for injected anomalous patterns\n- Verify classification engine adds appropriate semantic tags\n- Test model updates and A/B deployment scenarios\n\n**Checkpoint 4: Real-time Analytics**\n- Create streaming dashboard with live log volume and error rates\n- Verify dashboard updates within 5 seconds of log ingestion\n- Test drill-down capabilities from dashboard visualizations\n- Measure memory usage of streaming aggregation windows\n\n#### Common Extension Pitfalls\n\n⚠️ **Pitfall: Premature Distribution**\nAvoid implementing distributed features before understanding single-node bottlenecks. Many performance problems stem from inefficient algorithms rather than resource constraints. Profile the single-node implementation thoroughly and optimize hot paths before adding distribution complexity.\n\n⚠️ **Pitfall: Inconsistent Sharding**\nChanging sharding strategies after deployment requires expensive data migration. Choose sharding approaches based on actual query patterns from production workloads, not theoretical optimization. Implement shard rebalancing from the beginning rather than adding it later.\n\n⚠️ **Pitfall: ML Model Drift**\nMachine learning models degrade over time as log patterns change. Implement continuous model evaluation and automated retraining pipelines from the beginning. Monitor model performance metrics and alert when accuracy drops below acceptable thresholds.\n\n⚠️ **Pitfall: Resource Contention**\nExtension features can interfere with core log processing if they share computational resources. Use separate thread pools, memory allocation, and I/O bandwidth for extension features. Implement circuit breakers that disable extensions during resource pressure.\n\n⚠️ **Pitfall: Data Export Bottlenecks**\nLarge-scale data export can overwhelm storage systems and impact query performance. Implement throttling, backpressure, and priority queuing for export operations. Use separate storage replicas for export workloads when possible.\n\nThe extension architecture maintains backward compatibility while enabling sophisticated capabilities. Start with simple implementations and gradually add complexity based on actual operational requirements rather than speculative feature needs.\n\n\n## Glossary\n\n> **Milestone(s):** This section provides comprehensive definitions for all technical terms, acronyms, and domain-specific vocabulary used throughout the design document, applying to all milestones (1-5).\n\n### Mental Model: The Technical Dictionary\n\nThink of this glossary as the technical dictionary for our log aggregation system - just as a good dictionary doesn't just provide word definitions but explains etymology, usage examples, and relationships between concepts, this glossary serves as your comprehensive reference for understanding not just what each term means, but how it fits into the broader system architecture and why specific terminology choices matter for clear communication.\n\nUnlike a simple word list, this glossary organizes terms by their conceptual relationships and provides context about how terms are used specifically within log aggregation systems. Each definition includes not just the meaning, but the practical implications and connections to other system components.\n\n### Core System Concepts\n\nThe foundational vocabulary that defines what we're building and why it matters.\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **log aggregation** | The process of collecting and centralizing log data from multiple distributed sources into a unified system for storage, indexing, and analysis | Central purpose of our entire system - distinguishes from simple log forwarding by emphasizing centralized processing and analysis capabilities |\n| **LogQL** | Log query language inspired by Grafana Loki that combines label selectors for stream identification with line filters for content matching | Our query interface standard - provides familiar syntax for users coming from Prometheus/Loki ecosystem |\n| **ingestion pipeline** | Sequence of processing stages that transform incoming raw log data through parsing, validation, buffering, indexing, and storage | The data transformation backbone - logs enter as raw text/JSON and exit as structured, indexed, searchable entries |\n| **time series data** | Data points indexed primarily by timestamp, where queries frequently filter by time ranges | Logs are inherently time-ordered, making temporal indexing and partitioning critical for performance |\n| **structured logging** | Log format where each entry contains well-defined fields (timestamp, level, service, message) rather than free-form text | Enables efficient parsing and indexing - our system converts unstructured logs into this format |\n| **label** | Key-value metadata pair attached to log entries (e.g., service=api, level=error) that enables filtering and grouping | Primary indexing and querying mechanism - distinguishes our approach from full-text search engines |\n| **stream** | Sequence of log entries sharing identical label sets, representing logs from a specific source configuration | Fundamental unit of log organization - queries select streams first, then filter within them |\n\n### Data Structures and Storage\n\nTerms describing how we organize and persist log data.\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **LogEntry** | Core data structure containing timestamp, labels map, and message string representing a single log event | Central data type flowing through entire system from ingestion to query response |\n| **Labels** | Map[string]string containing metadata key-value pairs that identify and categorize log entries | Primary mechanism for log organization and query filtering - kept separate from message content |\n| **LogStream** | Collection of log entries sharing identical label sets, representing a distinct log source | Organizational unit that groups related log entries for efficient storage and querying |\n| **chunk** | Time-windowed compressed storage unit containing multiple log entries, typically covering 1-hour to 1-day periods | Physical storage unit that balances compression efficiency with query performance |\n| **chunk boundaries** | Time windows that define how logs are grouped into chunks, affecting compression ratios and query performance | Critical for balancing storage efficiency (larger chunks compress better) with query latency (smaller chunks reduce scan overhead) |\n| **TimeRange** | Data structure defining start and end timestamps for queries or storage operations | Enables time-based query optimization and storage partitioning |\n| **EntryReference** | Pointer structure containing chunk ID, offset, and timestamp for locating specific log entries within storage | Enables inverted index to reference actual log content without duplicating data |\n| **stream-level compression** | Technique of compressing log streams separately within chunks to improve compression ratios for similar content | Exploits similarity within streams (same service generates similar log patterns) for better compression |\n| **retention policy** | Configurable rules defining how long logs are stored before automatic deletion based on age, size, or other criteria | Manages storage growth and compliance requirements - can be global or per-tenant/stream |\n\n### Indexing and Search\n\nVocabulary for our search and indexing mechanisms.\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **inverted index** | Data structure mapping each unique term to the list of documents (log entries) containing that term | Enables fast full-text search by avoiding sequential scans through all log content |\n| **PostingsList** | Array of entry references for a specific term in the inverted index, sorted by timestamp | Core index data structure that allows term lookups to return relevant log entries efficiently |\n| **bloom filter** | Probabilistic data structure that can quickly determine if an element is definitely NOT in a set | Optimizes negative lookups - if bloom filter says term isn't present, skip expensive index lookup |\n| **false positive** | When bloom filter incorrectly reports that an element might be present in the set | Acceptable trade-off - requires verification against actual index, but false positives are rare with proper tuning |\n| **false negative** | Bloom filter incorrectly reporting that a present element is absent - impossible by design | Mathematical impossibility for bloom filters - they never miss actual matches, only add extra candidates |\n| **label cardinality** | Number of unique values for a specific label key across all log entries | Critical metric for index sizing - high cardinality labels (like request_id) can cause index explosion |\n| **cardinality explosion** | Exponential growth of unique label combinations that overwhelms indexing and storage systems | Primary performance threat - must limit high-cardinality labels or use specialized handling |\n| **time-based partitioning** | Strategy of organizing indexes and storage by time windows to enable efficient time-range queries | Allows queries to scan only relevant time periods instead of entire dataset |\n| **IndexSegment** | Self-contained portion of inverted index covering specific time range with its own terms map and bloom filters | Unit of index management that enables parallel processing and incremental updates |\n\n### Query Processing\n\nTerms related to how we parse, optimize, and execute queries.\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **AST** | Abstract syntax tree representing the parsed structure of a LogQL query with operators and operands | Intermediate representation that enables query optimization and execution planning |\n| **label selector** | LogQL component that specifies which log streams to query using label matching criteria | First stage of query execution - narrows search space before applying content filters |\n| **line filter** | LogQL component that matches against log message content using text search, regex, or other patterns | Second stage that examines actual log content within selected streams |\n| **predicate pushdown** | Query optimization technique that moves filter conditions as early as possible in the execution pipeline | Reduces data processed at each stage by eliminating irrelevant entries early |\n| **cursor-based pagination** | Pagination approach using opaque position tokens instead of numeric offsets for efficient large result traversal | Handles large result sets without expensive offset-based scanning |\n| **streaming execution** | Processing query results incrementally without materializing the complete result set in memory | Enables bounded memory usage for large queries and faster time-to-first-result |\n| **Token** | Lexical unit produced by query parser representing keywords, operators, strings, or identifiers | Building blocks of query parsing - lexer converts input string into token sequence |\n| **Lexer** | Component that breaks LogQL query strings into tokens for parsing | First stage of query processing that handles syntax recognition and basic validation |\n| **ResultStream** | Channel-based structure for streaming query results back to clients with metadata | Enables incremental result delivery and client-controlled result consumption |\n\n### Network and Protocol Terms\n\nVocabulary for how logs enter our system.\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **syslog** | Standardized network protocol (RFC 3164/5424) for transmitting log messages over TCP/UDP | Industry standard for log transmission - our system must parse both legacy and modern syslog formats |\n| **backpressure** | Flow control mechanism activated when downstream components cannot keep up with incoming request rate | Prevents system overload by slowing or buffering inputs when processing capacity is exceeded |\n| **ring buffer** | Circular buffer data structure with fixed capacity that overwrites oldest entries when full | Provides bounded memory buffering for log ingestion with predictable memory usage |\n| **buffering** | Temporary storage of log entries to handle burst traffic and smooth out processing load | Critical for handling traffic spikes - prevents dropping logs during temporary overload |\n| **protocol handler** | Component responsible for receiving and parsing log data from specific network protocols (HTTP, TCP, UDP) | Separates protocol concerns from log processing logic - enables supporting multiple ingestion methods |\n\n### Storage and Persistence\n\nTerms describing how we durably store log data.\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **write-ahead log** | Durable transaction log that records intended operations before they're applied to main storage | Ensures no data loss during crashes by allowing replay of committed but unapplied operations |\n| **WAL rotation** | Process of creating new WAL files when current files exceed size limits | Prevents unbounded WAL growth while maintaining crash recovery capabilities |\n| **WAL replay** | Recovery process that reads WAL records after crash and reapplies any committed operations | Critical for maintaining data durability guarantees across system failures |\n| **compression ratio** | Percentage of storage space saved through compression algorithms | Key metric for storage efficiency - log data often achieves 70-90% compression ratios |\n| **retention cleanup** | Automated process that deletes expired chunks according to configured retention policies | Manages storage growth by removing old data - must coordinate with active queries |\n| **grace period** | Delay between retention policy triggering and actual data deletion | Safety mechanism that allows recovery from accidental retention policy changes |\n| **reference counting** | Technique tracking how many active queries are accessing each chunk to prevent unsafe deletion | Prevents data corruption by ensuring chunks aren't deleted while being read |\n| **ChunkHeader** | Metadata structure containing compression type, entry counts, time ranges, and other chunk information | Enables efficient chunk processing without decompressing entire contents |\n| **WALRecord** | Individual entry in write-ahead log containing operation type, timestamp, and operation data | Unit of durability - each record represents one atomic operation that can be replayed |\n\n### Multi-Tenancy and Security\n\nTerms for isolating different users and organizations.\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **multi-tenancy** | Architecture pattern that securely isolates data and resources between different organizations or teams | Enables SaaS deployments where multiple customers share infrastructure without data leakage |\n| **tenant isolation** | Security mechanisms ensuring that one tenant cannot access another tenant's logs or affect their performance | Fundamental security requirement - prevents data breaches and noisy neighbor problems |\n| **rate limiting** | Mechanism controlling the frequency of requests to prevent resource exhaustion | Protects system stability by preventing any single tenant from overwhelming shared resources |\n| **token bucket** | Algorithm that allows burst traffic while enforcing sustained rate limits over time | Balances flexibility (allowing occasional spikes) with protection (preventing sustained overload) |\n| **hierarchical rate limiting** | Multi-level rate limiting applied at tenant, stream, and system levels | Provides granular control over resource allocation at different organizational levels |\n| **TenantContext** | Data structure containing tenant ID, permissions, quotas, and other authorization information | Carries tenant information through request processing pipeline for access control decisions |\n| **ResourceQuotas** | Limits on ingestion rate, storage usage, query concurrency, and other resources per tenant | Prevents resource monopolization and enables predictable service levels |\n| **tenant ID injection** | Security attack where malicious clients manipulate tenant identification to access other tenants' data | Critical vulnerability requiring careful authentication and authorization validation |\n\n### Alerting and Monitoring\n\nTerms for detecting and responding to log patterns.\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **log-based alerting** | System that triggers notifications based on patterns detected in incoming log streams | Enables proactive incident response by detecting problems through log analysis |\n| **alert deduplication** | Process preventing multiple notifications for the same underlying issue | Reduces alert fatigue by grouping related alerts and suppressing duplicates |\n| **ThresholdCondition** | Configuration specifying what log patterns trigger alerts (error rate, specific messages, etc.) | Defines the logic for when alerts should fire based on log content or frequency |\n| **AlertRule** | Complete alert specification including query, condition, notification settings, and metadata | Unit of alert configuration that can be enabled/disabled and modified independently |\n| **alert storm** | Cascading alert generation that overwhelms notification systems with excessive messages | Dangerous failure mode that can mask real issues - prevented through deduplication and rate limiting |\n| **SlidingWindow** | Time-based buffer that tracks events over a moving time period for alert evaluation | Enables time-based alert conditions like \"more than 10 errors in 5 minutes\" |\n\n### Reliability and Error Handling\n\nTerms for building robust, fault-tolerant systems.\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **graceful degradation** | System behavior that maintains reduced functionality when components fail | Preferred failure mode that keeps serving core functionality even with partial outages |\n| **circuit breaker** | Design pattern that prevents cascading failures by failing fast when downstream services are unhealthy | Protects system stability by avoiding expensive operations that are likely to fail |\n| **health check** | Automated verification that a component is functioning correctly and can serve requests | Enables monitoring systems to detect problems and route traffic away from failing components |\n| **split-brain** | Scenario where network partitions create multiple independent views of system state | Dangerous condition requiring careful design to prevent data corruption and conflicts |\n| **cascade failure** | Failure propagation pattern where one component failure causes other components to fail | System-level failure mode requiring circuit breakers, bulkheads, and other resilience patterns |\n| **HealthStatus** | Enumeration representing component health states (healthy, degraded, unhealthy) | Standardized health representation enabling consistent monitoring and alerting |\n| **CheckResult** | Data structure containing health check outcomes with status, message, and timing information | Provides detailed health information for diagnostics and automated response |\n\n### Testing and Debugging\n\nTerms for verifying system correctness and diagnosing issues.\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **unit testing** | Component-level testing that verifies individual functions and classes in isolation | Foundation of testing pyramid - catches bugs early and enables confident refactoring |\n| **integration testing** | End-to-end testing that validates interactions between multiple components | Verifies that components work together correctly beyond just individual unit correctness |\n| **milestone checkpoints** | Verification points after each development phase confirming expected functionality | Structured approach to incremental development with clear success criteria |\n| **property-based testing** | Testing approach using randomly generated inputs to verify system invariants | Finds edge cases that fixed test cases might miss - especially valuable for parsers and indexes |\n| **failure injection** | Testing technique that simulates error conditions to verify recovery mechanisms | Ensures system behaves correctly under failure conditions, not just happy path scenarios |\n| **log flow tracing** | Debugging technique tracking individual log entries through the entire processing pipeline | Essential for diagnosing data loss, corruption, or performance bottlenecks |\n| **index health diagnostics** | Specialized debugging procedures for index corruption and performance issues | Domain-specific debugging addressing common index problems like cardinality explosion |\n| **throughput cascade analysis** | Performance debugging technique measuring throughput at each pipeline stage | Identifies bottlenecks by comparing input/output rates across system components |\n| **latency percentile analysis** | Performance analysis using different percentile measurements to identify performance patterns | Reveals different types of performance issues - p50 vs p99 problems often have different causes |\n\n### Performance and Scalability\n\nTerms describing system performance characteristics and optimization approaches.\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **horizontal scaling** | Approach to handling increased load by adding more machines rather than upgrading existing ones | Enables unlimited scalability by distributing load across multiple nodes |\n| **sharding** | Data partitioning technique that distributes data across multiple storage nodes | Enables parallel processing and storage distribution for large datasets |\n| **scatter-gather** | Query execution pattern that distributes work across multiple nodes and collects results | Common pattern in distributed systems for parallelizing query execution |\n| **consensus protocol** | Algorithm ensuring that distributed nodes agree on shared state despite failures | Required for maintaining data consistency across multiple storage nodes |\n| **replication factor** | Number of copies of each piece of data maintained across different nodes | Balances availability (more copies survive failures) with storage cost and write complexity |\n| **materialized views** | Pre-computed query results that are updated incrementally as new data arrives | Optimization technique for frequently-accessed aggregate queries |\n| **streaming aggregation** | Continuous computation over data streams that maintains running totals and statistics | Enables real-time analytics without storing all individual events |\n\n### Advanced Features and Extensions\n\nTerms for sophisticated functionality beyond basic log aggregation.\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **anomaly detection** | Machine learning technique for identifying unusual patterns in log streams | Advanced analytics capability that can detect incidents before they cause user-visible problems |\n| **model drift** | Machine learning model accuracy degradation over time as data patterns change | Common problem requiring model retraining and performance monitoring |\n| **semantic tagging** | Automatic addition of meaningful labels based on log content analysis | Uses ML to enhance log metadata beyond what's explicitly provided by log sources |\n| **AnomalyScore** | Data structure containing anomaly probability, confidence level, and contributing factors | Structured output from anomaly detection that enables automated response and human investigation |\n| **ClassificationEngine** | Component that automatically categorizes log entries into predefined classes | Enables automatic log organization and filtering based on content analysis |\n\n### Implementation and Architecture\n\nTerms describing how the system is structured and built.\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **Architecture Decision Record** | Structured documentation capturing the context, options, and rationale behind design choices | Critical for maintaining design knowledge and understanding trade-offs made during development |\n| **component isolation** | Design principle ensuring components have clear boundaries and minimal coupling | Enables independent development, testing, and replacement of system parts |\n| **interface segregation** | Principle of defining focused interfaces rather than large monolithic ones | Makes components more testable and reduces coupling between system parts |\n| **dependency injection** | Pattern where components receive their dependencies rather than creating them internally | Enables easier testing by allowing mock dependencies and clearer dependency relationships |\n| **configuration management** | System for managing application settings, connection strings, and other operational parameters | Critical for deployment flexibility and environment-specific customization |\n\n### Common Acronyms and Abbreviations\n\nStandard abbreviations used throughout the system documentation.\n\n| Acronym | Full Form | Definition |\n|---------|-----------|------------|\n| **WAL** | Write-Ahead Log | Durable transaction log ensuring no data loss during failures |\n| **ADR** | Architecture Decision Record | Structured documentation of design decisions and their rationale |\n| **AST** | Abstract Syntax Tree | Parsed representation of query structure used for optimization and execution |\n| **TCP** | Transmission Control Protocol | Reliable network protocol used for syslog and other log transmission |\n| **UDP** | User Datagram Protocol | Unreliable but fast network protocol used for high-volume log transmission |\n| **HTTP** | HyperText Transfer Protocol | Web protocol used for REST API log ingestion endpoints |\n| **JSON** | JavaScript Object Notation | Text-based data interchange format commonly used for structured logs |\n| **RFC** | Request for Comments | Internet standards documents (RFC 3164/5424 define syslog formats) |\n| **TTL** | Time To Live | Duration data should be retained before automatic deletion |\n| **SLA** | Service Level Agreement | Contractual commitment to specific performance and availability levels |\n| **SLI** | Service Level Indicator | Measurable metric used to assess service performance |\n| **SLO** | Service Level Objective | Target value for service level indicators |\n| **MTBF** | Mean Time Between Failures | Average time between system failures |\n| **MTTR** | Mean Time To Recovery | Average time to restore service after failure |\n| **RTO** | Recovery Time Objective | Maximum acceptable downtime during disaster recovery |\n| **RPO** | Recovery Point Objective | Maximum acceptable data loss during disaster recovery |\n\n### Constants and Configuration Values\n\nStandard values and limits used throughout the system.\n\n| Constant | Value | Purpose |\n|----------|-------|---------|\n| **HTTP_PORT** | 8080 | Default port for HTTP log ingestion endpoint |\n| **TCP_PORT** | 1514 | Default port for TCP syslog reception |\n| **UDP_PORT** | 1514 | Default port for UDP syslog reception |\n| **BUFFER_SIZE** | 10000 | Default size for in-memory log entry buffers |\n| **CHUNK_SIZE** | 1MB | Target size for compressed storage chunks |\n| **RETENTION_DAYS** | 30 | Default log retention period |\n| **REPLICATION_FACTOR** | 3 | Default number of data replicas in distributed deployment |\n| **ML_BATCH_SIZE** | 1000 | Batch size for machine learning model inference |\n| **ANOMALY_THRESHOLD** | 0.95 | Default threshold for anomaly detection alerts |\n\n### Design Patterns and Principles\n\nFundamental patterns used throughout the system architecture.\n\n| Pattern | Description | Application |\n|---------|-------------|-------------|\n| **Publisher-Subscriber** | Decoupled communication where producers send events without knowing consumers | Used for log ingestion pipeline stages and alert notification delivery |\n| **Command Pattern** | Encapsulating operations as objects that can be queued, logged, and undone | WAL records represent commands that can be replayed for crash recovery |\n| **Observer Pattern** | Automatic notification of dependent objects when state changes | Health monitoring and metrics collection throughout the system |\n| **Factory Pattern** | Creating objects without specifying their exact classes | Parser factory creates appropriate parsers based on log format detection |\n| **Strategy Pattern** | Selecting algorithms at runtime based on context | Compression algorithm selection and query optimization strategies |\n| **Circuit Breaker Pattern** | Preventing cascading failures by failing fast when services are unhealthy | Protecting against downstream service failures and resource exhaustion |\n| **Bulkhead Pattern** | Isolating resources to prevent total system failure | Tenant isolation and resource quotas prevent noisy neighbor problems |\n\n### Performance Metrics and Measurements\n\nKey metrics for understanding system behavior and performance.\n\n| Metric Category | Examples | Purpose |\n|-----------------|----------|---------|\n| **Throughput** | logs/second, queries/second, bytes/second | Measures system capacity and utilization |\n| **Latency** | p50/p95/p99 response times, ingestion delay | Measures user experience and system responsiveness |\n| **Resource Usage** | CPU%, memory usage, disk I/O, network bandwidth | Identifies bottlenecks and capacity planning needs |\n| **Error Rates** | failed ingestions, query errors, timeout rates | Measures system reliability and quality |\n| **Business Metrics** | active tenants, storage growth, query complexity | Tracks system adoption and resource requirements |\n\n### Implementation Guidance\n\nThe glossary serves as more than just definitions - it establishes the vocabulary foundation that enables clear communication throughout the project. Understanding these terms deeply helps in several ways: they provide the conceptual framework for understanding system architecture, establish consistent naming conventions that make code more readable, enable precise communication about design decisions and trade-offs, and help identify relationships between different system components.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Documentation | Markdown files in repository | GitBook or similar documentation platform |\n| Term Management | Manual glossary maintenance | Automated glossary generation from code comments |\n| Cross-References | Manual linking between terms | Automated cross-reference detection and linking |\n\n#### Recommended File Structure\n\n```\ndocs/\n  glossary.md              ← this comprehensive glossary\n  architecture/\n    data-model.md          ← detailed data structure documentation\n    component-interfaces.md ← API and interface specifications\n  operations/\n    debugging-guide.md     ← troubleshooting procedures\n    performance-tuning.md  ← optimization techniques\n```\n\n#### Glossary Maintenance Guidelines\n\nMaintaining a comprehensive glossary requires ongoing attention as the system evolves. New features introduce new terminology that must be clearly defined and consistently used. Existing definitions may need refinement as understanding deepens or requirements change. Cross-references between terms should be verified and updated when related concepts change.\n\nThe glossary should be treated as a living document that grows with the system. Each new component or feature should contribute its key terms with clear definitions. Ambiguous or overloaded terms should be identified and disambiguated. Technical debt in terminology - where the same concept is referred to by multiple names - should be regularly cleaned up.\n\n#### Common Documentation Pitfalls\n\n⚠️ **Pitfall: Circular Definitions**\nDefining terms using other undefined terms creates confusion rather than clarity. Each definition should be self-contained or reference only previously defined terms. When circular dependencies are unavoidable, provide a brief informal explanation before introducing the formal definitions.\n\n⚠️ **Pitfall: Implementation-Specific Definitions**\nDefining terms in ways that are tied to specific implementation choices limits reusability and understanding. Focus on the conceptual meaning rather than how something is implemented in a particular programming language or framework.\n\n⚠️ **Pitfall: Missing Context**\nTechnical terms often have different meanings in different domains. A \"stream\" in our log aggregation context is different from a \"stream\" in general programming or database contexts. Always provide the domain-specific context for how terms are used in this system.\n\n⚠️ **Pitfall: Stale Definitions**\nGlossaries that aren't maintained become misleading as systems evolve. Establish a review process that updates definitions when the underlying concepts change. Consider automated checks that flag potential inconsistencies between code and documentation.\n\n#### Debugging Terminology Issues\n\nWhen team members use different terms for the same concept or misunderstand established terminology, it creates communication barriers that slow development and increase bugs. Here's how to diagnose and fix terminology problems:\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Code reviews with terminology debates | Missing or unclear definitions | Check if disputed terms are in glossary | Add missing definitions with clear examples |\n| Bug reports using inconsistent language | Team members have different mental models | Review related documentation and code comments | Standardize terminology and update all references |\n| New team members asking same questions repeatedly | Key concepts not clearly documented | Track frequently asked questions | Expand glossary with commonly misunderstood terms |\n| Design discussions going in circles | Participants using same words for different concepts | Map out what each person means by key terms | Disambiguate overloaded terms with precise definitions |\n\n#### Milestone Checkpoints\n\nAfter implementing each major milestone, verify that the associated terminology is clearly understood and consistently used:\n\n**Milestone 1 Checkpoint**: Team can clearly distinguish between log entries, streams, and labels. Code reviews use consistent terminology for ingestion pipeline stages.\n\n**Milestone 2 Checkpoint**: Discussions about indexing use precise terms for inverted indexes, bloom filters, and partitioning strategies without confusion.\n\n**Milestone 3 Checkpoint**: Query-related conversations distinguish clearly between lexing, parsing, optimization, and execution phases.\n\n**Milestone 4 Checkpoint**: Storage discussions precisely differentiate between chunks, WAL records, compression, and retention policies.\n\n**Milestone 5 Checkpoint**: Multi-tenancy and alerting terminology is used consistently across security discussions and implementation.\n\nThe glossary is complete when team members can communicate complex system concepts clearly and unambiguously using the established vocabulary. This creates a foundation for maintainable code, effective debugging, and successful knowledge transfer to new team members.\n"}